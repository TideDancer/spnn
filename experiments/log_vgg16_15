Files already downloaded and verified
Files already downloaded and verified
VGGLIKE_neuron(
  (vgg): VGG(
    (features): Sequential(
      (0): Conv2d(3, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace)
      (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace)
      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (7): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (8): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (9): ReLU(inplace)
      (10): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (11): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (12): ReLU(inplace)
      (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (14): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (15): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (16): ReLU(inplace)
      (17): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (18): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (19): ReLU(inplace)
      (20): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (21): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (22): ReLU(inplace)
      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (24): Conv2d(384, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (25): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (26): ReLU(inplace)
      (27): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (28): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (29): ReLU(inplace)
      (30): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (31): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (32): ReLU(inplace)
      (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (34): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (35): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (36): ReLU(inplace)
      (37): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (38): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (39): ReLU(inplace)
      (40): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (41): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (42): ReLU(inplace)
      (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (44): AvgPool2d(kernel_size=1, stride=1, padding=0)
    )
    (classifier): Linear(in_features=768, out_features=10, bias=True)
  )
  (mask): ParameterList(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 96 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 96 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 192 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 192 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 384 (GPU 0)]
      (5): Parameter containing: [torch.cuda.FloatTensor of size 384 (GPU 0)]
      (6): Parameter containing: [torch.cuda.FloatTensor of size 384 (GPU 0)]
      (7): Parameter containing: [torch.cuda.FloatTensor of size 768 (GPU 0)]
      (8): Parameter containing: [torch.cuda.FloatTensor of size 768 (GPU 0)]
      (9): Parameter containing: [torch.cuda.FloatTensor of size 768 (GPU 0)]
      (10): Parameter containing: [torch.cuda.FloatTensor of size 768 (GPU 0)]
      (11): Parameter containing: [torch.cuda.FloatTensor of size 768 (GPU 0)]
      (12): Parameter containing: [torch.cuda.FloatTensor of size 768 (GPU 0)]
      (13): Parameter containing: [torch.cuda.FloatTensor of size 768 (GPU 0)]
    
  )
)
 ---------- recover phase -----------
/usr/local/lib/python3.6/dist-packages/torch/serialization.py:367: SourceChangeWarning: source code of class 'models.vgg.VGG' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.
  warnings.warn(msg, SourceChangeWarning)
test acc:  0.9349
optimize layer  0
backward train epoch:  1
test acc:  0.1
forward train acc:  0.75566  and loss:  387.9434115886688
test acc:  0.8275
forward train acc:  0.86886  and loss:  295.64168894290924
test acc:  0.8643
forward train acc:  0.90232  and loss:  254.15226531028748
test acc:  0.8766
forward train acc:  0.91468  and loss:  231.23978805541992
test acc:  0.8827
forward train acc:  0.92248  and loss:  216.69193786382675
test acc:  0.8849
forward train acc:  0.9252  and loss:  205.4617482125759
test acc:  0.8893
forward train acc:  0.93036  and loss:  193.8052453994751
test acc:  0.8897
forward train acc:  0.9338  and loss:  185.43667641282082
test acc:  0.8907
forward train acc:  0.9367  and loss:  178.8606595993042
test acc:  0.8915
forward train acc:  0.94  and loss:  173.30987960100174
test acc:  0.8931
********** reverse layer  0  *********
layer  0  :  0.0  ==>  96 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4166666666666667  ==>  224 / 384
layer  5  :  0.3958333333333333  ==>  232 / 384
layer  6  :  0.4166666666666667  ==>  224 / 384
layer  7  :  0.7083333333333334  ==>  224 / 768
layer  8  :  0.7916666666666666  ==>  160 / 768
layer  9  :  0.5  ==>  384 / 768
layer  10  :  0.5  ==>  384 / 768
layer  11  :  0.5416666666666666  ==>  352 / 768
layer  12  :  0.75  ==>  192 / 768
layer  13  :  0.3958333333333333  ==>  464 / 768
optimize layer  1
backward train epoch:  10
test acc:  0.1183
forward train acc:  0.9246  and loss:  179.490927785635
test acc:  0.8891
forward train acc:  0.93788  and loss:  153.81623113155365
test acc:  0.8962
forward train acc:  0.94514  and loss:  135.5366497784853
test acc:  0.8993
forward train acc:  0.9512  and loss:  122.25030913949013
test acc:  0.9003
forward train acc:  0.95282  and loss:  115.56923848390579
test acc:  0.8999
forward train acc:  0.956  and loss:  108.58138628304005
test acc:  0.9017
forward train acc:  0.95678  and loss:  102.08055029809475
test acc:  0.9033
forward train acc:  0.95894  and loss:  96.80317793786526
test acc:  0.9025
forward train acc:  0.95974  and loss:  94.19923993945122
test acc:  0.9037
forward train acc:  0.95978  and loss:  91.08274668455124
test acc:  0.9028
********** reverse layer  1  *********
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4166666666666667  ==>  224 / 384
layer  5  :  0.3958333333333333  ==>  232 / 384
layer  6  :  0.4166666666666667  ==>  224 / 384
layer  7  :  0.7083333333333334  ==>  224 / 768
layer  8  :  0.7916666666666666  ==>  160 / 768
layer  9  :  0.5  ==>  384 / 768
layer  10  :  0.5  ==>  384 / 768
layer  11  :  0.5416666666666666  ==>  352 / 768
layer  12  :  0.75  ==>  192 / 768
layer  13  :  0.3958333333333333  ==>  464 / 768
optimize layer  2
backward train epoch:  6
test acc:  0.5209
forward train acc:  0.95836  and loss:  88.53178144991398
test acc:  0.9036
forward train acc:  0.96046  and loss:  79.79346849024296
test acc:  0.9053
forward train acc:  0.96462  and loss:  70.7857665643096
test acc:  0.9065
forward train acc:  0.9662  and loss:  65.32455057650805
test acc:  0.9067
forward train acc:  0.96786  and loss:  60.94071001559496
test acc:  0.9083
forward train acc:  0.96932  and loss:  58.0133078917861
test acc:  0.9065
forward train acc:  0.97012  and loss:  54.76130034029484
test acc:  0.9083
forward train acc:  0.97074  and loss:  53.11108685284853
test acc:  0.9083
forward train acc:  0.97078  and loss:  51.86290132254362
test acc:  0.9089
forward train acc:  0.97192  and loss:  50.77578955516219
test acc:  0.9084
********** reverse layer  2  *********
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4166666666666667  ==>  224 / 384
layer  5  :  0.3958333333333333  ==>  232 / 384
layer  6  :  0.4166666666666667  ==>  224 / 384
layer  7  :  0.7083333333333334  ==>  224 / 768
layer  8  :  0.7916666666666666  ==>  160 / 768
layer  9  :  0.5  ==>  384 / 768
layer  10  :  0.5  ==>  384 / 768
layer  11  :  0.5416666666666666  ==>  352 / 768
layer  12  :  0.75  ==>  192 / 768
layer  13  :  0.3958333333333333  ==>  464 / 768
optimize layer  3
backward train epoch:  1
test acc:  0.8864
forward train acc:  0.97026  and loss:  50.2792390473187
test acc:  0.9074
forward train acc:  0.97192  and loss:  46.07875617593527
test acc:  0.9064
forward train acc:  0.97366  and loss:  42.871596448123455
test acc:  0.9077
forward train acc:  0.97508  and loss:  39.27971509844065
test acc:  0.9089
forward train acc:  0.97572  and loss:  38.18029324710369
test acc:  0.9104
forward train acc:  0.9768  and loss:  35.74101181700826
test acc:  0.9107
forward train acc:  0.97696  and loss:  35.57044876180589
test acc:  0.9114
forward train acc:  0.97822  and loss:  33.49789587780833
test acc:  0.9101
forward train acc:  0.97806  and loss:  33.74603986181319
test acc:  0.9102
forward train acc:  0.97904  and loss:  32.47171558625996
test acc:  0.9108
********** reverse layer  3  *********
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4166666666666667  ==>  224 / 384
layer  5  :  0.3958333333333333  ==>  232 / 384
layer  6  :  0.4166666666666667  ==>  224 / 384
layer  7  :  0.7083333333333334  ==>  224 / 768
layer  8  :  0.7916666666666666  ==>  160 / 768
layer  9  :  0.5  ==>  384 / 768
layer  10  :  0.5  ==>  384 / 768
layer  11  :  0.5416666666666666  ==>  352 / 768
layer  12  :  0.75  ==>  192 / 768
layer  13  :  0.3958333333333333  ==>  464 / 768
optimize layer  4
backward train epoch:  1
test acc:  0.7652
forward train acc:  0.97812  and loss:  33.15560276992619
test acc:  0.9103
forward train acc:  0.9797  and loss:  30.699135400354862
test acc:  0.9129
forward train acc:  0.9796  and loss:  29.28490937501192
test acc:  0.9129
forward train acc:  0.98038  and loss:  28.06261624582112
test acc:  0.9125
forward train acc:  0.98238  and loss:  26.304010305553675
test acc:  0.912
forward train acc:  0.9827  and loss:  24.700411953032017
test acc:  0.9134
forward train acc:  0.98376  and loss:  23.79837826639414
test acc:  0.9128
forward train acc:  0.9843  and loss:  23.088394032791257
test acc:  0.9138
forward train acc:  0.98514  and loss:  21.60514528118074
test acc:  0.9129
forward train acc:  0.98494  and loss:  21.84316110238433
test acc:  0.9128
********** reverse layer  4  *********
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4166666666666667  ==>  224 / 384
layer  5  :  0.3958333333333333  ==>  232 / 384
layer  6  :  0.4166666666666667  ==>  224 / 384
layer  7  :  0.7083333333333334  ==>  224 / 768
layer  8  :  0.7916666666666666  ==>  160 / 768
layer  9  :  0.5  ==>  384 / 768
layer  10  :  0.5  ==>  384 / 768
layer  11  :  0.5416666666666666  ==>  352 / 768
layer  12  :  0.75  ==>  192 / 768
layer  13  :  0.3958333333333333  ==>  464 / 768
optimize layer  5
backward train epoch:  3
test acc:  0.802
forward train acc:  0.98256  and loss:  23.694972960278392
test acc:  0.9118
forward train acc:  0.98318  and loss:  23.527488010004163
test acc:  0.912
forward train acc:  0.98468  and loss:  21.561951650306582
test acc:  0.9128
forward train acc:  0.98616  and loss:  19.065134967677295
test acc:  0.9128
forward train acc:  0.98648  and loss:  19.18055852688849
test acc:  0.9123
forward train acc:  0.98654  and loss:  18.691279293969274
test acc:  0.9132
forward train acc:  0.98696  and loss:  18.19364745914936
test acc:  0.9121
forward train acc:  0.98766  and loss:  17.39641147106886
test acc:  0.9129
forward train acc:  0.9888  and loss:  15.961679019033909
test acc:  0.9128
forward train acc:  0.98846  and loss:  16.191568727605045
test acc:  0.9129
********** reverse layer  5  *********
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4166666666666667  ==>  224 / 384
layer  5  :  0.3958333333333333  ==>  232 / 384
layer  6  :  0.4166666666666667  ==>  224 / 384
layer  7  :  0.7083333333333334  ==>  224 / 768
layer  8  :  0.7916666666666666  ==>  160 / 768
layer  9  :  0.5  ==>  384 / 768
layer  10  :  0.5  ==>  384 / 768
layer  11  :  0.5416666666666666  ==>  352 / 768
layer  12  :  0.75  ==>  192 / 768
layer  13  :  0.3958333333333333  ==>  464 / 768
optimize layer  6
backward train epoch:  6
test acc:  0.6327
forward train acc:  0.98682  and loss:  17.613937842659652
test acc:  0.9127
forward train acc:  0.98732  and loss:  17.282328153960407
test acc:  0.9113
forward train acc:  0.98754  and loss:  16.94494474120438
test acc:  0.9115
forward train acc:  0.98842  and loss:  15.19383226428181
test acc:  0.9147
forward train acc:  0.9894  and loss:  14.755863212049007
test acc:  0.9126
forward train acc:  0.99006  and loss:  13.755804487504065
test acc:  0.9119
forward train acc:  0.98984  and loss:  13.863769196905196
test acc:  0.9133
forward train acc:  0.99094  and loss:  13.020340125076473
test acc:  0.9149
forward train acc:  0.99176  and loss:  11.980212626047432
test acc:  0.9146
forward train acc:  0.99132  and loss:  12.049072780646384
test acc:  0.9153
********** reverse layer  6  *********
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4166666666666667  ==>  224 / 384
layer  5  :  0.3958333333333333  ==>  232 / 384
layer  6  :  0.4166666666666667  ==>  224 / 384
layer  7  :  0.7083333333333334  ==>  224 / 768
layer  8  :  0.7916666666666666  ==>  160 / 768
layer  9  :  0.5  ==>  384 / 768
layer  10  :  0.5  ==>  384 / 768
layer  11  :  0.5416666666666666  ==>  352 / 768
layer  12  :  0.75  ==>  192 / 768
layer  13  :  0.3958333333333333  ==>  464 / 768
optimize layer  7
backward train epoch:  78
test acc:  0.1183
forward train acc:  0.98936  and loss:  13.90525946766138
test acc:  0.9132
forward train acc:  0.98964  and loss:  13.675903402268887
test acc:  0.9125
forward train acc:  0.98932  and loss:  13.428964941762388
test acc:  0.9139
forward train acc:  0.9907  and loss:  12.370907382108271
test acc:  0.9133
forward train acc:  0.99162  and loss:  11.159188057295978
test acc:  0.914
forward train acc:  0.9923  and loss:  10.563862770795822
test acc:  0.9136
forward train acc:  0.9921  and loss:  10.68572346959263
test acc:  0.9151
forward train acc:  0.99284  and loss:  10.25782575365156
test acc:  0.9151
forward train acc:  0.99342  and loss:  9.25377482548356
test acc:  0.9149
forward train acc:  0.9928  and loss:  9.964188426733017
test acc:  0.9155
********** reverse layer  7  *********
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4166666666666667  ==>  224 / 384
layer  5  :  0.3958333333333333  ==>  232 / 384
layer  6  :  0.4166666666666667  ==>  224 / 384
layer  7  :  0.7083333333333334  ==>  224 / 768
layer  8  :  0.7916666666666666  ==>  160 / 768
layer  9  :  0.5  ==>  384 / 768
layer  10  :  0.5  ==>  384 / 768
layer  11  :  0.5416666666666666  ==>  352 / 768
layer  12  :  0.75  ==>  192 / 768
layer  13  :  0.3958333333333333  ==>  464 / 768
optimize layer  8
backward train epoch:  6
test acc:  0.6155
forward train acc:  0.99134  and loss:  11.75669835228473
test acc:  0.914
forward train acc:  0.99196  and loss:  10.920078728348017
test acc:  0.914
forward train acc:  0.99106  and loss:  11.550355348736048
test acc:  0.9131
forward train acc:  0.99246  and loss:  9.422034904360771
test acc:  0.9136
forward train acc:  0.99292  and loss:  9.210957581177354
test acc:  0.9163
forward train acc:  0.9938  and loss:  8.713770012371242
test acc:  0.915
forward train acc:  0.99402  and loss:  8.485545423813164
test acc:  0.9146
forward train acc:  0.99366  and loss:  8.458474222570658
test acc:  0.9148
forward train acc:  0.99456  and loss:  7.4558594096452
test acc:  0.915
forward train acc:  0.99406  and loss:  7.719198353588581
test acc:  0.9169
********** reverse layer  8  *********
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4166666666666667  ==>  224 / 384
layer  5  :  0.3958333333333333  ==>  232 / 384
layer  6  :  0.4166666666666667  ==>  224 / 384
layer  7  :  0.7083333333333334  ==>  224 / 768
layer  8  :  0.7916666666666666  ==>  160 / 768
layer  9  :  0.5  ==>  384 / 768
layer  10  :  0.5  ==>  384 / 768
layer  11  :  0.5416666666666666  ==>  352 / 768
layer  12  :  0.75  ==>  192 / 768
layer  13  :  0.3958333333333333  ==>  464 / 768
optimize layer  9
backward train epoch:  97
test acc:  0.1236
forward train acc:  0.99254  and loss:  9.93001875653863
test acc:  0.9143
forward train acc:  0.9923  and loss:  9.71142777428031
test acc:  0.9135
forward train acc:  0.99314  and loss:  9.052067254669964
test acc:  0.9147
forward train acc:  0.99454  and loss:  7.616393877426162
test acc:  0.9161
forward train acc:  0.99444  and loss:  7.440553821623325
test acc:  0.9167
forward train acc:  0.99462  and loss:  7.26007988397032
test acc:  0.9162
forward train acc:  0.99428  and loss:  7.325147109106183
test acc:  0.915
forward train acc:  0.99496  and loss:  6.802123786881566
test acc:  0.9172
forward train acc:  0.99562  and loss:  6.394435140304267
test acc:  0.9163
forward train acc:  0.99536  and loss:  6.494454231578857
test acc:  0.9165
********** reverse layer  9  *********
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4166666666666667  ==>  224 / 384
layer  5  :  0.3958333333333333  ==>  232 / 384
layer  6  :  0.4166666666666667  ==>  224 / 384
layer  7  :  0.7083333333333334  ==>  224 / 768
layer  8  :  0.7916666666666666  ==>  160 / 768
layer  9  :  0.5  ==>  384 / 768
layer  10  :  0.5  ==>  384 / 768
layer  11  :  0.5416666666666666  ==>  352 / 768
layer  12  :  0.75  ==>  192 / 768
layer  13  :  0.3958333333333333  ==>  464 / 768
optimize layer  10
backward train epoch:  44
test acc:  0.1288
forward train acc:  0.9949  and loss:  6.976585511118174
test acc:  0.9137
forward train acc:  0.99432  and loss:  7.216601279564202
test acc:  0.9166
forward train acc:  0.9941  and loss:  7.579613330541179
test acc:  0.9142
forward train acc:  0.9945  and loss:  6.9636586382985115
test acc:  0.9161
forward train acc:  0.99536  and loss:  5.637589174788445
test acc:  0.9143
forward train acc:  0.99546  and loss:  6.130272228270769
test acc:  0.9164
forward train acc:  0.9954  and loss:  5.756396770011634
test acc:  0.916
forward train acc:  0.99594  and loss:  5.3278214677702636
test acc:  0.916
forward train acc:  0.99594  and loss:  5.3554491410031915
test acc:  0.9164
forward train acc:  0.99568  and loss:  5.690301188267767
test acc:  0.9174
********** reverse layer  10  *********
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4166666666666667  ==>  224 / 384
layer  5  :  0.3958333333333333  ==>  232 / 384
layer  6  :  0.4166666666666667  ==>  224 / 384
layer  7  :  0.7083333333333334  ==>  224 / 768
layer  8  :  0.7916666666666666  ==>  160 / 768
layer  9  :  0.5  ==>  384 / 768
layer  10  :  0.5  ==>  384 / 768
layer  11  :  0.5416666666666666  ==>  352 / 768
layer  12  :  0.75  ==>  192 / 768
layer  13  :  0.3958333333333333  ==>  464 / 768
optimize layer  11
backward train epoch:  33
test acc:  0.125
forward train acc:  0.99482  and loss:  6.624003884382546
test acc:  0.9143
forward train acc:  0.99468  and loss:  6.2266451709438115
test acc:  0.9167
forward train acc:  0.9948  and loss:  6.462580373510718
test acc:  0.9153
forward train acc:  0.99552  and loss:  5.882767739705741
test acc:  0.915
forward train acc:  0.99608  and loss:  5.46249183267355
test acc:  0.9143
forward train acc:  0.99604  and loss:  5.106516303960234
test acc:  0.9164
forward train acc:  0.99586  and loss:  5.051360955927521
test acc:  0.9145
forward train acc:  0.99632  and loss:  4.951890302821994
test acc:  0.9153
forward train acc:  0.997  and loss:  4.475021632853895
test acc:  0.9163
forward train acc:  0.99652  and loss:  4.429046832956374
test acc:  0.9174
********** reverse layer  11  *********
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4166666666666667  ==>  224 / 384
layer  5  :  0.3958333333333333  ==>  232 / 384
layer  6  :  0.4166666666666667  ==>  224 / 384
layer  7  :  0.7083333333333334  ==>  224 / 768
layer  8  :  0.7916666666666666  ==>  160 / 768
layer  9  :  0.5  ==>  384 / 768
layer  10  :  0.5  ==>  384 / 768
layer  11  :  0.5416666666666666  ==>  352 / 768
layer  12  :  0.75  ==>  192 / 768
layer  13  :  0.3958333333333333  ==>  464 / 768
optimize layer  12
backward train epoch:  121
test acc:  0.0931
forward train acc:  0.9958  and loss:  10.1624846579507
test acc:  0.9171
forward train acc:  0.99532  and loss:  9.497472469694912
test acc:  0.9188
forward train acc:  0.99556  and loss:  8.434265437535942
test acc:  0.9182
forward train acc:  0.99652  and loss:  7.279761604964733
test acc:  0.918
forward train acc:  0.9964  and loss:  6.889890521764755
test acc:  0.9156
forward train acc:  0.99668  and loss:  6.789332517422736
test acc:  0.9184
forward train acc:  0.99706  and loss:  6.1982760252431035
test acc:  0.9183
forward train acc:  0.9968  and loss:  6.142926854081452
test acc:  0.9189
forward train acc:  0.99702  and loss:  5.8449355168268085
test acc:  0.9175
forward train acc:  0.99726  and loss:  5.366052193567157
test acc:  0.9181
********** reverse layer  12  *********
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4166666666666667  ==>  224 / 384
layer  5  :  0.3958333333333333  ==>  232 / 384
layer  6  :  0.4166666666666667  ==>  224 / 384
layer  7  :  0.7083333333333334  ==>  224 / 768
layer  8  :  0.7916666666666666  ==>  160 / 768
layer  9  :  0.5  ==>  384 / 768
layer  10  :  0.5  ==>  384 / 768
layer  11  :  0.5416666666666666  ==>  352 / 768
layer  12  :  0.75  ==>  192 / 768
layer  13  :  0.3958333333333333  ==>  464 / 768
optimize layer  13
backward train epoch:  102
test acc:  0.1184
forward train acc:  0.9963  and loss:  4.789043121505529
test acc:  0.9164
forward train acc:  0.99586  and loss:  5.379211537539959
test acc:  0.9162
forward train acc:  0.9963  and loss:  4.767464588745497
test acc:  0.9161
forward train acc:  0.99672  and loss:  4.35348246130161
test acc:  0.9175
forward train acc:  0.9966  and loss:  4.730383764370345
test acc:  0.9187
forward train acc:  0.99684  and loss:  4.280225185677409
test acc:  0.9181
forward train acc:  0.99718  and loss:  3.728438221849501
test acc:  0.9188
forward train acc:  0.99744  and loss:  3.213224715553224
test acc:  0.9173
forward train acc:  0.9972  and loss:  3.451871944242157
test acc:  0.9189
forward train acc:  0.99752  and loss:  3.6425361442379653
test acc:  0.9187
********** reverse layer  13  *********
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4166666666666667  ==>  224 / 384
layer  5  :  0.3958333333333333  ==>  232 / 384
layer  6  :  0.4166666666666667  ==>  224 / 384
layer  7  :  0.7083333333333334  ==>  224 / 768
layer  8  :  0.7916666666666666  ==>  160 / 768
layer  9  :  0.5  ==>  384 / 768
layer  10  :  0.5  ==>  384 / 768
layer  11  :  0.5416666666666666  ==>  352 / 768
layer  12  :  0.75  ==>  192 / 768
layer  13  :  0.3958333333333333  ==>  464 / 768
***** skip layer  0
[4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
***** skip layer  1
[4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
***** skip layer  2
[4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
***** skip layer  3
[4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
***** skip layer  4
[4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5]
***** skip layer  5
[4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5]
***** skip layer  6
[4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5]
***** skip layer  7
[4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5]
***** skip layer  8
[4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5]
***** skip layer  9
[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5]
***** skip layer  10
[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5]
***** skip layer  11
[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5]
***** skip layer  12
[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5]
***** skip layer  13
[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]
***** skip layer  0
[3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]
***** skip layer  1
[3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]
***** skip layer  2
[3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]
***** skip layer  3
[3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]
***** skip layer  4
[3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4]
***** skip layer  5
[3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4]
***** skip layer  6
[3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4]
***** skip layer  7
[3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4]
***** skip layer  8
[3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4]
***** skip layer  9
[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4]
***** skip layer  10
[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4]
***** skip layer  11
[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4]
***** skip layer  12
[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4]
***** skip layer  13
[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
***** skip layer  0
[2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
***** skip layer  1
[2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
***** skip layer  2
[2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
***** skip layer  3
[2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
***** skip layer  4
[2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3]
***** skip layer  5
[2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3]
***** skip layer  6
[2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3]
***** skip layer  7
[2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3]
***** skip layer  8
[2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3]
***** skip layer  9
[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3]
***** skip layer  10
[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3]
***** skip layer  11
[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3]
***** skip layer  12
[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3]
***** skip layer  13
[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
***** skip layer  0
[1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
***** skip layer  1
[1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
***** skip layer  2
[1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
***** skip layer  3
[1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
***** skip layer  4
[1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2]
***** skip layer  5
[1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2]
***** skip layer  6
[1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2]
***** skip layer  7
[1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2]
***** skip layer  8
[1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2]
***** skip layer  9
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2]
***** skip layer  10
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2]
***** skip layer  11
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2]
***** skip layer  12
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]
***** skip layer  13
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
***** skip layer  0
[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
***** skip layer  1
[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
***** skip layer  2
[0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
***** skip layer  3
[0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
***** skip layer  4
[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]
***** skip layer  5
[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]
***** skip layer  6
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]
***** skip layer  7
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]
***** skip layer  8
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
***** skip layer  9
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]
***** skip layer  10
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]
***** skip layer  11
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]
***** skip layer  12
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
***** skip layer  13
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
optimize layer  0
backward train epoch:  28
test acc:  0.1552
forward train acc:  0.99686  and loss:  4.0061603331705555
test acc:  0.9182
forward train acc:  0.99628  and loss:  4.503628416452557
test acc:  0.9183
forward train acc:  0.99636  and loss:  4.1578647054266185
test acc:  0.9154
forward train acc:  0.9967  and loss:  4.090003937948495
test acc:  0.9175
forward train acc:  0.9967  and loss:  4.056061140843667
test acc:  0.9169
forward train acc:  0.9966  and loss:  4.068655801820569
test acc:  0.9166
forward train acc:  0.9973  and loss:  3.122073670849204
test acc:  0.9168
forward train acc:  0.99774  and loss:  3.121692982967943
test acc:  0.9174
forward train acc:  0.99768  and loss:  2.8730135415680707
test acc:  0.9175
forward train acc:  0.99764  and loss:  3.066779084969312
test acc:  0.9181
********** reverse layer  0  *********
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4166666666666667  ==>  224 / 384
layer  5  :  0.3958333333333333  ==>  232 / 384
layer  6  :  0.4166666666666667  ==>  224 / 384
layer  7  :  0.7083333333333334  ==>  224 / 768
layer  8  :  0.7916666666666666  ==>  160 / 768
layer  9  :  0.5  ==>  384 / 768
layer  10  :  0.5  ==>  384 / 768
layer  11  :  0.5416666666666666  ==>  352 / 768
layer  12  :  0.75  ==>  192 / 768
layer  13  :  0.3958333333333333  ==>  464 / 768
optimize layer  1
backward train epoch:  7
test acc:  0.5389
forward train acc:  0.99694  and loss:  3.820116514340043
test acc:  0.9183
forward train acc:  0.9966  and loss:  4.260369632393122
test acc:  0.9169
forward train acc:  0.99656  and loss:  4.33616709522903
test acc:  0.9149
forward train acc:  0.99638  and loss:  4.279664023430087
test acc:  0.9168
forward train acc:  0.99752  and loss:  3.1045924723148346
test acc:  0.9174
forward train acc:  0.99726  and loss:  3.335280562983826
test acc:  0.9167
forward train acc:  0.9979  and loss:  2.9579291796544567
test acc:  0.9169
forward train acc:  0.99788  and loss:  2.8483228778932244
test acc:  0.918
forward train acc:  0.99794  and loss:  2.7137267091311514
test acc:  0.9173
forward train acc:  0.99754  and loss:  3.0109750271076337
test acc:  0.9166
********** reverse layer  1  *********
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4166666666666667  ==>  224 / 384
layer  5  :  0.3958333333333333  ==>  232 / 384
layer  6  :  0.4166666666666667  ==>  224 / 384
layer  7  :  0.7083333333333334  ==>  224 / 768
layer  8  :  0.7916666666666666  ==>  160 / 768
layer  9  :  0.5  ==>  384 / 768
layer  10  :  0.5  ==>  384 / 768
layer  11  :  0.5416666666666666  ==>  352 / 768
layer  12  :  0.75  ==>  192 / 768
layer  13  :  0.3958333333333333  ==>  464 / 768
optimize layer  2
backward train epoch:  348
test acc:  0.1039
forward train acc:  0.99364  and loss:  7.655125314486213
test acc:  0.9146
forward train acc:  0.99394  and loss:  6.870279642753303
test acc:  0.9155
forward train acc:  0.99456  and loss:  6.364233760163188
test acc:  0.9148
forward train acc:  0.99572  and loss:  5.099550779443234
test acc:  0.9183
forward train acc:  0.99602  and loss:  4.7840381199494
test acc:  0.919
forward train acc:  0.99646  and loss:  4.182393637485802
test acc:  0.9188
forward train acc:  0.99674  and loss:  4.0746004356769845
test acc:  0.9179
forward train acc:  0.99684  and loss:  4.097764315432869
test acc:  0.917
forward train acc:  0.99674  and loss:  4.049982391297817
test acc:  0.9182
forward train acc:  0.99682  and loss:  3.686251995153725
test acc:  0.9179
********** reverse layer  2  *********
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4166666666666667  ==>  224 / 384
layer  5  :  0.3958333333333333  ==>  232 / 384
layer  6  :  0.4166666666666667  ==>  224 / 384
layer  7  :  0.7083333333333334  ==>  224 / 768
layer  8  :  0.7916666666666666  ==>  160 / 768
layer  9  :  0.5  ==>  384 / 768
layer  10  :  0.5  ==>  384 / 768
layer  11  :  0.5416666666666666  ==>  352 / 768
layer  12  :  0.75  ==>  192 / 768
layer  13  :  0.3958333333333333  ==>  464 / 768
optimize layer  3
backward train epoch:  1
test acc:  0.7981
forward train acc:  0.99638  and loss:  4.54611889179796
test acc:  0.9156
forward train acc:  0.99698  and loss:  3.7937843916006386
test acc:  0.9156
forward train acc:  0.9968  and loss:  3.8100507324561477
test acc:  0.9164
forward train acc:  0.9969  and loss:  3.7458606446161866
test acc:  0.9171
forward train acc:  0.99784  and loss:  2.924317713594064
test acc:  0.9169
forward train acc:  0.99786  and loss:  2.8567886343225837
test acc:  0.9173
forward train acc:  0.99784  and loss:  2.6746770865283906
test acc:  0.9174
forward train acc:  0.99792  and loss:  2.509493053337792
test acc:  0.9183
forward train acc:  0.99808  and loss:  2.4179471569368616
test acc:  0.9179
forward train acc:  0.99808  and loss:  2.494768576696515
test acc:  0.9181
********** reverse layer  3  *********
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4166666666666667  ==>  224 / 384
layer  5  :  0.3958333333333333  ==>  232 / 384
layer  6  :  0.4166666666666667  ==>  224 / 384
layer  7  :  0.7083333333333334  ==>  224 / 768
layer  8  :  0.7916666666666666  ==>  160 / 768
layer  9  :  0.5  ==>  384 / 768
layer  10  :  0.5  ==>  384 / 768
layer  11  :  0.5416666666666666  ==>  352 / 768
layer  12  :  0.75  ==>  192 / 768
layer  13  :  0.3958333333333333  ==>  464 / 768
optimize layer  4
backward train epoch:  1
test acc:  0.7887
forward train acc:  0.99706  and loss:  3.6409283303655684
test acc:  0.9176
forward train acc:  0.99708  and loss:  3.554089677054435
test acc:  0.9167
forward train acc:  0.9968  and loss:  3.711540521355346
test acc:  0.919
forward train acc:  0.99768  and loss:  2.971245359396562
test acc:  0.9194
forward train acc:  0.99776  and loss:  2.8491914914920926
test acc:  0.9184
forward train acc:  0.99766  and loss:  2.722245073877275
test acc:  0.9197
forward train acc:  0.99792  and loss:  2.595181834883988
test acc:  0.921
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4270833333333333  ==>  220 / 384
layer  5  :  0.3958333333333333  ==>  232 / 384
layer  6  :  0.4166666666666667  ==>  224 / 384
layer  7  :  0.7083333333333334  ==>  224 / 768
layer  8  :  0.7916666666666666  ==>  160 / 768
layer  9  :  0.5  ==>  384 / 768
layer  10  :  0.5  ==>  384 / 768
layer  11  :  0.5416666666666666  ==>  352 / 768
layer  12  :  0.75  ==>  192 / 768
layer  13  :  0.3958333333333333  ==>  464 / 768
optimize layer  5
backward train epoch:  9
test acc:  0.4083
forward train acc:  0.9969  and loss:  4.325318173971027
test acc:  0.9202
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4270833333333333  ==>  220 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4166666666666667  ==>  224 / 384
layer  7  :  0.7083333333333334  ==>  224 / 768
layer  8  :  0.7916666666666666  ==>  160 / 768
layer  9  :  0.5  ==>  384 / 768
layer  10  :  0.5  ==>  384 / 768
layer  11  :  0.5416666666666666  ==>  352 / 768
layer  12  :  0.75  ==>  192 / 768
layer  13  :  0.3958333333333333  ==>  464 / 768
optimize layer  6
backward train epoch:  2
test acc:  0.8636
forward train acc:  0.9968  and loss:  3.8595320135354996
test acc:  0.919
forward train acc:  0.99666  and loss:  4.201321609551087
test acc:  0.9175
forward train acc:  0.99746  and loss:  3.2441886654123664
test acc:  0.9191
forward train acc:  0.9974  and loss:  3.321784193161875
test acc:  0.9207
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4270833333333333  ==>  220 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4270833333333333  ==>  220 / 384
layer  7  :  0.7083333333333334  ==>  224 / 768
layer  8  :  0.7916666666666666  ==>  160 / 768
layer  9  :  0.5  ==>  384 / 768
layer  10  :  0.5  ==>  384 / 768
layer  11  :  0.5416666666666666  ==>  352 / 768
layer  12  :  0.75  ==>  192 / 768
layer  13  :  0.3958333333333333  ==>  464 / 768
optimize layer  7
backward train epoch:  97
test acc:  0.1335
forward train acc:  0.9972  and loss:  3.5515770367346704
test acc:  0.9183
forward train acc:  0.99636  and loss:  4.294202617369592
test acc:  0.919
forward train acc:  0.9972  and loss:  3.663012571632862
test acc:  0.9196
forward train acc:  0.99764  and loss:  2.996944554615766
test acc:  0.9205
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4270833333333333  ==>  220 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4270833333333333  ==>  220 / 384
layer  7  :  0.71875  ==>  216 / 768
layer  8  :  0.7916666666666666  ==>  160 / 768
layer  9  :  0.5  ==>  384 / 768
layer  10  :  0.5  ==>  384 / 768
layer  11  :  0.5416666666666666  ==>  352 / 768
layer  12  :  0.75  ==>  192 / 768
layer  13  :  0.3958333333333333  ==>  464 / 768
optimize layer  8
backward train epoch:  32
test acc:  0.1202
forward train acc:  0.99718  and loss:  3.5098138201865368
test acc:  0.9195
forward train acc:  0.99716  and loss:  3.3345565229537897
test acc:  0.9202
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4270833333333333  ==>  220 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4270833333333333  ==>  220 / 384
layer  7  :  0.71875  ==>  216 / 768
layer  8  :  0.8020833333333334  ==>  152 / 768
layer  9  :  0.5  ==>  384 / 768
layer  10  :  0.5  ==>  384 / 768
layer  11  :  0.5416666666666666  ==>  352 / 768
layer  12  :  0.75  ==>  192 / 768
layer  13  :  0.3958333333333333  ==>  464 / 768
optimize layer  9
backward train epoch:  133
test acc:  0.0885
forward train acc:  0.99726  and loss:  3.428732776083052
test acc:  0.921
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4270833333333333  ==>  220 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4270833333333333  ==>  220 / 384
layer  7  :  0.71875  ==>  216 / 768
layer  8  :  0.8020833333333334  ==>  152 / 768
layer  9  :  0.5078125  ==>  378 / 768
layer  10  :  0.5  ==>  384 / 768
layer  11  :  0.5416666666666666  ==>  352 / 768
layer  12  :  0.75  ==>  192 / 768
layer  13  :  0.3958333333333333  ==>  464 / 768
optimize layer  10
backward train epoch:  300
test acc:  0.0959
forward train acc:  0.99722  and loss:  3.4165271501988173
test acc:  0.9183
forward train acc:  0.99708  and loss:  3.820488910016138
test acc:  0.919
forward train acc:  0.99768  and loss:  2.8521185127901845
test acc:  0.9195
forward train acc:  0.99744  and loss:  2.9362723883241415
test acc:  0.919
forward train acc:  0.99832  and loss:  2.1182968677021563
test acc:  0.921
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4270833333333333  ==>  220 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4270833333333333  ==>  220 / 384
layer  7  :  0.71875  ==>  216 / 768
layer  8  :  0.8020833333333334  ==>  152 / 768
layer  9  :  0.5078125  ==>  378 / 768
layer  10  :  0.5078125  ==>  378 / 768
layer  11  :  0.5416666666666666  ==>  352 / 768
layer  12  :  0.75  ==>  192 / 768
layer  13  :  0.3958333333333333  ==>  464 / 768
optimize layer  11
backward train epoch:  93
test acc:  0.1078
forward train acc:  0.99726  and loss:  3.2588388955336995
test acc:  0.9197
forward train acc:  0.9978  and loss:  2.64236055361107
test acc:  0.9202
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4270833333333333  ==>  220 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4270833333333333  ==>  220 / 384
layer  7  :  0.71875  ==>  216 / 768
layer  8  :  0.8020833333333334  ==>  152 / 768
layer  9  :  0.5078125  ==>  378 / 768
layer  10  :  0.5078125  ==>  378 / 768
layer  11  :  0.5494791666666666  ==>  346 / 768
layer  12  :  0.75  ==>  192 / 768
layer  13  :  0.3958333333333333  ==>  464 / 768
optimize layer  12
backward train epoch:  95
test acc:  0.1108
forward train acc:  0.99768  and loss:  4.138370797038078
test acc:  0.9188
forward train acc:  0.99778  and loss:  3.916629770770669
test acc:  0.9207
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4270833333333333  ==>  220 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4270833333333333  ==>  220 / 384
layer  7  :  0.71875  ==>  216 / 768
layer  8  :  0.8020833333333334  ==>  152 / 768
layer  9  :  0.5078125  ==>  378 / 768
layer  10  :  0.5078125  ==>  378 / 768
layer  11  :  0.5494791666666666  ==>  346 / 768
layer  12  :  0.7604166666666666  ==>  184 / 768
layer  13  :  0.3958333333333333  ==>  464 / 768
optimize layer  13
backward train epoch:  102
test acc:  0.084
forward train acc:  0.99788  and loss:  3.576416963711381
test acc:  0.9196
forward train acc:  0.99782  and loss:  3.4985148240812123
test acc:  0.918
forward train acc:  0.9976  and loss:  3.3432453237473965
test acc:  0.9184
forward train acc:  0.99838  and loss:  2.570733668282628
test acc:  0.9187
forward train acc:  0.9981  and loss:  2.7251931173959747
test acc:  0.9208
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4270833333333333  ==>  220 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4270833333333333  ==>  220 / 384
layer  7  :  0.71875  ==>  216 / 768
layer  8  :  0.8020833333333334  ==>  152 / 768
layer  9  :  0.5078125  ==>  378 / 768
layer  10  :  0.5078125  ==>  378 / 768
layer  11  :  0.5494791666666666  ==>  346 / 768
layer  12  :  0.7604166666666666  ==>  184 / 768
layer  13  :  0.4036458333333333  ==>  458 / 768
***** skip layer  0
[4, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
***** skip layer  1
[4, 4, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
***** skip layer  2
[4, 4, 4, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
***** skip layer  3
[4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
optimize layer  4
backward train epoch:  1
test acc:  0.8999
forward train acc:  0.99762  and loss:  3.6218724456848577
test acc:  0.9177
forward train acc:  0.99716  and loss:  3.961722339503467
test acc:  0.918
forward train acc:  0.99756  and loss:  3.55289943004027
test acc:  0.9149
forward train acc:  0.99792  and loss:  3.109161795116961
test acc:  0.9196
forward train acc:  0.99832  and loss:  2.4874194064177573
test acc:  0.9189
forward train acc:  0.99812  and loss:  2.848400396062061
test acc:  0.9198
forward train acc:  0.99828  and loss:  2.7617878606542945
test acc:  0.9209
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4375  ==>  216 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4270833333333333  ==>  220 / 384
layer  7  :  0.71875  ==>  216 / 768
layer  8  :  0.8020833333333334  ==>  152 / 768
layer  9  :  0.5078125  ==>  378 / 768
layer  10  :  0.5078125  ==>  378 / 768
layer  11  :  0.5494791666666666  ==>  346 / 768
layer  12  :  0.7604166666666666  ==>  184 / 768
layer  13  :  0.4036458333333333  ==>  458 / 768
optimize layer  5
backward train epoch:  3
test acc:  0.8331
forward train acc:  0.99732  and loss:  3.541627268306911
test acc:  0.9172
forward train acc:  0.9972  and loss:  3.605456036515534
test acc:  0.919
forward train acc:  0.99764  and loss:  3.255012394976802
test acc:  0.9167
forward train acc:  0.99802  and loss:  2.676718711387366
test acc:  0.919
forward train acc:  0.99806  and loss:  2.7199624406639487
test acc:  0.9179
forward train acc:  0.99822  and loss:  2.620314163621515
test acc:  0.9189
forward train acc:  0.99822  and loss:  2.5719954017549753
test acc:  0.9186
forward train acc:  0.9986  and loss:  2.2108662603423
test acc:  0.9197
forward train acc:  0.99854  and loss:  2.148392978589982
test acc:  0.9196
forward train acc:  0.99874  and loss:  1.8727954311762005
test acc:  0.9191
********** reverse layer  5  *********
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4375  ==>  216 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4270833333333333  ==>  220 / 384
layer  7  :  0.71875  ==>  216 / 768
layer  8  :  0.8020833333333334  ==>  152 / 768
layer  9  :  0.5078125  ==>  378 / 768
layer  10  :  0.5078125  ==>  378 / 768
layer  11  :  0.5494791666666666  ==>  346 / 768
layer  12  :  0.7604166666666666  ==>  184 / 768
layer  13  :  0.4036458333333333  ==>  458 / 768
optimize layer  6
backward train epoch:  25
test acc:  0.103
forward train acc:  0.99814  and loss:  2.621178642846644
test acc:  0.9188
forward train acc:  0.99792  and loss:  2.615388386650011
test acc:  0.9176
forward train acc:  0.998  and loss:  2.899184918263927
test acc:  0.9182
forward train acc:  0.9981  and loss:  2.7015399306546897
test acc:  0.9207
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4375  ==>  216 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.71875  ==>  216 / 768
layer  8  :  0.8020833333333334  ==>  152 / 768
layer  9  :  0.5078125  ==>  378 / 768
layer  10  :  0.5078125  ==>  378 / 768
layer  11  :  0.5494791666666666  ==>  346 / 768
layer  12  :  0.7604166666666666  ==>  184 / 768
layer  13  :  0.4036458333333333  ==>  458 / 768
optimize layer  7
backward train epoch:  25
test acc:  0.1042
forward train acc:  0.99794  and loss:  2.656917709042318
test acc:  0.9177
forward train acc:  0.99786  and loss:  3.0154785132035613
test acc:  0.918
forward train acc:  0.99812  and loss:  2.5868339436128736
test acc:  0.9165
forward train acc:  0.99836  and loss:  2.1779929334297776
test acc:  0.9202
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4375  ==>  216 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7291666666666666  ==>  208 / 768
layer  8  :  0.8020833333333334  ==>  152 / 768
layer  9  :  0.5078125  ==>  378 / 768
layer  10  :  0.5078125  ==>  378 / 768
layer  11  :  0.5494791666666666  ==>  346 / 768
layer  12  :  0.7604166666666666  ==>  184 / 768
layer  13  :  0.4036458333333333  ==>  458 / 768
optimize layer  8
backward train epoch:  26
test acc:  0.1283
forward train acc:  0.9978  and loss:  2.800188094493933
test acc:  0.918
forward train acc:  0.9979  and loss:  2.532761065522209
test acc:  0.9186
forward train acc:  0.99798  and loss:  2.8043046593666077
test acc:  0.9174
forward train acc:  0.9981  and loss:  2.431410513818264
test acc:  0.9196
forward train acc:  0.99868  and loss:  1.8286589464405552
test acc:  0.9193
forward train acc:  0.9985  and loss:  1.978716331825126
test acc:  0.9199
forward train acc:  0.99896  and loss:  1.6927787017193623
test acc:  0.9197
forward train acc:  0.99858  and loss:  1.8785583147546276
test acc:  0.9205
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4375  ==>  216 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7291666666666666  ==>  208 / 768
layer  8  :  0.8125  ==>  144 / 768
layer  9  :  0.5078125  ==>  378 / 768
layer  10  :  0.5078125  ==>  378 / 768
layer  11  :  0.5494791666666666  ==>  346 / 768
layer  12  :  0.7604166666666666  ==>  184 / 768
layer  13  :  0.4036458333333333  ==>  458 / 768
optimize layer  9
backward train epoch:  127
test acc:  0.1183
forward train acc:  0.99854  and loss:  2.121625011553988
test acc:  0.9191
forward train acc:  0.99816  and loss:  2.603504450060427
test acc:  0.9188
forward train acc:  0.99802  and loss:  2.4792853528633714
test acc:  0.9164
forward train acc:  0.99868  and loss:  1.9377858452498913
test acc:  0.9193
forward train acc:  0.99862  and loss:  1.7245150279486552
test acc:  0.9202
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4375  ==>  216 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7291666666666666  ==>  208 / 768
layer  8  :  0.8125  ==>  144 / 768
layer  9  :  0.515625  ==>  372 / 768
layer  10  :  0.5078125  ==>  378 / 768
layer  11  :  0.5494791666666666  ==>  346 / 768
layer  12  :  0.7604166666666666  ==>  184 / 768
layer  13  :  0.4036458333333333  ==>  458 / 768
optimize layer  10
backward train epoch:  26
test acc:  0.1329
forward train acc:  0.99846  and loss:  2.1509767355164513
test acc:  0.9204
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4375  ==>  216 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7291666666666666  ==>  208 / 768
layer  8  :  0.8125  ==>  144 / 768
layer  9  :  0.515625  ==>  372 / 768
layer  10  :  0.515625  ==>  372 / 768
layer  11  :  0.5494791666666666  ==>  346 / 768
layer  12  :  0.7604166666666666  ==>  184 / 768
layer  13  :  0.4036458333333333  ==>  458 / 768
optimize layer  11
backward train epoch:  150
test acc:  0.115
forward train acc:  0.99848  and loss:  2.1489936584839597
test acc:  0.9172
forward train acc:  0.99824  and loss:  2.158080644556321
test acc:  0.9181
forward train acc:  0.99822  and loss:  2.2837689220905304
test acc:  0.9172
forward train acc:  0.99844  and loss:  2.104526531882584
test acc:  0.919
forward train acc:  0.99876  and loss:  1.7601755589712411
test acc:  0.9202
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4375  ==>  216 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7291666666666666  ==>  208 / 768
layer  8  :  0.8125  ==>  144 / 768
layer  9  :  0.515625  ==>  372 / 768
layer  10  :  0.515625  ==>  372 / 768
layer  11  :  0.5572916666666666  ==>  340 / 768
layer  12  :  0.7604166666666666  ==>  184 / 768
layer  13  :  0.4036458333333333  ==>  458 / 768
optimize layer  12
backward train epoch:  101
test acc:  0.148
forward train acc:  0.99888  and loss:  2.6400439022108912
test acc:  0.9201
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4375  ==>  216 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7291666666666666  ==>  208 / 768
layer  8  :  0.8125  ==>  144 / 768
layer  9  :  0.515625  ==>  372 / 768
layer  10  :  0.515625  ==>  372 / 768
layer  11  :  0.5572916666666666  ==>  340 / 768
layer  12  :  0.7708333333333334  ==>  176 / 768
layer  13  :  0.4036458333333333  ==>  458 / 768
optimize layer  13
backward train epoch:  102
test acc:  0.1324
forward train acc:  0.9988  and loss:  2.7280600094236434
test acc:  0.9202
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4375  ==>  216 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7291666666666666  ==>  208 / 768
layer  8  :  0.8125  ==>  144 / 768
layer  9  :  0.515625  ==>  372 / 768
layer  10  :  0.515625  ==>  372 / 768
layer  11  :  0.5572916666666666  ==>  340 / 768
layer  12  :  0.7708333333333334  ==>  176 / 768
layer  13  :  0.4114583333333333  ==>  452 / 768
***** skip layer  0
[3, 4, 4, 4, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0]
***** skip layer  1
[3, 3, 4, 4, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0]
***** skip layer  2
[3, 3, 3, 4, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0]
***** skip layer  3
[3, 3, 3, 3, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0]
optimize layer  4
backward train epoch:  1
test acc:  0.899
forward train acc:  0.99746  and loss:  3.8843308286741376
test acc:  0.9194
forward train acc:  0.99844  and loss:  2.836171658243984
test acc:  0.9192
forward train acc:  0.99804  and loss:  3.1376542751677334
test acc:  0.9186
forward train acc:  0.99824  and loss:  2.7702421955764294
test acc:  0.9203
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4479166666666667  ==>  212 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7291666666666666  ==>  208 / 768
layer  8  :  0.8125  ==>  144 / 768
layer  9  :  0.515625  ==>  372 / 768
layer  10  :  0.515625  ==>  372 / 768
layer  11  :  0.5572916666666666  ==>  340 / 768
layer  12  :  0.7708333333333334  ==>  176 / 768
layer  13  :  0.4114583333333333  ==>  452 / 768
***** skip layer  5
[3, 3, 3, 3, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0]
optimize layer  6
backward train epoch:  8
test acc:  0.4938
forward train acc:  0.9981  and loss:  3.0464124635327607
test acc:  0.9174
forward train acc:  0.99794  and loss:  2.8419795059598982
test acc:  0.916
forward train acc:  0.99836  and loss:  2.630231694318354
test acc:  0.9177
forward train acc:  0.99828  and loss:  2.478685792069882
test acc:  0.9187
forward train acc:  0.99846  and loss:  2.40740915434435
test acc:  0.9184
forward train acc:  0.99882  and loss:  1.8444771191570908
test acc:  0.9188
forward train acc:  0.99898  and loss:  1.922388504492119
test acc:  0.9182
forward train acc:  0.99868  and loss:  1.9554812796413898
test acc:  0.9186
forward train acc:  0.9989  and loss:  2.0861786495661363
test acc:  0.9192
forward train acc:  0.99902  and loss:  1.7398725722450763
test acc:  0.9184
********** reverse layer  6  *********
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4479166666666667  ==>  212 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7291666666666666  ==>  208 / 768
layer  8  :  0.8125  ==>  144 / 768
layer  9  :  0.515625  ==>  372 / 768
layer  10  :  0.515625  ==>  372 / 768
layer  11  :  0.5572916666666666  ==>  340 / 768
layer  12  :  0.7708333333333334  ==>  176 / 768
layer  13  :  0.4114583333333333  ==>  452 / 768
optimize layer  7
backward train epoch:  76
test acc:  0.1377
forward train acc:  0.99866  and loss:  2.1998377011623234
test acc:  0.918
forward train acc:  0.99814  and loss:  2.5233813187805936
test acc:  0.9171
forward train acc:  0.99822  and loss:  2.7093984081875533
test acc:  0.9181
forward train acc:  0.9984  and loss:  2.2182481677737087
test acc:  0.9197
forward train acc:  0.99898  and loss:  1.6899321074597538
test acc:  0.9187
forward train acc:  0.99864  and loss:  1.9249348408775404
test acc:  0.9198
forward train acc:  0.999  and loss:  1.696826804196462
test acc:  0.9211
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4479166666666667  ==>  212 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7395833333333334  ==>  200 / 768
layer  8  :  0.8125  ==>  144 / 768
layer  9  :  0.515625  ==>  372 / 768
layer  10  :  0.515625  ==>  372 / 768
layer  11  :  0.5572916666666666  ==>  340 / 768
layer  12  :  0.7708333333333334  ==>  176 / 768
layer  13  :  0.4114583333333333  ==>  452 / 768
optimize layer  8
backward train epoch:  29
test acc:  0.1518
forward train acc:  0.9986  and loss:  2.0852838668506593
test acc:  0.9193
forward train acc:  0.99858  and loss:  2.0323535518255085
test acc:  0.9206
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4479166666666667  ==>  212 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7395833333333334  ==>  200 / 768
layer  8  :  0.8229166666666666  ==>  136 / 768
layer  9  :  0.515625  ==>  372 / 768
layer  10  :  0.515625  ==>  372 / 768
layer  11  :  0.5572916666666666  ==>  340 / 768
layer  12  :  0.7708333333333334  ==>  176 / 768
layer  13  :  0.4114583333333333  ==>  452 / 768
optimize layer  9
backward train epoch:  99
test acc:  0.0971
forward train acc:  0.99844  and loss:  2.262836725451052
test acc:  0.9202
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4479166666666667  ==>  212 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7395833333333334  ==>  200 / 768
layer  8  :  0.8229166666666666  ==>  136 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.515625  ==>  372 / 768
layer  11  :  0.5572916666666666  ==>  340 / 768
layer  12  :  0.7708333333333334  ==>  176 / 768
layer  13  :  0.4114583333333333  ==>  452 / 768
optimize layer  10
backward train epoch:  16
test acc:  0.1907
forward train acc:  0.99822  and loss:  2.4751890127081424
test acc:  0.9208
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4479166666666667  ==>  212 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7395833333333334  ==>  200 / 768
layer  8  :  0.8229166666666666  ==>  136 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5234375  ==>  366 / 768
layer  11  :  0.5572916666666666  ==>  340 / 768
layer  12  :  0.7708333333333334  ==>  176 / 768
layer  13  :  0.4114583333333333  ==>  452 / 768
optimize layer  11
backward train epoch:  29
test acc:  0.1274
forward train acc:  0.99868  and loss:  1.8692887932993472
test acc:  0.9202
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4479166666666667  ==>  212 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7395833333333334  ==>  200 / 768
layer  8  :  0.8229166666666666  ==>  136 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5234375  ==>  366 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.7708333333333334  ==>  176 / 768
layer  13  :  0.4114583333333333  ==>  452 / 768
optimize layer  12
backward train epoch:  95
test acc:  0.1434
forward train acc:  0.9987  and loss:  3.1197443697601557
test acc:  0.9194
forward train acc:  0.9985  and loss:  2.9837915915995836
test acc:  0.9204
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4479166666666667  ==>  212 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7395833333333334  ==>  200 / 768
layer  8  :  0.8229166666666666  ==>  136 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5234375  ==>  366 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.78125  ==>  168 / 768
layer  13  :  0.4114583333333333  ==>  452 / 768
optimize layer  13
backward train epoch:  102
test acc:  0.1058
forward train acc:  0.99892  and loss:  2.3815649240277708
test acc:  0.9197
forward train acc:  0.99858  and loss:  2.77426671423018
test acc:  0.9198
forward train acc:  0.99854  and loss:  2.5990093271248043
test acc:  0.9183
forward train acc:  0.99892  and loss:  2.134187211981043
test acc:  0.9208
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4479166666666667  ==>  212 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7395833333333334  ==>  200 / 768
layer  8  :  0.8229166666666666  ==>  136 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5234375  ==>  366 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.78125  ==>  168 / 768
layer  13  :  0.4192708333333333  ==>  446 / 768
***** skip layer  0
[2, 3, 3, 3, 0, 4, 5, 0, 0, 0, 0, 0, 0, 0]
***** skip layer  1
[2, 2, 3, 3, 0, 4, 5, 0, 0, 0, 0, 0, 0, 0]
***** skip layer  2
[2, 2, 2, 3, 0, 4, 5, 0, 0, 0, 0, 0, 0, 0]
***** skip layer  3
[2, 2, 2, 2, 0, 4, 5, 0, 0, 0, 0, 0, 0, 0]
optimize layer  4
backward train epoch:  1
test acc:  0.9009
forward train acc:  0.99822  and loss:  2.851502969977446
test acc:  0.9194
forward train acc:  0.99786  and loss:  3.1320014260709286
test acc:  0.9173
forward train acc:  0.99834  and loss:  2.854837392223999
test acc:  0.9184
forward train acc:  0.99858  and loss:  2.327169473282993
test acc:  0.9186
forward train acc:  0.9988  and loss:  1.9434067748952657
test acc:  0.9201
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4583333333333333  ==>  208 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7395833333333334  ==>  200 / 768
layer  8  :  0.8229166666666666  ==>  136 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5234375  ==>  366 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.78125  ==>  168 / 768
layer  13  :  0.4192708333333333  ==>  446 / 768
***** skip layer  5
[2, 2, 2, 2, 0, 3, 5, 0, 0, 0, 0, 0, 0, 0]
***** skip layer  6
[2, 2, 2, 2, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0]
optimize layer  7
backward train epoch:  22
test acc:  0.1368
forward train acc:  0.99846  and loss:  2.470588637283072
test acc:  0.9181
forward train acc:  0.99822  and loss:  2.7056882409378886
test acc:  0.9173
forward train acc:  0.9982  and loss:  2.4611336053349078
test acc:  0.9165
forward train acc:  0.99866  and loss:  2.0856180801056325
test acc:  0.9172
forward train acc:  0.99856  and loss:  1.9906172337941825
test acc:  0.9182
forward train acc:  0.99896  and loss:  1.7347607684787363
test acc:  0.9195
forward train acc:  0.99894  and loss:  1.8705301533918828
test acc:  0.9201
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4583333333333333  ==>  208 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.75  ==>  192 / 768
layer  8  :  0.8229166666666666  ==>  136 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5234375  ==>  366 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.78125  ==>  168 / 768
layer  13  :  0.4192708333333333  ==>  446 / 768
optimize layer  8
backward train epoch:  45
test acc:  0.0914
forward train acc:  0.99842  and loss:  2.309537445893511
test acc:  0.9193
forward train acc:  0.9986  and loss:  1.9095661928877234
test acc:  0.9191
forward train acc:  0.99836  and loss:  2.187218395061791
test acc:  0.9177
forward train acc:  0.99846  and loss:  1.940074794460088
test acc:  0.92
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4583333333333333  ==>  208 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.75  ==>  192 / 768
layer  8  :  0.8333333333333334  ==>  128 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5234375  ==>  366 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.78125  ==>  168 / 768
layer  13  :  0.4192708333333333  ==>  446 / 768
optimize layer  9
backward train epoch:  111
test acc:  0.0964
forward train acc:  0.99862  and loss:  2.1617329117143527
test acc:  0.9175
forward train acc:  0.99846  and loss:  2.2408684282563627
test acc:  0.9179
forward train acc:  0.99846  and loss:  2.101664182031527
test acc:  0.9179
forward train acc:  0.99854  and loss:  1.939621645025909
test acc:  0.9183
forward train acc:  0.99886  and loss:  1.5758910102886148
test acc:  0.9181
forward train acc:  0.99896  and loss:  1.5041542656254023
test acc:  0.9193
forward train acc:  0.9989  and loss:  1.5648395064054057
test acc:  0.9194
forward train acc:  0.9992  and loss:  1.2315885846037418
test acc:  0.9199
forward train acc:  0.99912  and loss:  1.268494088377338
test acc:  0.9198
forward train acc:  0.99922  and loss:  1.2429082178277895
test acc:  0.9178
********** reverse layer  9  *********
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4583333333333333  ==>  208 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.75  ==>  192 / 768
layer  8  :  0.8333333333333334  ==>  128 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5234375  ==>  366 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.78125  ==>  168 / 768
layer  13  :  0.4192708333333333  ==>  446 / 768
optimize layer  10
backward train epoch:  22
test acc:  0.142
forward train acc:  0.99842  and loss:  1.9446620848029852
test acc:  0.9196
forward train acc:  0.99884  and loss:  1.8225916964001954
test acc:  0.9203
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4583333333333333  ==>  208 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.75  ==>  192 / 768
layer  8  :  0.8333333333333334  ==>  128 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.53125  ==>  360 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.78125  ==>  168 / 768
layer  13  :  0.4192708333333333  ==>  446 / 768
optimize layer  11
backward train epoch:  27
test acc:  0.122
forward train acc:  0.99866  and loss:  1.91283154534176
test acc:  0.9191
forward train acc:  0.99866  and loss:  1.8143505519255996
test acc:  0.9179
forward train acc:  0.9986  and loss:  1.9830363648943603
test acc:  0.9164
forward train acc:  0.99866  and loss:  1.7848016573116183
test acc:  0.919
forward train acc:  0.999  and loss:  1.4288335866294801
test acc:  0.9182
forward train acc:  0.99912  and loss:  1.2924547360744327
test acc:  0.9176
forward train acc:  0.99872  and loss:  1.6294432394206524
test acc:  0.9175
forward train acc:  0.99926  and loss:  1.1336318051326089
test acc:  0.918
forward train acc:  0.99908  and loss:  1.2108411989174783
test acc:  0.919
forward train acc:  0.99946  and loss:  1.0644484884105623
test acc:  0.9179
********** reverse layer  11  *********
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4583333333333333  ==>  208 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.75  ==>  192 / 768
layer  8  :  0.8333333333333334  ==>  128 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.53125  ==>  360 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.78125  ==>  168 / 768
layer  13  :  0.4192708333333333  ==>  446 / 768
optimize layer  12
backward train epoch:  165
test acc:  0.0986
forward train acc:  0.99902  and loss:  4.106589279137552
test acc:  0.9184
forward train acc:  0.99874  and loss:  3.6359013840556145
test acc:  0.9195
forward train acc:  0.9988  and loss:  3.273834368214011
test acc:  0.9178
forward train acc:  0.99916  and loss:  2.643517064396292
test acc:  0.9178
forward train acc:  0.99904  and loss:  2.604287601541728
test acc:  0.9182
forward train acc:  0.9992  and loss:  2.365280523430556
test acc:  0.919
forward train acc:  0.99932  and loss:  2.1356449897866696
test acc:  0.9197
forward train acc:  0.99914  and loss:  2.27098812838085
test acc:  0.9201
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4583333333333333  ==>  208 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.75  ==>  192 / 768
layer  8  :  0.8333333333333334  ==>  128 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.53125  ==>  360 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.7916666666666666  ==>  160 / 768
layer  13  :  0.4192708333333333  ==>  446 / 768
optimize layer  13
backward train epoch:  102
test acc:  0.1016
forward train acc:  0.99882  and loss:  2.426344428677112
test acc:  0.9189
forward train acc:  0.99908  and loss:  2.227882529841736
test acc:  0.9205
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4583333333333333  ==>  208 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.75  ==>  192 / 768
layer  8  :  0.8333333333333334  ==>  128 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.53125  ==>  360 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.7916666666666666  ==>  160 / 768
layer  13  :  0.4270833333333333  ==>  440 / 768
***** skip layer  0
[1, 2, 2, 2, 0, 3, 4, 0, 0, 5, 0, 5, 0, 0]
***** skip layer  1
[1, 1, 2, 2, 0, 3, 4, 0, 0, 5, 0, 5, 0, 0]
***** skip layer  2
[1, 1, 1, 2, 0, 3, 4, 0, 0, 5, 0, 5, 0, 0]
***** skip layer  3
[1, 1, 1, 1, 0, 3, 4, 0, 0, 5, 0, 5, 0, 0]
optimize layer  4
backward train epoch:  1
test acc:  0.899
forward train acc:  0.99858  and loss:  2.6394488306250423
test acc:  0.9182
forward train acc:  0.99832  and loss:  2.8279687222093344
test acc:  0.9178
forward train acc:  0.9986  and loss:  2.54255147324875
test acc:  0.9173
forward train acc:  0.99882  and loss:  2.1485353752505034
test acc:  0.9195
forward train acc:  0.99916  and loss:  1.7161728530190885
test acc:  0.9195
forward train acc:  0.99886  and loss:  2.1251852817367762
test acc:  0.9192
forward train acc:  0.9991  and loss:  1.7864872750360519
test acc:  0.9211
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.46875  ==>  204 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.75  ==>  192 / 768
layer  8  :  0.8333333333333334  ==>  128 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.53125  ==>  360 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.7916666666666666  ==>  160 / 768
layer  13  :  0.4270833333333333  ==>  440 / 768
***** skip layer  5
[1, 1, 1, 1, 0, 2, 4, 0, 0, 5, 0, 5, 0, 0]
***** skip layer  6
[1, 1, 1, 1, 0, 2, 3, 0, 0, 5, 0, 5, 0, 0]
optimize layer  7
backward train epoch:  26
test acc:  0.099
forward train acc:  0.99902  and loss:  1.9165282002650201
test acc:  0.9194
forward train acc:  0.99844  and loss:  2.4614739906974137
test acc:  0.9191
forward train acc:  0.99832  and loss:  2.544589326251298
test acc:  0.9179
forward train acc:  0.99896  and loss:  1.8590198101010174
test acc:  0.9163
forward train acc:  0.99914  and loss:  1.7916758246719837
test acc:  0.917
forward train acc:  0.99898  and loss:  1.4622396221384406
test acc:  0.9187
forward train acc:  0.99912  and loss:  1.3742594718933105
test acc:  0.9191
forward train acc:  0.99928  and loss:  1.3116330032935366
test acc:  0.9196
forward train acc:  0.99934  and loss:  1.1923336742911488
test acc:  0.9207
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.46875  ==>  204 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7604166666666666  ==>  184 / 768
layer  8  :  0.8333333333333334  ==>  128 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.53125  ==>  360 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.7916666666666666  ==>  160 / 768
layer  13  :  0.4270833333333333  ==>  440 / 768
optimize layer  8
backward train epoch:  28
test acc:  0.0941
forward train acc:  0.9988  and loss:  1.7950378120876849
test acc:  0.9185
forward train acc:  0.99904  and loss:  1.671040918212384
test acc:  0.9193
forward train acc:  0.99904  and loss:  1.7577737104147673
test acc:  0.919
forward train acc:  0.99912  and loss:  1.536520232912153
test acc:  0.9184
forward train acc:  0.99916  and loss:  1.4554292745888233
test acc:  0.92
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.46875  ==>  204 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7604166666666666  ==>  184 / 768
layer  8  :  0.84375  ==>  120 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.53125  ==>  360 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.7916666666666666  ==>  160 / 768
layer  13  :  0.4270833333333333  ==>  440 / 768
***** skip layer  9
[1, 1, 1, 1, 0, 2, 3, 0, 0, 4, 0, 5, 0, 0]
optimize layer  10
backward train epoch:  104
test acc:  0.0832
forward train acc:  0.99886  and loss:  1.6906802342273295
test acc:  0.9176
forward train acc:  0.99876  and loss:  1.7359671647427604
test acc:  0.9191
forward train acc:  0.99824  and loss:  2.283683216839563
test acc:  0.9173
forward train acc:  0.99908  and loss:  1.4579760584747419
test acc:  0.9181
forward train acc:  0.99904  and loss:  1.3952525014756247
test acc:  0.9194
forward train acc:  0.99914  and loss:  1.3037964696995914
test acc:  0.9202
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.46875  ==>  204 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7604166666666666  ==>  184 / 768
layer  8  :  0.84375  ==>  120 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5390625  ==>  354 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.7916666666666666  ==>  160 / 768
layer  13  :  0.4270833333333333  ==>  440 / 768
***** skip layer  11
[1, 1, 1, 1, 0, 2, 3, 0, 0, 4, 0, 4, 0, 0]
optimize layer  12
backward train epoch:  171
test acc:  0.0754
forward train acc:  0.99886  and loss:  3.8463471299037337
test acc:  0.9186
forward train acc:  0.99906  and loss:  3.238677754532546
test acc:  0.9176
forward train acc:  0.99886  and loss:  2.9117306978441775
test acc:  0.9178
forward train acc:  0.9989  and loss:  2.6728816726244986
test acc:  0.9197
forward train acc:  0.9994  and loss:  2.155949544161558
test acc:  0.9197
forward train acc:  0.99938  and loss:  1.9819508621003479
test acc:  0.92
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.46875  ==>  204 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7604166666666666  ==>  184 / 768
layer  8  :  0.84375  ==>  120 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5390625  ==>  354 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8020833333333334  ==>  152 / 768
layer  13  :  0.4270833333333333  ==>  440 / 768
optimize layer  13
backward train epoch:  102
test acc:  0.0755
forward train acc:  0.9989  and loss:  2.4232208924368024
test acc:  0.9193
forward train acc:  0.99912  and loss:  2.162631786428392
test acc:  0.9184
forward train acc:  0.99906  and loss:  2.1846453826874495
test acc:  0.9198
forward train acc:  0.99898  and loss:  2.1385265830904245
test acc:  0.9182
forward train acc:  0.99918  and loss:  1.936195710208267
test acc:  0.9195
forward train acc:  0.99934  and loss:  1.6535247708670795
test acc:  0.9195
forward train acc:  0.99928  and loss:  1.721744910813868
test acc:  0.9185
forward train acc:  0.99936  and loss:  1.5406634625978768
test acc:  0.9191
forward train acc:  0.99948  and loss:  1.3163690603105351
test acc:  0.92
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.46875  ==>  204 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7604166666666666  ==>  184 / 768
layer  8  :  0.84375  ==>  120 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5390625  ==>  354 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8020833333333334  ==>  152 / 768
layer  13  :  0.4348958333333333  ==>  434 / 768
***** skip layer  0
[0, 1, 1, 1, 0, 2, 3, 0, 0, 4, 0, 4, 0, 0]
***** skip layer  1
[0, 0, 1, 1, 0, 2, 3, 0, 0, 4, 0, 4, 0, 0]
***** skip layer  2
[0, 0, 0, 1, 0, 2, 3, 0, 0, 4, 0, 4, 0, 0]
***** skip layer  3
[0, 0, 0, 0, 0, 2, 3, 0, 0, 4, 0, 4, 0, 0]
optimize layer  4
backward train epoch:  1
test acc:  0.895
forward train acc:  0.99884  and loss:  2.09985979530029
test acc:  0.9184
forward train acc:  0.99838  and loss:  2.553864336106926
test acc:  0.9185
forward train acc:  0.99894  and loss:  1.9769279388710856
test acc:  0.9205
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7604166666666666  ==>  184 / 768
layer  8  :  0.84375  ==>  120 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5390625  ==>  354 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8020833333333334  ==>  152 / 768
layer  13  :  0.4348958333333333  ==>  434 / 768
***** skip layer  5
[0, 0, 0, 0, 0, 1, 3, 0, 0, 4, 0, 4, 0, 0]
***** skip layer  6
[0, 0, 0, 0, 0, 1, 2, 0, 0, 4, 0, 4, 0, 0]
optimize layer  7
backward train epoch:  11
test acc:  0.2949
forward train acc:  0.99866  and loss:  2.110215315129608
test acc:  0.9195
forward train acc:  0.99864  and loss:  2.2448246707208455
test acc:  0.9198
forward train acc:  0.99866  and loss:  2.0799659041222185
test acc:  0.9201
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7708333333333334  ==>  176 / 768
layer  8  :  0.84375  ==>  120 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5390625  ==>  354 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8020833333333334  ==>  152 / 768
layer  13  :  0.4348958333333333  ==>  434 / 768
optimize layer  8
backward train epoch:  31
test acc:  0.0942
forward train acc:  0.99862  and loss:  2.0086089480901137
test acc:  0.9185
forward train acc:  0.99828  and loss:  2.5056724939495325
test acc:  0.919
forward train acc:  0.99856  and loss:  1.965515874326229
test acc:  0.9173
forward train acc:  0.99918  and loss:  1.3929869018029422
test acc:  0.9203
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7708333333333334  ==>  176 / 768
layer  8  :  0.8541666666666666  ==>  112 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5390625  ==>  354 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8020833333333334  ==>  152 / 768
layer  13  :  0.4348958333333333  ==>  434 / 768
***** skip layer  9
[0, 0, 0, 0, 0, 1, 2, 0, 0, 3, 0, 4, 0, 0]
optimize layer  10
backward train epoch:  11
test acc:  0.3032
forward train acc:  0.99874  and loss:  1.7527398772072047
test acc:  0.9175
forward train acc:  0.99864  and loss:  2.1963327295379713
test acc:  0.9197
forward train acc:  0.99864  and loss:  1.7753343009389937
test acc:  0.9207
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7708333333333334  ==>  176 / 768
layer  8  :  0.8541666666666666  ==>  112 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.546875  ==>  348 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8020833333333334  ==>  152 / 768
layer  13  :  0.4348958333333333  ==>  434 / 768
***** skip layer  11
[0, 0, 0, 0, 0, 1, 2, 0, 0, 3, 0, 3, 0, 0]
optimize layer  12
backward train epoch:  133
test acc:  0.0993
forward train acc:  0.99882  and loss:  4.6872887085191905
test acc:  0.9206
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7708333333333334  ==>  176 / 768
layer  8  :  0.8541666666666666  ==>  112 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.546875  ==>  348 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8125  ==>  144 / 768
layer  13  :  0.4348958333333333  ==>  434 / 768
optimize layer  13
backward train epoch:  102
test acc:  0.1139
forward train acc:  0.99894  and loss:  3.5004489584825933
test acc:  0.9171
forward train acc:  0.99896  and loss:  3.018586634658277
test acc:  0.9207
layer  0  :  0.2916666666666667  ==>  68 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7708333333333334  ==>  176 / 768
layer  8  :  0.8541666666666666  ==>  112 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.546875  ==>  348 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8125  ==>  144 / 768
layer  13  :  0.4427083333333333  ==>  428 / 768
optimize layer  0
backward train epoch:  32
test acc:  0.1816
forward train acc:  0.99886  and loss:  2.831869916059077
test acc:  0.9182
forward train acc:  0.9989  and loss:  2.654961275868118
test acc:  0.9195
forward train acc:  0.99896  and loss:  2.392449979670346
test acc:  0.9181
forward train acc:  0.99914  and loss:  2.050639442866668
test acc:  0.919
forward train acc:  0.99912  and loss:  1.918829708127305
test acc:  0.9191
forward train acc:  0.99912  and loss:  1.938944150460884
test acc:  0.919
forward train acc:  0.99928  and loss:  1.7773385369218886
test acc:  0.919
forward train acc:  0.99946  and loss:  1.598137155873701
test acc:  0.9203
layer  0  :  0.3020833333333333  ==>  67 / 96
layer  1  :  0.4166666666666667  ==>  56 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7708333333333334  ==>  176 / 768
layer  8  :  0.8541666666666666  ==>  112 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.546875  ==>  348 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8125  ==>  144 / 768
layer  13  :  0.4427083333333333  ==>  428 / 768
optimize layer  1
backward train epoch:  16
test acc:  0.1831
forward train acc:  0.99924  and loss:  1.748420825228095
test acc:  0.9199
forward train acc:  0.99892  and loss:  2.1323203430511057
test acc:  0.921
layer  0  :  0.3020833333333333  ==>  67 / 96
layer  1  :  0.4270833333333333  ==>  55 / 96
layer  2  :  0.3958333333333333  ==>  116 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7708333333333334  ==>  176 / 768
layer  8  :  0.8541666666666666  ==>  112 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.546875  ==>  348 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8125  ==>  144 / 768
layer  13  :  0.4427083333333333  ==>  428 / 768
optimize layer  2
backward train epoch:  30
test acc:  0.105
forward train acc:  0.99864  and loss:  2.4632182801142335
test acc:  0.917
forward train acc:  0.99868  and loss:  2.198019059607759
test acc:  0.9153
forward train acc:  0.99858  and loss:  2.3232490743976086
test acc:  0.9172
forward train acc:  0.99914  and loss:  1.792449064552784
test acc:  0.9181
forward train acc:  0.9992  and loss:  1.3415924890432507
test acc:  0.9174
forward train acc:  0.99926  and loss:  1.5140458518872038
test acc:  0.9172
forward train acc:  0.99932  and loss:  1.5507933391490951
test acc:  0.9195
forward train acc:  0.99924  and loss:  1.466021949891001
test acc:  0.92
layer  0  :  0.3020833333333333  ==>  67 / 96
layer  1  :  0.4270833333333333  ==>  55 / 96
layer  2  :  0.40625  ==>  114 / 192
layer  3  :  0.3958333333333333  ==>  116 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7708333333333334  ==>  176 / 768
layer  8  :  0.8541666666666666  ==>  112 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.546875  ==>  348 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8125  ==>  144 / 768
layer  13  :  0.4427083333333333  ==>  428 / 768
optimize layer  3
backward train epoch:  1
test acc:  0.8976
forward train acc:  0.99882  and loss:  2.0964206038042903
test acc:  0.9187
forward train acc:  0.99872  and loss:  2.0426285415887833
test acc:  0.9166
forward train acc:  0.99862  and loss:  2.190242082811892
test acc:  0.918
forward train acc:  0.99894  and loss:  1.789420222863555
test acc:  0.9175
forward train acc:  0.99876  and loss:  1.8171357164392248
test acc:  0.9175
forward train acc:  0.9991  and loss:  1.4330072354059666
test acc:  0.92
layer  0  :  0.3020833333333333  ==>  67 / 96
layer  1  :  0.4270833333333333  ==>  55 / 96
layer  2  :  0.40625  ==>  114 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7708333333333334  ==>  176 / 768
layer  8  :  0.8541666666666666  ==>  112 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.546875  ==>  348 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8125  ==>  144 / 768
layer  13  :  0.4427083333333333  ==>  428 / 768
optimize layer  4
backward train epoch:  2
test acc:  0.8665
forward train acc:  0.99844  and loss:  2.045583354891278
test acc:  0.9176
forward train acc:  0.9984  and loss:  2.3884104639291763
test acc:  0.9176
forward train acc:  0.99868  and loss:  2.0315148218069226
test acc:  0.9162
forward train acc:  0.99856  and loss:  2.041232415707782
test acc:  0.9185
forward train acc:  0.99874  and loss:  1.6474013407714665
test acc:  0.9188
forward train acc:  0.99924  and loss:  1.2205727256368846
test acc:  0.9188
forward train acc:  0.99926  and loss:  1.2916910462081432
test acc:  0.9186
forward train acc:  0.999  and loss:  1.438901387504302
test acc:  0.9189
forward train acc:  0.99938  and loss:  1.1577261054189876
test acc:  0.9188
forward train acc:  0.9994  and loss:  1.1769918513018638
test acc:  0.9183
********** reverse layer  4  *********
layer  0  :  0.3020833333333333  ==>  67 / 96
layer  1  :  0.4270833333333333  ==>  55 / 96
layer  2  :  0.40625  ==>  114 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.7708333333333334  ==>  176 / 768
layer  8  :  0.8541666666666666  ==>  112 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.546875  ==>  348 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8125  ==>  144 / 768
layer  13  :  0.4427083333333333  ==>  428 / 768
***** skip layer  5
[0, 0, 0, 0, 5, 0, 2, 0, 0, 3, 0, 3, 0, 0]
***** skip layer  6
[0, 0, 0, 0, 5, 0, 1, 0, 0, 3, 0, 3, 0, 0]
optimize layer  7
backward train epoch:  87
test acc:  0.0884
forward train acc:  0.99874  and loss:  1.6830747425556183
test acc:  0.9191
forward train acc:  0.99872  and loss:  1.913752025924623
test acc:  0.9199
forward train acc:  0.99876  and loss:  1.8671167165739462
test acc:  0.9186
forward train acc:  0.9991  and loss:  1.265973126864992
test acc:  0.9181
forward train acc:  0.99912  and loss:  1.2181393092032522
test acc:  0.9191
forward train acc:  0.9995  and loss:  0.9684739540680312
test acc:  0.9177
forward train acc:  0.99922  and loss:  1.2259738808497787
test acc:  0.9199
forward train acc:  0.99952  and loss:  0.790696884971112
test acc:  0.9207
layer  0  :  0.3020833333333333  ==>  67 / 96
layer  1  :  0.4270833333333333  ==>  55 / 96
layer  2  :  0.40625  ==>  114 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.8541666666666666  ==>  112 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.546875  ==>  348 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8125  ==>  144 / 768
layer  13  :  0.4427083333333333  ==>  428 / 768
optimize layer  8
backward train epoch:  22
test acc:  0.136
forward train acc:  0.99924  and loss:  1.1729764874326065
test acc:  0.9184
forward train acc:  0.99886  and loss:  1.5085140797309577
test acc:  0.9194
forward train acc:  0.99866  and loss:  1.8426940866047516
test acc:  0.9204
layer  0  :  0.3020833333333333  ==>  67 / 96
layer  1  :  0.4270833333333333  ==>  55 / 96
layer  2  :  0.40625  ==>  114 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.8645833333333334  ==>  104 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.546875  ==>  348 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8125  ==>  144 / 768
layer  13  :  0.4427083333333333  ==>  428 / 768
***** skip layer  9
[0, 0, 0, 0, 5, 0, 1, 0, 0, 2, 0, 3, 0, 0]
optimize layer  10
backward train epoch:  8
test acc:  0.455
forward train acc:  0.9987  and loss:  1.8208246501162648
test acc:  0.9192
forward train acc:  0.99902  and loss:  1.5509411767707206
test acc:  0.9184
forward train acc:  0.99906  and loss:  1.4377665461506695
test acc:  0.9202
layer  0  :  0.3020833333333333  ==>  67 / 96
layer  1  :  0.4270833333333333  ==>  55 / 96
layer  2  :  0.40625  ==>  114 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.8645833333333334  ==>  104 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8125  ==>  144 / 768
layer  13  :  0.4427083333333333  ==>  428 / 768
***** skip layer  11
[0, 0, 0, 0, 5, 0, 1, 0, 0, 2, 0, 2, 0, 0]
optimize layer  12
backward train epoch:  109
test acc:  0.1073
forward train acc:  0.99876  and loss:  3.8829831047914922
test acc:  0.9204
layer  0  :  0.3020833333333333  ==>  67 / 96
layer  1  :  0.4270833333333333  ==>  55 / 96
layer  2  :  0.40625  ==>  114 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.8645833333333334  ==>  104 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4427083333333333  ==>  428 / 768
optimize layer  13
backward train epoch:  102
test acc:  0.1026
forward train acc:  0.99904  and loss:  2.945355690084398
test acc:  0.9193
forward train acc:  0.99884  and loss:  2.673164486885071
test acc:  0.9195
forward train acc:  0.99872  and loss:  2.783013093750924
test acc:  0.9179
forward train acc:  0.99914  and loss:  2.3152171545661986
test acc:  0.9202
layer  0  :  0.3020833333333333  ==>  67 / 96
layer  1  :  0.4270833333333333  ==>  55 / 96
layer  2  :  0.40625  ==>  114 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.8645833333333334  ==>  104 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  0
backward train epoch:  48
test acc:  0.1124
forward train acc:  0.99908  and loss:  2.187943802215159
test acc:  0.9175
forward train acc:  0.9991  and loss:  2.20138608221896
test acc:  0.9196
forward train acc:  0.99906  and loss:  2.098842343548313
test acc:  0.9191
forward train acc:  0.9992  and loss:  1.8073003545869142
test acc:  0.9202
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4270833333333333  ==>  55 / 96
layer  2  :  0.40625  ==>  114 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.8645833333333334  ==>  104 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  1
backward train epoch:  35
test acc:  0.103
forward train acc:  0.99878  and loss:  2.1317045327741653
test acc:  0.9175
forward train acc:  0.99886  and loss:  2.101603512885049
test acc:  0.9182
forward train acc:  0.99914  and loss:  1.6119490081910044
test acc:  0.9179
forward train acc:  0.999  and loss:  1.7470203533302993
test acc:  0.9208
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4375  ==>  54 / 96
layer  2  :  0.40625  ==>  114 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.8645833333333334  ==>  104 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  2
backward train epoch:  1
test acc:  0.897
forward train acc:  0.9989  and loss:  2.052677273750305
test acc:  0.92
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4375  ==>  54 / 96
layer  2  :  0.4166666666666667  ==>  112 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.8645833333333334  ==>  104 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  3
backward train epoch:  1
test acc:  0.895
forward train acc:  0.99834  and loss:  2.509374990244396
test acc:  0.9186
forward train acc:  0.99846  and loss:  2.346310127992183
test acc:  0.917
forward train acc:  0.99836  and loss:  2.3020373778417706
test acc:  0.9153
forward train acc:  0.999  and loss:  1.7554552755318582
test acc:  0.9164
forward train acc:  0.99896  and loss:  1.6620545911137015
test acc:  0.9175
forward train acc:  0.99892  and loss:  1.6804476402467117
test acc:  0.9167
forward train acc:  0.9989  and loss:  1.6653598127886653
test acc:  0.9161
forward train acc:  0.9991  and loss:  1.393102006695699
test acc:  0.9174
forward train acc:  0.9992  and loss:  1.2801447759848088
test acc:  0.918
forward train acc:  0.99914  and loss:  1.4384336498333141
test acc:  0.9176
********** reverse layer  3  *********
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4375  ==>  54 / 96
layer  2  :  0.4166666666666667  ==>  112 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.40625  ==>  228 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.8645833333333334  ==>  104 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
***** skip layer  4
[0, 0, 0, 5, 4, 0, 1, 0, 0, 2, 0, 2, 0, 0]
optimize layer  5
backward train epoch:  2
test acc:  0.3682
forward train acc:  0.99902  and loss:  1.7156069823540747
test acc:  0.9195
forward train acc:  0.99888  and loss:  1.8376225034007803
test acc:  0.9161
forward train acc:  0.99886  and loss:  1.6226959931664169
test acc:  0.9205
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4375  ==>  54 / 96
layer  2  :  0.4166666666666667  ==>  112 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4166666666666667  ==>  224 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.8645833333333334  ==>  104 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
***** skip layer  6
[0, 0, 0, 5, 4, 0, 0, 0, 0, 2, 0, 2, 0, 0]
optimize layer  7
backward train epoch:  87
test acc:  0.104
forward train acc:  0.99866  and loss:  1.972853647894226
test acc:  0.9192
forward train acc:  0.9991  and loss:  1.5905140757095069
test acc:  0.9188
forward train acc:  0.99878  and loss:  1.750463496078737
test acc:  0.9184
forward train acc:  0.999  and loss:  1.4062721869559027
test acc:  0.9191
forward train acc:  0.99908  and loss:  1.2807104197563604
test acc:  0.9191
forward train acc:  0.99912  and loss:  1.5083743932191283
test acc:  0.9184
forward train acc:  0.99926  and loss:  1.0796214375877753
test acc:  0.9186
forward train acc:  0.99964  and loss:  0.836745381646324
test acc:  0.9195
forward train acc:  0.99942  and loss:  0.9593805385520682
test acc:  0.9198
forward train acc:  0.99948  and loss:  1.0791055542649701
test acc:  0.9197
********** reverse layer  7  *********
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4375  ==>  54 / 96
layer  2  :  0.4166666666666667  ==>  112 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4166666666666667  ==>  224 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.8645833333333334  ==>  104 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  8
backward train epoch:  12
test acc:  0.2497
forward train acc:  0.99888  and loss:  1.4705960088176653
test acc:  0.917
forward train acc:  0.99898  and loss:  1.6532388322521
test acc:  0.9194
forward train acc:  0.999  and loss:  1.4469455413054675
test acc:  0.9183
forward train acc:  0.99932  and loss:  1.081956813693978
test acc:  0.9179
forward train acc:  0.99924  and loss:  1.0743031764868647
test acc:  0.9177
forward train acc:  0.99928  and loss:  1.1052221216959879
test acc:  0.9188
forward train acc:  0.9992  and loss:  1.1849578650435433
test acc:  0.9183
forward train acc:  0.9993  and loss:  0.9544950826093554
test acc:  0.9201
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4375  ==>  54 / 96
layer  2  :  0.4166666666666667  ==>  112 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4166666666666667  ==>  224 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.875  ==>  96 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
***** skip layer  9
[0, 0, 0, 5, 4, 0, 0, 5, 0, 1, 0, 2, 0, 0]
optimize layer  10
backward train epoch:  4
test acc:  0.7722
forward train acc:  0.99932  and loss:  1.2355326088145375
test acc:  0.9182
forward train acc:  0.99892  and loss:  1.3340277938987128
test acc:  0.9184
forward train acc:  0.99902  and loss:  1.2287616116227582
test acc:  0.9179
forward train acc:  0.9992  and loss:  1.3066700565395877
test acc:  0.9169
forward train acc:  0.99932  and loss:  0.9845393232535571
test acc:  0.9185
forward train acc:  0.99928  and loss:  0.95308735629078
test acc:  0.9181
forward train acc:  0.99934  and loss:  1.0360050171148032
test acc:  0.9187
forward train acc:  0.99956  and loss:  0.7760986139765009
test acc:  0.9174
forward train acc:  0.99964  and loss:  0.7823798999888822
test acc:  0.9185
forward train acc:  0.99944  and loss:  0.8209185774321668
test acc:  0.918
********** reverse layer  10  *********
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4375  ==>  54 / 96
layer  2  :  0.4166666666666667  ==>  112 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4166666666666667  ==>  224 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.875  ==>  96 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
***** skip layer  11
[0, 0, 0, 5, 4, 0, 0, 5, 0, 1, 5, 1, 0, 0]
optimize layer  12
backward train epoch:  94
test acc:  0.0948
forward train acc:  0.9994  and loss:  4.774810777045786
test acc:  0.9185
forward train acc:  0.99946  and loss:  3.5788077251054347
test acc:  0.9155
forward train acc:  0.99936  and loss:  3.2324238792061806
test acc:  0.9186
forward train acc:  0.99932  and loss:  2.906436998397112
test acc:  0.9181
forward train acc:  0.99942  and loss:  2.5996634387411177
test acc:  0.9189
forward train acc:  0.99958  and loss:  2.409074964467436
test acc:  0.9186
forward train acc:  0.99938  and loss:  2.3002064034808427
test acc:  0.919
forward train acc:  0.9995  and loss:  2.080371263436973
test acc:  0.9196
forward train acc:  0.99944  and loss:  2.1586440494284034
test acc:  0.9183
forward train acc:  0.99956  and loss:  1.9694872095715255
test acc:  0.9197
********** reverse layer  12  *********
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4375  ==>  54 / 96
layer  2  :  0.4166666666666667  ==>  112 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4166666666666667  ==>  224 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.875  ==>  96 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  13
backward train epoch:  102
test acc:  0.0955
forward train acc:  0.99934  and loss:  0.9963641847716644
test acc:  0.9192
forward train acc:  0.99918  and loss:  1.1908706093672663
test acc:  0.9198
forward train acc:  0.99894  and loss:  1.4677740832266863
test acc:  0.9191
forward train acc:  0.99934  and loss:  1.127634425763972
test acc:  0.9179
forward train acc:  0.9994  and loss:  0.8466322659514844
test acc:  0.9178
forward train acc:  0.99948  and loss:  0.7249186885019299
test acc:  0.9195
forward train acc:  0.99952  and loss:  0.6641967083560303
test acc:  0.918
forward train acc:  0.99966  and loss:  0.5473439171910286
test acc:  0.9191
forward train acc:  0.99956  and loss:  0.53472916217288
test acc:  0.9182
forward train acc:  0.99952  and loss:  0.6257072050066199
test acc:  0.9198
********** reverse layer  13  *********
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4375  ==>  54 / 96
layer  2  :  0.4166666666666667  ==>  112 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4166666666666667  ==>  224 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.875  ==>  96 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  0
backward train epoch:  268
test acc:  0.0913
forward train acc:  0.8951  and loss:  162.42451180517673
test acc:  0.8657
forward train acc:  0.93498  and loss:  82.47188022732735
test acc:  0.8824
forward train acc:  0.9553  and loss:  55.01599299162626
test acc:  0.8901
forward train acc:  0.9641  and loss:  43.11541742272675
test acc:  0.8933
forward train acc:  0.97044  and loss:  35.8823550902307
test acc:  0.8958
forward train acc:  0.97422  and loss:  31.656608682125807
test acc:  0.899
forward train acc:  0.97678  and loss:  27.51099256798625
test acc:  0.901
forward train acc:  0.97828  and loss:  26.21180508751422
test acc:  0.9012
forward train acc:  0.9809  and loss:  22.980159359052777
test acc:  0.9031
forward train acc:  0.98116  and loss:  21.533446479588747
test acc:  0.9036
********** reverse layer  0  *********
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4375  ==>  54 / 96
layer  2  :  0.4166666666666667  ==>  112 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4166666666666667  ==>  224 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.875  ==>  96 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  1
backward train epoch:  44
test acc:  0.1016
forward train acc:  0.9963  and loss:  4.6305913728429005
test acc:  0.9185
forward train acc:  0.9978  and loss:  2.7932005897164345
test acc:  0.9189
forward train acc:  0.99842  and loss:  1.9884065499063581
test acc:  0.9191
forward train acc:  0.99888  and loss:  1.5050338366418146
test acc:  0.9203
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4479166666666667  ==>  53 / 96
layer  2  :  0.4166666666666667  ==>  112 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4166666666666667  ==>  224 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.875  ==>  96 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  2
backward train epoch:  29
test acc:  0.1482
forward train acc:  0.99854  and loss:  1.9962199839064851
test acc:  0.9183
forward train acc:  0.9986  and loss:  1.6815339701715857
test acc:  0.9192
forward train acc:  0.99896  and loss:  1.5209488945547491
test acc:  0.9191
forward train acc:  0.99896  and loss:  1.4235291929508094
test acc:  0.9194
forward train acc:  0.99898  and loss:  1.4066325461608358
test acc:  0.9193
forward train acc:  0.9993  and loss:  1.047038366086781
test acc:  0.9192
forward train acc:  0.99938  and loss:  0.9698379110777751
test acc:  0.9185
forward train acc:  0.99942  and loss:  1.0384944715769961
test acc:  0.9193
forward train acc:  0.9995  and loss:  0.7325553957780357
test acc:  0.92
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4479166666666667  ==>  53 / 96
layer  2  :  0.4270833333333333  ==>  110 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4166666666666667  ==>  224 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.875  ==>  96 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
***** skip layer  3
[5, 0, 0, 4, 4, 0, 0, 5, 0, 1, 5, 1, 5, 5]
***** skip layer  4
[5, 0, 0, 4, 3, 0, 0, 5, 0, 1, 5, 1, 5, 5]
optimize layer  5
backward train epoch:  5
test acc:  0.7245
forward train acc:  0.99896  and loss:  1.3008504462486599
test acc:  0.9177
forward train acc:  0.99892  and loss:  1.4646967504231725
test acc:  0.9177
forward train acc:  0.99906  and loss:  1.3559987811604515
test acc:  0.9187
forward train acc:  0.99924  and loss:  0.9913136404938996
test acc:  0.9206
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4479166666666667  ==>  53 / 96
layer  2  :  0.4270833333333333  ==>  110 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4270833333333333  ==>  220 / 384
layer  6  :  0.4375  ==>  216 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.875  ==>  96 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  6
backward train epoch:  14
test acc:  0.247
forward train acc:  0.99894  and loss:  1.3764466737338807
test acc:  0.9184
forward train acc:  0.99888  and loss:  1.3477160958573222
test acc:  0.9171
forward train acc:  0.9983  and loss:  1.934071266790852
test acc:  0.9165
forward train acc:  0.9988  and loss:  1.4140076906769536
test acc:  0.9198
forward train acc:  0.9995  and loss:  0.7678049794631079
test acc:  0.9187
forward train acc:  0.99918  and loss:  0.8401697184308432
test acc:  0.9195
forward train acc:  0.99928  and loss:  1.0610564955277368
test acc:  0.9196
forward train acc:  0.99942  and loss:  0.823198692407459
test acc:  0.9193
forward train acc:  0.99946  and loss:  0.6704216483631171
test acc:  0.922
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4479166666666667  ==>  53 / 96
layer  2  :  0.4270833333333333  ==>  110 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4270833333333333  ==>  220 / 384
layer  6  :  0.4479166666666667  ==>  212 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.875  ==>  96 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
***** skip layer  7
[5, 0, 0, 4, 3, 0, 0, 4, 0, 1, 5, 1, 5, 5]
optimize layer  8
backward train epoch:  9
test acc:  0.4316
forward train acc:  0.9992  and loss:  1.1338391653553117
test acc:  0.9186
forward train acc:  0.99896  and loss:  1.2699587059323676
test acc:  0.9197
forward train acc:  0.99882  and loss:  1.4058844626997598
test acc:  0.9185
forward train acc:  0.99904  and loss:  1.2826747738872655
test acc:  0.9199
forward train acc:  0.99918  and loss:  1.0530879334546626
test acc:  0.9211
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4479166666666667  ==>  53 / 96
layer  2  :  0.4270833333333333  ==>  110 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4270833333333333  ==>  220 / 384
layer  6  :  0.4479166666666667  ==>  212 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.8854166666666666  ==>  88 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
***** skip layer  9
[5, 0, 0, 4, 3, 0, 0, 4, 0, 0, 5, 1, 5, 5]
***** skip layer  10
[5, 0, 0, 4, 3, 0, 0, 4, 0, 0, 4, 1, 5, 5]
***** skip layer  11
[5, 0, 0, 4, 3, 0, 0, 4, 0, 0, 4, 0, 5, 5]
***** skip layer  12
[5, 0, 0, 4, 3, 0, 0, 4, 0, 0, 4, 0, 4, 5]
***** skip layer  13
[5, 0, 0, 4, 3, 0, 0, 4, 0, 0, 4, 0, 4, 4]
***** skip layer  0
[4, 0, 0, 4, 3, 0, 0, 4, 0, 0, 4, 0, 4, 4]
optimize layer  1
backward train epoch:  57
test acc:  0.0864
forward train acc:  0.99896  and loss:  1.31169382261578
test acc:  0.9193
forward train acc:  0.99906  and loss:  1.2307826424948871
test acc:  0.9169
forward train acc:  0.9992  and loss:  1.109706280491082
test acc:  0.9166
forward train acc:  0.99916  and loss:  1.137708708527498
test acc:  0.9194
forward train acc:  0.99902  and loss:  1.281896436412353
test acc:  0.9198
forward train acc:  0.99932  and loss:  0.9068335984193254
test acc:  0.9183
forward train acc:  0.9992  and loss:  1.0271839414490387
test acc:  0.9187
forward train acc:  0.99932  and loss:  0.843583274865523
test acc:  0.9208
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4583333333333333  ==>  52 / 96
layer  2  :  0.4270833333333333  ==>  110 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4270833333333333  ==>  220 / 384
layer  6  :  0.4479166666666667  ==>  212 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.8854166666666666  ==>  88 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  2
backward train epoch:  44
test acc:  0.1353
forward train acc:  0.99916  and loss:  1.2053519756591413
test acc:  0.9193
forward train acc:  0.99904  and loss:  1.2821146038768347
test acc:  0.92
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4583333333333333  ==>  52 / 96
layer  2  :  0.4375  ==>  108 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4270833333333333  ==>  220 / 384
layer  6  :  0.4479166666666667  ==>  212 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.8854166666666666  ==>  88 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
***** skip layer  3
[4, 0, 0, 3, 3, 0, 0, 4, 0, 0, 4, 0, 4, 4]
***** skip layer  4
[4, 0, 0, 3, 2, 0, 0, 4, 0, 0, 4, 0, 4, 4]
optimize layer  5
backward train epoch:  91
test acc:  0.1098
forward train acc:  0.99872  and loss:  1.4669712765607983
test acc:  0.9172
forward train acc:  0.99874  and loss:  1.6729409163817763
test acc:  0.9171
forward train acc:  0.9987  and loss:  1.5625254758633673
test acc:  0.9199
forward train acc:  0.99898  and loss:  1.334429707028903
test acc:  0.92
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4583333333333333  ==>  52 / 96
layer  2  :  0.4375  ==>  108 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4375  ==>  216 / 384
layer  6  :  0.4479166666666667  ==>  212 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.8854166666666666  ==>  88 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  6
backward train epoch:  94
test acc:  0.087
forward train acc:  0.99866  and loss:  1.628988781652879
test acc:  0.9204
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4583333333333333  ==>  52 / 96
layer  2  :  0.4375  ==>  108 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4375  ==>  216 / 384
layer  6  :  0.4583333333333333  ==>  208 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.8854166666666666  ==>  88 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
***** skip layer  7
[4, 0, 0, 3, 2, 0, 0, 3, 0, 0, 4, 0, 4, 4]
optimize layer  8
backward train epoch:  54
test acc:  0.097
forward train acc:  0.99892  and loss:  1.3408069689758122
test acc:  0.92
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4583333333333333  ==>  52 / 96
layer  2  :  0.4375  ==>  108 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4375  ==>  216 / 384
layer  6  :  0.4583333333333333  ==>  208 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.8958333333333334  ==>  80 / 768
layer  9  :  0.5234375  ==>  366 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  9
backward train epoch:  98
test acc:  0.1331
forward train acc:  0.99872  and loss:  1.5426599553320557
test acc:  0.9188
forward train acc:  0.99872  and loss:  1.5491518479539081
test acc:  0.9214
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4583333333333333  ==>  52 / 96
layer  2  :  0.4375  ==>  108 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4375  ==>  216 / 384
layer  6  :  0.4583333333333333  ==>  208 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.8958333333333334  ==>  80 / 768
layer  9  :  0.53125  ==>  360 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5651041666666666  ==>  334 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
***** skip layer  10
[4, 0, 0, 3, 2, 0, 0, 3, 0, 0, 3, 0, 4, 4]
optimize layer  11
backward train epoch:  21
test acc:  0.1168
forward train acc:  0.99878  and loss:  1.5748864196939394
test acc:  0.9203
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4583333333333333  ==>  52 / 96
layer  2  :  0.4375  ==>  108 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4375  ==>  216 / 384
layer  6  :  0.4583333333333333  ==>  208 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.8958333333333334  ==>  80 / 768
layer  9  :  0.53125  ==>  360 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5729166666666666  ==>  328 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
***** skip layer  12
[4, 0, 0, 3, 2, 0, 0, 3, 0, 0, 3, 0, 3, 4]
***** skip layer  13
[4, 0, 0, 3, 2, 0, 0, 3, 0, 0, 3, 0, 3, 3]
***** skip layer  0
[3, 0, 0, 3, 2, 0, 0, 3, 0, 0, 3, 0, 3, 3]
optimize layer  1
backward train epoch:  34
test acc:  0.112
forward train acc:  0.99856  and loss:  1.6632178067229688
test acc:  0.9172
forward train acc:  0.99846  and loss:  1.9196098165994044
test acc:  0.9178
forward train acc:  0.99898  and loss:  1.501153222285211
test acc:  0.9187
forward train acc:  0.99912  and loss:  1.166431488469243
test acc:  0.9201
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.46875  ==>  51 / 96
layer  2  :  0.4375  ==>  108 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4375  ==>  216 / 384
layer  6  :  0.4583333333333333  ==>  208 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.8958333333333334  ==>  80 / 768
layer  9  :  0.53125  ==>  360 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5729166666666666  ==>  328 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  2
backward train epoch:  41
test acc:  0.1252
forward train acc:  0.99882  and loss:  1.5750511473743245
test acc:  0.9192
forward train acc:  0.99878  and loss:  1.528328630141914
test acc:  0.9184
forward train acc:  0.99838  and loss:  1.9910971579956822
test acc:  0.9194
forward train acc:  0.99882  and loss:  1.6060553258575965
test acc:  0.9211
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.46875  ==>  51 / 96
layer  2  :  0.4479166666666667  ==>  106 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4375  ==>  216 / 384
layer  6  :  0.4583333333333333  ==>  208 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.8958333333333334  ==>  80 / 768
layer  9  :  0.53125  ==>  360 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5729166666666666  ==>  328 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
***** skip layer  3
[3, 0, 0, 2, 2, 0, 0, 3, 0, 0, 3, 0, 3, 3]
***** skip layer  4
[3, 0, 0, 2, 1, 0, 0, 3, 0, 0, 3, 0, 3, 3]
optimize layer  5
backward train epoch:  2
test acc:  0.8715
forward train acc:  0.99864  and loss:  1.7354068880085833
test acc:  0.9212
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.46875  ==>  51 / 96
layer  2  :  0.4479166666666667  ==>  106 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4479166666666667  ==>  212 / 384
layer  6  :  0.4583333333333333  ==>  208 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.8958333333333334  ==>  80 / 768
layer  9  :  0.53125  ==>  360 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5729166666666666  ==>  328 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  6
backward train epoch:  68
test acc:  0.0981
forward train acc:  0.99836  and loss:  1.75587838361389
test acc:  0.9205
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.46875  ==>  51 / 96
layer  2  :  0.4479166666666667  ==>  106 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4479166666666667  ==>  212 / 384
layer  6  :  0.46875  ==>  204 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.8958333333333334  ==>  80 / 768
layer  9  :  0.53125  ==>  360 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5729166666666666  ==>  328 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
***** skip layer  7
[3, 0, 0, 2, 1, 0, 0, 2, 0, 0, 3, 0, 3, 3]
optimize layer  8
backward train epoch:  10
test acc:  0.3427
forward train acc:  0.99838  and loss:  1.8450330563355237
test acc:  0.9189
forward train acc:  0.99872  and loss:  1.7506624433153775
test acc:  0.9193
forward train acc:  0.99834  and loss:  2.0315781426616013
test acc:  0.9217
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.46875  ==>  51 / 96
layer  2  :  0.4479166666666667  ==>  106 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4479166666666667  ==>  212 / 384
layer  6  :  0.46875  ==>  204 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.90625  ==>  72 / 768
layer  9  :  0.53125  ==>  360 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5729166666666666  ==>  328 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  9
backward train epoch:  59
test acc:  0.1058
forward train acc:  0.99864  and loss:  1.8829173247795552
test acc:  0.9218
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.46875  ==>  51 / 96
layer  2  :  0.4479166666666667  ==>  106 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4479166666666667  ==>  212 / 384
layer  6  :  0.46875  ==>  204 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.90625  ==>  72 / 768
layer  9  :  0.5390625  ==>  354 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5729166666666666  ==>  328 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
***** skip layer  10
[3, 0, 0, 2, 1, 0, 0, 2, 0, 0, 2, 0, 3, 3]
optimize layer  11
backward train epoch:  79
test acc:  0.1066
forward train acc:  0.99848  and loss:  1.6905641506891698
test acc:  0.919
forward train acc:  0.99862  and loss:  1.5150304104899988
test acc:  0.9202
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.46875  ==>  51 / 96
layer  2  :  0.4479166666666667  ==>  106 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4479166666666667  ==>  212 / 384
layer  6  :  0.46875  ==>  204 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.90625  ==>  72 / 768
layer  9  :  0.5390625  ==>  354 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5807291666666666  ==>  322 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
***** skip layer  12
[3, 0, 0, 2, 1, 0, 0, 2, 0, 0, 2, 0, 2, 3]
***** skip layer  13
[3, 0, 0, 2, 1, 0, 0, 2, 0, 0, 2, 0, 2, 2]
***** skip layer  0
[2, 0, 0, 2, 1, 0, 0, 2, 0, 0, 2, 0, 2, 2]
optimize layer  1
backward train epoch:  136
test acc:  0.133
forward train acc:  0.98594  and loss:  20.174875303404406
test acc:  0.9135
forward train acc:  0.99278  and loss:  8.761922566685826
test acc:  0.9165
forward train acc:  0.99494  and loss:  5.9059740317752585
test acc:  0.917
forward train acc:  0.99568  and loss:  5.042748351406772
test acc:  0.9192
forward train acc:  0.99658  and loss:  3.821544826583704
test acc:  0.9175
forward train acc:  0.99694  and loss:  3.9202925199642777
test acc:  0.919
forward train acc:  0.99764  and loss:  2.856233983999118
test acc:  0.9188
forward train acc:  0.99766  and loss:  2.8488810160197318
test acc:  0.9202
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4791666666666667  ==>  50 / 96
layer  2  :  0.4479166666666667  ==>  106 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4479166666666667  ==>  212 / 384
layer  6  :  0.46875  ==>  204 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.90625  ==>  72 / 768
layer  9  :  0.5390625  ==>  354 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5807291666666666  ==>  322 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  2
backward train epoch:  18
test acc:  0.1488
forward train acc:  0.99676  and loss:  3.6295337515184656
test acc:  0.9185
forward train acc:  0.99734  and loss:  3.401130333833862
test acc:  0.9178
forward train acc:  0.9975  and loss:  3.3013698155991733
test acc:  0.9178
forward train acc:  0.99786  and loss:  2.735678883444052
test acc:  0.9192
forward train acc:  0.9981  and loss:  2.429182032472454
test acc:  0.9201
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4791666666666667  ==>  50 / 96
layer  2  :  0.4583333333333333  ==>  104 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4479166666666667  ==>  212 / 384
layer  6  :  0.46875  ==>  204 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.90625  ==>  72 / 768
layer  9  :  0.5390625  ==>  354 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5807291666666666  ==>  322 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
***** skip layer  3
[2, 0, 0, 1, 1, 0, 0, 2, 0, 0, 2, 0, 2, 2]
***** skip layer  4
[2, 0, 0, 1, 0, 0, 0, 2, 0, 0, 2, 0, 2, 2]
optimize layer  5
backward train epoch:  10
test acc:  0.3235
forward train acc:  0.99778  and loss:  2.9152510068379343
test acc:  0.9181
forward train acc:  0.99784  and loss:  2.7050573636079207
test acc:  0.9186
forward train acc:  0.99768  and loss:  2.9829582704696804
test acc:  0.9194
forward train acc:  0.9978  and loss:  2.5691416790359654
test acc:  0.9205
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4791666666666667  ==>  50 / 96
layer  2  :  0.4583333333333333  ==>  104 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.46875  ==>  204 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.90625  ==>  72 / 768
layer  9  :  0.5390625  ==>  354 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5807291666666666  ==>  322 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  6
backward train epoch:  16
test acc:  0.1531
forward train acc:  0.99788  and loss:  2.6837131700012833
test acc:  0.9186
forward train acc:  0.99752  and loss:  2.9807631694711745
test acc:  0.9213
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4791666666666667  ==>  50 / 96
layer  2  :  0.4583333333333333  ==>  104 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4791666666666667  ==>  200 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.90625  ==>  72 / 768
layer  9  :  0.5390625  ==>  354 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5807291666666666  ==>  322 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
***** skip layer  7
[2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2, 0, 2, 2]
optimize layer  8
backward train epoch:  38
test acc:  0.1014
forward train acc:  0.99764  and loss:  2.7090669255703688
test acc:  0.9184
forward train acc:  0.99814  and loss:  2.2916761189699173
test acc:  0.9216
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4791666666666667  ==>  50 / 96
layer  2  :  0.4583333333333333  ==>  104 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4791666666666667  ==>  200 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.9166666666666666  ==>  64 / 768
layer  9  :  0.5390625  ==>  354 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5807291666666666  ==>  322 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  9
backward train epoch:  23
test acc:  0.1167
forward train acc:  0.9983  and loss:  2.318890043068677
test acc:  0.9211
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4791666666666667  ==>  50 / 96
layer  2  :  0.4583333333333333  ==>  104 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4791666666666667  ==>  200 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.9166666666666666  ==>  64 / 768
layer  9  :  0.546875  ==>  348 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5807291666666666  ==>  322 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
***** skip layer  10
[2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 2, 2]
optimize layer  11
backward train epoch:  157
test acc:  0.0994
forward train acc:  0.9982  and loss:  2.45606363582192
test acc:  0.9195
forward train acc:  0.99804  and loss:  2.3513108557090163
test acc:  0.9194
forward train acc:  0.99862  and loss:  1.9601933383382857
test acc:  0.9202
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4791666666666667  ==>  50 / 96
layer  2  :  0.4583333333333333  ==>  104 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4791666666666667  ==>  200 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.9166666666666666  ==>  64 / 768
layer  9  :  0.546875  ==>  348 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5885416666666666  ==>  316 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
***** skip layer  12
[2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 2]
***** skip layer  13
[2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1]
***** skip layer  0
[1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1]
optimize layer  1
backward train epoch:  12
test acc:  0.2173
forward train acc:  0.99812  and loss:  2.313140324433334
test acc:  0.9188
forward train acc:  0.99846  and loss:  1.9281549081206322
test acc:  0.9177
forward train acc:  0.99826  and loss:  2.0333927390165627
test acc:  0.9203
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4895833333333333  ==>  49 / 96
layer  2  :  0.4583333333333333  ==>  104 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4791666666666667  ==>  200 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.9166666666666666  ==>  64 / 768
layer  9  :  0.546875  ==>  348 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5885416666666666  ==>  316 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  2
backward train epoch:  29
test acc:  0.1155
forward train acc:  0.99832  and loss:  1.827485345711466
test acc:  0.9203
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4895833333333333  ==>  49 / 96
layer  2  :  0.46875  ==>  102 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4791666666666667  ==>  200 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4791666666666667  ==>  200 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.9166666666666666  ==>  64 / 768
layer  9  :  0.546875  ==>  348 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5885416666666666  ==>  316 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
***** skip layer  3
[1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1]
optimize layer  4
backward train epoch:  56
test acc:  0.1118
forward train acc:  0.99714  and loss:  3.342164448928088
test acc:  0.9197
forward train acc:  0.99814  and loss:  2.0734696133295074
test acc:  0.919
forward train acc:  0.9981  and loss:  2.286094514420256
test acc:  0.9194
forward train acc:  0.9983  and loss:  2.049054372939281
test acc:  0.9186
forward train acc:  0.99854  and loss:  1.696964772127103
test acc:  0.9184
forward train acc:  0.99856  and loss:  1.6206873451301362
test acc:  0.9202
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4895833333333333  ==>  49 / 96
layer  2  :  0.46875  ==>  102 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4791666666666667  ==>  200 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.9166666666666666  ==>  64 / 768
layer  9  :  0.546875  ==>  348 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5885416666666666  ==>  316 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  5
backward train epoch:  11
test acc:  0.2737
forward train acc:  0.9975  and loss:  2.817240764852613
test acc:  0.9174
forward train acc:  0.99808  and loss:  2.366158252261812
test acc:  0.9181
forward train acc:  0.99792  and loss:  2.5235369850415736
test acc:  0.9174
forward train acc:  0.99826  and loss:  2.161976048257202
test acc:  0.9198
forward train acc:  0.99862  and loss:  1.7094418453634717
test acc:  0.9179
forward train acc:  0.99858  and loss:  1.6677932572783902
test acc:  0.918
forward train acc:  0.99876  and loss:  1.5406993394717574
test acc:  0.918
forward train acc:  0.99884  and loss:  1.4438178846612573
test acc:  0.9186
forward train acc:  0.99888  and loss:  1.584251797030447
test acc:  0.9198
forward train acc:  0.99918  and loss:  1.023988826898858
test acc:  0.9193
********** reverse layer  5  *********
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4895833333333333  ==>  49 / 96
layer  2  :  0.46875  ==>  102 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4791666666666667  ==>  200 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.9166666666666666  ==>  64 / 768
layer  9  :  0.546875  ==>  348 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5885416666666666  ==>  316 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  6
backward train epoch:  26
test acc:  0.1192
forward train acc:  0.99854  and loss:  1.9899806372704916
test acc:  0.92
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4895833333333333  ==>  49 / 96
layer  2  :  0.46875  ==>  102 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.9166666666666666  ==>  64 / 768
layer  9  :  0.546875  ==>  348 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5885416666666666  ==>  316 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
***** skip layer  7
[1, 0, 0, 0, 0, 5, 0, 0, 0, 0, 1, 0, 1, 1]
optimize layer  8
backward train epoch:  128
test acc:  0.1019
forward train acc:  0.99842  and loss:  1.949860538559733
test acc:  0.9176
forward train acc:  0.99838  and loss:  1.9432924955617636
test acc:  0.9186
forward train acc:  0.998  and loss:  2.3717090958962217
test acc:  0.9199
forward train acc:  0.99854  and loss:  1.7803737266513053
test acc:  0.919
forward train acc:  0.99906  and loss:  1.2192028403514996
test acc:  0.9216
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4895833333333333  ==>  49 / 96
layer  2  :  0.46875  ==>  102 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.9270833333333334  ==>  56 / 768
layer  9  :  0.546875  ==>  348 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5885416666666666  ==>  316 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  9
backward train epoch:  34
test acc:  0.1166
forward train acc:  0.99882  and loss:  1.4477170924656093
test acc:  0.9184
forward train acc:  0.9984  and loss:  1.7160867650381988
test acc:  0.9195
forward train acc:  0.99836  and loss:  1.734452722244896
test acc:  0.9184
forward train acc:  0.99898  and loss:  1.251899071619846
test acc:  0.9207
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4895833333333333  ==>  49 / 96
layer  2  :  0.46875  ==>  102 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.9270833333333334  ==>  56 / 768
layer  9  :  0.5546875  ==>  342 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5885416666666666  ==>  316 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
***** skip layer  10
[1, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 1, 1]
optimize layer  11
backward train epoch:  58
test acc:  0.102
forward train acc:  0.99872  and loss:  1.5691187384945806
test acc:  0.9174
forward train acc:  0.99848  and loss:  1.8041077245725319
test acc:  0.9197
forward train acc:  0.99878  and loss:  1.5778745732968673
test acc:  0.9186
forward train acc:  0.99894  and loss:  1.320152402156964
test acc:  0.9199
forward train acc:  0.9988  and loss:  1.3497946273419075
test acc:  0.9211
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.4895833333333333  ==>  49 / 96
layer  2  :  0.46875  ==>  102 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.9270833333333334  ==>  56 / 768
layer  9  :  0.5546875  ==>  342 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5963541666666666  ==>  310 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
***** skip layer  12
[1, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 1]
***** skip layer  13
[1, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0]
***** skip layer  0
[0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0]
optimize layer  1
backward train epoch:  13
test acc:  0.249
forward train acc:  0.9989  and loss:  1.4483309954521246
test acc:  0.9188
forward train acc:  0.9987  and loss:  1.60322958760662
test acc:  0.9194
forward train acc:  0.99866  and loss:  1.6733742774958955
test acc:  0.9189
forward train acc:  0.99908  and loss:  1.0871922144433483
test acc:  0.9197
forward train acc:  0.99896  and loss:  1.3410008057253435
test acc:  0.9186
forward train acc:  0.99902  and loss:  1.2731253810343333
test acc:  0.9199
forward train acc:  0.99932  and loss:  0.9477989280130714
test acc:  0.9192
forward train acc:  0.99932  and loss:  0.8713543035846669
test acc:  0.9186
forward train acc:  0.9993  and loss:  0.8725476651452482
test acc:  0.9192
forward train acc:  0.99922  and loss:  0.8969942827825435
test acc:  0.9204
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5  ==>  48 / 96
layer  2  :  0.46875  ==>  102 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.9270833333333334  ==>  56 / 768
layer  9  :  0.5546875  ==>  342 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5963541666666666  ==>  310 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  2
backward train epoch:  33
test acc:  0.1002
forward train acc:  0.99862  and loss:  1.6061120954109356
test acc:  0.9183
forward train acc:  0.99852  and loss:  1.9803937735268846
test acc:  0.9191
forward train acc:  0.9986  and loss:  1.717482018750161
test acc:  0.9173
forward train acc:  0.99886  and loss:  1.3916193009790732
test acc:  0.9188
forward train acc:  0.99862  and loss:  1.5361924545723014
test acc:  0.9208
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5  ==>  48 / 96
layer  2  :  0.4791666666666667  ==>  100 / 192
layer  3  :  0.40625  ==>  114 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.9270833333333334  ==>  56 / 768
layer  9  :  0.5546875  ==>  342 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5963541666666666  ==>  310 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  3
backward train epoch:  1
test acc:  0.8942
forward train acc:  0.99804  and loss:  2.2713755205040798
test acc:  0.9179
forward train acc:  0.99808  and loss:  2.195657887903508
test acc:  0.9182
forward train acc:  0.99832  and loss:  2.0262559514085297
test acc:  0.9193
forward train acc:  0.99876  and loss:  1.6773417118238285
test acc:  0.9193
forward train acc:  0.99872  and loss:  1.6548718472477049
test acc:  0.9217
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5  ==>  48 / 96
layer  2  :  0.4791666666666667  ==>  100 / 192
layer  3  :  0.4166666666666667  ==>  112 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.9270833333333334  ==>  56 / 768
layer  9  :  0.5546875  ==>  342 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5963541666666666  ==>  310 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  4
backward train epoch:  2
test acc:  0.8652
forward train acc:  0.9979  and loss:  2.424323209794238
test acc:  0.9181
forward train acc:  0.99794  and loss:  2.585516053106403
test acc:  0.9178
forward train acc:  0.99828  and loss:  2.0716078688856214
test acc:  0.9149
forward train acc:  0.9986  and loss:  1.713022120296955
test acc:  0.9186
forward train acc:  0.99858  and loss:  1.611295123846503
test acc:  0.9186
forward train acc:  0.9987  and loss:  1.6679708521842258
test acc:  0.9184
forward train acc:  0.99868  and loss:  1.652465544641018
test acc:  0.9194
forward train acc:  0.99904  and loss:  1.2594405452691717
test acc:  0.9193
forward train acc:  0.9989  and loss:  1.2935514336568303
test acc:  0.9187
forward train acc:  0.99924  and loss:  1.049792000936577
test acc:  0.9187
********** reverse layer  4  *********
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5  ==>  48 / 96
layer  2  :  0.4791666666666667  ==>  100 / 192
layer  3  :  0.4166666666666667  ==>  112 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.9270833333333334  ==>  56 / 768
layer  9  :  0.5546875  ==>  342 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5963541666666666  ==>  310 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
***** skip layer  5
[0, 0, 0, 0, 5, 4, 0, 0, 0, 0, 0, 0, 0, 0]
optimize layer  6
backward train epoch:  6
test acc:  0.5767
forward train acc:  0.99898  and loss:  1.4082016963511705
test acc:  0.9198
forward train acc:  0.99848  and loss:  1.8054625184158795
test acc:  0.919
forward train acc:  0.99888  and loss:  1.5019480789778754
test acc:  0.9171
forward train acc:  0.99888  and loss:  1.2380813430063426
test acc:  0.9166
forward train acc:  0.99904  and loss:  1.1435634972294793
test acc:  0.919
forward train acc:  0.99896  and loss:  1.148903708322905
test acc:  0.9181
forward train acc:  0.9989  and loss:  1.253715188242495
test acc:  0.9171
forward train acc:  0.99908  and loss:  1.1832427104527596
test acc:  0.9193
forward train acc:  0.9992  and loss:  0.9816305616404861
test acc:  0.9195
forward train acc:  0.99944  and loss:  0.7137668149953242
test acc:  0.9192
********** reverse layer  6  *********
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5  ==>  48 / 96
layer  2  :  0.4791666666666667  ==>  100 / 192
layer  3  :  0.4166666666666667  ==>  112 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.78125  ==>  168 / 768
layer  8  :  0.9270833333333334  ==>  56 / 768
layer  9  :  0.5546875  ==>  342 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5963541666666666  ==>  310 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  7
backward train epoch:  32
test acc:  0.1242
forward train acc:  0.99886  and loss:  1.4036235177627532
test acc:  0.9193
forward train acc:  0.99902  and loss:  1.1361051296116784
test acc:  0.9193
forward train acc:  0.99902  and loss:  1.1506988723413087
test acc:  0.9197
forward train acc:  0.9991  and loss:  1.133124441606924
test acc:  0.9193
forward train acc:  0.99884  and loss:  1.0831550639122725
test acc:  0.9182
forward train acc:  0.99904  and loss:  1.0266570016392507
test acc:  0.9194
forward train acc:  0.9993  and loss:  0.9082983583211899
test acc:  0.9201
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5  ==>  48 / 96
layer  2  :  0.4791666666666667  ==>  100 / 192
layer  3  :  0.4166666666666667  ==>  112 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.7916666666666666  ==>  160 / 768
layer  8  :  0.9270833333333334  ==>  56 / 768
layer  9  :  0.5546875  ==>  342 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5963541666666666  ==>  310 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  8
backward train epoch:  75
test acc:  0.0987
forward train acc:  0.99892  and loss:  1.24759806151269
test acc:  0.9172
forward train acc:  0.99842  and loss:  1.920004913059529
test acc:  0.9179
forward train acc:  0.99856  and loss:  1.632739177410258
test acc:  0.9185
forward train acc:  0.999  and loss:  1.1790453572757542
test acc:  0.919
forward train acc:  0.99924  and loss:  0.9273241735063493
test acc:  0.9191
forward train acc:  0.99916  and loss:  1.1025867859425489
test acc:  0.9194
forward train acc:  0.99926  and loss:  1.0200816136784852
test acc:  0.9203
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5  ==>  48 / 96
layer  2  :  0.4791666666666667  ==>  100 / 192
layer  3  :  0.4166666666666667  ==>  112 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.7916666666666666  ==>  160 / 768
layer  8  :  0.9375  ==>  48 / 768
layer  9  :  0.5546875  ==>  342 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5963541666666666  ==>  310 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  9
backward train epoch:  54
test acc:  0.098
forward train acc:  0.99896  and loss:  1.3578876485989895
test acc:  0.9199
forward train acc:  0.99896  and loss:  1.3173835325578693
test acc:  0.9214
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5  ==>  48 / 96
layer  2  :  0.4791666666666667  ==>  100 / 192
layer  3  :  0.4166666666666667  ==>  112 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.7916666666666666  ==>  160 / 768
layer  8  :  0.9375  ==>  48 / 768
layer  9  :  0.5625  ==>  336 / 768
layer  10  :  0.5546875  ==>  342 / 768
layer  11  :  0.5963541666666666  ==>  310 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  10
backward train epoch:  88
test acc:  0.0968
forward train acc:  0.99894  and loss:  1.2824023750436027
test acc:  0.9173
forward train acc:  0.99864  and loss:  1.6357817274983972
test acc:  0.9154
forward train acc:  0.99882  and loss:  1.3758744689403102
test acc:  0.9185
forward train acc:  0.9989  and loss:  1.3142566810565768
test acc:  0.9191
forward train acc:  0.99912  and loss:  1.0710202520131133
test acc:  0.92
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5  ==>  48 / 96
layer  2  :  0.4791666666666667  ==>  100 / 192
layer  3  :  0.4166666666666667  ==>  112 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.7916666666666666  ==>  160 / 768
layer  8  :  0.9375  ==>  48 / 768
layer  9  :  0.5625  ==>  336 / 768
layer  10  :  0.5625  ==>  336 / 768
layer  11  :  0.5963541666666666  ==>  310 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  11
backward train epoch:  21
test acc:  0.1439
forward train acc:  0.99916  and loss:  0.997197954216972
test acc:  0.9198
forward train acc:  0.99894  and loss:  1.2996325908461586
test acc:  0.9199
forward train acc:  0.99886  and loss:  1.5389500513265375
test acc:  0.9174
forward train acc:  0.9991  and loss:  1.0917307139607146
test acc:  0.9202
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5  ==>  48 / 96
layer  2  :  0.4791666666666667  ==>  100 / 192
layer  3  :  0.4166666666666667  ==>  112 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.7916666666666666  ==>  160 / 768
layer  8  :  0.9375  ==>  48 / 768
layer  9  :  0.5625  ==>  336 / 768
layer  10  :  0.5625  ==>  336 / 768
layer  11  :  0.6041666666666666  ==>  304 / 768
layer  12  :  0.8229166666666666  ==>  136 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  12
backward train epoch:  90
test acc:  0.0947
forward train acc:  0.9992  and loss:  2.6569269811734557
test acc:  0.9212
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5  ==>  48 / 96
layer  2  :  0.4791666666666667  ==>  100 / 192
layer  3  :  0.4166666666666667  ==>  112 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.7916666666666666  ==>  160 / 768
layer  8  :  0.9375  ==>  48 / 768
layer  9  :  0.5625  ==>  336 / 768
layer  10  :  0.5625  ==>  336 / 768
layer  11  :  0.6041666666666666  ==>  304 / 768
layer  12  :  0.8333333333333334  ==>  128 / 768
layer  13  :  0.4505208333333333  ==>  422 / 768
optimize layer  13
backward train epoch:  102
test acc:  0.0992
forward train acc:  0.99882  and loss:  2.3251931006088853
test acc:  0.9204
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5  ==>  48 / 96
layer  2  :  0.4791666666666667  ==>  100 / 192
layer  3  :  0.4166666666666667  ==>  112 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.7916666666666666  ==>  160 / 768
layer  8  :  0.9375  ==>  48 / 768
layer  9  :  0.5625  ==>  336 / 768
layer  10  :  0.5625  ==>  336 / 768
layer  11  :  0.6041666666666666  ==>  304 / 768
layer  12  :  0.8333333333333334  ==>  128 / 768
layer  13  :  0.4583333333333333  ==>  416 / 768
optimize layer  0
backward train epoch:  81
test acc:  0.0958
forward train acc:  0.95128  and loss:  69.17361387610435
test acc:  0.8916
forward train acc:  0.97322  and loss:  33.93032689020038
test acc:  0.8991
forward train acc:  0.98148  and loss:  23.76934609748423
test acc:  0.9038
forward train acc:  0.98662  and loss:  17.141065535135567
test acc:  0.9046
forward train acc:  0.98806  and loss:  15.184371822047979
test acc:  0.9065
forward train acc:  0.98914  and loss:  12.94388073682785
test acc:  0.9077
forward train acc:  0.99168  and loss:  10.242775638587773
test acc:  0.9076
forward train acc:  0.99226  and loss:  9.639819948934019
test acc:  0.9097
forward train acc:  0.99234  and loss:  9.42957199877128
test acc:  0.9083
forward train acc:  0.9938  and loss:  8.112787435529754
test acc:  0.9105
********** reverse layer  0  *********
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5  ==>  48 / 96
layer  2  :  0.4791666666666667  ==>  100 / 192
layer  3  :  0.4166666666666667  ==>  112 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.7916666666666666  ==>  160 / 768
layer  8  :  0.9375  ==>  48 / 768
layer  9  :  0.5625  ==>  336 / 768
layer  10  :  0.5625  ==>  336 / 768
layer  11  :  0.6041666666666666  ==>  304 / 768
layer  12  :  0.8333333333333334  ==>  128 / 768
layer  13  :  0.4583333333333333  ==>  416 / 768
optimize layer  1
backward train epoch:  55
test acc:  0.1011
forward train acc:  0.99598  and loss:  5.543995364801958
test acc:  0.9169
forward train acc:  0.9979  and loss:  3.017033224226907
test acc:  0.9184
forward train acc:  0.99854  and loss:  2.5260054459795356
test acc:  0.9192
forward train acc:  0.9987  and loss:  2.1689160076202825
test acc:  0.9189
forward train acc:  0.99892  and loss:  1.9266905022086576
test acc:  0.9193
forward train acc:  0.99936  and loss:  1.4137444084044546
test acc:  0.9195
forward train acc:  0.999  and loss:  1.6088197811041027
test acc:  0.9182
forward train acc:  0.99912  and loss:  1.507001721067354
test acc:  0.9191
forward train acc:  0.99946  and loss:  1.2062765231821686
test acc:  0.9211
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5104166666666666  ==>  47 / 96
layer  2  :  0.4791666666666667  ==>  100 / 192
layer  3  :  0.4166666666666667  ==>  112 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.7916666666666666  ==>  160 / 768
layer  8  :  0.9375  ==>  48 / 768
layer  9  :  0.5625  ==>  336 / 768
layer  10  :  0.5625  ==>  336 / 768
layer  11  :  0.6041666666666666  ==>  304 / 768
layer  12  :  0.8333333333333334  ==>  128 / 768
layer  13  :  0.4583333333333333  ==>  416 / 768
optimize layer  2
backward train epoch:  59
test acc:  0.1002
forward train acc:  0.99878  and loss:  1.8472401313483715
test acc:  0.9186
forward train acc:  0.9989  and loss:  1.724248270271346
test acc:  0.9202
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5104166666666666  ==>  47 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4166666666666667  ==>  112 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.7916666666666666  ==>  160 / 768
layer  8  :  0.9375  ==>  48 / 768
layer  9  :  0.5625  ==>  336 / 768
layer  10  :  0.5625  ==>  336 / 768
layer  11  :  0.6041666666666666  ==>  304 / 768
layer  12  :  0.8333333333333334  ==>  128 / 768
layer  13  :  0.4583333333333333  ==>  416 / 768
optimize layer  3
backward train epoch:  1
test acc:  0.8828
forward train acc:  0.99828  and loss:  2.5273967219982296
test acc:  0.9196
forward train acc:  0.99846  and loss:  2.398058965220116
test acc:  0.9191
forward train acc:  0.999  and loss:  1.6279520317912102
test acc:  0.9195
forward train acc:  0.99894  and loss:  1.6022025180282071
test acc:  0.9201
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5104166666666666  ==>  47 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.7916666666666666  ==>  160 / 768
layer  8  :  0.9375  ==>  48 / 768
layer  9  :  0.5625  ==>  336 / 768
layer  10  :  0.5625  ==>  336 / 768
layer  11  :  0.6041666666666666  ==>  304 / 768
layer  12  :  0.8333333333333334  ==>  128 / 768
layer  13  :  0.4583333333333333  ==>  416 / 768
***** skip layer  4
[5, 0, 0, 0, 4, 4, 5, 0, 0, 0, 0, 0, 0, 0]
***** skip layer  5
[5, 0, 0, 0, 4, 3, 5, 0, 0, 0, 0, 0, 0, 0]
***** skip layer  6
[5, 0, 0, 0, 4, 3, 4, 0, 0, 0, 0, 0, 0, 0]
optimize layer  7
backward train epoch:  34
test acc:  0.1005
forward train acc:  0.9987  and loss:  1.835753216757439
test acc:  0.9205
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5104166666666666  ==>  47 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.8020833333333334  ==>  152 / 768
layer  8  :  0.9375  ==>  48 / 768
layer  9  :  0.5625  ==>  336 / 768
layer  10  :  0.5625  ==>  336 / 768
layer  11  :  0.6041666666666666  ==>  304 / 768
layer  12  :  0.8333333333333334  ==>  128 / 768
layer  13  :  0.4583333333333333  ==>  416 / 768
optimize layer  8
backward train epoch:  26
test acc:  0.1119
forward train acc:  0.99808  and loss:  2.942307196324691
test acc:  0.919
forward train acc:  0.99842  and loss:  2.235394205665216
test acc:  0.9201
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5104166666666666  ==>  47 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.8020833333333334  ==>  152 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5625  ==>  336 / 768
layer  10  :  0.5625  ==>  336 / 768
layer  11  :  0.6041666666666666  ==>  304 / 768
layer  12  :  0.8333333333333334  ==>  128 / 768
layer  13  :  0.4583333333333333  ==>  416 / 768
optimize layer  9
backward train epoch:  74
test acc:  0.085
forward train acc:  0.99802  and loss:  2.5646607514936477
test acc:  0.9188
forward train acc:  0.9987  and loss:  2.0532296509481966
test acc:  0.9174
forward train acc:  0.99882  and loss:  1.7690430544316769
test acc:  0.9162
forward train acc:  0.999  and loss:  1.4764722099062055
test acc:  0.9173
forward train acc:  0.99892  and loss:  1.5827732849866152
test acc:  0.9165
forward train acc:  0.9992  and loss:  1.128881560289301
test acc:  0.9193
forward train acc:  0.999  and loss:  1.4086952842772007
test acc:  0.9202
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5104166666666666  ==>  47 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.8020833333333334  ==>  152 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.5625  ==>  336 / 768
layer  11  :  0.6041666666666666  ==>  304 / 768
layer  12  :  0.8333333333333334  ==>  128 / 768
layer  13  :  0.4583333333333333  ==>  416 / 768
optimize layer  10
backward train epoch:  123
test acc:  0.1542
forward train acc:  0.99908  and loss:  1.3248467585071921
test acc:  0.9183
forward train acc:  0.99864  and loss:  1.8669341469649225
test acc:  0.9168
forward train acc:  0.99886  and loss:  1.7892024966422468
test acc:  0.9188
forward train acc:  0.99894  and loss:  1.3065715284319595
test acc:  0.9194
forward train acc:  0.99898  and loss:  1.4146678952965885
test acc:  0.9203
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5104166666666666  ==>  47 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.8020833333333334  ==>  152 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.5703125  ==>  330 / 768
layer  11  :  0.6041666666666666  ==>  304 / 768
layer  12  :  0.8333333333333334  ==>  128 / 768
layer  13  :  0.4583333333333333  ==>  416 / 768
optimize layer  11
backward train epoch:  47
test acc:  0.0922
forward train acc:  0.9989  and loss:  1.4443937216419727
test acc:  0.9185
forward train acc:  0.99892  and loss:  1.5066598597331904
test acc:  0.9182
forward train acc:  0.99892  and loss:  1.482188198948279
test acc:  0.9197
forward train acc:  0.99908  and loss:  1.379523578856606
test acc:  0.9191
forward train acc:  0.99912  and loss:  1.3124251349945553
test acc:  0.9203
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5104166666666666  ==>  47 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.8020833333333334  ==>  152 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.5703125  ==>  330 / 768
layer  11  :  0.6119791666666666  ==>  298 / 768
layer  12  :  0.8333333333333334  ==>  128 / 768
layer  13  :  0.4583333333333333  ==>  416 / 768
optimize layer  12
backward train epoch:  24
test acc:  0.1038
forward train acc:  0.99922  and loss:  10.607839019503444
test acc:  0.9166
forward train acc:  0.99876  and loss:  7.65200685756281
test acc:  0.9168
forward train acc:  0.99898  and loss:  6.02160362014547
test acc:  0.9177
forward train acc:  0.9992  and loss:  5.021923220716417
test acc:  0.9189
forward train acc:  0.99944  and loss:  4.26082999072969
test acc:  0.918
forward train acc:  0.99944  and loss:  3.948584441561252
test acc:  0.9198
forward train acc:  0.99912  and loss:  3.848466127878055
test acc:  0.9196
forward train acc:  0.99938  and loss:  3.3688595169223845
test acc:  0.921
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5104166666666666  ==>  47 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.8020833333333334  ==>  152 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.5703125  ==>  330 / 768
layer  11  :  0.6119791666666666  ==>  298 / 768
layer  12  :  0.84375  ==>  120 / 768
layer  13  :  0.4583333333333333  ==>  416 / 768
optimize layer  13
backward train epoch:  102
test acc:  0.1044
forward train acc:  0.99908  and loss:  3.3900857984554023
test acc:  0.919
forward train acc:  0.99926  and loss:  2.674865152919665
test acc:  0.92
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5104166666666666  ==>  47 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.8020833333333334  ==>  152 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.5703125  ==>  330 / 768
layer  11  :  0.6119791666666666  ==>  298 / 768
layer  12  :  0.84375  ==>  120 / 768
layer  13  :  0.4661458333333333  ==>  410 / 768
***** skip layer  0
[4, 0, 0, 0, 4, 3, 4, 0, 0, 0, 0, 0, 0, 0]
optimize layer  1
backward train epoch:  15
test acc:  0.1399
forward train acc:  0.99908  and loss:  2.5320251705124974
test acc:  0.9191
forward train acc:  0.99904  and loss:  2.3955277509521693
test acc:  0.9188
forward train acc:  0.99864  and loss:  2.5520452361088246
test acc:  0.9192
forward train acc:  0.99918  and loss:  1.8484568528365344
test acc:  0.9205
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.8020833333333334  ==>  152 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.5703125  ==>  330 / 768
layer  11  :  0.6119791666666666  ==>  298 / 768
layer  12  :  0.84375  ==>  120 / 768
layer  13  :  0.4661458333333333  ==>  410 / 768
optimize layer  2
backward train epoch:  66
test acc:  0.0782
forward train acc:  0.99826  and loss:  3.038271692348644
test acc:  0.9172
forward train acc:  0.99872  and loss:  2.416136530227959
test acc:  0.9162
forward train acc:  0.99834  and loss:  2.4880976136773825
test acc:  0.917
forward train acc:  0.99902  and loss:  1.9292766938451678
test acc:  0.9181
forward train acc:  0.99874  and loss:  2.1180096588796005
test acc:  0.9171
forward train acc:  0.99916  and loss:  1.8098259333055466
test acc:  0.9168
forward train acc:  0.99912  and loss:  1.5536106034414843
test acc:  0.9182
forward train acc:  0.99902  and loss:  1.6304115232778713
test acc:  0.9179
forward train acc:  0.99934  and loss:  1.329784489236772
test acc:  0.9193
forward train acc:  0.99932  and loss:  1.5160334300016984
test acc:  0.9186
********** reverse layer  2  *********
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.8020833333333334  ==>  152 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.5703125  ==>  330 / 768
layer  11  :  0.6119791666666666  ==>  298 / 768
layer  12  :  0.84375  ==>  120 / 768
layer  13  :  0.4661458333333333  ==>  410 / 768
optimize layer  3
backward train epoch:  1
test acc:  0.1242
forward train acc:  0.99898  and loss:  1.7699207934783772
test acc:  0.917
forward train acc:  0.99878  and loss:  1.9888391328277066
test acc:  0.9167
forward train acc:  0.99882  and loss:  1.9048580306116492
test acc:  0.916
forward train acc:  0.99896  and loss:  1.5601123998640105
test acc:  0.9168
forward train acc:  0.99884  and loss:  1.7241431347792968
test acc:  0.916
forward train acc:  0.99912  and loss:  1.4089938537217677
test acc:  0.9166
forward train acc:  0.9991  and loss:  1.4483939096098766
test acc:  0.916
forward train acc:  0.99944  and loss:  1.1100392752559856
test acc:  0.9148
forward train acc:  0.99926  and loss:  1.1006961928214878
test acc:  0.9164
forward train acc:  0.99928  and loss:  1.3704380659619346
test acc:  0.9163
********** reverse layer  3  *********
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.8020833333333334  ==>  152 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.5703125  ==>  330 / 768
layer  11  :  0.6119791666666666  ==>  298 / 768
layer  12  :  0.84375  ==>  120 / 768
layer  13  :  0.4661458333333333  ==>  410 / 768
***** skip layer  4
[4, 0, 5, 5, 3, 3, 4, 0, 0, 0, 0, 0, 0, 0]
***** skip layer  5
[4, 0, 5, 5, 3, 2, 4, 0, 0, 0, 0, 0, 0, 0]
***** skip layer  6
[4, 0, 5, 5, 3, 2, 3, 0, 0, 0, 0, 0, 0, 0]
optimize layer  7
backward train epoch:  88
test acc:  0.0892
forward train acc:  0.99898  and loss:  1.5264507374959067
test acc:  0.9162
forward train acc:  0.99876  and loss:  1.7581002877559513
test acc:  0.9186
forward train acc:  0.9991  and loss:  1.6155631239525974
test acc:  0.9178
forward train acc:  0.99928  and loss:  1.1169815265457146
test acc:  0.92
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.8125  ==>  144 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.5703125  ==>  330 / 768
layer  11  :  0.6119791666666666  ==>  298 / 768
layer  12  :  0.84375  ==>  120 / 768
layer  13  :  0.4661458333333333  ==>  410 / 768
optimize layer  8
backward train epoch:  7
test acc:  0.5151
forward train acc:  0.99936  and loss:  1.214641853235662
test acc:  0.9174
forward train acc:  0.9992  and loss:  1.2727260027313605
test acc:  0.9178
forward train acc:  0.99906  and loss:  1.6362523913849145
test acc:  0.916
forward train acc:  0.99912  and loss:  1.2835351498797536
test acc:  0.9185
forward train acc:  0.99928  and loss:  1.1274115529377013
test acc:  0.9173
forward train acc:  0.99944  and loss:  0.9807623934466392
test acc:  0.9171
forward train acc:  0.99958  and loss:  0.7746781319729052
test acc:  0.9163
forward train acc:  0.99944  and loss:  0.9253433374688029
test acc:  0.9174
forward train acc:  0.99956  and loss:  0.8403279521153308
test acc:  0.9181
forward train acc:  0.99958  and loss:  0.698597410460934
test acc:  0.9182
********** reverse layer  8  *********
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.8125  ==>  144 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.5703125  ==>  330 / 768
layer  11  :  0.6119791666666666  ==>  298 / 768
layer  12  :  0.84375  ==>  120 / 768
layer  13  :  0.4661458333333333  ==>  410 / 768
optimize layer  9
backward train epoch:  35
test acc:  0.1
forward train acc:  0.99904  and loss:  1.1933051146916114
test acc:  0.9178
forward train acc:  0.99932  and loss:  1.3346818648278713
test acc:  0.9168
forward train acc:  0.9993  and loss:  1.1564999299589545
test acc:  0.9161
forward train acc:  0.9994  and loss:  0.9672800756525248
test acc:  0.917
forward train acc:  0.99946  and loss:  0.9769182045711204
test acc:  0.9181
forward train acc:  0.9994  and loss:  0.9799675790127367
test acc:  0.9179
forward train acc:  0.9994  and loss:  0.819241342251189
test acc:  0.9179
forward train acc:  0.99946  and loss:  0.8350910927401856
test acc:  0.9183
forward train acc:  0.99962  and loss:  0.7045564618892968
test acc:  0.918
forward train acc:  0.99962  and loss:  0.6169364517554641
test acc:  0.9174
********** reverse layer  9  *********
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.8125  ==>  144 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.5703125  ==>  330 / 768
layer  11  :  0.6119791666666666  ==>  298 / 768
layer  12  :  0.84375  ==>  120 / 768
layer  13  :  0.4661458333333333  ==>  410 / 768
optimize layer  10
backward train epoch:  262
test acc:  0.0942
forward train acc:  0.99928  and loss:  1.0015012439107522
test acc:  0.9157
forward train acc:  0.9989  and loss:  1.440333662671037
test acc:  0.9179
forward train acc:  0.99944  and loss:  0.8143991830293089
test acc:  0.9174
forward train acc:  0.9994  and loss:  0.9947053052019328
test acc:  0.918
forward train acc:  0.99956  and loss:  0.7731385126244277
test acc:  0.9188
forward train acc:  0.9995  and loss:  0.772094774118159
test acc:  0.9201
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.8125  ==>  144 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.578125  ==>  324 / 768
layer  11  :  0.6119791666666666  ==>  298 / 768
layer  12  :  0.84375  ==>  120 / 768
layer  13  :  0.4661458333333333  ==>  410 / 768
optimize layer  11
backward train epoch:  68
test acc:  0.0811
forward train acc:  0.99922  and loss:  0.9883233719156124
test acc:  0.9169
forward train acc:  0.9994  and loss:  0.9016172631527297
test acc:  0.9189
forward train acc:  0.99926  and loss:  0.9969812851632014
test acc:  0.9176
forward train acc:  0.99962  and loss:  0.8261821767082438
test acc:  0.9187
forward train acc:  0.99928  and loss:  1.0308427843265235
test acc:  0.9192
forward train acc:  0.99932  and loss:  0.8866125911008567
test acc:  0.919
forward train acc:  0.9996  and loss:  0.5954946914571337
test acc:  0.9205
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.8125  ==>  144 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.578125  ==>  324 / 768
layer  11  :  0.6197916666666666  ==>  292 / 768
layer  12  :  0.84375  ==>  120 / 768
layer  13  :  0.4661458333333333  ==>  410 / 768
optimize layer  12
backward train epoch:  204
test acc:  0.1007
forward train acc:  0.98448  and loss:  52.34817831963301
test acc:  0.9187
forward train acc:  0.99914  and loss:  31.950436037033796
test acc:  0.92
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.8125  ==>  144 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.578125  ==>  324 / 768
layer  11  :  0.6197916666666666  ==>  292 / 768
layer  12  :  0.8541666666666666  ==>  112 / 768
layer  13  :  0.4661458333333333  ==>  410 / 768
optimize layer  13
backward train epoch:  102
test acc:  0.1005
forward train acc:  0.99948  and loss:  23.50517436116934
test acc:  0.9184
forward train acc:  0.9994  and loss:  17.694628421217203
test acc:  0.9185
forward train acc:  0.99952  and loss:  13.877834627404809
test acc:  0.9186
forward train acc:  0.99958  and loss:  11.749943068251014
test acc:  0.9209
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.8125  ==>  144 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.578125  ==>  324 / 768
layer  11  :  0.6197916666666666  ==>  292 / 768
layer  12  :  0.8541666666666666  ==>  112 / 768
layer  13  :  0.4739583333333333  ==>  404 / 768
***** skip layer  0
[3, 0, 5, 5, 3, 2, 3, 0, 5, 5, 0, 0, 0, 0]
optimize layer  1
backward train epoch:  23
test acc:  0.1115
forward train acc:  0.99838  and loss:  11.178138110786676
test acc:  0.9167
forward train acc:  0.99864  and loss:  8.880035657435656
test acc:  0.917
forward train acc:  0.99892  and loss:  7.1722933715209365
test acc:  0.9165
forward train acc:  0.99902  and loss:  6.122274494729936
test acc:  0.916
forward train acc:  0.99942  and loss:  5.253902792930603
test acc:  0.918
forward train acc:  0.99938  and loss:  4.82199946558103
test acc:  0.9175
forward train acc:  0.99946  and loss:  4.420641914010048
test acc:  0.9164
forward train acc:  0.99954  and loss:  4.095185162033886
test acc:  0.9177
forward train acc:  0.9995  and loss:  4.001488532871008
test acc:  0.9186
forward train acc:  0.99942  and loss:  3.8864228730089962
test acc:  0.9192
********** reverse layer  1  *********
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.8125  ==>  144 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.578125  ==>  324 / 768
layer  11  :  0.6197916666666666  ==>  292 / 768
layer  12  :  0.8541666666666666  ==>  112 / 768
layer  13  :  0.4739583333333333  ==>  404 / 768
***** skip layer  2
[3, 5, 4, 5, 3, 2, 3, 0, 5, 5, 0, 0, 0, 0]
***** skip layer  3
[3, 5, 4, 4, 3, 2, 3, 0, 5, 5, 0, 0, 0, 0]
***** skip layer  4
[3, 5, 4, 4, 2, 2, 3, 0, 5, 5, 0, 0, 0, 0]
***** skip layer  5
[3, 5, 4, 4, 2, 1, 3, 0, 5, 5, 0, 0, 0, 0]
***** skip layer  6
[3, 5, 4, 4, 2, 1, 2, 0, 5, 5, 0, 0, 0, 0]
optimize layer  7
backward train epoch:  22
test acc:  0.0815
forward train acc:  0.9991  and loss:  3.875174544751644
test acc:  0.9219
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.8229166666666666  ==>  136 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.578125  ==>  324 / 768
layer  11  :  0.6197916666666666  ==>  292 / 768
layer  12  :  0.8541666666666666  ==>  112 / 768
layer  13  :  0.4739583333333333  ==>  404 / 768
***** skip layer  8
[3, 5, 4, 4, 2, 1, 2, 0, 4, 5, 0, 0, 0, 0]
***** skip layer  9
[3, 5, 4, 4, 2, 1, 2, 0, 4, 4, 0, 0, 0, 0]
optimize layer  10
backward train epoch:  72
test acc:  0.1261
forward train acc:  0.99942  and loss:  2.962993502151221
test acc:  0.9219
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.8229166666666666  ==>  136 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.5859375  ==>  318 / 768
layer  11  :  0.6197916666666666  ==>  292 / 768
layer  12  :  0.8541666666666666  ==>  112 / 768
layer  13  :  0.4739583333333333  ==>  404 / 768
optimize layer  11
backward train epoch:  61
test acc:  0.107
forward train acc:  0.9992  and loss:  2.876799952471629
test acc:  0.9175
forward train acc:  0.99922  and loss:  2.5891626982484013
test acc:  0.9185
forward train acc:  0.99956  and loss:  2.066290890565142
test acc:  0.9205
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.8229166666666666  ==>  136 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.5859375  ==>  318 / 768
layer  11  :  0.6276041666666666  ==>  286 / 768
layer  12  :  0.8541666666666666  ==>  112 / 768
layer  13  :  0.4739583333333333  ==>  404 / 768
optimize layer  12
backward train epoch:  155
test acc:  0.1131
forward train acc:  0.9993  and loss:  12.89280042424798
test acc:  0.9196
forward train acc:  0.99926  and loss:  9.62362718116492
test acc:  0.9178
forward train acc:  0.99936  and loss:  8.202152645215392
test acc:  0.9193
forward train acc:  0.9995  and loss:  7.097106771543622
test acc:  0.9192
forward train acc:  0.99952  and loss:  6.5883094454184175
test acc:  0.9198
forward train acc:  0.99974  and loss:  5.986630410887301
test acc:  0.9201
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.8229166666666666  ==>  136 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.5859375  ==>  318 / 768
layer  11  :  0.6276041666666666  ==>  286 / 768
layer  12  :  0.8645833333333334  ==>  104 / 768
layer  13  :  0.4739583333333333  ==>  404 / 768
optimize layer  13
backward train epoch:  102
test acc:  0.1168
forward train acc:  0.99958  and loss:  5.461265288293362
test acc:  0.9195
forward train acc:  0.99932  and loss:  4.844885692000389
test acc:  0.9185
forward train acc:  0.99928  and loss:  4.5672968053258955
test acc:  0.9199
forward train acc:  0.9996  and loss:  3.684844181407243
test acc:  0.9198
forward train acc:  0.99952  and loss:  3.5754030030220747
test acc:  0.9205
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.8229166666666666  ==>  136 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.5859375  ==>  318 / 768
layer  11  :  0.6276041666666666  ==>  286 / 768
layer  12  :  0.8645833333333334  ==>  104 / 768
layer  13  :  0.4817708333333333  ==>  398 / 768
***** skip layer  0
[2, 5, 4, 4, 2, 1, 2, 0, 4, 4, 0, 0, 0, 0]
***** skip layer  1
[2, 4, 4, 4, 2, 1, 2, 0, 4, 4, 0, 0, 0, 0]
***** skip layer  2
[2, 4, 3, 4, 2, 1, 2, 0, 4, 4, 0, 0, 0, 0]
***** skip layer  3
[2, 4, 3, 3, 2, 1, 2, 0, 4, 4, 0, 0, 0, 0]
***** skip layer  4
[2, 4, 3, 3, 1, 1, 2, 0, 4, 4, 0, 0, 0, 0]
***** skip layer  5
[2, 4, 3, 3, 1, 0, 2, 0, 4, 4, 0, 0, 0, 0]
***** skip layer  6
[2, 4, 3, 3, 1, 0, 1, 0, 4, 4, 0, 0, 0, 0]
optimize layer  7
backward train epoch:  162
test acc:  0.0673
forward train acc:  0.99926  and loss:  3.663801094982773
test acc:  0.9193
forward train acc:  0.99952  and loss:  3.058281170204282
test acc:  0.92
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.8333333333333334  ==>  128 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.5859375  ==>  318 / 768
layer  11  :  0.6276041666666666  ==>  286 / 768
layer  12  :  0.8645833333333334  ==>  104 / 768
layer  13  :  0.4817708333333333  ==>  398 / 768
***** skip layer  8
[2, 4, 3, 3, 1, 0, 1, 0, 3, 4, 0, 0, 0, 0]
***** skip layer  9
[2, 4, 3, 3, 1, 0, 1, 0, 3, 3, 0, 0, 0, 0]
optimize layer  10
backward train epoch:  141
test acc:  0.1114
forward train acc:  0.99926  and loss:  2.9550139028578997
test acc:  0.923
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.8333333333333334  ==>  128 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.59375  ==>  312 / 768
layer  11  :  0.6276041666666666  ==>  286 / 768
layer  12  :  0.8645833333333334  ==>  104 / 768
layer  13  :  0.4817708333333333  ==>  398 / 768
optimize layer  11
backward train epoch:  314
test acc:  0.1026
forward train acc:  0.99944  and loss:  2.455652268137783
test acc:  0.9209
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.8333333333333334  ==>  128 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.59375  ==>  312 / 768
layer  11  :  0.6354166666666666  ==>  280 / 768
layer  12  :  0.8645833333333334  ==>  104 / 768
layer  13  :  0.4817708333333333  ==>  398 / 768
optimize layer  12
backward train epoch:  162
test acc:  0.0706
forward train acc:  0.99936  and loss:  26.480257373303175
test acc:  0.9201
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.8333333333333334  ==>  128 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.59375  ==>  312 / 768
layer  11  :  0.6354166666666666  ==>  280 / 768
layer  12  :  0.875  ==>  96 / 768
layer  13  :  0.4817708333333333  ==>  398 / 768
optimize layer  13
backward train epoch:  102
test acc:  0.103
forward train acc:  0.99948  and loss:  19.788300842046738
test acc:  0.9225
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.4583333333333333  ==>  208 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.8333333333333334  ==>  128 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.59375  ==>  312 / 768
layer  11  :  0.6354166666666666  ==>  280 / 768
layer  12  :  0.875  ==>  96 / 768
layer  13  :  0.4895833333333333  ==>  392 / 768
***** skip layer  0
[1, 4, 3, 3, 1, 0, 1, 0, 3, 3, 0, 0, 0, 0]
***** skip layer  1
[1, 3, 3, 3, 1, 0, 1, 0, 3, 3, 0, 0, 0, 0]
***** skip layer  2
[1, 3, 2, 3, 1, 0, 1, 0, 3, 3, 0, 0, 0, 0]
***** skip layer  3
[1, 3, 2, 2, 1, 0, 1, 0, 3, 3, 0, 0, 0, 0]
***** skip layer  4
[1, 3, 2, 2, 0, 0, 1, 0, 3, 3, 0, 0, 0, 0]
optimize layer  5
backward train epoch:  120
test acc:  0.1021
forward train acc:  0.9993  and loss:  17.03244872018695
test acc:  0.9204
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.46875  ==>  204 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.8333333333333334  ==>  128 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.59375  ==>  312 / 768
layer  11  :  0.6354166666666666  ==>  280 / 768
layer  12  :  0.875  ==>  96 / 768
layer  13  :  0.4895833333333333  ==>  392 / 768
***** skip layer  6
[1, 3, 2, 2, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0]
optimize layer  7
backward train epoch:  32
test acc:  0.0879
forward train acc:  0.99926  and loss:  14.539129577577114
test acc:  0.9187
forward train acc:  0.99924  and loss:  12.592687724158168
test acc:  0.9193
forward train acc:  0.9991  and loss:  11.219015013426542
test acc:  0.9199
forward train acc:  0.99934  and loss:  10.148977378383279
test acc:  0.922
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.46875  ==>  204 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.84375  ==>  120 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.59375  ==>  312 / 768
layer  11  :  0.6354166666666666  ==>  280 / 768
layer  12  :  0.875  ==>  96 / 768
layer  13  :  0.4895833333333333  ==>  392 / 768
***** skip layer  8
[1, 3, 2, 2, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0]
***** skip layer  9
[1, 3, 2, 2, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0]
optimize layer  10
backward train epoch:  95
test acc:  0.0877
forward train acc:  0.99918  and loss:  9.351351171731949
test acc:  0.9193
forward train acc:  0.99934  and loss:  8.212638534605503
test acc:  0.9193
forward train acc:  0.99934  and loss:  7.352320853620768
test acc:  0.9209
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.46875  ==>  204 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.84375  ==>  120 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.6015625  ==>  306 / 768
layer  11  :  0.6354166666666666  ==>  280 / 768
layer  12  :  0.875  ==>  96 / 768
layer  13  :  0.4895833333333333  ==>  392 / 768
optimize layer  11
backward train epoch:  75
test acc:  0.0858
forward train acc:  0.99912  and loss:  6.669046240858734
test acc:  0.9202
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.46875  ==>  204 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.84375  ==>  120 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.6015625  ==>  306 / 768
layer  11  :  0.6432291666666666  ==>  274 / 768
layer  12  :  0.875  ==>  96 / 768
layer  13  :  0.4895833333333333  ==>  392 / 768
optimize layer  12
backward train epoch:  25
test acc:  0.0986
forward train acc:  0.99924  and loss:  67.17430289834738
test acc:  0.9225
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.46875  ==>  204 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.84375  ==>  120 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.6015625  ==>  306 / 768
layer  11  :  0.6432291666666666  ==>  274 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.4895833333333333  ==>  392 / 768
optimize layer  13
backward train epoch:  102
test acc:  0.1022
forward train acc:  0.99932  and loss:  56.92527589201927
test acc:  0.9215
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.4895833333333333  ==>  196 / 384
layer  5  :  0.46875  ==>  204 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.84375  ==>  120 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.6015625  ==>  306 / 768
layer  11  :  0.6432291666666666  ==>  274 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.4973958333333333  ==>  386 / 768
***** skip layer  0
[0, 3, 2, 2, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0]
***** skip layer  1
[0, 2, 2, 2, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0]
***** skip layer  2
[0, 2, 1, 2, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0]
***** skip layer  3
[0, 2, 1, 1, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0]
optimize layer  4
backward train epoch:  105
test acc:  0.11
forward train acc:  0.99884  and loss:  49.64715402573347
test acc:  0.92
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.5  ==>  192 / 384
layer  5  :  0.46875  ==>  204 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.84375  ==>  120 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.6015625  ==>  306 / 768
layer  11  :  0.6432291666666666  ==>  274 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.4973958333333333  ==>  386 / 768
optimize layer  5
backward train epoch:  72
test acc:  0.0757
forward train acc:  0.999  and loss:  42.808283254504204
test acc:  0.9208
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.5  ==>  192 / 384
layer  5  :  0.4791666666666667  ==>  200 / 384
layer  6  :  0.4895833333333333  ==>  196 / 384
layer  7  :  0.84375  ==>  120 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.6015625  ==>  306 / 768
layer  11  :  0.6432291666666666  ==>  274 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.4973958333333333  ==>  386 / 768
optimize layer  6
backward train epoch:  48
test acc:  0.0846
forward train acc:  0.99908  and loss:  36.730298820883036
test acc:  0.9198
forward train acc:  0.99886  and loss:  32.206260938197374
test acc:  0.919
forward train acc:  0.99898  and loss:  28.031390853226185
test acc:  0.9206
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.5  ==>  192 / 384
layer  5  :  0.4791666666666667  ==>  200 / 384
layer  6  :  0.5  ==>  192 / 384
layer  7  :  0.84375  ==>  120 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.6015625  ==>  306 / 768
layer  11  :  0.6432291666666666  ==>  274 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.4973958333333333  ==>  386 / 768
optimize layer  7
backward train epoch:  102
test acc:  0.0728
forward train acc:  0.9986  and loss:  25.22122072800994
test acc:  0.9201
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.5  ==>  192 / 384
layer  5  :  0.4791666666666667  ==>  200 / 384
layer  6  :  0.5  ==>  192 / 384
layer  7  :  0.8541666666666666  ==>  112 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.6015625  ==>  306 / 768
layer  11  :  0.6432291666666666  ==>  274 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.4973958333333333  ==>  386 / 768
***** skip layer  8
[0, 2, 1, 1, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0]
***** skip layer  9
[0, 2, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0]
optimize layer  10
backward train epoch:  89
test acc:  0.0735
forward train acc:  0.99906  and loss:  21.81550522148609
test acc:  0.9213
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.5  ==>  192 / 384
layer  5  :  0.4791666666666667  ==>  200 / 384
layer  6  :  0.5  ==>  192 / 384
layer  7  :  0.8541666666666666  ==>  112 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.609375  ==>  300 / 768
layer  11  :  0.6432291666666666  ==>  274 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.4973958333333333  ==>  386 / 768
optimize layer  11
backward train epoch:  173
test acc:  0.1153
forward train acc:  0.9987  and loss:  19.77276687696576
test acc:  0.9195
forward train acc:  0.99914  and loss:  17.61165026202798
test acc:  0.9206
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.5  ==>  192 / 384
layer  5  :  0.4791666666666667  ==>  200 / 384
layer  6  :  0.5  ==>  192 / 384
layer  7  :  0.8541666666666666  ==>  112 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.609375  ==>  300 / 768
layer  11  :  0.6510416666666666  ==>  268 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.4973958333333333  ==>  386 / 768
optimize layer  12
backward train epoch:  139
test acc:  0.1055
forward train acc:  0.53874  and loss:  491.86424362659454
test acc:  0.5435
forward train acc:  0.60072  and loss:  432.3719382882118
test acc:  0.547
forward train acc:  0.70804  and loss:  387.5166543126106
test acc:  0.7234
forward train acc:  0.79662  and loss:  358.18126851320267
test acc:  0.7262
forward train acc:  0.79826  and loss:  340.70750987529755
test acc:  0.7313
forward train acc:  0.7988  and loss:  324.3009167313576
test acc:  0.7319
forward train acc:  0.79938  and loss:  309.1280266046524
test acc:  0.7336
forward train acc:  0.80118  and loss:  298.3735432624817
test acc:  0.735
forward train acc:  0.80994  and loss:  291.50384443998337
test acc:  0.7353
forward train acc:  0.83952  and loss:  284.9197008609772
test acc:  0.8103
********** reverse layer  12  *********
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.5  ==>  192 / 384
layer  5  :  0.4791666666666667  ==>  200 / 384
layer  6  :  0.5  ==>  192 / 384
layer  7  :  0.8541666666666666  ==>  112 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.609375  ==>  300 / 768
layer  11  :  0.6510416666666666  ==>  268 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.4973958333333333  ==>  386 / 768
optimize layer  13
backward train epoch:  102
test acc:  0.1044
forward train acc:  0.9996  and loss:  23.283289723098278
test acc:  0.9221
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.5  ==>  192 / 384
layer  5  :  0.4791666666666667  ==>  200 / 384
layer  6  :  0.5  ==>  192 / 384
layer  7  :  0.8541666666666666  ==>  112 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.609375  ==>  300 / 768
layer  11  :  0.6510416666666666  ==>  268 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5052083333333334  ==>  380 / 768
optimize layer  0
backward train epoch:  238
test acc:  0.0973
forward train acc:  0.89246  and loss:  166.0611972361803
test acc:  0.8564
forward train acc:  0.93104  and loss:  103.90851186960936
test acc:  0.8725
forward train acc:  0.94908  and loss:  77.71582300961018
test acc:  0.8818
forward train acc:  0.95748  and loss:  66.1298302821815
test acc:  0.8839
forward train acc:  0.96274  and loss:  58.504127975553274
test acc:  0.8866
forward train acc:  0.96702  and loss:  53.39311255142093
test acc:  0.8854
forward train acc:  0.96906  and loss:  50.26570699363947
test acc:  0.8894
forward train acc:  0.97146  and loss:  46.200384613126516
test acc:  0.8894
forward train acc:  0.97422  and loss:  43.76304901763797
test acc:  0.8898
forward train acc:  0.97466  and loss:  42.615514904260635
test acc:  0.892
********** reverse layer  0  *********
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.5  ==>  192 / 384
layer  5  :  0.4791666666666667  ==>  200 / 384
layer  6  :  0.5  ==>  192 / 384
layer  7  :  0.8541666666666666  ==>  112 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.609375  ==>  300 / 768
layer  11  :  0.6510416666666666  ==>  268 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5052083333333334  ==>  380 / 768
***** skip layer  1
[5, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 5, 0]
***** skip layer  2
[5, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 5, 0]
***** skip layer  3
[5, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 5, 0]
optimize layer  4
backward train epoch:  98
test acc:  0.1068
forward train acc:  0.99572  and loss:  17.05365989357233
test acc:  0.9177
forward train acc:  0.99794  and loss:  12.953774375841022
test acc:  0.9176
forward train acc:  0.99844  and loss:  11.178734809160233
test acc:  0.9192
forward train acc:  0.99876  and loss:  10.013076055794954
test acc:  0.9209
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.5104166666666666  ==>  188 / 384
layer  5  :  0.4791666666666667  ==>  200 / 384
layer  6  :  0.5  ==>  192 / 384
layer  7  :  0.8541666666666666  ==>  112 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.609375  ==>  300 / 768
layer  11  :  0.6510416666666666  ==>  268 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5052083333333334  ==>  380 / 768
optimize layer  5
backward train epoch:  119
test acc:  0.1109
forward train acc:  0.99844  and loss:  9.906482761725783
test acc:  0.9188
forward train acc:  0.99832  and loss:  8.96185674611479
test acc:  0.9207
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.5104166666666666  ==>  188 / 384
layer  5  :  0.4895833333333333  ==>  196 / 384
layer  6  :  0.5  ==>  192 / 384
layer  7  :  0.8541666666666666  ==>  112 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.609375  ==>  300 / 768
layer  11  :  0.6510416666666666  ==>  268 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5052083333333334  ==>  380 / 768
optimize layer  6
backward train epoch:  127
test acc:  0.1063
forward train acc:  0.99832  and loss:  8.384737358428538
test acc:  0.9187
forward train acc:  0.99856  and loss:  7.468296431936324
test acc:  0.921
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.5104166666666666  ==>  188 / 384
layer  5  :  0.4895833333333333  ==>  196 / 384
layer  6  :  0.5104166666666666  ==>  188 / 384
layer  7  :  0.8541666666666666  ==>  112 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.609375  ==>  300 / 768
layer  11  :  0.6510416666666666  ==>  268 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5052083333333334  ==>  380 / 768
optimize layer  7
backward train epoch:  131
test acc:  0.077
forward train acc:  0.99878  and loss:  6.6969583770260215
test acc:  0.9212
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.5104166666666666  ==>  188 / 384
layer  5  :  0.4895833333333333  ==>  196 / 384
layer  6  :  0.5104166666666666  ==>  188 / 384
layer  7  :  0.8645833333333334  ==>  104 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.609375  ==>  300 / 768
layer  11  :  0.6510416666666666  ==>  268 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5052083333333334  ==>  380 / 768
***** skip layer  8
[5, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 5, 0]
***** skip layer  9
[5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0]
optimize layer  10
backward train epoch:  193
test acc:  0.0906
forward train acc:  0.9987  and loss:  6.5097494861111045
test acc:  0.9199
forward train acc:  0.99872  and loss:  5.812980078160763
test acc:  0.9202
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.5104166666666666  ==>  188 / 384
layer  5  :  0.4895833333333333  ==>  196 / 384
layer  6  :  0.5104166666666666  ==>  188 / 384
layer  7  :  0.8645833333333334  ==>  104 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.6171875  ==>  294 / 768
layer  11  :  0.6510416666666666  ==>  268 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5052083333333334  ==>  380 / 768
optimize layer  11
backward train epoch:  280
test acc:  0.0878
forward train acc:  0.99906  and loss:  5.276202790439129
test acc:  0.9218
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.5104166666666666  ==>  188 / 384
layer  5  :  0.4895833333333333  ==>  196 / 384
layer  6  :  0.5104166666666666  ==>  188 / 384
layer  7  :  0.8645833333333334  ==>  104 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.6171875  ==>  294 / 768
layer  11  :  0.6588541666666666  ==>  262 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5052083333333334  ==>  380 / 768
***** skip layer  12
[5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.0895
forward train acc:  0.99886  and loss:  5.199294577352703
test acc:  0.9203
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.5104166666666666  ==>  188 / 384
layer  5  :  0.4895833333333333  ==>  196 / 384
layer  6  :  0.5104166666666666  ==>  188 / 384
layer  7  :  0.8645833333333334  ==>  104 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.6171875  ==>  294 / 768
layer  11  :  0.6588541666666666  ==>  262 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5130208333333334  ==>  374 / 768
***** skip layer  0
[4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0]
***** skip layer  1
[4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0]
optimize layer  2
backward train epoch:  205
test acc:  0.102
forward train acc:  0.94138  and loss:  86.99427781254053
test acc:  0.8883
forward train acc:  0.96664  and loss:  45.29282698407769
test acc:  0.8963
forward train acc:  0.97644  and loss:  32.449642351828516
test acc:  0.9008
forward train acc:  0.98298  and loss:  23.87879026774317
test acc:  0.9039
forward train acc:  0.98542  and loss:  21.996955419890583
test acc:  0.9035
forward train acc:  0.98538  and loss:  20.696904896758497
test acc:  0.9056
forward train acc:  0.98834  and loss:  16.798130865208805
test acc:  0.9069
forward train acc:  0.99004  and loss:  15.313004265539348
test acc:  0.9075
forward train acc:  0.99062  and loss:  14.30721931438893
test acc:  0.9093
forward train acc:  0.9915  and loss:  13.045092655345798
test acc:  0.9079
********** reverse layer  2  *********
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4270833333333333  ==>  110 / 192
layer  4  :  0.5104166666666666  ==>  188 / 384
layer  5  :  0.4895833333333333  ==>  196 / 384
layer  6  :  0.5104166666666666  ==>  188 / 384
layer  7  :  0.8645833333333334  ==>  104 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.6171875  ==>  294 / 768
layer  11  :  0.6588541666666666  ==>  262 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5130208333333334  ==>  374 / 768
optimize layer  3
backward train epoch:  40
test acc:  0.1046
forward train acc:  0.99812  and loss:  5.6589694195427
test acc:  0.9217
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4375  ==>  108 / 192
layer  4  :  0.5104166666666666  ==>  188 / 384
layer  5  :  0.4895833333333333  ==>  196 / 384
layer  6  :  0.5104166666666666  ==>  188 / 384
layer  7  :  0.8645833333333334  ==>  104 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.6171875  ==>  294 / 768
layer  11  :  0.6588541666666666  ==>  262 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5130208333333334  ==>  374 / 768
optimize layer  4
backward train epoch:  138
test acc:  0.09
forward train acc:  0.99786  and loss:  5.447226976510137
test acc:  0.9195
forward train acc:  0.99834  and loss:  4.700917770154774
test acc:  0.921
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4375  ==>  108 / 192
layer  4  :  0.5208333333333334  ==>  184 / 384
layer  5  :  0.4895833333333333  ==>  196 / 384
layer  6  :  0.5104166666666666  ==>  188 / 384
layer  7  :  0.8645833333333334  ==>  104 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.6171875  ==>  294 / 768
layer  11  :  0.6588541666666666  ==>  262 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5130208333333334  ==>  374 / 768
optimize layer  5
backward train epoch:  122
test acc:  0.0981
forward train acc:  0.99812  and loss:  4.61514642694965
test acc:  0.9201
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4375  ==>  108 / 192
layer  4  :  0.5208333333333334  ==>  184 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5104166666666666  ==>  188 / 384
layer  7  :  0.8645833333333334  ==>  104 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.6171875  ==>  294 / 768
layer  11  :  0.6588541666666666  ==>  262 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5130208333333334  ==>  374 / 768
optimize layer  6
backward train epoch:  54
test acc:  0.1003
forward train acc:  0.998  and loss:  4.60230230120942
test acc:  0.9199
forward train acc:  0.99842  and loss:  4.089376762043685
test acc:  0.9192
forward train acc:  0.99822  and loss:  4.093726816121489
test acc:  0.9186
forward train acc:  0.99872  and loss:  3.617917372379452
test acc:  0.919
forward train acc:  0.99894  and loss:  3.2039821799844503
test acc:  0.9201
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4375  ==>  108 / 192
layer  4  :  0.5208333333333334  ==>  184 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8645833333333334  ==>  104 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.6171875  ==>  294 / 768
layer  11  :  0.6588541666666666  ==>  262 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5130208333333334  ==>  374 / 768
optimize layer  7
backward train epoch:  74
test acc:  0.1219
forward train acc:  0.99812  and loss:  4.117355650756508
test acc:  0.9181
forward train acc:  0.99844  and loss:  3.5159327927976847
test acc:  0.9201
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4375  ==>  108 / 192
layer  4  :  0.5208333333333334  ==>  184 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.875  ==>  96 / 768
layer  8  :  0.9479166666666666  ==>  40 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.6171875  ==>  294 / 768
layer  11  :  0.6588541666666666  ==>  262 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5130208333333334  ==>  374 / 768
optimize layer  8
backward train epoch:  131
test acc:  0.1114
forward train acc:  0.9982  and loss:  4.024898428469896
test acc:  0.9175
forward train acc:  0.99872  and loss:  3.2226179586723447
test acc:  0.9178
forward train acc:  0.99812  and loss:  3.526152648963034
test acc:  0.9174
forward train acc:  0.99874  and loss:  2.8519310380797833
test acc:  0.9187
forward train acc:  0.99864  and loss:  2.8622067468240857
test acc:  0.9206
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4375  ==>  108 / 192
layer  4  :  0.5208333333333334  ==>  184 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.875  ==>  96 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.5703125  ==>  330 / 768
layer  10  :  0.6171875  ==>  294 / 768
layer  11  :  0.6588541666666666  ==>  262 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5130208333333334  ==>  374 / 768
optimize layer  9
backward train epoch:  53
test acc:  0.1156
forward train acc:  0.99854  and loss:  2.8946189074777067
test acc:  0.9187
forward train acc:  0.99844  and loss:  3.029714554315433
test acc:  0.9172
forward train acc:  0.9983  and loss:  3.6549033124465495
test acc:  0.9188
forward train acc:  0.99874  and loss:  2.6443236083723605
test acc:  0.9206
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4375  ==>  108 / 192
layer  4  :  0.5208333333333334  ==>  184 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.875  ==>  96 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.6171875  ==>  294 / 768
layer  11  :  0.6588541666666666  ==>  262 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5130208333333334  ==>  374 / 768
optimize layer  10
backward train epoch:  234
test acc:  0.1263
forward train acc:  0.99882  and loss:  2.43131375964731
test acc:  0.9199
forward train acc:  0.99862  and loss:  2.6139912114012986
test acc:  0.9217
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4375  ==>  108 / 192
layer  4  :  0.5208333333333334  ==>  184 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.875  ==>  96 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.625  ==>  288 / 768
layer  11  :  0.6588541666666666  ==>  262 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5130208333333334  ==>  374 / 768
optimize layer  11
backward train epoch:  47
test acc:  0.099
forward train acc:  0.99878  and loss:  2.491270139813423
test acc:  0.9193
forward train acc:  0.9985  and loss:  2.4235522386152297
test acc:  0.9193
forward train acc:  0.99856  and loss:  2.6838965381029993
test acc:  0.918
forward train acc:  0.99894  and loss:  2.2071226607076824
test acc:  0.9192
forward train acc:  0.99898  and loss:  2.069406376220286
test acc:  0.9213
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4375  ==>  108 / 192
layer  4  :  0.5208333333333334  ==>  184 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.875  ==>  96 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.625  ==>  288 / 768
layer  11  :  0.6666666666666666  ==>  256 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5130208333333334  ==>  374 / 768
***** skip layer  12
[4, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.1306
forward train acc:  0.99894  and loss:  2.1005284190177917
test acc:  0.92
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4375  ==>  108 / 192
layer  4  :  0.5208333333333334  ==>  184 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.875  ==>  96 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.625  ==>  288 / 768
layer  11  :  0.6666666666666666  ==>  256 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5208333333333334  ==>  368 / 768
***** skip layer  0
[3, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0]
optimize layer  1
backward train epoch:  12
test acc:  0.1616
forward train acc:  0.99874  and loss:  2.575681734830141
test acc:  0.9182
forward train acc:  0.9986  and loss:  2.6416289844783023
test acc:  0.9188
forward train acc:  0.9984  and loss:  2.4779054884565994
test acc:  0.9191
forward train acc:  0.99894  and loss:  2.0132745368173346
test acc:  0.918
forward train acc:  0.99928  and loss:  1.6528126588091254
test acc:  0.9188
forward train acc:  0.9992  and loss:  1.7009426284348592
test acc:  0.9188
forward train acc:  0.9992  and loss:  1.5811337563209236
test acc:  0.9192
forward train acc:  0.99916  and loss:  1.7382033785106614
test acc:  0.9186
forward train acc:  0.99918  and loss:  1.6473268039990216
test acc:  0.9194
forward train acc:  0.99936  and loss:  1.4391789870569482
test acc:  0.9194
********** reverse layer  1  *********
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4375  ==>  108 / 192
layer  4  :  0.5208333333333334  ==>  184 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.875  ==>  96 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.625  ==>  288 / 768
layer  11  :  0.6666666666666666  ==>  256 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5208333333333334  ==>  368 / 768
***** skip layer  2
[3, 5, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0]
optimize layer  3
backward train epoch:  2
test acc:  0.1802
forward train acc:  0.99856  and loss:  2.3622352718375623
test acc:  0.9182
forward train acc:  0.99874  and loss:  2.3242636618670076
test acc:  0.9182
forward train acc:  0.9988  and loss:  2.221285195904784
test acc:  0.9193
forward train acc:  0.99884  and loss:  1.941551307332702
test acc:  0.9211
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.5208333333333334  ==>  184 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.875  ==>  96 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.625  ==>  288 / 768
layer  11  :  0.6666666666666666  ==>  256 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5208333333333334  ==>  368 / 768
optimize layer  4
backward train epoch:  247
test acc:  0.0999
forward train acc:  0.99822  and loss:  2.9205898637883365
test acc:  0.9188
forward train acc:  0.99824  and loss:  2.600197877618484
test acc:  0.9182
forward train acc:  0.9985  and loss:  2.321050353581086
test acc:  0.9199
forward train acc:  0.99884  and loss:  2.2320442751515657
test acc:  0.9192
forward train acc:  0.99882  and loss:  1.9781316821463406
test acc:  0.9198
forward train acc:  0.99898  and loss:  1.809545082040131
test acc:  0.9186
forward train acc:  0.999  and loss:  1.775871856836602
test acc:  0.92
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.53125  ==>  180 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.875  ==>  96 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.625  ==>  288 / 768
layer  11  :  0.6666666666666666  ==>  256 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5208333333333334  ==>  368 / 768
optimize layer  5
backward train epoch:  169
test acc:  0.112
forward train acc:  0.9981  and loss:  2.8419673886382952
test acc:  0.9171
forward train acc:  0.99848  and loss:  2.304229739587754
test acc:  0.9176
forward train acc:  0.99906  and loss:  1.7159019771497697
test acc:  0.9163
forward train acc:  0.99858  and loss:  1.9688901626504958
test acc:  0.9181
forward train acc:  0.99886  and loss:  1.899962033960037
test acc:  0.9176
forward train acc:  0.99902  and loss:  1.6101717974524945
test acc:  0.9171
forward train acc:  0.99884  and loss:  1.8925059156026691
test acc:  0.9178
forward train acc:  0.99906  and loss:  1.7047445316566154
test acc:  0.9179
forward train acc:  0.9991  and loss:  1.6921926101204008
test acc:  0.9176
forward train acc:  0.99898  and loss:  1.5673703694483265
test acc:  0.9179
********** reverse layer  5  *********
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.53125  ==>  180 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.875  ==>  96 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.625  ==>  288 / 768
layer  11  :  0.6666666666666666  ==>  256 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5208333333333334  ==>  368 / 768
optimize layer  6
backward train epoch:  188
test acc:  0.1462
forward train acc:  0.99906  and loss:  1.7210441537899897
test acc:  0.9168
forward train acc:  0.99884  and loss:  1.8316798121668398
test acc:  0.9167
forward train acc:  0.99894  and loss:  1.8485205583274364
test acc:  0.9187
forward train acc:  0.999  and loss:  1.5859716252889484
test acc:  0.9179
forward train acc:  0.999  and loss:  1.6588379748282023
test acc:  0.918
forward train acc:  0.99902  and loss:  1.592930378858
test acc:  0.9181
forward train acc:  0.99916  and loss:  1.4143085664254613
test acc:  0.9183
forward train acc:  0.99926  and loss:  1.27017512510065
test acc:  0.9187
forward train acc:  0.99924  and loss:  1.4244710163329728
test acc:  0.9184
forward train acc:  0.99914  and loss:  1.3577162699075416
test acc:  0.9186
********** reverse layer  6  *********
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.53125  ==>  180 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.875  ==>  96 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.625  ==>  288 / 768
layer  11  :  0.6666666666666666  ==>  256 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5208333333333334  ==>  368 / 768
optimize layer  7
backward train epoch:  79
test acc:  0.0827
forward train acc:  0.99868  and loss:  1.918128118966706
test acc:  0.9178
forward train acc:  0.99908  and loss:  1.612771447864361
test acc:  0.9183
forward train acc:  0.99868  and loss:  1.9855199161102064
test acc:  0.9212
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.53125  ==>  180 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.625  ==>  288 / 768
layer  11  :  0.6666666666666666  ==>  256 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5208333333333334  ==>  368 / 768
optimize layer  8
backward train epoch:  133
test acc:  0.0932
forward train acc:  0.9984  and loss:  2.513880812737625
test acc:  0.917
forward train acc:  0.99874  and loss:  1.7959548969520256
test acc:  0.9177
forward train acc:  0.99864  and loss:  2.3396033872268163
test acc:  0.9181
forward train acc:  0.99882  and loss:  1.571664234623313
test acc:  0.9189
forward train acc:  0.99902  and loss:  1.3819597000838257
test acc:  0.9194
forward train acc:  0.99876  and loss:  1.7245268971892074
test acc:  0.9174
forward train acc:  0.99928  and loss:  1.1045084984507412
test acc:  0.9179
forward train acc:  0.99928  and loss:  1.2078937704791315
test acc:  0.919
forward train acc:  0.99926  and loss:  1.2409947013366036
test acc:  0.9175
forward train acc:  0.99908  and loss:  1.3157284672488458
test acc:  0.9188
********** reverse layer  8  *********
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.53125  ==>  180 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.625  ==>  288 / 768
layer  11  :  0.6666666666666666  ==>  256 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5208333333333334  ==>  368 / 768
optimize layer  9
backward train epoch:  97
test acc:  0.0867
forward train acc:  0.99922  and loss:  1.4870820212527178
test acc:  0.9165
forward train acc:  0.99898  and loss:  1.5682378220371902
test acc:  0.9181
forward train acc:  0.99918  and loss:  1.336382033827249
test acc:  0.9186
forward train acc:  0.99896  and loss:  1.2758289722260088
test acc:  0.9182
forward train acc:  0.9993  and loss:  1.2552347824675962
test acc:  0.9183
forward train acc:  0.99932  and loss:  1.2182511339196935
test acc:  0.9179
forward train acc:  0.99926  and loss:  1.2220928745227866
test acc:  0.9172
forward train acc:  0.99934  and loss:  1.0270637154462747
test acc:  0.9196
forward train acc:  0.99938  and loss:  1.0245688563445583
test acc:  0.9179
forward train acc:  0.99928  and loss:  1.0666038400377147
test acc:  0.9183
********** reverse layer  9  *********
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.53125  ==>  180 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.625  ==>  288 / 768
layer  11  :  0.6666666666666666  ==>  256 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5208333333333334  ==>  368 / 768
optimize layer  10
backward train epoch:  112
test acc:  0.0869
forward train acc:  0.99926  and loss:  1.210828188166488
test acc:  0.9183
forward train acc:  0.9992  and loss:  1.2727638299693353
test acc:  0.9188
forward train acc:  0.99908  and loss:  1.2072529332363047
test acc:  0.9188
forward train acc:  0.99916  and loss:  1.279087335395161
test acc:  0.9213
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.53125  ==>  180 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.6328125  ==>  282 / 768
layer  11  :  0.6666666666666666  ==>  256 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5208333333333334  ==>  368 / 768
optimize layer  11
backward train epoch:  15
test acc:  0.1877
forward train acc:  0.999  and loss:  1.377294938720297
test acc:  0.9189
forward train acc:  0.99916  and loss:  1.3729086696985178
test acc:  0.9195
forward train acc:  0.99908  and loss:  1.3223189805285074
test acc:  0.9185
forward train acc:  0.99936  and loss:  0.9432335709570907
test acc:  0.919
forward train acc:  0.99944  and loss:  0.9878717784304172
test acc:  0.921
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.53125  ==>  180 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.6328125  ==>  282 / 768
layer  11  :  0.6744791666666666  ==>  250 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5208333333333334  ==>  368 / 768
***** skip layer  12
[3, 5, 4, 0, 0, 5, 5, 0, 5, 5, 0, 0, 2, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.0861
forward train acc:  0.99912  and loss:  1.12504008540418
test acc:  0.9182
forward train acc:  0.99912  and loss:  1.234614969289396
test acc:  0.9185
forward train acc:  0.99896  and loss:  1.571078963985201
test acc:  0.9191
forward train acc:  0.9992  and loss:  1.1775310286320746
test acc:  0.9187
forward train acc:  0.99934  and loss:  1.0618624837952666
test acc:  0.9202
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.53125  ==>  180 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.6328125  ==>  282 / 768
layer  11  :  0.6744791666666666  ==>  250 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
***** skip layer  0
[2, 5, 4, 0, 0, 5, 5, 0, 5, 5, 0, 0, 2, 0]
***** skip layer  1
[2, 4, 4, 0, 0, 5, 5, 0, 5, 5, 0, 0, 2, 0]
***** skip layer  2
[2, 4, 3, 0, 0, 5, 5, 0, 5, 5, 0, 0, 2, 0]
optimize layer  3
backward train epoch:  128
test acc:  0.1028
forward train acc:  0.99862  and loss:  1.921710684953723
test acc:  0.9162
forward train acc:  0.99896  and loss:  1.5574216379318386
test acc:  0.9149
forward train acc:  0.99864  and loss:  1.9881979248602875
test acc:  0.9181
forward train acc:  0.9989  and loss:  1.51164216484176
test acc:  0.9171
forward train acc:  0.99908  and loss:  1.311480977863539
test acc:  0.9178
forward train acc:  0.99888  and loss:  1.4625028836308047
test acc:  0.9171
forward train acc:  0.99944  and loss:  1.0157016818993725
test acc:  0.9177
forward train acc:  0.99924  and loss:  1.2694772401009686
test acc:  0.9198
forward train acc:  0.99924  and loss:  1.0368695780052803
test acc:  0.919
forward train acc:  0.99934  and loss:  0.9183974015177228
test acc:  0.9178
********** reverse layer  3  *********
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.53125  ==>  180 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.6328125  ==>  282 / 768
layer  11  :  0.6744791666666666  ==>  250 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
optimize layer  4
backward train epoch:  207
test acc:  0.1034
forward train acc:  0.99866  and loss:  1.7401435407227837
test acc:  0.9191
forward train acc:  0.99916  and loss:  1.2021215605782345
test acc:  0.9192
forward train acc:  0.99884  and loss:  1.5313398549333215
test acc:  0.9186
forward train acc:  0.99936  and loss:  0.8897191874566488
test acc:  0.9186
forward train acc:  0.9992  and loss:  1.2194277419475839
test acc:  0.9189
forward train acc:  0.99942  and loss:  0.9905633699963801
test acc:  0.9202
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.6328125  ==>  282 / 768
layer  11  :  0.6744791666666666  ==>  250 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
***** skip layer  5
[2, 4, 3, 5, 0, 4, 5, 0, 5, 5, 0, 0, 2, 0]
***** skip layer  6
[2, 4, 3, 5, 0, 4, 4, 0, 5, 5, 0, 0, 2, 0]
optimize layer  7
backward train epoch:  161
test acc:  0.0953
forward train acc:  0.99856  and loss:  2.0959657265339047
test acc:  0.9188
forward train acc:  0.99882  and loss:  1.666804960812442
test acc:  0.9175
forward train acc:  0.99858  and loss:  2.027542528929189
test acc:  0.9179
forward train acc:  0.99906  and loss:  1.4301818744279444
test acc:  0.9185
forward train acc:  0.999  and loss:  1.1721951922518201
test acc:  0.9196
forward train acc:  0.99892  and loss:  1.3289210980874486
test acc:  0.9182
forward train acc:  0.99944  and loss:  0.9317733649222646
test acc:  0.9189
forward train acc:  0.9994  and loss:  0.8373086382634938
test acc:  0.9195
forward train acc:  0.9994  and loss:  0.966519237117609
test acc:  0.9194
forward train acc:  0.9994  and loss:  0.8274764686939307
test acc:  0.9185
********** reverse layer  7  *********
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.6328125  ==>  282 / 768
layer  11  :  0.6744791666666666  ==>  250 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
***** skip layer  8
[2, 4, 3, 5, 0, 4, 4, 5, 4, 5, 0, 0, 2, 0]
***** skip layer  9
[2, 4, 3, 5, 0, 4, 4, 5, 4, 4, 0, 0, 2, 0]
optimize layer  10
backward train epoch:  96
test acc:  0.0946
forward train acc:  0.9993  and loss:  0.9552214504219592
test acc:  0.9191
forward train acc:  0.99912  and loss:  1.4164608598221093
test acc:  0.9193
forward train acc:  0.99914  and loss:  1.310453331010649
test acc:  0.9186
forward train acc:  0.99936  and loss:  1.1307053088676184
test acc:  0.9185
forward train acc:  0.9994  and loss:  0.8158773225732148
test acc:  0.9187
forward train acc:  0.99922  and loss:  0.8945622972678393
test acc:  0.9201
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.640625  ==>  276 / 768
layer  11  :  0.6744791666666666  ==>  250 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
optimize layer  11
backward train epoch:  330
test acc:  0.0935
forward train acc:  0.99928  and loss:  0.9661407223320566
test acc:  0.9184
forward train acc:  0.99912  and loss:  1.214614349824842
test acc:  0.9171
forward train acc:  0.99896  and loss:  1.5529909461620264
test acc:  0.9187
forward train acc:  0.99928  and loss:  1.0459082302404568
test acc:  0.9197
forward train acc:  0.99946  and loss:  0.874987749644788
test acc:  0.9182
forward train acc:  0.99946  and loss:  0.7864395845972467
test acc:  0.9195
forward train acc:  0.99936  and loss:  0.9365104352473281
test acc:  0.9194
forward train acc:  0.99946  and loss:  0.8250075771356933
test acc:  0.9214
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.640625  ==>  276 / 768
layer  11  :  0.6822916666666666  ==>  244 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
***** skip layer  12
[2, 4, 3, 5, 0, 4, 4, 5, 4, 4, 0, 0, 1, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.0895
forward train acc:  0.99934  and loss:  0.946476988872746
test acc:  0.9182
forward train acc:  0.99896  and loss:  1.2575417028856464
test acc:  0.9182
forward train acc:  0.99914  and loss:  1.2032388768566307
test acc:  0.9186
forward train acc:  0.99922  and loss:  1.2210917147167493
test acc:  0.919
forward train acc:  0.99928  and loss:  0.9049140300194267
test acc:  0.9186
forward train acc:  0.99944  and loss:  0.8390681014861912
test acc:  0.9189
forward train acc:  0.99964  and loss:  0.818143465614412
test acc:  0.9199
forward train acc:  0.9996  and loss:  0.7958317270968109
test acc:  0.9192
forward train acc:  0.99954  and loss:  0.6598534141958226
test acc:  0.9189
forward train acc:  0.9996  and loss:  0.6060275330382865
test acc:  0.9193
********** reverse layer  13  *********
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.640625  ==>  276 / 768
layer  11  :  0.6822916666666666  ==>  244 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
***** skip layer  0
[1, 4, 3, 5, 0, 4, 4, 5, 4, 4, 0, 0, 1, 5]
***** skip layer  1
[1, 3, 3, 5, 0, 4, 4, 5, 4, 4, 0, 0, 1, 5]
***** skip layer  2
[1, 3, 2, 5, 0, 4, 4, 5, 4, 4, 0, 0, 1, 5]
***** skip layer  3
[1, 3, 2, 4, 0, 4, 4, 5, 4, 4, 0, 0, 1, 5]
optimize layer  4
backward train epoch:  77
test acc:  0.1063
forward train acc:  0.99872  and loss:  1.4737081871426199
test acc:  0.916
forward train acc:  0.9992  and loss:  1.2202891951892525
test acc:  0.9167
forward train acc:  0.99878  and loss:  1.6997816632210743
test acc:  0.918
forward train acc:  0.99898  and loss:  1.2126455341349356
test acc:  0.9182
forward train acc:  0.99916  and loss:  0.993574060004903
test acc:  0.9193
forward train acc:  0.99926  and loss:  0.9348203178087715
test acc:  0.9185
forward train acc:  0.99908  and loss:  1.241784504527459
test acc:  0.9186
forward train acc:  0.9992  and loss:  1.0550238322466612
test acc:  0.9198
forward train acc:  0.9992  and loss:  1.0793353436747566
test acc:  0.9188
forward train acc:  0.99936  and loss:  0.7874610632134136
test acc:  0.9192
********** reverse layer  4  *********
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.640625  ==>  276 / 768
layer  11  :  0.6822916666666666  ==>  244 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
***** skip layer  5
[1, 3, 2, 4, 5, 3, 4, 5, 4, 4, 0, 0, 1, 5]
***** skip layer  6
[1, 3, 2, 4, 5, 3, 3, 5, 4, 4, 0, 0, 1, 5]
***** skip layer  7
[1, 3, 2, 4, 5, 3, 3, 4, 4, 4, 0, 0, 1, 5]
***** skip layer  8
[1, 3, 2, 4, 5, 3, 3, 4, 3, 4, 0, 0, 1, 5]
***** skip layer  9
[1, 3, 2, 4, 5, 3, 3, 4, 3, 3, 0, 0, 1, 5]
optimize layer  10
backward train epoch:  26
test acc:  0.0989
forward train acc:  0.99932  and loss:  0.9396493398526218
test acc:  0.919
forward train acc:  0.99932  and loss:  0.9396701343066525
test acc:  0.9182
forward train acc:  0.9991  and loss:  1.221663517295383
test acc:  0.9205
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.6484375  ==>  270 / 768
layer  11  :  0.6822916666666666  ==>  244 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
optimize layer  11
backward train epoch:  235
test acc:  0.1001
forward train acc:  0.99914  and loss:  1.0069746499648318
test acc:  0.9181
forward train acc:  0.99898  and loss:  1.4851591989863664
test acc:  0.9198
forward train acc:  0.99932  and loss:  0.8851437006087508
test acc:  0.9191
forward train acc:  0.99946  and loss:  0.9240299911762122
test acc:  0.9196
forward train acc:  0.99944  and loss:  0.841835553641431
test acc:  0.9193
forward train acc:  0.9994  and loss:  0.7923951121920254
test acc:  0.9216
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.6484375  ==>  270 / 768
layer  11  :  0.6901041666666666  ==>  238 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
***** skip layer  12
[1, 3, 2, 4, 5, 3, 3, 4, 3, 3, 0, 0, 0, 5]
***** skip layer  13
[1, 3, 2, 4, 5, 3, 3, 4, 3, 3, 0, 0, 0, 4]
***** skip layer  0
[0, 3, 2, 4, 5, 3, 3, 4, 3, 3, 0, 0, 0, 4]
***** skip layer  1
[0, 2, 2, 4, 5, 3, 3, 4, 3, 3, 0, 0, 0, 4]
***** skip layer  2
[0, 2, 1, 4, 5, 3, 3, 4, 3, 3, 0, 0, 0, 4]
***** skip layer  3
[0, 2, 1, 3, 5, 3, 3, 4, 3, 3, 0, 0, 0, 4]
***** skip layer  4
[0, 2, 1, 3, 4, 3, 3, 4, 3, 3, 0, 0, 0, 4]
***** skip layer  5
[0, 2, 1, 3, 4, 2, 3, 4, 3, 3, 0, 0, 0, 4]
***** skip layer  6
[0, 2, 1, 3, 4, 2, 2, 4, 3, 3, 0, 0, 0, 4]
***** skip layer  7
[0, 2, 1, 3, 4, 2, 2, 3, 3, 3, 0, 0, 0, 4]
***** skip layer  8
[0, 2, 1, 3, 4, 2, 2, 3, 2, 3, 0, 0, 0, 4]
***** skip layer  9
[0, 2, 1, 3, 4, 2, 2, 3, 2, 2, 0, 0, 0, 4]
optimize layer  10
backward train epoch:  133
test acc:  0.087
forward train acc:  0.99926  and loss:  0.9455194325419143
test acc:  0.9204
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.65625  ==>  264 / 768
layer  11  :  0.6901041666666666  ==>  238 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
optimize layer  11
backward train epoch:  159
test acc:  0.1011
forward train acc:  0.99936  and loss:  0.9962034309864976
test acc:  0.9198
forward train acc:  0.99924  and loss:  1.2115479363419581
test acc:  0.9205
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.65625  ==>  264 / 768
layer  11  :  0.6979166666666666  ==>  232 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
optimize layer  12
backward train epoch:  231
test acc:  0.1002
forward train acc:  0.12176  and loss:  873.6381632089615
test acc:  0.1796
forward train acc:  0.19542  and loss:  705.3508423566818
test acc:  0.1876
forward train acc:  0.27554  and loss:  618.3105102777481
test acc:  0.3344
forward train acc:  0.36538  and loss:  560.4938286542892
test acc:  0.3416
forward train acc:  0.39054  and loss:  525.1690711975098
test acc:  0.3496
forward train acc:  0.41182  and loss:  492.77125108242035
test acc:  0.3512
forward train acc:  0.44376  and loss:  461.72651505470276
test acc:  0.4403
forward train acc:  0.47342  and loss:  439.79580730199814
test acc:  0.4452
forward train acc:  0.48976  and loss:  425.22818529605865
test acc:  0.4472
forward train acc:  0.50158  and loss:  411.46279484033585
test acc:  0.4471
********** reverse layer  12  *********
layer  0  :  0.3125  ==>  66 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.65625  ==>  264 / 768
layer  11  :  0.6979166666666666  ==>  232 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
***** skip layer  13
[0, 2, 1, 3, 4, 2, 2, 3, 2, 2, 0, 0, 5, 3]
optimize layer  0
backward train epoch:  38
test acc:  0.1132
forward train acc:  0.99856  and loss:  2.1995487218373455
test acc:  0.9217
layer  0  :  0.3229166666666667  ==>  65 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.65625  ==>  264 / 768
layer  11  :  0.6979166666666666  ==>  232 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
***** skip layer  1
[0, 1, 1, 3, 4, 2, 2, 3, 2, 2, 0, 0, 5, 3]
***** skip layer  2
[0, 1, 0, 3, 4, 2, 2, 3, 2, 2, 0, 0, 5, 3]
***** skip layer  3
[0, 1, 0, 2, 4, 2, 2, 3, 2, 2, 0, 0, 5, 3]
***** skip layer  4
[0, 1, 0, 2, 3, 2, 2, 3, 2, 2, 0, 0, 5, 3]
***** skip layer  5
[0, 1, 0, 2, 3, 1, 2, 3, 2, 2, 0, 0, 5, 3]
***** skip layer  6
[0, 1, 0, 2, 3, 1, 1, 3, 2, 2, 0, 0, 5, 3]
***** skip layer  7
[0, 1, 0, 2, 3, 1, 1, 2, 2, 2, 0, 0, 5, 3]
***** skip layer  8
[0, 1, 0, 2, 3, 1, 1, 2, 1, 2, 0, 0, 5, 3]
***** skip layer  9
[0, 1, 0, 2, 3, 1, 1, 2, 1, 1, 0, 0, 5, 3]
optimize layer  10
backward train epoch:  224
test acc:  0.0957
forward train acc:  0.9988  and loss:  1.915846384014003
test acc:  0.9164
forward train acc:  0.999  and loss:  1.3884831892792135
test acc:  0.9162
forward train acc:  0.99888  and loss:  1.6948662112699822
test acc:  0.918
forward train acc:  0.99884  and loss:  1.7234309593914077
test acc:  0.9185
forward train acc:  0.999  and loss:  1.2667701102909632
test acc:  0.9202
layer  0  :  0.3229166666666667  ==>  65 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.6640625  ==>  258 / 768
layer  11  :  0.6979166666666666  ==>  232 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
optimize layer  11
backward train epoch:  969
test acc:  0.0968
forward train acc:  0.99908  and loss:  1.3857514397823252
test acc:  0.9187
forward train acc:  0.99904  and loss:  1.3633831330225803
test acc:  0.9175
forward train acc:  0.99894  and loss:  1.5819180132239126
test acc:  0.92
layer  0  :  0.3229166666666667  ==>  65 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.6640625  ==>  258 / 768
layer  11  :  0.7057291666666666  ==>  226 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
***** skip layer  12
[0, 1, 0, 2, 3, 1, 1, 2, 1, 1, 0, 0, 4, 3]
***** skip layer  13
[0, 1, 0, 2, 3, 1, 1, 2, 1, 1, 0, 0, 4, 2]
optimize layer  0
backward train epoch:  68
test acc:  0.1134
forward train acc:  0.99702  and loss:  3.81454513448989
test acc:  0.9148
forward train acc:  0.99706  and loss:  4.101748219167348
test acc:  0.9153
forward train acc:  0.99806  and loss:  2.647565533989109
test acc:  0.9141
forward train acc:  0.9982  and loss:  2.47674926253967
test acc:  0.9176
forward train acc:  0.99834  and loss:  2.295332405657973
test acc:  0.9179
forward train acc:  0.99816  and loss:  2.675788886961527
test acc:  0.9178
forward train acc:  0.9987  and loss:  1.8168286415166222
test acc:  0.9187
forward train acc:  0.99864  and loss:  1.8871625954052433
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.4895833333333333  ==>  98 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.6640625  ==>  258 / 768
layer  11  :  0.7057291666666666  ==>  226 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
***** skip layer  1
[0, 0, 0, 2, 3, 1, 1, 2, 1, 1, 0, 0, 4, 2]
optimize layer  2
backward train epoch:  102
test acc:  0.1101
forward train acc:  0.99846  and loss:  2.109484691929538
test acc:  0.9173
forward train acc:  0.99804  and loss:  2.7198970630415715
test acc:  0.9204
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.6640625  ==>  258 / 768
layer  11  :  0.7057291666666666  ==>  226 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
***** skip layer  3
[0, 0, 0, 1, 3, 1, 1, 2, 1, 1, 0, 0, 4, 2]
***** skip layer  4
[0, 0, 0, 1, 2, 1, 1, 2, 1, 1, 0, 0, 4, 2]
***** skip layer  5
[0, 0, 0, 1, 2, 0, 1, 2, 1, 1, 0, 0, 4, 2]
***** skip layer  6
[0, 0, 0, 1, 2, 0, 0, 2, 1, 1, 0, 0, 4, 2]
***** skip layer  7
[0, 0, 0, 1, 2, 0, 0, 1, 1, 1, 0, 0, 4, 2]
***** skip layer  8
[0, 0, 0, 1, 2, 0, 0, 1, 0, 1, 0, 0, 4, 2]
***** skip layer  9
[0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 0, 0, 4, 2]
optimize layer  10
backward train epoch:  239
test acc:  0.1
forward train acc:  0.998  and loss:  2.591440742195118
test acc:  0.9184
forward train acc:  0.9981  and loss:  2.455908154719509
test acc:  0.9185
forward train acc:  0.99836  and loss:  2.175936394458404
test acc:  0.9171
forward train acc:  0.99844  and loss:  2.1422889016685076
test acc:  0.9213
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.671875  ==>  252 / 768
layer  11  :  0.7057291666666666  ==>  226 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
optimize layer  11
backward train epoch:  102
test acc:  0.0998
forward train acc:  0.9986  and loss:  1.925776527961716
test acc:  0.9191
forward train acc:  0.99866  and loss:  1.938463635451626
test acc:  0.919
forward train acc:  0.99846  and loss:  2.0797775866813026
test acc:  0.9176
forward train acc:  0.99852  and loss:  1.792669315909734
test acc:  0.9191
forward train acc:  0.99874  and loss:  1.661997607210651
test acc:  0.9183
forward train acc:  0.99908  and loss:  1.4083013794734143
test acc:  0.9189
forward train acc:  0.99888  and loss:  1.60603567736689
test acc:  0.9184
forward train acc:  0.99902  and loss:  1.3182174540997948
test acc:  0.9212
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.671875  ==>  252 / 768
layer  11  :  0.7135416666666666  ==>  220 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
***** skip layer  12
[0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 0, 0, 3, 2]
***** skip layer  13
[0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 0, 0, 3, 1]
optimize layer  0
backward train epoch:  87
test acc:  0.1032
forward train acc:  0.9988  and loss:  1.6323955790721811
test acc:  0.9187
forward train acc:  0.99872  and loss:  1.7792214714281727
test acc:  0.9185
forward train acc:  0.99866  and loss:  1.5054591072839685
test acc:  0.9168
forward train acc:  0.9989  and loss:  1.4966504569747485
test acc:  0.9173
forward train acc:  0.99906  and loss:  1.2120752652699593
test acc:  0.9182
forward train acc:  0.99926  and loss:  1.1502848481177352
test acc:  0.9181
forward train acc:  0.9993  and loss:  0.9144912858610041
test acc:  0.9185
forward train acc:  0.99926  and loss:  1.129907066351734
test acc:  0.9191
forward train acc:  0.99914  and loss:  1.0553229597280733
test acc:  0.9188
forward train acc:  0.99924  and loss:  1.1282236746046692
test acc:  0.9199
********** reverse layer  0  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.671875  ==>  252 / 768
layer  11  :  0.7135416666666666  ==>  220 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
optimize layer  1
backward train epoch:  252
test acc:  0.1081
forward train acc:  0.9797  and loss:  29.954508313676342
test acc:  0.9073
forward train acc:  0.99008  and loss:  12.652018367196433
test acc:  0.9095
forward train acc:  0.99262  and loss:  9.238223772146739
test acc:  0.9122
forward train acc:  0.99504  and loss:  6.200620352756232
test acc:  0.9126
forward train acc:  0.99534  and loss:  6.124305380042642
test acc:  0.9129
forward train acc:  0.99582  and loss:  5.06395037000766
test acc:  0.9136
forward train acc:  0.99606  and loss:  4.761446557822637
test acc:  0.9129
forward train acc:  0.99654  and loss:  4.314323909115046
test acc:  0.9136
forward train acc:  0.99666  and loss:  4.0330235250876285
test acc:  0.9143
forward train acc:  0.99732  and loss:  3.332921082037501
test acc:  0.9137
********** reverse layer  1  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.671875  ==>  252 / 768
layer  11  :  0.7135416666666666  ==>  220 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
optimize layer  2
backward train epoch:  169
test acc:  0.1
forward train acc:  0.94838  and loss:  69.05798759870231
test acc:  0.887
forward train acc:  0.96956  and loss:  36.91722190566361
test acc:  0.8952
forward train acc:  0.97898  and loss:  25.213807048276067
test acc:  0.8992
forward train acc:  0.9833  and loss:  19.85177064407617
test acc:  0.9021
forward train acc:  0.98566  and loss:  16.91987193841487
test acc:  0.9046
forward train acc:  0.98848  and loss:  14.29816050361842
test acc:  0.9067
forward train acc:  0.98878  and loss:  13.2631296828622
test acc:  0.9076
forward train acc:  0.98874  and loss:  13.125363644678146
test acc:  0.9087
forward train acc:  0.99042  and loss:  11.41736511234194
test acc:  0.9074
forward train acc:  0.99108  and loss:  10.211123583954759
test acc:  0.9094
********** reverse layer  2  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5  ==>  192 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.671875  ==>  252 / 768
layer  11  :  0.7135416666666666  ==>  220 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
***** skip layer  3
[5, 5, 5, 0, 2, 0, 0, 1, 0, 0, 0, 0, 3, 1]
***** skip layer  4
[5, 5, 5, 0, 1, 0, 0, 1, 0, 0, 0, 0, 3, 1]
optimize layer  5
backward train epoch:  42
test acc:  0.1264
forward train acc:  0.99836  and loss:  2.3137444561580196
test acc:  0.9175
forward train acc:  0.99842  and loss:  1.9471464561647736
test acc:  0.9162
forward train acc:  0.99854  and loss:  1.6638331692374777
test acc:  0.915
forward train acc:  0.99888  and loss:  1.5663248522905633
test acc:  0.9178
forward train acc:  0.99904  and loss:  1.2059520985931158
test acc:  0.9169
forward train acc:  0.99908  and loss:  1.1930874976096675
test acc:  0.9176
forward train acc:  0.99896  and loss:  1.3664522096514702
test acc:  0.9178
forward train acc:  0.99928  and loss:  0.9766087024181616
test acc:  0.9196
forward train acc:  0.9993  and loss:  0.9721543748455588
test acc:  0.9204
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5104166666666666  ==>  188 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.671875  ==>  252 / 768
layer  11  :  0.7135416666666666  ==>  220 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
optimize layer  6
backward train epoch:  142
test acc:  0.1056
forward train acc:  0.999  and loss:  1.220697919401573
test acc:  0.9185
forward train acc:  0.99872  and loss:  1.4318867791444063
test acc:  0.9197
forward train acc:  0.99894  and loss:  1.5618486870080233
test acc:  0.9178
forward train acc:  0.99904  and loss:  1.205874793638941
test acc:  0.9193
forward train acc:  0.99918  and loss:  1.1531096403195988
test acc:  0.9157
forward train acc:  0.99924  and loss:  0.9393530759843998
test acc:  0.9172
forward train acc:  0.99928  and loss:  1.0525986991415266
test acc:  0.9183
forward train acc:  0.9995  and loss:  0.7104702177457511
test acc:  0.9196
forward train acc:  0.9994  and loss:  0.7956677988404408
test acc:  0.9187
forward train acc:  0.99944  and loss:  0.7712542868976016
test acc:  0.9194
********** reverse layer  6  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5104166666666666  ==>  188 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.9583333333333334  ==>  32 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.671875  ==>  252 / 768
layer  11  :  0.7135416666666666  ==>  220 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
***** skip layer  7
[5, 5, 5, 0, 1, 0, 5, 0, 0, 0, 0, 0, 3, 1]
optimize layer  8
backward train epoch:  47
test acc:  0.0997
forward train acc:  0.99882  and loss:  1.562096756446408
test acc:  0.9157
forward train acc:  0.99904  and loss:  1.25764799801982
test acc:  0.917
forward train acc:  0.9987  and loss:  1.7280612628383096
test acc:  0.9193
forward train acc:  0.99934  and loss:  1.0816559003433213
test acc:  0.9192
forward train acc:  0.99894  and loss:  1.207883382186992
test acc:  0.9173
forward train acc:  0.9991  and loss:  1.1276124595897272
test acc:  0.9197
forward train acc:  0.99926  and loss:  1.0141589629056398
test acc:  0.9203
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5104166666666666  ==>  188 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.578125  ==>  324 / 768
layer  10  :  0.671875  ==>  252 / 768
layer  11  :  0.7135416666666666  ==>  220 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
optimize layer  9
backward train epoch:  101
test acc:  0.0966
forward train acc:  0.99924  and loss:  1.1438502945529763
test acc:  0.9191
forward train acc:  0.99914  and loss:  1.2125180051079951
test acc:  0.9207
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5104166666666666  ==>  188 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.5859375  ==>  318 / 768
layer  10  :  0.671875  ==>  252 / 768
layer  11  :  0.7135416666666666  ==>  220 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
optimize layer  10
backward train epoch:  147
test acc:  0.0998
forward train acc:  0.99936  and loss:  0.9526335700356867
test acc:  0.9199
forward train acc:  0.9992  and loss:  1.2029720807331614
test acc:  0.9192
forward train acc:  0.9989  and loss:  1.2186121501144953
test acc:  0.9176
forward train acc:  0.99906  and loss:  1.0603024017764255
test acc:  0.9188
forward train acc:  0.99932  and loss:  0.9204026429506484
test acc:  0.9187
forward train acc:  0.99912  and loss:  1.1089425643149298
test acc:  0.9195
forward train acc:  0.99944  and loss:  0.714313907927135
test acc:  0.921
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5104166666666666  ==>  188 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.5859375  ==>  318 / 768
layer  10  :  0.6796875  ==>  246 / 768
layer  11  :  0.7135416666666666  ==>  220 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
optimize layer  11
backward train epoch:  11
test acc:  0.3538
forward train acc:  0.99922  and loss:  0.9824561021232512
test acc:  0.9207
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4479166666666667  ==>  106 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5104166666666666  ==>  188 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.5859375  ==>  318 / 768
layer  10  :  0.6796875  ==>  246 / 768
layer  11  :  0.7213541666666666  ==>  214 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
***** skip layer  12
[5, 5, 5, 0, 1, 0, 5, 0, 0, 0, 0, 0, 2, 1]
***** skip layer  13
[5, 5, 5, 0, 1, 0, 5, 0, 0, 0, 0, 0, 2, 0]
***** skip layer  0
[4, 5, 5, 0, 1, 0, 5, 0, 0, 0, 0, 0, 2, 0]
***** skip layer  1
[4, 4, 5, 0, 1, 0, 5, 0, 0, 0, 0, 0, 2, 0]
***** skip layer  2
[4, 4, 4, 0, 1, 0, 5, 0, 0, 0, 0, 0, 2, 0]
optimize layer  3
backward train epoch:  4
test acc:  0.7778
forward train acc:  0.99864  and loss:  1.5798893402097747
test acc:  0.9209
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4583333333333333  ==>  104 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5104166666666666  ==>  188 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.5859375  ==>  318 / 768
layer  10  :  0.6796875  ==>  246 / 768
layer  11  :  0.7213541666666666  ==>  214 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
***** skip layer  4
[4, 4, 4, 0, 0, 0, 5, 0, 0, 0, 0, 0, 2, 0]
optimize layer  5
backward train epoch:  39
test acc:  0.0968
forward train acc:  0.99838  and loss:  2.125279719795799
test acc:  0.9194
forward train acc:  0.99836  and loss:  1.9067478272190783
test acc:  0.918
forward train acc:  0.99848  and loss:  1.9171546862344258
test acc:  0.9177
forward train acc:  0.99902  and loss:  1.3174799519765656
test acc:  0.9183
forward train acc:  0.99916  and loss:  1.0878592761582695
test acc:  0.9181
forward train acc:  0.99904  and loss:  1.115739714470692
test acc:  0.9167
forward train acc:  0.99886  and loss:  1.2664186507172417
test acc:  0.9194
forward train acc:  0.99904  and loss:  1.1741422449413221
test acc:  0.9199
forward train acc:  0.99928  and loss:  1.0583851860719733
test acc:  0.9189
forward train acc:  0.9993  and loss:  0.774732389691053
test acc:  0.9204
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4583333333333333  ==>  104 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.5859375  ==>  318 / 768
layer  10  :  0.6796875  ==>  246 / 768
layer  11  :  0.7213541666666666  ==>  214 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
***** skip layer  6
[4, 4, 4, 0, 0, 0, 4, 0, 0, 0, 0, 0, 2, 0]
optimize layer  7
backward train epoch:  68
test acc:  0.1011
forward train acc:  0.9989  and loss:  1.4584922364447266
test acc:  0.9172
forward train acc:  0.99852  and loss:  1.845281265952508
test acc:  0.9157
forward train acc:  0.9988  and loss:  1.6235977824544534
test acc:  0.9168
forward train acc:  0.99884  and loss:  1.4584142001112923
test acc:  0.9161
forward train acc:  0.99916  and loss:  1.0770807380322367
test acc:  0.9167
forward train acc:  0.99916  and loss:  1.0395865222089924
test acc:  0.9168
forward train acc:  0.99906  and loss:  1.3131407148612197
test acc:  0.916
forward train acc:  0.9992  and loss:  1.0138644464022946
test acc:  0.9174
forward train acc:  0.99908  and loss:  1.1481970171735156
test acc:  0.9184
forward train acc:  0.9992  and loss:  1.1295094437082298
test acc:  0.9182
********** reverse layer  7  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4583333333333333  ==>  104 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.5859375  ==>  318 / 768
layer  10  :  0.6796875  ==>  246 / 768
layer  11  :  0.7213541666666666  ==>  214 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
optimize layer  8
backward train epoch:  175
test acc:  0.0969
forward train acc:  0.99688  and loss:  3.9462075384217314
test acc:  0.9162
forward train acc:  0.99818  and loss:  2.3188983659201767
test acc:  0.9171
forward train acc:  0.99784  and loss:  2.611730050324695
test acc:  0.914
forward train acc:  0.9986  and loss:  1.7324590741773136
test acc:  0.9156
forward train acc:  0.99872  and loss:  1.582741456280928
test acc:  0.9155
forward train acc:  0.9989  and loss:  1.4265541128988843
test acc:  0.9166
forward train acc:  0.99912  and loss:  1.25108274325612
test acc:  0.918
forward train acc:  0.99908  and loss:  1.2860918694059364
test acc:  0.9178
forward train acc:  0.99916  and loss:  1.138729427329963
test acc:  0.919
forward train acc:  0.9991  and loss:  0.9604012004565448
test acc:  0.9187
********** reverse layer  8  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4583333333333333  ==>  104 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.5859375  ==>  318 / 768
layer  10  :  0.6796875  ==>  246 / 768
layer  11  :  0.7213541666666666  ==>  214 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
optimize layer  9
backward train epoch:  25
test acc:  0.127
forward train acc:  0.9993  and loss:  1.084085586539004
test acc:  0.9193
forward train acc:  0.99904  and loss:  1.4706255315977614
test acc:  0.9174
forward train acc:  0.99884  and loss:  1.5153271265153307
test acc:  0.9197
forward train acc:  0.99938  and loss:  0.8638250009971671
test acc:  0.9194
forward train acc:  0.9995  and loss:  0.7012875929212896
test acc:  0.9194
forward train acc:  0.9993  and loss:  0.8240047008148395
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4583333333333333  ==>  104 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6796875  ==>  246 / 768
layer  11  :  0.7213541666666666  ==>  214 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
optimize layer  10
backward train epoch:  157
test acc:  0.1048
forward train acc:  0.99914  and loss:  1.139760325255338
test acc:  0.9204
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4583333333333333  ==>  104 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7213541666666666  ==>  214 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
optimize layer  11
backward train epoch:  215
test acc:  0.106
forward train acc:  0.9989  and loss:  1.4547558863996528
test acc:  0.9174
forward train acc:  0.99924  and loss:  0.9912019921466708
test acc:  0.919
forward train acc:  0.99938  and loss:  0.78223520971369
test acc:  0.9185
forward train acc:  0.99944  and loss:  0.8339034245873336
test acc:  0.9213
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4583333333333333  ==>  104 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7291666666666666  ==>  208 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5286458333333334  ==>  362 / 768
***** skip layer  12
[4, 4, 4, 0, 0, 0, 4, 5, 5, 0, 0, 0, 1, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.0996
forward train acc:  0.99924  and loss:  1.2307767611637246
test acc:  0.9195
forward train acc:  0.99912  and loss:  1.1092178887338378
test acc:  0.9213
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4583333333333333  ==>  104 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7291666666666666  ==>  208 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5364583333333334  ==>  356 / 768
***** skip layer  0
[3, 4, 4, 0, 0, 0, 4, 5, 5, 0, 0, 0, 1, 0]
***** skip layer  1
[3, 3, 4, 0, 0, 0, 4, 5, 5, 0, 0, 0, 1, 0]
***** skip layer  2
[3, 3, 3, 0, 0, 0, 4, 5, 5, 0, 0, 0, 1, 0]
optimize layer  3
backward train epoch:  108
test acc:  0.0972
forward train acc:  0.99888  and loss:  1.4443645158025902
test acc:  0.9183
forward train acc:  0.99862  and loss:  1.4510181801451836
test acc:  0.9198
forward train acc:  0.99872  and loss:  1.8937792736978736
test acc:  0.9188
forward train acc:  0.99898  and loss:  1.3959126198315062
test acc:  0.9203
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7291666666666666  ==>  208 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5364583333333334  ==>  356 / 768
optimize layer  4
backward train epoch:  109
test acc:  0.1023
forward train acc:  0.99854  and loss:  1.6881574550352525
test acc:  0.917
forward train acc:  0.99872  and loss:  1.5872818078496493
test acc:  0.9168
forward train acc:  0.99884  and loss:  1.538158157170983
test acc:  0.9183
forward train acc:  0.99876  and loss:  1.4638843327993527
test acc:  0.9175
forward train acc:  0.99896  and loss:  1.1600714772648644
test acc:  0.9179
forward train acc:  0.99902  and loss:  1.4406982555228751
test acc:  0.9185
forward train acc:  0.99902  and loss:  1.0691606762411539
test acc:  0.9187
forward train acc:  0.99942  and loss:  0.7722515526402276
test acc:  0.9191
forward train acc:  0.99898  and loss:  1.3335557474929374
test acc:  0.9197
forward train acc:  0.99912  and loss:  1.0367316862830194
test acc:  0.9177
********** reverse layer  4  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7291666666666666  ==>  208 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5364583333333334  ==>  356 / 768
optimize layer  5
backward train epoch:  99
test acc:  0.1335
forward train acc:  0.99888  and loss:  1.4244961254007649
test acc:  0.9174
forward train acc:  0.9987  and loss:  1.4641666660318151
test acc:  0.9181
forward train acc:  0.9988  and loss:  1.747263801196823
test acc:  0.9157
forward train acc:  0.99914  and loss:  1.1605816938099451
test acc:  0.9159
forward train acc:  0.99918  and loss:  1.131958019657759
test acc:  0.9173
forward train acc:  0.99922  and loss:  0.7865915845904965
test acc:  0.9165
forward train acc:  0.99946  and loss:  0.7864242091309279
test acc:  0.9174
forward train acc:  0.9995  and loss:  0.7149888544518035
test acc:  0.9187
forward train acc:  0.99932  and loss:  0.8698985014634673
test acc:  0.9171
forward train acc:  0.9994  and loss:  0.7573270923167001
test acc:  0.9179
********** reverse layer  5  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7291666666666666  ==>  208 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5364583333333334  ==>  356 / 768
***** skip layer  6
[3, 3, 3, 0, 5, 5, 3, 5, 5, 0, 0, 0, 1, 0]
***** skip layer  7
[3, 3, 3, 0, 5, 5, 3, 4, 5, 0, 0, 0, 1, 0]
***** skip layer  8
[3, 3, 3, 0, 5, 5, 3, 4, 4, 0, 0, 0, 1, 0]
optimize layer  9
backward train epoch:  264
test acc:  0.1113
forward train acc:  0.99928  and loss:  0.8620067666488467
test acc:  0.9186
forward train acc:  0.99916  and loss:  0.9396993778063916
test acc:  0.9183
forward train acc:  0.9992  and loss:  0.9735993324429728
test acc:  0.9189
forward train acc:  0.99926  and loss:  0.9938649960095063
test acc:  0.9196
forward train acc:  0.9993  and loss:  0.7758333743258845
test acc:  0.9188
forward train acc:  0.99944  and loss:  0.7880406569165643
test acc:  0.9183
forward train acc:  0.99956  and loss:  0.6348499372543301
test acc:  0.9181
forward train acc:  0.99932  and loss:  0.7894864817062626
test acc:  0.9183
forward train acc:  0.99958  and loss:  0.5456280864164
test acc:  0.9194
forward train acc:  0.99968  and loss:  0.45199684643012006
test acc:  0.919
********** reverse layer  9  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7291666666666666  ==>  208 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5364583333333334  ==>  356 / 768
optimize layer  10
backward train epoch:  51
test acc:  0.1063
forward train acc:  0.99914  and loss:  0.866996417476912
test acc:  0.919
forward train acc:  0.9991  and loss:  1.0579174827289535
test acc:  0.9183
forward train acc:  0.99924  and loss:  1.0642799505149014
test acc:  0.9174
forward train acc:  0.99946  and loss:  0.6516579686140176
test acc:  0.9186
forward train acc:  0.99946  and loss:  0.6738619398965966
test acc:  0.9182
forward train acc:  0.99914  and loss:  0.830131285751122
test acc:  0.9189
forward train acc:  0.99948  and loss:  0.7068872799864039
test acc:  0.9189
forward train acc:  0.99956  and loss:  0.6594490076240618
test acc:  0.918
forward train acc:  0.99964  and loss:  0.4874582745542284
test acc:  0.9185
forward train acc:  0.99962  and loss:  0.42843358565005474
test acc:  0.9196
********** reverse layer  10  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7291666666666666  ==>  208 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5364583333333334  ==>  356 / 768
optimize layer  11
backward train epoch:  190
test acc:  0.0936
forward train acc:  0.99932  and loss:  0.8291857216972858
test acc:  0.9212
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5364583333333334  ==>  356 / 768
***** skip layer  12
[3, 3, 3, 0, 5, 5, 3, 4, 4, 5, 5, 0, 0, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.0976
forward train acc:  0.99934  and loss:  0.8480030538630672
test acc:  0.9191
forward train acc:  0.99934  and loss:  0.837599393009441
test acc:  0.9188
forward train acc:  0.99922  and loss:  0.9600554253847804
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5442708333333334  ==>  350 / 768
***** skip layer  0
[2, 3, 3, 0, 5, 5, 3, 4, 4, 5, 5, 0, 0, 0]
***** skip layer  1
[2, 2, 3, 0, 5, 5, 3, 4, 4, 5, 5, 0, 0, 0]
***** skip layer  2
[2, 2, 2, 0, 5, 5, 3, 4, 4, 5, 5, 0, 0, 0]
optimize layer  3
backward train epoch:  190
test acc:  0.1234
forward train acc:  0.99882  and loss:  1.5103750962298363
test acc:  0.9152
forward train acc:  0.99894  and loss:  1.4743129722774029
test acc:  0.9151
forward train acc:  0.99856  and loss:  1.7410773150622845
test acc:  0.9181
forward train acc:  0.99914  and loss:  1.224726873479085
test acc:  0.9174
forward train acc:  0.9994  and loss:  0.8816807500843424
test acc:  0.9187
forward train acc:  0.99938  and loss:  0.8696301709278487
test acc:  0.9181
forward train acc:  0.99932  and loss:  0.8449521627626382
test acc:  0.9185
forward train acc:  0.99948  and loss:  0.8530004924396053
test acc:  0.9179
forward train acc:  0.99936  and loss:  0.9639653573103715
test acc:  0.9193
forward train acc:  0.99966  and loss:  0.552595207671402
test acc:  0.9188
********** reverse layer  3  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5442708333333334  ==>  350 / 768
***** skip layer  4
[2, 2, 2, 5, 4, 5, 3, 4, 4, 5, 5, 0, 0, 0]
***** skip layer  5
[2, 2, 2, 5, 4, 4, 3, 4, 4, 5, 5, 0, 0, 0]
***** skip layer  6
[2, 2, 2, 5, 4, 4, 2, 4, 4, 5, 5, 0, 0, 0]
***** skip layer  7
[2, 2, 2, 5, 4, 4, 2, 3, 4, 5, 5, 0, 0, 0]
***** skip layer  8
[2, 2, 2, 5, 4, 4, 2, 3, 3, 5, 5, 0, 0, 0]
***** skip layer  9
[2, 2, 2, 5, 4, 4, 2, 3, 3, 4, 5, 0, 0, 0]
***** skip layer  10
[2, 2, 2, 5, 4, 4, 2, 3, 3, 4, 4, 0, 0, 0]
optimize layer  11
backward train epoch:  32
test acc:  0.1058
forward train acc:  0.99932  and loss:  0.9172790662560146
test acc:  0.918
forward train acc:  0.99938  and loss:  0.7742297207150841
test acc:  0.9177
forward train acc:  0.9993  and loss:  0.9300736974109896
test acc:  0.9182
forward train acc:  0.99928  and loss:  0.9200973215629347
test acc:  0.9194
forward train acc:  0.99942  and loss:  0.8053976118098944
test acc:  0.9179
forward train acc:  0.99942  and loss:  0.7038143086538184
test acc:  0.9184
forward train acc:  0.99944  and loss:  0.727322898193961
test acc:  0.9176
forward train acc:  0.99966  and loss:  0.4592493350501172
test acc:  0.9184
forward train acc:  0.99968  and loss:  0.5442544619145337
test acc:  0.9176
forward train acc:  0.99974  and loss:  0.45720630782307126
test acc:  0.9192
********** reverse layer  11  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5442708333333334  ==>  350 / 768
optimize layer  12
backward train epoch:  138
test acc:  0.0823
forward train acc:  0.91422  and loss:  154.27490982413292
test acc:  0.9054
forward train acc:  0.99472  and loss:  117.03124335408211
test acc:  0.9095
forward train acc:  0.99866  and loss:  97.39450564980507
test acc:  0.9125
forward train acc:  0.9992  and loss:  85.63293215632439
test acc:  0.9137
forward train acc:  0.99954  and loss:  78.49263526499271
test acc:  0.9148
forward train acc:  0.99954  and loss:  72.51197902858257
test acc:  0.9173
forward train acc:  0.99952  and loss:  67.07230135053396
test acc:  0.9175
forward train acc:  0.99964  and loss:  63.25966552644968
test acc:  0.9183
forward train acc:  0.99938  and loss:  61.00681205838919
test acc:  0.9187
forward train acc:  0.99964  and loss:  58.271208956837654
test acc:  0.9171
********** reverse layer  12  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5442708333333334  ==>  350 / 768
optimize layer  13
backward train epoch:  102
test acc:  0.1006
forward train acc:  0.99968  and loss:  0.6264838764764136
test acc:  0.918
forward train acc:  0.99926  and loss:  1.2335814269899856
test acc:  0.9171
forward train acc:  0.99926  and loss:  1.343900979263708
test acc:  0.9179
forward train acc:  0.99958  and loss:  0.7208102064032573
test acc:  0.9192
forward train acc:  0.99934  and loss:  0.8248362088779686
test acc:  0.9185
forward train acc:  0.99966  and loss:  0.5037686252035201
test acc:  0.9184
forward train acc:  0.9995  and loss:  0.7007727006566711
test acc:  0.9164
forward train acc:  0.9994  and loss:  0.8225866005232092
test acc:  0.9193
forward train acc:  0.99976  and loss:  0.3763515596219804
test acc:  0.9198
forward train acc:  0.99956  and loss:  0.5577420895715477
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5520833333333334  ==>  344 / 768
***** skip layer  0
[1, 2, 2, 5, 4, 4, 2, 3, 3, 4, 4, 5, 5, 0]
***** skip layer  1
[1, 1, 2, 5, 4, 4, 2, 3, 3, 4, 4, 5, 5, 0]
***** skip layer  2
[1, 1, 1, 5, 4, 4, 2, 3, 3, 4, 4, 5, 5, 0]
***** skip layer  3
[1, 1, 1, 4, 4, 4, 2, 3, 3, 4, 4, 5, 5, 0]
***** skip layer  4
[1, 1, 1, 4, 3, 4, 2, 3, 3, 4, 4, 5, 5, 0]
***** skip layer  5
[1, 1, 1, 4, 3, 3, 2, 3, 3, 4, 4, 5, 5, 0]
***** skip layer  6
[1, 1, 1, 4, 3, 3, 1, 3, 3, 4, 4, 5, 5, 0]
***** skip layer  7
[1, 1, 1, 4, 3, 3, 1, 2, 3, 4, 4, 5, 5, 0]
***** skip layer  8
[1, 1, 1, 4, 3, 3, 1, 2, 2, 4, 4, 5, 5, 0]
***** skip layer  9
[1, 1, 1, 4, 3, 3, 1, 2, 2, 3, 4, 5, 5, 0]
***** skip layer  10
[1, 1, 1, 4, 3, 3, 1, 2, 2, 3, 3, 5, 5, 0]
***** skip layer  11
[1, 1, 1, 4, 3, 3, 1, 2, 2, 3, 3, 4, 5, 0]
***** skip layer  12
[1, 1, 1, 4, 3, 3, 1, 2, 2, 3, 3, 4, 4, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.1007
forward train acc:  0.99962  and loss:  0.6198551987763494
test acc:  0.9205
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5598958333333334  ==>  338 / 768
***** skip layer  0
[0, 1, 1, 4, 3, 3, 1, 2, 2, 3, 3, 4, 4, 0]
***** skip layer  1
[0, 0, 1, 4, 3, 3, 1, 2, 2, 3, 3, 4, 4, 0]
***** skip layer  2
[0, 0, 0, 4, 3, 3, 1, 2, 2, 3, 3, 4, 4, 0]
***** skip layer  3
[0, 0, 0, 3, 3, 3, 1, 2, 2, 3, 3, 4, 4, 0]
***** skip layer  4
[0, 0, 0, 3, 2, 3, 1, 2, 2, 3, 3, 4, 4, 0]
***** skip layer  5
[0, 0, 0, 3, 2, 2, 1, 2, 2, 3, 3, 4, 4, 0]
***** skip layer  6
[0, 0, 0, 3, 2, 2, 0, 2, 2, 3, 3, 4, 4, 0]
***** skip layer  7
[0, 0, 0, 3, 2, 2, 0, 1, 2, 3, 3, 4, 4, 0]
***** skip layer  8
[0, 0, 0, 3, 2, 2, 0, 1, 1, 3, 3, 4, 4, 0]
***** skip layer  9
[0, 0, 0, 3, 2, 2, 0, 1, 1, 2, 3, 4, 4, 0]
***** skip layer  10
[0, 0, 0, 3, 2, 2, 0, 1, 1, 2, 2, 4, 4, 0]
***** skip layer  11
[0, 0, 0, 3, 2, 2, 0, 1, 1, 2, 2, 3, 4, 0]
***** skip layer  12
[0, 0, 0, 3, 2, 2, 0, 1, 1, 2, 2, 3, 3, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.0997
forward train acc:  0.99942  and loss:  0.7275098033424001
test acc:  0.9182
forward train acc:  0.99946  and loss:  0.8525544344010996
test acc:  0.9209
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5677083333333334  ==>  332 / 768
optimize layer  0
backward train epoch:  225
test acc:  0.0975
forward train acc:  0.78172  and loss:  367.1515364944935
test acc:  0.8053
forward train acc:  0.86034  and loss:  178.45362421870232
test acc:  0.8376
forward train acc:  0.89648  and loss:  129.2073041945696
test acc:  0.8567
forward train acc:  0.91602  and loss:  101.24618165194988
test acc:  0.8639
forward train acc:  0.92654  and loss:  88.75715477764606
test acc:  0.8705
forward train acc:  0.93514  and loss:  76.95507925748825
test acc:  0.8755
forward train acc:  0.94236  and loss:  68.47805053368211
test acc:  0.877
forward train acc:  0.94718  and loss:  61.83468199521303
test acc:  0.8789
forward train acc:  0.95122  and loss:  57.95343096554279
test acc:  0.8799
forward train acc:  0.95346  and loss:  54.10535967722535
test acc:  0.8829
********** reverse layer  0  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5208333333333334  ==>  46 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5677083333333334  ==>  332 / 768
optimize layer  1
backward train epoch:  19
test acc:  0.1069
forward train acc:  0.99432  and loss:  7.4242925582802854
test acc:  0.9172
forward train acc:  0.99752  and loss:  2.985089998401236
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5677083333333334  ==>  332 / 768
optimize layer  2
backward train epoch:  309
test acc:  0.1014
forward train acc:  0.95024  and loss:  64.43190042302012
test acc:  0.8923
forward train acc:  0.96962  and loss:  34.72386956587434
test acc:  0.8992
forward train acc:  0.97946  and loss:  23.72347469907254
test acc:  0.9019
forward train acc:  0.9845  and loss:  18.384815853787586
test acc:  0.9042
forward train acc:  0.98656  and loss:  16.369198638712987
test acc:  0.9066
forward train acc:  0.98848  and loss:  14.245372422039509
test acc:  0.9053
forward train acc:  0.98918  and loss:  13.247988084563985
test acc:  0.9069
forward train acc:  0.98982  and loss:  11.514332635095343
test acc:  0.9072
forward train acc:  0.99178  and loss:  10.281151079456322
test acc:  0.9082
forward train acc:  0.99206  and loss:  9.342258521122858
test acc:  0.9087
********** reverse layer  2  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.5208333333333334  ==>  184 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5677083333333334  ==>  332 / 768
***** skip layer  3
[5, 0, 5, 2, 2, 2, 0, 1, 1, 2, 2, 3, 3, 0]
***** skip layer  4
[5, 0, 5, 2, 1, 2, 0, 1, 1, 2, 2, 3, 3, 0]
***** skip layer  5
[5, 0, 5, 2, 1, 1, 0, 1, 1, 2, 2, 3, 3, 0]
optimize layer  6
backward train epoch:  118
test acc:  0.108
forward train acc:  0.99816  and loss:  2.27567245363025
test acc:  0.9198
forward train acc:  0.99888  and loss:  1.613453010621015
test acc:  0.9204
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.53125  ==>  180 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5677083333333334  ==>  332 / 768
***** skip layer  7
[5, 0, 5, 2, 1, 1, 0, 0, 1, 2, 2, 3, 3, 0]
***** skip layer  8
[5, 0, 5, 2, 1, 1, 0, 0, 0, 2, 2, 3, 3, 0]
***** skip layer  9
[5, 0, 5, 2, 1, 1, 0, 0, 0, 1, 2, 3, 3, 0]
***** skip layer  10
[5, 0, 5, 2, 1, 1, 0, 0, 0, 1, 1, 3, 3, 0]
***** skip layer  11
[5, 0, 5, 2, 1, 1, 0, 0, 0, 1, 1, 2, 3, 0]
***** skip layer  12
[5, 0, 5, 2, 1, 1, 0, 0, 0, 1, 1, 2, 2, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.1061
forward train acc:  0.99894  and loss:  1.396790264203446
test acc:  0.9215
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.53125  ==>  180 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5755208333333334  ==>  326 / 768
***** skip layer  0
[4, 0, 5, 2, 1, 1, 0, 0, 0, 1, 1, 2, 2, 0]
optimize layer  1
backward train epoch:  469
test acc:  0.1081
forward train acc:  0.98178  and loss:  25.231787076219916
test acc:  0.9042
forward train acc:  0.99002  and loss:  11.95931283547543
test acc:  0.9077
forward train acc:  0.99344  and loss:  8.271436628536321
test acc:  0.9087
forward train acc:  0.99454  and loss:  6.233010007010307
test acc:  0.9095
forward train acc:  0.99534  and loss:  5.6889004211407155
test acc:  0.9101
forward train acc:  0.99604  and loss:  4.850366557715461
test acc:  0.9113
forward train acc:  0.9966  and loss:  4.408917577937245
test acc:  0.9104
forward train acc:  0.99654  and loss:  4.259675401553977
test acc:  0.9117
forward train acc:  0.99674  and loss:  4.1199508555582725
test acc:  0.9116
forward train acc:  0.9972  and loss:  3.3090132115758024
test acc:  0.9116
********** reverse layer  1  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.53125  ==>  180 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5755208333333334  ==>  326 / 768
***** skip layer  2
[4, 5, 4, 2, 1, 1, 0, 0, 0, 1, 1, 2, 2, 0]
***** skip layer  3
[4, 5, 4, 1, 1, 1, 0, 0, 0, 1, 1, 2, 2, 0]
***** skip layer  4
[4, 5, 4, 1, 0, 1, 0, 0, 0, 1, 1, 2, 2, 0]
***** skip layer  5
[4, 5, 4, 1, 0, 0, 0, 0, 0, 1, 1, 2, 2, 0]
optimize layer  6
backward train epoch:  84
test acc:  0.1117
forward train acc:  0.99818  and loss:  2.203146157262381
test acc:  0.918
forward train acc:  0.99904  and loss:  1.2711580981849693
test acc:  0.9188
forward train acc:  0.9992  and loss:  1.0860882814740762
test acc:  0.9196
forward train acc:  0.99898  and loss:  1.2163291678007226
test acc:  0.9204
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.5416666666666666  ==>  176 / 384
layer  7  :  0.8854166666666666  ==>  88 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5755208333333334  ==>  326 / 768
optimize layer  7
backward train epoch:  208
test acc:  0.1104
forward train acc:  0.99916  and loss:  1.3702120448288042
test acc:  0.9199
forward train acc:  0.99898  and loss:  1.201585663293372
test acc:  0.9204
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.5416666666666666  ==>  176 / 384
layer  7  :  0.8958333333333334  ==>  80 / 768
layer  8  :  0.96875  ==>  24 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5755208333333334  ==>  326 / 768
optimize layer  8
backward train epoch:  119
test acc:  0.1102
forward train acc:  0.99758  and loss:  3.136873538431246
test acc:  0.9192
forward train acc:  0.99846  and loss:  1.9584093671874143
test acc:  0.9189
forward train acc:  0.99854  and loss:  1.8942967155890074
test acc:  0.9173
forward train acc:  0.99874  and loss:  1.6691595915181097
test acc:  0.9202
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.5416666666666666  ==>  176 / 384
layer  7  :  0.8958333333333334  ==>  80 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5755208333333334  ==>  326 / 768
***** skip layer  9
[4, 5, 4, 1, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0]
***** skip layer  10
[4, 5, 4, 1, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0]
***** skip layer  11
[4, 5, 4, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0]
***** skip layer  12
[4, 5, 4, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.1054
forward train acc:  0.99902  and loss:  1.2769620005128672
test acc:  0.919
forward train acc:  0.99886  and loss:  1.3240586240426637
test acc:  0.9184
forward train acc:  0.9989  and loss:  1.2596006393723655
test acc:  0.9191
forward train acc:  0.99858  and loss:  1.692586798715638
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5416666666666666  ==>  176 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.5416666666666666  ==>  176 / 384
layer  7  :  0.8958333333333334  ==>  80 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5833333333333334  ==>  320 / 768
***** skip layer  0
[3, 5, 4, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]
***** skip layer  1
[3, 4, 4, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]
***** skip layer  2
[3, 4, 3, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]
***** skip layer  3
[3, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]
optimize layer  4
backward train epoch:  179
test acc:  0.0994
forward train acc:  0.99824  and loss:  2.4509301356156357
test acc:  0.9191
forward train acc:  0.9981  and loss:  2.3929770565009676
test acc:  0.9184
forward train acc:  0.9984  and loss:  2.0573859369033016
test acc:  0.9208
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.5208333333333334  ==>  184 / 384
layer  6  :  0.5416666666666666  ==>  176 / 384
layer  7  :  0.8958333333333334  ==>  80 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5833333333333334  ==>  320 / 768
optimize layer  5
backward train epoch:  357
test acc:  0.1306
forward train acc:  0.99802  and loss:  2.468643497413723
test acc:  0.9181
forward train acc:  0.99846  and loss:  1.943834848323604
test acc:  0.9183
forward train acc:  0.99868  and loss:  1.824407908396097
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5416666666666666  ==>  176 / 384
layer  7  :  0.8958333333333334  ==>  80 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5833333333333334  ==>  320 / 768
optimize layer  6
backward train epoch:  73
test acc:  0.1081
forward train acc:  0.99826  and loss:  2.2039041899552103
test acc:  0.9165
forward train acc:  0.99862  and loss:  1.7698061145783868
test acc:  0.9172
forward train acc:  0.99856  and loss:  1.8508199763018638
test acc:  0.9149
forward train acc:  0.99886  and loss:  1.497592388506746
test acc:  0.9175
forward train acc:  0.9986  and loss:  1.614579037297517
test acc:  0.919
forward train acc:  0.99916  and loss:  1.2508564699382987
test acc:  0.9194
forward train acc:  0.9992  and loss:  0.979068365006242
test acc:  0.9194
forward train acc:  0.99908  and loss:  1.009089889252209
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.8958333333333334  ==>  80 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5833333333333334  ==>  320 / 768
optimize layer  7
backward train epoch:  193
test acc:  0.1168
forward train acc:  0.99864  and loss:  1.7214511520869564
test acc:  0.9178
forward train acc:  0.9984  and loss:  2.1378785069973674
test acc:  0.9174
forward train acc:  0.9985  and loss:  1.8228749581321608
test acc:  0.9194
forward train acc:  0.99906  and loss:  1.2726567809586413
test acc:  0.9204
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5833333333333334  ==>  320 / 768
optimize layer  8
backward train epoch:  314
test acc:  0.1277
forward train acc:  0.98866  and loss:  14.239703476545401
test acc:  0.9114
forward train acc:  0.99496  and loss:  6.442561802105047
test acc:  0.9142
forward train acc:  0.9962  and loss:  4.777129680092912
test acc:  0.9131
forward train acc:  0.99684  and loss:  3.8792221445473842
test acc:  0.9162
forward train acc:  0.99742  and loss:  3.3595496646594256
test acc:  0.9156
forward train acc:  0.9978  and loss:  2.8434451502980664
test acc:  0.9161
forward train acc:  0.99754  and loss:  3.01945882756263
test acc:  0.9149
forward train acc:  0.9978  and loss:  2.7877665584674105
test acc:  0.9173
forward train acc:  0.99852  and loss:  2.068094798945822
test acc:  0.9174
forward train acc:  0.9982  and loss:  2.137264112068806
test acc:  0.9174
********** reverse layer  8  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.59375  ==>  312 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5833333333333334  ==>  320 / 768
optimize layer  9
backward train epoch:  49
test acc:  0.1125
forward train acc:  0.9989  and loss:  1.3119739164540078
test acc:  0.9216
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6015625  ==>  306 / 768
layer  10  :  0.6875  ==>  240 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5833333333333334  ==>  320 / 768
optimize layer  10
backward train epoch:  23
test acc:  0.1343
forward train acc:  0.99872  and loss:  1.6912425672635436
test acc:  0.9198
forward train acc:  0.99892  and loss:  1.5786764866206795
test acc:  0.9188
forward train acc:  0.99904  and loss:  1.224556427274365
test acc:  0.9205
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6015625  ==>  306 / 768
layer  10  :  0.6953125  ==>  234 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5833333333333334  ==>  320 / 768
***** skip layer  11
[3, 4, 3, 0, 0, 0, 0, 0, 5, 0, 0, 0, 1, 0]
***** skip layer  12
[3, 4, 3, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.1109
forward train acc:  0.99898  and loss:  1.6862634695135057
test acc:  0.9193
forward train acc:  0.9989  and loss:  1.7335224163834937
test acc:  0.9182
forward train acc:  0.99922  and loss:  1.435491532785818
test acc:  0.9188
forward train acc:  0.99924  and loss:  1.3514253760222346
test acc:  0.9203
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.46875  ==>  102 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6015625  ==>  306 / 768
layer  10  :  0.6953125  ==>  234 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5911458333333334  ==>  314 / 768
***** skip layer  0
[2, 4, 3, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0]
***** skip layer  1
[2, 3, 3, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0]
***** skip layer  2
[2, 3, 2, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0]
optimize layer  3
backward train epoch:  2
test acc:  0.8759
forward train acc:  0.9984  and loss:  2.2814321164623834
test acc:  0.9161
forward train acc:  0.99828  and loss:  2.4130447940551676
test acc:  0.9182
forward train acc:  0.9987  and loss:  1.9958324763574637
test acc:  0.9196
forward train acc:  0.99898  and loss:  1.5132705337018706
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4791666666666667  ==>  100 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6015625  ==>  306 / 768
layer  10  :  0.6953125  ==>  234 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5911458333333334  ==>  314 / 768
optimize layer  4
backward train epoch:  29
test acc:  0.1201
forward train acc:  0.99852  and loss:  2.25133430986898
test acc:  0.9168
forward train acc:  0.99842  and loss:  2.222205276659224
test acc:  0.9184
forward train acc:  0.99844  and loss:  2.103911340527702
test acc:  0.9193
forward train acc:  0.99858  and loss:  2.030135021021124
test acc:  0.9197
forward train acc:  0.99872  and loss:  1.8841336678015068
test acc:  0.9172
forward train acc:  0.99896  and loss:  1.5375042966916226
test acc:  0.9156
forward train acc:  0.99896  and loss:  1.641630131751299
test acc:  0.9179
forward train acc:  0.99884  and loss:  1.682164701691363
test acc:  0.9188
forward train acc:  0.9991  and loss:  1.3492173260892741
test acc:  0.918
forward train acc:  0.99924  and loss:  1.1305471342639066
test acc:  0.9178
********** reverse layer  4  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4791666666666667  ==>  100 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6015625  ==>  306 / 768
layer  10  :  0.6953125  ==>  234 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5911458333333334  ==>  314 / 768
optimize layer  5
backward train epoch:  136
test acc:  0.1182
forward train acc:  0.9985  and loss:  2.225171860307455
test acc:  0.9169
forward train acc:  0.9987  and loss:  1.8304190146154724
test acc:  0.9187
forward train acc:  0.99864  and loss:  1.8457141797989607
test acc:  0.9162
forward train acc:  0.99924  and loss:  1.1999909433652647
test acc:  0.9168
forward train acc:  0.99886  and loss:  1.4859681127709337
test acc:  0.9175
forward train acc:  0.99882  and loss:  1.5342540744459257
test acc:  0.9182
forward train acc:  0.99896  and loss:  1.3704133202554658
test acc:  0.9166
forward train acc:  0.99934  and loss:  1.0739856990985572
test acc:  0.9173
forward train acc:  0.99914  and loss:  1.0953415785334073
test acc:  0.9172
forward train acc:  0.99938  and loss:  0.9587407888029702
test acc:  0.919
********** reverse layer  5  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4791666666666667  ==>  100 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6015625  ==>  306 / 768
layer  10  :  0.6953125  ==>  234 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5911458333333334  ==>  314 / 768
optimize layer  6
backward train epoch:  94
test acc:  0.1068
forward train acc:  0.99896  and loss:  1.3909273485769518
test acc:  0.9182
forward train acc:  0.99898  and loss:  1.2910995165002532
test acc:  0.917
forward train acc:  0.99876  and loss:  1.475285758730024
test acc:  0.915
forward train acc:  0.99898  and loss:  1.5158563437580597
test acc:  0.9168
forward train acc:  0.99936  and loss:  1.109985340910498
test acc:  0.9185
forward train acc:  0.99938  and loss:  1.0166054943401832
test acc:  0.9195
forward train acc:  0.99908  and loss:  1.3449398438096978
test acc:  0.9196
forward train acc:  0.99952  and loss:  0.8580706600914709
test acc:  0.9194
forward train acc:  0.9995  and loss:  0.7267717347713187
test acc:  0.9192
forward train acc:  0.99948  and loss:  0.7762212370871566
test acc:  0.9197
********** reverse layer  6  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4791666666666667  ==>  100 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6015625  ==>  306 / 768
layer  10  :  0.6953125  ==>  234 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5911458333333334  ==>  314 / 768
optimize layer  7
backward train epoch:  7
test acc:  0.5606
forward train acc:  0.99878  and loss:  1.5813570241443813
test acc:  0.9176
forward train acc:  0.99864  and loss:  1.8599324452225119
test acc:  0.9168
forward train acc:  0.9988  and loss:  1.639451838156674
test acc:  0.9177
forward train acc:  0.99916  and loss:  1.4484823651437182
test acc:  0.9193
forward train acc:  0.99898  and loss:  1.2299918301578145
test acc:  0.9186
forward train acc:  0.9992  and loss:  1.0941657283401582
test acc:  0.9173
forward train acc:  0.99948  and loss:  0.8747374129598029
test acc:  0.9189
forward train acc:  0.99942  and loss:  0.9520981255918741
test acc:  0.9194
forward train acc:  0.99944  and loss:  0.8276731671649031
test acc:  0.9199
forward train acc:  0.99938  and loss:  0.8447589031420648
test acc:  0.919
********** reverse layer  7  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4791666666666667  ==>  100 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6015625  ==>  306 / 768
layer  10  :  0.6953125  ==>  234 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5911458333333334  ==>  314 / 768
***** skip layer  8
[2, 3, 2, 0, 5, 5, 5, 5, 4, 0, 0, 0, 0, 0]
optimize layer  9
backward train epoch:  110
test acc:  0.1369
forward train acc:  0.99904  and loss:  1.1136837625526823
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4791666666666667  ==>  100 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.609375  ==>  300 / 768
layer  10  :  0.6953125  ==>  234 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5911458333333334  ==>  314 / 768
optimize layer  10
backward train epoch:  157
test acc:  0.1323
forward train acc:  0.99936  and loss:  0.9517175322398543
test acc:  0.9182
forward train acc:  0.99926  and loss:  1.0814729615813121
test acc:  0.9186
forward train acc:  0.99916  and loss:  1.1469849715358578
test acc:  0.918
forward train acc:  0.99948  and loss:  0.8251870525709819
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4791666666666667  ==>  100 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.609375  ==>  300 / 768
layer  10  :  0.703125  ==>  228 / 768
layer  11  :  0.7369791666666666  ==>  202 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5911458333333334  ==>  314 / 768
optimize layer  11
backward train epoch:  40
test acc:  0.131
forward train acc:  0.9994  and loss:  0.9180960080993827
test acc:  0.9154
forward train acc:  0.99936  and loss:  0.8875987173232716
test acc:  0.9187
forward train acc:  0.99924  and loss:  0.9921274822845589
test acc:  0.9172
forward train acc:  0.9993  and loss:  1.0868992202158552
test acc:  0.9183
forward train acc:  0.99942  and loss:  0.768755891447654
test acc:  0.917
forward train acc:  0.99954  and loss:  0.6841993624402676
test acc:  0.9203
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4791666666666667  ==>  100 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.609375  ==>  300 / 768
layer  10  :  0.703125  ==>  228 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5911458333333334  ==>  314 / 768
optimize layer  12
backward train epoch:  242
test acc:  0.0871
forward train acc:  0.2409  and loss:  752.7060899734497
test acc:  0.2802
forward train acc:  0.29388  and loss:  614.5613297224045
test acc:  0.2858
forward train acc:  0.31264  and loss:  541.1282070875168
test acc:  0.3529
forward train acc:  0.35272  and loss:  491.85627460479736
test acc:  0.3596
forward train acc:  0.37868  and loss:  461.71539187431335
test acc:  0.3659
forward train acc:  0.399  and loss:  433.42589485645294
test acc:  0.367
forward train acc:  0.45362  and loss:  406.2597167491913
test acc:  0.4513
forward train acc:  0.48862  and loss:  386.60103899240494
test acc:  0.4554
forward train acc:  0.4987  and loss:  374.4876108765602
test acc:  0.4578
forward train acc:  0.51136  and loss:  361.8225032687187
test acc:  0.4592
********** reverse layer  12  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4791666666666667  ==>  100 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.609375  ==>  300 / 768
layer  10  :  0.703125  ==>  228 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5911458333333334  ==>  314 / 768
optimize layer  13
backward train epoch:  102
test acc:  0.1016
forward train acc:  0.99936  and loss:  1.2953677773475647
test acc:  0.9199
forward train acc:  0.99914  and loss:  1.2324785371893086
test acc:  0.9185
forward train acc:  0.99904  and loss:  1.3164921657880768
test acc:  0.9181
forward train acc:  0.99928  and loss:  0.8715581704163924
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4791666666666667  ==>  100 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.609375  ==>  300 / 768
layer  10  :  0.703125  ==>  228 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5989583333333334  ==>  308 / 768
***** skip layer  0
[1, 3, 2, 0, 5, 5, 5, 5, 4, 0, 0, 0, 5, 0]
***** skip layer  1
[1, 2, 2, 0, 5, 5, 5, 5, 4, 0, 0, 0, 5, 0]
***** skip layer  2
[1, 2, 1, 0, 5, 5, 5, 5, 4, 0, 0, 0, 5, 0]
optimize layer  3
backward train epoch:  1
test acc:  0.8973
forward train acc:  0.99886  and loss:  1.9262077994644642
test acc:  0.9185
forward train acc:  0.9989  and loss:  1.3885498068266315
test acc:  0.9169
forward train acc:  0.9989  and loss:  1.3192598171590362
test acc:  0.9173
forward train acc:  0.99906  and loss:  1.2704782868095208
test acc:  0.9177
forward train acc:  0.99914  and loss:  1.088430166739272
test acc:  0.9183
forward train acc:  0.99938  and loss:  0.9644838267704472
test acc:  0.9187
forward train acc:  0.99912  and loss:  1.166991678197519
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.609375  ==>  300 / 768
layer  10  :  0.703125  ==>  228 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5989583333333334  ==>  308 / 768
***** skip layer  4
[1, 2, 1, 0, 4, 5, 5, 5, 4, 0, 0, 0, 5, 0]
***** skip layer  5
[1, 2, 1, 0, 4, 4, 5, 5, 4, 0, 0, 0, 5, 0]
***** skip layer  6
[1, 2, 1, 0, 4, 4, 4, 5, 4, 0, 0, 0, 5, 0]
***** skip layer  7
[1, 2, 1, 0, 4, 4, 4, 4, 4, 0, 0, 0, 5, 0]
***** skip layer  8
[1, 2, 1, 0, 4, 4, 4, 4, 3, 0, 0, 0, 5, 0]
optimize layer  9
backward train epoch:  124
test acc:  0.109
forward train acc:  0.99912  and loss:  1.1548261253337841
test acc:  0.9179
forward train acc:  0.999  and loss:  1.2652498657698743
test acc:  0.9175
forward train acc:  0.99898  and loss:  1.2567014421219938
test acc:  0.9178
forward train acc:  0.99914  and loss:  1.2218185338279
test acc:  0.9185
forward train acc:  0.99906  and loss:  1.1119303331070114
test acc:  0.9189
forward train acc:  0.99932  and loss:  0.8303179230570095
test acc:  0.9192
forward train acc:  0.99932  and loss:  0.9246823071152903
test acc:  0.9191
forward train acc:  0.9995  and loss:  0.6756287722528214
test acc:  0.9206
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6171875  ==>  294 / 768
layer  10  :  0.703125  ==>  228 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5989583333333334  ==>  308 / 768
optimize layer  10
backward train epoch:  187
test acc:  0.1116
forward train acc:  0.99894  and loss:  1.3607637880777474
test acc:  0.9189
forward train acc:  0.9992  and loss:  0.8728484320163261
test acc:  0.9187
forward train acc:  0.99918  and loss:  1.011942310462473
test acc:  0.9193
forward train acc:  0.99936  and loss:  1.0087007686597644
test acc:  0.9177
forward train acc:  0.99928  and loss:  0.930343742336845
test acc:  0.9179
forward train acc:  0.99936  and loss:  0.8278971455874853
test acc:  0.9179
forward train acc:  0.99942  and loss:  0.7139115892496193
test acc:  0.919
forward train acc:  0.9994  and loss:  0.78808593681606
test acc:  0.9188
forward train acc:  0.99972  and loss:  0.4280532484408468
test acc:  0.9205
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6171875  ==>  294 / 768
layer  10  :  0.7109375  ==>  222 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5989583333333334  ==>  308 / 768
optimize layer  11
backward train epoch:  134
test acc:  0.102
forward train acc:  0.9993  and loss:  0.9616301041533006
test acc:  0.9177
forward train acc:  0.999  and loss:  1.3544897584652063
test acc:  0.9171
forward train acc:  0.99902  and loss:  1.0659702992270468
test acc:  0.9172
forward train acc:  0.99942  and loss:  0.6297916924522724
test acc:  0.9181
forward train acc:  0.9994  and loss:  0.8022178727551363
test acc:  0.9191
forward train acc:  0.9994  and loss:  0.7279190601111623
test acc:  0.9195
forward train acc:  0.9995  and loss:  0.688909189106198
test acc:  0.9181
forward train acc:  0.9995  and loss:  0.6121089384541847
test acc:  0.9199
forward train acc:  0.99966  and loss:  0.4481142157455906
test acc:  0.9185
forward train acc:  0.9996  and loss:  0.47774291479436215
test acc:  0.9193
********** reverse layer  11  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6171875  ==>  294 / 768
layer  10  :  0.7109375  ==>  222 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.5989583333333334  ==>  308 / 768
***** skip layer  12
[1, 2, 1, 0, 4, 4, 4, 4, 3, 0, 0, 5, 4, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.1044
forward train acc:  0.99932  and loss:  0.8685820591199445
test acc:  0.918
forward train acc:  0.99938  and loss:  0.8280453899351414
test acc:  0.9177
forward train acc:  0.99946  and loss:  0.828383664455032
test acc:  0.9203
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6171875  ==>  294 / 768
layer  10  :  0.7109375  ==>  222 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.6067708333333334  ==>  302 / 768
***** skip layer  0
[0, 2, 1, 0, 4, 4, 4, 4, 3, 0, 0, 5, 4, 0]
***** skip layer  1
[0, 1, 1, 0, 4, 4, 4, 4, 3, 0, 0, 5, 4, 0]
***** skip layer  2
[0, 1, 0, 0, 4, 4, 4, 4, 3, 0, 0, 5, 4, 0]
optimize layer  3
backward train epoch:  2
test acc:  0.865
forward train acc:  0.99884  and loss:  1.2787919177790172
test acc:  0.9171
forward train acc:  0.99872  and loss:  1.656391970711411
test acc:  0.9168
forward train acc:  0.99896  and loss:  1.2119410624727607
test acc:  0.918
forward train acc:  0.9996  and loss:  0.6283587528159842
test acc:  0.9175
forward train acc:  0.99934  and loss:  0.9310463001020253
test acc:  0.918
forward train acc:  0.9994  and loss:  0.9040281015040819
test acc:  0.9185
forward train acc:  0.99944  and loss:  0.813081026339205
test acc:  0.9195
forward train acc:  0.9994  and loss:  0.7678288714669179
test acc:  0.9182
forward train acc:  0.99956  and loss:  0.646055652294308
test acc:  0.9181
forward train acc:  0.99936  and loss:  0.8631325408350676
test acc:  0.9177
********** reverse layer  3  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6171875  ==>  294 / 768
layer  10  :  0.7109375  ==>  222 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.6067708333333334  ==>  302 / 768
***** skip layer  4
[0, 1, 0, 5, 3, 4, 4, 4, 3, 0, 0, 5, 4, 0]
***** skip layer  5
[0, 1, 0, 5, 3, 3, 4, 4, 3, 0, 0, 5, 4, 0]
***** skip layer  6
[0, 1, 0, 5, 3, 3, 3, 4, 3, 0, 0, 5, 4, 0]
***** skip layer  7
[0, 1, 0, 5, 3, 3, 3, 3, 3, 0, 0, 5, 4, 0]
***** skip layer  8
[0, 1, 0, 5, 3, 3, 3, 3, 2, 0, 0, 5, 4, 0]
optimize layer  9
backward train epoch:  93
test acc:  0.1072
forward train acc:  0.99932  and loss:  0.7990326124418061
test acc:  0.9196
forward train acc:  0.99938  and loss:  0.7328470516222296
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.625  ==>  288 / 768
layer  10  :  0.7109375  ==>  222 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.6067708333333334  ==>  302 / 768
optimize layer  10
backward train epoch:  381
test acc:  0.1115
forward train acc:  0.99936  and loss:  0.88724866262055
test acc:  0.9188
forward train acc:  0.99936  and loss:  0.8798541905125603
test acc:  0.9205
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.625  ==>  288 / 768
layer  10  :  0.71875  ==>  216 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.6067708333333334  ==>  302 / 768
***** skip layer  11
[0, 1, 0, 5, 3, 3, 3, 3, 2, 0, 0, 4, 4, 0]
***** skip layer  12
[0, 1, 0, 5, 3, 3, 3, 3, 2, 0, 0, 4, 3, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.1042
forward train acc:  0.9994  and loss:  1.0018774856580421
test acc:  0.9194
forward train acc:  0.99922  and loss:  1.1907003528322093
test acc:  0.9197
forward train acc:  0.99906  and loss:  1.3502168077975512
test acc:  0.9176
forward train acc:  0.99966  and loss:  0.7877549789845943
test acc:  0.9204
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.625  ==>  288 / 768
layer  10  :  0.71875  ==>  216 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.6145833333333334  ==>  296 / 768
optimize layer  0
backward train epoch:  538
test acc:  0.1084
forward train acc:  0.69998  and loss:  471.026572227478
test acc:  0.7466
forward train acc:  0.79804  and loss:  247.79390743374825
test acc:  0.7984
forward train acc:  0.84446  and loss:  186.4780372083187
test acc:  0.8232
forward train acc:  0.87038  and loss:  153.08101457357407
test acc:  0.8336
forward train acc:  0.88194  and loss:  141.37175898253918
test acc:  0.8413
forward train acc:  0.89268  and loss:  127.09130623936653
test acc:  0.8493
forward train acc:  0.90244  and loss:  114.82868291437626
test acc:  0.8554
forward train acc:  0.91016  and loss:  104.82830039411783
test acc:  0.858
forward train acc:  0.91478  and loss:  100.67417453229427
test acc:  0.8589
forward train acc:  0.91696  and loss:  96.47144906967878
test acc:  0.8613
********** reverse layer  0  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.625  ==>  288 / 768
layer  10  :  0.71875  ==>  216 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.6145833333333334  ==>  296 / 768
***** skip layer  1
[5, 0, 0, 5, 3, 3, 3, 3, 2, 0, 0, 4, 3, 0]
optimize layer  2
backward train epoch:  230
test acc:  0.097
forward train acc:  0.97576  and loss:  28.696101615205407
test acc:  0.9051
forward train acc:  0.98744  and loss:  15.128111051628366
test acc:  0.9093
forward train acc:  0.9909  and loss:  10.952054070541635
test acc:  0.9122
forward train acc:  0.99308  and loss:  8.77665950008668
test acc:  0.9129
forward train acc:  0.99418  and loss:  6.967972386977635
test acc:  0.9131
forward train acc:  0.99524  and loss:  5.939914631773718
test acc:  0.9131
forward train acc:  0.99502  and loss:  5.9342398308217525
test acc:  0.914
forward train acc:  0.9958  and loss:  5.335646666353568
test acc:  0.9142
forward train acc:  0.99618  and loss:  4.627682524733245
test acc:  0.9137
forward train acc:  0.99644  and loss:  4.622760665719397
test acc:  0.9147
********** reverse layer  2  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.625  ==>  288 / 768
layer  10  :  0.71875  ==>  216 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.6145833333333334  ==>  296 / 768
***** skip layer  3
[5, 0, 5, 4, 3, 3, 3, 3, 2, 0, 0, 4, 3, 0]
***** skip layer  4
[5, 0, 5, 4, 2, 3, 3, 3, 2, 0, 0, 4, 3, 0]
***** skip layer  5
[5, 0, 5, 4, 2, 2, 3, 3, 2, 0, 0, 4, 3, 0]
***** skip layer  6
[5, 0, 5, 4, 2, 2, 2, 3, 2, 0, 0, 4, 3, 0]
***** skip layer  7
[5, 0, 5, 4, 2, 2, 2, 2, 2, 0, 0, 4, 3, 0]
***** skip layer  8
[5, 0, 5, 4, 2, 2, 2, 2, 1, 0, 0, 4, 3, 0]
optimize layer  9
backward train epoch:  66
test acc:  0.106
forward train acc:  0.99862  and loss:  1.7686637172009796
test acc:  0.9191
forward train acc:  0.99872  and loss:  1.6813051904318854
test acc:  0.9213
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6328125  ==>  282 / 768
layer  10  :  0.71875  ==>  216 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.6145833333333334  ==>  296 / 768
optimize layer  10
backward train epoch:  367
test acc:  0.1083
forward train acc:  0.99896  and loss:  1.3766403489862569
test acc:  0.9203
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6328125  ==>  282 / 768
layer  10  :  0.7265625  ==>  210 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.6145833333333334  ==>  296 / 768
***** skip layer  11
[5, 0, 5, 4, 2, 2, 2, 2, 1, 0, 0, 3, 3, 0]
***** skip layer  12
[5, 0, 5, 4, 2, 2, 2, 2, 1, 0, 0, 3, 2, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.1012
forward train acc:  0.99904  and loss:  1.5718372547999024
test acc:  0.9203
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.53125  ==>  45 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6328125  ==>  282 / 768
layer  10  :  0.7265625  ==>  210 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.6223958333333334  ==>  290 / 768
***** skip layer  0
[4, 0, 5, 4, 2, 2, 2, 2, 1, 0, 0, 3, 2, 0]
optimize layer  1
backward train epoch:  72
test acc:  0.1097
forward train acc:  0.99896  and loss:  1.763081199140288
test acc:  0.918
forward train acc:  0.9989  and loss:  1.6950794991571456
test acc:  0.9194
forward train acc:  0.9991  and loss:  1.2366086000110954
test acc:  0.9186
forward train acc:  0.99944  and loss:  0.845814204774797
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5416666666666666  ==>  44 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6328125  ==>  282 / 768
layer  10  :  0.7265625  ==>  210 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.6223958333333334  ==>  290 / 768
***** skip layer  2
[4, 0, 4, 4, 2, 2, 2, 2, 1, 0, 0, 3, 2, 0]
***** skip layer  3
[4, 0, 4, 3, 2, 2, 2, 2, 1, 0, 0, 3, 2, 0]
***** skip layer  4
[4, 0, 4, 3, 1, 2, 2, 2, 1, 0, 0, 3, 2, 0]
***** skip layer  5
[4, 0, 4, 3, 1, 1, 2, 2, 1, 0, 0, 3, 2, 0]
***** skip layer  6
[4, 0, 4, 3, 1, 1, 1, 2, 1, 0, 0, 3, 2, 0]
***** skip layer  7
[4, 0, 4, 3, 1, 1, 1, 1, 1, 0, 0, 3, 2, 0]
***** skip layer  8
[4, 0, 4, 3, 1, 1, 1, 1, 0, 0, 0, 3, 2, 0]
optimize layer  9
backward train epoch:  11
test acc:  0.3252
forward train acc:  0.99896  and loss:  1.4030426813405938
test acc:  0.9182
forward train acc:  0.99918  and loss:  1.1169297732412815
test acc:  0.9208
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5416666666666666  ==>  44 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.640625  ==>  276 / 768
layer  10  :  0.7265625  ==>  210 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.6223958333333334  ==>  290 / 768
optimize layer  10
backward train epoch:  503
test acc:  0.1035
forward train acc:  0.99914  and loss:  1.1902198875613976
test acc:  0.9217
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5416666666666666  ==>  44 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.640625  ==>  276 / 768
layer  10  :  0.734375  ==>  204 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.6223958333333334  ==>  290 / 768
***** skip layer  11
[4, 0, 4, 3, 1, 1, 1, 1, 0, 0, 0, 2, 2, 0]
***** skip layer  12
[4, 0, 4, 3, 1, 1, 1, 1, 0, 0, 0, 2, 1, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.0948
forward train acc:  0.99904  and loss:  1.2724358317791484
test acc:  0.9184
forward train acc:  0.99916  and loss:  1.205032871774165
test acc:  0.9194
forward train acc:  0.9992  and loss:  1.223833373922389
test acc:  0.9212
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5416666666666666  ==>  44 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.640625  ==>  276 / 768
layer  10  :  0.734375  ==>  204 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.6302083333333334  ==>  284 / 768
***** skip layer  0
[3, 0, 4, 3, 1, 1, 1, 1, 0, 0, 0, 2, 1, 0]
optimize layer  1
backward train epoch:  190
test acc:  0.0934
forward train acc:  0.99794  and loss:  2.826387846842408
test acc:  0.9178
forward train acc:  0.99846  and loss:  2.088930987752974
test acc:  0.9205
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5520833333333334  ==>  43 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.640625  ==>  276 / 768
layer  10  :  0.734375  ==>  204 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.6302083333333334  ==>  284 / 768
***** skip layer  2
[3, 0, 3, 3, 1, 1, 1, 1, 0, 0, 0, 2, 1, 0]
***** skip layer  3
[3, 0, 3, 2, 1, 1, 1, 1, 0, 0, 0, 2, 1, 0]
***** skip layer  4
[3, 0, 3, 2, 0, 1, 1, 1, 0, 0, 0, 2, 1, 0]
***** skip layer  5
[3, 0, 3, 2, 0, 0, 1, 1, 0, 0, 0, 2, 1, 0]
***** skip layer  6
[3, 0, 3, 2, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0]
***** skip layer  7
[3, 0, 3, 2, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0]
optimize layer  8
backward train epoch:  14
test acc:  0.1458
forward train acc:  0.98882  and loss:  14.52302897372283
test acc:  0.9136
forward train acc:  0.99532  and loss:  6.100078296265565
test acc:  0.9143
forward train acc:  0.996  and loss:  5.295969610102475
test acc:  0.9135
forward train acc:  0.99724  and loss:  3.799697482259944
test acc:  0.9161
forward train acc:  0.9973  and loss:  3.475105334597174
test acc:  0.9171
forward train acc:  0.9979  and loss:  2.833749756682664
test acc:  0.9173
forward train acc:  0.99774  and loss:  2.9861061007832177
test acc:  0.9173
forward train acc:  0.99804  and loss:  2.647673884552205
test acc:  0.9183
forward train acc:  0.99808  and loss:  2.506151886889711
test acc:  0.9173
forward train acc:  0.9981  and loss:  2.335206250369083
test acc:  0.9187
********** reverse layer  8  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5520833333333334  ==>  43 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.640625  ==>  276 / 768
layer  10  :  0.734375  ==>  204 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.6302083333333334  ==>  284 / 768
optimize layer  9
backward train epoch:  33
test acc:  0.1075
forward train acc:  0.99902  and loss:  1.2361763648223132
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5520833333333334  ==>  43 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6484375  ==>  270 / 768
layer  10  :  0.734375  ==>  204 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.6302083333333334  ==>  284 / 768
optimize layer  10
backward train epoch:  15
test acc:  0.1903
forward train acc:  0.99912  and loss:  1.1658340736757964
test acc:  0.919
forward train acc:  0.999  and loss:  1.3787799309939146
test acc:  0.9179
forward train acc:  0.99894  and loss:  1.3770844726532232
test acc:  0.9191
forward train acc:  0.99916  and loss:  1.0395316202193499
test acc:  0.9212
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5520833333333334  ==>  43 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6484375  ==>  270 / 768
layer  10  :  0.7421875  ==>  198 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.6302083333333334  ==>  284 / 768
***** skip layer  11
[3, 0, 3, 2, 0, 0, 0, 0, 5, 0, 0, 1, 1, 0]
***** skip layer  12
[3, 0, 3, 2, 0, 0, 0, 0, 5, 0, 0, 1, 0, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.1042
forward train acc:  0.9991  and loss:  1.4591037931386381
test acc:  0.9209
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5520833333333334  ==>  43 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6484375  ==>  270 / 768
layer  10  :  0.7421875  ==>  198 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.6380208333333334  ==>  278 / 768
***** skip layer  0
[2, 0, 3, 2, 0, 0, 0, 0, 5, 0, 0, 1, 0, 0]
optimize layer  1
backward train epoch:  10
test acc:  0.2863
forward train acc:  0.99872  and loss:  1.9202055722707883
test acc:  0.9186
forward train acc:  0.9989  and loss:  1.8132665198645554
test acc:  0.9204
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5520833333333334  ==>  172 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6484375  ==>  270 / 768
layer  10  :  0.7421875  ==>  198 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.6380208333333334  ==>  278 / 768
***** skip layer  2
[2, 0, 2, 2, 0, 0, 0, 0, 5, 0, 0, 1, 0, 0]
***** skip layer  3
[2, 0, 2, 1, 0, 0, 0, 0, 5, 0, 0, 1, 0, 0]
optimize layer  4
backward train epoch:  177
test acc:  0.094
forward train acc:  0.9982  and loss:  2.6546427738503553
test acc:  0.9198
forward train acc:  0.99856  and loss:  2.0001065508113243
test acc:  0.9207
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5625  ==>  168 / 384
layer  5  :  0.53125  ==>  180 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6484375  ==>  270 / 768
layer  10  :  0.7421875  ==>  198 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.6380208333333334  ==>  278 / 768
optimize layer  5
backward train epoch:  31
test acc:  0.1149
forward train acc:  0.99808  and loss:  2.5050256764516234
test acc:  0.9173
forward train acc:  0.99846  and loss:  2.081418239278719
test acc:  0.9196
forward train acc:  0.99856  and loss:  1.8147683563875034
test acc:  0.9173
forward train acc:  0.99862  and loss:  1.8627290511503816
test acc:  0.9187
forward train acc:  0.99856  and loss:  1.9343018698273227
test acc:  0.9184
forward train acc:  0.99894  and loss:  1.394821776309982
test acc:  0.9203
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5625  ==>  168 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.5520833333333334  ==>  172 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6484375  ==>  270 / 768
layer  10  :  0.7421875  ==>  198 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.6380208333333334  ==>  278 / 768
optimize layer  6
backward train epoch:  154
test acc:  0.1226
forward train acc:  0.99842  and loss:  2.1228677269537
test acc:  0.9205
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5625  ==>  168 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.5625  ==>  168 / 384
layer  7  :  0.90625  ==>  72 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6484375  ==>  270 / 768
layer  10  :  0.7421875  ==>  198 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.6380208333333334  ==>  278 / 768
optimize layer  7
backward train epoch:  221
test acc:  0.1107
forward train acc:  0.99782  and loss:  2.5675618095556274
test acc:  0.9185
forward train acc:  0.9985  and loss:  2.0828983511892147
test acc:  0.9175
forward train acc:  0.99784  and loss:  2.4660902549803723
test acc:  0.9173
forward train acc:  0.99872  and loss:  1.6437450785306282
test acc:  0.9178
forward train acc:  0.99898  and loss:  1.3875881478888914
test acc:  0.9192
forward train acc:  0.99852  and loss:  1.7129247428383678
test acc:  0.9187
forward train acc:  0.99918  and loss:  1.1533320246380754
test acc:  0.9208
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5625  ==>  168 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.5625  ==>  168 / 384
layer  7  :  0.9166666666666666  ==>  64 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6484375  ==>  270 / 768
layer  10  :  0.7421875  ==>  198 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.6380208333333334  ==>  278 / 768
***** skip layer  8
[2, 0, 2, 1, 0, 0, 0, 0, 4, 0, 0, 1, 0, 0]
optimize layer  9
backward train epoch:  32
test acc:  0.141
forward train acc:  0.99884  and loss:  1.5686078439466655
test acc:  0.9179
forward train acc:  0.9986  and loss:  1.7255206159316003
test acc:  0.9199
forward train acc:  0.9989  and loss:  1.6357863731100224
test acc:  0.9172
forward train acc:  0.99886  and loss:  1.5057663618354127
test acc:  0.9198
forward train acc:  0.99908  and loss:  1.307983326696558
test acc:  0.9192
forward train acc:  0.99904  and loss:  1.2012337051564828
test acc:  0.9193
forward train acc:  0.99924  and loss:  1.0714701633551158
test acc:  0.9181
forward train acc:  0.99916  and loss:  1.049392283486668
test acc:  0.9193
forward train acc:  0.99944  and loss:  0.8309332216158509
test acc:  0.9184
forward train acc:  0.99946  and loss:  0.7735558000858873
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5625  ==>  168 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.5625  ==>  168 / 384
layer  7  :  0.9166666666666666  ==>  64 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.7421875  ==>  198 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.6380208333333334  ==>  278 / 768
optimize layer  10
backward train epoch:  121
test acc:  0.1119
forward train acc:  0.99902  and loss:  1.4124761694401968
test acc:  0.9199
forward train acc:  0.99874  and loss:  1.5375525777926669
test acc:  0.9193
forward train acc:  0.99904  and loss:  1.3271204002958257
test acc:  0.9181
forward train acc:  0.99906  and loss:  1.26316086971201
test acc:  0.9204
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5625  ==>  168 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.5625  ==>  168 / 384
layer  7  :  0.9166666666666666  ==>  64 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.75  ==>  192 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8854166666666666  ==>  88 / 768
layer  13  :  0.6380208333333334  ==>  278 / 768
***** skip layer  11
[2, 0, 2, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0]
optimize layer  12
backward train epoch:  124
test acc:  0.1026
forward train acc:  0.9993  and loss:  6.749376758933067
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5625  ==>  168 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.5625  ==>  168 / 384
layer  7  :  0.9166666666666666  ==>  64 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.75  ==>  192 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8958333333333334  ==>  80 / 768
layer  13  :  0.6380208333333334  ==>  278 / 768
optimize layer  13
backward train epoch:  102
test acc:  0.1064
forward train acc:  0.99922  and loss:  3.7315241121686995
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5625  ==>  168 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.5625  ==>  168 / 384
layer  7  :  0.9166666666666666  ==>  64 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.75  ==>  192 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8958333333333334  ==>  80 / 768
layer  13  :  0.6458333333333334  ==>  272 / 768
***** skip layer  0
[1, 0, 2, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0]
optimize layer  1
backward train epoch:  126
test acc:  0.1002
forward train acc:  0.98446  and loss:  23.2571375137195
test acc:  0.9084
forward train acc:  0.99024  and loss:  13.954347855877131
test acc:  0.9097
forward train acc:  0.99426  and loss:  9.463962532114238
test acc:  0.9118
forward train acc:  0.99506  and loss:  8.270811531692743
test acc:  0.9117
forward train acc:  0.99552  and loss:  7.50471284147352
test acc:  0.9143
forward train acc:  0.99644  and loss:  6.673481199424714
test acc:  0.9145
forward train acc:  0.99628  and loss:  6.55415571667254
test acc:  0.9149
forward train acc:  0.9966  and loss:  6.120398254599422
test acc:  0.9148
forward train acc:  0.99702  and loss:  5.440696530044079
test acc:  0.9147
forward train acc:  0.99706  and loss:  5.299449470359832
test acc:  0.9142
********** reverse layer  1  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5625  ==>  168 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.5625  ==>  168 / 384
layer  7  :  0.9166666666666666  ==>  64 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.75  ==>  192 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8958333333333334  ==>  80 / 768
layer  13  :  0.6458333333333334  ==>  272 / 768
***** skip layer  2
[1, 5, 1, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0]
***** skip layer  3
[1, 5, 1, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0]
optimize layer  4
backward train epoch:  2
test acc:  0.0848
forward train acc:  0.998  and loss:  4.3087985017336905
test acc:  0.9161
forward train acc:  0.99838  and loss:  3.506177320610732
test acc:  0.9182
forward train acc:  0.99858  and loss:  3.3139421404339373
test acc:  0.9195
forward train acc:  0.9987  and loss:  3.225567884510383
test acc:  0.9185
forward train acc:  0.99898  and loss:  2.7827287102118134
test acc:  0.9196
forward train acc:  0.99888  and loss:  2.798566218232736
test acc:  0.9187
forward train acc:  0.99906  and loss:  2.6050509384367615
test acc:  0.9196
forward train acc:  0.99932  and loss:  2.2715418853331357
test acc:  0.9207
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5729166666666666  ==>  164 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.5625  ==>  168 / 384
layer  7  :  0.9166666666666666  ==>  64 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.75  ==>  192 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8958333333333334  ==>  80 / 768
layer  13  :  0.6458333333333334  ==>  272 / 768
optimize layer  5
backward train epoch:  132
test acc:  0.1015
forward train acc:  0.99844  and loss:  3.218858697451651
test acc:  0.9172
forward train acc:  0.99878  and loss:  2.923507171217352
test acc:  0.9177
forward train acc:  0.99834  and loss:  3.180117760086432
test acc:  0.9169
forward train acc:  0.9986  and loss:  2.692906054900959
test acc:  0.9181
forward train acc:  0.99892  and loss:  2.599231304368004
test acc:  0.9186
forward train acc:  0.99924  and loss:  2.1038897479884326
test acc:  0.9176
forward train acc:  0.99906  and loss:  2.2101170462556183
test acc:  0.9168
forward train acc:  0.9992  and loss:  1.9869583076797426
test acc:  0.9194
forward train acc:  0.99918  and loss:  1.9020941259805113
test acc:  0.9189
forward train acc:  0.99906  and loss:  2.1627857808489352
test acc:  0.9185
********** reverse layer  5  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5729166666666666  ==>  164 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.5625  ==>  168 / 384
layer  7  :  0.9166666666666666  ==>  64 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.75  ==>  192 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8958333333333334  ==>  80 / 768
layer  13  :  0.6458333333333334  ==>  272 / 768
optimize layer  6
backward train epoch:  94
test acc:  0.0935
forward train acc:  0.99874  and loss:  2.478203370468691
test acc:  0.918
forward train acc:  0.99894  and loss:  2.1480324699077755
test acc:  0.9173
forward train acc:  0.99904  and loss:  2.0372290601953864
test acc:  0.9176
forward train acc:  0.999  and loss:  2.0642481190152466
test acc:  0.9184
forward train acc:  0.99946  and loss:  1.6798555625136942
test acc:  0.9191
forward train acc:  0.99902  and loss:  1.9394275585655123
test acc:  0.9186
forward train acc:  0.9994  and loss:  1.619394401437603
test acc:  0.9202
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5729166666666666  ==>  164 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.5729166666666666  ==>  164 / 384
layer  7  :  0.9166666666666666  ==>  64 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.75  ==>  192 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8958333333333334  ==>  80 / 768
layer  13  :  0.6458333333333334  ==>  272 / 768
optimize layer  7
backward train epoch:  19
test acc:  0.1885
forward train acc:  0.9983  and loss:  2.776353031164035
test acc:  0.9169
forward train acc:  0.99858  and loss:  2.3167402129620314
test acc:  0.9162
forward train acc:  0.99858  and loss:  2.3924276442267
test acc:  0.917
forward train acc:  0.99866  and loss:  2.1795297553762794
test acc:  0.918
forward train acc:  0.99896  and loss:  1.997539889998734
test acc:  0.9198
forward train acc:  0.99922  and loss:  1.6513126710196957
test acc:  0.9205
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5729166666666666  ==>  164 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.5729166666666666  ==>  164 / 384
layer  7  :  0.9270833333333334  ==>  56 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.75  ==>  192 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8958333333333334  ==>  80 / 768
layer  13  :  0.6458333333333334  ==>  272 / 768
***** skip layer  8
[1, 5, 1, 0, 0, 5, 0, 0, 3, 0, 0, 0, 0, 0]
optimize layer  9
backward train epoch:  46
test acc:  0.1116
forward train acc:  0.99892  and loss:  1.9877821664558724
test acc:  0.9174
forward train acc:  0.99894  and loss:  2.029948349110782
test acc:  0.919
forward train acc:  0.99882  and loss:  2.0699432777473703
test acc:  0.9167
forward train acc:  0.99922  and loss:  1.5520244161598384
test acc:  0.9177
forward train acc:  0.99916  and loss:  1.5747029721969739
test acc:  0.918
forward train acc:  0.99914  and loss:  1.6421811650507152
test acc:  0.9198
forward train acc:  0.99912  and loss:  1.6996103608980775
test acc:  0.9193
forward train acc:  0.99918  and loss:  1.60804223571904
test acc:  0.9184
forward train acc:  0.99934  and loss:  1.3514949955279008
test acc:  0.9182
forward train acc:  0.99934  and loss:  1.3815180286765099
test acc:  0.918
********** reverse layer  9  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5729166666666666  ==>  164 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.5729166666666666  ==>  164 / 384
layer  7  :  0.9270833333333334  ==>  56 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.75  ==>  192 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8958333333333334  ==>  80 / 768
layer  13  :  0.6458333333333334  ==>  272 / 768
optimize layer  10
backward train epoch:  191
test acc:  0.1104
forward train acc:  0.9992  and loss:  1.5930924051208422
test acc:  0.9184
forward train acc:  0.9988  and loss:  2.057516364264302
test acc:  0.9179
forward train acc:  0.99904  and loss:  1.8709990426432341
test acc:  0.9189
forward train acc:  0.9991  and loss:  1.5677335631335154
test acc:  0.9196
forward train acc:  0.99904  and loss:  1.4658156373770908
test acc:  0.921
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5729166666666666  ==>  164 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.5729166666666666  ==>  164 / 384
layer  7  :  0.9270833333333334  ==>  56 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7447916666666666  ==>  196 / 768
layer  12  :  0.8958333333333334  ==>  80 / 768
layer  13  :  0.6458333333333334  ==>  272 / 768
optimize layer  11
backward train epoch:  76
test acc:  0.1079
forward train acc:  0.99906  and loss:  1.5330653451383114
test acc:  0.9191
forward train acc:  0.9989  and loss:  1.6882004225626588
test acc:  0.9199
forward train acc:  0.99916  and loss:  1.5319711905904114
test acc:  0.9202
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5729166666666666  ==>  164 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.5729166666666666  ==>  164 / 384
layer  7  :  0.9270833333333334  ==>  56 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.8958333333333334  ==>  80 / 768
layer  13  :  0.6458333333333334  ==>  272 / 768
optimize layer  12
backward train epoch:  142
test acc:  0.1106
forward train acc:  0.88918  and loss:  226.40889385342598
test acc:  0.8249
forward train acc:  0.89872  and loss:  205.08281221985817
test acc:  0.8276
forward train acc:  0.89912  and loss:  189.58431392908096
test acc:  0.8297
forward train acc:  0.89916  and loss:  179.3511265218258
test acc:  0.8307
forward train acc:  0.89946  and loss:  172.84901422262192
test acc:  0.832
forward train acc:  0.90232  and loss:  166.84110164642334
test acc:  0.832
forward train acc:  0.92166  and loss:  161.04209503531456
test acc:  0.9145
forward train acc:  0.95456  and loss:  156.92801347374916
test acc:  0.9171
forward train acc:  0.97464  and loss:  154.41841211915016
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5729166666666666  ==>  164 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.5729166666666666  ==>  164 / 384
layer  7  :  0.9270833333333334  ==>  56 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6458333333333334  ==>  272 / 768
optimize layer  13
backward train epoch:  102
test acc:  0.0908
forward train acc:  0.9987  and loss:  152.3280891776085
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.4895833333333333  ==>  98 / 192
layer  4  :  0.5729166666666666  ==>  164 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.5729166666666666  ==>  164 / 384
layer  7  :  0.9270833333333334  ==>  56 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6536458333333334  ==>  266 / 768
***** skip layer  0
[0, 5, 1, 0, 0, 5, 0, 0, 3, 5, 0, 0, 0, 0]
***** skip layer  1
[0, 4, 1, 0, 0, 5, 0, 0, 3, 5, 0, 0, 0, 0]
***** skip layer  2
[0, 4, 0, 0, 0, 5, 0, 0, 3, 5, 0, 0, 0, 0]
optimize layer  3
backward train epoch:  1
test acc:  0.8685
forward train acc:  0.99832  and loss:  142.70392167568207
test acc:  0.9191
forward train acc:  0.99884  and loss:  133.30487763881683
test acc:  0.9167
forward train acc:  0.9986  and loss:  125.78959357738495
test acc:  0.9205
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5  ==>  96 / 192
layer  4  :  0.5729166666666666  ==>  164 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.5729166666666666  ==>  164 / 384
layer  7  :  0.9270833333333334  ==>  56 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6536458333333334  ==>  266 / 768
optimize layer  4
backward train epoch:  1
test acc:  0.8912
forward train acc:  0.99804  and loss:  119.36726033687592
test acc:  0.9163
forward train acc:  0.99846  and loss:  112.44524857401848
test acc:  0.9173
forward train acc:  0.9986  and loss:  106.70054215192795
test acc:  0.9184
forward train acc:  0.99882  and loss:  102.67911933362484
test acc:  0.9188
forward train acc:  0.9989  and loss:  99.84449382126331
test acc:  0.9175
forward train acc:  0.999  and loss:  97.6206356137991
test acc:  0.9178
forward train acc:  0.99896  and loss:  95.21063384413719
test acc:  0.918
forward train acc:  0.99902  and loss:  93.45012494921684
test acc:  0.9183
forward train acc:  0.99914  and loss:  92.21581673622131
test acc:  0.9196
forward train acc:  0.9992  and loss:  91.08490531146526
test acc:  0.9202
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5  ==>  96 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.5729166666666666  ==>  164 / 384
layer  7  :  0.9270833333333334  ==>  56 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6536458333333334  ==>  266 / 768
***** skip layer  5
[0, 4, 0, 0, 0, 4, 0, 0, 3, 5, 0, 0, 0, 0]
optimize layer  6
backward train epoch:  43
test acc:  0.0897
forward train acc:  0.99876  and loss:  89.07260590791702
test acc:  0.9185
forward train acc:  0.99904  and loss:  84.95935718715191
test acc:  0.9184
forward train acc:  0.99872  and loss:  81.591264590621
test acc:  0.9187
forward train acc:  0.99876  and loss:  78.80119152367115
test acc:  0.9179
forward train acc:  0.99894  and loss:  77.05844628810883
test acc:  0.9185
forward train acc:  0.99888  and loss:  75.18816949427128
test acc:  0.9193
forward train acc:  0.99922  and loss:  73.23444700241089
test acc:  0.9188
forward train acc:  0.99916  and loss:  72.3490589261055
test acc:  0.9193
forward train acc:  0.99908  and loss:  71.51732912659645
test acc:  0.9206
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5  ==>  96 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.5833333333333334  ==>  160 / 384
layer  7  :  0.9270833333333334  ==>  56 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6536458333333334  ==>  266 / 768
optimize layer  7
backward train epoch:  73
test acc:  0.0916
forward train acc:  0.99832  and loss:  70.60377734899521
test acc:  0.9182
forward train acc:  0.99866  and loss:  67.28913995623589
test acc:  0.9184
forward train acc:  0.99868  and loss:  64.3539450764656
test acc:  0.9193
forward train acc:  0.99898  and loss:  62.021445125341415
test acc:  0.9191
forward train acc:  0.99904  and loss:  60.553749069571495
test acc:  0.9186
forward train acc:  0.99912  and loss:  59.34307198226452
test acc:  0.9174
forward train acc:  0.9989  and loss:  58.09807647764683
test acc:  0.9196
forward train acc:  0.99902  and loss:  57.06710276007652
test acc:  0.9192
forward train acc:  0.99914  and loss:  56.2938357591629
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5  ==>  96 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.5833333333333334  ==>  160 / 384
layer  7  :  0.9375  ==>  48 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6536458333333334  ==>  266 / 768
***** skip layer  8
[0, 4, 0, 0, 0, 4, 0, 0, 2, 5, 0, 0, 0, 0]
***** skip layer  9
[0, 4, 0, 0, 0, 4, 0, 0, 2, 4, 0, 0, 0, 0]
optimize layer  10
backward train epoch:  17
test acc:  0.134
forward train acc:  0.9992  and loss:  54.835525304079056
test acc:  0.9181
forward train acc:  0.99874  and loss:  52.84748165309429
test acc:  0.9187
forward train acc:  0.99904  and loss:  50.23057944327593
test acc:  0.9176
forward train acc:  0.99916  and loss:  48.58404337614775
test acc:  0.917
forward train acc:  0.99904  and loss:  47.319596007466316
test acc:  0.9187
forward train acc:  0.99942  and loss:  45.95509044826031
test acc:  0.9178
forward train acc:  0.999  and loss:  45.36396807432175
test acc:  0.9182
forward train acc:  0.99918  and loss:  44.401151925325394
test acc:  0.9182
forward train acc:  0.99924  and loss:  43.86996586620808
test acc:  0.9186
forward train acc:  0.99934  and loss:  43.26402150094509
test acc:  0.9177
********** reverse layer  10  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5  ==>  96 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.5833333333333334  ==>  160 / 384
layer  7  :  0.9375  ==>  48 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6536458333333334  ==>  266 / 768
optimize layer  11
backward train epoch:  181
test acc:  0.0971
forward train acc:  0.9993  and loss:  42.333813682198524
test acc:  0.9198
forward train acc:  0.99884  and loss:  40.68148931860924
test acc:  0.9177
forward train acc:  0.99884  and loss:  38.87913899123669
test acc:  0.9174
forward train acc:  0.99914  and loss:  37.13226317614317
test acc:  0.919
forward train acc:  0.9995  and loss:  36.08727091550827
test acc:  0.9168
forward train acc:  0.99926  and loss:  35.45205515623093
test acc:  0.9174
forward train acc:  0.99914  and loss:  34.61881500482559
test acc:  0.919
forward train acc:  0.99936  and loss:  33.8331333398819
test acc:  0.9188
forward train acc:  0.99936  and loss:  33.401497945189476
test acc:  0.9185
forward train acc:  0.99934  and loss:  32.932158626616
test acc:  0.918
********** reverse layer  11  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5  ==>  96 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.5833333333333334  ==>  160 / 384
layer  7  :  0.9375  ==>  48 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6536458333333334  ==>  266 / 768
optimize layer  12
backward train epoch:  132
test acc:  0.1
forward train acc:  0.1  and loss:  1120.3087792396545
test acc:  0.1
forward train acc:  0.1  and loss:  1089.6177847385406
test acc:  0.1
forward train acc:  0.1  and loss:  1063.0320003032684
test acc:  0.1
forward train acc:  0.1  and loss:  1045.1836431026459
test acc:  0.1
forward train acc:  0.1  and loss:  1034.2297570705414
test acc:  0.1
forward train acc:  0.1  and loss:  1023.8724453449249
test acc:  0.1
forward train acc:  0.1  and loss:  1014.0488715171814
test acc:  0.1
forward train acc:  0.1  and loss:  1006.9821112155914
test acc:  0.1
forward train acc:  0.1  and loss:  1002.5159783363342
test acc:  0.1
forward train acc:  0.1  and loss:  998.0906448364258
test acc:  0.1
********** reverse layer  12  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5  ==>  96 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.5833333333333334  ==>  160 / 384
layer  7  :  0.9375  ==>  48 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6536458333333334  ==>  266 / 768
optimize layer  13
backward train epoch:  102
test acc:  0.0957
forward train acc:  0.99924  and loss:  44.69793588668108
test acc:  0.9184
forward train acc:  0.99898  and loss:  40.70274868607521
test acc:  0.9183
forward train acc:  0.99886  and loss:  37.673561818897724
test acc:  0.9166
forward train acc:  0.99926  and loss:  35.06434265524149
test acc:  0.9172
forward train acc:  0.99918  and loss:  33.794748932123184
test acc:  0.919
forward train acc:  0.99946  and loss:  32.39704671502113
test acc:  0.9197
forward train acc:  0.99918  and loss:  31.49376367032528
test acc:  0.9198
forward train acc:  0.99942  and loss:  30.343506015837193
test acc:  0.9184
forward train acc:  0.99948  and loss:  29.63750845938921
test acc:  0.9188
forward train acc:  0.99946  and loss:  29.16153598576784
test acc:  0.9206
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5  ==>  96 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.5833333333333334  ==>  160 / 384
layer  7  :  0.9375  ==>  48 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6614583333333334  ==>  260 / 768
optimize layer  0
backward train epoch:  469
test acc:  0.1075
forward train acc:  0.67228  and loss:  470.5772947669029
test acc:  0.7277
forward train acc:  0.77172  and loss:  307.04609775543213
test acc:  0.7714
forward train acc:  0.81816  and loss:  248.04254767298698
test acc:  0.8002
forward train acc:  0.84056  and loss:  217.84386759996414
test acc:  0.8126
forward train acc:  0.85518  and loss:  199.51335778832436
test acc:  0.8193
forward train acc:  0.86716  and loss:  185.548688352108
test acc:  0.8294
forward train acc:  0.87472  and loss:  173.1987537741661
test acc:  0.8351
forward train acc:  0.88392  and loss:  164.36169841885567
test acc:  0.8378
forward train acc:  0.88902  and loss:  156.88996118307114
test acc:  0.8416
forward train acc:  0.89038  and loss:  154.12342062592506
test acc:  0.8456
********** reverse layer  0  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5  ==>  96 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.5833333333333334  ==>  160 / 384
layer  7  :  0.9375  ==>  48 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6614583333333334  ==>  260 / 768
***** skip layer  1
[5, 3, 0, 0, 0, 4, 0, 0, 2, 4, 5, 5, 5, 0]
optimize layer  2
backward train epoch:  45
test acc:  0.1168
forward train acc:  0.99276  and loss:  34.8900930210948
test acc:  0.9162
forward train acc:  0.99672  and loss:  27.212624836713076
test acc:  0.917
forward train acc:  0.99782  and loss:  24.346364300698042
test acc:  0.9169
forward train acc:  0.99826  and loss:  22.734712053090334
test acc:  0.9166
forward train acc:  0.99854  and loss:  21.93525007367134
test acc:  0.9193
forward train acc:  0.99844  and loss:  21.239186260849237
test acc:  0.9194
forward train acc:  0.99862  and loss:  20.591477416455746
test acc:  0.9183
forward train acc:  0.99914  and loss:  19.71792972460389
test acc:  0.9176
forward train acc:  0.9989  and loss:  19.467348016798496
test acc:  0.9191
forward train acc:  0.99896  and loss:  19.36891543865204
test acc:  0.9184
********** reverse layer  2  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5  ==>  96 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.5833333333333334  ==>  160 / 384
layer  7  :  0.9375  ==>  48 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6614583333333334  ==>  260 / 768
optimize layer  3
backward train epoch:  2
test acc:  0.1341
forward train acc:  0.99844  and loss:  19.224436968564987
test acc:  0.9189
forward train acc:  0.99862  and loss:  18.30480658635497
test acc:  0.9193
forward train acc:  0.9984  and loss:  17.57670433446765
test acc:  0.9171
forward train acc:  0.99874  and loss:  16.357839737087488
test acc:  0.9197
forward train acc:  0.9988  and loss:  15.895540215075016
test acc:  0.9198
forward train acc:  0.99882  and loss:  15.495348177850246
test acc:  0.9197
forward train acc:  0.999  and loss:  15.058586109429598
test acc:  0.9208
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5104166666666666  ==>  94 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.5833333333333334  ==>  160 / 384
layer  7  :  0.9375  ==>  48 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6614583333333334  ==>  260 / 768
optimize layer  4
backward train epoch:  228
test acc:  0.0841
forward train acc:  0.99848  and loss:  15.398850463330746
test acc:  0.9178
forward train acc:  0.99778  and loss:  15.2940982170403
test acc:  0.9166
forward train acc:  0.99848  and loss:  13.97808207757771
test acc:  0.9186
forward train acc:  0.99842  and loss:  13.494063340127468
test acc:  0.9196
forward train acc:  0.99854  and loss:  12.754965977743268
test acc:  0.9187
forward train acc:  0.99864  and loss:  12.73400167003274
test acc:  0.9187
forward train acc:  0.99906  and loss:  11.813997022807598
test acc:  0.9191
forward train acc:  0.99918  and loss:  11.516106830909848
test acc:  0.9192
forward train acc:  0.99894  and loss:  11.55312891677022
test acc:  0.9198
forward train acc:  0.99916  and loss:  11.07785103097558
test acc:  0.9187
********** reverse layer  4  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5104166666666666  ==>  94 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.5833333333333334  ==>  160 / 384
layer  7  :  0.9375  ==>  48 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6614583333333334  ==>  260 / 768
***** skip layer  5
[5, 3, 5, 0, 5, 3, 0, 0, 2, 4, 5, 5, 5, 0]
optimize layer  6
backward train epoch:  401
test acc:  0.1075
forward train acc:  0.99886  and loss:  11.083212964236736
test acc:  0.9198
forward train acc:  0.99862  and loss:  10.904598876833916
test acc:  0.9195
forward train acc:  0.99898  and loss:  10.03987249918282
test acc:  0.9189
forward train acc:  0.99916  and loss:  9.653270466253161
test acc:  0.9191
forward train acc:  0.99908  and loss:  9.29140017554164
test acc:  0.9198
forward train acc:  0.99932  and loss:  8.984081942588091
test acc:  0.9197
forward train acc:  0.99922  and loss:  8.883904511108994
test acc:  0.9185
forward train acc:  0.99914  and loss:  8.976769506931305
test acc:  0.9207
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5104166666666666  ==>  94 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.59375  ==>  156 / 384
layer  7  :  0.9375  ==>  48 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6614583333333334  ==>  260 / 768
optimize layer  7
backward train epoch:  49
test acc:  0.0742
forward train acc:  0.9979  and loss:  10.261690443381667
test acc:  0.918
forward train acc:  0.99826  and loss:  9.275841096416116
test acc:  0.9168
forward train acc:  0.99852  and loss:  8.905935525894165
test acc:  0.9191
forward train acc:  0.99864  and loss:  8.586040575057268
test acc:  0.9202
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5104166666666666  ==>  94 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.59375  ==>  156 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6614583333333334  ==>  260 / 768
***** skip layer  8
[5, 3, 5, 0, 5, 3, 0, 0, 1, 4, 5, 5, 5, 0]
***** skip layer  9
[5, 3, 5, 0, 5, 3, 0, 0, 1, 3, 5, 5, 5, 0]
***** skip layer  10
[5, 3, 5, 0, 5, 3, 0, 0, 1, 3, 4, 5, 5, 0]
***** skip layer  11
[5, 3, 5, 0, 5, 3, 0, 0, 1, 3, 4, 4, 5, 0]
***** skip layer  12
[5, 3, 5, 0, 5, 3, 0, 0, 1, 3, 4, 4, 4, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.1041
forward train acc:  0.99858  and loss:  35.22563187032938
test acc:  0.9168
forward train acc:  0.9985  and loss:  31.85675013065338
test acc:  0.9174
forward train acc:  0.99892  and loss:  28.737991720438004
test acc:  0.9195
forward train acc:  0.99914  and loss:  26.627132784575224
test acc:  0.9193
forward train acc:  0.99906  and loss:  25.773631889373064
test acc:  0.9182
forward train acc:  0.99902  and loss:  24.76629176363349
test acc:  0.9187
forward train acc:  0.999  and loss:  23.646872390061617
test acc:  0.9195
forward train acc:  0.99932  and loss:  22.716900922358036
test acc:  0.9195
forward train acc:  0.9993  and loss:  22.398429438471794
test acc:  0.9199
forward train acc:  0.99926  and loss:  21.919605538249016
test acc:  0.9179
********** reverse layer  13  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5104166666666666  ==>  94 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.59375  ==>  156 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6614583333333334  ==>  260 / 768
***** skip layer  0
[4, 3, 5, 0, 5, 3, 0, 0, 1, 3, 4, 4, 4, 5]
***** skip layer  1
[4, 2, 5, 0, 5, 3, 0, 0, 1, 3, 4, 4, 4, 5]
***** skip layer  2
[4, 2, 4, 0, 5, 3, 0, 0, 1, 3, 4, 4, 4, 5]
optimize layer  3
backward train epoch:  2
test acc:  0.8683
forward train acc:  0.9986  and loss:  7.350444394163787
test acc:  0.9169
forward train acc:  0.99846  and loss:  6.909784902818501
test acc:  0.918
forward train acc:  0.99858  and loss:  6.48682729806751
test acc:  0.9171
forward train acc:  0.99874  and loss:  6.374865937978029
test acc:  0.9179
forward train acc:  0.9988  and loss:  6.134581603109837
test acc:  0.918
forward train acc:  0.99898  and loss:  5.617187502793968
test acc:  0.9178
forward train acc:  0.999  and loss:  5.44649719260633
test acc:  0.9178
forward train acc:  0.99904  and loss:  5.370886663906276
test acc:  0.9181
forward train acc:  0.99896  and loss:  5.572561074979603
test acc:  0.918
forward train acc:  0.99926  and loss:  5.07639769744128
test acc:  0.9187
********** reverse layer  3  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5104166666666666  ==>  94 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.59375  ==>  156 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6614583333333334  ==>  260 / 768
***** skip layer  4
[4, 2, 4, 5, 4, 3, 0, 0, 1, 3, 4, 4, 4, 5]
***** skip layer  5
[4, 2, 4, 5, 4, 2, 0, 0, 1, 3, 4, 4, 4, 5]
optimize layer  6
backward train epoch:  11
test acc:  0.2227
forward train acc:  0.99892  and loss:  5.554571574553847
test acc:  0.9183
forward train acc:  0.999  and loss:  5.051240966655314
test acc:  0.9185
forward train acc:  0.99888  and loss:  5.114156853407621
test acc:  0.9168
forward train acc:  0.99892  and loss:  4.905768873170018
test acc:  0.918
forward train acc:  0.99896  and loss:  4.643135274294764
test acc:  0.9206
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5104166666666666  ==>  94 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6614583333333334  ==>  260 / 768
optimize layer  7
backward train epoch:  112
test acc:  0.1019
forward train acc:  0.99738  and loss:  6.512534918263555
test acc:  0.9152
forward train acc:  0.99794  and loss:  5.953936057165265
test acc:  0.9145
forward train acc:  0.99792  and loss:  5.572540279943496
test acc:  0.9172
forward train acc:  0.99846  and loss:  4.907469010911882
test acc:  0.916
forward train acc:  0.99872  and loss:  4.698976980987936
test acc:  0.9158
forward train acc:  0.99846  and loss:  4.7349328068085015
test acc:  0.9168
forward train acc:  0.99872  and loss:  4.528894713148475
test acc:  0.9165
forward train acc:  0.99922  and loss:  3.894884018227458
test acc:  0.9161
forward train acc:  0.99878  and loss:  4.107073925435543
test acc:  0.9172
forward train acc:  0.99918  and loss:  3.7358245705254376
test acc:  0.9168
********** reverse layer  7  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5104166666666666  ==>  94 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6614583333333334  ==>  260 / 768
***** skip layer  8
[4, 2, 4, 5, 4, 2, 0, 5, 0, 3, 4, 4, 4, 5]
***** skip layer  9
[4, 2, 4, 5, 4, 2, 0, 5, 0, 2, 4, 4, 4, 5]
***** skip layer  10
[4, 2, 4, 5, 4, 2, 0, 5, 0, 2, 3, 4, 4, 5]
***** skip layer  11
[4, 2, 4, 5, 4, 2, 0, 5, 0, 2, 3, 3, 4, 5]
***** skip layer  12
[4, 2, 4, 5, 4, 2, 0, 5, 0, 2, 3, 3, 3, 5]
***** skip layer  13
[4, 2, 4, 5, 4, 2, 0, 5, 0, 2, 3, 3, 3, 4]
***** skip layer  0
[3, 2, 4, 5, 4, 2, 0, 5, 0, 2, 3, 3, 3, 4]
***** skip layer  1
[3, 1, 4, 5, 4, 2, 0, 5, 0, 2, 3, 3, 3, 4]
***** skip layer  2
[3, 1, 3, 5, 4, 2, 0, 5, 0, 2, 3, 3, 3, 4]
***** skip layer  3
[3, 1, 3, 4, 4, 2, 0, 5, 0, 2, 3, 3, 3, 4]
***** skip layer  4
[3, 1, 3, 4, 3, 2, 0, 5, 0, 2, 3, 3, 3, 4]
***** skip layer  5
[3, 1, 3, 4, 3, 1, 0, 5, 0, 2, 3, 3, 3, 4]
optimize layer  6
backward train epoch:  30
test acc:  0.0884
forward train acc:  0.99884  and loss:  4.021048547234386
test acc:  0.9184
forward train acc:  0.99886  and loss:  4.027899243403226
test acc:  0.917
forward train acc:  0.9986  and loss:  4.1687256321311
test acc:  0.9162
forward train acc:  0.99898  and loss:  3.731911246199161
test acc:  0.9184
forward train acc:  0.99922  and loss:  3.414418787229806
test acc:  0.9173
forward train acc:  0.99924  and loss:  3.276358800008893
test acc:  0.917
forward train acc:  0.99908  and loss:  3.303499926812947
test acc:  0.9175
forward train acc:  0.99938  and loss:  3.1000840696506202
test acc:  0.9173
forward train acc:  0.99926  and loss:  3.1216995948925614
test acc:  0.9164
forward train acc:  0.9991  and loss:  3.170923450961709
test acc:  0.9186
********** reverse layer  6  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5104166666666666  ==>  94 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6614583333333334  ==>  260 / 768
***** skip layer  7
[3, 1, 3, 4, 3, 1, 5, 4, 0, 2, 3, 3, 3, 4]
optimize layer  8
backward train epoch:  144
test acc:  0.0988
forward train acc:  0.99754  and loss:  5.666963952127844
test acc:  0.9134
forward train acc:  0.99838  and loss:  4.544899174943566
test acc:  0.915
forward train acc:  0.9985  and loss:  4.1236825454980135
test acc:  0.9158
forward train acc:  0.99866  and loss:  3.7511283229105175
test acc:  0.9158
forward train acc:  0.9989  and loss:  3.3173684761859477
test acc:  0.9159
forward train acc:  0.99908  and loss:  3.24692012835294
test acc:  0.9163
forward train acc:  0.999  and loss:  3.452225205488503
test acc:  0.9164
forward train acc:  0.99922  and loss:  3.1064813639968634
test acc:  0.9173
forward train acc:  0.99894  and loss:  3.022091348655522
test acc:  0.917
forward train acc:  0.99916  and loss:  2.7957289149053395
test acc:  0.9166
********** reverse layer  8  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5104166666666666  ==>  94 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6614583333333334  ==>  260 / 768
***** skip layer  9
[3, 1, 3, 4, 3, 1, 5, 4, 5, 1, 3, 3, 3, 4]
***** skip layer  10
[3, 1, 3, 4, 3, 1, 5, 4, 5, 1, 2, 3, 3, 4]
***** skip layer  11
[3, 1, 3, 4, 3, 1, 5, 4, 5, 1, 2, 2, 3, 4]
***** skip layer  12
[3, 1, 3, 4, 3, 1, 5, 4, 5, 1, 2, 2, 2, 4]
***** skip layer  13
[3, 1, 3, 4, 3, 1, 5, 4, 5, 1, 2, 2, 2, 3]
***** skip layer  0
[2, 1, 3, 4, 3, 1, 5, 4, 5, 1, 2, 2, 2, 3]
***** skip layer  1
[2, 0, 3, 4, 3, 1, 5, 4, 5, 1, 2, 2, 2, 3]
***** skip layer  2
[2, 0, 2, 4, 3, 1, 5, 4, 5, 1, 2, 2, 2, 3]
***** skip layer  3
[2, 0, 2, 3, 3, 1, 5, 4, 5, 1, 2, 2, 2, 3]
***** skip layer  4
[2, 0, 2, 3, 2, 1, 5, 4, 5, 1, 2, 2, 2, 3]
***** skip layer  5
[2, 0, 2, 3, 2, 0, 5, 4, 5, 1, 2, 2, 2, 3]
***** skip layer  6
[2, 0, 2, 3, 2, 0, 4, 4, 5, 1, 2, 2, 2, 3]
***** skip layer  7
[2, 0, 2, 3, 2, 0, 4, 3, 5, 1, 2, 2, 2, 3]
***** skip layer  8
[2, 0, 2, 3, 2, 0, 4, 3, 4, 1, 2, 2, 2, 3]
***** skip layer  9
[2, 0, 2, 3, 2, 0, 4, 3, 4, 0, 2, 2, 2, 3]
***** skip layer  10
[2, 0, 2, 3, 2, 0, 4, 3, 4, 0, 1, 2, 2, 3]
***** skip layer  11
[2, 0, 2, 3, 2, 0, 4, 3, 4, 0, 1, 1, 2, 3]
***** skip layer  12
[2, 0, 2, 3, 2, 0, 4, 3, 4, 0, 1, 1, 1, 3]
***** skip layer  13
[2, 0, 2, 3, 2, 0, 4, 3, 4, 0, 1, 1, 1, 2]
***** skip layer  0
[1, 0, 2, 3, 2, 0, 4, 3, 4, 0, 1, 1, 1, 2]
optimize layer  1
backward train epoch:  4
test acc:  0.5935
forward train acc:  0.99912  and loss:  2.7337747141718864
test acc:  0.918
forward train acc:  0.99922  and loss:  2.7422618626151234
test acc:  0.9168
forward train acc:  0.99888  and loss:  3.1849806637037545
test acc:  0.9184
forward train acc:  0.99932  and loss:  2.5510433630552143
test acc:  0.9171
forward train acc:  0.9991  and loss:  2.7857321086339653
test acc:  0.9178
forward train acc:  0.99914  and loss:  2.4766756962053478
test acc:  0.9181
forward train acc:  0.9993  and loss:  2.456523383036256
test acc:  0.9196
forward train acc:  0.99958  and loss:  2.092466345988214
test acc:  0.9193
forward train acc:  0.99954  and loss:  2.0902668212074786
test acc:  0.9186
forward train acc:  0.9992  and loss:  2.225242299726233
test acc:  0.9196
********** reverse layer  1  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5104166666666666  ==>  94 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6614583333333334  ==>  260 / 768
***** skip layer  2
[1, 5, 1, 3, 2, 0, 4, 3, 4, 0, 1, 1, 1, 2]
***** skip layer  3
[1, 5, 1, 2, 2, 0, 4, 3, 4, 0, 1, 1, 1, 2]
***** skip layer  4
[1, 5, 1, 2, 1, 0, 4, 3, 4, 0, 1, 1, 1, 2]
optimize layer  5
backward train epoch:  418
test acc:  0.1035
forward train acc:  0.999  and loss:  2.8664674861356616
test acc:  0.9184
forward train acc:  0.99888  and loss:  2.7863874970935285
test acc:  0.9179
forward train acc:  0.99862  and loss:  3.0877140890806913
test acc:  0.9165
forward train acc:  0.99908  and loss:  2.453303054673597
test acc:  0.9172
forward train acc:  0.99922  and loss:  2.370629080105573
test acc:  0.9167
forward train acc:  0.99938  and loss:  2.193176097702235
test acc:  0.9166
forward train acc:  0.99924  and loss:  2.1302486113272607
test acc:  0.9178
forward train acc:  0.99926  and loss:  2.1009906134568155
test acc:  0.9164
forward train acc:  0.99946  and loss:  2.0322240665555
test acc:  0.9178
forward train acc:  0.99946  and loss:  2.094117240048945
test acc:  0.9166
********** reverse layer  5  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5104166666666666  ==>  94 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.65625  ==>  264 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6614583333333334  ==>  260 / 768
***** skip layer  6
[1, 5, 1, 2, 1, 5, 3, 3, 4, 0, 1, 1, 1, 2]
***** skip layer  7
[1, 5, 1, 2, 1, 5, 3, 2, 4, 0, 1, 1, 1, 2]
***** skip layer  8
[1, 5, 1, 2, 1, 5, 3, 2, 3, 0, 1, 1, 1, 2]
optimize layer  9
backward train epoch:  89
test acc:  0.1072
forward train acc:  0.99934  and loss:  2.0669814797583967
test acc:  0.9205
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5104166666666666  ==>  94 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6640625  ==>  258 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6614583333333334  ==>  260 / 768
***** skip layer  10
[1, 5, 1, 2, 1, 5, 3, 2, 3, 0, 0, 1, 1, 2]
***** skip layer  11
[1, 5, 1, 2, 1, 5, 3, 2, 3, 0, 0, 0, 1, 2]
***** skip layer  12
[1, 5, 1, 2, 1, 5, 3, 2, 3, 0, 0, 0, 0, 2]
***** skip layer  13
[1, 5, 1, 2, 1, 5, 3, 2, 3, 0, 0, 0, 0, 1]
***** skip layer  0
[0, 5, 1, 2, 1, 5, 3, 2, 3, 0, 0, 0, 0, 1]
***** skip layer  1
[0, 4, 1, 2, 1, 5, 3, 2, 3, 0, 0, 0, 0, 1]
***** skip layer  2
[0, 4, 0, 2, 1, 5, 3, 2, 3, 0, 0, 0, 0, 1]
***** skip layer  3
[0, 4, 0, 1, 1, 5, 3, 2, 3, 0, 0, 0, 0, 1]
***** skip layer  4
[0, 4, 0, 1, 0, 5, 3, 2, 3, 0, 0, 0, 0, 1]
***** skip layer  5
[0, 4, 0, 1, 0, 4, 3, 2, 3, 0, 0, 0, 0, 1]
***** skip layer  6
[0, 4, 0, 1, 0, 4, 2, 2, 3, 0, 0, 0, 0, 1]
***** skip layer  7
[0, 4, 0, 1, 0, 4, 2, 1, 3, 0, 0, 0, 0, 1]
***** skip layer  8
[0, 4, 0, 1, 0, 4, 2, 1, 2, 0, 0, 0, 0, 1]
optimize layer  9
backward train epoch:  141
test acc:  0.1041
forward train acc:  0.99938  and loss:  2.0382173208054155
test acc:  0.9168
forward train acc:  0.99918  and loss:  2.088923948118463
test acc:  0.9169
forward train acc:  0.9993  and loss:  2.002304640831426
test acc:  0.9189
forward train acc:  0.99936  and loss:  1.8439302677288651
test acc:  0.9181
forward train acc:  0.99938  and loss:  1.8491187924519181
test acc:  0.9198
forward train acc:  0.9994  and loss:  1.8461612893734127
test acc:  0.9192
forward train acc:  0.99944  and loss:  1.6609436608850956
test acc:  0.9179
forward train acc:  0.9996  and loss:  1.5463974887970835
test acc:  0.9181
forward train acc:  0.9994  and loss:  1.627667751396075
test acc:  0.9208
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5104166666666666  ==>  94 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.671875  ==>  252 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6614583333333334  ==>  260 / 768
optimize layer  10
backward train epoch:  91
test acc:  0.1
forward train acc:  0.99926  and loss:  1.9287240186240524
test acc:  0.9178
forward train acc:  0.99922  and loss:  1.9167773458175361
test acc:  0.917
forward train acc:  0.99928  and loss:  1.8711824368219823
test acc:  0.9156
forward train acc:  0.99928  and loss:  1.8107884963974357
test acc:  0.9176
forward train acc:  0.99954  and loss:  1.4437511311843991
test acc:  0.918
forward train acc:  0.9994  and loss:  1.7208861685357988
test acc:  0.9168
forward train acc:  0.99928  and loss:  1.6870058360509574
test acc:  0.9166
forward train acc:  0.9996  and loss:  1.391865716315806
test acc:  0.9172
forward train acc:  0.9996  and loss:  1.4081558608449996
test acc:  0.9173
forward train acc:  0.99936  and loss:  1.4588418735656887
test acc:  0.9182
********** reverse layer  10  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5104166666666666  ==>  94 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.671875  ==>  252 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6614583333333334  ==>  260 / 768
optimize layer  11
backward train epoch:  363
test acc:  0.1
forward train acc:  0.99934  and loss:  1.7285598078742623
test acc:  0.9183
forward train acc:  0.99922  and loss:  1.962423802120611
test acc:  0.9161
forward train acc:  0.99906  and loss:  2.059171260218136
test acc:  0.9172
forward train acc:  0.99952  and loss:  1.4165163432480767
test acc:  0.9181
forward train acc:  0.99946  and loss:  1.4275946194538847
test acc:  0.9172
forward train acc:  0.99942  and loss:  1.5277862714137882
test acc:  0.9175
forward train acc:  0.9996  and loss:  1.4277086239308119
test acc:  0.9179
forward train acc:  0.99968  and loss:  1.1413872052216902
test acc:  0.9181
forward train acc:  0.9994  and loss:  1.4780608885921538
test acc:  0.9169
forward train acc:  0.99952  and loss:  1.3936816011555493
test acc:  0.9176
********** reverse layer  11  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5104166666666666  ==>  94 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.671875  ==>  252 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6614583333333334  ==>  260 / 768
optimize layer  12
backward train epoch:  28
test acc:  0.1
forward train acc:  0.1  and loss:  1240.193163394928
test acc:  0.1
forward train acc:  0.1  and loss:  1202.336753129959
test acc:  0.1
forward train acc:  0.1  and loss:  1168.2867786884308
test acc:  0.1
forward train acc:  0.1  and loss:  1145.0423250198364
test acc:  0.1
forward train acc:  0.1  and loss:  1130.344874382019
test acc:  0.1
forward train acc:  0.1  and loss:  1116.275809288025
test acc:  0.1
forward train acc:  0.1  and loss:  1102.7309725284576
test acc:  0.1
forward train acc:  0.1  and loss:  1092.9437308311462
test acc:  0.1
forward train acc:  0.1  and loss:  1086.5722560882568
test acc:  0.1
forward train acc:  0.1  and loss:  1080.3171091079712
test acc:  0.1
********** reverse layer  12  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5104166666666666  ==>  94 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.671875  ==>  252 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6614583333333334  ==>  260 / 768
***** skip layer  13
[0, 4, 0, 1, 0, 4, 2, 1, 2, 0, 5, 5, 5, 0]
optimize layer  0
backward train epoch:  349
test acc:  0.0933
forward train acc:  0.80374  and loss:  321.9685608148575
test acc:  0.8068
forward train acc:  0.86198  and loss:  182.49434451758862
test acc:  0.8262
forward train acc:  0.887  and loss:  147.05288107693195
test acc:  0.8411
forward train acc:  0.89926  and loss:  128.06316097080708
test acc:  0.8446
forward train acc:  0.9071  and loss:  117.63700442016125
test acc:  0.8491
forward train acc:  0.91198  and loss:  110.7679990530014
test acc:  0.852
forward train acc:  0.91912  and loss:  101.7761986553669
test acc:  0.8575
forward train acc:  0.92128  and loss:  98.21754487603903
test acc:  0.8596
forward train acc:  0.92536  and loss:  93.6237705796957
test acc:  0.8608
forward train acc:  0.92804  and loss:  89.80707036703825
test acc:  0.8627
********** reverse layer  0  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5104166666666666  ==>  94 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.671875  ==>  252 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6614583333333334  ==>  260 / 768
***** skip layer  1
[5, 3, 0, 1, 0, 4, 2, 1, 2, 0, 5, 5, 5, 0]
optimize layer  2
backward train epoch:  469
test acc:  0.0853
forward train acc:  0.92992  and loss:  87.13472833484411
test acc:  0.8863
forward train acc:  0.95978  and loss:  49.960340443998575
test acc:  0.8924
forward train acc:  0.97016  and loss:  37.152169873937964
test acc:  0.899
forward train acc:  0.97742  and loss:  30.13489674590528
test acc:  0.8998
forward train acc:  0.97896  and loss:  27.307379136793315
test acc:  0.9009
forward train acc:  0.98182  and loss:  23.66610355116427
test acc:  0.9033
forward train acc:  0.98352  and loss:  21.871544804424047
test acc:  0.9029
forward train acc:  0.98522  and loss:  19.402879467699677
test acc:  0.9041
forward train acc:  0.98526  and loss:  18.361809377092868
test acc:  0.9031
forward train acc:  0.98562  and loss:  18.36432090261951
test acc:  0.9041
********** reverse layer  2  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5104166666666666  ==>  94 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.671875  ==>  252 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6614583333333334  ==>  260 / 768
***** skip layer  3
[5, 3, 5, 0, 0, 4, 2, 1, 2, 0, 5, 5, 5, 0]
optimize layer  4
backward train epoch:  77
test acc:  0.1347
forward train acc:  0.99748  and loss:  4.344944950891659
test acc:  0.9174
forward train acc:  0.9979  and loss:  3.4141551614739
test acc:  0.9186
forward train acc:  0.99856  and loss:  2.6287003410980105
test acc:  0.9174
forward train acc:  0.99876  and loss:  2.3875929582864046
test acc:  0.9172
forward train acc:  0.99892  and loss:  2.0598362726159394
test acc:  0.9178
forward train acc:  0.9991  and loss:  1.9297333371359855
test acc:  0.9191
forward train acc:  0.99914  and loss:  1.8382907265331596
test acc:  0.918
forward train acc:  0.99886  and loss:  2.1509897452779114
test acc:  0.9188
forward train acc:  0.99908  and loss:  2.0389238237403333
test acc:  0.9195
forward train acc:  0.99916  and loss:  1.834770166897215
test acc:  0.9188
********** reverse layer  4  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5104166666666666  ==>  94 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.671875  ==>  252 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6614583333333334  ==>  260 / 768
***** skip layer  5
[5, 3, 5, 0, 5, 3, 2, 1, 2, 0, 5, 5, 5, 0]
***** skip layer  6
[5, 3, 5, 0, 5, 3, 1, 1, 2, 0, 5, 5, 5, 0]
***** skip layer  7
[5, 3, 5, 0, 5, 3, 1, 0, 2, 0, 5, 5, 5, 0]
***** skip layer  8
[5, 3, 5, 0, 5, 3, 1, 0, 1, 0, 5, 5, 5, 0]
optimize layer  9
backward train epoch:  50
test acc:  0.1128
forward train acc:  0.99926  and loss:  1.8103583089541644
test acc:  0.9179
forward train acc:  0.99942  and loss:  1.4554644734598696
test acc:  0.918
forward train acc:  0.99914  and loss:  1.8009517672471702
test acc:  0.916
forward train acc:  0.99924  and loss:  1.6822858359664679
test acc:  0.9193
forward train acc:  0.99934  and loss:  1.37482218304649
test acc:  0.9185
forward train acc:  0.99938  and loss:  1.4073010187130421
test acc:  0.9194
forward train acc:  0.99962  and loss:  1.1960484171286225
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5104166666666666  ==>  94 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6796875  ==>  246 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6614583333333334  ==>  260 / 768
***** skip layer  10
[5, 3, 5, 0, 5, 3, 1, 0, 1, 0, 4, 5, 5, 0]
***** skip layer  11
[5, 3, 5, 0, 5, 3, 1, 0, 1, 0, 4, 4, 5, 0]
***** skip layer  12
[5, 3, 5, 0, 5, 3, 1, 0, 1, 0, 4, 4, 4, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.1111
forward train acc:  0.99922  and loss:  6.245303462725133
test acc:  0.9171
forward train acc:  0.9991  and loss:  5.425192526075989
test acc:  0.9199
forward train acc:  0.99926  and loss:  4.8732160232029855
test acc:  0.9173
forward train acc:  0.99934  and loss:  4.239902578294277
test acc:  0.9204
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5104166666666666  ==>  94 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6796875  ==>  246 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  0
[4, 3, 5, 0, 5, 3, 1, 0, 1, 0, 4, 4, 4, 0]
***** skip layer  1
[4, 2, 5, 0, 5, 3, 1, 0, 1, 0, 4, 4, 4, 0]
***** skip layer  2
[4, 2, 4, 0, 5, 3, 1, 0, 1, 0, 4, 4, 4, 0]
optimize layer  3
backward train epoch:  1
test acc:  0.8991
forward train acc:  0.99898  and loss:  4.513206923380494
test acc:  0.918
forward train acc:  0.99912  and loss:  4.120761224068701
test acc:  0.918
forward train acc:  0.99872  and loss:  3.997414614073932
test acc:  0.9183
forward train acc:  0.99904  and loss:  3.6522244601510465
test acc:  0.918
forward train acc:  0.99926  and loss:  3.319188351277262
test acc:  0.9184
forward train acc:  0.9994  and loss:  3.044070334173739
test acc:  0.9175
forward train acc:  0.99926  and loss:  3.1043954754713923
test acc:  0.918
forward train acc:  0.99928  and loss:  2.9246599446050823
test acc:  0.9181
forward train acc:  0.99934  and loss:  2.8095882553607225
test acc:  0.9189
forward train acc:  0.9995  and loss:  2.5758542297407985
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5208333333333334  ==>  92 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6796875  ==>  246 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  4
[4, 2, 4, 0, 4, 3, 1, 0, 1, 0, 4, 4, 4, 0]
***** skip layer  5
[4, 2, 4, 0, 4, 2, 1, 0, 1, 0, 4, 4, 4, 0]
***** skip layer  6
[4, 2, 4, 0, 4, 2, 0, 0, 1, 0, 4, 4, 4, 0]
optimize layer  7
backward train epoch:  141
test acc:  0.1118
forward train acc:  0.99732  and loss:  5.637473234906793
test acc:  0.9136
forward train acc:  0.99806  and loss:  4.2903031492605805
test acc:  0.9169
forward train acc:  0.99854  and loss:  3.8271225651260465
test acc:  0.9156
forward train acc:  0.99834  and loss:  4.012013131054118
test acc:  0.9161
forward train acc:  0.99846  and loss:  3.55175877478905
test acc:  0.9153
forward train acc:  0.999  and loss:  2.799420830560848
test acc:  0.916
forward train acc:  0.99898  and loss:  2.906546703306958
test acc:  0.9167
forward train acc:  0.99902  and loss:  2.825652946019545
test acc:  0.9175
forward train acc:  0.99882  and loss:  3.0978859069291502
test acc:  0.9181
forward train acc:  0.99914  and loss:  2.758485913509503
test acc:  0.9187
********** reverse layer  7  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5208333333333334  ==>  92 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6796875  ==>  246 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  8
[4, 2, 4, 0, 4, 2, 0, 5, 0, 0, 4, 4, 4, 0]
optimize layer  9
backward train epoch:  84
test acc:  0.1104
forward train acc:  0.9993  and loss:  2.553694577421993
test acc:  0.9179
forward train acc:  0.99928  and loss:  2.3760268627665937
test acc:  0.9176
forward train acc:  0.99942  and loss:  2.1503507615998387
test acc:  0.9181
forward train acc:  0.99936  and loss:  2.0436297049745917
test acc:  0.9184
forward train acc:  0.99924  and loss:  2.1653970521874726
test acc:  0.9175
forward train acc:  0.99956  and loss:  1.7644096573349088
test acc:  0.9194
forward train acc:  0.9993  and loss:  1.9864122518338263
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5208333333333334  ==>  92 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6875  ==>  240 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  10
[4, 2, 4, 0, 4, 2, 0, 5, 0, 0, 3, 4, 4, 0]
***** skip layer  11
[4, 2, 4, 0, 4, 2, 0, 5, 0, 0, 3, 3, 4, 0]
***** skip layer  12
[4, 2, 4, 0, 4, 2, 0, 5, 0, 0, 3, 3, 3, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.0993
forward train acc:  0.98958  and loss:  46.66953883320093
test acc:  0.9158
forward train acc:  0.99902  and loss:  41.74084259569645
test acc:  0.9176
forward train acc:  0.99886  and loss:  38.29121496528387
test acc:  0.9174
forward train acc:  0.99916  and loss:  35.29000290483236
test acc:  0.9174
forward train acc:  0.99936  and loss:  33.32441654801369
test acc:  0.9175
forward train acc:  0.99956  and loss:  31.712515875697136
test acc:  0.9182
forward train acc:  0.9995  and loss:  30.012193970382214
test acc:  0.9176
forward train acc:  0.99928  and loss:  29.032533176243305
test acc:  0.918
forward train acc:  0.99944  and loss:  28.24741993844509
test acc:  0.9177
forward train acc:  0.99956  and loss:  27.308536406606436
test acc:  0.9192
********** reverse layer  13  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5208333333333334  ==>  92 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6875  ==>  240 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  0
[3, 2, 4, 0, 4, 2, 0, 5, 0, 0, 3, 3, 3, 5]
***** skip layer  1
[3, 1, 4, 0, 4, 2, 0, 5, 0, 0, 3, 3, 3, 5]
***** skip layer  2
[3, 1, 3, 0, 4, 2, 0, 5, 0, 0, 3, 3, 3, 5]
optimize layer  3
backward train epoch:  2
test acc:  0.8694
forward train acc:  0.99904  and loss:  2.5791435413993895
test acc:  0.9174
forward train acc:  0.99894  and loss:  2.720031914068386
test acc:  0.9166
forward train acc:  0.99914  and loss:  2.288854284444824
test acc:  0.919
forward train acc:  0.99916  and loss:  2.2823456844780594
test acc:  0.9179
forward train acc:  0.99898  and loss:  2.180377774173394
test acc:  0.9191
forward train acc:  0.99926  and loss:  1.9560452857986093
test acc:  0.9197
forward train acc:  0.99912  and loss:  2.0639290150720626
test acc:  0.9191
forward train acc:  0.99942  and loss:  1.8323939139954746
test acc:  0.9197
forward train acc:  0.99952  and loss:  1.6077835611067712
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.53125  ==>  90 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6875  ==>  240 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  4
[3, 1, 3, 0, 3, 2, 0, 5, 0, 0, 3, 3, 3, 5]
***** skip layer  5
[3, 1, 3, 0, 3, 1, 0, 5, 0, 0, 3, 3, 3, 5]
optimize layer  6
backward train epoch:  12
test acc:  0.2499
forward train acc:  0.99898  and loss:  2.2171427700668573
test acc:  0.9179
forward train acc:  0.9988  and loss:  2.34049730328843
test acc:  0.915
forward train acc:  0.9991  and loss:  2.093808980192989
test acc:  0.9161
forward train acc:  0.9994  and loss:  1.645393179380335
test acc:  0.918
forward train acc:  0.99942  and loss:  1.724833833053708
test acc:  0.9188
forward train acc:  0.99944  and loss:  1.5802202720660716
test acc:  0.9176
forward train acc:  0.99912  and loss:  1.916900941869244
test acc:  0.9194
forward train acc:  0.99926  and loss:  1.8541491696378216
test acc:  0.9188
forward train acc:  0.99948  and loss:  1.4765076090116054
test acc:  0.9192
forward train acc:  0.9994  and loss:  1.5363249870715663
test acc:  0.9187
********** reverse layer  6  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.53125  ==>  90 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6875  ==>  240 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  7
[3, 1, 3, 0, 3, 1, 5, 4, 0, 0, 3, 3, 3, 5]
optimize layer  8
backward train epoch:  69
test acc:  0.1209
forward train acc:  0.98496  and loss:  23.619114523287863
test acc:  0.9098
forward train acc:  0.9944  and loss:  8.92261273576878
test acc:  0.9131
forward train acc:  0.99574  and loss:  7.036246183793992
test acc:  0.9137
forward train acc:  0.99674  and loss:  5.542216937057674
test acc:  0.9141
forward train acc:  0.9974  and loss:  4.71882277331315
test acc:  0.9142
forward train acc:  0.99774  and loss:  4.422805246897042
test acc:  0.9155
forward train acc:  0.9977  and loss:  4.317042744252831
test acc:  0.9165
forward train acc:  0.99776  and loss:  4.063838642789051
test acc:  0.9138
forward train acc:  0.99792  and loss:  4.109123923582956
test acc:  0.9154
forward train acc:  0.99778  and loss:  3.827005280414596
test acc:  0.915
********** reverse layer  8  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.53125  ==>  90 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6875  ==>  240 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
optimize layer  9
backward train epoch:  173
test acc:  0.1094
forward train acc:  0.99944  and loss:  1.5732551154214889
test acc:  0.9225
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.53125  ==>  90 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6953125  ==>  234 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  10
[3, 1, 3, 0, 3, 1, 5, 4, 5, 0, 2, 3, 3, 5]
***** skip layer  11
[3, 1, 3, 0, 3, 1, 5, 4, 5, 0, 2, 2, 3, 5]
***** skip layer  12
[3, 1, 3, 0, 3, 1, 5, 4, 5, 0, 2, 2, 2, 5]
***** skip layer  13
[3, 1, 3, 0, 3, 1, 5, 4, 5, 0, 2, 2, 2, 4]
***** skip layer  0
[2, 1, 3, 0, 3, 1, 5, 4, 5, 0, 2, 2, 2, 4]
***** skip layer  1
[2, 0, 3, 0, 3, 1, 5, 4, 5, 0, 2, 2, 2, 4]
***** skip layer  2
[2, 0, 2, 0, 3, 1, 5, 4, 5, 0, 2, 2, 2, 4]
optimize layer  3
backward train epoch:  2
test acc:  0.8706
forward train acc:  0.9988  and loss:  2.328283889684826
test acc:  0.9192
forward train acc:  0.99848  and loss:  2.680420274962671
test acc:  0.9193
forward train acc:  0.99864  and loss:  2.5802064776653424
test acc:  0.9183
forward train acc:  0.99894  and loss:  2.0871790630044416
test acc:  0.9186
forward train acc:  0.99906  and loss:  2.0274348128587008
test acc:  0.9204
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5416666666666666  ==>  88 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.6953125  ==>  234 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  4
[2, 0, 2, 0, 2, 1, 5, 4, 5, 0, 2, 2, 2, 4]
***** skip layer  5
[2, 0, 2, 0, 2, 0, 5, 4, 5, 0, 2, 2, 2, 4]
***** skip layer  6
[2, 0, 2, 0, 2, 0, 4, 4, 5, 0, 2, 2, 2, 4]
***** skip layer  7
[2, 0, 2, 0, 2, 0, 4, 3, 5, 0, 2, 2, 2, 4]
***** skip layer  8
[2, 0, 2, 0, 2, 0, 4, 3, 4, 0, 2, 2, 2, 4]
optimize layer  9
backward train epoch:  9
test acc:  0.3859
forward train acc:  0.99902  and loss:  1.77776031231042
test acc:  0.9181
forward train acc:  0.99894  and loss:  2.0073026512982324
test acc:  0.9169
forward train acc:  0.99894  and loss:  2.0192899868125096
test acc:  0.918
forward train acc:  0.99892  and loss:  1.7718200507806614
test acc:  0.9184
forward train acc:  0.99938  and loss:  1.410994250443764
test acc:  0.9176
forward train acc:  0.99904  and loss:  1.766282711760141
test acc:  0.9189
forward train acc:  0.99942  and loss:  1.3524658631067723
test acc:  0.9193
forward train acc:  0.99942  and loss:  1.3815189353190362
test acc:  0.9194
forward train acc:  0.99934  and loss:  1.3388062914600596
test acc:  0.9203
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5625  ==>  42 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5416666666666666  ==>  88 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.703125  ==>  228 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  10
[2, 0, 2, 0, 2, 0, 4, 3, 4, 0, 1, 2, 2, 4]
***** skip layer  11
[2, 0, 2, 0, 2, 0, 4, 3, 4, 0, 1, 1, 2, 4]
***** skip layer  12
[2, 0, 2, 0, 2, 0, 4, 3, 4, 0, 1, 1, 1, 4]
***** skip layer  13
[2, 0, 2, 0, 2, 0, 4, 3, 4, 0, 1, 1, 1, 3]
***** skip layer  0
[1, 0, 2, 0, 2, 0, 4, 3, 4, 0, 1, 1, 1, 3]
optimize layer  1
backward train epoch:  47
test acc:  0.0971
forward train acc:  0.99852  and loss:  2.5435983322095126
test acc:  0.9189
forward train acc:  0.9986  and loss:  2.5623915786854923
test acc:  0.9179
forward train acc:  0.99872  and loss:  2.118953567929566
test acc:  0.9156
forward train acc:  0.9988  and loss:  2.1074126969324425
test acc:  0.919
forward train acc:  0.99926  and loss:  1.5559234546963125
test acc:  0.9209
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5416666666666666  ==>  88 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.703125  ==>  228 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  2
[1, 0, 1, 0, 2, 0, 4, 3, 4, 0, 1, 1, 1, 3]
optimize layer  3
backward train epoch:  2
test acc:  0.8628
forward train acc:  0.99828  and loss:  2.914124763919972
test acc:  0.916
forward train acc:  0.99824  and loss:  2.874261392164044
test acc:  0.9169
forward train acc:  0.99832  and loss:  2.61722933256533
test acc:  0.9157
forward train acc:  0.99872  and loss:  2.334924518945627
test acc:  0.9171
forward train acc:  0.99886  and loss:  2.1426551822805777
test acc:  0.9164
forward train acc:  0.99882  and loss:  2.106006951886229
test acc:  0.917
forward train acc:  0.99914  and loss:  1.6272834200644866
test acc:  0.9166
forward train acc:  0.99906  and loss:  1.6389059122884646
test acc:  0.9169
forward train acc:  0.99894  and loss:  1.9801772728096694
test acc:  0.9176
forward train acc:  0.99916  and loss:  1.6069824120495468
test acc:  0.9157
********** reverse layer  3  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5416666666666666  ==>  88 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.703125  ==>  228 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  4
[1, 0, 1, 5, 1, 0, 4, 3, 4, 0, 1, 1, 1, 3]
optimize layer  5
backward train epoch:  96
test acc:  0.1077
forward train acc:  0.99796  and loss:  3.39916456816718
test acc:  0.9181
forward train acc:  0.9983  and loss:  2.5943212785059586
test acc:  0.9173
forward train acc:  0.9984  and loss:  2.5826847968855873
test acc:  0.9164
forward train acc:  0.99882  and loss:  2.2637550241779536
test acc:  0.917
forward train acc:  0.99872  and loss:  2.159284292254597
test acc:  0.9163
forward train acc:  0.99908  and loss:  1.6370337137486786
test acc:  0.9172
forward train acc:  0.99882  and loss:  1.9664548783330247
test acc:  0.9172
forward train acc:  0.9994  and loss:  1.565735595417209
test acc:  0.9178
forward train acc:  0.99916  and loss:  1.7113909287145361
test acc:  0.9184
forward train acc:  0.99912  and loss:  1.628376640030183
test acc:  0.9191
********** reverse layer  5  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5416666666666666  ==>  88 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.703125  ==>  228 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  6
[1, 0, 1, 5, 1, 5, 3, 3, 4, 0, 1, 1, 1, 3]
***** skip layer  7
[1, 0, 1, 5, 1, 5, 3, 2, 4, 0, 1, 1, 1, 3]
***** skip layer  8
[1, 0, 1, 5, 1, 5, 3, 2, 3, 0, 1, 1, 1, 3]
optimize layer  9
backward train epoch:  262
test acc:  0.1006
forward train acc:  0.99912  and loss:  1.6594245440792292
test acc:  0.9171
forward train acc:  0.99912  and loss:  1.5926582975080237
test acc:  0.9185
forward train acc:  0.99896  and loss:  1.642987908795476
test acc:  0.9179
forward train acc:  0.9991  and loss:  1.6451693440321833
test acc:  0.9194
forward train acc:  0.99914  and loss:  1.7132248636335135
test acc:  0.921
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5416666666666666  ==>  88 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  10
[1, 0, 1, 5, 1, 5, 3, 2, 3, 0, 0, 1, 1, 3]
***** skip layer  11
[1, 0, 1, 5, 1, 5, 3, 2, 3, 0, 0, 0, 1, 3]
***** skip layer  12
[1, 0, 1, 5, 1, 5, 3, 2, 3, 0, 0, 0, 0, 3]
***** skip layer  13
[1, 0, 1, 5, 1, 5, 3, 2, 3, 0, 0, 0, 0, 2]
***** skip layer  0
[0, 0, 1, 5, 1, 5, 3, 2, 3, 0, 0, 0, 0, 2]
optimize layer  1
backward train epoch:  140
test acc:  0.097
forward train acc:  0.9991  and loss:  1.7900679359445348
test acc:  0.9169
forward train acc:  0.9986  and loss:  2.2358087812317535
test acc:  0.916
forward train acc:  0.99872  and loss:  2.196353302220814
test acc:  0.9156
forward train acc:  0.99896  and loss:  1.6120880306698382
test acc:  0.916
forward train acc:  0.99906  and loss:  1.6707771223736927
test acc:  0.9172
forward train acc:  0.99914  and loss:  1.6751304874196649
test acc:  0.9189
forward train acc:  0.9991  and loss:  1.5774480636464432
test acc:  0.9173
forward train acc:  0.99936  and loss:  1.2805586613249034
test acc:  0.9175
forward train acc:  0.99936  and loss:  1.4229244336020201
test acc:  0.9182
forward train acc:  0.99926  and loss:  1.3157077374635264
test acc:  0.9176
********** reverse layer  1  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5416666666666666  ==>  88 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  2
[0, 5, 0, 5, 1, 5, 3, 2, 3, 0, 0, 0, 0, 2]
***** skip layer  3
[0, 5, 0, 4, 1, 5, 3, 2, 3, 0, 0, 0, 0, 2]
***** skip layer  4
[0, 5, 0, 4, 0, 5, 3, 2, 3, 0, 0, 0, 0, 2]
***** skip layer  5
[0, 5, 0, 4, 0, 4, 3, 2, 3, 0, 0, 0, 0, 2]
***** skip layer  6
[0, 5, 0, 4, 0, 4, 2, 2, 3, 0, 0, 0, 0, 2]
***** skip layer  7
[0, 5, 0, 4, 0, 4, 2, 1, 3, 0, 0, 0, 0, 2]
***** skip layer  8
[0, 5, 0, 4, 0, 4, 2, 1, 2, 0, 0, 0, 0, 2]
optimize layer  9
backward train epoch:  14
test acc:  0.2366
forward train acc:  0.99928  and loss:  1.4361591197084635
test acc:  0.9169
forward train acc:  0.9991  and loss:  1.6911828748998232
test acc:  0.9178
forward train acc:  0.99932  and loss:  1.3028534379554912
test acc:  0.916
forward train acc:  0.9993  and loss:  1.377458933624439
test acc:  0.9184
forward train acc:  0.9993  and loss:  1.3227805941132829
test acc:  0.9184
forward train acc:  0.99958  and loss:  1.0611974755302072
test acc:  0.9182
forward train acc:  0.99938  and loss:  1.152786695631221
test acc:  0.9185
forward train acc:  0.99936  and loss:  1.2829703387105837
test acc:  0.918
forward train acc:  0.99946  and loss:  1.0378401761408895
test acc:  0.9181
forward train acc:  0.99956  and loss:  1.0019274742808193
test acc:  0.9192
********** reverse layer  9  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5416666666666666  ==>  88 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
optimize layer  10
backward train epoch:  254
test acc:  0.1101
forward train acc:  0.9991  and loss:  1.395268586697057
test acc:  0.9168
forward train acc:  0.99928  and loss:  1.3708873548312113
test acc:  0.919
forward train acc:  0.99906  and loss:  1.5603049941710196
test acc:  0.9183
forward train acc:  0.99924  and loss:  1.2007244149572216
test acc:  0.9194
forward train acc:  0.99942  and loss:  1.0938966521061957
test acc:  0.9172
forward train acc:  0.9995  and loss:  0.9688040341134183
test acc:  0.9192
forward train acc:  0.9996  and loss:  0.9391699762200005
test acc:  0.9189
forward train acc:  0.9996  and loss:  0.9876356823951937
test acc:  0.9193
forward train acc:  0.99958  and loss:  0.9648975171148777
test acc:  0.9184
forward train acc:  0.9997  and loss:  0.745712963573169
test acc:  0.9187
********** reverse layer  10  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5416666666666666  ==>  88 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7526041666666666  ==>  190 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
optimize layer  11
backward train epoch:  113
test acc:  0.1282
forward train acc:  0.99954  and loss:  1.0202486086054705
test acc:  0.9186
forward train acc:  0.99954  and loss:  1.017250718025025
test acc:  0.9164
forward train acc:  0.9991  and loss:  1.5908671402721666
test acc:  0.9167
forward train acc:  0.99916  and loss:  1.5077337144175544
test acc:  0.9186
forward train acc:  0.9994  and loss:  1.0438951939577237
test acc:  0.9176
forward train acc:  0.9996  and loss:  0.9655467173433863
test acc:  0.9175
forward train acc:  0.99954  and loss:  1.0430670726345852
test acc:  0.9194
forward train acc:  0.9996  and loss:  0.857222632272169
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5416666666666666  ==>  88 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7604166666666666  ==>  184 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
optimize layer  12
backward train epoch:  175
test acc:  0.1
forward train acc:  0.1  and loss:  1460.1166596412659
test acc:  0.1
forward train acc:  0.1  and loss:  1408.4770154953003
test acc:  0.1
forward train acc:  0.1  and loss:  1362.2045469284058
test acc:  0.1
forward train acc:  0.1  and loss:  1330.3345568180084
test acc:  0.1
forward train acc:  0.1  and loss:  1310.0703485012054
test acc:  0.1
forward train acc:  0.1  and loss:  1290.6902322769165
test acc:  0.1
forward train acc:  0.1  and loss:  1271.6371505260468
test acc:  0.1
forward train acc:  0.1  and loss:  1257.8733446598053
test acc:  0.1
forward train acc:  0.1  and loss:  1248.5744724273682
test acc:  0.1
forward train acc:  0.1  and loss:  1239.706614971161
test acc:  0.1
********** reverse layer  12  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5416666666666666  ==>  88 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7604166666666666  ==>  184 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  13
[0, 5, 0, 4, 0, 4, 2, 1, 2, 5, 5, 0, 5, 1]
optimize layer  0
backward train epoch:  125
test acc:  0.1009
forward train acc:  0.99936  and loss:  1.6325009781867266
test acc:  0.9185
forward train acc:  0.99946  and loss:  1.1496421832125634
test acc:  0.9174
forward train acc:  0.99904  and loss:  1.7460070499219
test acc:  0.9154
forward train acc:  0.99928  and loss:  1.51540818286594
test acc:  0.9181
forward train acc:  0.9993  and loss:  1.247585722943768
test acc:  0.9198
forward train acc:  0.99926  and loss:  1.2118172770133242
test acc:  0.9183
forward train acc:  0.99914  and loss:  1.3176797558553517
test acc:  0.9182
forward train acc:  0.99956  and loss:  0.9928533235215582
test acc:  0.9195
forward train acc:  0.99968  and loss:  0.8907382861361839
test acc:  0.9188
forward train acc:  0.99964  and loss:  0.9255803974810988
test acc:  0.9182
********** reverse layer  0  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5416666666666666  ==>  88 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7604166666666666  ==>  184 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  1
[5, 4, 0, 4, 0, 4, 2, 1, 2, 5, 5, 0, 5, 1]
optimize layer  2
backward train epoch:  109
test acc:  0.1262
forward train acc:  0.999  and loss:  1.7199464884470217
test acc:  0.917
forward train acc:  0.99898  and loss:  1.685339941876009
test acc:  0.9162
forward train acc:  0.99934  and loss:  1.3005726973642595
test acc:  0.9147
forward train acc:  0.99942  and loss:  1.0994714617845602
test acc:  0.9144
forward train acc:  0.9992  and loss:  1.539852435060311
test acc:  0.9175
forward train acc:  0.99936  and loss:  1.1017132776323706
test acc:  0.9175
forward train acc:  0.99936  and loss:  1.202828079869505
test acc:  0.9167
forward train acc:  0.9993  and loss:  1.3274627123610117
test acc:  0.9178
forward train acc:  0.9994  and loss:  1.1323899049893953
test acc:  0.916
forward train acc:  0.99942  and loss:  1.0697607088950463
test acc:  0.9159
********** reverse layer  2  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5416666666666666  ==>  88 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7604166666666666  ==>  184 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  3
[5, 4, 5, 3, 0, 4, 2, 1, 2, 5, 5, 0, 5, 1]
optimize layer  4
backward train epoch:  14
test acc:  0.2034
forward train acc:  0.99874  and loss:  2.141479993413668
test acc:  0.9156
forward train acc:  0.9985  and loss:  2.4436023001908325
test acc:  0.9171
forward train acc:  0.99882  and loss:  1.7667763879871927
test acc:  0.916
forward train acc:  0.99912  and loss:  1.5546452097478323
test acc:  0.9188
forward train acc:  0.99926  and loss:  1.2569312736741267
test acc:  0.9179
forward train acc:  0.99934  and loss:  1.2243861409951933
test acc:  0.9182
forward train acc:  0.99918  and loss:  1.2690054206759669
test acc:  0.9186
forward train acc:  0.99954  and loss:  1.187146139156539
test acc:  0.9188
forward train acc:  0.9995  and loss:  0.9498527043033391
test acc:  0.9172
forward train acc:  0.99954  and loss:  0.9390728309517726
test acc:  0.918
********** reverse layer  4  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5416666666666666  ==>  88 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7604166666666666  ==>  184 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  5
[5, 4, 5, 3, 5, 3, 2, 1, 2, 5, 5, 0, 5, 1]
***** skip layer  6
[5, 4, 5, 3, 5, 3, 1, 1, 2, 5, 5, 0, 5, 1]
***** skip layer  7
[5, 4, 5, 3, 5, 3, 1, 0, 2, 5, 5, 0, 5, 1]
***** skip layer  8
[5, 4, 5, 3, 5, 3, 1, 0, 1, 5, 5, 0, 5, 1]
***** skip layer  9
[5, 4, 5, 3, 5, 3, 1, 0, 1, 4, 5, 0, 5, 1]
***** skip layer  10
[5, 4, 5, 3, 5, 3, 1, 0, 1, 4, 4, 0, 5, 1]
optimize layer  11
backward train epoch:  333
test acc:  0.1014
forward train acc:  0.99948  and loss:  0.8958443715237081
test acc:  0.9168
forward train acc:  0.99934  and loss:  1.1539329393417574
test acc:  0.9158
forward train acc:  0.99938  and loss:  1.0384801365435123
test acc:  0.9185
forward train acc:  0.99912  and loss:  1.2461222722777165
test acc:  0.9178
forward train acc:  0.99956  and loss:  0.8591229277662933
test acc:  0.918
forward train acc:  0.99942  and loss:  1.158713158336468
test acc:  0.9193
forward train acc:  0.9997  and loss:  0.8373383559519425
test acc:  0.9173
forward train acc:  0.99968  and loss:  0.7230774821364321
test acc:  0.9186
forward train acc:  0.9996  and loss:  0.7617167357238941
test acc:  0.9179
forward train acc:  0.99964  and loss:  0.7397059171344154
test acc:  0.9166
********** reverse layer  11  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5416666666666666  ==>  88 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7604166666666666  ==>  184 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  12
[5, 4, 5, 3, 5, 3, 1, 0, 1, 4, 4, 5, 4, 1]
***** skip layer  13
[5, 4, 5, 3, 5, 3, 1, 0, 1, 4, 4, 5, 4, 0]
***** skip layer  0
[4, 4, 5, 3, 5, 3, 1, 0, 1, 4, 4, 5, 4, 0]
***** skip layer  1
[4, 3, 5, 3, 5, 3, 1, 0, 1, 4, 4, 5, 4, 0]
***** skip layer  2
[4, 3, 4, 3, 5, 3, 1, 0, 1, 4, 4, 5, 4, 0]
***** skip layer  3
[4, 3, 4, 2, 5, 3, 1, 0, 1, 4, 4, 5, 4, 0]
***** skip layer  4
[4, 3, 4, 2, 4, 3, 1, 0, 1, 4, 4, 5, 4, 0]
***** skip layer  5
[4, 3, 4, 2, 4, 2, 1, 0, 1, 4, 4, 5, 4, 0]
***** skip layer  6
[4, 3, 4, 2, 4, 2, 0, 0, 1, 4, 4, 5, 4, 0]
optimize layer  7
backward train epoch:  18
test acc:  0.1745
forward train acc:  0.99774  and loss:  3.4702794270706363
test acc:  0.9136
forward train acc:  0.99884  and loss:  1.9325894364737906
test acc:  0.9172
forward train acc:  0.99848  and loss:  2.0418484701076522
test acc:  0.9166
forward train acc:  0.99888  and loss:  1.765227017691359
test acc:  0.9166
forward train acc:  0.99908  and loss:  1.404095844074618
test acc:  0.9159
forward train acc:  0.9994  and loss:  1.0280287766363472
test acc:  0.9157
forward train acc:  0.9993  and loss:  1.2208432537736371
test acc:  0.9162
forward train acc:  0.99916  and loss:  1.3054710944416001
test acc:  0.9171
forward train acc:  0.99936  and loss:  1.0807481040246785
test acc:  0.9158
forward train acc:  0.9995  and loss:  1.028806418296881
test acc:  0.9173
********** reverse layer  7  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5416666666666666  ==>  88 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7604166666666666  ==>  184 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  8
[4, 3, 4, 2, 4, 2, 0, 5, 0, 4, 4, 5, 4, 0]
***** skip layer  9
[4, 3, 4, 2, 4, 2, 0, 5, 0, 3, 4, 5, 4, 0]
***** skip layer  10
[4, 3, 4, 2, 4, 2, 0, 5, 0, 3, 3, 5, 4, 0]
***** skip layer  11
[4, 3, 4, 2, 4, 2, 0, 5, 0, 3, 3, 4, 4, 0]
***** skip layer  12
[4, 3, 4, 2, 4, 2, 0, 5, 0, 3, 3, 4, 3, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.1091
forward train acc:  0.99944  and loss:  0.779159186349716
test acc:  0.9192
forward train acc:  0.9995  and loss:  0.7857232016103808
test acc:  0.9176
forward train acc:  0.99904  and loss:  1.5343750922183972
test acc:  0.9171
forward train acc:  0.9994  and loss:  0.976366581977345
test acc:  0.9171
forward train acc:  0.99952  and loss:  0.8412750705319922
test acc:  0.9193
forward train acc:  0.99954  and loss:  0.6894374290131964
test acc:  0.9184
forward train acc:  0.99958  and loss:  0.7213773835101165
test acc:  0.9179
forward train acc:  0.99974  and loss:  0.4519527241354808
test acc:  0.918
forward train acc:  0.9997  and loss:  0.5128796040080488
test acc:  0.918
forward train acc:  0.99974  and loss:  0.5121562095591798
test acc:  0.9196
********** reverse layer  13  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5416666666666666  ==>  88 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6041666666666666  ==>  152 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7604166666666666  ==>  184 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  0
[3, 3, 4, 2, 4, 2, 0, 5, 0, 3, 3, 4, 3, 5]
***** skip layer  1
[3, 2, 4, 2, 4, 2, 0, 5, 0, 3, 3, 4, 3, 5]
***** skip layer  2
[3, 2, 3, 2, 4, 2, 0, 5, 0, 3, 3, 4, 3, 5]
***** skip layer  3
[3, 2, 3, 1, 4, 2, 0, 5, 0, 3, 3, 4, 3, 5]
***** skip layer  4
[3, 2, 3, 1, 3, 2, 0, 5, 0, 3, 3, 4, 3, 5]
***** skip layer  5
[3, 2, 3, 1, 3, 1, 0, 5, 0, 3, 3, 4, 3, 5]
optimize layer  6
backward train epoch:  178
test acc:  0.0917
forward train acc:  0.99932  and loss:  1.1950533170020208
test acc:  0.9162
forward train acc:  0.99944  and loss:  0.935881374112796
test acc:  0.9176
forward train acc:  0.9992  and loss:  1.1624658563523553
test acc:  0.9184
forward train acc:  0.99938  and loss:  0.9449710692861117
test acc:  0.9183
forward train acc:  0.99944  and loss:  0.9665252270642668
test acc:  0.9186
forward train acc:  0.99934  and loss:  1.0318910692003556
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5416666666666666  ==>  88 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6145833333333334  ==>  148 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7604166666666666  ==>  184 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  7
[3, 2, 3, 1, 3, 1, 0, 4, 0, 3, 3, 4, 3, 5]
optimize layer  8
backward train epoch:  14
test acc:  0.118
forward train acc:  0.9875  and loss:  16.549765621777624
test acc:  0.9062
forward train acc:  0.99566  and loss:  6.229895310360007
test acc:  0.9098
forward train acc:  0.99628  and loss:  5.015856136102229
test acc:  0.9107
forward train acc:  0.99698  and loss:  4.143764709820971
test acc:  0.9118
forward train acc:  0.99764  and loss:  3.487013003206812
test acc:  0.9128
forward train acc:  0.99746  and loss:  3.528519889106974
test acc:  0.914
forward train acc:  0.99776  and loss:  3.3972302083857358
test acc:  0.9129
forward train acc:  0.99798  and loss:  2.8496421020827256
test acc:  0.9139
forward train acc:  0.99792  and loss:  3.089083762955852
test acc:  0.9153
forward train acc:  0.99834  and loss:  2.6779508212930523
test acc:  0.9142
********** reverse layer  8  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5416666666666666  ==>  88 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6145833333333334  ==>  148 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7604166666666666  ==>  184 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  9
[3, 2, 3, 1, 3, 1, 0, 4, 5, 2, 3, 4, 3, 5]
***** skip layer  10
[3, 2, 3, 1, 3, 1, 0, 4, 5, 2, 2, 4, 3, 5]
***** skip layer  11
[3, 2, 3, 1, 3, 1, 0, 4, 5, 2, 2, 3, 3, 5]
***** skip layer  12
[3, 2, 3, 1, 3, 1, 0, 4, 5, 2, 2, 3, 2, 5]
***** skip layer  13
[3, 2, 3, 1, 3, 1, 0, 4, 5, 2, 2, 3, 2, 4]
***** skip layer  0
[2, 2, 3, 1, 3, 1, 0, 4, 5, 2, 2, 3, 2, 4]
***** skip layer  1
[2, 1, 3, 1, 3, 1, 0, 4, 5, 2, 2, 3, 2, 4]
***** skip layer  2
[2, 1, 2, 1, 3, 1, 0, 4, 5, 2, 2, 3, 2, 4]
***** skip layer  3
[2, 1, 2, 0, 3, 1, 0, 4, 5, 2, 2, 3, 2, 4]
***** skip layer  4
[2, 1, 2, 0, 2, 1, 0, 4, 5, 2, 2, 3, 2, 4]
***** skip layer  5
[2, 1, 2, 0, 2, 0, 0, 4, 5, 2, 2, 3, 2, 4]
optimize layer  6
backward train epoch:  37
test acc:  0.1425
forward train acc:  0.99896  and loss:  1.5064539758022875
test acc:  0.9208
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5416666666666666  ==>  88 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7604166666666666  ==>  184 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  7
[2, 1, 2, 0, 2, 0, 0, 3, 5, 2, 2, 3, 2, 4]
***** skip layer  8
[2, 1, 2, 0, 2, 0, 0, 3, 4, 2, 2, 3, 2, 4]
***** skip layer  9
[2, 1, 2, 0, 2, 0, 0, 3, 4, 1, 2, 3, 2, 4]
***** skip layer  10
[2, 1, 2, 0, 2, 0, 0, 3, 4, 1, 1, 3, 2, 4]
***** skip layer  11
[2, 1, 2, 0, 2, 0, 0, 3, 4, 1, 1, 2, 2, 4]
***** skip layer  12
[2, 1, 2, 0, 2, 0, 0, 3, 4, 1, 1, 2, 1, 4]
***** skip layer  13
[2, 1, 2, 0, 2, 0, 0, 3, 4, 1, 1, 2, 1, 3]
***** skip layer  0
[1, 1, 2, 0, 2, 0, 0, 3, 4, 1, 1, 2, 1, 3]
***** skip layer  1
[1, 0, 2, 0, 2, 0, 0, 3, 4, 1, 1, 2, 1, 3]
***** skip layer  2
[1, 0, 1, 0, 2, 0, 0, 3, 4, 1, 1, 2, 1, 3]
optimize layer  3
backward train epoch:  3
test acc:  0.8077
forward train acc:  0.99868  and loss:  1.9509342187084258
test acc:  0.9177
forward train acc:  0.9993  and loss:  1.234134363825433
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7604166666666666  ==>  184 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  4
[1, 0, 1, 0, 1, 0, 0, 3, 4, 1, 1, 2, 1, 3]
optimize layer  5
backward train epoch:  61
test acc:  0.1007
forward train acc:  0.9986  and loss:  2.054293781868182
test acc:  0.9162
forward train acc:  0.9986  and loss:  1.9949856054154225
test acc:  0.9184
forward train acc:  0.99876  and loss:  2.048134697775822
test acc:  0.9187
forward train acc:  0.99894  and loss:  1.831746058538556
test acc:  0.9181
forward train acc:  0.99918  and loss:  1.3734045187011361
test acc:  0.9187
forward train acc:  0.99898  and loss:  1.3747424364555627
test acc:  0.9187
forward train acc:  0.99936  and loss:  1.0432900172309019
test acc:  0.9182
forward train acc:  0.99926  and loss:  1.1543741998029873
test acc:  0.9188
forward train acc:  0.9994  and loss:  1.073556762363296
test acc:  0.9188
forward train acc:  0.99958  and loss:  0.8495548692299053
test acc:  0.9192
********** reverse layer  5  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7604166666666666  ==>  184 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
optimize layer  6
backward train epoch:  90
test acc:  0.1078
forward train acc:  0.99906  and loss:  1.4142751187901013
test acc:  0.916
forward train acc:  0.99916  and loss:  1.2094856693292968
test acc:  0.917
forward train acc:  0.99906  and loss:  1.5804099508095533
test acc:  0.9152
forward train acc:  0.99912  and loss:  1.3367208442068659
test acc:  0.9183
forward train acc:  0.99938  and loss:  1.1033936113817617
test acc:  0.9188
forward train acc:  0.99922  and loss:  1.1614998012082651
test acc:  0.9191
forward train acc:  0.99956  and loss:  0.7936000485206023
test acc:  0.9179
forward train acc:  0.99946  and loss:  0.8686234359629452
test acc:  0.9191
forward train acc:  0.99948  and loss:  0.8930478264810517
test acc:  0.9191
forward train acc:  0.99944  and loss:  0.9155910487752408
test acc:  0.9186
********** reverse layer  6  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7604166666666666  ==>  184 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  7
[1, 0, 1, 0, 1, 5, 5, 2, 4, 1, 1, 2, 1, 3]
***** skip layer  8
[1, 0, 1, 0, 1, 5, 5, 2, 3, 1, 1, 2, 1, 3]
***** skip layer  9
[1, 0, 1, 0, 1, 5, 5, 2, 3, 0, 1, 2, 1, 3]
***** skip layer  10
[1, 0, 1, 0, 1, 5, 5, 2, 3, 0, 0, 2, 1, 3]
***** skip layer  11
[1, 0, 1, 0, 1, 5, 5, 2, 3, 0, 0, 1, 1, 3]
***** skip layer  12
[1, 0, 1, 0, 1, 5, 5, 2, 3, 0, 0, 1, 0, 3]
***** skip layer  13
[1, 0, 1, 0, 1, 5, 5, 2, 3, 0, 0, 1, 0, 2]
***** skip layer  0
[0, 0, 1, 0, 1, 5, 5, 2, 3, 0, 0, 1, 0, 2]
optimize layer  1
backward train epoch:  28
test acc:  0.1178
forward train acc:  0.99906  and loss:  1.4771728965570219
test acc:  0.9175
forward train acc:  0.99908  and loss:  1.3906135305878706
test acc:  0.9174
forward train acc:  0.99912  and loss:  1.5417780206771567
test acc:  0.9177
forward train acc:  0.99918  and loss:  1.2844907470280305
test acc:  0.9171
forward train acc:  0.99944  and loss:  0.874720752122812
test acc:  0.917
forward train acc:  0.99926  and loss:  1.1934455810696818
test acc:  0.917
forward train acc:  0.99918  and loss:  1.2360215057851747
test acc:  0.9173
forward train acc:  0.99946  and loss:  0.9763895848882385
test acc:  0.9179
forward train acc:  0.99962  and loss:  0.7213854196015745
test acc:  0.9184
forward train acc:  0.99944  and loss:  0.9171474209870212
test acc:  0.918
********** reverse layer  1  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7604166666666666  ==>  184 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  2
[0, 5, 0, 0, 1, 5, 5, 2, 3, 0, 0, 1, 0, 2]
optimize layer  3
backward train epoch:  2
test acc:  0.1216
forward train acc:  0.99896  and loss:  1.8431906423647888
test acc:  0.9164
forward train acc:  0.99878  and loss:  1.9187562901643105
test acc:  0.9162
forward train acc:  0.99916  and loss:  1.15268056379864
test acc:  0.916
forward train acc:  0.99912  and loss:  1.3084601035807282
test acc:  0.9171
forward train acc:  0.99922  and loss:  1.1996908477740362
test acc:  0.9184
forward train acc:  0.99928  and loss:  1.1451817459310405
test acc:  0.9173
forward train acc:  0.99934  and loss:  0.957023941737134
test acc:  0.9168
forward train acc:  0.99938  and loss:  1.0743039474473335
test acc:  0.9172
forward train acc:  0.99916  and loss:  1.17050527248648
test acc:  0.9167
forward train acc:  0.99946  and loss:  0.8815109304268844
test acc:  0.9164
********** reverse layer  3  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7604166666666666  ==>  184 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  4
[0, 5, 0, 5, 0, 5, 5, 2, 3, 0, 0, 1, 0, 2]
***** skip layer  5
[0, 5, 0, 5, 0, 4, 5, 2, 3, 0, 0, 1, 0, 2]
***** skip layer  6
[0, 5, 0, 5, 0, 4, 4, 2, 3, 0, 0, 1, 0, 2]
***** skip layer  7
[0, 5, 0, 5, 0, 4, 4, 1, 3, 0, 0, 1, 0, 2]
***** skip layer  8
[0, 5, 0, 5, 0, 4, 4, 1, 2, 0, 0, 1, 0, 2]
optimize layer  9
backward train epoch:  70
test acc:  0.1044
forward train acc:  0.9994  and loss:  0.8566005497705191
test acc:  0.9151
forward train acc:  0.99938  and loss:  0.9545554649084806
test acc:  0.9184
forward train acc:  0.99922  and loss:  1.1884756775107235
test acc:  0.9173
forward train acc:  0.99942  and loss:  0.8612459839496296
test acc:  0.9178
forward train acc:  0.99938  and loss:  0.933452962199226
test acc:  0.917
forward train acc:  0.99956  and loss:  0.8379799765534699
test acc:  0.9183
forward train acc:  0.99946  and loss:  0.7359845925820991
test acc:  0.9177
forward train acc:  0.99968  and loss:  0.6890007535694167
test acc:  0.9174
forward train acc:  0.99964  and loss:  0.6710513365687802
test acc:  0.9183
forward train acc:  0.9996  and loss:  0.661784215742955
test acc:  0.9186
********** reverse layer  9  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7604166666666666  ==>  184 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
optimize layer  10
backward train epoch:  98
test acc:  0.0981
forward train acc:  0.99944  and loss:  0.8642968484782614
test acc:  0.9187
forward train acc:  0.99938  and loss:  0.931353327119723
test acc:  0.9187
forward train acc:  0.99942  and loss:  1.0018971199751832
test acc:  0.9168
forward train acc:  0.99934  and loss:  1.239208068465814
test acc:  0.9172
forward train acc:  0.9996  and loss:  0.7166261387756094
test acc:  0.9177
forward train acc:  0.99966  and loss:  0.621516104467446
test acc:  0.9181
forward train acc:  0.99944  and loss:  0.988228289730614
test acc:  0.917
forward train acc:  0.99956  and loss:  0.7809840847039595
test acc:  0.9182
forward train acc:  0.99968  and loss:  0.641584640921792
test acc:  0.9179
forward train acc:  0.99968  and loss:  0.5309369062306359
test acc:  0.9179
********** reverse layer  10  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7604166666666666  ==>  184 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  11
[0, 5, 0, 5, 0, 4, 4, 1, 2, 5, 5, 0, 0, 2]
optimize layer  12
backward train epoch:  364
test acc:  0.1
forward train acc:  0.1  and loss:  1400.5737793445587
test acc:  0.1
forward train acc:  0.1  and loss:  1354.0365936756134
test acc:  0.1
forward train acc:  0.1  and loss:  1312.0394973754883
test acc:  0.1
forward train acc:  0.1  and loss:  1282.6660025119781
test acc:  0.1
forward train acc:  0.1  and loss:  1264.2855033874512
test acc:  0.1
forward train acc:  0.1  and loss:  1245.9716851711273
test acc:  0.1
forward train acc:  0.1  and loss:  1228.4927778244019
test acc:  0.1
forward train acc:  0.1  and loss:  1215.7296268939972
test acc:  0.1
forward train acc:  0.1  and loss:  1207.2980210781097
test acc:  0.1
forward train acc:  0.1  and loss:  1199.0007269382477
test acc:  0.1
********** reverse layer  12  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7604166666666666  ==>  184 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  13
[0, 5, 0, 5, 0, 4, 4, 1, 2, 5, 5, 0, 5, 1]
optimize layer  0
backward train epoch:  1408
test acc:  0.1008
forward train acc:  0.73814  and loss:  454.1645721793175
test acc:  0.772
forward train acc:  0.81774  and loss:  238.46617224812508
test acc:  0.8085
forward train acc:  0.85392  and loss:  188.50250828266144
test acc:  0.8307
forward train acc:  0.87128  and loss:  162.722988396883
test acc:  0.8387
forward train acc:  0.88434  and loss:  145.34542809426785
test acc:  0.8456
forward train acc:  0.89516  and loss:  131.9557507187128
test acc:  0.8514
forward train acc:  0.90158  and loss:  121.78118643164635
test acc:  0.8537
forward train acc:  0.91128  and loss:  112.09229830652475
test acc:  0.8562
forward train acc:  0.91462  and loss:  106.44617084413767
test acc:  0.8586
forward train acc:  0.91748  and loss:  103.59749218076468
test acc:  0.8618
********** reverse layer  0  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5  ==>  96 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7604166666666666  ==>  184 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  1
[5, 4, 0, 5, 0, 4, 4, 1, 2, 5, 5, 0, 5, 1]
optimize layer  2
backward train epoch:  30
test acc:  0.1418
forward train acc:  0.9958  and loss:  6.923360984539613
test acc:  0.917
forward train acc:  0.99804  and loss:  2.992567122913897
test acc:  0.9184
forward train acc:  0.99852  and loss:  2.136839445796795
test acc:  0.9185
forward train acc:  0.99914  and loss:  1.4548468054272234
test acc:  0.919
forward train acc:  0.99904  and loss:  1.559238973830361
test acc:  0.9196
forward train acc:  0.99906  and loss:  1.3326614061370492
test acc:  0.9202
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5104166666666666  ==>  94 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7604166666666666  ==>  184 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  3
[5, 4, 0, 4, 0, 4, 4, 1, 2, 5, 5, 0, 5, 1]
optimize layer  4
backward train epoch:  579
test acc:  0.1025
forward train acc:  0.99874  and loss:  2.093554214166943
test acc:  0.9163
forward train acc:  0.99836  and loss:  2.3094420039560646
test acc:  0.916
forward train acc:  0.99844  and loss:  2.099999226455111
test acc:  0.9182
forward train acc:  0.9991  and loss:  1.460392880893778
test acc:  0.9184
forward train acc:  0.99908  and loss:  1.466615095967427
test acc:  0.9192
forward train acc:  0.99912  and loss:  1.3485239208093844
test acc:  0.9184
forward train acc:  0.99924  and loss:  1.2213435189332813
test acc:  0.9175
forward train acc:  0.9992  and loss:  1.127996756345965
test acc:  0.9184
forward train acc:  0.99926  and loss:  1.1205830363323912
test acc:  0.9191
forward train acc:  0.99958  and loss:  0.8042214934830554
test acc:  0.9192
********** reverse layer  4  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5104166666666666  ==>  94 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7604166666666666  ==>  184 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  5
[5, 4, 0, 4, 5, 3, 4, 1, 2, 5, 5, 0, 5, 1]
***** skip layer  6
[5, 4, 0, 4, 5, 3, 3, 1, 2, 5, 5, 0, 5, 1]
***** skip layer  7
[5, 4, 0, 4, 5, 3, 3, 0, 2, 5, 5, 0, 5, 1]
***** skip layer  8
[5, 4, 0, 4, 5, 3, 3, 0, 1, 5, 5, 0, 5, 1]
***** skip layer  9
[5, 4, 0, 4, 5, 3, 3, 0, 1, 4, 5, 0, 5, 1]
***** skip layer  10
[5, 4, 0, 4, 5, 3, 3, 0, 1, 4, 4, 0, 5, 1]
optimize layer  11
backward train epoch:  105
test acc:  0.1
forward train acc:  0.99922  and loss:  1.2882572178496048
test acc:  0.9213
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5104166666666666  ==>  94 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7682291666666666  ==>  178 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  12
[5, 4, 0, 4, 5, 3, 3, 0, 1, 4, 4, 0, 4, 1]
***** skip layer  13
[5, 4, 0, 4, 5, 3, 3, 0, 1, 4, 4, 0, 4, 0]
***** skip layer  0
[4, 4, 0, 4, 5, 3, 3, 0, 1, 4, 4, 0, 4, 0]
***** skip layer  1
[4, 3, 0, 4, 5, 3, 3, 0, 1, 4, 4, 0, 4, 0]
optimize layer  2
backward train epoch:  25
test acc:  0.1462
forward train acc:  0.99896  and loss:  1.20797731122002
test acc:  0.9193
forward train acc:  0.9992  and loss:  1.2429664382361807
test acc:  0.9182
forward train acc:  0.99924  and loss:  1.3341584802255966
test acc:  0.9183
forward train acc:  0.99952  and loss:  1.0214125171187334
test acc:  0.9176
forward train acc:  0.99952  and loss:  0.761426294920966
test acc:  0.918
forward train acc:  0.99946  and loss:  1.0038066531997174
test acc:  0.9189
forward train acc:  0.99934  and loss:  0.9830842353985645
test acc:  0.9188
forward train acc:  0.99954  and loss:  0.7552856082911603
test acc:  0.9204
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7682291666666666  ==>  178 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  3
[4, 3, 0, 3, 5, 3, 3, 0, 1, 4, 4, 0, 4, 0]
***** skip layer  4
[4, 3, 0, 3, 4, 3, 3, 0, 1, 4, 4, 0, 4, 0]
***** skip layer  5
[4, 3, 0, 3, 4, 2, 3, 0, 1, 4, 4, 0, 4, 0]
***** skip layer  6
[4, 3, 0, 3, 4, 2, 2, 0, 1, 4, 4, 0, 4, 0]
optimize layer  7
backward train epoch:  94
test acc:  0.129
forward train acc:  0.99842  and loss:  2.25612070877105
test acc:  0.9146
forward train acc:  0.99856  and loss:  2.176871397823561
test acc:  0.9159
forward train acc:  0.99864  and loss:  1.9496537477825768
test acc:  0.917
forward train acc:  0.99854  and loss:  1.810127342818305
test acc:  0.9194
forward train acc:  0.99908  and loss:  1.3519575557438657
test acc:  0.9165
forward train acc:  0.99902  and loss:  1.4171411251300015
test acc:  0.9181
forward train acc:  0.9991  and loss:  1.3757556153577752
test acc:  0.9195
forward train acc:  0.99932  and loss:  1.1740446496987715
test acc:  0.9198
forward train acc:  0.99926  and loss:  1.1020758638042025
test acc:  0.9181
forward train acc:  0.9993  and loss:  1.077770836360287
test acc:  0.9193
********** reverse layer  7  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7682291666666666  ==>  178 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  8
[4, 3, 0, 3, 4, 2, 2, 5, 0, 4, 4, 0, 4, 0]
***** skip layer  9
[4, 3, 0, 3, 4, 2, 2, 5, 0, 3, 4, 0, 4, 0]
***** skip layer  10
[4, 3, 0, 3, 4, 2, 2, 5, 0, 3, 3, 0, 4, 0]
optimize layer  11
backward train epoch:  392
test acc:  0.1001
forward train acc:  0.99926  and loss:  1.1271280653309077
test acc:  0.9177
forward train acc:  0.99946  and loss:  0.849806590413209
test acc:  0.916
forward train acc:  0.99946  and loss:  0.9584598893125076
test acc:  0.9187
forward train acc:  0.99944  and loss:  0.8241074789548293
test acc:  0.9185
forward train acc:  0.99946  and loss:  1.1097211790911388
test acc:  0.9192
forward train acc:  0.99952  and loss:  0.7753089967300184
test acc:  0.918
forward train acc:  0.99958  and loss:  0.6597982973325998
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7760416666666666  ==>  172 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6692708333333334  ==>  254 / 768
***** skip layer  12
[4, 3, 0, 3, 4, 2, 2, 5, 0, 3, 3, 0, 3, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.0968
forward train acc:  0.99912  and loss:  5.47174432175234
test acc:  0.9182
forward train acc:  0.99932  and loss:  3.8678725813515484
test acc:  0.9185
forward train acc:  0.99922  and loss:  3.6465445063076913
test acc:  0.9199
forward train acc:  0.99902  and loss:  3.520083049777895
test acc:  0.9176
forward train acc:  0.99936  and loss:  3.2309500104747713
test acc:  0.9197
forward train acc:  0.99928  and loss:  3.052448024507612
test acc:  0.9194
forward train acc:  0.99954  and loss:  2.5304345490876585
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7760416666666666  ==>  172 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6770833333333334  ==>  248 / 768
***** skip layer  0
[3, 3, 0, 3, 4, 2, 2, 5, 0, 3, 3, 0, 3, 0]
***** skip layer  1
[3, 2, 0, 3, 4, 2, 2, 5, 0, 3, 3, 0, 3, 0]
optimize layer  2
backward train epoch:  38
test acc:  0.0895
forward train acc:  0.9988  and loss:  3.267906256718561
test acc:  0.9177
forward train acc:  0.9988  and loss:  3.2186924437992275
test acc:  0.9182
forward train acc:  0.99922  and loss:  2.673577172216028
test acc:  0.9175
forward train acc:  0.99902  and loss:  2.5821920102462173
test acc:  0.9185
forward train acc:  0.99922  and loss:  2.3984123282134533
test acc:  0.918
forward train acc:  0.99926  and loss:  2.4781640169676393
test acc:  0.9195
forward train acc:  0.99944  and loss:  1.9565413238015026
test acc:  0.9183
forward train acc:  0.99934  and loss:  2.0471694809384644
test acc:  0.9184
forward train acc:  0.99938  and loss:  1.8096114520449191
test acc:  0.9178
forward train acc:  0.9994  and loss:  1.6924228314310312
test acc:  0.9178
********** reverse layer  2  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7760416666666666  ==>  172 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6770833333333334  ==>  248 / 768
***** skip layer  3
[3, 2, 5, 2, 4, 2, 2, 5, 0, 3, 3, 0, 3, 0]
***** skip layer  4
[3, 2, 5, 2, 3, 2, 2, 5, 0, 3, 3, 0, 3, 0]
***** skip layer  5
[3, 2, 5, 2, 3, 1, 2, 5, 0, 3, 3, 0, 3, 0]
***** skip layer  6
[3, 2, 5, 2, 3, 1, 1, 5, 0, 3, 3, 0, 3, 0]
***** skip layer  7
[3, 2, 5, 2, 3, 1, 1, 4, 0, 3, 3, 0, 3, 0]
optimize layer  8
backward train epoch:  281
test acc:  0.1062
forward train acc:  0.99672  and loss:  5.672572786454111
test acc:  0.9126
forward train acc:  0.9982  and loss:  3.990885105682537
test acc:  0.9149
forward train acc:  0.99834  and loss:  3.3026543089654297
test acc:  0.915
forward train acc:  0.99876  and loss:  2.557993771508336
test acc:  0.9156
forward train acc:  0.99876  and loss:  2.6035814217757434
test acc:  0.9149
forward train acc:  0.99882  and loss:  2.41077681153547
test acc:  0.9168
forward train acc:  0.99896  and loss:  2.0479140625102445
test acc:  0.9165
forward train acc:  0.99916  and loss:  1.9424182636430487
test acc:  0.9164
forward train acc:  0.99928  and loss:  1.8708124692784622
test acc:  0.9175
forward train acc:  0.99928  and loss:  1.7477430476574227
test acc:  0.9169
********** reverse layer  8  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7760416666666666  ==>  172 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6770833333333334  ==>  248 / 768
***** skip layer  9
[3, 2, 5, 2, 3, 1, 1, 4, 5, 2, 3, 0, 3, 0]
***** skip layer  10
[3, 2, 5, 2, 3, 1, 1, 4, 5, 2, 2, 0, 3, 0]
optimize layer  11
backward train epoch:  108
test acc:  0.0968
forward train acc:  0.9994  and loss:  1.5544784350786358
test acc:  0.9186
forward train acc:  0.99964  and loss:  1.3465137222083285
test acc:  0.9182
forward train acc:  0.99942  and loss:  1.4869965178659186
test acc:  0.9169
forward train acc:  0.99958  and loss:  1.1291640134295449
test acc:  0.9188
forward train acc:  0.99956  and loss:  1.1649971368024126
test acc:  0.918
forward train acc:  0.99958  and loss:  1.110919819562696
test acc:  0.9184
forward train acc:  0.99956  and loss:  1.210983972880058
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7838541666666666  ==>  166 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6770833333333334  ==>  248 / 768
***** skip layer  12
[3, 2, 5, 2, 3, 1, 1, 4, 5, 2, 2, 0, 2, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.1014
forward train acc:  0.99876  and loss:  19.138292411342263
test acc:  0.9173
forward train acc:  0.99932  and loss:  15.593208085745573
test acc:  0.9182
forward train acc:  0.99944  and loss:  13.756801440380514
test acc:  0.9164
forward train acc:  0.99954  and loss:  12.654529572464526
test acc:  0.9197
forward train acc:  0.99954  and loss:  11.802291016094387
test acc:  0.9194
forward train acc:  0.9998  and loss:  10.862368776462972
test acc:  0.9194
forward train acc:  0.99954  and loss:  10.444598474539816
test acc:  0.9195
forward train acc:  0.99974  and loss:  9.840072550810874
test acc:  0.9178
forward train acc:  0.99966  and loss:  9.705181836150587
test acc:  0.9182
forward train acc:  0.99974  and loss:  9.37246726732701
test acc:  0.9197
********** reverse layer  13  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7838541666666666  ==>  166 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6770833333333334  ==>  248 / 768
***** skip layer  0
[2, 2, 5, 2, 3, 1, 1, 4, 5, 2, 2, 0, 2, 5]
***** skip layer  1
[2, 1, 5, 2, 3, 1, 1, 4, 5, 2, 2, 0, 2, 5]
***** skip layer  2
[2, 1, 4, 2, 3, 1, 1, 4, 5, 2, 2, 0, 2, 5]
***** skip layer  3
[2, 1, 4, 1, 3, 1, 1, 4, 5, 2, 2, 0, 2, 5]
***** skip layer  4
[2, 1, 4, 1, 2, 1, 1, 4, 5, 2, 2, 0, 2, 5]
***** skip layer  5
[2, 1, 4, 1, 2, 0, 1, 4, 5, 2, 2, 0, 2, 5]
***** skip layer  6
[2, 1, 4, 1, 2, 0, 0, 4, 5, 2, 2, 0, 2, 5]
***** skip layer  7
[2, 1, 4, 1, 2, 0, 0, 3, 5, 2, 2, 0, 2, 5]
***** skip layer  8
[2, 1, 4, 1, 2, 0, 0, 3, 4, 2, 2, 0, 2, 5]
***** skip layer  9
[2, 1, 4, 1, 2, 0, 0, 3, 4, 1, 2, 0, 2, 5]
***** skip layer  10
[2, 1, 4, 1, 2, 0, 0, 3, 4, 1, 1, 0, 2, 5]
optimize layer  11
backward train epoch:  812
test acc:  0.0986
forward train acc:  0.99964  and loss:  1.6546653250698
test acc:  0.9177
forward train acc:  0.99962  and loss:  1.1790694618830457
test acc:  0.9174
forward train acc:  0.99926  and loss:  1.6004842382390052
test acc:  0.9159
forward train acc:  0.9994  and loss:  1.3276942713418975
test acc:  0.9195
forward train acc:  0.99938  and loss:  1.6443974502617493
test acc:  0.9197
forward train acc:  0.99952  and loss:  1.3034119055373594
test acc:  0.9194
forward train acc:  0.9994  and loss:  1.243597301770933
test acc:  0.9184
forward train acc:  0.9996  and loss:  1.068994652829133
test acc:  0.9194
forward train acc:  0.9997  and loss:  0.8618748493026942
test acc:  0.9196
forward train acc:  0.99962  and loss:  0.9525434475508519
test acc:  0.9187
********** reverse layer  11  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7838541666666666  ==>  166 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6770833333333334  ==>  248 / 768
***** skip layer  12
[2, 1, 4, 1, 2, 0, 0, 3, 4, 1, 1, 5, 1, 5]
***** skip layer  13
[2, 1, 4, 1, 2, 0, 0, 3, 4, 1, 1, 5, 1, 4]
***** skip layer  0
[1, 1, 4, 1, 2, 0, 0, 3, 4, 1, 1, 5, 1, 4]
***** skip layer  1
[1, 0, 4, 1, 2, 0, 0, 3, 4, 1, 1, 5, 1, 4]
***** skip layer  2
[1, 0, 3, 1, 2, 0, 0, 3, 4, 1, 1, 5, 1, 4]
***** skip layer  3
[1, 0, 3, 0, 2, 0, 0, 3, 4, 1, 1, 5, 1, 4]
***** skip layer  4
[1, 0, 3, 0, 1, 0, 0, 3, 4, 1, 1, 5, 1, 4]
optimize layer  5
backward train epoch:  259
test acc:  0.0923
forward train acc:  0.99918  and loss:  1.5333052640780807
test acc:  0.9158
forward train acc:  0.9989  and loss:  1.897737328778021
test acc:  0.9164
forward train acc:  0.99908  and loss:  1.6823099799221382
test acc:  0.9166
forward train acc:  0.99934  and loss:  1.2512700089719146
test acc:  0.9176
forward train acc:  0.99954  and loss:  1.1108486419543624
test acc:  0.9166
forward train acc:  0.9996  and loss:  1.030570085393265
test acc:  0.9173
forward train acc:  0.99956  and loss:  0.951950864167884
test acc:  0.9171
forward train acc:  0.99966  and loss:  0.882638924929779
test acc:  0.917
forward train acc:  0.99962  and loss:  0.8537547928863205
test acc:  0.9179
forward train acc:  0.9996  and loss:  0.9924534494057298
test acc:  0.9184
********** reverse layer  5  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7838541666666666  ==>  166 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6770833333333334  ==>  248 / 768
optimize layer  6
backward train epoch:  154
test acc:  0.0998
forward train acc:  0.9994  and loss:  1.010329647688195
test acc:  0.918
forward train acc:  0.99926  and loss:  1.291110036894679
test acc:  0.9182
forward train acc:  0.99924  and loss:  1.3526598576572724
test acc:  0.9187
forward train acc:  0.99918  and loss:  1.3765798998065293
test acc:  0.9171
forward train acc:  0.99948  and loss:  0.9868470098590478
test acc:  0.919
forward train acc:  0.99954  and loss:  0.9518267859239131
test acc:  0.9197
forward train acc:  0.99948  and loss:  0.9859182684449479
test acc:  0.9183
forward train acc:  0.99954  and loss:  0.7634160335292108
test acc:  0.9179
forward train acc:  0.99958  and loss:  0.8939604208571836
test acc:  0.9191
forward train acc:  0.99968  and loss:  0.7680028329486959
test acc:  0.9192
********** reverse layer  6  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5729166666666666  ==>  41 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7838541666666666  ==>  166 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6770833333333334  ==>  248 / 768
***** skip layer  7
[1, 0, 3, 0, 1, 5, 5, 2, 4, 1, 1, 5, 1, 4]
***** skip layer  8
[1, 0, 3, 0, 1, 5, 5, 2, 3, 1, 1, 5, 1, 4]
***** skip layer  9
[1, 0, 3, 0, 1, 5, 5, 2, 3, 0, 1, 5, 1, 4]
***** skip layer  10
[1, 0, 3, 0, 1, 5, 5, 2, 3, 0, 0, 5, 1, 4]
***** skip layer  11
[1, 0, 3, 0, 1, 5, 5, 2, 3, 0, 0, 4, 1, 4]
***** skip layer  12
[1, 0, 3, 0, 1, 5, 5, 2, 3, 0, 0, 4, 0, 4]
***** skip layer  13
[1, 0, 3, 0, 1, 5, 5, 2, 3, 0, 0, 4, 0, 3]
***** skip layer  0
[0, 0, 3, 0, 1, 5, 5, 2, 3, 0, 0, 4, 0, 3]
optimize layer  1
backward train epoch:  464
test acc:  0.0915
forward train acc:  0.99932  and loss:  1.0985231769736856
test acc:  0.9196
forward train acc:  0.99926  and loss:  1.2011266722693108
test acc:  0.9175
forward train acc:  0.99912  and loss:  1.50290081219282
test acc:  0.9179
forward train acc:  0.99926  and loss:  1.1808191252639517
test acc:  0.9188
forward train acc:  0.99948  and loss:  0.9267861997941509
test acc:  0.9197
forward train acc:  0.99948  and loss:  1.0244715630542487
test acc:  0.9187
forward train acc:  0.99962  and loss:  0.8656930375727825
test acc:  0.9181
forward train acc:  0.99958  and loss:  0.930551026831381
test acc:  0.9186
forward train acc:  0.99954  and loss:  0.8454941430827603
test acc:  0.9195
forward train acc:  0.99958  and loss:  0.9536264594062231
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5833333333333334  ==>  40 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7838541666666666  ==>  166 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6770833333333334  ==>  248 / 768
***** skip layer  2
[0, 0, 2, 0, 1, 5, 5, 2, 3, 0, 0, 4, 0, 3]
optimize layer  3
backward train epoch:  3
test acc:  0.7964
forward train acc:  0.99852  and loss:  2.271101698745042
test acc:  0.9171
forward train acc:  0.99866  and loss:  2.0395051742671058
test acc:  0.9141
forward train acc:  0.99886  and loss:  1.935963922413066
test acc:  0.9176
forward train acc:  0.99882  and loss:  1.8515992216998711
test acc:  0.9168
forward train acc:  0.9992  and loss:  1.3034335072152317
test acc:  0.9169
forward train acc:  0.99896  and loss:  1.5969810380483977
test acc:  0.9187
forward train acc:  0.99946  and loss:  1.077094760723412
test acc:  0.9179
forward train acc:  0.9995  and loss:  0.9427244730759412
test acc:  0.916
forward train acc:  0.99938  and loss:  1.0270960521302186
test acc:  0.9171
forward train acc:  0.99932  and loss:  1.070985498954542
test acc:  0.9172
********** reverse layer  3  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5833333333333334  ==>  40 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7838541666666666  ==>  166 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6770833333333334  ==>  248 / 768
***** skip layer  4
[0, 0, 2, 5, 0, 5, 5, 2, 3, 0, 0, 4, 0, 3]
***** skip layer  5
[0, 0, 2, 5, 0, 4, 5, 2, 3, 0, 0, 4, 0, 3]
***** skip layer  6
[0, 0, 2, 5, 0, 4, 4, 2, 3, 0, 0, 4, 0, 3]
***** skip layer  7
[0, 0, 2, 5, 0, 4, 4, 1, 3, 0, 0, 4, 0, 3]
***** skip layer  8
[0, 0, 2, 5, 0, 4, 4, 1, 2, 0, 0, 4, 0, 3]
optimize layer  9
backward train epoch:  60
test acc:  0.0859
forward train acc:  0.99964  and loss:  0.9168270687805489
test acc:  0.9173
forward train acc:  0.9994  and loss:  1.174078099313192
test acc:  0.9174
forward train acc:  0.9993  and loss:  1.2572109416360036
test acc:  0.9192
forward train acc:  0.99934  and loss:  1.2944278412032872
test acc:  0.9184
forward train acc:  0.99974  and loss:  0.6091799552668817
test acc:  0.9189
forward train acc:  0.99956  and loss:  0.8274875020724721
test acc:  0.9186
forward train acc:  0.99956  and loss:  0.9008646343136206
test acc:  0.9184
forward train acc:  0.99956  and loss:  0.7555832599755377
test acc:  0.9181
forward train acc:  0.99976  and loss:  0.5683561791665852
test acc:  0.9198
forward train acc:  0.99972  and loss:  0.5988952036714181
test acc:  0.9192
********** reverse layer  9  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5833333333333334  ==>  40 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7838541666666666  ==>  166 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6770833333333334  ==>  248 / 768
optimize layer  10
backward train epoch:  24
test acc:  0.0881
forward train acc:  0.99962  and loss:  0.6711940542445518
test acc:  0.9169
forward train acc:  0.9995  and loss:  0.9515172817627899
test acc:  0.917
forward train acc:  0.99944  and loss:  1.0263963042525575
test acc:  0.9174
forward train acc:  0.99954  and loss:  0.8151414007879794
test acc:  0.9193
forward train acc:  0.99966  and loss:  0.6377395290182903
test acc:  0.9166
forward train acc:  0.99948  and loss:  0.8662669321638532
test acc:  0.9163
forward train acc:  0.99966  and loss:  0.843300369597273
test acc:  0.9179
forward train acc:  0.9997  and loss:  0.6207720233360305
test acc:  0.9185
forward train acc:  0.99974  and loss:  0.6155732229235582
test acc:  0.9179
forward train acc:  0.9997  and loss:  0.6091357683180831
test acc:  0.9182
********** reverse layer  10  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5833333333333334  ==>  40 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7838541666666666  ==>  166 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6770833333333334  ==>  248 / 768
***** skip layer  11
[0, 0, 2, 5, 0, 4, 4, 1, 2, 5, 5, 3, 0, 3]
optimize layer  12
backward train epoch:  443
test acc:  0.1
forward train acc:  0.1  and loss:  1296.6347942352295
test acc:  0.1
forward train acc:  0.1  and loss:  1264.2874913215637
test acc:  0.1
forward train acc:  0.1  and loss:  1234.3144445419312
test acc:  0.1
forward train acc:  0.1  and loss:  1213.0812859535217
test acc:  0.1
forward train acc:  0.1  and loss:  1199.3514404296875
test acc:  0.1
forward train acc:  0.1  and loss:  1186.1289069652557
test acc:  0.1
forward train acc:  0.1  and loss:  1173.1477029323578
test acc:  0.1
forward train acc:  0.1  and loss:  1163.5952575206757
test acc:  0.1
forward train acc:  0.1  and loss:  1157.3801350593567
test acc:  0.1
forward train acc:  0.1  and loss:  1151.1686198711395
test acc:  0.1
********** reverse layer  12  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5833333333333334  ==>  40 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7838541666666666  ==>  166 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6770833333333334  ==>  248 / 768
***** skip layer  13
[0, 0, 2, 5, 0, 4, 4, 1, 2, 5, 5, 3, 5, 2]
optimize layer  0
backward train epoch:  255
test acc:  0.0922
forward train acc:  0.88478  and loss:  201.1257947832346
test acc:  0.8483
forward train acc:  0.9216  and loss:  105.24832912534475
test acc:  0.8606
forward train acc:  0.9358  and loss:  84.83383421599865
test acc:  0.867
forward train acc:  0.94544  and loss:  69.8589067272842
test acc:  0.8708
forward train acc:  0.9502  and loss:  64.36762734875083
test acc:  0.8746
forward train acc:  0.9513  and loss:  60.91454986482859
test acc:  0.8763
forward train acc:  0.95702  and loss:  54.26653256267309
test acc:  0.8779
forward train acc:  0.96022  and loss:  50.815979136154056
test acc:  0.8789
forward train acc:  0.96232  and loss:  48.44339795783162
test acc:  0.8802
forward train acc:  0.962  and loss:  48.11949134245515
test acc:  0.8817
********** reverse layer  0  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.5833333333333334  ==>  40 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7838541666666666  ==>  166 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6770833333333334  ==>  248 / 768
optimize layer  1
backward train epoch:  112
test acc:  0.0877
forward train acc:  0.99662  and loss:  4.980947624775581
test acc:  0.9145
forward train acc:  0.99838  and loss:  2.406500615295954
test acc:  0.9176
forward train acc:  0.99902  and loss:  1.70447774551576
test acc:  0.9182
forward train acc:  0.99902  and loss:  1.5552687334711663
test acc:  0.9182
forward train acc:  0.99906  and loss:  1.5831121423980221
test acc:  0.917
forward train acc:  0.99944  and loss:  1.0468118688440882
test acc:  0.9183
forward train acc:  0.9991  and loss:  1.472508902195841
test acc:  0.919
forward train acc:  0.9992  and loss:  1.2115566448192112
test acc:  0.9207
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7838541666666666  ==>  166 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6770833333333334  ==>  248 / 768
***** skip layer  2
[5, 0, 1, 5, 0, 4, 4, 1, 2, 5, 5, 3, 5, 2]
***** skip layer  3
[5, 0, 1, 4, 0, 4, 4, 1, 2, 5, 5, 3, 5, 2]
optimize layer  4
backward train epoch:  45
test acc:  0.0816
forward train acc:  0.99854  and loss:  2.117453232232947
test acc:  0.9151
forward train acc:  0.99862  and loss:  2.1588233954971656
test acc:  0.9185
forward train acc:  0.99884  and loss:  1.8021273122285493
test acc:  0.9177
forward train acc:  0.9992  and loss:  1.3684577007661574
test acc:  0.9191
forward train acc:  0.99894  and loss:  1.5575378218200058
test acc:  0.9193
forward train acc:  0.99904  and loss:  1.3575610323459841
test acc:  0.9175
forward train acc:  0.99942  and loss:  1.0979155210661702
test acc:  0.9174
forward train acc:  0.99922  and loss:  1.1650434976327233
test acc:  0.9184
forward train acc:  0.9994  and loss:  1.0672330672387034
test acc:  0.9187
forward train acc:  0.99926  and loss:  1.203671561786905
test acc:  0.9191
********** reverse layer  4  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7838541666666666  ==>  166 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6770833333333334  ==>  248 / 768
***** skip layer  5
[5, 0, 1, 4, 5, 3, 4, 1, 2, 5, 5, 3, 5, 2]
***** skip layer  6
[5, 0, 1, 4, 5, 3, 3, 1, 2, 5, 5, 3, 5, 2]
***** skip layer  7
[5, 0, 1, 4, 5, 3, 3, 0, 2, 5, 5, 3, 5, 2]
***** skip layer  8
[5, 0, 1, 4, 5, 3, 3, 0, 1, 5, 5, 3, 5, 2]
***** skip layer  9
[5, 0, 1, 4, 5, 3, 3, 0, 1, 4, 5, 3, 5, 2]
***** skip layer  10
[5, 0, 1, 4, 5, 3, 3, 0, 1, 4, 4, 3, 5, 2]
***** skip layer  11
[5, 0, 1, 4, 5, 3, 3, 0, 1, 4, 4, 2, 5, 2]
***** skip layer  12
[5, 0, 1, 4, 5, 3, 3, 0, 1, 4, 4, 2, 4, 2]
***** skip layer  13
[5, 0, 1, 4, 5, 3, 3, 0, 1, 4, 4, 2, 4, 1]
***** skip layer  0
[4, 0, 1, 4, 5, 3, 3, 0, 1, 4, 4, 2, 4, 1]
optimize layer  1
backward train epoch:  36
test acc:  0.0959
forward train acc:  0.99886  and loss:  1.8674060456105508
test acc:  0.9144
forward train acc:  0.999  and loss:  1.5890815262682736
test acc:  0.9164
forward train acc:  0.99912  and loss:  1.3580930946627632
test acc:  0.9166
forward train acc:  0.99926  and loss:  1.2932461160817184
test acc:  0.9178
forward train acc:  0.99912  and loss:  1.3128682140959427
test acc:  0.9172
forward train acc:  0.99942  and loss:  0.9592878887779079
test acc:  0.9156
forward train acc:  0.99938  and loss:  0.9485845272429287
test acc:  0.9173
forward train acc:  0.99958  and loss:  0.8181877787574194
test acc:  0.9179
forward train acc:  0.99954  and loss:  0.778555196186062
test acc:  0.9175
forward train acc:  0.99944  and loss:  0.9551363670616411
test acc:  0.9171
********** reverse layer  1  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7838541666666666  ==>  166 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6770833333333334  ==>  248 / 768
***** skip layer  2
[4, 5, 0, 4, 5, 3, 3, 0, 1, 4, 4, 2, 4, 1]
***** skip layer  3
[4, 5, 0, 3, 5, 3, 3, 0, 1, 4, 4, 2, 4, 1]
***** skip layer  4
[4, 5, 0, 3, 4, 3, 3, 0, 1, 4, 4, 2, 4, 1]
***** skip layer  5
[4, 5, 0, 3, 4, 2, 3, 0, 1, 4, 4, 2, 4, 1]
***** skip layer  6
[4, 5, 0, 3, 4, 2, 2, 0, 1, 4, 4, 2, 4, 1]
optimize layer  7
backward train epoch:  241
test acc:  0.094
forward train acc:  0.99816  and loss:  2.233222296112217
test acc:  0.9136
forward train acc:  0.9988  and loss:  1.744472791266162
test acc:  0.9157
forward train acc:  0.9988  and loss:  1.6653517735539936
test acc:  0.9166
forward train acc:  0.99914  and loss:  1.2761147961136885
test acc:  0.9186
forward train acc:  0.99932  and loss:  1.0719520318089053
test acc:  0.9175
forward train acc:  0.99918  and loss:  1.0823124077287503
test acc:  0.9192
forward train acc:  0.9993  and loss:  1.0974895777762868
test acc:  0.9177
forward train acc:  0.99944  and loss:  0.9200372491613962
test acc:  0.9173
forward train acc:  0.99922  and loss:  1.1368200689321384
test acc:  0.9192
forward train acc:  0.99934  and loss:  1.1360636848839931
test acc:  0.9174
********** reverse layer  7  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7838541666666666  ==>  166 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6770833333333334  ==>  248 / 768
***** skip layer  8
[4, 5, 0, 3, 4, 2, 2, 5, 0, 4, 4, 2, 4, 1]
***** skip layer  9
[4, 5, 0, 3, 4, 2, 2, 5, 0, 3, 4, 2, 4, 1]
***** skip layer  10
[4, 5, 0, 3, 4, 2, 2, 5, 0, 3, 3, 2, 4, 1]
***** skip layer  11
[4, 5, 0, 3, 4, 2, 2, 5, 0, 3, 3, 1, 4, 1]
***** skip layer  12
[4, 5, 0, 3, 4, 2, 2, 5, 0, 3, 3, 1, 3, 1]
***** skip layer  13
[4, 5, 0, 3, 4, 2, 2, 5, 0, 3, 3, 1, 3, 0]
***** skip layer  0
[3, 5, 0, 3, 4, 2, 2, 5, 0, 3, 3, 1, 3, 0]
***** skip layer  1
[3, 4, 0, 3, 4, 2, 2, 5, 0, 3, 3, 1, 3, 0]
optimize layer  2
backward train epoch:  433
test acc:  0.116
forward train acc:  0.94102  and loss:  93.71978110447526
test acc:  0.8797
forward train acc:  0.95768  and loss:  56.784322801977396
test acc:  0.8878
forward train acc:  0.96562  and loss:  42.852411936968565
test acc:  0.8921
forward train acc:  0.97194  and loss:  35.16113970987499
test acc:  0.8946
forward train acc:  0.97498  and loss:  31.228032457642257
test acc:  0.8955
forward train acc:  0.97686  and loss:  29.151518271304667
test acc:  0.8969
forward train acc:  0.97932  and loss:  25.70142710255459
test acc:  0.8971
forward train acc:  0.98194  and loss:  22.687603290192783
test acc:  0.8982
forward train acc:  0.98136  and loss:  23.132800987455994
test acc:  0.8983
forward train acc:  0.9827  and loss:  22.00244941562414
test acc:  0.8994
********** reverse layer  2  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7838541666666666  ==>  166 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6770833333333334  ==>  248 / 768
***** skip layer  3
[3, 4, 5, 2, 4, 2, 2, 5, 0, 3, 3, 1, 3, 0]
***** skip layer  4
[3, 4, 5, 2, 3, 2, 2, 5, 0, 3, 3, 1, 3, 0]
***** skip layer  5
[3, 4, 5, 2, 3, 1, 2, 5, 0, 3, 3, 1, 3, 0]
***** skip layer  6
[3, 4, 5, 2, 3, 1, 1, 5, 0, 3, 3, 1, 3, 0]
***** skip layer  7
[3, 4, 5, 2, 3, 1, 1, 4, 0, 3, 3, 1, 3, 0]
optimize layer  8
backward train epoch:  44
test acc:  0.1217
forward train acc:  0.99442  and loss:  8.340972802252509
test acc:  0.9142
forward train acc:  0.99762  and loss:  3.3835461592534557
test acc:  0.9137
forward train acc:  0.9983  and loss:  2.600057397678029
test acc:  0.9164
forward train acc:  0.99868  and loss:  2.0835924252751283
test acc:  0.9167
forward train acc:  0.9984  and loss:  2.2734812035923824
test acc:  0.9174
forward train acc:  0.99898  and loss:  1.7515991670661606
test acc:  0.9166
forward train acc:  0.999  and loss:  1.5847901148954406
test acc:  0.9175
forward train acc:  0.99888  and loss:  1.614416511671152
test acc:  0.9173
forward train acc:  0.99912  and loss:  1.3911779372720048
test acc:  0.9175
forward train acc:  0.99912  and loss:  1.3748759994050488
test acc:  0.918
********** reverse layer  8  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7838541666666666  ==>  166 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6770833333333334  ==>  248 / 768
***** skip layer  9
[3, 4, 5, 2, 3, 1, 1, 4, 5, 2, 3, 1, 3, 0]
***** skip layer  10
[3, 4, 5, 2, 3, 1, 1, 4, 5, 2, 2, 1, 3, 0]
***** skip layer  11
[3, 4, 5, 2, 3, 1, 1, 4, 5, 2, 2, 0, 3, 0]
***** skip layer  12
[3, 4, 5, 2, 3, 1, 1, 4, 5, 2, 2, 0, 2, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.0958
forward train acc:  0.99938  and loss:  1.148685828258749
test acc:  0.9197
forward train acc:  0.99974  and loss:  0.6475562831619754
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7838541666666666  ==>  166 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  0
[2, 4, 5, 2, 3, 1, 1, 4, 5, 2, 2, 0, 2, 0]
***** skip layer  1
[2, 3, 5, 2, 3, 1, 1, 4, 5, 2, 2, 0, 2, 0]
***** skip layer  2
[2, 3, 4, 2, 3, 1, 1, 4, 5, 2, 2, 0, 2, 0]
***** skip layer  3
[2, 3, 4, 1, 3, 1, 1, 4, 5, 2, 2, 0, 2, 0]
***** skip layer  4
[2, 3, 4, 1, 2, 1, 1, 4, 5, 2, 2, 0, 2, 0]
***** skip layer  5
[2, 3, 4, 1, 2, 0, 1, 4, 5, 2, 2, 0, 2, 0]
***** skip layer  6
[2, 3, 4, 1, 2, 0, 0, 4, 5, 2, 2, 0, 2, 0]
***** skip layer  7
[2, 3, 4, 1, 2, 0, 0, 3, 5, 2, 2, 0, 2, 0]
***** skip layer  8
[2, 3, 4, 1, 2, 0, 0, 3, 4, 2, 2, 0, 2, 0]
***** skip layer  9
[2, 3, 4, 1, 2, 0, 0, 3, 4, 1, 2, 0, 2, 0]
***** skip layer  10
[2, 3, 4, 1, 2, 0, 0, 3, 4, 1, 1, 0, 2, 0]
optimize layer  11
backward train epoch:  110
test acc:  0.0862
forward train acc:  0.99966  and loss:  0.6731424092431553
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7916666666666666  ==>  160 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  12
[2, 3, 4, 1, 2, 0, 0, 3, 4, 1, 1, 0, 1, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.0891
forward train acc:  0.79262  and loss:  232.75038370490074
test acc:  0.7292
forward train acc:  0.79942  and loss:  213.25962561368942
test acc:  0.7304
forward train acc:  0.7994  and loss:  204.92973524332047
test acc:  0.7304
forward train acc:  0.79958  and loss:  198.58145648241043
test acc:  0.7316
forward train acc:  0.79964  and loss:  194.65144728124142
test acc:  0.7334
forward train acc:  0.79948  and loss:  190.90966561436653
test acc:  0.7305
forward train acc:  0.7996  and loss:  187.01965257525444
test acc:  0.7331
forward train acc:  0.79984  and loss:  183.8165214061737
test acc:  0.7341
forward train acc:  0.80008  and loss:  182.10524907708168
test acc:  0.7349
forward train acc:  0.8007  and loss:  180.04841542243958
test acc:  0.7361
********** reverse layer  13  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7916666666666666  ==>  160 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  0
[1, 3, 4, 1, 2, 0, 0, 3, 4, 1, 1, 0, 1, 5]
***** skip layer  1
[1, 2, 4, 1, 2, 0, 0, 3, 4, 1, 1, 0, 1, 5]
***** skip layer  2
[1, 2, 3, 1, 2, 0, 0, 3, 4, 1, 1, 0, 1, 5]
***** skip layer  3
[1, 2, 3, 0, 2, 0, 0, 3, 4, 1, 1, 0, 1, 5]
***** skip layer  4
[1, 2, 3, 0, 1, 0, 0, 3, 4, 1, 1, 0, 1, 5]
optimize layer  5
backward train epoch:  93
test acc:  0.0955
forward train acc:  0.99928  and loss:  1.358149733627215
test acc:  0.9147
forward train acc:  0.99914  and loss:  1.4671662720502354
test acc:  0.9154
forward train acc:  0.9989  and loss:  1.5619714698987082
test acc:  0.9168
forward train acc:  0.99928  and loss:  1.1756661836698186
test acc:  0.9174
forward train acc:  0.99952  and loss:  0.7963154266471975
test acc:  0.9184
forward train acc:  0.99958  and loss:  0.7142564418900292
test acc:  0.918
forward train acc:  0.9995  and loss:  0.9214368849934544
test acc:  0.9163
forward train acc:  0.99956  and loss:  0.8095027128583752
test acc:  0.9181
forward train acc:  0.99966  and loss:  0.49683105101576075
test acc:  0.9181
forward train acc:  0.99966  and loss:  0.48691552464151755
test acc:  0.9189
********** reverse layer  5  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7916666666666666  ==>  160 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
optimize layer  6
backward train epoch:  291
test acc:  0.1003
forward train acc:  0.99924  and loss:  1.128844460559776
test acc:  0.9171
forward train acc:  0.99942  and loss:  0.7802544600563124
test acc:  0.9182
forward train acc:  0.99954  and loss:  0.7046986574132461
test acc:  0.916
forward train acc:  0.99936  and loss:  1.0778662789962254
test acc:  0.9166
forward train acc:  0.99964  and loss:  0.5787376194493845
test acc:  0.9163
forward train acc:  0.99966  and loss:  0.6204235519107897
test acc:  0.9173
forward train acc:  0.99952  and loss:  0.7131447166320868
test acc:  0.9175
forward train acc:  0.99952  and loss:  0.6059985080355546
test acc:  0.9171
forward train acc:  0.99976  and loss:  0.47002864466048777
test acc:  0.9177
forward train acc:  0.99972  and loss:  0.3976749261491932
test acc:  0.916
********** reverse layer  6  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7916666666666666  ==>  160 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  7
[1, 2, 3, 0, 1, 5, 5, 2, 4, 1, 1, 0, 1, 5]
***** skip layer  8
[1, 2, 3, 0, 1, 5, 5, 2, 3, 1, 1, 0, 1, 5]
***** skip layer  9
[1, 2, 3, 0, 1, 5, 5, 2, 3, 0, 1, 0, 1, 5]
***** skip layer  10
[1, 2, 3, 0, 1, 5, 5, 2, 3, 0, 0, 0, 1, 5]
optimize layer  11
backward train epoch:  88
test acc:  0.0862
forward train acc:  0.99968  and loss:  0.5178899224119959
test acc:  0.9159
forward train acc:  0.99958  and loss:  0.7097243935859296
test acc:  0.9165
forward train acc:  0.99944  and loss:  0.90061671246076
test acc:  0.9171
forward train acc:  0.9996  and loss:  0.7266898915404454
test acc:  0.9178
forward train acc:  0.99958  and loss:  0.6369054933602456
test acc:  0.9177
forward train acc:  0.99958  and loss:  0.6087940436555073
test acc:  0.9173
forward train acc:  0.99946  and loss:  0.6648377345700283
test acc:  0.9159
forward train acc:  0.99976  and loss:  0.38081989070633426
test acc:  0.9171
forward train acc:  0.99984  and loss:  0.3339139281597454
test acc:  0.917
forward train acc:  0.99968  and loss:  0.38417654592194594
test acc:  0.9172
********** reverse layer  11  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7916666666666666  ==>  160 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  12
[1, 2, 3, 0, 1, 5, 5, 2, 3, 0, 0, 5, 0, 5]
***** skip layer  13
[1, 2, 3, 0, 1, 5, 5, 2, 3, 0, 0, 5, 0, 4]
***** skip layer  0
[0, 2, 3, 0, 1, 5, 5, 2, 3, 0, 0, 5, 0, 4]
***** skip layer  1
[0, 1, 3, 0, 1, 5, 5, 2, 3, 0, 0, 5, 0, 4]
***** skip layer  2
[0, 1, 2, 0, 1, 5, 5, 2, 3, 0, 0, 5, 0, 4]
optimize layer  3
backward train epoch:  1
test acc:  0.8919
forward train acc:  0.99912  and loss:  1.2021086225868203
test acc:  0.9139
forward train acc:  0.99906  and loss:  1.1586346810217947
test acc:  0.9139
forward train acc:  0.99918  and loss:  1.0370142502506496
test acc:  0.9158
forward train acc:  0.99946  and loss:  0.7966227387078106
test acc:  0.9162
forward train acc:  0.99936  and loss:  0.7795554793556221
test acc:  0.9164
forward train acc:  0.99944  and loss:  0.7466646555694751
test acc:  0.9158
forward train acc:  0.99938  and loss:  1.0369685179903172
test acc:  0.9159
forward train acc:  0.99956  and loss:  0.7155701886804309
test acc:  0.9169
forward train acc:  0.99942  and loss:  0.6836864036449697
test acc:  0.9171
forward train acc:  0.99956  and loss:  0.6056949578778585
test acc:  0.918
********** reverse layer  3  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7916666666666666  ==>  160 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  4
[0, 1, 2, 5, 0, 5, 5, 2, 3, 0, 0, 5, 0, 4]
***** skip layer  5
[0, 1, 2, 5, 0, 4, 5, 2, 3, 0, 0, 5, 0, 4]
***** skip layer  6
[0, 1, 2, 5, 0, 4, 4, 2, 3, 0, 0, 5, 0, 4]
***** skip layer  7
[0, 1, 2, 5, 0, 4, 4, 1, 3, 0, 0, 5, 0, 4]
***** skip layer  8
[0, 1, 2, 5, 0, 4, 4, 1, 2, 0, 0, 5, 0, 4]
optimize layer  9
backward train epoch:  53
test acc:  0.0919
forward train acc:  0.99962  and loss:  0.6157119196141139
test acc:  0.9174
forward train acc:  0.99972  and loss:  0.47403016296448186
test acc:  0.9169
forward train acc:  0.99932  and loss:  0.9280873644165695
test acc:  0.9173
forward train acc:  0.99958  and loss:  0.7219550322770374
test acc:  0.9182
forward train acc:  0.99962  and loss:  0.5418908877763897
test acc:  0.918
forward train acc:  0.99984  and loss:  0.39227297113393433
test acc:  0.9175
forward train acc:  0.9998  and loss:  0.40558926897938363
test acc:  0.9176
forward train acc:  0.9998  and loss:  0.40874577540671453
test acc:  0.9186
forward train acc:  0.99966  and loss:  0.5610641110833967
test acc:  0.9171
forward train acc:  0.99976  and loss:  0.3880185247398913
test acc:  0.9159
********** reverse layer  9  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7578125  ==>  186 / 768
layer  11  :  0.7916666666666666  ==>  160 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
optimize layer  10
backward train epoch:  72
test acc:  0.1014
forward train acc:  0.99958  and loss:  0.7027689375099726
test acc:  0.9178
forward train acc:  0.99962  and loss:  0.5575074967346154
test acc:  0.9182
forward train acc:  0.9994  and loss:  0.6452365074655972
test acc:  0.9199
forward train acc:  0.99954  and loss:  0.5940069008647697
test acc:  0.9187
forward train acc:  0.99956  and loss:  0.5810665367753245
test acc:  0.9191
forward train acc:  0.99968  and loss:  0.5687229905452114
test acc:  0.9185
forward train acc:  0.9998  and loss:  0.4449328284827061
test acc:  0.9184
forward train acc:  0.99984  and loss:  0.37425101366534363
test acc:  0.9191
forward train acc:  0.99978  and loss:  0.45868258184054866
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.765625  ==>  180 / 768
layer  11  :  0.7916666666666666  ==>  160 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  11
[0, 1, 2, 5, 0, 4, 4, 1, 2, 5, 0, 4, 0, 4]
optimize layer  12
backward train epoch:  211
test acc:  0.1
forward train acc:  0.1  and loss:  1141.9452579021454
test acc:  0.1
forward train acc:  0.1  and loss:  1115.581173658371
test acc:  0.1
forward train acc:  0.1  and loss:  1091.2380759716034
test acc:  0.1
forward train acc:  0.1  and loss:  1074.3555710315704
test acc:  0.1
forward train acc:  0.1  and loss:  1063.9269394874573
test acc:  0.1
forward train acc:  0.1  and loss:  1053.8418073654175
test acc:  0.1
forward train acc:  0.1  and loss:  1044.0928084850311
test acc:  0.1
forward train acc:  0.1  and loss:  1037.195837020874
test acc:  0.1
forward train acc:  0.1  and loss:  1032.614542722702
test acc:  0.1
forward train acc:  0.1  and loss:  1028.144965171814
test acc:  0.1
********** reverse layer  12  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.765625  ==>  180 / 768
layer  11  :  0.7916666666666666  ==>  160 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  13
[0, 1, 2, 5, 0, 4, 4, 1, 2, 5, 0, 4, 5, 3]
optimize layer  0
backward train epoch:  241
test acc:  0.0994
forward train acc:  0.7857  and loss:  386.79463732242584
test acc:  0.7876
forward train acc:  0.8447  and loss:  198.43304248154163
test acc:  0.8181
forward train acc:  0.86966  and loss:  161.9213075041771
test acc:  0.8272
forward train acc:  0.88734  and loss:  140.69458343088627
test acc:  0.8386
forward train acc:  0.89536  and loss:  130.7792134359479
test acc:  0.8405
forward train acc:  0.90186  and loss:  119.10258024930954
test acc:  0.8472
forward train acc:  0.90814  and loss:  112.57155111432076
test acc:  0.8531
forward train acc:  0.91504  and loss:  105.89157573133707
test acc:  0.8558
forward train acc:  0.91778  and loss:  101.852667696774
test acc:  0.8571
forward train acc:  0.92052  and loss:  97.57037451118231
test acc:  0.8585
********** reverse layer  0  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.5833333333333334  ==>  160 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.765625  ==>  180 / 768
layer  11  :  0.7916666666666666  ==>  160 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  1
[5, 0, 2, 5, 0, 4, 4, 1, 2, 5, 0, 4, 5, 3]
***** skip layer  2
[5, 0, 1, 5, 0, 4, 4, 1, 2, 5, 0, 4, 5, 3]
***** skip layer  3
[5, 0, 1, 4, 0, 4, 4, 1, 2, 5, 0, 4, 5, 3]
optimize layer  4
backward train epoch:  90
test acc:  0.1144
forward train acc:  0.99724  and loss:  4.756567960837856
test acc:  0.9165
forward train acc:  0.99842  and loss:  2.001177040510811
test acc:  0.9191
forward train acc:  0.99874  and loss:  1.909289774368517
test acc:  0.9185
forward train acc:  0.99914  and loss:  1.2428327381494455
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.59375  ==>  156 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.765625  ==>  180 / 768
layer  11  :  0.7916666666666666  ==>  160 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  5
[5, 0, 1, 4, 0, 3, 4, 1, 2, 5, 0, 4, 5, 3]
***** skip layer  6
[5, 0, 1, 4, 0, 3, 3, 1, 2, 5, 0, 4, 5, 3]
***** skip layer  7
[5, 0, 1, 4, 0, 3, 3, 0, 2, 5, 0, 4, 5, 3]
***** skip layer  8
[5, 0, 1, 4, 0, 3, 3, 0, 1, 5, 0, 4, 5, 3]
***** skip layer  9
[5, 0, 1, 4, 0, 3, 3, 0, 1, 4, 0, 4, 5, 3]
optimize layer  10
backward train epoch:  103
test acc:  0.0988
forward train acc:  0.9989  and loss:  1.3697688384272624
test acc:  0.9196
forward train acc:  0.999  and loss:  1.3722078165155835
test acc:  0.9176
forward train acc:  0.99898  and loss:  1.4204808048089035
test acc:  0.9174
forward train acc:  0.99936  and loss:  0.8975417620385997
test acc:  0.9195
forward train acc:  0.99936  and loss:  0.880046421152656
test acc:  0.9157
forward train acc:  0.9994  and loss:  0.9203313314064872
test acc:  0.9181
forward train acc:  0.99954  and loss:  0.6041636337031377
test acc:  0.919
forward train acc:  0.99948  and loss:  0.7359978219901677
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.59375  ==>  156 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7734375  ==>  174 / 768
layer  11  :  0.7916666666666666  ==>  160 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  11
[5, 0, 1, 4, 0, 3, 3, 0, 1, 4, 0, 3, 5, 3]
***** skip layer  12
[5, 0, 1, 4, 0, 3, 3, 0, 1, 4, 0, 3, 4, 3]
***** skip layer  13
[5, 0, 1, 4, 0, 3, 3, 0, 1, 4, 0, 3, 4, 2]
***** skip layer  0
[4, 0, 1, 4, 0, 3, 3, 0, 1, 4, 0, 3, 4, 2]
optimize layer  1
backward train epoch:  184
test acc:  0.1184
forward train acc:  0.99672  and loss:  4.677313800493721
test acc:  0.9141
forward train acc:  0.9974  and loss:  3.3923632137593813
test acc:  0.915
forward train acc:  0.99786  and loss:  2.4870923704002053
test acc:  0.9137
forward train acc:  0.99834  and loss:  2.411612172378227
test acc:  0.9163
forward train acc:  0.99894  and loss:  1.4215697620238643
test acc:  0.9172
forward train acc:  0.99886  and loss:  1.545445664523868
test acc:  0.9169
forward train acc:  0.99902  and loss:  1.2430810969672166
test acc:  0.9161
forward train acc:  0.99884  and loss:  1.3089325591136003
test acc:  0.9158
forward train acc:  0.99886  and loss:  1.4274471504322719
test acc:  0.9169
forward train acc:  0.99892  and loss:  1.295479846012313
test acc:  0.9157
********** reverse layer  1  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.59375  ==>  156 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7734375  ==>  174 / 768
layer  11  :  0.7916666666666666  ==>  160 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  2
[4, 5, 0, 4, 0, 3, 3, 0, 1, 4, 0, 3, 4, 2]
***** skip layer  3
[4, 5, 0, 3, 0, 3, 3, 0, 1, 4, 0, 3, 4, 2]
optimize layer  4
backward train epoch:  121
test acc:  0.1246
forward train acc:  0.99878  and loss:  1.8425177090684883
test acc:  0.9162
forward train acc:  0.99876  and loss:  1.5091190045350231
test acc:  0.9195
forward train acc:  0.99918  and loss:  1.0646945615590084
test acc:  0.9161
forward train acc:  0.99934  and loss:  0.8663395920884795
test acc:  0.9183
forward train acc:  0.9993  and loss:  0.8204376085777767
test acc:  0.9198
forward train acc:  0.99946  and loss:  0.7409908826230094
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7734375  ==>  174 / 768
layer  11  :  0.7916666666666666  ==>  160 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  5
[4, 5, 0, 3, 0, 2, 3, 0, 1, 4, 0, 3, 4, 2]
***** skip layer  6
[4, 5, 0, 3, 0, 2, 2, 0, 1, 4, 0, 3, 4, 2]
optimize layer  7
backward train epoch:  90
test acc:  0.0948
forward train acc:  0.99752  and loss:  3.650780414347537
test acc:  0.9127
forward train acc:  0.99818  and loss:  2.412242488760967
test acc:  0.9147
forward train acc:  0.99852  and loss:  1.861048247141298
test acc:  0.9139
forward train acc:  0.99848  and loss:  1.8220592417928856
test acc:  0.9154
forward train acc:  0.9989  and loss:  1.3444572565786075
test acc:  0.915
forward train acc:  0.99912  and loss:  1.1169213322573341
test acc:  0.9159
forward train acc:  0.99926  and loss:  1.2118316006963141
test acc:  0.916
forward train acc:  0.9989  and loss:  1.4815173650276847
test acc:  0.9169
forward train acc:  0.99922  and loss:  1.0584238437586464
test acc:  0.9166
forward train acc:  0.9989  and loss:  1.1826162648503669
test acc:  0.9168
********** reverse layer  7  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7734375  ==>  174 / 768
layer  11  :  0.7916666666666666  ==>  160 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  8
[4, 5, 0, 3, 0, 2, 2, 5, 0, 4, 0, 3, 4, 2]
***** skip layer  9
[4, 5, 0, 3, 0, 2, 2, 5, 0, 3, 0, 3, 4, 2]
optimize layer  10
backward train epoch:  66
test acc:  0.0977
forward train acc:  0.99928  and loss:  1.0722745894890977
test acc:  0.917
forward train acc:  0.99934  and loss:  0.9292132949922234
test acc:  0.9184
forward train acc:  0.99914  and loss:  1.0765924236620776
test acc:  0.9174
forward train acc:  0.9992  and loss:  0.8114274903200567
test acc:  0.9183
forward train acc:  0.9995  and loss:  0.754194166554953
test acc:  0.9185
forward train acc:  0.99938  and loss:  0.7718258036766201
test acc:  0.9192
forward train acc:  0.99944  and loss:  0.8085020149010234
test acc:  0.9202
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5208333333333334  ==>  92 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.78125  ==>  168 / 768
layer  11  :  0.7916666666666666  ==>  160 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  11
[4, 5, 0, 3, 0, 2, 2, 5, 0, 3, 0, 2, 4, 2]
***** skip layer  12
[4, 5, 0, 3, 0, 2, 2, 5, 0, 3, 0, 2, 3, 2]
***** skip layer  13
[4, 5, 0, 3, 0, 2, 2, 5, 0, 3, 0, 2, 3, 1]
***** skip layer  0
[3, 5, 0, 3, 0, 2, 2, 5, 0, 3, 0, 2, 3, 1]
***** skip layer  1
[3, 4, 0, 3, 0, 2, 2, 5, 0, 3, 0, 2, 3, 1]
optimize layer  2
backward train epoch:  71
test acc:  0.0996
forward train acc:  0.99936  and loss:  0.9220407927641645
test acc:  0.9181
forward train acc:  0.99936  and loss:  0.9980736369179795
test acc:  0.9175
forward train acc:  0.99914  and loss:  1.1442290653067175
test acc:  0.9173
forward train acc:  0.99928  and loss:  0.9726757492462639
test acc:  0.9203
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.53125  ==>  90 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.78125  ==>  168 / 768
layer  11  :  0.7916666666666666  ==>  160 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  3
[3, 4, 0, 2, 0, 2, 2, 5, 0, 3, 0, 2, 3, 1]
optimize layer  4
backward train epoch:  246
test acc:  0.0944
forward train acc:  0.99866  and loss:  1.7134301007026806
test acc:  0.9152
forward train acc:  0.9988  and loss:  1.7869230602809694
test acc:  0.9173
forward train acc:  0.99868  and loss:  1.6985247076954693
test acc:  0.9165
forward train acc:  0.99906  and loss:  1.1514732633368112
test acc:  0.9187
forward train acc:  0.99916  and loss:  0.9656720525817946
test acc:  0.9194
forward train acc:  0.99924  and loss:  1.0941967940016184
test acc:  0.9186
forward train acc:  0.99936  and loss:  0.7873274579178542
test acc:  0.9179
forward train acc:  0.9994  and loss:  0.8705478282208787
test acc:  0.9174
forward train acc:  0.99952  and loss:  0.6437923005432822
test acc:  0.9185
forward train acc:  0.99942  and loss:  0.8630877938703634
test acc:  0.9163
********** reverse layer  4  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.53125  ==>  90 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.78125  ==>  168 / 768
layer  11  :  0.7916666666666666  ==>  160 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  5
[3, 4, 0, 2, 5, 1, 2, 5, 0, 3, 0, 2, 3, 1]
***** skip layer  6
[3, 4, 0, 2, 5, 1, 1, 5, 0, 3, 0, 2, 3, 1]
***** skip layer  7
[3, 4, 0, 2, 5, 1, 1, 4, 0, 3, 0, 2, 3, 1]
optimize layer  8
backward train epoch:  61
test acc:  0.1
forward train acc:  0.98536  and loss:  19.26390712929424
test acc:  0.9105
forward train acc:  0.99552  and loss:  6.03117212257348
test acc:  0.9142
forward train acc:  0.9962  and loss:  4.841253272781614
test acc:  0.9142
forward train acc:  0.9975  and loss:  3.19155437318841
test acc:  0.9161
forward train acc:  0.99762  and loss:  2.923441601160448
test acc:  0.9165
forward train acc:  0.99788  and loss:  2.6198884486802854
test acc:  0.9146
forward train acc:  0.99792  and loss:  2.6271392076741904
test acc:  0.9164
forward train acc:  0.99818  and loss:  2.274570054956712
test acc:  0.9161
forward train acc:  0.99838  and loss:  2.345249558624346
test acc:  0.917
forward train acc:  0.99818  and loss:  2.2059388394118287
test acc:  0.9167
********** reverse layer  8  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.53125  ==>  90 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.78125  ==>  168 / 768
layer  11  :  0.7916666666666666  ==>  160 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  9
[3, 4, 0, 2, 5, 1, 1, 4, 5, 2, 0, 2, 3, 1]
optimize layer  10
backward train epoch:  184
test acc:  0.0972
forward train acc:  0.99922  and loss:  1.0739698240649886
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.53125  ==>  90 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7890625  ==>  162 / 768
layer  11  :  0.7916666666666666  ==>  160 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  11
[3, 4, 0, 2, 5, 1, 1, 4, 5, 2, 0, 1, 3, 1]
***** skip layer  12
[3, 4, 0, 2, 5, 1, 1, 4, 5, 2, 0, 1, 2, 1]
***** skip layer  13
[3, 4, 0, 2, 5, 1, 1, 4, 5, 2, 0, 1, 2, 0]
***** skip layer  0
[2, 4, 0, 2, 5, 1, 1, 4, 5, 2, 0, 1, 2, 0]
***** skip layer  1
[2, 3, 0, 2, 5, 1, 1, 4, 5, 2, 0, 1, 2, 0]
optimize layer  2
backward train epoch:  47
test acc:  0.0955
forward train acc:  0.9991  and loss:  1.2130372474639444
test acc:  0.9176
forward train acc:  0.99912  and loss:  1.2185674636566546
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7890625  ==>  162 / 768
layer  11  :  0.7916666666666666  ==>  160 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  3
[2, 3, 0, 1, 5, 1, 1, 4, 5, 2, 0, 1, 2, 0]
***** skip layer  4
[2, 3, 0, 1, 4, 1, 1, 4, 5, 2, 0, 1, 2, 0]
***** skip layer  5
[2, 3, 0, 1, 4, 0, 1, 4, 5, 2, 0, 1, 2, 0]
***** skip layer  6
[2, 3, 0, 1, 4, 0, 0, 4, 5, 2, 0, 1, 2, 0]
***** skip layer  7
[2, 3, 0, 1, 4, 0, 0, 3, 5, 2, 0, 1, 2, 0]
***** skip layer  8
[2, 3, 0, 1, 4, 0, 0, 3, 4, 2, 0, 1, 2, 0]
***** skip layer  9
[2, 3, 0, 1, 4, 0, 0, 3, 4, 1, 0, 1, 2, 0]
optimize layer  10
backward train epoch:  288
test acc:  0.0963
forward train acc:  0.99906  and loss:  1.2606294719444122
test acc:  0.9178
forward train acc:  0.99914  and loss:  1.2967920597293414
test acc:  0.918
forward train acc:  0.9991  and loss:  1.2081026771047618
test acc:  0.9173
forward train acc:  0.99934  and loss:  0.7567378260137048
test acc:  0.9166
forward train acc:  0.99954  and loss:  0.7159855674253777
test acc:  0.917
forward train acc:  0.9993  and loss:  0.7851282405608799
test acc:  0.9186
forward train acc:  0.9995  and loss:  0.740096634006477
test acc:  0.9166
forward train acc:  0.99964  and loss:  0.5004927461413899
test acc:  0.9187
forward train acc:  0.9994  and loss:  0.6490933957975358
test acc:  0.9177
forward train acc:  0.99944  and loss:  0.7258953688724432
test acc:  0.9179
********** reverse layer  10  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7890625  ==>  162 / 768
layer  11  :  0.7916666666666666  ==>  160 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  11
[2, 3, 0, 1, 4, 0, 0, 3, 4, 1, 5, 0, 2, 0]
***** skip layer  12
[2, 3, 0, 1, 4, 0, 0, 3, 4, 1, 5, 0, 1, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.0909
forward train acc:  0.99942  and loss:  4.512392525095493
test acc:  0.917
forward train acc:  0.99928  and loss:  3.9371785181574523
test acc:  0.9172
forward train acc:  0.99912  and loss:  3.7030870649032295
test acc:  0.9183
forward train acc:  0.9994  and loss:  3.1099591040983796
test acc:  0.9186
forward train acc:  0.99938  and loss:  2.939019462792203
test acc:  0.9182
forward train acc:  0.99922  and loss:  2.9493003049865365
test acc:  0.9177
forward train acc:  0.99954  and loss:  2.542041752487421
test acc:  0.9193
forward train acc:  0.99964  and loss:  2.4378882562741637
test acc:  0.9184
forward train acc:  0.99956  and loss:  2.272696170490235
test acc:  0.9186
forward train acc:  0.99966  and loss:  2.156229899963364
test acc:  0.9197
********** reverse layer  13  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7890625  ==>  162 / 768
layer  11  :  0.7916666666666666  ==>  160 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  0
[1, 3, 0, 1, 4, 0, 0, 3, 4, 1, 5, 0, 1, 5]
***** skip layer  1
[1, 2, 0, 1, 4, 0, 0, 3, 4, 1, 5, 0, 1, 5]
optimize layer  2
backward train epoch:  252
test acc:  0.1008
forward train acc:  0.94806  and loss:  86.3164572454989
test acc:  0.8842
forward train acc:  0.96608  and loss:  43.94716984219849
test acc:  0.8932
forward train acc:  0.97402  and loss:  32.63985579367727
test acc:  0.8985
forward train acc:  0.97964  and loss:  25.17538660299033
test acc:  0.9005
forward train acc:  0.98204  and loss:  22.350380171556026
test acc:  0.9028
forward train acc:  0.98424  and loss:  18.415385770378634
test acc:  0.9032
forward train acc:  0.98536  and loss:  18.06839106953703
test acc:  0.9046
forward train acc:  0.98666  and loss:  16.316635887837037
test acc:  0.9044
forward train acc:  0.98782  and loss:  14.761181250214577
test acc:  0.906
forward train acc:  0.98854  and loss:  14.491406438639387
test acc:  0.9062
********** reverse layer  2  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7890625  ==>  162 / 768
layer  11  :  0.7916666666666666  ==>  160 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  3
[1, 2, 5, 0, 4, 0, 0, 3, 4, 1, 5, 0, 1, 5]
***** skip layer  4
[1, 2, 5, 0, 3, 0, 0, 3, 4, 1, 5, 0, 1, 5]
optimize layer  5
backward train epoch:  135
test acc:  0.1056
forward train acc:  0.999  and loss:  1.5218955029267818
test acc:  0.9178
forward train acc:  0.99872  and loss:  1.5543715030071326
test acc:  0.918
forward train acc:  0.99888  and loss:  1.6682565634546336
test acc:  0.918
forward train acc:  0.99924  and loss:  1.0296867117285728
test acc:  0.9175
forward train acc:  0.9995  and loss:  0.8363286072562914
test acc:  0.9178
forward train acc:  0.99926  and loss:  0.9746326708991546
test acc:  0.9173
forward train acc:  0.99936  and loss:  0.9731427951774094
test acc:  0.9194
forward train acc:  0.99958  and loss:  0.6572702008852502
test acc:  0.9174
forward train acc:  0.99952  and loss:  0.636038314900361
test acc:  0.9185
forward train acc:  0.99962  and loss:  0.5307947396941017
test acc:  0.9189
********** reverse layer  5  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7890625  ==>  162 / 768
layer  11  :  0.7916666666666666  ==>  160 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
optimize layer  6
backward train epoch:  87
test acc:  0.0891
forward train acc:  0.99934  and loss:  0.8478000500472263
test acc:  0.9186
forward train acc:  0.99928  and loss:  0.7416731660778169
test acc:  0.9179
forward train acc:  0.99918  and loss:  1.0180273272490012
test acc:  0.9175
forward train acc:  0.99924  and loss:  1.0149371312290896
test acc:  0.9182
forward train acc:  0.99942  and loss:  0.6933719896478578
test acc:  0.9177
forward train acc:  0.99934  and loss:  0.7186271602113266
test acc:  0.9171
forward train acc:  0.9994  and loss:  0.7185713778017089
test acc:  0.9173
forward train acc:  0.99964  and loss:  0.48089615361823235
test acc:  0.9172
forward train acc:  0.99956  and loss:  0.5867084280762356
test acc:  0.9171
forward train acc:  0.99968  and loss:  0.4426803227397613
test acc:  0.9172
********** reverse layer  6  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7890625  ==>  162 / 768
layer  11  :  0.7916666666666666  ==>  160 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  7
[1, 2, 5, 0, 3, 5, 5, 2, 4, 1, 5, 0, 1, 5]
***** skip layer  8
[1, 2, 5, 0, 3, 5, 5, 2, 3, 1, 5, 0, 1, 5]
***** skip layer  9
[1, 2, 5, 0, 3, 5, 5, 2, 3, 0, 5, 0, 1, 5]
***** skip layer  10
[1, 2, 5, 0, 3, 5, 5, 2, 3, 0, 4, 0, 1, 5]
optimize layer  11
backward train epoch:  161
test acc:  0.096
forward train acc:  0.99958  and loss:  0.5180631882831221
test acc:  0.9186
forward train acc:  0.99918  and loss:  1.2251786883571185
test acc:  0.9166
forward train acc:  0.99942  and loss:  0.7319979271269403
test acc:  0.9198
forward train acc:  0.99948  and loss:  0.6011628837732133
test acc:  0.9187
forward train acc:  0.99946  and loss:  0.6667372611409519
test acc:  0.9195
forward train acc:  0.9996  and loss:  0.4677466223074589
test acc:  0.9185
forward train acc:  0.9995  and loss:  0.614691720897099
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7890625  ==>  162 / 768
layer  11  :  0.7994791666666666  ==>  154 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  12
[1, 2, 5, 0, 3, 5, 5, 2, 3, 0, 4, 0, 0, 5]
***** skip layer  13
[1, 2, 5, 0, 3, 5, 5, 2, 3, 0, 4, 0, 0, 4]
***** skip layer  0
[0, 2, 5, 0, 3, 5, 5, 2, 3, 0, 4, 0, 0, 4]
***** skip layer  1
[0, 1, 5, 0, 3, 5, 5, 2, 3, 0, 4, 0, 0, 4]
***** skip layer  2
[0, 1, 4, 0, 3, 5, 5, 2, 3, 0, 4, 0, 0, 4]
optimize layer  3
backward train epoch:  2
test acc:  0.8665
forward train acc:  0.99876  and loss:  1.4578339741856325
test acc:  0.9171
forward train acc:  0.99902  and loss:  1.2422988392645493
test acc:  0.9187
forward train acc:  0.99886  and loss:  1.4658786306536058
test acc:  0.9176
forward train acc:  0.99914  and loss:  1.0166882999328664
test acc:  0.9184
forward train acc:  0.99936  and loss:  0.7980973556695972
test acc:  0.9184
forward train acc:  0.9993  and loss:  0.8981916742559406
test acc:  0.9166
forward train acc:  0.99934  and loss:  0.8310248444613535
test acc:  0.916
forward train acc:  0.99948  and loss:  0.71919814159628
test acc:  0.9169
forward train acc:  0.99952  and loss:  0.5368213800684316
test acc:  0.9169
forward train acc:  0.99942  and loss:  0.6658277388341958
test acc:  0.918
********** reverse layer  3  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7890625  ==>  162 / 768
layer  11  :  0.7994791666666666  ==>  154 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  4
[0, 1, 4, 5, 2, 5, 5, 2, 3, 0, 4, 0, 0, 4]
***** skip layer  5
[0, 1, 4, 5, 2, 4, 5, 2, 3, 0, 4, 0, 0, 4]
***** skip layer  6
[0, 1, 4, 5, 2, 4, 4, 2, 3, 0, 4, 0, 0, 4]
***** skip layer  7
[0, 1, 4, 5, 2, 4, 4, 1, 3, 0, 4, 0, 0, 4]
***** skip layer  8
[0, 1, 4, 5, 2, 4, 4, 1, 2, 0, 4, 0, 0, 4]
optimize layer  9
backward train epoch:  59
test acc:  0.0691
forward train acc:  0.99954  and loss:  0.6163318359031109
test acc:  0.9169
forward train acc:  0.99948  and loss:  0.7073010835374589
test acc:  0.9199
forward train acc:  0.99936  and loss:  0.8135870528640226
test acc:  0.9185
forward train acc:  0.9994  and loss:  0.7773324965091888
test acc:  0.9193
forward train acc:  0.9995  and loss:  0.5859972259640926
test acc:  0.9177
forward train acc:  0.99952  and loss:  0.5929247571330052
test acc:  0.9186
forward train acc:  0.99966  and loss:  0.4093746735015884
test acc:  0.9179
forward train acc:  0.9998  and loss:  0.4134249949711375
test acc:  0.9191
forward train acc:  0.99962  and loss:  0.4593643021216849
test acc:  0.9186
forward train acc:  0.99966  and loss:  0.3971275046351366
test acc:  0.9183
********** reverse layer  9  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7890625  ==>  162 / 768
layer  11  :  0.7994791666666666  ==>  154 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  10
[0, 1, 4, 5, 2, 4, 4, 1, 2, 5, 3, 0, 0, 4]
optimize layer  11
backward train epoch:  252
test acc:  0.0865
forward train acc:  0.99954  and loss:  0.5494810972741107
test acc:  0.9208
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7890625  ==>  162 / 768
layer  11  :  0.8072916666666666  ==>  148 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
optimize layer  12
backward train epoch:  384
test acc:  0.1
forward train acc:  0.1  and loss:  1095.6362800598145
test acc:  0.1
forward train acc:  0.1  and loss:  1073.9809176921844
test acc:  0.1
forward train acc:  0.1  and loss:  1054.1117024421692
test acc:  0.1
forward train acc:  0.1  and loss:  1040.0729610919952
test acc:  0.1
forward train acc:  0.1  and loss:  1031.149934053421
test acc:  0.1
forward train acc:  0.1  and loss:  1022.6456413269043
test acc:  0.1
forward train acc:  0.1  and loss:  1014.3980071544647
test acc:  0.1
forward train acc:  0.1  and loss:  1008.480416059494
test acc:  0.1
forward train acc:  0.1  and loss:  1004.6349837779999
test acc:  0.1
forward train acc:  0.1  and loss:  1000.7997252941132
test acc:  0.1
********** reverse layer  12  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7890625  ==>  162 / 768
layer  11  :  0.8072916666666666  ==>  148 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  13
[0, 1, 4, 5, 2, 4, 4, 1, 2, 5, 3, 0, 5, 3]
optimize layer  0
backward train epoch:  23
test acc:  0.1387
forward train acc:  0.99692  and loss:  4.498842099332251
test acc:  0.9171
forward train acc:  0.99698  and loss:  4.374401822278742
test acc:  0.9138
forward train acc:  0.99792  and loss:  3.0055324459681287
test acc:  0.9157
forward train acc:  0.99796  and loss:  2.9468828786630183
test acc:  0.9143
forward train acc:  0.99824  and loss:  2.15975008031819
test acc:  0.9167
forward train acc:  0.9985  and loss:  2.0445172072795685
test acc:  0.9154
forward train acc:  0.99864  and loss:  1.8724231562227942
test acc:  0.9188
forward train acc:  0.99878  and loss:  1.6775004544469994
test acc:  0.9172
forward train acc:  0.99896  and loss:  1.3869311849121004
test acc:  0.9185
forward train acc:  0.999  and loss:  1.4127065022621537
test acc:  0.9181
********** reverse layer  0  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7890625  ==>  162 / 768
layer  11  :  0.8072916666666666  ==>  148 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  1
[5, 0, 4, 5, 2, 4, 4, 1, 2, 5, 3, 0, 5, 3]
***** skip layer  2
[5, 0, 3, 5, 2, 4, 4, 1, 2, 5, 3, 0, 5, 3]
***** skip layer  3
[5, 0, 3, 4, 2, 4, 4, 1, 2, 5, 3, 0, 5, 3]
***** skip layer  4
[5, 0, 3, 4, 1, 4, 4, 1, 2, 5, 3, 0, 5, 3]
***** skip layer  5
[5, 0, 3, 4, 1, 3, 4, 1, 2, 5, 3, 0, 5, 3]
***** skip layer  6
[5, 0, 3, 4, 1, 3, 3, 1, 2, 5, 3, 0, 5, 3]
***** skip layer  7
[5, 0, 3, 4, 1, 3, 3, 0, 2, 5, 3, 0, 5, 3]
***** skip layer  8
[5, 0, 3, 4, 1, 3, 3, 0, 1, 5, 3, 0, 5, 3]
***** skip layer  9
[5, 0, 3, 4, 1, 3, 3, 0, 1, 4, 3, 0, 5, 3]
***** skip layer  10
[5, 0, 3, 4, 1, 3, 3, 0, 1, 4, 2, 0, 5, 3]
optimize layer  11
backward train epoch:  49
test acc:  0.0983
forward train acc:  0.99936  and loss:  0.8307870633434504
test acc:  0.9166
forward train acc:  0.9996  and loss:  0.6754640214785468
test acc:  0.9189
forward train acc:  0.99942  and loss:  0.8473387392004952
test acc:  0.9184
forward train acc:  0.99946  and loss:  0.5858751022315118
test acc:  0.9196
forward train acc:  0.9996  and loss:  0.6253898002905771
test acc:  0.9206
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.59375  ==>  39 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7890625  ==>  162 / 768
layer  11  :  0.8151041666666666  ==>  142 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  12
[5, 0, 3, 4, 1, 3, 3, 0, 1, 4, 2, 0, 4, 3]
***** skip layer  13
[5, 0, 3, 4, 1, 3, 3, 0, 1, 4, 2, 0, 4, 2]
***** skip layer  0
[4, 0, 3, 4, 1, 3, 3, 0, 1, 4, 2, 0, 4, 2]
optimize layer  1
backward train epoch:  718
test acc:  0.0999
forward train acc:  0.99766  and loss:  3.560301185483695
test acc:  0.9176
forward train acc:  0.9983  and loss:  2.211154327960685
test acc:  0.919
forward train acc:  0.99884  and loss:  1.4554772559204139
test acc:  0.9191
forward train acc:  0.99886  and loss:  1.4802548542793375
test acc:  0.919
forward train acc:  0.99918  and loss:  1.0016254187648883
test acc:  0.9192
forward train acc:  0.99912  and loss:  1.141016620094888
test acc:  0.9204
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6041666666666666  ==>  38 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7890625  ==>  162 / 768
layer  11  :  0.8151041666666666  ==>  142 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  2
[4, 0, 2, 4, 1, 3, 3, 0, 1, 4, 2, 0, 4, 2]
***** skip layer  3
[4, 0, 2, 3, 1, 3, 3, 0, 1, 4, 2, 0, 4, 2]
***** skip layer  4
[4, 0, 2, 3, 0, 3, 3, 0, 1, 4, 2, 0, 4, 2]
***** skip layer  5
[4, 0, 2, 3, 0, 2, 3, 0, 1, 4, 2, 0, 4, 2]
***** skip layer  6
[4, 0, 2, 3, 0, 2, 2, 0, 1, 4, 2, 0, 4, 2]
optimize layer  7
backward train epoch:  42
test acc:  0.1049
forward train acc:  0.99738  and loss:  3.4397936589375604
test acc:  0.9151
forward train acc:  0.99814  and loss:  2.408087777206674
test acc:  0.9161
forward train acc:  0.99844  and loss:  1.9044152600399684
test acc:  0.9174
forward train acc:  0.99858  and loss:  1.805742266267771
test acc:  0.9188
forward train acc:  0.99886  and loss:  1.470135974144796
test acc:  0.9175
forward train acc:  0.99912  and loss:  1.2545073057699483
test acc:  0.9169
forward train acc:  0.9989  and loss:  1.4343502857955173
test acc:  0.9179
forward train acc:  0.99914  and loss:  1.166140877729049
test acc:  0.9172
forward train acc:  0.99882  and loss:  1.509719472087454
test acc:  0.9168
forward train acc:  0.99924  and loss:  1.0594707750715315
test acc:  0.9173
********** reverse layer  7  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6041666666666666  ==>  38 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7890625  ==>  162 / 768
layer  11  :  0.8151041666666666  ==>  142 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  8
[4, 0, 2, 3, 0, 2, 2, 5, 0, 4, 2, 0, 4, 2]
***** skip layer  9
[4, 0, 2, 3, 0, 2, 2, 5, 0, 3, 2, 0, 4, 2]
***** skip layer  10
[4, 0, 2, 3, 0, 2, 2, 5, 0, 3, 1, 0, 4, 2]
optimize layer  11
backward train epoch:  69
test acc:  0.1082
forward train acc:  0.99934  and loss:  1.0248721597308759
test acc:  0.919
forward train acc:  0.9995  and loss:  0.7356437586859101
test acc:  0.921
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6041666666666666  ==>  38 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7890625  ==>  162 / 768
layer  11  :  0.8229166666666666  ==>  136 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  12
[4, 0, 2, 3, 0, 2, 2, 5, 0, 3, 1, 0, 3, 2]
***** skip layer  13
[4, 0, 2, 3, 0, 2, 2, 5, 0, 3, 1, 0, 3, 1]
***** skip layer  0
[3, 0, 2, 3, 0, 2, 2, 5, 0, 3, 1, 0, 3, 1]
optimize layer  1
backward train epoch:  85
test acc:  0.1017
forward train acc:  0.99916  and loss:  0.9588814028247725
test acc:  0.9175
forward train acc:  0.9993  and loss:  0.8836364119633799
test acc:  0.9177
forward train acc:  0.99932  and loss:  0.8510994180251146
test acc:  0.9184
forward train acc:  0.99956  and loss:  0.6511219548265217
test acc:  0.9197
forward train acc:  0.9994  and loss:  0.8605352878657868
test acc:  0.9182
forward train acc:  0.99932  and loss:  0.7994111711595906
test acc:  0.9191
forward train acc:  0.99962  and loss:  0.575409940967802
test acc:  0.9203
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6145833333333334  ==>  37 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7890625  ==>  162 / 768
layer  11  :  0.8229166666666666  ==>  136 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  2
[3, 0, 1, 3, 0, 2, 2, 5, 0, 3, 1, 0, 3, 1]
***** skip layer  3
[3, 0, 1, 2, 0, 2, 2, 5, 0, 3, 1, 0, 3, 1]
optimize layer  4
backward train epoch:  651
test acc:  0.1
forward train acc:  0.99904  and loss:  1.465139863852528
test acc:  0.9152
forward train acc:  0.99842  and loss:  2.069868961989414
test acc:  0.9173
forward train acc:  0.99858  and loss:  1.8588551627472043
test acc:  0.9172
forward train acc:  0.9991  and loss:  1.3070486614014953
test acc:  0.9165
forward train acc:  0.99904  and loss:  1.1466788037214428
test acc:  0.9168
forward train acc:  0.99926  and loss:  0.9855523959267884
test acc:  0.916
forward train acc:  0.9993  and loss:  1.06373764219461
test acc:  0.9168
forward train acc:  0.9993  and loss:  0.9086052634229418
test acc:  0.9175
forward train acc:  0.99934  and loss:  1.006691324932035
test acc:  0.9178
forward train acc:  0.99936  and loss:  0.8004425834078575
test acc:  0.9174
********** reverse layer  4  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6145833333333334  ==>  37 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7890625  ==>  162 / 768
layer  11  :  0.8229166666666666  ==>  136 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  5
[3, 0, 1, 2, 5, 1, 2, 5, 0, 3, 1, 0, 3, 1]
***** skip layer  6
[3, 0, 1, 2, 5, 1, 1, 5, 0, 3, 1, 0, 3, 1]
***** skip layer  7
[3, 0, 1, 2, 5, 1, 1, 4, 0, 3, 1, 0, 3, 1]
optimize layer  8
backward train epoch:  7
test acc:  0.438
forward train acc:  0.99598  and loss:  5.4642729866318405
test acc:  0.9131
forward train acc:  0.99808  and loss:  2.4924488159595057
test acc:  0.9147
forward train acc:  0.99808  and loss:  2.21083769845427
test acc:  0.9146
forward train acc:  0.99828  and loss:  2.0357538248354103
test acc:  0.9158
forward train acc:  0.99894  and loss:  1.5573181824875064
test acc:  0.9159
forward train acc:  0.99898  and loss:  1.3688433801871724
test acc:  0.9162
forward train acc:  0.9992  and loss:  1.2767214741907082
test acc:  0.9174
forward train acc:  0.99878  and loss:  1.53195069252979
test acc:  0.9169
forward train acc:  0.99916  and loss:  1.3547454960353207
test acc:  0.9171
forward train acc:  0.9993  and loss:  0.9529946472757729
test acc:  0.9173
********** reverse layer  8  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6145833333333334  ==>  37 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7890625  ==>  162 / 768
layer  11  :  0.8229166666666666  ==>  136 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  9
[3, 0, 1, 2, 5, 1, 1, 4, 5, 2, 1, 0, 3, 1]
***** skip layer  10
[3, 0, 1, 2, 5, 1, 1, 4, 5, 2, 0, 0, 3, 1]
optimize layer  11
backward train epoch:  176
test acc:  0.0966
forward train acc:  0.99956  and loss:  0.5568675325630466
test acc:  0.9165
forward train acc:  0.99934  and loss:  0.9920730143639958
test acc:  0.9171
forward train acc:  0.99908  and loss:  1.0439257887483109
test acc:  0.9181
forward train acc:  0.99962  and loss:  0.6100273645570269
test acc:  0.9185
forward train acc:  0.99966  and loss:  0.4796980659593828
test acc:  0.9212
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6145833333333334  ==>  37 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7890625  ==>  162 / 768
layer  11  :  0.8307291666666666  ==>  130 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  12
[3, 0, 1, 2, 5, 1, 1, 4, 5, 2, 0, 0, 2, 1]
***** skip layer  13
[3, 0, 1, 2, 5, 1, 1, 4, 5, 2, 0, 0, 2, 0]
***** skip layer  0
[2, 0, 1, 2, 5, 1, 1, 4, 5, 2, 0, 0, 2, 0]
optimize layer  1
backward train epoch:  18
test acc:  0.1294
forward train acc:  0.9991  and loss:  1.412514544907026
test acc:  0.9166
forward train acc:  0.99902  and loss:  1.2939067528350279
test acc:  0.9172
forward train acc:  0.99926  and loss:  1.012749485700624
test acc:  0.9167
forward train acc:  0.9994  and loss:  0.7638056585274171
test acc:  0.918
forward train acc:  0.99946  and loss:  0.666225639230106
test acc:  0.9183
forward train acc:  0.99962  and loss:  0.5303059457510244
test acc:  0.9188
forward train acc:  0.99956  and loss:  0.592659367073793
test acc:  0.9183
forward train acc:  0.99976  and loss:  0.3417690946953371
test acc:  0.9194
forward train acc:  0.99966  and loss:  0.5998908574256347
test acc:  0.9183
forward train acc:  0.99954  and loss:  0.48053786501986906
test acc:  0.9181
********** reverse layer  1  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6145833333333334  ==>  37 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.7890625  ==>  162 / 768
layer  11  :  0.8307291666666666  ==>  130 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  2
[2, 5, 0, 2, 5, 1, 1, 4, 5, 2, 0, 0, 2, 0]
***** skip layer  3
[2, 5, 0, 1, 5, 1, 1, 4, 5, 2, 0, 0, 2, 0]
***** skip layer  4
[2, 5, 0, 1, 4, 1, 1, 4, 5, 2, 0, 0, 2, 0]
***** skip layer  5
[2, 5, 0, 1, 4, 0, 1, 4, 5, 2, 0, 0, 2, 0]
***** skip layer  6
[2, 5, 0, 1, 4, 0, 0, 4, 5, 2, 0, 0, 2, 0]
***** skip layer  7
[2, 5, 0, 1, 4, 0, 0, 3, 5, 2, 0, 0, 2, 0]
***** skip layer  8
[2, 5, 0, 1, 4, 0, 0, 3, 4, 2, 0, 0, 2, 0]
***** skip layer  9
[2, 5, 0, 1, 4, 0, 0, 3, 4, 1, 0, 0, 2, 0]
optimize layer  10
backward train epoch:  292
test acc:  0.0958
forward train acc:  0.99948  and loss:  0.5796165095525794
test acc:  0.9185
forward train acc:  0.99932  and loss:  0.897725106231519
test acc:  0.9183
forward train acc:  0.99928  and loss:  0.8366561815637397
test acc:  0.9185
forward train acc:  0.99964  and loss:  0.5506692202761769
test acc:  0.9189
forward train acc:  0.9997  and loss:  0.4783833433757536
test acc:  0.9194
forward train acc:  0.99976  and loss:  0.4045824703935068
test acc:  0.9195
forward train acc:  0.99958  and loss:  0.4244744266034104
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6145833333333334  ==>  37 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8307291666666666  ==>  130 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
optimize layer  11
backward train epoch:  115
test acc:  0.0981
forward train acc:  0.99948  and loss:  0.7635059799649753
test acc:  0.9177
forward train acc:  0.99944  and loss:  0.8340530935383867
test acc:  0.9167
forward train acc:  0.99958  and loss:  0.5385653962366632
test acc:  0.918
forward train acc:  0.99944  and loss:  0.6605781607795507
test acc:  0.9194
forward train acc:  0.99976  and loss:  0.3408429452829296
test acc:  0.9177
forward train acc:  0.99958  and loss:  0.5127451223088428
test acc:  0.9188
forward train acc:  0.99958  and loss:  0.4631821477523772
test acc:  0.9189
forward train acc:  0.99954  and loss:  0.5611862551304512
test acc:  0.9194
forward train acc:  0.99978  and loss:  0.40389639348722994
test acc:  0.9182
forward train acc:  0.99978  and loss:  0.272590101667447
test acc:  0.9198
********** reverse layer  11  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6145833333333334  ==>  37 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8307291666666666  ==>  130 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  12
[2, 5, 0, 1, 4, 0, 0, 3, 4, 1, 0, 5, 1, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.1011
forward train acc:  0.99706  and loss:  34.412462051957846
test acc:  0.9165
forward train acc:  0.99948  and loss:  28.504214769229293
test acc:  0.9178
forward train acc:  0.99962  and loss:  25.67453185096383
test acc:  0.9172
forward train acc:  0.99936  and loss:  24.03679975308478
test acc:  0.9177
forward train acc:  0.99966  and loss:  22.587653759866953
test acc:  0.9186
forward train acc:  0.9996  and loss:  21.373300474137068
test acc:  0.9164
forward train acc:  0.99972  and loss:  20.305368771776557
test acc:  0.9161
forward train acc:  0.99966  and loss:  19.586538271978498
test acc:  0.9178
forward train acc:  0.99962  and loss:  19.218920297920704
test acc:  0.9186
forward train acc:  0.99968  and loss:  18.571010760962963
test acc:  0.9175
********** reverse layer  13  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6145833333333334  ==>  37 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8307291666666666  ==>  130 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  0
[1, 5, 0, 1, 4, 0, 0, 3, 4, 1, 0, 5, 1, 5]
***** skip layer  1
[1, 4, 0, 1, 4, 0, 0, 3, 4, 1, 0, 5, 1, 5]
optimize layer  2
backward train epoch:  94
test acc:  0.0877
forward train acc:  0.99938  and loss:  0.9496637278061826
test acc:  0.9171
forward train acc:  0.99936  and loss:  0.9041959876922192
test acc:  0.9156
forward train acc:  0.99916  and loss:  1.221264145380701
test acc:  0.9168
forward train acc:  0.99944  and loss:  0.6545999262016267
test acc:  0.918
forward train acc:  0.99946  and loss:  0.7683662930503488
test acc:  0.9171
forward train acc:  0.9996  and loss:  0.6481103239348158
test acc:  0.9165
forward train acc:  0.99958  and loss:  0.5329931151791243
test acc:  0.9175
forward train acc:  0.99932  and loss:  0.9106945507446653
test acc:  0.9181
forward train acc:  0.99964  and loss:  0.5857765589316841
test acc:  0.9175
forward train acc:  0.99972  and loss:  0.4563876896718284
test acc:  0.9188
********** reverse layer  2  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6145833333333334  ==>  37 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8307291666666666  ==>  130 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  3
[1, 4, 5, 0, 4, 0, 0, 3, 4, 1, 0, 5, 1, 5]
***** skip layer  4
[1, 4, 5, 0, 3, 0, 0, 3, 4, 1, 0, 5, 1, 5]
optimize layer  5
backward train epoch:  159
test acc:  0.0986
forward train acc:  0.9992  and loss:  1.0474299080888159
test acc:  0.9161
forward train acc:  0.9995  and loss:  0.6916205372690456
test acc:  0.916
forward train acc:  0.99916  and loss:  1.1623728473787196
test acc:  0.9148
forward train acc:  0.99924  and loss:  0.9323604151140898
test acc:  0.9168
forward train acc:  0.9993  and loss:  0.8686730314802844
test acc:  0.9174
forward train acc:  0.99946  and loss:  0.7984462314052507
test acc:  0.9179
forward train acc:  0.99972  and loss:  0.4324010380951222
test acc:  0.917
forward train acc:  0.99964  and loss:  0.5357720178726595
test acc:  0.9169
forward train acc:  0.99954  and loss:  0.579402588919038
test acc:  0.9179
forward train acc:  0.99976  and loss:  0.3468117107258877
test acc:  0.9182
********** reverse layer  5  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6145833333333334  ==>  37 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.625  ==>  144 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8307291666666666  ==>  130 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
optimize layer  6
backward train epoch:  35
test acc:  0.0976
forward train acc:  0.9996  and loss:  0.6039093542349292
test acc:  0.9178
forward train acc:  0.99924  and loss:  1.001747550588334
test acc:  0.9178
forward train acc:  0.99934  and loss:  0.8449742825614521
test acc:  0.9176
forward train acc:  0.99964  and loss:  0.6837724585057003
test acc:  0.9172
forward train acc:  0.99976  and loss:  0.38587455080414657
test acc:  0.9193
forward train acc:  0.99972  and loss:  0.3609025238765753
test acc:  0.9193
forward train acc:  0.99978  and loss:  0.4455227110447595
test acc:  0.9197
forward train acc:  0.99972  and loss:  0.35711099104810273
test acc:  0.9198
forward train acc:  0.9997  and loss:  0.46266614030173514
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6145833333333334  ==>  37 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8307291666666666  ==>  130 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  7
[1, 4, 5, 0, 3, 5, 0, 2, 4, 1, 0, 5, 1, 5]
***** skip layer  8
[1, 4, 5, 0, 3, 5, 0, 2, 3, 1, 0, 5, 1, 5]
***** skip layer  9
[1, 4, 5, 0, 3, 5, 0, 2, 3, 0, 0, 5, 1, 5]
optimize layer  10
backward train epoch:  35
test acc:  0.0975
forward train acc:  0.99974  and loss:  0.42402787150786025
test acc:  0.9189
forward train acc:  0.99952  and loss:  0.6539871639106423
test acc:  0.9168
forward train acc:  0.99904  and loss:  1.2231316107936436
test acc:  0.9183
forward train acc:  0.99922  and loss:  0.8003569448774215
test acc:  0.9184
forward train acc:  0.99958  and loss:  0.5960209797631251
test acc:  0.9178
forward train acc:  0.99958  and loss:  0.5154146425775252
test acc:  0.9191
forward train acc:  0.99962  and loss:  0.5593759104958735
test acc:  0.9189
forward train acc:  0.99958  and loss:  0.4626218170305947
test acc:  0.9186
forward train acc:  0.99968  and loss:  0.4226610717087169
test acc:  0.919
forward train acc:  0.9997  and loss:  0.4042903507652227
test acc:  0.9189
********** reverse layer  10  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6145833333333334  ==>  37 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8307291666666666  ==>  130 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  11
[1, 4, 5, 0, 3, 5, 0, 2, 3, 0, 5, 4, 1, 5]
***** skip layer  12
[1, 4, 5, 0, 3, 5, 0, 2, 3, 0, 5, 4, 0, 5]
***** skip layer  13
[1, 4, 5, 0, 3, 5, 0, 2, 3, 0, 5, 4, 0, 4]
***** skip layer  0
[0, 4, 5, 0, 3, 5, 0, 2, 3, 0, 5, 4, 0, 4]
***** skip layer  1
[0, 3, 5, 0, 3, 5, 0, 2, 3, 0, 5, 4, 0, 4]
***** skip layer  2
[0, 3, 4, 0, 3, 5, 0, 2, 3, 0, 5, 4, 0, 4]
optimize layer  3
backward train epoch:  2
test acc:  0.8646
forward train acc:  0.99894  and loss:  1.3306557633331977
test acc:  0.9165
forward train acc:  0.99916  and loss:  1.0066132246720372
test acc:  0.9132
forward train acc:  0.9992  and loss:  1.2250991458349745
test acc:  0.9157
forward train acc:  0.99934  and loss:  0.841049408816616
test acc:  0.9185
forward train acc:  0.9993  and loss:  0.8000066269305535
test acc:  0.9188
forward train acc:  0.99944  and loss:  0.6018257752875797
test acc:  0.9171
forward train acc:  0.99944  and loss:  0.6894504449810483
test acc:  0.9168
forward train acc:  0.99954  and loss:  0.599862012968515
test acc:  0.9173
forward train acc:  0.99964  and loss:  0.47031245881225914
test acc:  0.9189
forward train acc:  0.99948  and loss:  0.8681829295892385
test acc:  0.9182
********** reverse layer  3  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6145833333333334  ==>  37 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8307291666666666  ==>  130 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  4
[0, 3, 4, 5, 2, 5, 0, 2, 3, 0, 5, 4, 0, 4]
***** skip layer  5
[0, 3, 4, 5, 2, 4, 0, 2, 3, 0, 5, 4, 0, 4]
optimize layer  6
backward train epoch:  181
test acc:  0.094
forward train acc:  0.99936  and loss:  0.8356709093786776
test acc:  0.9166
forward train acc:  0.99932  and loss:  0.9128007205144968
test acc:  0.9135
forward train acc:  0.99922  and loss:  1.0261437334338552
test acc:  0.9157
forward train acc:  0.9994  and loss:  0.6800865904297098
test acc:  0.9162
forward train acc:  0.99936  and loss:  0.7347367613110691
test acc:  0.9159
forward train acc:  0.99954  and loss:  0.6013565096363891
test acc:  0.918
forward train acc:  0.99966  and loss:  0.4689976372901583
test acc:  0.9173
forward train acc:  0.9996  and loss:  0.42215111586119747
test acc:  0.9189
forward train acc:  0.9998  and loss:  0.31858938492950983
test acc:  0.9175
forward train acc:  0.99964  and loss:  0.3839838218264049
test acc:  0.9189
********** reverse layer  6  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6145833333333334  ==>  37 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7109375  ==>  222 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8307291666666666  ==>  130 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  7
[0, 3, 4, 5, 2, 4, 5, 1, 3, 0, 5, 4, 0, 4]
***** skip layer  8
[0, 3, 4, 5, 2, 4, 5, 1, 2, 0, 5, 4, 0, 4]
optimize layer  9
backward train epoch:  57
test acc:  0.1005
forward train acc:  0.99962  and loss:  0.42174012216128176
test acc:  0.9169
forward train acc:  0.99948  and loss:  0.6970375536911888
test acc:  0.9175
forward train acc:  0.99936  and loss:  0.7898729273729259
test acc:  0.9167
forward train acc:  0.99936  and loss:  0.7409852951968787
test acc:  0.9174
forward train acc:  0.99956  and loss:  0.4494108412618516
test acc:  0.918
forward train acc:  0.99986  and loss:  0.250050724731409
test acc:  0.9177
forward train acc:  0.99958  and loss:  0.460129237275396
test acc:  0.9197
forward train acc:  0.99982  and loss:  0.25458166604221333
test acc:  0.9186
forward train acc:  0.9997  and loss:  0.3948504044346919
test acc:  0.9199
forward train acc:  0.9997  and loss:  0.3854060857920558
test acc:  0.9206
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6145833333333334  ==>  37 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.71875  ==>  216 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8307291666666666  ==>  130 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  10
[0, 3, 4, 5, 2, 4, 5, 1, 2, 0, 4, 4, 0, 4]
***** skip layer  11
[0, 3, 4, 5, 2, 4, 5, 1, 2, 0, 4, 3, 0, 4]
optimize layer  12
backward train epoch:  280
test acc:  0.1
forward train acc:  0.1  and loss:  1083.0351886749268
test acc:  0.1
forward train acc:  0.1  and loss:  1062.472038269043
test acc:  0.1
forward train acc:  0.1  and loss:  1043.880987405777
test acc:  0.1
forward train acc:  0.1  and loss:  1031.045197725296
test acc:  0.1
forward train acc:  0.1  and loss:  1022.9045028686523
test acc:  0.1
forward train acc:  0.1  and loss:  1015.0634899139404
test acc:  0.1
forward train acc:  0.1  and loss:  1007.5029821395874
test acc:  0.1
forward train acc:  0.1  and loss:  1001.9391617774963
test acc:  0.1
forward train acc:  0.1  and loss:  998.358484506607
test acc:  0.1
forward train acc:  0.1  and loss:  994.7639813423157
test acc:  0.1
********** reverse layer  12  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6145833333333334  ==>  37 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.71875  ==>  216 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8307291666666666  ==>  130 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  13
[0, 3, 4, 5, 2, 4, 5, 1, 2, 0, 4, 3, 5, 3]
optimize layer  0
backward train epoch:  103
test acc:  0.0993
forward train acc:  0.99738  and loss:  3.9346320692275185
test acc:  0.914
forward train acc:  0.99742  and loss:  3.305583847744856
test acc:  0.9134
forward train acc:  0.99772  and loss:  2.9666739486565348
test acc:  0.9127
forward train acc:  0.99814  and loss:  2.565608796896413
test acc:  0.9155
forward train acc:  0.9983  and loss:  2.4331232205731794
test acc:  0.9135
forward train acc:  0.99862  and loss:  1.7955900717061013
test acc:  0.9152
forward train acc:  0.99896  and loss:  1.5501606040925253
test acc:  0.9152
forward train acc:  0.99862  and loss:  1.692350690747844
test acc:  0.9167
forward train acc:  0.99886  and loss:  1.5278384801349603
test acc:  0.9174
forward train acc:  0.99902  and loss:  1.3184433120040921
test acc:  0.9168
********** reverse layer  0  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6145833333333334  ==>  37 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.71875  ==>  216 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8307291666666666  ==>  130 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  1
[5, 2, 4, 5, 2, 4, 5, 1, 2, 0, 4, 3, 5, 3]
***** skip layer  2
[5, 2, 3, 5, 2, 4, 5, 1, 2, 0, 4, 3, 5, 3]
***** skip layer  3
[5, 2, 3, 4, 2, 4, 5, 1, 2, 0, 4, 3, 5, 3]
***** skip layer  4
[5, 2, 3, 4, 1, 4, 5, 1, 2, 0, 4, 3, 5, 3]
***** skip layer  5
[5, 2, 3, 4, 1, 3, 5, 1, 2, 0, 4, 3, 5, 3]
***** skip layer  6
[5, 2, 3, 4, 1, 3, 4, 1, 2, 0, 4, 3, 5, 3]
***** skip layer  7
[5, 2, 3, 4, 1, 3, 4, 0, 2, 0, 4, 3, 5, 3]
***** skip layer  8
[5, 2, 3, 4, 1, 3, 4, 0, 1, 0, 4, 3, 5, 3]
optimize layer  9
backward train epoch:  337
test acc:  0.1033
forward train acc:  0.99956  and loss:  0.6602261891966918
test acc:  0.9185
forward train acc:  0.99964  and loss:  0.4624141813983442
test acc:  0.9183
forward train acc:  0.99924  and loss:  1.033925532727153
test acc:  0.9164
forward train acc:  0.9996  and loss:  0.5481673893227708
test acc:  0.917
forward train acc:  0.99976  and loss:  0.45216038997023134
test acc:  0.9175
forward train acc:  0.99956  and loss:  0.571963863549172
test acc:  0.9192
forward train acc:  0.99968  and loss:  0.5267899862956256
test acc:  0.9172
forward train acc:  0.9996  and loss:  0.4560338786104694
test acc:  0.918
forward train acc:  0.9999  and loss:  0.20013899850891903
test acc:  0.9179
forward train acc:  0.99982  and loss:  0.24901825965207536
test acc:  0.9183
********** reverse layer  9  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6145833333333334  ==>  37 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.71875  ==>  216 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8307291666666666  ==>  130 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  10
[5, 2, 3, 4, 1, 3, 4, 0, 1, 5, 3, 3, 5, 3]
***** skip layer  11
[5, 2, 3, 4, 1, 3, 4, 0, 1, 5, 3, 2, 5, 3]
***** skip layer  12
[5, 2, 3, 4, 1, 3, 4, 0, 1, 5, 3, 2, 4, 3]
***** skip layer  13
[5, 2, 3, 4, 1, 3, 4, 0, 1, 5, 3, 2, 4, 2]
***** skip layer  0
[4, 2, 3, 4, 1, 3, 4, 0, 1, 5, 3, 2, 4, 2]
***** skip layer  1
[4, 1, 3, 4, 1, 3, 4, 0, 1, 5, 3, 2, 4, 2]
***** skip layer  2
[4, 1, 2, 4, 1, 3, 4, 0, 1, 5, 3, 2, 4, 2]
***** skip layer  3
[4, 1, 2, 3, 1, 3, 4, 0, 1, 5, 3, 2, 4, 2]
***** skip layer  4
[4, 1, 2, 3, 0, 3, 4, 0, 1, 5, 3, 2, 4, 2]
***** skip layer  5
[4, 1, 2, 3, 0, 2, 4, 0, 1, 5, 3, 2, 4, 2]
***** skip layer  6
[4, 1, 2, 3, 0, 2, 3, 0, 1, 5, 3, 2, 4, 2]
optimize layer  7
backward train epoch:  71
test acc:  0.0801
forward train acc:  0.99838  and loss:  2.166160987690091
test acc:  0.9135
forward train acc:  0.99888  and loss:  1.4366943407658255
test acc:  0.9164
forward train acc:  0.99892  and loss:  1.4220969831221737
test acc:  0.9152
forward train acc:  0.9994  and loss:  0.8496220469533
test acc:  0.9157
forward train acc:  0.99942  and loss:  0.6855032172752544
test acc:  0.9185
forward train acc:  0.99938  and loss:  0.8111179408151656
test acc:  0.9156
forward train acc:  0.99964  and loss:  0.6145595260459231
test acc:  0.917
forward train acc:  0.9993  and loss:  0.8530753710219869
test acc:  0.9185
forward train acc:  0.9992  and loss:  0.9133524646458682
test acc:  0.9184
forward train acc:  0.9996  and loss:  0.5185430828132667
test acc:  0.9175
********** reverse layer  7  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6145833333333334  ==>  37 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.71875  ==>  216 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8307291666666666  ==>  130 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  8
[4, 1, 2, 3, 0, 2, 3, 5, 0, 5, 3, 2, 4, 2]
***** skip layer  9
[4, 1, 2, 3, 0, 2, 3, 5, 0, 4, 3, 2, 4, 2]
***** skip layer  10
[4, 1, 2, 3, 0, 2, 3, 5, 0, 4, 2, 2, 4, 2]
***** skip layer  11
[4, 1, 2, 3, 0, 2, 3, 5, 0, 4, 2, 1, 4, 2]
***** skip layer  12
[4, 1, 2, 3, 0, 2, 3, 5, 0, 4, 2, 1, 3, 2]
***** skip layer  13
[4, 1, 2, 3, 0, 2, 3, 5, 0, 4, 2, 1, 3, 1]
***** skip layer  0
[3, 1, 2, 3, 0, 2, 3, 5, 0, 4, 2, 1, 3, 1]
***** skip layer  1
[3, 0, 2, 3, 0, 2, 3, 5, 0, 4, 2, 1, 3, 1]
***** skip layer  2
[3, 0, 1, 3, 0, 2, 3, 5, 0, 4, 2, 1, 3, 1]
***** skip layer  3
[3, 0, 1, 2, 0, 2, 3, 5, 0, 4, 2, 1, 3, 1]
optimize layer  4
backward train epoch:  142
test acc:  0.0874
forward train acc:  0.99878  and loss:  1.8118142015009653
test acc:  0.9157
forward train acc:  0.99904  and loss:  1.2584235282847658
test acc:  0.915
forward train acc:  0.99908  and loss:  1.2823383240465773
test acc:  0.9172
forward train acc:  0.99936  and loss:  0.8814678744965931
test acc:  0.9164
forward train acc:  0.9993  and loss:  1.000769742357079
test acc:  0.9179
forward train acc:  0.99948  and loss:  0.7479859433660749
test acc:  0.9176
forward train acc:  0.99968  and loss:  0.5730525454273447
test acc:  0.9175
forward train acc:  0.99966  and loss:  0.5144905364577426
test acc:  0.9172
forward train acc:  0.99958  and loss:  0.640863576263655
test acc:  0.9173
forward train acc:  0.99974  and loss:  0.44410139697720297
test acc:  0.9165
********** reverse layer  4  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6145833333333334  ==>  37 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.71875  ==>  216 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8307291666666666  ==>  130 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  5
[3, 0, 1, 2, 5, 1, 3, 5, 0, 4, 2, 1, 3, 1]
***** skip layer  6
[3, 0, 1, 2, 5, 1, 2, 5, 0, 4, 2, 1, 3, 1]
***** skip layer  7
[3, 0, 1, 2, 5, 1, 2, 4, 0, 4, 2, 1, 3, 1]
optimize layer  8
backward train epoch:  428
test acc:  0.1062
forward train acc:  0.98566  and loss:  18.780319998739287
test acc:  0.9069
forward train acc:  0.99568  and loss:  5.924715533910785
test acc:  0.9097
forward train acc:  0.99708  and loss:  4.00999225943815
test acc:  0.9106
forward train acc:  0.99726  and loss:  3.4053285658883397
test acc:  0.9114
forward train acc:  0.99776  and loss:  2.8022757984290365
test acc:  0.9119
forward train acc:  0.99814  and loss:  2.386562437081011
test acc:  0.9141
forward train acc:  0.9983  and loss:  1.9785165168286767
test acc:  0.9144
forward train acc:  0.99842  and loss:  2.068892665702151
test acc:  0.9138
forward train acc:  0.9985  and loss:  2.133018673659535
test acc:  0.9158
forward train acc:  0.99866  and loss:  1.7531439704471268
test acc:  0.9153
********** reverse layer  8  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6145833333333334  ==>  37 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.71875  ==>  216 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8307291666666666  ==>  130 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  9
[3, 0, 1, 2, 5, 1, 2, 4, 5, 3, 2, 1, 3, 1]
***** skip layer  10
[3, 0, 1, 2, 5, 1, 2, 4, 5, 3, 1, 1, 3, 1]
***** skip layer  11
[3, 0, 1, 2, 5, 1, 2, 4, 5, 3, 1, 0, 3, 1]
***** skip layer  12
[3, 0, 1, 2, 5, 1, 2, 4, 5, 3, 1, 0, 2, 1]
***** skip layer  13
[3, 0, 1, 2, 5, 1, 2, 4, 5, 3, 1, 0, 2, 0]
***** skip layer  0
[2, 0, 1, 2, 5, 1, 2, 4, 5, 3, 1, 0, 2, 0]
optimize layer  1
backward train epoch:  144
test acc:  0.1029
forward train acc:  0.99922  and loss:  1.1144955039053457
test acc:  0.9182
forward train acc:  0.9994  and loss:  0.8013695363188162
test acc:  0.9173
forward train acc:  0.99954  and loss:  0.6136230110059842
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.71875  ==>  216 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8307291666666666  ==>  130 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  2
[2, 0, 0, 2, 5, 1, 2, 4, 5, 3, 1, 0, 2, 0]
***** skip layer  3
[2, 0, 0, 1, 5, 1, 2, 4, 5, 3, 1, 0, 2, 0]
***** skip layer  4
[2, 0, 0, 1, 4, 1, 2, 4, 5, 3, 1, 0, 2, 0]
***** skip layer  5
[2, 0, 0, 1, 4, 0, 2, 4, 5, 3, 1, 0, 2, 0]
***** skip layer  6
[2, 0, 0, 1, 4, 0, 1, 4, 5, 3, 1, 0, 2, 0]
***** skip layer  7
[2, 0, 0, 1, 4, 0, 1, 3, 5, 3, 1, 0, 2, 0]
***** skip layer  8
[2, 0, 0, 1, 4, 0, 1, 3, 4, 3, 1, 0, 2, 0]
***** skip layer  9
[2, 0, 0, 1, 4, 0, 1, 3, 4, 2, 1, 0, 2, 0]
***** skip layer  10
[2, 0, 0, 1, 4, 0, 1, 3, 4, 2, 0, 0, 2, 0]
optimize layer  11
backward train epoch:  80
test acc:  0.0947
forward train acc:  0.99968  and loss:  0.5443211824458558
test acc:  0.9188
forward train acc:  0.99934  and loss:  0.817807241139235
test acc:  0.917
forward train acc:  0.99942  and loss:  0.8857854237139691
test acc:  0.919
forward train acc:  0.99946  and loss:  0.752416630697553
test acc:  0.9188
forward train acc:  0.9995  and loss:  0.5731175956170773
test acc:  0.9187
forward train acc:  0.9996  and loss:  0.4285054604988545
test acc:  0.9204
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.71875  ==>  216 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6848958333333334  ==>  242 / 768
***** skip layer  12
[2, 0, 0, 1, 4, 0, 1, 3, 4, 2, 0, 0, 1, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.1111
forward train acc:  0.9995  and loss:  10.105279613286257
test acc:  0.9194
forward train acc:  0.99934  and loss:  8.191993908956647
test acc:  0.9214
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.71875  ==>  216 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  0
[1, 0, 0, 1, 4, 0, 1, 3, 4, 2, 0, 0, 1, 0]
optimize layer  1
backward train epoch:  210
test acc:  0.1009
forward train acc:  0.99842  and loss:  8.664868296124041
test acc:  0.9161
forward train acc:  0.99846  and loss:  7.809129660949111
test acc:  0.9157
forward train acc:  0.99892  and loss:  6.736176428385079
test acc:  0.9162
forward train acc:  0.99896  and loss:  6.0999419540166855
test acc:  0.9158
forward train acc:  0.99918  and loss:  5.5320585342124104
test acc:  0.9146
forward train acc:  0.99912  and loss:  5.396574416197836
test acc:  0.9164
forward train acc:  0.99934  and loss:  5.1446052747778594
test acc:  0.9158
forward train acc:  0.9991  and loss:  4.937359561212361
test acc:  0.9156
forward train acc:  0.99938  and loss:  4.54640799574554
test acc:  0.9167
forward train acc:  0.99924  and loss:  4.495083687826991
test acc:  0.9155
********** reverse layer  1  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.71875  ==>  216 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
optimize layer  2
backward train epoch:  45
test acc:  0.0874
forward train acc:  0.99902  and loss:  4.7298630070872605
test acc:  0.9171
forward train acc:  0.99942  and loss:  3.7426711372099817
test acc:  0.9188
forward train acc:  0.99964  and loss:  3.2834727754816413
test acc:  0.9174
forward train acc:  0.99976  and loss:  2.9075581352226436
test acc:  0.9176
forward train acc:  0.99962  and loss:  2.8312310758046806
test acc:  0.9175
forward train acc:  0.99958  and loss:  2.922657831106335
test acc:  0.9179
forward train acc:  0.99964  and loss:  2.7015457982197404
test acc:  0.9164
forward train acc:  0.99948  and loss:  2.5618918729014695
test acc:  0.9169
forward train acc:  0.99976  and loss:  2.3655034522525966
test acc:  0.9173
forward train acc:  0.9997  and loss:  2.4360268730670214
test acc:  0.9174
********** reverse layer  2  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.71875  ==>  216 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  3
[1, 5, 5, 0, 4, 0, 1, 3, 4, 2, 0, 0, 1, 0]
***** skip layer  4
[1, 5, 5, 0, 3, 0, 1, 3, 4, 2, 0, 0, 1, 0]
optimize layer  5
backward train epoch:  287
test acc:  0.0986
forward train acc:  0.99922  and loss:  3.171206976985559
test acc:  0.917
forward train acc:  0.99914  and loss:  2.970678280107677
test acc:  0.9157
forward train acc:  0.99944  and loss:  2.3443238257896155
test acc:  0.9177
forward train acc:  0.99954  and loss:  2.1566487855743617
test acc:  0.9182
forward train acc:  0.99954  and loss:  2.033681036438793
test acc:  0.9179
forward train acc:  0.99958  and loss:  1.8825597097165883
test acc:  0.9166
forward train acc:  0.99958  and loss:  1.8980573946610093
test acc:  0.9172
forward train acc:  0.99952  and loss:  1.7305828758981079
test acc:  0.9181
forward train acc:  0.9996  and loss:  1.92267363704741
test acc:  0.9172
forward train acc:  0.99966  and loss:  1.6067647619638592
test acc:  0.9172
********** reverse layer  5  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.71875  ==>  216 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  6
[1, 5, 5, 0, 3, 5, 0, 3, 4, 2, 0, 0, 1, 0]
***** skip layer  7
[1, 5, 5, 0, 3, 5, 0, 2, 4, 2, 0, 0, 1, 0]
***** skip layer  8
[1, 5, 5, 0, 3, 5, 0, 2, 3, 2, 0, 0, 1, 0]
***** skip layer  9
[1, 5, 5, 0, 3, 5, 0, 2, 3, 1, 0, 0, 1, 0]
optimize layer  10
backward train epoch:  153
test acc:  0.1097
forward train acc:  0.99968  and loss:  1.697526151780039
test acc:  0.9193
forward train acc:  0.99948  and loss:  1.8009657384827733
test acc:  0.9181
forward train acc:  0.99938  and loss:  1.8605233581038192
test acc:  0.918
forward train acc:  0.99966  and loss:  1.4245316064916551
test acc:  0.9183
forward train acc:  0.99958  and loss:  1.4352790089324117
test acc:  0.9194
forward train acc:  0.99974  and loss:  1.2838079908397049
test acc:  0.918
forward train acc:  0.99968  and loss:  1.3588093185098842
test acc:  0.9191
forward train acc:  0.9997  and loss:  1.2354447486577556
test acc:  0.9183
forward train acc:  0.99968  and loss:  1.1581892548128963
test acc:  0.9186
forward train acc:  0.99986  and loss:  1.0750554647529498
test acc:  0.9193
********** reverse layer  10  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.71875  ==>  216 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
optimize layer  11
backward train epoch:  27
test acc:  0.1046
forward train acc:  0.9996  and loss:  1.3631629749434069
test acc:  0.918
forward train acc:  0.99978  and loss:  1.0404255712637678
test acc:  0.9189
forward train acc:  0.99956  and loss:  1.2855111185926944
test acc:  0.9166
forward train acc:  0.99962  and loss:  1.1917267942335457
test acc:  0.9175
forward train acc:  0.99962  and loss:  1.14119801169727
test acc:  0.9187
forward train acc:  0.99974  and loss:  1.1309889376861975
test acc:  0.9183
forward train acc:  0.99968  and loss:  1.1473503387533128
test acc:  0.9178
forward train acc:  0.99988  and loss:  0.8798729068366811
test acc:  0.9184
forward train acc:  0.9997  and loss:  1.0619623133679852
test acc:  0.9192
forward train acc:  0.9998  and loss:  0.8811361718690023
test acc:  0.9199
********** reverse layer  11  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.71875  ==>  216 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  12
[1, 5, 5, 0, 3, 5, 0, 2, 3, 1, 5, 5, 0, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.0791
forward train acc:  0.89612  and loss:  173.00773817300797
test acc:  0.8199
forward train acc:  0.89952  and loss:  159.39894254505634
test acc:  0.8222
forward train acc:  0.89946  and loss:  153.56156941503286
test acc:  0.8222
forward train acc:  0.8996  and loss:  149.11859695613384
test acc:  0.8232
forward train acc:  0.8995  and loss:  146.06864899396896
test acc:  0.8237
forward train acc:  0.8997  and loss:  142.8207938671112
test acc:  0.823
forward train acc:  0.89966  and loss:  140.11768070608377
test acc:  0.8232
forward train acc:  0.8997  and loss:  137.84971818327904
test acc:  0.824
forward train acc:  0.89966  and loss:  136.35946568101645
test acc:  0.8222
forward train acc:  0.89976  and loss:  134.77555276453495
test acc:  0.823
********** reverse layer  13  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5520833333333334  ==>  86 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.71875  ==>  216 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  0
[0, 5, 5, 0, 3, 5, 0, 2, 3, 1, 5, 5, 0, 5]
***** skip layer  1
[0, 4, 5, 0, 3, 5, 0, 2, 3, 1, 5, 5, 0, 5]
***** skip layer  2
[0, 4, 4, 0, 3, 5, 0, 2, 3, 1, 5, 5, 0, 5]
optimize layer  3
backward train epoch:  547
test acc:  0.1011
forward train acc:  0.9979  and loss:  4.83310660358984
test acc:  0.9173
forward train acc:  0.99924  and loss:  1.8799738785019144
test acc:  0.9175
forward train acc:  0.99912  and loss:  2.0408558779163286
test acc:  0.9174
forward train acc:  0.99908  and loss:  2.135703824693337
test acc:  0.9193
forward train acc:  0.99966  and loss:  1.322138324030675
test acc:  0.9186
forward train acc:  0.99948  and loss:  1.5888775849016383
test acc:  0.9182
forward train acc:  0.99964  and loss:  1.156999421888031
test acc:  0.9185
forward train acc:  0.9994  and loss:  1.3246552245691419
test acc:  0.9194
forward train acc:  0.99958  and loss:  1.34951490489766
test acc:  0.9202
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.71875  ==>  216 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  4
[0, 4, 4, 0, 2, 5, 0, 2, 3, 1, 5, 5, 0, 5]
***** skip layer  5
[0, 4, 4, 0, 2, 4, 0, 2, 3, 1, 5, 5, 0, 5]
optimize layer  6
backward train epoch:  113
test acc:  0.0973
forward train acc:  0.99928  and loss:  1.7074129751417786
test acc:  0.9193
forward train acc:  0.99896  and loss:  2.1746657016919926
test acc:  0.9153
forward train acc:  0.99932  and loss:  1.6141447518020868
test acc:  0.9177
forward train acc:  0.99928  and loss:  1.5434862566180527
test acc:  0.9181
forward train acc:  0.99948  and loss:  1.2843458203133196
test acc:  0.9192
forward train acc:  0.99942  and loss:  1.2615946258883923
test acc:  0.9198
forward train acc:  0.99968  and loss:  1.0631185785168782
test acc:  0.9189
forward train acc:  0.99948  and loss:  1.2698971967911348
test acc:  0.918
forward train acc:  0.9994  and loss:  1.355567924445495
test acc:  0.9194
forward train acc:  0.99956  and loss:  0.9908618191257119
test acc:  0.9179
********** reverse layer  6  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.71875  ==>  216 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  7
[0, 4, 4, 0, 2, 4, 5, 1, 3, 1, 5, 5, 0, 5]
***** skip layer  8
[0, 4, 4, 0, 2, 4, 5, 1, 2, 1, 5, 5, 0, 5]
***** skip layer  9
[0, 4, 4, 0, 2, 4, 5, 1, 2, 0, 5, 5, 0, 5]
***** skip layer  10
[0, 4, 4, 0, 2, 4, 5, 1, 2, 0, 4, 5, 0, 5]
***** skip layer  11
[0, 4, 4, 0, 2, 4, 5, 1, 2, 0, 4, 4, 0, 5]
optimize layer  12
backward train epoch:  202
test acc:  0.1
forward train acc:  0.1  and loss:  1218.7790966033936
test acc:  0.1
forward train acc:  0.1  and loss:  1191.7605617046356
test acc:  0.1
forward train acc:  0.1  and loss:  1166.8809423446655
test acc:  0.1
forward train acc:  0.1  and loss:  1149.3936948776245
test acc:  0.1
forward train acc:  0.1  and loss:  1138.175099849701
test acc:  0.1
forward train acc:  0.1  and loss:  1127.3720967769623
test acc:  0.1
forward train acc:  0.1  and loss:  1116.9689650535583
test acc:  0.1
forward train acc:  0.1  and loss:  1109.3358113765717
test acc:  0.1
forward train acc:  0.1  and loss:  1104.3447105884552
test acc:  0.1
forward train acc:  0.1  and loss:  1099.3011054992676
test acc:  0.1
********** reverse layer  12  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.71875  ==>  216 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  13
[0, 4, 4, 0, 2, 4, 5, 1, 2, 0, 4, 4, 5, 4]
optimize layer  0
backward train epoch:  16
test acc:  0.1
forward train acc:  0.99712  and loss:  5.188821332529187
test acc:  0.9143
forward train acc:  0.99772  and loss:  3.8429557241033763
test acc:  0.9127
forward train acc:  0.9976  and loss:  4.11819321080111
test acc:  0.9141
forward train acc:  0.99778  and loss:  3.7370423800311983
test acc:  0.9148
forward train acc:  0.99858  and loss:  2.4279029997996986
test acc:  0.9156
forward train acc:  0.9983  and loss:  2.8670784599380568
test acc:  0.9149
forward train acc:  0.99866  and loss:  2.6318399094743654
test acc:  0.9144
forward train acc:  0.99848  and loss:  2.906250132713467
test acc:  0.9152
forward train acc:  0.99838  and loss:  2.805351287126541
test acc:  0.9167
forward train acc:  0.9988  and loss:  2.4688081092899665
test acc:  0.9168
********** reverse layer  0  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.71875  ==>  216 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  1
[5, 3, 4, 0, 2, 4, 5, 1, 2, 0, 4, 4, 5, 4]
***** skip layer  2
[5, 3, 3, 0, 2, 4, 5, 1, 2, 0, 4, 4, 5, 4]
optimize layer  3
backward train epoch:  261
test acc:  0.0949
forward train acc:  0.99888  and loss:  1.9679671906633303
test acc:  0.9176
forward train acc:  0.99908  and loss:  2.002097774646245
test acc:  0.9179
forward train acc:  0.99918  and loss:  1.7265109064755961
test acc:  0.9171
forward train acc:  0.99942  and loss:  1.2851780148921534
test acc:  0.9193
forward train acc:  0.99918  and loss:  1.6042051274562255
test acc:  0.918
forward train acc:  0.99944  and loss:  1.3359989355085418
test acc:  0.9193
forward train acc:  0.9995  and loss:  1.1794467779109254
test acc:  0.9192
forward train acc:  0.99968  and loss:  0.9456481675151736
test acc:  0.9184
forward train acc:  0.99948  and loss:  1.1294000311754644
test acc:  0.9189
forward train acc:  0.99932  and loss:  1.3026046599261463
test acc:  0.9195
********** reverse layer  3  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.71875  ==>  216 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  4
[5, 3, 3, 5, 1, 4, 5, 1, 2, 0, 4, 4, 5, 4]
***** skip layer  5
[5, 3, 3, 5, 1, 3, 5, 1, 2, 0, 4, 4, 5, 4]
***** skip layer  6
[5, 3, 3, 5, 1, 3, 4, 1, 2, 0, 4, 4, 5, 4]
***** skip layer  7
[5, 3, 3, 5, 1, 3, 4, 0, 2, 0, 4, 4, 5, 4]
***** skip layer  8
[5, 3, 3, 5, 1, 3, 4, 0, 1, 0, 4, 4, 5, 4]
optimize layer  9
backward train epoch:  222
test acc:  0.1077
forward train acc:  0.99948  and loss:  1.0898482870543376
test acc:  0.9193
forward train acc:  0.9997  and loss:  0.9750261872541159
test acc:  0.9202
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7265625  ==>  210 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  10
[5, 3, 3, 5, 1, 3, 4, 0, 1, 0, 3, 4, 5, 4]
***** skip layer  11
[5, 3, 3, 5, 1, 3, 4, 0, 1, 0, 3, 3, 5, 4]
***** skip layer  12
[5, 3, 3, 5, 1, 3, 4, 0, 1, 0, 3, 3, 4, 4]
***** skip layer  13
[5, 3, 3, 5, 1, 3, 4, 0, 1, 0, 3, 3, 4, 3]
***** skip layer  0
[4, 3, 3, 5, 1, 3, 4, 0, 1, 0, 3, 3, 4, 3]
***** skip layer  1
[4, 2, 3, 5, 1, 3, 4, 0, 1, 0, 3, 3, 4, 3]
***** skip layer  2
[4, 2, 2, 5, 1, 3, 4, 0, 1, 0, 3, 3, 4, 3]
***** skip layer  3
[4, 2, 2, 4, 1, 3, 4, 0, 1, 0, 3, 3, 4, 3]
***** skip layer  4
[4, 2, 2, 4, 0, 3, 4, 0, 1, 0, 3, 3, 4, 3]
***** skip layer  5
[4, 2, 2, 4, 0, 2, 4, 0, 1, 0, 3, 3, 4, 3]
***** skip layer  6
[4, 2, 2, 4, 0, 2, 3, 0, 1, 0, 3, 3, 4, 3]
optimize layer  7
backward train epoch:  6
test acc:  0.5749
forward train acc:  0.99842  and loss:  2.7051475911866874
test acc:  0.9176
forward train acc:  0.999  and loss:  2.1203837312059477
test acc:  0.9165
forward train acc:  0.99902  and loss:  1.6284334809752181
test acc:  0.9157
forward train acc:  0.9993  and loss:  1.387426914414391
test acc:  0.9142
forward train acc:  0.99922  and loss:  1.2383937314734794
test acc:  0.9169
forward train acc:  0.99932  and loss:  1.4105624530930072
test acc:  0.9161
forward train acc:  0.99958  and loss:  1.1567490736488253
test acc:  0.9176
forward train acc:  0.99942  and loss:  1.193365441111382
test acc:  0.9185
forward train acc:  0.99938  and loss:  1.1578420002479106
test acc:  0.9169
forward train acc:  0.99954  and loss:  1.0430329954833724
test acc:  0.9185
********** reverse layer  7  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7265625  ==>  210 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  8
[4, 2, 2, 4, 0, 2, 3, 5, 0, 0, 3, 3, 4, 3]
optimize layer  9
backward train epoch:  174
test acc:  0.1033
forward train acc:  0.99934  and loss:  1.1565680284402333
test acc:  0.9189
forward train acc:  0.99964  and loss:  0.8550702970242128
test acc:  0.9202
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6041666666666666  ==>  152 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.734375  ==>  204 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  10
[4, 2, 2, 4, 0, 2, 3, 5, 0, 0, 2, 3, 4, 3]
***** skip layer  11
[4, 2, 2, 4, 0, 2, 3, 5, 0, 0, 2, 2, 4, 3]
***** skip layer  12
[4, 2, 2, 4, 0, 2, 3, 5, 0, 0, 2, 2, 3, 3]
***** skip layer  13
[4, 2, 2, 4, 0, 2, 3, 5, 0, 0, 2, 2, 3, 2]
***** skip layer  0
[3, 2, 2, 4, 0, 2, 3, 5, 0, 0, 2, 2, 3, 2]
***** skip layer  1
[3, 1, 2, 4, 0, 2, 3, 5, 0, 0, 2, 2, 3, 2]
***** skip layer  2
[3, 1, 1, 4, 0, 2, 3, 5, 0, 0, 2, 2, 3, 2]
***** skip layer  3
[3, 1, 1, 3, 0, 2, 3, 5, 0, 0, 2, 2, 3, 2]
optimize layer  4
backward train epoch:  160
test acc:  0.1102
forward train acc:  0.9989  and loss:  1.7780340479221195
test acc:  0.9162
forward train acc:  0.99886  and loss:  1.8826852099155076
test acc:  0.9186
forward train acc:  0.99928  and loss:  1.3384700369206257
test acc:  0.9186
forward train acc:  0.99924  and loss:  1.4217138878302649
test acc:  0.9183
forward train acc:  0.99912  and loss:  1.3926607729517855
test acc:  0.9175
forward train acc:  0.99926  and loss:  1.3391969746444374
test acc:  0.9184
forward train acc:  0.99938  and loss:  1.1610644111060537
test acc:  0.9196
forward train acc:  0.99952  and loss:  1.0256968920002691
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.734375  ==>  204 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  5
[3, 1, 1, 3, 0, 1, 3, 5, 0, 0, 2, 2, 3, 2]
***** skip layer  6
[3, 1, 1, 3, 0, 1, 2, 5, 0, 0, 2, 2, 3, 2]
***** skip layer  7
[3, 1, 1, 3, 0, 1, 2, 4, 0, 0, 2, 2, 3, 2]
optimize layer  8
backward train epoch:  262
test acc:  0.0909
forward train acc:  0.996  and loss:  5.5232596542919055
test acc:  0.9133
forward train acc:  0.99764  and loss:  3.602271765936166
test acc:  0.9159
forward train acc:  0.9982  and loss:  2.496234546531923
test acc:  0.9162
forward train acc:  0.9986  and loss:  2.261860005906783
test acc:  0.9152
forward train acc:  0.99884  and loss:  2.0122982665197924
test acc:  0.9163
forward train acc:  0.99898  and loss:  1.7010968368849717
test acc:  0.918
forward train acc:  0.9991  and loss:  1.8105752369738184
test acc:  0.9168
forward train acc:  0.9989  and loss:  1.7547615116345696
test acc:  0.918
forward train acc:  0.99918  and loss:  1.5428691171691753
test acc:  0.9173
forward train acc:  0.99902  and loss:  1.7049639439792372
test acc:  0.9169
********** reverse layer  8  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.734375  ==>  204 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
optimize layer  9
backward train epoch:  363
test acc:  0.1031
forward train acc:  0.99934  and loss:  1.215122535068076
test acc:  0.9154
forward train acc:  0.99932  and loss:  1.1089966978761367
test acc:  0.9175
forward train acc:  0.99948  and loss:  1.0270249507157132
test acc:  0.9173
forward train acc:  0.99944  and loss:  0.9887592876912095
test acc:  0.9194
forward train acc:  0.99956  and loss:  0.8184520829818211
test acc:  0.9172
forward train acc:  0.99954  and loss:  0.9372608758858405
test acc:  0.9182
forward train acc:  0.99954  and loss:  0.9935652638669126
test acc:  0.9202
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7421875  ==>  198 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  10
[3, 1, 1, 3, 0, 1, 2, 4, 5, 0, 1, 2, 3, 2]
***** skip layer  11
[3, 1, 1, 3, 0, 1, 2, 4, 5, 0, 1, 1, 3, 2]
***** skip layer  12
[3, 1, 1, 3, 0, 1, 2, 4, 5, 0, 1, 1, 2, 2]
***** skip layer  13
[3, 1, 1, 3, 0, 1, 2, 4, 5, 0, 1, 1, 2, 1]
***** skip layer  0
[2, 1, 1, 3, 0, 1, 2, 4, 5, 0, 1, 1, 2, 1]
***** skip layer  1
[2, 0, 1, 3, 0, 1, 2, 4, 5, 0, 1, 1, 2, 1]
***** skip layer  2
[2, 0, 0, 3, 0, 1, 2, 4, 5, 0, 1, 1, 2, 1]
***** skip layer  3
[2, 0, 0, 2, 0, 1, 2, 4, 5, 0, 1, 1, 2, 1]
optimize layer  4
backward train epoch:  266
test acc:  0.1052
forward train acc:  0.99886  and loss:  1.9201856328290887
test acc:  0.9175
forward train acc:  0.99892  and loss:  1.7814833015436307
test acc:  0.9175
forward train acc:  0.99916  and loss:  1.3383194971247576
test acc:  0.9165
forward train acc:  0.99902  and loss:  1.4969222414074466
test acc:  0.9177
forward train acc:  0.99906  and loss:  1.6043775964644738
test acc:  0.9185
forward train acc:  0.9993  and loss:  1.0126439534360543
test acc:  0.9185
forward train acc:  0.9993  and loss:  1.2261765209259465
test acc:  0.9176
forward train acc:  0.99938  and loss:  1.117452018253971
test acc:  0.9182
forward train acc:  0.9997  and loss:  0.6906458337325603
test acc:  0.9185
forward train acc:  0.9993  and loss:  0.9352586904424243
test acc:  0.9195
********** reverse layer  4  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7421875  ==>  198 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  5
[2, 0, 0, 2, 5, 0, 2, 4, 5, 0, 1, 1, 2, 1]
***** skip layer  6
[2, 0, 0, 2, 5, 0, 1, 4, 5, 0, 1, 1, 2, 1]
***** skip layer  7
[2, 0, 0, 2, 5, 0, 1, 3, 5, 0, 1, 1, 2, 1]
***** skip layer  8
[2, 0, 0, 2, 5, 0, 1, 3, 4, 0, 1, 1, 2, 1]
optimize layer  9
backward train epoch:  68
test acc:  0.098
forward train acc:  0.99968  and loss:  0.7469564347411506
test acc:  0.9198
forward train acc:  0.99938  and loss:  1.0065293773659505
test acc:  0.9181
forward train acc:  0.99934  and loss:  1.2799020920065232
test acc:  0.9203
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  10
[2, 0, 0, 2, 5, 0, 1, 3, 4, 0, 0, 1, 2, 1]
***** skip layer  11
[2, 0, 0, 2, 5, 0, 1, 3, 4, 0, 0, 0, 2, 1]
***** skip layer  12
[2, 0, 0, 2, 5, 0, 1, 3, 4, 0, 0, 0, 1, 1]
***** skip layer  13
[2, 0, 0, 2, 5, 0, 1, 3, 4, 0, 0, 0, 1, 0]
***** skip layer  0
[1, 0, 0, 2, 5, 0, 1, 3, 4, 0, 0, 0, 1, 0]
optimize layer  1
backward train epoch:  61
test acc:  0.0898
forward train acc:  0.99624  and loss:  5.790929456066806
test acc:  0.9114
forward train acc:  0.9976  and loss:  3.360521425434854
test acc:  0.9134
forward train acc:  0.99816  and loss:  2.6748335410957225
test acc:  0.9148
forward train acc:  0.9987  and loss:  2.1069688944262452
test acc:  0.9152
forward train acc:  0.99856  and loss:  2.0073705895338207
test acc:  0.9167
forward train acc:  0.99904  and loss:  1.6669180578319356
test acc:  0.9149
forward train acc:  0.99856  and loss:  1.8462373022921383
test acc:  0.9161
forward train acc:  0.99902  and loss:  1.593707506021019
test acc:  0.9162
forward train acc:  0.99894  and loss:  1.5799878313555382
test acc:  0.9155
forward train acc:  0.99922  and loss:  1.3313976361532696
test acc:  0.9168
********** reverse layer  1  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5416666666666666  ==>  88 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
optimize layer  2
backward train epoch:  52
test acc:  0.1039
forward train acc:  0.9988  and loss:  1.9409564365050755
test acc:  0.9156
forward train acc:  0.99918  and loss:  1.1388000219594687
test acc:  0.9179
forward train acc:  0.99934  and loss:  1.2864929438219406
test acc:  0.9186
forward train acc:  0.9994  and loss:  1.0669227153994143
test acc:  0.918
forward train acc:  0.99952  and loss:  0.8280112773063593
test acc:  0.9165
forward train acc:  0.99952  and loss:  0.9105220505734906
test acc:  0.918
forward train acc:  0.99966  and loss:  0.7127441455377266
test acc:  0.9202
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  3
[1, 5, 0, 1, 5, 0, 1, 3, 4, 0, 0, 0, 1, 0]
***** skip layer  4
[1, 5, 0, 1, 4, 0, 1, 3, 4, 0, 0, 0, 1, 0]
optimize layer  5
backward train epoch:  38
test acc:  0.0823
forward train acc:  0.99894  and loss:  1.585104429279454
test acc:  0.9163
forward train acc:  0.99932  and loss:  1.1424992209649645
test acc:  0.9178
forward train acc:  0.99912  and loss:  1.3714919345802628
test acc:  0.917
forward train acc:  0.99912  and loss:  1.2748484433395788
test acc:  0.9187
forward train acc:  0.99932  and loss:  1.0358243909431621
test acc:  0.9178
forward train acc:  0.9994  and loss:  1.045955151785165
test acc:  0.9181
forward train acc:  0.99936  and loss:  1.0558455206919461
test acc:  0.9171
forward train acc:  0.99956  and loss:  0.7026670391205698
test acc:  0.9172
forward train acc:  0.99964  and loss:  0.7453967206529342
test acc:  0.9169
forward train acc:  0.99954  and loss:  0.9011671515181661
test acc:  0.9173
********** reverse layer  5  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  6
[1, 5, 0, 1, 4, 5, 0, 3, 4, 0, 0, 0, 1, 0]
***** skip layer  7
[1, 5, 0, 1, 4, 5, 0, 2, 4, 0, 0, 0, 1, 0]
***** skip layer  8
[1, 5, 0, 1, 4, 5, 0, 2, 3, 0, 0, 0, 1, 0]
optimize layer  9
backward train epoch:  105
test acc:  0.0983
forward train acc:  0.99956  and loss:  0.8177000804571435
test acc:  0.9165
forward train acc:  0.99934  and loss:  1.0152212447137572
test acc:  0.9172
forward train acc:  0.99946  and loss:  0.9208694873959757
test acc:  0.9174
forward train acc:  0.99944  and loss:  0.91299954263377
test acc:  0.9178
forward train acc:  0.99966  and loss:  0.5916546395746991
test acc:  0.917
forward train acc:  0.9996  and loss:  0.7275283321505412
test acc:  0.9167
forward train acc:  0.9995  and loss:  0.7160815399838611
test acc:  0.9188
forward train acc:  0.99976  and loss:  0.5277243466407526
test acc:  0.9182
forward train acc:  0.99976  and loss:  0.5382930751657113
test acc:  0.9191
forward train acc:  0.99982  and loss:  0.5002441654214635
test acc:  0.9192
********** reverse layer  9  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
optimize layer  10
backward train epoch:  141
test acc:  0.0915
forward train acc:  0.9997  and loss:  0.6599483408499509
test acc:  0.9178
forward train acc:  0.9993  and loss:  1.003091945371125
test acc:  0.9161
forward train acc:  0.99932  and loss:  1.0799408893217333
test acc:  0.9133
forward train acc:  0.99924  and loss:  0.9921822880860418
test acc:  0.9167
forward train acc:  0.99926  and loss:  1.042028138646856
test acc:  0.9171
forward train acc:  0.9996  and loss:  0.8471391247876454
test acc:  0.9185
forward train acc:  0.99948  and loss:  0.7749583479599096
test acc:  0.9181
forward train acc:  0.9996  and loss:  0.6674378757597879
test acc:  0.9197
forward train acc:  0.99958  and loss:  0.599508786253864
test acc:  0.9191
forward train acc:  0.99968  and loss:  0.4545368271355983
test acc:  0.9194
********** reverse layer  10  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
optimize layer  11
backward train epoch:  150
test acc:  0.0989
forward train acc:  0.99962  and loss:  0.8030710788443685
test acc:  0.9165
forward train acc:  0.99944  and loss:  0.8129695611132775
test acc:  0.9173
forward train acc:  0.99932  and loss:  1.107632043218473
test acc:  0.9166
forward train acc:  0.99954  and loss:  0.7984841681027319
test acc:  0.9169
forward train acc:  0.9996  and loss:  0.6755200729821809
test acc:  0.9191
forward train acc:  0.9995  and loss:  0.7491563393559773
test acc:  0.9197
forward train acc:  0.99966  and loss:  0.7278330264962278
test acc:  0.9165
forward train acc:  0.99968  and loss:  0.5342857228242792
test acc:  0.9178
forward train acc:  0.99968  and loss:  0.5929388799704611
test acc:  0.9177
forward train acc:  0.99954  and loss:  0.7323348242498469
test acc:  0.918
********** reverse layer  11  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  12
[1, 5, 0, 1, 4, 5, 0, 2, 3, 5, 5, 5, 0, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.1075
forward train acc:  0.98546  and loss:  52.62230184301734
test acc:  0.915
forward train acc:  0.9992  and loss:  34.74508743733168
test acc:  0.9115
forward train acc:  0.99908  and loss:  32.53106901049614
test acc:  0.9136
forward train acc:  0.99922  and loss:  30.043046187609434
test acc:  0.9152
forward train acc:  0.99946  and loss:  28.733323864638805
test acc:  0.9152
forward train acc:  0.99954  and loss:  27.4085681643337
test acc:  0.915
forward train acc:  0.99938  and loss:  26.31706376001239
test acc:  0.9153
forward train acc:  0.99944  and loss:  25.399591270834208
test acc:  0.916
forward train acc:  0.9995  and loss:  24.694330293685198
test acc:  0.9166
forward train acc:  0.99956  and loss:  24.070448191836476
test acc:  0.9155
********** reverse layer  13  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  0
[0, 5, 0, 1, 4, 5, 0, 2, 3, 5, 5, 5, 0, 5]
***** skip layer  1
[0, 4, 0, 1, 4, 5, 0, 2, 3, 5, 5, 5, 0, 5]
optimize layer  2
backward train epoch:  73
test acc:  0.0939
forward train acc:  0.9989  and loss:  2.6690049358294345
test acc:  0.9168
forward train acc:  0.99944  and loss:  1.1024275452946313
test acc:  0.9173
forward train acc:  0.99938  and loss:  1.1616533892811276
test acc:  0.9172
forward train acc:  0.99912  and loss:  1.2104306346736848
test acc:  0.9175
forward train acc:  0.99962  and loss:  0.6254273591330275
test acc:  0.9173
forward train acc:  0.99952  and loss:  0.7084848482045345
test acc:  0.9191
forward train acc:  0.9997  and loss:  0.6412119425367564
test acc:  0.9172
forward train acc:  0.99968  and loss:  0.666957653767895
test acc:  0.9195
forward train acc:  0.99976  and loss:  0.5771349366987124
test acc:  0.9188
forward train acc:  0.99962  and loss:  0.7740632042405196
test acc:  0.9189
********** reverse layer  2  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6354166666666666  ==>  140 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  3
[0, 4, 5, 0, 4, 5, 0, 2, 3, 5, 5, 5, 0, 5]
***** skip layer  4
[0, 4, 5, 0, 3, 5, 0, 2, 3, 5, 5, 5, 0, 5]
***** skip layer  5
[0, 4, 5, 0, 3, 4, 0, 2, 3, 5, 5, 5, 0, 5]
optimize layer  6
backward train epoch:  38
test acc:  0.0971
forward train acc:  0.99916  and loss:  1.3719124475028366
test acc:  0.9168
forward train acc:  0.99936  and loss:  0.98079837980913
test acc:  0.9167
forward train acc:  0.99928  and loss:  1.0910358200781047
test acc:  0.9182
forward train acc:  0.9994  and loss:  0.9153656393173151
test acc:  0.9183
forward train acc:  0.99954  and loss:  0.6450417643936817
test acc:  0.9185
forward train acc:  0.99948  and loss:  0.9680919556121808
test acc:  0.9185
forward train acc:  0.99974  and loss:  0.5448341515439097
test acc:  0.9203
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  7
[0, 4, 5, 0, 3, 4, 0, 1, 3, 5, 5, 5, 0, 5]
***** skip layer  8
[0, 4, 5, 0, 3, 4, 0, 1, 2, 5, 5, 5, 0, 5]
***** skip layer  9
[0, 4, 5, 0, 3, 4, 0, 1, 2, 4, 5, 5, 0, 5]
***** skip layer  10
[0, 4, 5, 0, 3, 4, 0, 1, 2, 4, 4, 5, 0, 5]
***** skip layer  11
[0, 4, 5, 0, 3, 4, 0, 1, 2, 4, 4, 4, 0, 5]
optimize layer  12
backward train epoch:  239
test acc:  0.1
forward train acc:  0.1  and loss:  1317.8760118484497
test acc:  0.1
forward train acc:  0.1  and loss:  1285.7827682495117
test acc:  0.1
forward train acc:  0.1  and loss:  1255.7399463653564
test acc:  0.1
forward train acc:  0.1  and loss:  1234.4787225723267
test acc:  0.1
forward train acc:  0.1  and loss:  1220.5635356903076
test acc:  0.1
forward train acc:  0.1  and loss:  1207.1947391033173
test acc:  0.1
forward train acc:  0.1  and loss:  1194.1165418624878
test acc:  0.1
forward train acc:  0.1  and loss:  1184.5910999774933
test acc:  0.1
forward train acc:  0.1  and loss:  1178.1835010051727
test acc:  0.1
forward train acc:  0.1  and loss:  1171.9721462726593
test acc:  0.1
********** reverse layer  12  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  13
[0, 4, 5, 0, 3, 4, 0, 1, 2, 4, 4, 4, 5, 4]
optimize layer  0
backward train epoch:  68
test acc:  0.0855
forward train acc:  0.9993  and loss:  1.1588156984071247
test acc:  0.9172
forward train acc:  0.999  and loss:  1.4442592324339785
test acc:  0.9162
forward train acc:  0.99924  and loss:  1.126827122294344
test acc:  0.9169
forward train acc:  0.99948  and loss:  0.9990735731553286
test acc:  0.9179
forward train acc:  0.9995  and loss:  0.8639530308137182
test acc:  0.9177
forward train acc:  0.99946  and loss:  0.8524114477913827
test acc:  0.9177
forward train acc:  0.99946  and loss:  1.088884637632873
test acc:  0.9168
forward train acc:  0.99962  and loss:  0.6851408581715077
test acc:  0.9182
forward train acc:  0.99964  and loss:  0.620182603248395
test acc:  0.9177
forward train acc:  0.99962  and loss:  0.6788330493145622
test acc:  0.9173
********** reverse layer  0  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  1
[5, 3, 5, 0, 3, 4, 0, 1, 2, 4, 4, 4, 5, 4]
***** skip layer  2
[5, 3, 4, 0, 3, 4, 0, 1, 2, 4, 4, 4, 5, 4]
optimize layer  3
backward train epoch:  227
test acc:  0.0978
forward train acc:  0.99914  and loss:  1.3078862680122256
test acc:  0.9139
forward train acc:  0.999  and loss:  1.6259198923944496
test acc:  0.9156
forward train acc:  0.99892  and loss:  1.530560894170776
test acc:  0.9138
forward train acc:  0.99898  and loss:  1.4523004720103927
test acc:  0.9156
forward train acc:  0.99926  and loss:  1.2324841001536697
test acc:  0.9159
forward train acc:  0.99936  and loss:  0.972895071725361
test acc:  0.9163
forward train acc:  0.99936  and loss:  1.0664172981632873
test acc:  0.917
forward train acc:  0.99934  and loss:  0.9523754401598126
test acc:  0.9178
forward train acc:  0.99938  and loss:  0.9325429135933518
test acc:  0.9173
forward train acc:  0.99968  and loss:  0.6509445394040085
test acc:  0.9174
********** reverse layer  3  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  4
[5, 3, 4, 5, 2, 4, 0, 1, 2, 4, 4, 4, 5, 4]
***** skip layer  5
[5, 3, 4, 5, 2, 3, 0, 1, 2, 4, 4, 4, 5, 4]
optimize layer  6
backward train epoch:  84
test acc:  0.0889
forward train acc:  0.99946  and loss:  0.8283402197412215
test acc:  0.9163
forward train acc:  0.99938  and loss:  1.025523295335006
test acc:  0.9174
forward train acc:  0.99934  and loss:  1.0332119903468993
test acc:  0.9166
forward train acc:  0.99958  and loss:  0.7684972208517138
test acc:  0.9183
forward train acc:  0.99954  and loss:  0.707226594153326
test acc:  0.9178
forward train acc:  0.99968  and loss:  0.5579912475659512
test acc:  0.9169
forward train acc:  0.99962  and loss:  0.6850136401772033
test acc:  0.9189
forward train acc:  0.9995  and loss:  0.6571299134811852
test acc:  0.9184
forward train acc:  0.99958  and loss:  0.675280157243833
test acc:  0.9188
forward train acc:  0.9997  and loss:  0.5150249581201933
test acc:  0.9191
********** reverse layer  6  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  7
[5, 3, 4, 5, 2, 3, 5, 0, 2, 4, 4, 4, 5, 4]
***** skip layer  8
[5, 3, 4, 5, 2, 3, 5, 0, 1, 4, 4, 4, 5, 4]
***** skip layer  9
[5, 3, 4, 5, 2, 3, 5, 0, 1, 3, 4, 4, 5, 4]
***** skip layer  10
[5, 3, 4, 5, 2, 3, 5, 0, 1, 3, 3, 4, 5, 4]
***** skip layer  11
[5, 3, 4, 5, 2, 3, 5, 0, 1, 3, 3, 3, 5, 4]
***** skip layer  12
[5, 3, 4, 5, 2, 3, 5, 0, 1, 3, 3, 3, 4, 4]
***** skip layer  13
[5, 3, 4, 5, 2, 3, 5, 0, 1, 3, 3, 3, 4, 3]
***** skip layer  0
[4, 3, 4, 5, 2, 3, 5, 0, 1, 3, 3, 3, 4, 3]
***** skip layer  1
[4, 2, 4, 5, 2, 3, 5, 0, 1, 3, 3, 3, 4, 3]
***** skip layer  2
[4, 2, 3, 5, 2, 3, 5, 0, 1, 3, 3, 3, 4, 3]
***** skip layer  3
[4, 2, 3, 4, 2, 3, 5, 0, 1, 3, 3, 3, 4, 3]
***** skip layer  4
[4, 2, 3, 4, 1, 3, 5, 0, 1, 3, 3, 3, 4, 3]
***** skip layer  5
[4, 2, 3, 4, 1, 2, 5, 0, 1, 3, 3, 3, 4, 3]
***** skip layer  6
[4, 2, 3, 4, 1, 2, 4, 0, 1, 3, 3, 3, 4, 3]
optimize layer  7
backward train epoch:  236
test acc:  0.0813
forward train acc:  0.99898  and loss:  1.5175008350342978
test acc:  0.9163
forward train acc:  0.99906  and loss:  1.5788097294280306
test acc:  0.9148
forward train acc:  0.99918  and loss:  1.089429213287076
test acc:  0.915
forward train acc:  0.99924  and loss:  1.1709406924492214
test acc:  0.9147
forward train acc:  0.99948  and loss:  0.8421573690138757
test acc:  0.9157
forward train acc:  0.99956  and loss:  0.6930224103562068
test acc:  0.9167
forward train acc:  0.99948  and loss:  0.8627006169699598
test acc:  0.9162
forward train acc:  0.99954  and loss:  0.7739449265645817
test acc:  0.9181
forward train acc:  0.9995  and loss:  0.895895532623399
test acc:  0.9166
forward train acc:  0.99948  and loss:  0.8515267568873242
test acc:  0.9171
********** reverse layer  7  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  8
[4, 2, 3, 4, 1, 2, 4, 5, 0, 3, 3, 3, 4, 3]
***** skip layer  9
[4, 2, 3, 4, 1, 2, 4, 5, 0, 2, 3, 3, 4, 3]
***** skip layer  10
[4, 2, 3, 4, 1, 2, 4, 5, 0, 2, 2, 3, 4, 3]
***** skip layer  11
[4, 2, 3, 4, 1, 2, 4, 5, 0, 2, 2, 2, 4, 3]
***** skip layer  12
[4, 2, 3, 4, 1, 2, 4, 5, 0, 2, 2, 2, 3, 3]
***** skip layer  13
[4, 2, 3, 4, 1, 2, 4, 5, 0, 2, 2, 2, 3, 2]
***** skip layer  0
[3, 2, 3, 4, 1, 2, 4, 5, 0, 2, 2, 2, 3, 2]
***** skip layer  1
[3, 1, 3, 4, 1, 2, 4, 5, 0, 2, 2, 2, 3, 2]
***** skip layer  2
[3, 1, 2, 4, 1, 2, 4, 5, 0, 2, 2, 2, 3, 2]
***** skip layer  3
[3, 1, 2, 3, 1, 2, 4, 5, 0, 2, 2, 2, 3, 2]
***** skip layer  4
[3, 1, 2, 3, 0, 2, 4, 5, 0, 2, 2, 2, 3, 2]
***** skip layer  5
[3, 1, 2, 3, 0, 1, 4, 5, 0, 2, 2, 2, 3, 2]
***** skip layer  6
[3, 1, 2, 3, 0, 1, 3, 5, 0, 2, 2, 2, 3, 2]
***** skip layer  7
[3, 1, 2, 3, 0, 1, 3, 4, 0, 2, 2, 2, 3, 2]
optimize layer  8
backward train epoch:  721
test acc:  0.091
forward train acc:  0.99906  and loss:  1.5625471582752652
test acc:  0.9142
forward train acc:  0.99914  and loss:  1.2193895879900083
test acc:  0.9145
forward train acc:  0.99916  and loss:  1.2899994069593959
test acc:  0.9138
forward train acc:  0.99912  and loss:  1.3965411520039197
test acc:  0.9159
forward train acc:  0.99946  and loss:  0.7573926866461989
test acc:  0.9159
forward train acc:  0.99946  and loss:  0.8630151802499313
test acc:  0.9161
forward train acc:  0.99948  and loss:  0.7983465595752932
test acc:  0.9158
forward train acc:  0.9995  and loss:  0.7175909766810946
test acc:  0.9164
forward train acc:  0.99942  and loss:  0.8889086595445406
test acc:  0.9164
forward train acc:  0.99938  and loss:  0.9202685434138402
test acc:  0.917
********** reverse layer  8  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  9
[3, 1, 2, 3, 0, 1, 3, 4, 5, 1, 2, 2, 3, 2]
***** skip layer  10
[3, 1, 2, 3, 0, 1, 3, 4, 5, 1, 1, 2, 3, 2]
***** skip layer  11
[3, 1, 2, 3, 0, 1, 3, 4, 5, 1, 1, 1, 3, 2]
***** skip layer  12
[3, 1, 2, 3, 0, 1, 3, 4, 5, 1, 1, 1, 2, 2]
***** skip layer  13
[3, 1, 2, 3, 0, 1, 3, 4, 5, 1, 1, 1, 2, 1]
***** skip layer  0
[2, 1, 2, 3, 0, 1, 3, 4, 5, 1, 1, 1, 2, 1]
***** skip layer  1
[2, 0, 2, 3, 0, 1, 3, 4, 5, 1, 1, 1, 2, 1]
***** skip layer  2
[2, 0, 1, 3, 0, 1, 3, 4, 5, 1, 1, 1, 2, 1]
***** skip layer  3
[2, 0, 1, 2, 0, 1, 3, 4, 5, 1, 1, 1, 2, 1]
optimize layer  4
backward train epoch:  165
test acc:  0.1011
forward train acc:  0.99904  and loss:  1.4533230355009437
test acc:  0.914
forward train acc:  0.99882  and loss:  1.793822434177855
test acc:  0.9147
forward train acc:  0.99922  and loss:  1.062133349449141
test acc:  0.9148
forward train acc:  0.9994  and loss:  1.0000522974005435
test acc:  0.9168
forward train acc:  0.99924  and loss:  1.1380161542329006
test acc:  0.9159
forward train acc:  0.9995  and loss:  0.8850161886657588
test acc:  0.9162
forward train acc:  0.99942  and loss:  0.9651428729412146
test acc:  0.9172
forward train acc:  0.99964  and loss:  0.6130094426916912
test acc:  0.9173
forward train acc:  0.9993  and loss:  0.8941248815390281
test acc:  0.9166
forward train acc:  0.99944  and loss:  1.027426122163888
test acc:  0.917
********** reverse layer  4  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.625  ==>  36 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  5
[2, 0, 1, 2, 5, 0, 3, 4, 5, 1, 1, 1, 2, 1]
***** skip layer  6
[2, 0, 1, 2, 5, 0, 2, 4, 5, 1, 1, 1, 2, 1]
***** skip layer  7
[2, 0, 1, 2, 5, 0, 2, 3, 5, 1, 1, 1, 2, 1]
***** skip layer  8
[2, 0, 1, 2, 5, 0, 2, 3, 4, 1, 1, 1, 2, 1]
***** skip layer  9
[2, 0, 1, 2, 5, 0, 2, 3, 4, 0, 1, 1, 2, 1]
***** skip layer  10
[2, 0, 1, 2, 5, 0, 2, 3, 4, 0, 0, 1, 2, 1]
***** skip layer  11
[2, 0, 1, 2, 5, 0, 2, 3, 4, 0, 0, 0, 2, 1]
***** skip layer  12
[2, 0, 1, 2, 5, 0, 2, 3, 4, 0, 0, 0, 1, 1]
***** skip layer  13
[2, 0, 1, 2, 5, 0, 2, 3, 4, 0, 0, 0, 1, 0]
***** skip layer  0
[1, 0, 1, 2, 5, 0, 2, 3, 4, 0, 0, 0, 1, 0]
optimize layer  1
backward train epoch:  84
test acc:  0.1
forward train acc:  0.99836  and loss:  2.2291120550362393
test acc:  0.9141
forward train acc:  0.9989  and loss:  1.3933031413471326
test acc:  0.9184
forward train acc:  0.99884  and loss:  1.6223302725702524
test acc:  0.9171
forward train acc:  0.99918  and loss:  1.228212934685871
test acc:  0.9181
forward train acc:  0.99898  and loss:  1.243534224195173
test acc:  0.9181
forward train acc:  0.99926  and loss:  1.051126059377566
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  2
[1, 0, 0, 2, 5, 0, 2, 3, 4, 0, 0, 0, 1, 0]
***** skip layer  3
[1, 0, 0, 1, 5, 0, 2, 3, 4, 0, 0, 0, 1, 0]
***** skip layer  4
[1, 0, 0, 1, 4, 0, 2, 3, 4, 0, 0, 0, 1, 0]
optimize layer  5
backward train epoch:  178
test acc:  0.1
forward train acc:  0.99878  and loss:  1.7045055438065901
test acc:  0.9159
forward train acc:  0.99896  and loss:  1.5512010804377496
test acc:  0.916
forward train acc:  0.9987  and loss:  1.823595985188149
test acc:  0.9156
forward train acc:  0.99896  and loss:  1.406869234459009
test acc:  0.9161
forward train acc:  0.9993  and loss:  0.953851152502466
test acc:  0.9168
forward train acc:  0.99912  and loss:  1.1951413130445872
test acc:  0.9168
forward train acc:  0.9989  and loss:  1.409117849660106
test acc:  0.915
forward train acc:  0.99924  and loss:  1.1204921452736016
test acc:  0.9162
forward train acc:  0.99942  and loss:  0.8676690018037334
test acc:  0.9158
forward train acc:  0.99954  and loss:  0.8313385179499164
test acc:  0.9162
********** reverse layer  5  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  6
[1, 0, 0, 1, 4, 5, 1, 3, 4, 0, 0, 0, 1, 0]
***** skip layer  7
[1, 0, 0, 1, 4, 5, 1, 2, 4, 0, 0, 0, 1, 0]
***** skip layer  8
[1, 0, 0, 1, 4, 5, 1, 2, 3, 0, 0, 0, 1, 0]
optimize layer  9
backward train epoch:  26
test acc:  0.1096
forward train acc:  0.99934  and loss:  0.9158740772982128
test acc:  0.915
forward train acc:  0.99944  and loss:  0.9190393112949096
test acc:  0.9163
forward train acc:  0.99956  and loss:  0.7597818063513841
test acc:  0.9155
forward train acc:  0.99932  and loss:  1.114206175901927
test acc:  0.9171
forward train acc:  0.99956  and loss:  0.7815702664083801
test acc:  0.9163
forward train acc:  0.99936  and loss:  0.9732808344124351
test acc:  0.9188
forward train acc:  0.99964  and loss:  0.6031987408641726
test acc:  0.918
forward train acc:  0.99954  and loss:  0.6864051094744354
test acc:  0.9189
forward train acc:  0.99956  and loss:  0.7012692563585006
test acc:  0.9164
forward train acc:  0.99964  and loss:  0.6051522977359127
test acc:  0.9177
********** reverse layer  9  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
optimize layer  10
backward train epoch:  192
test acc:  0.1006
forward train acc:  0.99954  and loss:  0.7844295882096048
test acc:  0.9177
forward train acc:  0.99938  and loss:  0.7548983194865286
test acc:  0.9175
forward train acc:  0.99926  and loss:  1.0711290711769834
test acc:  0.9173
forward train acc:  0.9995  and loss:  0.6845601848326623
test acc:  0.9173
forward train acc:  0.9995  and loss:  0.7145712733909022
test acc:  0.9174
forward train acc:  0.99946  and loss:  0.7891934697690886
test acc:  0.9173
forward train acc:  0.99952  and loss:  0.6773984856845345
test acc:  0.917
forward train acc:  0.9995  and loss:  0.8357096436375286
test acc:  0.9181
forward train acc:  0.99976  and loss:  0.4623893391981255
test acc:  0.9173
forward train acc:  0.99966  and loss:  0.5957723302417435
test acc:  0.9185
********** reverse layer  10  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
optimize layer  11
backward train epoch:  386
test acc:  0.0839
forward train acc:  0.99938  and loss:  1.0028842944011558
test acc:  0.9184
forward train acc:  0.99932  and loss:  1.0824706412968226
test acc:  0.9173
forward train acc:  0.99914  and loss:  1.2343556534906384
test acc:  0.9173
forward train acc:  0.99958  and loss:  0.9187885934952646
test acc:  0.9162
forward train acc:  0.99948  and loss:  0.7672345866158139
test acc:  0.9168
forward train acc:  0.99968  and loss:  0.5938175272603985
test acc:  0.918
forward train acc:  0.99962  and loss:  0.8015743873256724
test acc:  0.9165
forward train acc:  0.99956  and loss:  0.7516311039798893
test acc:  0.9168
forward train acc:  0.99974  and loss:  0.4938943344168365
test acc:  0.919
forward train acc:  0.99964  and loss:  0.5886957759794313
test acc:  0.9184
********** reverse layer  11  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  12
[1, 0, 0, 1, 4, 5, 1, 2, 3, 5, 5, 5, 0, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.1029
forward train acc:  0.89472  and loss:  278.7627212703228
test acc:  0.8319
forward train acc:  0.89914  and loss:  261.166986733675
test acc:  0.8347
forward train acc:  0.89916  and loss:  250.6878426671028
test acc:  0.8347
forward train acc:  0.89942  and loss:  243.16909968852997
test acc:  0.8364
forward train acc:  0.89944  and loss:  238.32293084263802
test acc:  0.8374
forward train acc:  0.89942  and loss:  233.57546228170395
test acc:  0.8364
forward train acc:  0.89954  and loss:  228.954488158226
test acc:  0.8363
forward train acc:  0.89956  and loss:  225.3719467818737
test acc:  0.838
forward train acc:  0.89968  and loss:  222.9201798737049
test acc:  0.8373
forward train acc:  0.89974  and loss:  220.76218929886818
test acc:  0.8372
********** reverse layer  13  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  0
[0, 0, 0, 1, 4, 5, 1, 2, 3, 5, 5, 5, 0, 5]
optimize layer  1
backward train epoch:  78
test acc:  0.1
forward train acc:  0.9982  and loss:  3.2833322646911256
test acc:  0.913
forward train acc:  0.9983  and loss:  2.4270780592923984
test acc:  0.9144
forward train acc:  0.99884  and loss:  1.9764753888885025
test acc:  0.9144
forward train acc:  0.99896  and loss:  1.6809911531745456
test acc:  0.9159
forward train acc:  0.99902  and loss:  1.5469130866113119
test acc:  0.9167
forward train acc:  0.99938  and loss:  0.9534924263425637
test acc:  0.9173
forward train acc:  0.9991  and loss:  1.2716060679813381
test acc:  0.9167
forward train acc:  0.99918  and loss:  1.1766896768822335
test acc:  0.9168
forward train acc:  0.99924  and loss:  0.9993818391230889
test acc:  0.9168
forward train acc:  0.99936  and loss:  0.9845117786608171
test acc:  0.917
********** reverse layer  1  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
optimize layer  2
backward train epoch:  95
test acc:  0.1071
forward train acc:  0.99926  and loss:  1.1527924495458137
test acc:  0.914
forward train acc:  0.99922  and loss:  1.0972418482415378
test acc:  0.9165
forward train acc:  0.9995  and loss:  0.7770219247031491
test acc:  0.9174
forward train acc:  0.99938  and loss:  1.0607810866786167
test acc:  0.9199
forward train acc:  0.9997  and loss:  0.5227907499356661
test acc:  0.9187
forward train acc:  0.99948  and loss:  0.7436483767814934
test acc:  0.919
forward train acc:  0.9994  and loss:  0.8060385794087779
test acc:  0.9186
forward train acc:  0.99968  and loss:  0.5560435362858698
test acc:  0.9177
forward train acc:  0.99964  and loss:  0.7032860351609997
test acc:  0.9184
forward train acc:  0.99968  and loss:  0.5710594253905583
test acc:  0.9191
********** reverse layer  2  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  3
[0, 5, 5, 0, 4, 5, 1, 2, 3, 5, 5, 5, 0, 5]
***** skip layer  4
[0, 5, 5, 0, 3, 5, 1, 2, 3, 5, 5, 5, 0, 5]
***** skip layer  5
[0, 5, 5, 0, 3, 4, 1, 2, 3, 5, 5, 5, 0, 5]
***** skip layer  6
[0, 5, 5, 0, 3, 4, 0, 2, 3, 5, 5, 5, 0, 5]
***** skip layer  7
[0, 5, 5, 0, 3, 4, 0, 1, 3, 5, 5, 5, 0, 5]
***** skip layer  8
[0, 5, 5, 0, 3, 4, 0, 1, 2, 5, 5, 5, 0, 5]
***** skip layer  9
[0, 5, 5, 0, 3, 4, 0, 1, 2, 4, 5, 5, 0, 5]
***** skip layer  10
[0, 5, 5, 0, 3, 4, 0, 1, 2, 4, 4, 5, 0, 5]
***** skip layer  11
[0, 5, 5, 0, 3, 4, 0, 1, 2, 4, 4, 4, 0, 5]
optimize layer  12
backward train epoch:  81
test acc:  0.1
forward train acc:  0.1  and loss:  1247.7889688014984
test acc:  0.1
forward train acc:  0.1  and loss:  1217.9664344787598
test acc:  0.1
forward train acc:  0.1  and loss:  1190.5838050842285
test acc:  0.1
forward train acc:  0.1  and loss:  1171.0367777347565
test acc:  0.1
forward train acc:  0.1  and loss:  1158.7946140766144
test acc:  0.1
forward train acc:  0.1  and loss:  1146.6814017295837
test acc:  0.1
forward train acc:  0.1  and loss:  1135.0248682498932
test acc:  0.1
forward train acc:  0.1  and loss:  1126.403285741806
test acc:  0.1
forward train acc:  0.1  and loss:  1120.7438488006592
test acc:  0.1
forward train acc:  0.1  and loss:  1115.3690581321716
test acc:  0.1
********** reverse layer  12  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  13
[0, 5, 5, 0, 3, 4, 0, 1, 2, 4, 4, 4, 5, 4]
optimize layer  0
backward train epoch:  292
test acc:  0.0978
forward train acc:  0.6703  and loss:  593.7738785147667
test acc:  0.7181
forward train acc:  0.7581  and loss:  312.18630999326706
test acc:  0.7636
forward train acc:  0.809  and loss:  242.68814146518707
test acc:  0.7908
forward train acc:  0.82996  and loss:  212.35318726301193
test acc:  0.8038
forward train acc:  0.8437  and loss:  193.38203358650208
test acc:  0.8127
forward train acc:  0.85964  and loss:  175.01262465119362
test acc:  0.8185
forward train acc:  0.86526  and loss:  167.760263890028
test acc:  0.8264
forward train acc:  0.87542  and loss:  156.5757638812065
test acc:  0.8286
forward train acc:  0.88002  and loss:  148.5600919276476
test acc:  0.831
forward train acc:  0.88368  and loss:  142.70131032168865
test acc:  0.8373
********** reverse layer  0  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  1
[5, 4, 5, 0, 3, 4, 0, 1, 2, 4, 4, 4, 5, 4]
***** skip layer  2
[5, 4, 4, 0, 3, 4, 0, 1, 2, 4, 4, 4, 5, 4]
optimize layer  3
backward train epoch:  1
test acc:  0.1
forward train acc:  0.9944  and loss:  9.0719823052641
test acc:  0.9152
forward train acc:  0.9974  and loss:  3.620032203849405
test acc:  0.9161
forward train acc:  0.99826  and loss:  2.475185177230742
test acc:  0.9161
forward train acc:  0.99872  and loss:  1.9294890491873957
test acc:  0.9172
forward train acc:  0.99892  and loss:  1.566379249561578
test acc:  0.9166
forward train acc:  0.99886  and loss:  1.5525697550037876
test acc:  0.9184
forward train acc:  0.9991  and loss:  1.3172004391090013
test acc:  0.9183
forward train acc:  0.99918  and loss:  1.4232818721211515
test acc:  0.9187
forward train acc:  0.9993  and loss:  1.2607782863778993
test acc:  0.9176
forward train acc:  0.9992  and loss:  1.2952085969154723
test acc:  0.9174
********** reverse layer  3  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  4
[5, 4, 4, 5, 2, 4, 0, 1, 2, 4, 4, 4, 5, 4]
***** skip layer  5
[5, 4, 4, 5, 2, 3, 0, 1, 2, 4, 4, 4, 5, 4]
optimize layer  6
backward train epoch:  84
test acc:  0.1046
forward train acc:  0.99898  and loss:  1.3345254150335677
test acc:  0.916
forward train acc:  0.99906  and loss:  1.2305255196406506
test acc:  0.9172
forward train acc:  0.99894  and loss:  1.6566324806772172
test acc:  0.917
forward train acc:  0.99914  and loss:  1.2623534075682983
test acc:  0.9143
forward train acc:  0.9994  and loss:  1.0215327840996906
test acc:  0.9169
forward train acc:  0.99948  and loss:  0.8263419691938907
test acc:  0.9167
forward train acc:  0.99944  and loss:  0.8834343348280527
test acc:  0.9179
forward train acc:  0.99932  and loss:  1.1054398701526225
test acc:  0.9167
forward train acc:  0.99944  and loss:  0.860340781044215
test acc:  0.9174
forward train acc:  0.99956  and loss:  0.7836213200353086
test acc:  0.9171
********** reverse layer  6  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  7
[5, 4, 4, 5, 2, 3, 5, 0, 2, 4, 4, 4, 5, 4]
***** skip layer  8
[5, 4, 4, 5, 2, 3, 5, 0, 1, 4, 4, 4, 5, 4]
***** skip layer  9
[5, 4, 4, 5, 2, 3, 5, 0, 1, 3, 4, 4, 5, 4]
***** skip layer  10
[5, 4, 4, 5, 2, 3, 5, 0, 1, 3, 3, 4, 5, 4]
***** skip layer  11
[5, 4, 4, 5, 2, 3, 5, 0, 1, 3, 3, 3, 5, 4]
***** skip layer  12
[5, 4, 4, 5, 2, 3, 5, 0, 1, 3, 3, 3, 4, 4]
***** skip layer  13
[5, 4, 4, 5, 2, 3, 5, 0, 1, 3, 3, 3, 4, 3]
***** skip layer  0
[4, 4, 4, 5, 2, 3, 5, 0, 1, 3, 3, 3, 4, 3]
***** skip layer  1
[4, 3, 4, 5, 2, 3, 5, 0, 1, 3, 3, 3, 4, 3]
***** skip layer  2
[4, 3, 3, 5, 2, 3, 5, 0, 1, 3, 3, 3, 4, 3]
***** skip layer  3
[4, 3, 3, 4, 2, 3, 5, 0, 1, 3, 3, 3, 4, 3]
***** skip layer  4
[4, 3, 3, 4, 1, 3, 5, 0, 1, 3, 3, 3, 4, 3]
***** skip layer  5
[4, 3, 3, 4, 1, 2, 5, 0, 1, 3, 3, 3, 4, 3]
***** skip layer  6
[4, 3, 3, 4, 1, 2, 4, 0, 1, 3, 3, 3, 4, 3]
optimize layer  7
backward train epoch:  169
test acc:  0.0997
forward train acc:  0.99826  and loss:  2.345280807930976
test acc:  0.915
forward train acc:  0.99912  and loss:  1.1916636490495875
test acc:  0.9168
forward train acc:  0.99916  and loss:  1.1740474937250838
test acc:  0.9162
forward train acc:  0.99914  and loss:  1.2180657546268776
test acc:  0.9162
forward train acc:  0.99916  and loss:  1.170155426603742
test acc:  0.9161
forward train acc:  0.99938  and loss:  0.9437056061287876
test acc:  0.9159
forward train acc:  0.9995  and loss:  0.7875075125484727
test acc:  0.9163
forward train acc:  0.99936  and loss:  1.0370128184440546
test acc:  0.9165
forward train acc:  0.99958  and loss:  0.6962532374018338
test acc:  0.9152
forward train acc:  0.9997  and loss:  0.5687166586576495
test acc:  0.9168
********** reverse layer  7  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  8
[4, 3, 3, 4, 1, 2, 4, 5, 0, 3, 3, 3, 4, 3]
***** skip layer  9
[4, 3, 3, 4, 1, 2, 4, 5, 0, 2, 3, 3, 4, 3]
***** skip layer  10
[4, 3, 3, 4, 1, 2, 4, 5, 0, 2, 2, 3, 4, 3]
***** skip layer  11
[4, 3, 3, 4, 1, 2, 4, 5, 0, 2, 2, 2, 4, 3]
***** skip layer  12
[4, 3, 3, 4, 1, 2, 4, 5, 0, 2, 2, 2, 3, 3]
***** skip layer  13
[4, 3, 3, 4, 1, 2, 4, 5, 0, 2, 2, 2, 3, 2]
***** skip layer  0
[3, 3, 3, 4, 1, 2, 4, 5, 0, 2, 2, 2, 3, 2]
***** skip layer  1
[3, 2, 3, 4, 1, 2, 4, 5, 0, 2, 2, 2, 3, 2]
***** skip layer  2
[3, 2, 2, 4, 1, 2, 4, 5, 0, 2, 2, 2, 3, 2]
***** skip layer  3
[3, 2, 2, 3, 1, 2, 4, 5, 0, 2, 2, 2, 3, 2]
***** skip layer  4
[3, 2, 2, 3, 0, 2, 4, 5, 0, 2, 2, 2, 3, 2]
***** skip layer  5
[3, 2, 2, 3, 0, 1, 4, 5, 0, 2, 2, 2, 3, 2]
***** skip layer  6
[3, 2, 2, 3, 0, 1, 3, 5, 0, 2, 2, 2, 3, 2]
***** skip layer  7
[3, 2, 2, 3, 0, 1, 3, 4, 0, 2, 2, 2, 3, 2]
optimize layer  8
backward train epoch:  59
test acc:  0.0972
forward train acc:  0.99774  and loss:  3.017052154173143
test acc:  0.9156
forward train acc:  0.99894  and loss:  1.6188574752304703
test acc:  0.9147
forward train acc:  0.9989  and loss:  1.816916883108206
test acc:  0.9159
forward train acc:  0.99912  and loss:  1.3340488484827802
test acc:  0.9165
forward train acc:  0.99922  and loss:  1.1835499689914286
test acc:  0.9168
forward train acc:  0.99898  and loss:  1.4819136656587943
test acc:  0.9171
forward train acc:  0.99914  and loss:  1.2782675952184945
test acc:  0.9162
forward train acc:  0.99952  and loss:  0.9236215380369686
test acc:  0.9162
forward train acc:  0.99922  and loss:  1.09447358790203
test acc:  0.9159
forward train acc:  0.99934  and loss:  1.127988570719026
test acc:  0.9166
********** reverse layer  8  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  9
[3, 2, 2, 3, 0, 1, 3, 4, 5, 1, 2, 2, 3, 2]
***** skip layer  10
[3, 2, 2, 3, 0, 1, 3, 4, 5, 1, 1, 2, 3, 2]
***** skip layer  11
[3, 2, 2, 3, 0, 1, 3, 4, 5, 1, 1, 1, 3, 2]
***** skip layer  12
[3, 2, 2, 3, 0, 1, 3, 4, 5, 1, 1, 1, 2, 2]
***** skip layer  13
[3, 2, 2, 3, 0, 1, 3, 4, 5, 1, 1, 1, 2, 1]
***** skip layer  0
[2, 2, 2, 3, 0, 1, 3, 4, 5, 1, 1, 1, 2, 1]
***** skip layer  1
[2, 1, 2, 3, 0, 1, 3, 4, 5, 1, 1, 1, 2, 1]
***** skip layer  2
[2, 1, 1, 3, 0, 1, 3, 4, 5, 1, 1, 1, 2, 1]
***** skip layer  3
[2, 1, 1, 2, 0, 1, 3, 4, 5, 1, 1, 1, 2, 1]
optimize layer  4
backward train epoch:  49
test acc:  0.0943
forward train acc:  0.9989  and loss:  1.805794467072701
test acc:  0.9144
forward train acc:  0.99894  and loss:  1.7258831469807774
test acc:  0.9161
forward train acc:  0.99928  and loss:  1.0105041161878034
test acc:  0.9157
forward train acc:  0.99892  and loss:  1.5915824851836078
test acc:  0.9161
forward train acc:  0.9994  and loss:  0.9419757733703591
test acc:  0.9156
forward train acc:  0.9993  and loss:  1.01538199026254
test acc:  0.9164
forward train acc:  0.9995  and loss:  0.8424305068620015
test acc:  0.9178
forward train acc:  0.99956  and loss:  0.7585173236730043
test acc:  0.9182
forward train acc:  0.99948  and loss:  0.7867199815518688
test acc:  0.9171
forward train acc:  0.99948  and loss:  0.9539434410398826
test acc:  0.9174
********** reverse layer  4  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  5
[2, 1, 1, 2, 5, 0, 3, 4, 5, 1, 1, 1, 2, 1]
***** skip layer  6
[2, 1, 1, 2, 5, 0, 2, 4, 5, 1, 1, 1, 2, 1]
***** skip layer  7
[2, 1, 1, 2, 5, 0, 2, 3, 5, 1, 1, 1, 2, 1]
***** skip layer  8
[2, 1, 1, 2, 5, 0, 2, 3, 4, 1, 1, 1, 2, 1]
***** skip layer  9
[2, 1, 1, 2, 5, 0, 2, 3, 4, 0, 1, 1, 2, 1]
***** skip layer  10
[2, 1, 1, 2, 5, 0, 2, 3, 4, 0, 0, 1, 2, 1]
***** skip layer  11
[2, 1, 1, 2, 5, 0, 2, 3, 4, 0, 0, 0, 2, 1]
***** skip layer  12
[2, 1, 1, 2, 5, 0, 2, 3, 4, 0, 0, 0, 1, 1]
***** skip layer  13
[2, 1, 1, 2, 5, 0, 2, 3, 4, 0, 0, 0, 1, 0]
***** skip layer  0
[1, 1, 1, 2, 5, 0, 2, 3, 4, 0, 0, 0, 1, 0]
***** skip layer  1
[1, 0, 1, 2, 5, 0, 2, 3, 4, 0, 0, 0, 1, 0]
***** skip layer  2
[1, 0, 0, 2, 5, 0, 2, 3, 4, 0, 0, 0, 1, 0]
***** skip layer  3
[1, 0, 0, 1, 5, 0, 2, 3, 4, 0, 0, 0, 1, 0]
***** skip layer  4
[1, 0, 0, 1, 4, 0, 2, 3, 4, 0, 0, 0, 1, 0]
optimize layer  5
backward train epoch:  10
test acc:  0.3361
forward train acc:  0.9991  and loss:  1.2552502223988995
test acc:  0.9183
forward train acc:  0.99932  and loss:  0.9691013715346344
test acc:  0.9168
forward train acc:  0.99914  and loss:  1.1401791980024427
test acc:  0.9158
forward train acc:  0.99952  and loss:  0.7105109976837412
test acc:  0.9176
forward train acc:  0.9994  and loss:  0.8863903630699497
test acc:  0.918
forward train acc:  0.99936  and loss:  0.7953587713709567
test acc:  0.9187
forward train acc:  0.99938  and loss:  0.9911994875001255
test acc:  0.9184
forward train acc:  0.9997  and loss:  0.5322264707938302
test acc:  0.9179
forward train acc:  0.99966  and loss:  0.48065137350931764
test acc:  0.9186
forward train acc:  0.99946  and loss:  0.7338395842234604
test acc:  0.9186
********** reverse layer  5  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  6
[1, 0, 0, 1, 4, 5, 1, 3, 4, 0, 0, 0, 1, 0]
***** skip layer  7
[1, 0, 0, 1, 4, 5, 1, 2, 4, 0, 0, 0, 1, 0]
***** skip layer  8
[1, 0, 0, 1, 4, 5, 1, 2, 3, 0, 0, 0, 1, 0]
optimize layer  9
backward train epoch:  89
test acc:  0.1001
forward train acc:  0.99944  and loss:  0.7757140531903133
test acc:  0.9168
forward train acc:  0.99962  and loss:  0.5558347178739496
test acc:  0.9181
forward train acc:  0.99962  and loss:  0.5464415576716419
test acc:  0.9176
forward train acc:  0.9996  and loss:  0.646266600524541
test acc:  0.918
forward train acc:  0.9997  and loss:  0.4969353750057053
test acc:  0.9185
forward train acc:  0.99974  and loss:  0.5115391182189342
test acc:  0.9177
forward train acc:  0.99982  and loss:  0.3436619591084309
test acc:  0.9173
forward train acc:  0.9997  and loss:  0.5215208684385289
test acc:  0.9178
forward train acc:  0.99958  and loss:  0.546876916923793
test acc:  0.918
forward train acc:  0.99968  and loss:  0.4560446582036093
test acc:  0.9177
********** reverse layer  9  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.796875  ==>  156 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
optimize layer  10
backward train epoch:  174
test acc:  0.1001
forward train acc:  0.99946  and loss:  0.8565874620107934
test acc:  0.9181
forward train acc:  0.99958  and loss:  0.5587812563753687
test acc:  0.9175
forward train acc:  0.99954  and loss:  0.669777821371099
test acc:  0.9175
forward train acc:  0.99954  and loss:  0.7164764330664184
test acc:  0.9185
forward train acc:  0.99968  and loss:  0.47663017714512534
test acc:  0.9176
forward train acc:  0.99972  and loss:  0.6299952534027398
test acc:  0.9191
forward train acc:  0.99976  and loss:  0.40076292981393635
test acc:  0.9187
forward train acc:  0.99974  and loss:  0.49634666918427683
test acc:  0.9198
forward train acc:  0.99976  and loss:  0.4295391777995974
test acc:  0.9196
forward train acc:  0.99976  and loss:  0.4228259010997135
test acc:  0.9207
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.8046875  ==>  150 / 768
layer  11  :  0.8385416666666666  ==>  124 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
optimize layer  11
backward train epoch:  368
test acc:  0.0998
forward train acc:  0.99966  and loss:  0.5157459530164488
test acc:  0.9205
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.8046875  ==>  150 / 768
layer  11  :  0.8463541666666666  ==>  118 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.6927083333333334  ==>  236 / 768
***** skip layer  12
[1, 0, 0, 1, 4, 5, 1, 2, 3, 5, 0, 0, 0, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.1
forward train acc:  0.9993  and loss:  2.5744291432201862
test acc:  0.9199
forward train acc:  0.99944  and loss:  1.9162958574015647
test acc:  0.9193
forward train acc:  0.99956  and loss:  1.4046636105049402
test acc:  0.9191
forward train acc:  0.99962  and loss:  1.3021127030951902
test acc:  0.9188
forward train acc:  0.9998  and loss:  1.1540196553105488
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.8046875  ==>  150 / 768
layer  11  :  0.8463541666666666  ==>  118 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  0
[0, 0, 0, 1, 4, 5, 1, 2, 3, 5, 0, 0, 0, 0]
optimize layer  1
backward train epoch:  61
test acc:  0.0928
forward train acc:  0.99906  and loss:  2.4377782092196867
test acc:  0.9153
forward train acc:  0.99882  and loss:  2.3785372630227357
test acc:  0.9167
forward train acc:  0.99904  and loss:  1.9581393552944064
test acc:  0.9138
forward train acc:  0.99928  and loss:  1.4781441313680261
test acc:  0.9163
forward train acc:  0.99924  and loss:  1.612400921760127
test acc:  0.9167
forward train acc:  0.99938  and loss:  1.4694119304185733
test acc:  0.9161
forward train acc:  0.99952  and loss:  1.1983162940014154
test acc:  0.9167
forward train acc:  0.99972  and loss:  0.8895995029015467
test acc:  0.9151
forward train acc:  0.99964  and loss:  0.9872602646937594
test acc:  0.9168
forward train acc:  0.9996  and loss:  0.8896895260550082
test acc:  0.9164
********** reverse layer  1  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5520833333333334  ==>  86 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.8046875  ==>  150 / 768
layer  11  :  0.8463541666666666  ==>  118 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
optimize layer  2
backward train epoch:  32
test acc:  0.1068
forward train acc:  0.99936  and loss:  1.576446566497907
test acc:  0.919
forward train acc:  0.99938  and loss:  1.3200157359824516
test acc:  0.918
forward train acc:  0.9993  and loss:  1.364539211790543
test acc:  0.918
forward train acc:  0.9994  and loss:  1.1342568330001086
test acc:  0.9193
forward train acc:  0.99944  and loss:  1.109486872504931
test acc:  0.9195
forward train acc:  0.99956  and loss:  1.1156390408286825
test acc:  0.9192
forward train acc:  0.99962  and loss:  0.8008142312755808
test acc:  0.9185
forward train acc:  0.99942  and loss:  1.1810524970060214
test acc:  0.9189
forward train acc:  0.99964  and loss:  0.9860889778938144
test acc:  0.9193
forward train acc:  0.99982  and loss:  0.611417553271167
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.8046875  ==>  150 / 768
layer  11  :  0.8463541666666666  ==>  118 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  3
[0, 5, 0, 0, 4, 5, 1, 2, 3, 5, 0, 0, 0, 0]
***** skip layer  4
[0, 5, 0, 0, 3, 5, 1, 2, 3, 5, 0, 0, 0, 0]
***** skip layer  5
[0, 5, 0, 0, 3, 4, 1, 2, 3, 5, 0, 0, 0, 0]
***** skip layer  6
[0, 5, 0, 0, 3, 4, 0, 2, 3, 5, 0, 0, 0, 0]
***** skip layer  7
[0, 5, 0, 0, 3, 4, 0, 1, 3, 5, 0, 0, 0, 0]
***** skip layer  8
[0, 5, 0, 0, 3, 4, 0, 1, 2, 5, 0, 0, 0, 0]
***** skip layer  9
[0, 5, 0, 0, 3, 4, 0, 1, 2, 4, 0, 0, 0, 0]
optimize layer  10
backward train epoch:  238
test acc:  0.0892
forward train acc:  0.99934  and loss:  1.2026940297801048
test acc:  0.9161
forward train acc:  0.99954  and loss:  1.0595974371535704
test acc:  0.9182
forward train acc:  0.99928  and loss:  1.2628958422574215
test acc:  0.9206
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.8125  ==>  144 / 768
layer  11  :  0.8463541666666666  ==>  118 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
optimize layer  11
backward train epoch:  162
test acc:  0.0916
forward train acc:  0.9995  and loss:  1.2178333056508563
test acc:  0.9188
forward train acc:  0.99952  and loss:  1.0063014848274179
test acc:  0.9179
forward train acc:  0.99936  and loss:  1.316067517560441
test acc:  0.9162
forward train acc:  0.99958  and loss:  0.8715248713851906
test acc:  0.9194
forward train acc:  0.99954  and loss:  0.7370077849482186
test acc:  0.9189
forward train acc:  0.9996  and loss:  0.666491533163935
test acc:  0.918
forward train acc:  0.9997  and loss:  0.5822177114896476
test acc:  0.9202
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.8125  ==>  144 / 768
layer  11  :  0.8541666666666666  ==>  112 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
optimize layer  12
backward train epoch:  130
test acc:  0.1
forward train acc:  0.1  and loss:  1175.3369653224945
test acc:  0.1
forward train acc:  0.1  and loss:  1148.3183636665344
test acc:  0.1
forward train acc:  0.1  and loss:  1124.061196088791
test acc:  0.1
forward train acc:  0.1  and loss:  1106.7921342849731
test acc:  0.1
forward train acc:  0.1  and loss:  1095.9301126003265
test acc:  0.1
forward train acc:  0.1  and loss:  1085.4091143608093
test acc:  0.1
forward train acc:  0.1  and loss:  1075.2607553005219
test acc:  0.1
forward train acc:  0.1  and loss:  1067.9132630825043
test acc:  0.1
forward train acc:  0.1  and loss:  1062.9748067855835
test acc:  0.1
forward train acc:  0.1  and loss:  1058.1932563781738
test acc:  0.1
********** reverse layer  12  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.8125  ==>  144 / 768
layer  11  :  0.8541666666666666  ==>  112 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
optimize layer  13
backward train epoch:  102
test acc:  0.097
forward train acc:  0.88846  and loss:  146.60150228440762
test acc:  0.8202
forward train acc:  0.8992  and loss:  125.71792694181204
test acc:  0.8197
forward train acc:  0.8991  and loss:  120.46040314435959
test acc:  0.8205
forward train acc:  0.89936  and loss:  115.88924264907837
test acc:  0.8215
forward train acc:  0.89936  and loss:  113.16179035604
test acc:  0.8209
forward train acc:  0.89948  and loss:  110.70821595191956
test acc:  0.8216
forward train acc:  0.89956  and loss:  108.12917394191027
test acc:  0.822
forward train acc:  0.89946  and loss:  106.41708665341139
test acc:  0.8223
forward train acc:  0.89942  and loss:  105.01800612360239
test acc:  0.8223
forward train acc:  0.89958  and loss:  103.7362313196063
test acc:  0.824
********** reverse layer  13  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.8125  ==>  144 / 768
layer  11  :  0.8541666666666666  ==>  112 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
optimize layer  0
backward train epoch:  173
test acc:  0.104
forward train acc:  0.76008  and loss:  409.6002290248871
test acc:  0.7968
forward train acc:  0.86584  and loss:  179.96106979250908
test acc:  0.8305
forward train acc:  0.90332  and loss:  129.64577867090702
test acc:  0.8509
forward train acc:  0.91712  and loss:  110.82053317129612
test acc:  0.8531
forward train acc:  0.92744  and loss:  96.86396634578705
test acc:  0.859
forward train acc:  0.93394  and loss:  87.2470356784761
test acc:  0.863
forward train acc:  0.93874  and loss:  80.58019159734249
test acc:  0.8661
forward train acc:  0.9451  and loss:  72.71232330799103
test acc:  0.8695
forward train acc:  0.94694  and loss:  69.93615006655455
test acc:  0.8705
forward train acc:  0.9489  and loss:  66.96447275951505
test acc:  0.8726
********** reverse layer  0  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.8125  ==>  144 / 768
layer  11  :  0.8541666666666666  ==>  112 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  1
[5, 4, 0, 0, 3, 4, 0, 1, 2, 4, 0, 0, 5, 5]
optimize layer  2
backward train epoch:  417
test acc:  0.1067
forward train acc:  0.9169  and loss:  109.03157916665077
test acc:  0.8759
forward train acc:  0.95202  and loss:  63.434500344097614
test acc:  0.8845
forward train acc:  0.9639  and loss:  47.670690678060055
test acc:  0.8898
forward train acc:  0.97092  and loss:  38.48090163618326
test acc:  0.8918
forward train acc:  0.97336  and loss:  34.45335627067834
test acc:  0.896
forward train acc:  0.97728  and loss:  30.24363280273974
test acc:  0.8965
forward train acc:  0.97886  and loss:  27.91989124007523
test acc:  0.8974
forward train acc:  0.98074  and loss:  26.107363050803542
test acc:  0.8972
forward train acc:  0.98186  and loss:  24.19257901236415
test acc:  0.8975
forward train acc:  0.98106  and loss:  25.032016208395362
test acc:  0.8983
********** reverse layer  2  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.8125  ==>  144 / 768
layer  11  :  0.8541666666666666  ==>  112 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
optimize layer  3
backward train epoch:  2
test acc:  0.096
forward train acc:  0.99794  and loss:  3.5378440351923928
test acc:  0.9163
forward train acc:  0.99852  and loss:  2.300481636833865
test acc:  0.9184
forward train acc:  0.99896  and loss:  2.061023447138723
test acc:  0.9172
forward train acc:  0.99912  and loss:  1.5514085764880292
test acc:  0.9188
forward train acc:  0.99914  and loss:  1.4369663423276506
test acc:  0.9181
forward train acc:  0.99908  and loss:  1.5506032807170413
test acc:  0.9195
forward train acc:  0.99928  and loss:  1.365429210534785
test acc:  0.9189
forward train acc:  0.99912  and loss:  1.3003283294965513
test acc:  0.9191
forward train acc:  0.99914  and loss:  1.3481728241895325
test acc:  0.9195
forward train acc:  0.99924  and loss:  1.327684995776508
test acc:  0.919
********** reverse layer  3  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6458333333333334  ==>  136 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.8125  ==>  144 / 768
layer  11  :  0.8541666666666666  ==>  112 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  4
[5, 4, 5, 5, 2, 4, 0, 1, 2, 4, 0, 0, 5, 5]
***** skip layer  5
[5, 4, 5, 5, 2, 3, 0, 1, 2, 4, 0, 0, 5, 5]
optimize layer  6
backward train epoch:  108
test acc:  0.0993
forward train acc:  0.99932  and loss:  1.0069129671901464
test acc:  0.921
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.65625  ==>  132 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.8125  ==>  144 / 768
layer  11  :  0.8541666666666666  ==>  112 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  7
[5, 4, 5, 5, 2, 3, 0, 0, 2, 4, 0, 0, 5, 5]
***** skip layer  8
[5, 4, 5, 5, 2, 3, 0, 0, 1, 4, 0, 0, 5, 5]
***** skip layer  9
[5, 4, 5, 5, 2, 3, 0, 0, 1, 3, 0, 0, 5, 5]
optimize layer  10
backward train epoch:  156
test acc:  0.103
forward train acc:  0.99924  and loss:  1.2518192994175479
test acc:  0.9194
forward train acc:  0.99934  and loss:  1.0293479955289513
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.65625  ==>  132 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.8203125  ==>  138 / 768
layer  11  :  0.8541666666666666  ==>  112 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
optimize layer  11
backward train epoch:  608
test acc:  0.1149
forward train acc:  0.99926  and loss:  1.1666406076401472
test acc:  0.9224
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.65625  ==>  132 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.8203125  ==>  138 / 768
layer  11  :  0.8619791666666666  ==>  106 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  12
[5, 4, 5, 5, 2, 3, 0, 0, 1, 3, 0, 0, 4, 5]
***** skip layer  13
[5, 4, 5, 5, 2, 3, 0, 0, 1, 3, 0, 0, 4, 4]
***** skip layer  0
[4, 4, 5, 5, 2, 3, 0, 0, 1, 3, 0, 0, 4, 4]
***** skip layer  1
[4, 3, 5, 5, 2, 3, 0, 0, 1, 3, 0, 0, 4, 4]
***** skip layer  2
[4, 3, 4, 5, 2, 3, 0, 0, 1, 3, 0, 0, 4, 4]
***** skip layer  3
[4, 3, 4, 4, 2, 3, 0, 0, 1, 3, 0, 0, 4, 4]
***** skip layer  4
[4, 3, 4, 4, 1, 3, 0, 0, 1, 3, 0, 0, 4, 4]
***** skip layer  5
[4, 3, 4, 4, 1, 2, 0, 0, 1, 3, 0, 0, 4, 4]
optimize layer  6
backward train epoch:  35
test acc:  0.1072
forward train acc:  0.9992  and loss:  1.3397803381667472
test acc:  0.9184
forward train acc:  0.99924  and loss:  1.2323227017186582
test acc:  0.9193
forward train acc:  0.99888  and loss:  1.5466619995422661
test acc:  0.9176
forward train acc:  0.99932  and loss:  1.0307063139043748
test acc:  0.9193
forward train acc:  0.99934  and loss:  0.9913080653641373
test acc:  0.9184
forward train acc:  0.9994  and loss:  0.923911020392552
test acc:  0.9196
forward train acc:  0.9994  and loss:  0.9422790565295145
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9479166666666666  ==>  40 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.8203125  ==>  138 / 768
layer  11  :  0.8619791666666666  ==>  106 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
optimize layer  7
backward train epoch:  44
test acc:  0.0997
forward train acc:  0.99848  and loss:  2.1125319052371196
test acc:  0.9158
forward train acc:  0.9987  and loss:  1.8524802866159007
test acc:  0.9163
forward train acc:  0.99862  and loss:  2.3566969379317015
test acc:  0.9172
forward train acc:  0.999  and loss:  1.5313438817393035
test acc:  0.9193
forward train acc:  0.9992  and loss:  1.4557758747250773
test acc:  0.9184
forward train acc:  0.99904  and loss:  1.4394328625057824
test acc:  0.9196
forward train acc:  0.99942  and loss:  1.0176628261106089
test acc:  0.9195
forward train acc:  0.99928  and loss:  1.001878616050817
test acc:  0.9194
forward train acc:  0.9994  and loss:  0.9017538854095619
test acc:  0.9198
forward train acc:  0.9995  and loss:  0.8630921260337345
test acc:  0.9202
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.8203125  ==>  138 / 768
layer  11  :  0.8619791666666666  ==>  106 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  8
[4, 3, 4, 4, 1, 2, 0, 0, 0, 3, 0, 0, 4, 4]
***** skip layer  9
[4, 3, 4, 4, 1, 2, 0, 0, 0, 2, 0, 0, 4, 4]
optimize layer  10
backward train epoch:  159
test acc:  0.1
forward train acc:  0.99904  and loss:  1.3824682220001705
test acc:  0.9192
forward train acc:  0.99908  and loss:  1.439875496784225
test acc:  0.9176
forward train acc:  0.99888  and loss:  1.6362949265749194
test acc:  0.9176
forward train acc:  0.99916  and loss:  1.2156413641641848
test acc:  0.9189
forward train acc:  0.99928  and loss:  1.084574474254623
test acc:  0.9195
forward train acc:  0.99928  and loss:  0.9283075173734687
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8619791666666666  ==>  106 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
optimize layer  11
backward train epoch:  835
test acc:  0.0999
forward train acc:  0.99908  and loss:  1.3082239602226764
test acc:  0.9191
forward train acc:  0.99922  and loss:  1.1931154670310207
test acc:  0.9187
forward train acc:  0.9992  and loss:  1.2959306478442159
test acc:  0.9176
forward train acc:  0.99954  and loss:  0.7499035687942524
test acc:  0.9194
forward train acc:  0.99918  and loss:  1.1562897273979615
test acc:  0.9184
forward train acc:  0.99962  and loss:  0.7750231554673519
test acc:  0.9176
forward train acc:  0.9993  and loss:  1.0074928765534423
test acc:  0.9191
forward train acc:  0.99944  and loss:  0.7926855214172974
test acc:  0.9196
forward train acc:  0.9995  and loss:  0.7128356852044817
test acc:  0.9184
forward train acc:  0.99938  and loss:  1.0751248484593816
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8697916666666666  ==>  100 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  12
[4, 3, 4, 4, 1, 2, 0, 0, 0, 2, 0, 0, 3, 4]
***** skip layer  13
[4, 3, 4, 4, 1, 2, 0, 0, 0, 2, 0, 0, 3, 3]
***** skip layer  0
[3, 3, 4, 4, 1, 2, 0, 0, 0, 2, 0, 0, 3, 3]
***** skip layer  1
[3, 2, 4, 4, 1, 2, 0, 0, 0, 2, 0, 0, 3, 3]
***** skip layer  2
[3, 2, 3, 4, 1, 2, 0, 0, 0, 2, 0, 0, 3, 3]
***** skip layer  3
[3, 2, 3, 3, 1, 2, 0, 0, 0, 2, 0, 0, 3, 3]
***** skip layer  4
[3, 2, 3, 3, 0, 2, 0, 0, 0, 2, 0, 0, 3, 3]
***** skip layer  5
[3, 2, 3, 3, 0, 1, 0, 0, 0, 2, 0, 0, 3, 3]
optimize layer  6
backward train epoch:  284
test acc:  0.1
forward train acc:  0.9994  and loss:  0.8174706854042597
test acc:  0.9152
forward train acc:  0.99884  and loss:  1.4633955726458225
test acc:  0.9168
forward train acc:  0.99922  and loss:  1.2097990271868184
test acc:  0.9164
forward train acc:  0.99938  and loss:  1.0980039565474726
test acc:  0.9171
forward train acc:  0.99946  and loss:  0.9062829414033331
test acc:  0.9181
forward train acc:  0.99934  and loss:  0.8182876489008777
test acc:  0.917
forward train acc:  0.99916  and loss:  1.1136042134894524
test acc:  0.9185
forward train acc:  0.99936  and loss:  0.8424010694725439
test acc:  0.9194
forward train acc:  0.99944  and loss:  0.7845672100083902
test acc:  0.9193
forward train acc:  0.99952  and loss:  0.6179875006200746
test acc:  0.9188
********** reverse layer  6  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8697916666666666  ==>  100 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
optimize layer  7
backward train epoch:  177
test acc:  0.1
forward train acc:  0.99652  and loss:  4.836641474859789
test acc:  0.912
forward train acc:  0.99762  and loss:  3.912784113897942
test acc:  0.9157
forward train acc:  0.99802  and loss:  2.7640416369831655
test acc:  0.9145
forward train acc:  0.9982  and loss:  2.4255850311601534
test acc:  0.9164
forward train acc:  0.99852  and loss:  2.046525444253348
test acc:  0.9162
forward train acc:  0.99858  and loss:  1.9711387861170806
test acc:  0.9155
forward train acc:  0.9987  and loss:  1.7163209779828321
test acc:  0.9176
forward train acc:  0.99872  and loss:  1.9133115453005303
test acc:  0.918
forward train acc:  0.99884  and loss:  1.5856555270729586
test acc:  0.917
forward train acc:  0.99906  and loss:  1.4220635005040094
test acc:  0.916
********** reverse layer  7  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8697916666666666  ==>  100 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
optimize layer  8
backward train epoch:  1086
test acc:  0.101
forward train acc:  0.98686  and loss:  17.308621223201044
test acc:  0.9128
forward train acc:  0.99656  and loss:  4.850720327405725
test acc:  0.9153
forward train acc:  0.9974  and loss:  3.56924430601066
test acc:  0.9145
forward train acc:  0.9978  and loss:  2.9373382512130775
test acc:  0.9164
forward train acc:  0.99836  and loss:  2.4467985669034533
test acc:  0.9164
forward train acc:  0.99822  and loss:  2.387516111251898
test acc:  0.9182
forward train acc:  0.9983  and loss:  2.2803737398353405
test acc:  0.9177
forward train acc:  0.9984  and loss:  2.1170596044394188
test acc:  0.9171
forward train acc:  0.99872  and loss:  1.8354992553358898
test acc:  0.9189
forward train acc:  0.99864  and loss:  2.078246747609228
test acc:  0.9172
********** reverse layer  8  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8697916666666666  ==>  100 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  9
[3, 2, 3, 3, 0, 1, 5, 5, 5, 1, 0, 0, 3, 3]
optimize layer  10
backward train epoch:  405
test acc:  0.0962
forward train acc:  0.99942  and loss:  0.8616110753791872
test acc:  0.9189
forward train acc:  0.99946  and loss:  0.7407315521850251
test acc:  0.917
forward train acc:  0.99918  and loss:  1.1748725126089994
test acc:  0.9178
forward train acc:  0.9996  and loss:  0.692777929478325
test acc:  0.9181
forward train acc:  0.99966  and loss:  0.5172744139854331
test acc:  0.9189
forward train acc:  0.99958  and loss:  0.6110603424604051
test acc:  0.9187
forward train acc:  0.99962  and loss:  0.6158336442313157
test acc:  0.9176
forward train acc:  0.9996  and loss:  0.6234469916671515
test acc:  0.9182
forward train acc:  0.99978  and loss:  0.36393946924363263
test acc:  0.9184
forward train acc:  0.9997  and loss:  0.46337024818058126
test acc:  0.9185
********** reverse layer  10  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8697916666666666  ==>  100 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
optimize layer  11
backward train epoch:  298
test acc:  0.1
forward train acc:  0.99956  and loss:  0.7375170876912307
test acc:  0.9186
forward train acc:  0.99912  and loss:  1.2817612120998092
test acc:  0.9194
forward train acc:  0.99958  and loss:  0.625243740825681
test acc:  0.9177
forward train acc:  0.99962  and loss:  0.6573349598911591
test acc:  0.9171
forward train acc:  0.99958  and loss:  0.6071473776828498
test acc:  0.9186
forward train acc:  0.99962  and loss:  0.6467478507838678
test acc:  0.9177
forward train acc:  0.99958  and loss:  0.7730061945912894
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8776041666666666  ==>  94 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  12
[3, 2, 3, 3, 0, 1, 5, 5, 5, 1, 5, 0, 2, 3]
***** skip layer  13
[3, 2, 3, 3, 0, 1, 5, 5, 5, 1, 5, 0, 2, 2]
***** skip layer  0
[2, 2, 3, 3, 0, 1, 5, 5, 5, 1, 5, 0, 2, 2]
***** skip layer  1
[2, 1, 3, 3, 0, 1, 5, 5, 5, 1, 5, 0, 2, 2]
***** skip layer  2
[2, 1, 2, 3, 0, 1, 5, 5, 5, 1, 5, 0, 2, 2]
***** skip layer  3
[2, 1, 2, 2, 0, 1, 5, 5, 5, 1, 5, 0, 2, 2]
optimize layer  4
backward train epoch:  141
test acc:  0.1
forward train acc:  0.99862  and loss:  2.0869520671549253
test acc:  0.9177
forward train acc:  0.99876  and loss:  1.8965150325093418
test acc:  0.9167
forward train acc:  0.99898  and loss:  1.408839994459413
test acc:  0.9184
forward train acc:  0.99912  and loss:  1.3172376720176544
test acc:  0.9199
forward train acc:  0.99904  and loss:  1.1767413111811038
test acc:  0.9188
forward train acc:  0.99956  and loss:  0.7845398151839618
test acc:  0.9193
forward train acc:  0.99954  and loss:  0.7346502557338681
test acc:  0.9176
forward train acc:  0.9995  and loss:  0.6750718108232832
test acc:  0.9193
forward train acc:  0.99956  and loss:  0.614761545235524
test acc:  0.9182
forward train acc:  0.99944  and loss:  0.8109253530274145
test acc:  0.9197
********** reverse layer  4  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8776041666666666  ==>  94 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  5
[2, 1, 2, 2, 5, 0, 5, 5, 5, 1, 5, 0, 2, 2]
***** skip layer  6
[2, 1, 2, 2, 5, 0, 4, 5, 5, 1, 5, 0, 2, 2]
***** skip layer  7
[2, 1, 2, 2, 5, 0, 4, 4, 5, 1, 5, 0, 2, 2]
***** skip layer  8
[2, 1, 2, 2, 5, 0, 4, 4, 4, 1, 5, 0, 2, 2]
***** skip layer  9
[2, 1, 2, 2, 5, 0, 4, 4, 4, 0, 5, 0, 2, 2]
***** skip layer  10
[2, 1, 2, 2, 5, 0, 4, 4, 4, 0, 4, 0, 2, 2]
optimize layer  11
backward train epoch:  181
test acc:  0.0912
forward train acc:  0.99962  and loss:  0.7151428273064084
test acc:  0.9175
forward train acc:  0.99956  and loss:  0.7688542212417815
test acc:  0.9175
forward train acc:  0.9995  and loss:  0.842892969463719
test acc:  0.9157
forward train acc:  0.9994  and loss:  0.938501790544251
test acc:  0.9177
forward train acc:  0.99954  and loss:  0.6971711706137285
test acc:  0.9179
forward train acc:  0.9996  and loss:  0.6125779653375503
test acc:  0.9187
forward train acc:  0.9994  and loss:  0.8667488791397773
test acc:  0.918
forward train acc:  0.99966  and loss:  0.6058490034774877
test acc:  0.9174
forward train acc:  0.99972  and loss:  0.4223208329640329
test acc:  0.9196
forward train acc:  0.99972  and loss:  0.4644933236995712
test acc:  0.9191
********** reverse layer  11  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8776041666666666  ==>  94 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  12
[2, 1, 2, 2, 5, 0, 4, 4, 4, 0, 4, 5, 1, 2]
***** skip layer  13
[2, 1, 2, 2, 5, 0, 4, 4, 4, 0, 4, 5, 1, 1]
***** skip layer  0
[1, 1, 2, 2, 5, 0, 4, 4, 4, 0, 4, 5, 1, 1]
***** skip layer  1
[1, 0, 2, 2, 5, 0, 4, 4, 4, 0, 4, 5, 1, 1]
***** skip layer  2
[1, 0, 1, 2, 5, 0, 4, 4, 4, 0, 4, 5, 1, 1]
***** skip layer  3
[1, 0, 1, 1, 5, 0, 4, 4, 4, 0, 4, 5, 1, 1]
***** skip layer  4
[1, 0, 1, 1, 4, 0, 4, 4, 4, 0, 4, 5, 1, 1]
optimize layer  5
backward train epoch:  123
test acc:  0.0999
forward train acc:  0.99896  and loss:  1.6035293190798257
test acc:  0.9179
forward train acc:  0.9992  and loss:  1.0005279919423629
test acc:  0.9171
forward train acc:  0.9992  and loss:  0.9927461085608229
test acc:  0.9163
forward train acc:  0.99944  and loss:  0.7929628811543807
test acc:  0.9178
forward train acc:  0.99946  and loss:  0.7995207847416168
test acc:  0.9172
forward train acc:  0.99946  and loss:  0.7435887764440849
test acc:  0.9165
forward train acc:  0.99954  and loss:  0.7288162034674315
test acc:  0.9185
forward train acc:  0.99962  and loss:  0.5597431523347041
test acc:  0.9175
forward train acc:  0.99974  and loss:  0.56444993246987
test acc:  0.9167
forward train acc:  0.99952  and loss:  0.755602987628663
test acc:  0.9186
********** reverse layer  5  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.75  ==>  192 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8776041666666666  ==>  94 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  6
[1, 0, 1, 1, 4, 5, 3, 4, 4, 0, 4, 5, 1, 1]
***** skip layer  7
[1, 0, 1, 1, 4, 5, 3, 3, 4, 0, 4, 5, 1, 1]
***** skip layer  8
[1, 0, 1, 1, 4, 5, 3, 3, 3, 0, 4, 5, 1, 1]
optimize layer  9
backward train epoch:  57
test acc:  0.0999
forward train acc:  0.9995  and loss:  0.6955166692496277
test acc:  0.9176
forward train acc:  0.99944  and loss:  0.7501660138659645
test acc:  0.9161
forward train acc:  0.99944  and loss:  0.8315446027700091
test acc:  0.9178
forward train acc:  0.99946  and loss:  0.8029887693410274
test acc:  0.9176
forward train acc:  0.9996  and loss:  0.5901710005709901
test acc:  0.9187
forward train acc:  0.99958  and loss:  0.7125276633450994
test acc:  0.9194
forward train acc:  0.9996  and loss:  0.7220813081803499
test acc:  0.9175
forward train acc:  0.99964  and loss:  0.5146084363368573
test acc:  0.9183
forward train acc:  0.99982  and loss:  0.41440388187766075
test acc:  0.9182
forward train acc:  0.99968  and loss:  0.5004674476222135
test acc:  0.9202
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7578125  ==>  186 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8776041666666666  ==>  94 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  10
[1, 0, 1, 1, 4, 5, 3, 3, 3, 0, 3, 5, 1, 1]
***** skip layer  11
[1, 0, 1, 1, 4, 5, 3, 3, 3, 0, 3, 4, 1, 1]
***** skip layer  12
[1, 0, 1, 1, 4, 5, 3, 3, 3, 0, 3, 4, 0, 1]
***** skip layer  13
[1, 0, 1, 1, 4, 5, 3, 3, 3, 0, 3, 4, 0, 0]
***** skip layer  0
[0, 0, 1, 1, 4, 5, 3, 3, 3, 0, 3, 4, 0, 0]
optimize layer  1
backward train epoch:  251
test acc:  0.1015
forward train acc:  0.99832  and loss:  2.4449022912885994
test acc:  0.9141
forward train acc:  0.99904  and loss:  1.4463767960987752
test acc:  0.9164
forward train acc:  0.99928  and loss:  1.1684964550076984
test acc:  0.9181
forward train acc:  0.99888  and loss:  1.4433170326810796
test acc:  0.9184
forward train acc:  0.9996  and loss:  0.5691815126338042
test acc:  0.9179
forward train acc:  0.99944  and loss:  0.8179325743403751
test acc:  0.9175
forward train acc:  0.9993  and loss:  1.174664177626255
test acc:  0.9175
forward train acc:  0.9994  and loss:  0.8369945523736533
test acc:  0.9179
forward train acc:  0.99942  and loss:  0.7843012751545757
test acc:  0.9187
forward train acc:  0.99954  and loss:  0.7360333847755101
test acc:  0.9189
********** reverse layer  1  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7578125  ==>  186 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8776041666666666  ==>  94 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  2
[0, 5, 0, 1, 4, 5, 3, 3, 3, 0, 3, 4, 0, 0]
***** skip layer  3
[0, 5, 0, 0, 4, 5, 3, 3, 3, 0, 3, 4, 0, 0]
***** skip layer  4
[0, 5, 0, 0, 3, 5, 3, 3, 3, 0, 3, 4, 0, 0]
***** skip layer  5
[0, 5, 0, 0, 3, 4, 3, 3, 3, 0, 3, 4, 0, 0]
***** skip layer  6
[0, 5, 0, 0, 3, 4, 2, 3, 3, 0, 3, 4, 0, 0]
***** skip layer  7
[0, 5, 0, 0, 3, 4, 2, 2, 3, 0, 3, 4, 0, 0]
***** skip layer  8
[0, 5, 0, 0, 3, 4, 2, 2, 2, 0, 3, 4, 0, 0]
optimize layer  9
backward train epoch:  149
test acc:  0.0946
forward train acc:  0.9995  and loss:  0.8551636379415868
test acc:  0.9179
forward train acc:  0.9996  and loss:  0.58585261333792
test acc:  0.9184
forward train acc:  0.99946  and loss:  0.7476682574779261
test acc:  0.9175
forward train acc:  0.9996  and loss:  0.6141202619619435
test acc:  0.9176
forward train acc:  0.99964  and loss:  0.6770651264669141
test acc:  0.9187
forward train acc:  0.99958  and loss:  0.4760314566810848
test acc:  0.9189
forward train acc:  0.99962  and loss:  0.48332791589200497
test acc:  0.9183
forward train acc:  0.99968  and loss:  0.5000764338619774
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.765625  ==>  180 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8776041666666666  ==>  94 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  10
[0, 5, 0, 0, 3, 4, 2, 2, 2, 0, 2, 4, 0, 0]
***** skip layer  11
[0, 5, 0, 0, 3, 4, 2, 2, 2, 0, 2, 3, 0, 0]
optimize layer  12
backward train epoch:  224
test acc:  0.1
forward train acc:  0.1  and loss:  1154.6839973926544
test acc:  0.1
forward train acc:  0.1003  and loss:  1130.8018352985382
test acc:  0.1
forward train acc:  0.1  and loss:  1108.702955007553
test acc:  0.1
forward train acc:  0.1  and loss:  1093.0363273620605
test acc:  0.1
forward train acc:  0.1  and loss:  1083.1212339401245
test acc:  0.1
forward train acc:  0.1  and loss:  1073.5293757915497
test acc:  0.1
forward train acc:  0.1  and loss:  1064.1730515956879
test acc:  0.1
forward train acc:  0.1  and loss:  1057.3204119205475
test acc:  0.1
forward train acc:  0.1  and loss:  1052.8421890735626
test acc:  0.1
forward train acc:  0.1  and loss:  1048.5327098369598
test acc:  0.1
********** reverse layer  12  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.765625  ==>  180 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8776041666666666  ==>  94 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
optimize layer  13
backward train epoch:  102
test acc:  0.0938
forward train acc:  0.88836  and loss:  131.26445165276527
test acc:  0.8294
forward train acc:  0.89858  and loss:  115.58988758921623
test acc:  0.8318
forward train acc:  0.89878  and loss:  108.86622580885887
test acc:  0.8312
forward train acc:  0.8991  and loss:  104.29941762983799
test acc:  0.8343
forward train acc:  0.89936  and loss:  101.00365313887596
test acc:  0.8349
forward train acc:  0.89922  and loss:  98.27946928143501
test acc:  0.8358
forward train acc:  0.89934  and loss:  95.32271355390549
test acc:  0.8356
forward train acc:  0.89952  and loss:  93.06773391366005
test acc:  0.8351
forward train acc:  0.89972  and loss:  91.51003719866276
test acc:  0.8347
forward train acc:  0.89972  and loss:  90.19203040003777
test acc:  0.8367
********** reverse layer  13  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.765625  ==>  180 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8776041666666666  ==>  94 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
optimize layer  0
backward train epoch:  1446
test acc:  0.1
forward train acc:  0.66042  and loss:  586.2632152438164
test acc:  0.7061
forward train acc:  0.74528  and loss:  330.7964969277382
test acc:  0.7489
forward train acc:  0.79062  and loss:  268.29993906617165
test acc:  0.7822
forward train acc:  0.80796  and loss:  240.59022146463394
test acc:  0.7902
forward train acc:  0.82288  and loss:  222.54657119512558
test acc:  0.7977
forward train acc:  0.83322  and loss:  209.25593620538712
test acc:  0.8065
forward train acc:  0.84278  and loss:  196.38945883512497
test acc:  0.8122
forward train acc:  0.85086  and loss:  186.4530358761549
test acc:  0.8149
forward train acc:  0.85692  and loss:  178.39805155992508
test acc:  0.8203
forward train acc:  0.8594  and loss:  174.38344736397266
test acc:  0.8232
********** reverse layer  0  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.765625  ==>  180 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8776041666666666  ==>  94 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  1
[5, 4, 0, 0, 3, 4, 2, 2, 2, 0, 2, 3, 5, 5]
optimize layer  2
backward train epoch:  197
test acc:  0.0989
forward train acc:  0.95378  and loss:  60.27210162766278
test acc:  0.8922
forward train acc:  0.97162  and loss:  36.89198077376932
test acc:  0.895
forward train acc:  0.97698  and loss:  28.718919798266143
test acc:  0.8974
forward train acc:  0.98026  and loss:  25.124044625088573
test acc:  0.9006
forward train acc:  0.98222  and loss:  22.071568245999515
test acc:  0.9022
forward train acc:  0.98424  and loss:  20.332770598120987
test acc:  0.9035
forward train acc:  0.98476  and loss:  19.42529797833413
test acc:  0.9046
forward train acc:  0.98578  and loss:  18.464017850346863
test acc:  0.9043
forward train acc:  0.9864  and loss:  16.874007559381425
test acc:  0.9047
forward train acc:  0.988  and loss:  15.741149793728255
test acc:  0.9052
********** reverse layer  2  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.765625  ==>  180 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8776041666666666  ==>  94 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
optimize layer  3
backward train epoch:  3
test acc:  0.1003
forward train acc:  0.99818  and loss:  2.812346448889002
test acc:  0.9168
forward train acc:  0.99866  and loss:  1.8748643483850174
test acc:  0.9196
forward train acc:  0.9991  and loss:  1.439411687431857
test acc:  0.9178
forward train acc:  0.99926  and loss:  1.3368741720041726
test acc:  0.9184
forward train acc:  0.99926  and loss:  1.0607693806232419
test acc:  0.9189
forward train acc:  0.99932  and loss:  0.943500490160659
test acc:  0.919
forward train acc:  0.9993  and loss:  1.076947475754423
test acc:  0.9181
forward train acc:  0.99946  and loss:  0.9087224807881285
test acc:  0.918
forward train acc:  0.9995  and loss:  0.7731042442610487
test acc:  0.9179
forward train acc:  0.99936  and loss:  0.8927134372352157
test acc:  0.9184
********** reverse layer  3  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.765625  ==>  180 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8776041666666666  ==>  94 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  4
[5, 4, 5, 5, 2, 4, 2, 2, 2, 0, 2, 3, 5, 5]
***** skip layer  5
[5, 4, 5, 5, 2, 3, 2, 2, 2, 0, 2, 3, 5, 5]
***** skip layer  6
[5, 4, 5, 5, 2, 3, 1, 2, 2, 0, 2, 3, 5, 5]
***** skip layer  7
[5, 4, 5, 5, 2, 3, 1, 1, 2, 0, 2, 3, 5, 5]
***** skip layer  8
[5, 4, 5, 5, 2, 3, 1, 1, 1, 0, 2, 3, 5, 5]
optimize layer  9
backward train epoch:  180
test acc:  0.0992
forward train acc:  0.99938  and loss:  0.8730523409903981
test acc:  0.9182
forward train acc:  0.99938  and loss:  0.9854960370576009
test acc:  0.9187
forward train acc:  0.99914  and loss:  1.0611118466185872
test acc:  0.918
forward train acc:  0.99944  and loss:  0.8081744197406806
test acc:  0.9194
forward train acc:  0.99962  and loss:  0.6302721820247825
test acc:  0.9185
forward train acc:  0.9997  and loss:  0.5307529166166205
test acc:  0.9185
forward train acc:  0.99976  and loss:  0.4917750408058055
test acc:  0.9181
forward train acc:  0.99974  and loss:  0.5071688563912176
test acc:  0.9182
forward train acc:  0.99976  and loss:  0.43451614410150796
test acc:  0.9195
forward train acc:  0.9997  and loss:  0.43969045105041005
test acc:  0.9195
********** reverse layer  9  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.765625  ==>  180 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8776041666666666  ==>  94 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  10
[5, 4, 5, 5, 2, 3, 1, 1, 1, 5, 1, 3, 5, 5]
***** skip layer  11
[5, 4, 5, 5, 2, 3, 1, 1, 1, 5, 1, 2, 5, 5]
***** skip layer  12
[5, 4, 5, 5, 2, 3, 1, 1, 1, 5, 1, 2, 4, 5]
***** skip layer  13
[5, 4, 5, 5, 2, 3, 1, 1, 1, 5, 1, 2, 4, 4]
***** skip layer  0
[4, 4, 5, 5, 2, 3, 1, 1, 1, 5, 1, 2, 4, 4]
***** skip layer  1
[4, 3, 5, 5, 2, 3, 1, 1, 1, 5, 1, 2, 4, 4]
***** skip layer  2
[4, 3, 4, 5, 2, 3, 1, 1, 1, 5, 1, 2, 4, 4]
***** skip layer  3
[4, 3, 4, 4, 2, 3, 1, 1, 1, 5, 1, 2, 4, 4]
***** skip layer  4
[4, 3, 4, 4, 1, 3, 1, 1, 1, 5, 1, 2, 4, 4]
***** skip layer  5
[4, 3, 4, 4, 1, 2, 1, 1, 1, 5, 1, 2, 4, 4]
***** skip layer  6
[4, 3, 4, 4, 1, 2, 0, 1, 1, 5, 1, 2, 4, 4]
***** skip layer  7
[4, 3, 4, 4, 1, 2, 0, 0, 1, 5, 1, 2, 4, 4]
***** skip layer  8
[4, 3, 4, 4, 1, 2, 0, 0, 0, 5, 1, 2, 4, 4]
***** skip layer  9
[4, 3, 4, 4, 1, 2, 0, 0, 0, 4, 1, 2, 4, 4]
***** skip layer  10
[4, 3, 4, 4, 1, 2, 0, 0, 0, 4, 0, 2, 4, 4]
***** skip layer  11
[4, 3, 4, 4, 1, 2, 0, 0, 0, 4, 0, 1, 4, 4]
***** skip layer  12
[4, 3, 4, 4, 1, 2, 0, 0, 0, 4, 0, 1, 3, 4]
***** skip layer  13
[4, 3, 4, 4, 1, 2, 0, 0, 0, 4, 0, 1, 3, 3]
***** skip layer  0
[3, 3, 4, 4, 1, 2, 0, 0, 0, 4, 0, 1, 3, 3]
***** skip layer  1
[3, 2, 4, 4, 1, 2, 0, 0, 0, 4, 0, 1, 3, 3]
***** skip layer  2
[3, 2, 3, 4, 1, 2, 0, 0, 0, 4, 0, 1, 3, 3]
***** skip layer  3
[3, 2, 3, 3, 1, 2, 0, 0, 0, 4, 0, 1, 3, 3]
***** skip layer  4
[3, 2, 3, 3, 0, 2, 0, 0, 0, 4, 0, 1, 3, 3]
***** skip layer  5
[3, 2, 3, 3, 0, 1, 0, 0, 0, 4, 0, 1, 3, 3]
optimize layer  6
backward train epoch:  539
test acc:  0.0935
forward train acc:  0.99912  and loss:  1.4260249791259412
test acc:  0.9178
forward train acc:  0.99934  and loss:  0.9311776921967976
test acc:  0.9184
forward train acc:  0.9994  and loss:  0.7998361812788062
test acc:  0.9181
forward train acc:  0.99936  and loss:  0.8250591024407186
test acc:  0.9196
forward train acc:  0.99954  and loss:  0.6503648797224741
test acc:  0.9185
forward train acc:  0.99956  and loss:  0.5415515088534448
test acc:  0.9188
forward train acc:  0.9996  and loss:  0.5996707735466771
test acc:  0.9194
forward train acc:  0.99954  and loss:  0.7927084916445892
test acc:  0.9196
forward train acc:  0.99966  and loss:  0.5342277112067677
test acc:  0.9197
forward train acc:  0.99968  and loss:  0.5306838731339667
test acc:  0.9196
********** reverse layer  6  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.765625  ==>  180 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8776041666666666  ==>  94 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
optimize layer  7
backward train epoch:  181
test acc:  0.0998
forward train acc:  0.99762  and loss:  3.0801104225974996
test acc:  0.9099
forward train acc:  0.99846  and loss:  1.9851137094956357
test acc:  0.9141
forward train acc:  0.99878  and loss:  1.6440857088309713
test acc:  0.9135
forward train acc:  0.99916  and loss:  1.2488469066738617
test acc:  0.9129
forward train acc:  0.99892  and loss:  1.2921983413689304
test acc:  0.9135
forward train acc:  0.99888  and loss:  1.5690380701271351
test acc:  0.9147
forward train acc:  0.99884  and loss:  1.4826409138913732
test acc:  0.9162
forward train acc:  0.99926  and loss:  0.9441895671770908
test acc:  0.9167
forward train acc:  0.99932  and loss:  0.8232388467004057
test acc:  0.9157
forward train acc:  0.99928  and loss:  0.9654269494058099
test acc:  0.915
********** reverse layer  7  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.765625  ==>  180 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8776041666666666  ==>  94 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
optimize layer  8
backward train epoch:  618
test acc:  0.1
forward train acc:  0.99304  and loss:  10.197427562088706
test acc:  0.9113
forward train acc:  0.99762  and loss:  3.6205111176823266
test acc:  0.9152
forward train acc:  0.99776  and loss:  2.9871369044994935
test acc:  0.9152
forward train acc:  0.99842  and loss:  2.152556916436879
test acc:  0.9156
forward train acc:  0.99842  and loss:  2.2772617771988735
test acc:  0.9155
forward train acc:  0.99856  and loss:  1.9589931803639047
test acc:  0.9166
forward train acc:  0.99902  and loss:  1.4319730483402964
test acc:  0.9169
forward train acc:  0.999  and loss:  1.322644141415367
test acc:  0.9167
forward train acc:  0.99912  and loss:  1.2745075588754844
test acc:  0.9162
forward train acc:  0.99922  and loss:  1.111858039163053
test acc:  0.916
********** reverse layer  8  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.765625  ==>  180 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8776041666666666  ==>  94 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  9
[3, 2, 3, 3, 0, 1, 5, 5, 5, 3, 0, 1, 3, 3]
optimize layer  10
backward train epoch:  6
test acc:  0.5289
forward train acc:  0.99948  and loss:  0.7679190255294088
test acc:  0.9189
forward train acc:  0.99964  and loss:  0.575287764397217
test acc:  0.9183
forward train acc:  0.9996  and loss:  0.661544090762618
test acc:  0.9185
forward train acc:  0.99972  and loss:  0.5771070129267173
test acc:  0.9188
forward train acc:  0.99964  and loss:  0.5782189927558647
test acc:  0.9188
forward train acc:  0.99964  and loss:  0.6551723524171393
test acc:  0.9196
forward train acc:  0.99972  and loss:  0.5161727648519445
test acc:  0.919
forward train acc:  0.99982  and loss:  0.4273873020283645
test acc:  0.9194
forward train acc:  0.9998  and loss:  0.29001546249492094
test acc:  0.9191
forward train acc:  0.9998  and loss:  0.3370690103038214
test acc:  0.9182
********** reverse layer  10  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6145833333333334  ==>  148 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.765625  ==>  180 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8776041666666666  ==>  94 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  11
[3, 2, 3, 3, 0, 1, 5, 5, 5, 3, 5, 0, 3, 3]
***** skip layer  12
[3, 2, 3, 3, 0, 1, 5, 5, 5, 3, 5, 0, 2, 3]
***** skip layer  13
[3, 2, 3, 3, 0, 1, 5, 5, 5, 3, 5, 0, 2, 2]
***** skip layer  0
[2, 2, 3, 3, 0, 1, 5, 5, 5, 3, 5, 0, 2, 2]
***** skip layer  1
[2, 1, 3, 3, 0, 1, 5, 5, 5, 3, 5, 0, 2, 2]
***** skip layer  2
[2, 1, 2, 3, 0, 1, 5, 5, 5, 3, 5, 0, 2, 2]
***** skip layer  3
[2, 1, 2, 2, 0, 1, 5, 5, 5, 3, 5, 0, 2, 2]
optimize layer  4
backward train epoch:  37
test acc:  0.1027
forward train acc:  0.99922  and loss:  1.0177611531544244
test acc:  0.9184
forward train acc:  0.99926  and loss:  1.0523668695532251
test acc:  0.9167
forward train acc:  0.99912  and loss:  1.0901718379464
test acc:  0.9183
forward train acc:  0.99918  and loss:  0.9652064410620369
test acc:  0.9174
forward train acc:  0.99932  and loss:  0.861111855803756
test acc:  0.9193
forward train acc:  0.99954  and loss:  0.6721339034556877
test acc:  0.9193
forward train acc:  0.99952  and loss:  0.5596933550696122
test acc:  0.9196
forward train acc:  0.99936  and loss:  0.6987633226526668
test acc:  0.9191
forward train acc:  0.9996  and loss:  0.6674242928565945
test acc:  0.9196
forward train acc:  0.99954  and loss:  0.7058057955728145
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.625  ==>  144 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.765625  ==>  180 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8776041666666666  ==>  94 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  5
[2, 1, 2, 2, 0, 0, 5, 5, 5, 3, 5, 0, 2, 2]
***** skip layer  6
[2, 1, 2, 2, 0, 0, 4, 5, 5, 3, 5, 0, 2, 2]
***** skip layer  7
[2, 1, 2, 2, 0, 0, 4, 4, 5, 3, 5, 0, 2, 2]
***** skip layer  8
[2, 1, 2, 2, 0, 0, 4, 4, 4, 3, 5, 0, 2, 2]
***** skip layer  9
[2, 1, 2, 2, 0, 0, 4, 4, 4, 2, 5, 0, 2, 2]
***** skip layer  10
[2, 1, 2, 2, 0, 0, 4, 4, 4, 2, 4, 0, 2, 2]
optimize layer  11
backward train epoch:  123
test acc:  0.0999
forward train acc:  0.99936  and loss:  0.7964679818542209
test acc:  0.9131
forward train acc:  0.99936  and loss:  1.014461606129771
test acc:  0.9195
forward train acc:  0.9995  and loss:  0.9316635977011174
test acc:  0.918
forward train acc:  0.99928  and loss:  0.9986644530727062
test acc:  0.9198
forward train acc:  0.99956  and loss:  0.7452341128664557
test acc:  0.9205
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.625  ==>  144 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.765625  ==>  180 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8854166666666666  ==>  88 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  12
[2, 1, 2, 2, 0, 0, 4, 4, 4, 2, 4, 0, 1, 2]
***** skip layer  13
[2, 1, 2, 2, 0, 0, 4, 4, 4, 2, 4, 0, 1, 1]
***** skip layer  0
[1, 1, 2, 2, 0, 0, 4, 4, 4, 2, 4, 0, 1, 1]
***** skip layer  1
[1, 0, 2, 2, 0, 0, 4, 4, 4, 2, 4, 0, 1, 1]
***** skip layer  2
[1, 0, 1, 2, 0, 0, 4, 4, 4, 2, 4, 0, 1, 1]
***** skip layer  3
[1, 0, 1, 1, 0, 0, 4, 4, 4, 2, 4, 0, 1, 1]
optimize layer  4
backward train epoch:  50
test acc:  0.0997
forward train acc:  0.9986  and loss:  2.0260897513944656
test acc:  0.916
forward train acc:  0.99856  and loss:  2.1237170354579575
test acc:  0.9169
forward train acc:  0.99852  and loss:  1.784905495645944
test acc:  0.9172
forward train acc:  0.99896  and loss:  1.2460417319089174
test acc:  0.9198
forward train acc:  0.99908  and loss:  1.1227217391424347
test acc:  0.9172
forward train acc:  0.99926  and loss:  0.9951665471307933
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.765625  ==>  180 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8854166666666666  ==>  88 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
optimize layer  5
backward train epoch:  82
test acc:  0.0982
forward train acc:  0.99878  and loss:  1.7407812393794302
test acc:  0.9162
forward train acc:  0.99844  and loss:  2.022621462063398
test acc:  0.9167
forward train acc:  0.99914  and loss:  1.3013592123461422
test acc:  0.9163
forward train acc:  0.9988  and loss:  1.502537078136811
test acc:  0.9163
forward train acc:  0.99916  and loss:  1.27556773746619
test acc:  0.9168
forward train acc:  0.99896  and loss:  1.3789996528648771
test acc:  0.9167
forward train acc:  0.99922  and loss:  1.0345540715497918
test acc:  0.9189
forward train acc:  0.99914  and loss:  1.2087847949296702
test acc:  0.9173
forward train acc:  0.99934  and loss:  0.9079819896141998
test acc:  0.9173
forward train acc:  0.99928  and loss:  1.0037118001200724
test acc:  0.919
********** reverse layer  5  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.765625  ==>  180 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8854166666666666  ==>  88 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  6
[1, 0, 1, 1, 0, 5, 3, 4, 4, 2, 4, 0, 1, 1]
***** skip layer  7
[1, 0, 1, 1, 0, 5, 3, 3, 4, 2, 4, 0, 1, 1]
***** skip layer  8
[1, 0, 1, 1, 0, 5, 3, 3, 3, 2, 4, 0, 1, 1]
***** skip layer  9
[1, 0, 1, 1, 0, 5, 3, 3, 3, 1, 4, 0, 1, 1]
***** skip layer  10
[1, 0, 1, 1, 0, 5, 3, 3, 3, 1, 3, 0, 1, 1]
optimize layer  11
backward train epoch:  111
test acc:  0.1
forward train acc:  0.99954  and loss:  0.7331301540252753
test acc:  0.918
forward train acc:  0.99926  and loss:  1.0608107336156536
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.765625  ==>  180 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8932291666666666  ==>  82 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  12
[1, 0, 1, 1, 0, 5, 3, 3, 3, 1, 3, 0, 0, 1]
***** skip layer  13
[1, 0, 1, 1, 0, 5, 3, 3, 3, 1, 3, 0, 0, 0]
***** skip layer  0
[0, 0, 1, 1, 0, 5, 3, 3, 3, 1, 3, 0, 0, 0]
optimize layer  1
backward train epoch:  917
test acc:  0.1123
forward train acc:  0.96664  and loss:  55.64016344957054
test acc:  0.8968
forward train acc:  0.98144  and loss:  24.720678329467773
test acc:  0.9009
forward train acc:  0.98752  and loss:  16.71955531835556
test acc:  0.9056
forward train acc:  0.98966  and loss:  13.211381570785306
test acc:  0.9061
forward train acc:  0.99184  and loss:  10.849601676920429
test acc:  0.907
forward train acc:  0.99174  and loss:  10.539113789563999
test acc:  0.9068
forward train acc:  0.99278  and loss:  8.92991122364765
test acc:  0.9089
forward train acc:  0.99446  and loss:  7.7875536170904525
test acc:  0.9089
forward train acc:  0.9939  and loss:  8.1508588535944
test acc:  0.9099
forward train acc:  0.99484  and loss:  7.05866467958549
test acc:  0.9089
********** reverse layer  1  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.765625  ==>  180 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8932291666666666  ==>  82 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  2
[0, 5, 0, 1, 0, 5, 3, 3, 3, 1, 3, 0, 0, 0]
***** skip layer  3
[0, 5, 0, 0, 0, 5, 3, 3, 3, 1, 3, 0, 0, 0]
optimize layer  4
backward train epoch:  221
test acc:  0.0999
forward train acc:  0.99748  and loss:  3.2580276838270947
test acc:  0.9144
forward train acc:  0.99824  and loss:  2.4366826106852386
test acc:  0.9159
forward train acc:  0.99832  and loss:  2.2721557164622936
test acc:  0.9141
forward train acc:  0.99874  and loss:  1.7622799558739644
test acc:  0.9157
forward train acc:  0.99898  and loss:  1.3845258798100986
test acc:  0.915
forward train acc:  0.99896  and loss:  1.4517584470595466
test acc:  0.9161
forward train acc:  0.99924  and loss:  1.1257707024051342
test acc:  0.9156
forward train acc:  0.99914  and loss:  1.092343454802176
test acc:  0.9167
forward train acc:  0.99924  and loss:  1.0346648731210735
test acc:  0.9164
forward train acc:  0.9993  and loss:  0.9240616953466088
test acc:  0.9171
********** reverse layer  4  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.765625  ==>  180 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.8932291666666666  ==>  82 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  5
[0, 5, 0, 0, 5, 4, 3, 3, 3, 1, 3, 0, 0, 0]
***** skip layer  6
[0, 5, 0, 0, 5, 4, 2, 3, 3, 1, 3, 0, 0, 0]
***** skip layer  7
[0, 5, 0, 0, 5, 4, 2, 2, 3, 1, 3, 0, 0, 0]
***** skip layer  8
[0, 5, 0, 0, 5, 4, 2, 2, 2, 1, 3, 0, 0, 0]
***** skip layer  9
[0, 5, 0, 0, 5, 4, 2, 2, 2, 0, 3, 0, 0, 0]
***** skip layer  10
[0, 5, 0, 0, 5, 4, 2, 2, 2, 0, 2, 0, 0, 0]
optimize layer  11
backward train epoch:  144
test acc:  0.0995
forward train acc:  0.99942  and loss:  0.8513280026672874
test acc:  0.9203
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.765625  ==>  180 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
optimize layer  12
backward train epoch:  105
test acc:  0.1
forward train acc:  0.1  and loss:  1152.7500953674316
test acc:  0.1
forward train acc:  0.1  and loss:  1128.652771949768
test acc:  0.1
forward train acc:  0.1  and loss:  1106.4187078475952
test acc:  0.1
forward train acc:  0.1  and loss:  1090.7967448234558
test acc:  0.1
forward train acc:  0.1  and loss:  1081.1046981811523
test acc:  0.1
forward train acc:  0.1  and loss:  1071.4189367294312
test acc:  0.1
forward train acc:  0.1  and loss:  1062.1847772598267
test acc:  0.1
forward train acc:  0.1  and loss:  1055.6090536117554
test acc:  0.1
forward train acc:  0.1  and loss:  1051.2221629619598
test acc:  0.1
forward train acc:  0.1  and loss:  1046.8430542945862
test acc:  0.1
********** reverse layer  12  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.765625  ==>  180 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
optimize layer  13
backward train epoch:  102
test acc:  0.0976
forward train acc:  0.99308  and loss:  43.43927000835538
test acc:  0.9135
forward train acc:  0.99908  and loss:  40.2425599321723
test acc:  0.917
forward train acc:  0.99894  and loss:  38.662740502506495
test acc:  0.9166
forward train acc:  0.99914  and loss:  36.71304402127862
test acc:  0.9162
forward train acc:  0.99932  and loss:  35.66825304552913
test acc:  0.9181
forward train acc:  0.9994  and loss:  34.582973912358284
test acc:  0.9183
forward train acc:  0.99932  and loss:  33.926055593416095
test acc:  0.9181
forward train acc:  0.99934  and loss:  33.110691679641604
test acc:  0.9169
forward train acc:  0.99952  and loss:  32.52701073139906
test acc:  0.9182
forward train acc:  0.99954  and loss:  32.12453052960336
test acc:  0.9181
********** reverse layer  13  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.765625  ==>  180 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
optimize layer  0
backward train epoch:  139
test acc:  0.1006
forward train acc:  0.9992  and loss:  0.9962558685801923
test acc:  0.9189
forward train acc:  0.99928  and loss:  1.2678029484522995
test acc:  0.9186
forward train acc:  0.99914  and loss:  1.3262482539284974
test acc:  0.9193
forward train acc:  0.99942  and loss:  0.9220647043257486
test acc:  0.9185
forward train acc:  0.99936  and loss:  0.8211235089984257
test acc:  0.9189
forward train acc:  0.99956  and loss:  0.6553762214316521
test acc:  0.9185
forward train acc:  0.99958  and loss:  0.6044435262738261
test acc:  0.9189
forward train acc:  0.9995  and loss:  0.6119076121540274
test acc:  0.9183
forward train acc:  0.99948  and loss:  0.5672546606510878
test acc:  0.9188
forward train acc:  0.99958  and loss:  0.578199312905781
test acc:  0.9189
********** reverse layer  0  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.765625  ==>  180 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  1
[5, 4, 0, 0, 5, 4, 2, 2, 2, 0, 2, 0, 5, 5]
optimize layer  2
backward train epoch:  74
test acc:  0.1114
forward train acc:  0.99928  and loss:  1.1873442482610699
test acc:  0.9179
forward train acc:  0.99912  and loss:  1.2550110576266889
test acc:  0.9179
forward train acc:  0.999  and loss:  1.4901219821767882
test acc:  0.9168
forward train acc:  0.99928  and loss:  1.0135064571222756
test acc:  0.917
forward train acc:  0.99922  and loss:  1.0249127769493498
test acc:  0.9179
forward train acc:  0.99928  and loss:  0.98386618544464
test acc:  0.9157
forward train acc:  0.99932  and loss:  1.0026951181644108
test acc:  0.9175
forward train acc:  0.99966  and loss:  0.5734866364800837
test acc:  0.9179
forward train acc:  0.9997  and loss:  0.5224943555076607
test acc:  0.9187
forward train acc:  0.99946  and loss:  0.8158849655883387
test acc:  0.9177
********** reverse layer  2  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.765625  ==>  180 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
optimize layer  3
backward train epoch:  2
test acc:  0.0981
forward train acc:  0.9988  and loss:  1.7714137135189958
test acc:  0.9155
forward train acc:  0.99906  and loss:  1.2931344488169998
test acc:  0.9175
forward train acc:  0.99906  and loss:  1.502461142546963
test acc:  0.9161
forward train acc:  0.99916  and loss:  1.2988393146079034
test acc:  0.9174
forward train acc:  0.9994  and loss:  0.8832810967578553
test acc:  0.9167
forward train acc:  0.99922  and loss:  1.0162202627689112
test acc:  0.916
forward train acc:  0.99928  and loss:  1.0285097207815852
test acc:  0.9168
forward train acc:  0.99934  and loss:  0.870727370493114
test acc:  0.917
forward train acc:  0.99948  and loss:  0.8722439824487083
test acc:  0.9168
forward train acc:  0.99948  and loss:  0.701386848668335
test acc:  0.9173
********** reverse layer  3  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.765625  ==>  180 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  4
[5, 4, 5, 5, 4, 4, 2, 2, 2, 0, 2, 0, 5, 5]
***** skip layer  5
[5, 4, 5, 5, 4, 3, 2, 2, 2, 0, 2, 0, 5, 5]
***** skip layer  6
[5, 4, 5, 5, 4, 3, 1, 2, 2, 0, 2, 0, 5, 5]
***** skip layer  7
[5, 4, 5, 5, 4, 3, 1, 1, 2, 0, 2, 0, 5, 5]
***** skip layer  8
[5, 4, 5, 5, 4, 3, 1, 1, 1, 0, 2, 0, 5, 5]
optimize layer  9
backward train epoch:  92
test acc:  0.1
forward train acc:  0.9995  and loss:  0.7781787406711373
test acc:  0.9175
forward train acc:  0.99964  and loss:  0.614844657597132
test acc:  0.916
forward train acc:  0.99976  and loss:  0.533028307458153
test acc:  0.9178
forward train acc:  0.99964  and loss:  0.5258607815194409
test acc:  0.9165
forward train acc:  0.99948  and loss:  0.6649904510122724
test acc:  0.9189
forward train acc:  0.9995  and loss:  0.7072830653924029
test acc:  0.9193
forward train acc:  0.99962  and loss:  0.7416501320549287
test acc:  0.9195
forward train acc:  0.99968  and loss:  0.5049512912228238
test acc:  0.9181
forward train acc:  0.99974  and loss:  0.4357046955556143
test acc:  0.9193
forward train acc:  0.99984  and loss:  0.3665385844942648
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7734375  ==>  174 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  10
[5, 4, 5, 5, 4, 3, 1, 1, 1, 0, 1, 0, 5, 5]
optimize layer  11
backward train epoch:  47
test acc:  0.1002
forward train acc:  0.9993  and loss:  0.8897039889852749
test acc:  0.9169
forward train acc:  0.9997  and loss:  0.5918309707049048
test acc:  0.9179
forward train acc:  0.99958  and loss:  0.6861507946741767
test acc:  0.9193
forward train acc:  0.99952  and loss:  0.6459373610559851
test acc:  0.9187
forward train acc:  0.9997  and loss:  0.5166675731888972
test acc:  0.9178
forward train acc:  0.99962  and loss:  0.6104417657916201
test acc:  0.9183
forward train acc:  0.9996  and loss:  0.576941383376834
test acc:  0.9176
forward train acc:  0.99966  and loss:  0.5287464386492502
test acc:  0.9194
forward train acc:  0.99966  and loss:  0.5164943019481143
test acc:  0.9195
forward train acc:  0.99972  and loss:  0.4832722591672791
test acc:  0.9188
********** reverse layer  11  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7734375  ==>  174 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  12
[5, 4, 5, 5, 4, 3, 1, 1, 1, 0, 1, 5, 4, 5]
***** skip layer  13
[5, 4, 5, 5, 4, 3, 1, 1, 1, 0, 1, 5, 4, 4]
***** skip layer  0
[4, 4, 5, 5, 4, 3, 1, 1, 1, 0, 1, 5, 4, 4]
***** skip layer  1
[4, 3, 5, 5, 4, 3, 1, 1, 1, 0, 1, 5, 4, 4]
***** skip layer  2
[4, 3, 4, 5, 4, 3, 1, 1, 1, 0, 1, 5, 4, 4]
***** skip layer  3
[4, 3, 4, 4, 4, 3, 1, 1, 1, 0, 1, 5, 4, 4]
***** skip layer  4
[4, 3, 4, 4, 3, 3, 1, 1, 1, 0, 1, 5, 4, 4]
***** skip layer  5
[4, 3, 4, 4, 3, 2, 1, 1, 1, 0, 1, 5, 4, 4]
***** skip layer  6
[4, 3, 4, 4, 3, 2, 0, 1, 1, 0, 1, 5, 4, 4]
***** skip layer  7
[4, 3, 4, 4, 3, 2, 0, 0, 1, 0, 1, 5, 4, 4]
***** skip layer  8
[4, 3, 4, 4, 3, 2, 0, 0, 0, 0, 1, 5, 4, 4]
optimize layer  9
backward train epoch:  132
test acc:  0.1
forward train acc:  0.99972  and loss:  0.4156772268033819
test acc:  0.9178
forward train acc:  0.99918  and loss:  1.165664835483767
test acc:  0.918
forward train acc:  0.9995  and loss:  0.773184723657323
test acc:  0.9193
forward train acc:  0.99966  and loss:  0.4567381673841737
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.78125  ==>  168 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  10
[4, 3, 4, 4, 3, 2, 0, 0, 0, 0, 0, 5, 4, 4]
***** skip layer  11
[4, 3, 4, 4, 3, 2, 0, 0, 0, 0, 0, 4, 4, 4]
***** skip layer  12
[4, 3, 4, 4, 3, 2, 0, 0, 0, 0, 0, 4, 3, 4]
***** skip layer  13
[4, 3, 4, 4, 3, 2, 0, 0, 0, 0, 0, 4, 3, 3]
***** skip layer  0
[3, 3, 4, 4, 3, 2, 0, 0, 0, 0, 0, 4, 3, 3]
***** skip layer  1
[3, 2, 4, 4, 3, 2, 0, 0, 0, 0, 0, 4, 3, 3]
***** skip layer  2
[3, 2, 3, 4, 3, 2, 0, 0, 0, 0, 0, 4, 3, 3]
***** skip layer  3
[3, 2, 3, 3, 3, 2, 0, 0, 0, 0, 0, 4, 3, 3]
***** skip layer  4
[3, 2, 3, 3, 2, 2, 0, 0, 0, 0, 0, 4, 3, 3]
***** skip layer  5
[3, 2, 3, 3, 2, 1, 0, 0, 0, 0, 0, 4, 3, 3]
optimize layer  6
backward train epoch:  281
test acc:  0.1
forward train acc:  0.99944  and loss:  0.7425583989243023
test acc:  0.9156
forward train acc:  0.99928  and loss:  1.0861469140509143
test acc:  0.9157
forward train acc:  0.9992  and loss:  1.0775248463032767
test acc:  0.9164
forward train acc:  0.99938  and loss:  0.8416047563514439
test acc:  0.9152
forward train acc:  0.99952  and loss:  0.7001647429424338
test acc:  0.9196
forward train acc:  0.99962  and loss:  0.561112884839531
test acc:  0.9171
forward train acc:  0.9993  and loss:  0.8631684728898108
test acc:  0.919
forward train acc:  0.99952  and loss:  0.5967883228731807
test acc:  0.9162
forward train acc:  0.9997  and loss:  0.5462101753073512
test acc:  0.9195
forward train acc:  0.99964  and loss:  0.5394761845527682
test acc:  0.9177
********** reverse layer  6  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.78125  ==>  168 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
optimize layer  7
backward train epoch:  104
test acc:  0.1
forward train acc:  0.99768  and loss:  3.2906945048307534
test acc:  0.9133
forward train acc:  0.9987  and loss:  1.9471871498099063
test acc:  0.9128
forward train acc:  0.99874  and loss:  1.8365848287648987
test acc:  0.9157
forward train acc:  0.99876  and loss:  1.6144360903126653
test acc:  0.9154
forward train acc:  0.99912  and loss:  1.1807782740797848
test acc:  0.9156
forward train acc:  0.99906  and loss:  1.2673231761582429
test acc:  0.9165
forward train acc:  0.99906  and loss:  1.1439975535904523
test acc:  0.916
forward train acc:  0.99938  and loss:  0.891455583361676
test acc:  0.9165
forward train acc:  0.99928  and loss:  0.9024123981362209
test acc:  0.9167
forward train acc:  0.99938  and loss:  0.8489812885818537
test acc:  0.9172
********** reverse layer  7  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.78125  ==>  168 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
optimize layer  8
backward train epoch:  133
test acc:  0.0999
forward train acc:  0.99266  and loss:  10.323297865921631
test acc:  0.911
forward train acc:  0.99614  and loss:  5.2768192730727606
test acc:  0.9121
forward train acc:  0.99768  and loss:  3.2997650395845994
test acc:  0.9144
forward train acc:  0.99786  and loss:  3.2122066951706074
test acc:  0.9143
forward train acc:  0.99812  and loss:  2.6726860270136967
test acc:  0.9162
forward train acc:  0.99842  and loss:  2.099000195681583
test acc:  0.917
forward train acc:  0.99858  and loss:  2.1261901871766895
test acc:  0.9165
forward train acc:  0.9986  and loss:  2.017337159370072
test acc:  0.917
forward train acc:  0.99846  and loss:  2.0782691391650587
test acc:  0.9172
forward train acc:  0.99876  and loss:  1.9455547387769911
test acc:  0.9173
********** reverse layer  8  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.78125  ==>  168 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
optimize layer  9
backward train epoch:  301
test acc:  0.1
forward train acc:  0.99944  and loss:  0.8670399491529679
test acc:  0.917
forward train acc:  0.99952  and loss:  0.7687115898152115
test acc:  0.9177
forward train acc:  0.99946  and loss:  0.6932789598795353
test acc:  0.9178
forward train acc:  0.99962  and loss:  0.5766536226437893
test acc:  0.9189
forward train acc:  0.99972  and loss:  0.4805366615910316
test acc:  0.9192
forward train acc:  0.99982  and loss:  0.3858271936624078
test acc:  0.9183
forward train acc:  0.99958  and loss:  0.5488698027766077
test acc:  0.9195
forward train acc:  0.99964  and loss:  0.6306509381392971
test acc:  0.918
forward train acc:  0.99964  and loss:  0.5253566025494365
test acc:  0.919
forward train acc:  0.9998  and loss:  0.274420643778285
test acc:  0.9181
********** reverse layer  9  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.78125  ==>  168 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
optimize layer  10
backward train epoch:  262
test acc:  0.1
forward train acc:  0.9997  and loss:  0.5123076758900424
test acc:  0.9167
forward train acc:  0.99946  and loss:  0.7908209721499588
test acc:  0.9172
forward train acc:  0.99954  and loss:  0.6077024824917316
test acc:  0.9168
forward train acc:  0.99956  and loss:  0.7110175693233032
test acc:  0.9182
forward train acc:  0.99954  and loss:  0.6118172601563856
test acc:  0.9183
forward train acc:  0.99944  and loss:  0.6659658518765355
test acc:  0.9194
forward train acc:  0.99962  and loss:  0.5324212542618625
test acc:  0.9184
forward train acc:  0.99982  and loss:  0.2813880488247378
test acc:  0.9173
forward train acc:  0.99974  and loss:  0.32273380091646686
test acc:  0.9184
forward train acc:  0.9996  and loss:  0.5981157333444571
test acc:  0.918
********** reverse layer  10  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.78125  ==>  168 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  11
[3, 2, 3, 3, 2, 1, 5, 5, 5, 5, 5, 3, 3, 3]
***** skip layer  12
[3, 2, 3, 3, 2, 1, 5, 5, 5, 5, 5, 3, 2, 3]
***** skip layer  13
[3, 2, 3, 3, 2, 1, 5, 5, 5, 5, 5, 3, 2, 2]
***** skip layer  0
[2, 2, 3, 3, 2, 1, 5, 5, 5, 5, 5, 3, 2, 2]
***** skip layer  1
[2, 1, 3, 3, 2, 1, 5, 5, 5, 5, 5, 3, 2, 2]
***** skip layer  2
[2, 1, 2, 3, 2, 1, 5, 5, 5, 5, 5, 3, 2, 2]
***** skip layer  3
[2, 1, 2, 2, 2, 1, 5, 5, 5, 5, 5, 3, 2, 2]
***** skip layer  4
[2, 1, 2, 2, 1, 1, 5, 5, 5, 5, 5, 3, 2, 2]
***** skip layer  5
[2, 1, 2, 2, 1, 0, 5, 5, 5, 5, 5, 3, 2, 2]
***** skip layer  6
[2, 1, 2, 2, 1, 0, 4, 5, 5, 5, 5, 3, 2, 2]
***** skip layer  7
[2, 1, 2, 2, 1, 0, 4, 4, 5, 5, 5, 3, 2, 2]
***** skip layer  8
[2, 1, 2, 2, 1, 0, 4, 4, 4, 5, 5, 3, 2, 2]
***** skip layer  9
[2, 1, 2, 2, 1, 0, 4, 4, 4, 4, 5, 3, 2, 2]
***** skip layer  10
[2, 1, 2, 2, 1, 0, 4, 4, 4, 4, 4, 3, 2, 2]
***** skip layer  11
[2, 1, 2, 2, 1, 0, 4, 4, 4, 4, 4, 2, 2, 2]
***** skip layer  12
[2, 1, 2, 2, 1, 0, 4, 4, 4, 4, 4, 2, 1, 2]
***** skip layer  13
[2, 1, 2, 2, 1, 0, 4, 4, 4, 4, 4, 2, 1, 1]
***** skip layer  0
[1, 1, 2, 2, 1, 0, 4, 4, 4, 4, 4, 2, 1, 1]
***** skip layer  1
[1, 0, 2, 2, 1, 0, 4, 4, 4, 4, 4, 2, 1, 1]
***** skip layer  2
[1, 0, 1, 2, 1, 0, 4, 4, 4, 4, 4, 2, 1, 1]
***** skip layer  3
[1, 0, 1, 1, 1, 0, 4, 4, 4, 4, 4, 2, 1, 1]
***** skip layer  4
[1, 0, 1, 1, 0, 0, 4, 4, 4, 4, 4, 2, 1, 1]
optimize layer  5
backward train epoch:  121
test acc:  0.0998
forward train acc:  0.99874  and loss:  1.517535442704684
test acc:  0.9178
forward train acc:  0.9992  and loss:  1.0570926804793999
test acc:  0.9179
forward train acc:  0.99934  and loss:  0.9926744924450759
test acc:  0.9181
forward train acc:  0.99942  and loss:  0.8034044476371491
test acc:  0.9195
forward train acc:  0.99952  and loss:  0.5426693310291739
test acc:  0.9187
forward train acc:  0.9996  and loss:  0.6068985837046057
test acc:  0.9191
forward train acc:  0.99964  and loss:  0.530597093020333
test acc:  0.9184
forward train acc:  0.9995  and loss:  0.6065192437381484
test acc:  0.9195
forward train acc:  0.99964  and loss:  0.6354186297976412
test acc:  0.9186
forward train acc:  0.99958  and loss:  0.48669708403758705
test acc:  0.9188
********** reverse layer  5  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6354166666666666  ==>  35 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.78125  ==>  168 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  6
[1, 0, 1, 1, 0, 5, 3, 4, 4, 4, 4, 2, 1, 1]
***** skip layer  7
[1, 0, 1, 1, 0, 5, 3, 3, 4, 4, 4, 2, 1, 1]
***** skip layer  8
[1, 0, 1, 1, 0, 5, 3, 3, 3, 4, 4, 2, 1, 1]
***** skip layer  9
[1, 0, 1, 1, 0, 5, 3, 3, 3, 3, 4, 2, 1, 1]
***** skip layer  10
[1, 0, 1, 1, 0, 5, 3, 3, 3, 3, 3, 2, 1, 1]
***** skip layer  11
[1, 0, 1, 1, 0, 5, 3, 3, 3, 3, 3, 1, 1, 1]
***** skip layer  12
[1, 0, 1, 1, 0, 5, 3, 3, 3, 3, 3, 1, 0, 1]
***** skip layer  13
[1, 0, 1, 1, 0, 5, 3, 3, 3, 3, 3, 1, 0, 0]
***** skip layer  0
[0, 0, 1, 1, 0, 5, 3, 3, 3, 3, 3, 1, 0, 0]
optimize layer  1
backward train epoch:  133
test acc:  0.1038
forward train acc:  0.99924  and loss:  0.980812330977642
test acc:  0.9172
forward train acc:  0.99922  and loss:  1.0298270694474922
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.78125  ==>  168 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  2
[0, 0, 0, 1, 0, 5, 3, 3, 3, 3, 3, 1, 0, 0]
***** skip layer  3
[0, 0, 0, 0, 0, 5, 3, 3, 3, 3, 3, 1, 0, 0]
optimize layer  4
backward train epoch:  136
test acc:  0.1
forward train acc:  0.99898  and loss:  1.8431228838016978
test acc:  0.9179
forward train acc:  0.99888  and loss:  1.7645186348236166
test acc:  0.9183
forward train acc:  0.99896  and loss:  1.6221450719167478
test acc:  0.9155
forward train acc:  0.9988  and loss:  1.5598547166300705
test acc:  0.9157
forward train acc:  0.99922  and loss:  0.9485067672067089
test acc:  0.9172
forward train acc:  0.99938  and loss:  0.7847555232729064
test acc:  0.9168
forward train acc:  0.99922  and loss:  1.1269627611909527
test acc:  0.9182
forward train acc:  0.99932  and loss:  0.7617838829755783
test acc:  0.9184
forward train acc:  0.9993  and loss:  0.8882702582195634
test acc:  0.9169
forward train acc:  0.99926  and loss:  1.0036883640859742
test acc:  0.917
********** reverse layer  4  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.78125  ==>  168 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
***** skip layer  5
[0, 0, 0, 0, 5, 4, 3, 3, 3, 3, 3, 1, 0, 0]
***** skip layer  6
[0, 0, 0, 0, 5, 4, 2, 3, 3, 3, 3, 1, 0, 0]
***** skip layer  7
[0, 0, 0, 0, 5, 4, 2, 2, 3, 3, 3, 1, 0, 0]
***** skip layer  8
[0, 0, 0, 0, 5, 4, 2, 2, 2, 3, 3, 1, 0, 0]
***** skip layer  9
[0, 0, 0, 0, 5, 4, 2, 2, 2, 2, 3, 1, 0, 0]
***** skip layer  10
[0, 0, 0, 0, 5, 4, 2, 2, 2, 2, 2, 1, 0, 0]
***** skip layer  11
[0, 0, 0, 0, 5, 4, 2, 2, 2, 2, 2, 0, 0, 0]
optimize layer  12
backward train epoch:  230
test acc:  0.1
forward train acc:  0.1  and loss:  1132.0639383792877
test acc:  0.1
forward train acc:  0.1  and loss:  1108.8265459537506
test acc:  0.1
forward train acc:  0.1  and loss:  1087.3751199245453
test acc:  0.1
forward train acc:  0.1  and loss:  1072.4403355121613
test acc:  0.1
forward train acc:  0.1  and loss:  1062.9297363758087
test acc:  0.1
forward train acc:  0.1  and loss:  1053.81671667099
test acc:  0.1
forward train acc:  0.1  and loss:  1045.0342118740082
test acc:  0.1
forward train acc:  0.1  and loss:  1038.711431980133
test acc:  0.1
forward train acc:  0.1  and loss:  1034.4527995586395
test acc:  0.1
forward train acc:  0.1  and loss:  1030.4836657047272
test acc:  0.1
********** reverse layer  12  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.78125  ==>  168 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7005208333333334  ==>  230 / 768
optimize layer  13
backward train epoch:  102
test acc:  0.1002
forward train acc:  0.99948  and loss:  0.6585093307949137
test acc:  0.9197
forward train acc:  0.9996  and loss:  0.7446798483142629
test acc:  0.918
forward train acc:  0.9995  and loss:  0.700572256318992
test acc:  0.9195
forward train acc:  0.99954  and loss:  0.618716909782961
test acc:  0.9196
forward train acc:  0.99958  and loss:  0.6252801408991218
test acc:  0.9187
forward train acc:  0.9995  and loss:  0.5781545925419778
test acc:  0.9202
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.78125  ==>  168 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  0
backward train epoch:  485
test acc:  0.0943
forward train acc:  0.76668  and loss:  439.0342762172222
test acc:  0.7801
forward train acc:  0.82934  and loss:  222.79997643828392
test acc:  0.8086
forward train acc:  0.8584  and loss:  177.747987896204
test acc:  0.8276
forward train acc:  0.87692  and loss:  155.61972391605377
test acc:  0.8321
forward train acc:  0.8868  and loss:  142.566293284297
test acc:  0.8393
forward train acc:  0.89484  and loss:  130.69237549602985
test acc:  0.844
forward train acc:  0.90526  and loss:  117.6054322347045
test acc:  0.8488
forward train acc:  0.91072  and loss:  110.28477676212788
test acc:  0.8541
forward train acc:  0.91282  and loss:  108.39475440979004
test acc:  0.8544
forward train acc:  0.91822  and loss:  103.18564788997173
test acc:  0.8587
********** reverse layer  0  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.78125  ==>  168 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  1
backward train epoch:  93
test acc:  0.1033
forward train acc:  0.99744  and loss:  4.358853563200682
test acc:  0.9169
forward train acc:  0.99884  and loss:  1.8988452405901626
test acc:  0.9177
forward train acc:  0.99904  and loss:  1.3272154710721225
test acc:  0.9177
forward train acc:  0.99922  and loss:  1.0745188624714501
test acc:  0.9169
forward train acc:  0.99924  and loss:  0.9812321927456651
test acc:  0.9166
forward train acc:  0.9995  and loss:  0.8093610202195123
test acc:  0.9179
forward train acc:  0.99946  and loss:  0.7880357636604458
test acc:  0.9172
forward train acc:  0.9995  and loss:  0.6637468182889279
test acc:  0.918
forward train acc:  0.99972  and loss:  0.5271230851358268
test acc:  0.919
forward train acc:  0.99958  and loss:  0.622502765385434
test acc:  0.9183
********** reverse layer  1  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5625  ==>  84 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.78125  ==>  168 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  2
backward train epoch:  40
test acc:  0.1067
forward train acc:  0.9989  and loss:  1.5735886607435532
test acc:  0.9174
forward train acc:  0.99946  and loss:  0.911389718297869
test acc:  0.918
forward train acc:  0.9992  and loss:  1.0645575720700435
test acc:  0.9184
forward train acc:  0.99938  and loss:  1.025948641821742
test acc:  0.9183
forward train acc:  0.99946  and loss:  0.7667104873689823
test acc:  0.9193
forward train acc:  0.99948  and loss:  0.6774494121200405
test acc:  0.9195
forward train acc:  0.99964  and loss:  0.5864487511280458
test acc:  0.9192
forward train acc:  0.99972  and loss:  0.4245574846136151
test acc:  0.9191
forward train acc:  0.99964  and loss:  0.5364209182298509
test acc:  0.9199
forward train acc:  0.99958  and loss:  0.4752830299548805
test acc:  0.9202
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5729166666666666  ==>  82 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.78125  ==>  168 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  3
backward train epoch:  3
test acc:  0.802
forward train acc:  0.9989  and loss:  1.4048313781968318
test acc:  0.9162
forward train acc:  0.9987  and loss:  1.659296804573387
test acc:  0.9179
forward train acc:  0.99918  and loss:  1.1226009815873113
test acc:  0.9152
forward train acc:  0.99922  and loss:  1.085192420752719
test acc:  0.9166
forward train acc:  0.99916  and loss:  0.9472244968055747
test acc:  0.9166
forward train acc:  0.99942  and loss:  0.8640704791760072
test acc:  0.9159
forward train acc:  0.9996  and loss:  0.727676358044846
test acc:  0.9167
forward train acc:  0.99934  and loss:  0.7204578721430153
test acc:  0.9163
forward train acc:  0.99952  and loss:  0.6538950420508627
test acc:  0.9164
forward train acc:  0.99956  and loss:  0.7062416291737463
test acc:  0.9169
********** reverse layer  3  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5729166666666666  ==>  82 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.78125  ==>  168 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  4
[5, 5, 0, 5, 4, 4, 2, 2, 2, 2, 2, 0, 5, 0]
***** skip layer  5
[5, 5, 0, 5, 4, 3, 2, 2, 2, 2, 2, 0, 5, 0]
***** skip layer  6
[5, 5, 0, 5, 4, 3, 1, 2, 2, 2, 2, 0, 5, 0]
***** skip layer  7
[5, 5, 0, 5, 4, 3, 1, 1, 2, 2, 2, 0, 5, 0]
***** skip layer  8
[5, 5, 0, 5, 4, 3, 1, 1, 1, 2, 2, 0, 5, 0]
***** skip layer  9
[5, 5, 0, 5, 4, 3, 1, 1, 1, 1, 2, 0, 5, 0]
***** skip layer  10
[5, 5, 0, 5, 4, 3, 1, 1, 1, 1, 1, 0, 5, 0]
optimize layer  11
backward train epoch:  95
test acc:  0.1061
forward train acc:  0.99948  and loss:  0.6369626280866214
test acc:  0.9184
forward train acc:  0.99956  and loss:  0.575162228080444
test acc:  0.9182
forward train acc:  0.99942  and loss:  0.8014366110728588
test acc:  0.9181
forward train acc:  0.99954  and loss:  0.6317618559114635
test acc:  0.9174
forward train acc:  0.99972  and loss:  0.4669813255604822
test acc:  0.9186
forward train acc:  0.99968  and loss:  0.5970681803737534
test acc:  0.9184
forward train acc:  0.99958  and loss:  0.5549269426846877
test acc:  0.9196
forward train acc:  0.99966  and loss:  0.4659250020995387
test acc:  0.9194
forward train acc:  0.99974  and loss:  0.33365179419342894
test acc:  0.9194
forward train acc:  0.9998  and loss:  0.36031510529574007
test acc:  0.9192
********** reverse layer  11  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5729166666666666  ==>  82 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.78125  ==>  168 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  12
[5, 5, 0, 5, 4, 3, 1, 1, 1, 1, 1, 5, 4, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.1008
forward train acc:  0.99456  and loss:  26.626654936000705
test acc:  0.9142
forward train acc:  0.99878  and loss:  20.58036814816296
test acc:  0.9147
forward train acc:  0.9991  and loss:  18.247491486370564
test acc:  0.9149
forward train acc:  0.99944  and loss:  16.553307221271098
test acc:  0.9145
forward train acc:  0.99938  and loss:  15.802892465144396
test acc:  0.9159
forward train acc:  0.99926  and loss:  15.169420872814953
test acc:  0.9159
forward train acc:  0.99958  and loss:  14.246762108057737
test acc:  0.9167
forward train acc:  0.99954  and loss:  13.501954930834472
test acc:  0.9161
forward train acc:  0.9995  and loss:  13.300639446824789
test acc:  0.9167
forward train acc:  0.9995  and loss:  12.940560854971409
test acc:  0.9179
********** reverse layer  13  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5729166666666666  ==>  82 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.78125  ==>  168 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  0
[4, 5, 0, 5, 4, 3, 1, 1, 1, 1, 1, 5, 4, 5]
***** skip layer  1
[4, 4, 0, 5, 4, 3, 1, 1, 1, 1, 1, 5, 4, 5]
optimize layer  2
backward train epoch:  331
test acc:  0.1001
forward train acc:  0.95  and loss:  92.7547288928181
test acc:  0.8847
forward train acc:  0.96528  and loss:  48.15320519544184
test acc:  0.8895
forward train acc:  0.974  and loss:  33.65900052012876
test acc:  0.8956
forward train acc:  0.97924  and loss:  26.51567636243999
test acc:  0.8976
forward train acc:  0.98178  and loss:  23.33055219706148
test acc:  0.8974
forward train acc:  0.98414  and loss:  20.461713509168476
test acc:  0.9
forward train acc:  0.98552  and loss:  18.829128396697342
test acc:  0.9002
forward train acc:  0.98694  and loss:  16.71037541422993
test acc:  0.9007
forward train acc:  0.9879  and loss:  15.98903597611934
test acc:  0.9022
forward train acc:  0.98836  and loss:  14.899168895091861
test acc:  0.9018
********** reverse layer  2  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5729166666666666  ==>  82 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.78125  ==>  168 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  3
[4, 4, 5, 4, 4, 3, 1, 1, 1, 1, 1, 5, 4, 5]
***** skip layer  4
[4, 4, 5, 4, 3, 3, 1, 1, 1, 1, 1, 5, 4, 5]
***** skip layer  5
[4, 4, 5, 4, 3, 2, 1, 1, 1, 1, 1, 5, 4, 5]
***** skip layer  6
[4, 4, 5, 4, 3, 2, 0, 1, 1, 1, 1, 5, 4, 5]
***** skip layer  7
[4, 4, 5, 4, 3, 2, 0, 0, 1, 1, 1, 5, 4, 5]
***** skip layer  8
[4, 4, 5, 4, 3, 2, 0, 0, 0, 1, 1, 5, 4, 5]
***** skip layer  9
[4, 4, 5, 4, 3, 2, 0, 0, 0, 0, 1, 5, 4, 5]
***** skip layer  10
[4, 4, 5, 4, 3, 2, 0, 0, 0, 0, 0, 5, 4, 5]
***** skip layer  11
[4, 4, 5, 4, 3, 2, 0, 0, 0, 0, 0, 4, 4, 5]
***** skip layer  12
[4, 4, 5, 4, 3, 2, 0, 0, 0, 0, 0, 4, 3, 5]
***** skip layer  13
[4, 4, 5, 4, 3, 2, 0, 0, 0, 0, 0, 4, 3, 4]
***** skip layer  0
[3, 4, 5, 4, 3, 2, 0, 0, 0, 0, 0, 4, 3, 4]
***** skip layer  1
[3, 3, 5, 4, 3, 2, 0, 0, 0, 0, 0, 4, 3, 4]
***** skip layer  2
[3, 3, 4, 4, 3, 2, 0, 0, 0, 0, 0, 4, 3, 4]
***** skip layer  3
[3, 3, 4, 3, 3, 2, 0, 0, 0, 0, 0, 4, 3, 4]
***** skip layer  4
[3, 3, 4, 3, 2, 2, 0, 0, 0, 0, 0, 4, 3, 4]
***** skip layer  5
[3, 3, 4, 3, 2, 1, 0, 0, 0, 0, 0, 4, 3, 4]
optimize layer  6
backward train epoch:  170
test acc:  0.1012
forward train acc:  0.99898  and loss:  1.4870205187471583
test acc:  0.9173
forward train acc:  0.99918  and loss:  1.1322263926267624
test acc:  0.9171
forward train acc:  0.99938  and loss:  0.9429061514092609
test acc:  0.9161
forward train acc:  0.99946  and loss:  0.7679688037605956
test acc:  0.9164
forward train acc:  0.9995  and loss:  0.60449480736861
test acc:  0.9154
forward train acc:  0.99958  and loss:  0.5459680914937053
test acc:  0.9176
forward train acc:  0.99932  and loss:  0.8178163103293628
test acc:  0.9163
forward train acc:  0.99958  and loss:  0.5813473419257207
test acc:  0.9182
forward train acc:  0.99972  and loss:  0.4459055281477049
test acc:  0.9185
forward train acc:  0.99964  and loss:  0.5120261850970564
test acc:  0.9183
********** reverse layer  6  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5729166666666666  ==>  82 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.9583333333333334  ==>  32 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.78125  ==>  168 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  7
backward train epoch:  540
test acc:  0.1115
forward train acc:  0.99718  and loss:  3.7568798484280705
test acc:  0.9167
forward train acc:  0.99832  and loss:  2.119698817405151
test acc:  0.9157
forward train acc:  0.99858  and loss:  1.9874533846741542
test acc:  0.9184
forward train acc:  0.9989  and loss:  1.498163024574751
test acc:  0.9187
forward train acc:  0.99914  and loss:  1.0985577247629408
test acc:  0.9205
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5729166666666666  ==>  82 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.78125  ==>  168 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  8
backward train epoch:  333
test acc:  0.1289
forward train acc:  0.98938  and loss:  15.2480689373333
test acc:  0.9125
forward train acc:  0.9949  and loss:  6.7227701037190855
test acc:  0.9125
forward train acc:  0.99638  and loss:  4.999042737181298
test acc:  0.9125
forward train acc:  0.99666  and loss:  4.747401921078563
test acc:  0.9139
forward train acc:  0.99708  and loss:  3.808721416280605
test acc:  0.9146
forward train acc:  0.99748  and loss:  3.4263546341098845
test acc:  0.9151
forward train acc:  0.99784  and loss:  2.8614593103411607
test acc:  0.9154
forward train acc:  0.9978  and loss:  3.0047546506975777
test acc:  0.9166
forward train acc:  0.9981  and loss:  2.3213171146344393
test acc:  0.917
forward train acc:  0.9984  and loss:  2.146464303135872
test acc:  0.9169
********** reverse layer  8  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5729166666666666  ==>  82 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.78125  ==>  168 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  9
backward train epoch:  266
test acc:  0.1095
forward train acc:  0.99898  and loss:  1.1525554287363775
test acc:  0.9189
forward train acc:  0.99922  and loss:  1.120479060075013
test acc:  0.9176
forward train acc:  0.99944  and loss:  0.8373878443962894
test acc:  0.9184
forward train acc:  0.9995  and loss:  0.677567911989172
test acc:  0.9171
forward train acc:  0.9994  and loss:  0.6893966745119542
test acc:  0.9202
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5729166666666666  ==>  82 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7890625  ==>  162 / 768
layer  10  :  0.828125  ==>  132 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  10
backward train epoch:  193
test acc:  0.1107
forward train acc:  0.99946  and loss:  0.902520231786184
test acc:  0.9207
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5729166666666666  ==>  82 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7890625  ==>  162 / 768
layer  10  :  0.8359375  ==>  126 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  11
[3, 3, 4, 3, 2, 1, 5, 0, 5, 0, 0, 3, 3, 4]
***** skip layer  12
[3, 3, 4, 3, 2, 1, 5, 0, 5, 0, 0, 3, 2, 4]
***** skip layer  13
[3, 3, 4, 3, 2, 1, 5, 0, 5, 0, 0, 3, 2, 3]
***** skip layer  0
[2, 3, 4, 3, 2, 1, 5, 0, 5, 0, 0, 3, 2, 3]
***** skip layer  1
[2, 2, 4, 3, 2, 1, 5, 0, 5, 0, 0, 3, 2, 3]
***** skip layer  2
[2, 2, 3, 3, 2, 1, 5, 0, 5, 0, 0, 3, 2, 3]
***** skip layer  3
[2, 2, 3, 2, 2, 1, 5, 0, 5, 0, 0, 3, 2, 3]
***** skip layer  4
[2, 2, 3, 2, 1, 1, 5, 0, 5, 0, 0, 3, 2, 3]
***** skip layer  5
[2, 2, 3, 2, 1, 0, 5, 0, 5, 0, 0, 3, 2, 3]
***** skip layer  6
[2, 2, 3, 2, 1, 0, 4, 0, 5, 0, 0, 3, 2, 3]
optimize layer  7
backward train epoch:  122
test acc:  0.1067
forward train acc:  0.99446  and loss:  7.679309884435497
test acc:  0.9126
forward train acc:  0.99652  and loss:  4.749890057282755
test acc:  0.914
forward train acc:  0.99742  and loss:  3.357282257056795
test acc:  0.9144
forward train acc:  0.99764  and loss:  2.9398489333107136
test acc:  0.9154
forward train acc:  0.99806  and loss:  2.616097287100274
test acc:  0.9163
forward train acc:  0.99796  and loss:  2.4904735979507677
test acc:  0.9165
forward train acc:  0.99826  and loss:  2.1807112689712085
test acc:  0.9165
forward train acc:  0.99844  and loss:  1.8987377941375598
test acc:  0.9174
forward train acc:  0.99846  and loss:  1.9097278483095579
test acc:  0.9172
forward train acc:  0.99848  and loss:  1.9127742378041148
test acc:  0.9174
********** reverse layer  7  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5729166666666666  ==>  82 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.7890625  ==>  162 / 768
layer  10  :  0.8359375  ==>  126 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  8
[2, 2, 3, 2, 1, 0, 4, 5, 4, 0, 0, 3, 2, 3]
optimize layer  9
backward train epoch:  200
test acc:  0.1051
forward train acc:  0.99922  and loss:  1.0554768909933046
test acc:  0.9189
forward train acc:  0.99926  and loss:  0.9375452215899713
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5729166666666666  ==>  82 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.796875  ==>  156 / 768
layer  10  :  0.8359375  ==>  126 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  10
backward train epoch:  1817
test acc:  0.1032
forward train acc:  0.99936  and loss:  0.8749133178498596
test acc:  0.9182
forward train acc:  0.99908  and loss:  1.0366352704586461
test acc:  0.9185
forward train acc:  0.99918  and loss:  1.2426210407429608
test acc:  0.9175
forward train acc:  0.9994  and loss:  0.8644370539695956
test acc:  0.9197
forward train acc:  0.99946  and loss:  0.7333049950248096
test acc:  0.9189
forward train acc:  0.9993  and loss:  0.9638841338237398
test acc:  0.9191
forward train acc:  0.99964  and loss:  0.61395612519118
test acc:  0.9193
forward train acc:  0.99956  and loss:  0.5115365061210468
test acc:  0.9202
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5729166666666666  ==>  82 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5416666666666666  ==>  176 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.796875  ==>  156 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  11
[2, 2, 3, 2, 1, 0, 4, 5, 4, 0, 0, 2, 2, 3]
***** skip layer  12
[2, 2, 3, 2, 1, 0, 4, 5, 4, 0, 0, 2, 1, 3]
***** skip layer  13
[2, 2, 3, 2, 1, 0, 4, 5, 4, 0, 0, 2, 1, 2]
***** skip layer  0
[1, 2, 3, 2, 1, 0, 4, 5, 4, 0, 0, 2, 1, 2]
***** skip layer  1
[1, 1, 3, 2, 1, 0, 4, 5, 4, 0, 0, 2, 1, 2]
***** skip layer  2
[1, 1, 2, 2, 1, 0, 4, 5, 4, 0, 0, 2, 1, 2]
***** skip layer  3
[1, 1, 2, 1, 1, 0, 4, 5, 4, 0, 0, 2, 1, 2]
***** skip layer  4
[1, 1, 2, 1, 0, 0, 4, 5, 4, 0, 0, 2, 1, 2]
optimize layer  5
backward train epoch:  136
test acc:  0.1008
forward train acc:  0.99902  and loss:  1.3134185552771669
test acc:  0.9182
forward train acc:  0.99902  and loss:  1.24098789942218
test acc:  0.9197
forward train acc:  0.99914  and loss:  1.2040845481678843
test acc:  0.9191
forward train acc:  0.99956  and loss:  0.7244862246443518
test acc:  0.918
forward train acc:  0.99924  and loss:  0.9929336725035682
test acc:  0.9195
forward train acc:  0.99936  and loss:  0.7571775722317398
test acc:  0.9198
forward train acc:  0.9993  and loss:  0.8885631241137162
test acc:  0.9191
forward train acc:  0.9994  and loss:  0.8255609644111246
test acc:  0.921
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5729166666666666  ==>  82 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5520833333333334  ==>  172 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.796875  ==>  156 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  6
[1, 1, 2, 1, 0, 0, 3, 5, 4, 0, 0, 2, 1, 2]
***** skip layer  7
[1, 1, 2, 1, 0, 0, 3, 4, 4, 0, 0, 2, 1, 2]
***** skip layer  8
[1, 1, 2, 1, 0, 0, 3, 4, 3, 0, 0, 2, 1, 2]
optimize layer  9
backward train epoch:  720
test acc:  0.1017
forward train acc:  0.9994  and loss:  0.859734176134225
test acc:  0.9184
forward train acc:  0.999  and loss:  1.355283955927007
test acc:  0.9165
forward train acc:  0.99904  and loss:  1.2467332860687748
test acc:  0.9187
forward train acc:  0.99896  and loss:  1.2155832608550554
test acc:  0.9198
forward train acc:  0.99928  and loss:  0.8677512535941787
test acc:  0.9192
forward train acc:  0.99958  and loss:  0.7953466908656992
test acc:  0.9207
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5729166666666666  ==>  82 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5520833333333334  ==>  172 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  10
backward train epoch:  194
test acc:  0.1001
forward train acc:  0.9993  and loss:  1.0969745422626147
test acc:  0.9169
forward train acc:  0.99926  and loss:  0.9482315644563641
test acc:  0.9196
forward train acc:  0.9992  and loss:  1.0968201889190823
test acc:  0.9188
forward train acc:  0.99948  and loss:  0.793615222981316
test acc:  0.9197
forward train acc:  0.9995  and loss:  0.7902002938790247
test acc:  0.917
forward train acc:  0.99962  and loss:  0.5255450795666547
test acc:  0.9179
forward train acc:  0.99938  and loss:  0.675467000939534
test acc:  0.9182
forward train acc:  0.99956  and loss:  0.6305519466259284
test acc:  0.9182
forward train acc:  0.99968  and loss:  0.5654833872540621
test acc:  0.9184
forward train acc:  0.9996  and loss:  0.5221202452667058
test acc:  0.9189
********** reverse layer  10  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5729166666666666  ==>  82 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5520833333333334  ==>  172 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  11
[1, 1, 2, 1, 0, 0, 3, 4, 3, 0, 5, 1, 1, 2]
***** skip layer  12
[1, 1, 2, 1, 0, 0, 3, 4, 3, 0, 5, 1, 0, 2]
***** skip layer  13
[1, 1, 2, 1, 0, 0, 3, 4, 3, 0, 5, 1, 0, 1]
***** skip layer  0
[0, 1, 2, 1, 0, 0, 3, 4, 3, 0, 5, 1, 0, 1]
***** skip layer  1
[0, 0, 2, 1, 0, 0, 3, 4, 3, 0, 5, 1, 0, 1]
***** skip layer  2
[0, 0, 1, 1, 0, 0, 3, 4, 3, 0, 5, 1, 0, 1]
***** skip layer  3
[0, 0, 1, 0, 0, 0, 3, 4, 3, 0, 5, 1, 0, 1]
optimize layer  4
backward train epoch:  96
test acc:  0.0989
forward train acc:  0.99846  and loss:  2.247762451726885
test acc:  0.9161
forward train acc:  0.99876  and loss:  1.7317729333299212
test acc:  0.9147
forward train acc:  0.99838  and loss:  1.902780759424786
test acc:  0.9175
forward train acc:  0.99886  and loss:  1.3875424594443757
test acc:  0.9168
forward train acc:  0.99912  and loss:  1.1794124150183052
test acc:  0.9168
forward train acc:  0.99904  and loss:  1.3172201418492477
test acc:  0.9185
forward train acc:  0.99926  and loss:  1.160657604363223
test acc:  0.918
forward train acc:  0.99904  and loss:  1.2230714924517088
test acc:  0.918
forward train acc:  0.99934  and loss:  0.9045347573483014
test acc:  0.9181
forward train acc:  0.9992  and loss:  1.1054149903502548
test acc:  0.9191
********** reverse layer  4  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5729166666666666  ==>  82 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5520833333333334  ==>  172 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  5
backward train epoch:  382
test acc:  0.0991
forward train acc:  0.99892  and loss:  1.4548112102638697
test acc:  0.9173
forward train acc:  0.99892  and loss:  1.2069911334401695
test acc:  0.9163
forward train acc:  0.99886  and loss:  1.5765577141864924
test acc:  0.9166
forward train acc:  0.99898  and loss:  1.2888278395985253
test acc:  0.9183
forward train acc:  0.9994  and loss:  0.9146373216644861
test acc:  0.919
forward train acc:  0.99914  and loss:  1.1107661209389335
test acc:  0.9194
forward train acc:  0.99942  and loss:  0.6786699423682876
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5729166666666666  ==>  82 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5625  ==>  168 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  6
[0, 0, 1, 0, 5, 0, 2, 4, 3, 0, 5, 1, 0, 1]
***** skip layer  7
[0, 0, 1, 0, 5, 0, 2, 3, 3, 0, 5, 1, 0, 1]
***** skip layer  8
[0, 0, 1, 0, 5, 0, 2, 3, 2, 0, 5, 1, 0, 1]
optimize layer  9
backward train epoch:  90
test acc:  0.1037
forward train acc:  0.99912  and loss:  1.0475131496496033
test acc:  0.9157
forward train acc:  0.9992  and loss:  1.1251737821439747
test acc:  0.9167
forward train acc:  0.99904  and loss:  1.2143107206065906
test acc:  0.9164
forward train acc:  0.99944  and loss:  0.7472001432106481
test acc:  0.919
forward train acc:  0.9993  and loss:  0.8782489923469257
test acc:  0.9184
forward train acc:  0.99946  and loss:  0.6738423086062539
test acc:  0.9184
forward train acc:  0.99938  and loss:  0.7584712309180759
test acc:  0.9184
forward train acc:  0.9995  and loss:  0.7725152440689271
test acc:  0.9196
forward train acc:  0.9995  and loss:  0.6532310154289007
test acc:  0.9187
forward train acc:  0.9995  and loss:  0.6497272024425911
test acc:  0.9188
********** reverse layer  9  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5729166666666666  ==>  82 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5625  ==>  168 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  10
[0, 0, 1, 0, 5, 0, 2, 3, 2, 5, 4, 1, 0, 1]
***** skip layer  11
[0, 0, 1, 0, 5, 0, 2, 3, 2, 5, 4, 0, 0, 1]
optimize layer  12
backward train epoch:  126
test acc:  0.1
forward train acc:  0.1  and loss:  1009.0706422328949
test acc:  0.1
forward train acc:  0.1  and loss:  994.4932925701141
test acc:  0.1
forward train acc:  0.1  and loss:  981.6931269168854
test acc:  0.1
forward train acc:  0.1  and loss:  972.9257683753967
test acc:  0.1
forward train acc:  0.1  and loss:  967.5153434276581
test acc:  0.1
forward train acc:  0.1  and loss:  962.3774952888489
test acc:  0.1
forward train acc:  0.1  and loss:  957.5322918891907
test acc:  0.1
forward train acc:  0.1  and loss:  953.990067243576
test acc:  0.1
forward train acc:  0.1  and loss:  951.8270177841187
test acc:  0.1
forward train acc:  0.1  and loss:  949.5750789642334
test acc:  0.1
********** reverse layer  12  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5729166666666666  ==>  82 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5625  ==>  168 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  13
[0, 0, 1, 0, 5, 0, 2, 3, 2, 5, 4, 0, 5, 0]
optimize layer  0
backward train epoch:  28
test acc:  0.1316
forward train acc:  0.99918  and loss:  0.9461756143718958
test acc:  0.918
forward train acc:  0.99926  and loss:  1.0171240936615504
test acc:  0.9178
forward train acc:  0.9995  and loss:  0.7514568597252946
test acc:  0.9168
forward train acc:  0.9994  and loss:  0.798454962001415
test acc:  0.9173
forward train acc:  0.9994  and loss:  0.7189174412342254
test acc:  0.919
forward train acc:  0.99944  and loss:  0.6523464106721804
test acc:  0.9189
forward train acc:  0.99934  and loss:  0.8308185281130136
test acc:  0.9175
forward train acc:  0.99942  and loss:  0.7770764057058841
test acc:  0.9182
forward train acc:  0.99938  and loss:  0.7575870781001868
test acc:  0.9174
forward train acc:  0.99946  and loss:  0.6835872858064249
test acc:  0.9174
********** reverse layer  0  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5729166666666666  ==>  82 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5625  ==>  168 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  1
backward train epoch:  468
test acc:  0.0992
forward train acc:  0.97632  and loss:  38.549403046257794
test acc:  0.9013
forward train acc:  0.98696  and loss:  16.988933376269415
test acc:  0.9047
forward train acc:  0.99064  and loss:  12.230922904913314
test acc:  0.9076
forward train acc:  0.99278  and loss:  9.29510468407534
test acc:  0.9098
forward train acc:  0.99422  and loss:  7.357168735587038
test acc:  0.9097
forward train acc:  0.99416  and loss:  7.21075949666556
test acc:  0.9106
forward train acc:  0.99508  and loss:  6.288663887302391
test acc:  0.9108
forward train acc:  0.99536  and loss:  5.67523323954083
test acc:  0.9115
forward train acc:  0.99606  and loss:  5.066258322854992
test acc:  0.9106
forward train acc:  0.99592  and loss:  4.98606484872289
test acc:  0.9114
********** reverse layer  1  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5729166666666666  ==>  82 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5625  ==>  168 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  2
[5, 5, 0, 0, 5, 0, 2, 3, 2, 5, 4, 0, 5, 0]
optimize layer  3
backward train epoch:  1
test acc:  0.1032
forward train acc:  0.99816  and loss:  2.312679840659257
test acc:  0.9143
forward train acc:  0.99868  and loss:  1.6809670731890947
test acc:  0.9162
forward train acc:  0.99876  and loss:  1.5253594787209295
test acc:  0.9147
forward train acc:  0.99922  and loss:  1.0468940887076315
test acc:  0.9149
forward train acc:  0.99916  and loss:  1.1030448030214757
test acc:  0.9147
forward train acc:  0.99902  and loss:  1.206790719385026
test acc:  0.9153
forward train acc:  0.99916  and loss:  1.0674828071059892
test acc:  0.9142
forward train acc:  0.99918  and loss:  1.0537943696108414
test acc:  0.9154
forward train acc:  0.99932  and loss:  0.9876132698555011
test acc:  0.9157
forward train acc:  0.9994  and loss:  0.724213989902637
test acc:  0.9164
********** reverse layer  3  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5729166666666666  ==>  82 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5625  ==>  168 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  4
[5, 5, 0, 5, 4, 0, 2, 3, 2, 5, 4, 0, 5, 0]
optimize layer  5
backward train epoch:  180
test acc:  0.0999
forward train acc:  0.99904  and loss:  1.1742288564855698
test acc:  0.9134
forward train acc:  0.99902  and loss:  1.2925390800082823
test acc:  0.9129
forward train acc:  0.99896  and loss:  1.2713373543519992
test acc:  0.9143
forward train acc:  0.99916  and loss:  1.1395027191902045
test acc:  0.9155
forward train acc:  0.99934  and loss:  0.7675927399322973
test acc:  0.915
forward train acc:  0.99926  and loss:  0.8634224700945197
test acc:  0.915
forward train acc:  0.9994  and loss:  0.7902259336551651
test acc:  0.9148
forward train acc:  0.9996  and loss:  0.6311359768296825
test acc:  0.9153
forward train acc:  0.99928  and loss:  0.8957288667370449
test acc:  0.9151
forward train acc:  0.9995  and loss:  0.7075586977880448
test acc:  0.915
********** reverse layer  5  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5729166666666666  ==>  82 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5625  ==>  168 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  6
[5, 5, 0, 5, 4, 5, 1, 3, 2, 5, 4, 0, 5, 0]
***** skip layer  7
[5, 5, 0, 5, 4, 5, 1, 2, 2, 5, 4, 0, 5, 0]
***** skip layer  8
[5, 5, 0, 5, 4, 5, 1, 2, 1, 5, 4, 0, 5, 0]
***** skip layer  9
[5, 5, 0, 5, 4, 5, 1, 2, 1, 4, 4, 0, 5, 0]
***** skip layer  10
[5, 5, 0, 5, 4, 5, 1, 2, 1, 4, 3, 0, 5, 0]
optimize layer  11
backward train epoch:  50
test acc:  0.1019
forward train acc:  0.99948  and loss:  0.6394172604195774
test acc:  0.9137
forward train acc:  0.99926  and loss:  1.0644530370482244
test acc:  0.9146
forward train acc:  0.99964  and loss:  0.6769221585709602
test acc:  0.9171
forward train acc:  0.9995  and loss:  0.5924010283197276
test acc:  0.9167
forward train acc:  0.9995  and loss:  0.628194688091753
test acc:  0.9171
forward train acc:  0.99946  and loss:  0.6435646613826975
test acc:  0.9158
forward train acc:  0.99958  and loss:  0.5500766001860029
test acc:  0.9161
forward train acc:  0.99956  and loss:  0.5621946129176649
test acc:  0.9166
forward train acc:  0.99966  and loss:  0.5304596418427536
test acc:  0.9172
forward train acc:  0.9997  and loss:  0.47319074743427336
test acc:  0.9178
********** reverse layer  11  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5729166666666666  ==>  82 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5625  ==>  168 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  12
[5, 5, 0, 5, 4, 5, 1, 2, 1, 4, 3, 5, 4, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.1292
forward train acc:  0.8994  and loss:  136.92329931259155
test acc:  0.8217
forward train acc:  0.89938  and loss:  130.80963575094938
test acc:  0.8239
forward train acc:  0.89928  and loss:  128.07391089200974
test acc:  0.822
forward train acc:  0.89954  and loss:  125.67427168786526
test acc:  0.8244
forward train acc:  0.89966  and loss:  124.07547201216221
test acc:  0.8241
forward train acc:  0.8996  and loss:  122.76154731214046
test acc:  0.8237
forward train acc:  0.89968  and loss:  121.39429019391537
test acc:  0.825
forward train acc:  0.89966  and loss:  120.24123095721006
test acc:  0.8254
forward train acc:  0.8996  and loss:  119.59935973584652
test acc:  0.8233
forward train acc:  0.8997  and loss:  118.66973636299372
test acc:  0.8239
********** reverse layer  13  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5729166666666666  ==>  82 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5625  ==>  168 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  0
[4, 5, 0, 5, 4, 5, 1, 2, 1, 4, 3, 5, 4, 5]
***** skip layer  1
[4, 4, 0, 5, 4, 5, 1, 2, 1, 4, 3, 5, 4, 5]
optimize layer  2
backward train epoch:  36
test acc:  0.1045
forward train acc:  0.999  and loss:  1.3018131129210815
test acc:  0.9177
forward train acc:  0.99882  and loss:  1.7036430133739486
test acc:  0.917
forward train acc:  0.99946  and loss:  0.9279407108188025
test acc:  0.9176
forward train acc:  0.9994  and loss:  0.8244732028979342
test acc:  0.9179
forward train acc:  0.99952  and loss:  0.7790951721253805
test acc:  0.9192
forward train acc:  0.99942  and loss:  0.807674489042256
test acc:  0.9202
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5625  ==>  168 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  3
[4, 4, 0, 4, 4, 5, 1, 2, 1, 4, 3, 5, 4, 5]
***** skip layer  4
[4, 4, 0, 4, 3, 5, 1, 2, 1, 4, 3, 5, 4, 5]
***** skip layer  5
[4, 4, 0, 4, 3, 4, 1, 2, 1, 4, 3, 5, 4, 5]
***** skip layer  6
[4, 4, 0, 4, 3, 4, 0, 2, 1, 4, 3, 5, 4, 5]
***** skip layer  7
[4, 4, 0, 4, 3, 4, 0, 1, 1, 4, 3, 5, 4, 5]
***** skip layer  8
[4, 4, 0, 4, 3, 4, 0, 1, 0, 4, 3, 5, 4, 5]
***** skip layer  9
[4, 4, 0, 4, 3, 4, 0, 1, 0, 3, 3, 5, 4, 5]
***** skip layer  10
[4, 4, 0, 4, 3, 4, 0, 1, 0, 3, 2, 5, 4, 5]
***** skip layer  11
[4, 4, 0, 4, 3, 4, 0, 1, 0, 3, 2, 4, 4, 5]
***** skip layer  12
[4, 4, 0, 4, 3, 4, 0, 1, 0, 3, 2, 4, 3, 5]
***** skip layer  13
[4, 4, 0, 4, 3, 4, 0, 1, 0, 3, 2, 4, 3, 4]
***** skip layer  0
[3, 4, 0, 4, 3, 4, 0, 1, 0, 3, 2, 4, 3, 4]
***** skip layer  1
[3, 3, 0, 4, 3, 4, 0, 1, 0, 3, 2, 4, 3, 4]
optimize layer  2
backward train epoch:  74
test acc:  0.1235
forward train acc:  0.99876  and loss:  1.6262991658295505
test acc:  0.9161
forward train acc:  0.99892  and loss:  1.4506386462235241
test acc:  0.9162
forward train acc:  0.9991  and loss:  1.209486527244735
test acc:  0.9151
forward train acc:  0.99918  and loss:  1.0650235731736757
test acc:  0.9185
forward train acc:  0.99924  and loss:  0.9084757610107772
test acc:  0.918
forward train acc:  0.99926  and loss:  1.04449577111518
test acc:  0.9185
forward train acc:  0.99932  and loss:  0.8615630683198106
test acc:  0.9185
forward train acc:  0.99932  and loss:  0.876174013093987
test acc:  0.9189
forward train acc:  0.99938  and loss:  0.7627098042721627
test acc:  0.9192
forward train acc:  0.99946  and loss:  0.7943305491426145
test acc:  0.9183
********** reverse layer  2  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5625  ==>  168 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  3
[3, 3, 5, 3, 3, 4, 0, 1, 0, 3, 2, 4, 3, 4]
***** skip layer  4
[3, 3, 5, 3, 2, 4, 0, 1, 0, 3, 2, 4, 3, 4]
***** skip layer  5
[3, 3, 5, 3, 2, 3, 0, 1, 0, 3, 2, 4, 3, 4]
optimize layer  6
backward train epoch:  194
test acc:  0.1001
forward train acc:  0.99918  and loss:  1.0099403267231537
test acc:  0.9147
forward train acc:  0.99894  and loss:  1.4144628251087852
test acc:  0.916
forward train acc:  0.99904  and loss:  1.4221503981098067
test acc:  0.9173
forward train acc:  0.9992  and loss:  0.9164460028841859
test acc:  0.9159
forward train acc:  0.9996  and loss:  0.6578699896926992
test acc:  0.9164
forward train acc:  0.99948  and loss:  0.7205174598202575
test acc:  0.9163
forward train acc:  0.9996  and loss:  0.4930309621267952
test acc:  0.9184
forward train acc:  0.99942  and loss:  0.7090100144268945
test acc:  0.9186
forward train acc:  0.99968  and loss:  0.5198471320909448
test acc:  0.9197
forward train acc:  0.99952  and loss:  0.7260597593849525
test acc:  0.9194
********** reverse layer  6  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5625  ==>  168 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  7
[3, 3, 5, 3, 2, 3, 5, 0, 0, 3, 2, 4, 3, 4]
optimize layer  8
backward train epoch:  182
test acc:  0.1
forward train acc:  0.98324  and loss:  20.995454482268542
test acc:  0.9072
forward train acc:  0.9942  and loss:  7.4949897545157
test acc:  0.9104
forward train acc:  0.9956  and loss:  4.933384734729771
test acc:  0.9111
forward train acc:  0.99644  and loss:  4.382851996109821
test acc:  0.9109
forward train acc:  0.99736  and loss:  3.5402600537636317
test acc:  0.9118
forward train acc:  0.99716  and loss:  3.358344757143641
test acc:  0.9117
forward train acc:  0.99734  and loss:  3.282849770388566
test acc:  0.9142
forward train acc:  0.99792  and loss:  2.8461268224345986
test acc:  0.9125
forward train acc:  0.99788  and loss:  2.576643346634228
test acc:  0.9137
forward train acc:  0.99822  and loss:  2.3667394074145705
test acc:  0.9132
********** reverse layer  8  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5625  ==>  168 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  9
[3, 3, 5, 3, 2, 3, 5, 0, 5, 2, 2, 4, 3, 4]
***** skip layer  10
[3, 3, 5, 3, 2, 3, 5, 0, 5, 2, 1, 4, 3, 4]
***** skip layer  11
[3, 3, 5, 3, 2, 3, 5, 0, 5, 2, 1, 3, 3, 4]
***** skip layer  12
[3, 3, 5, 3, 2, 3, 5, 0, 5, 2, 1, 3, 2, 4]
***** skip layer  13
[3, 3, 5, 3, 2, 3, 5, 0, 5, 2, 1, 3, 2, 3]
***** skip layer  0
[2, 3, 5, 3, 2, 3, 5, 0, 5, 2, 1, 3, 2, 3]
***** skip layer  1
[2, 2, 5, 3, 2, 3, 5, 0, 5, 2, 1, 3, 2, 3]
***** skip layer  2
[2, 2, 4, 3, 2, 3, 5, 0, 5, 2, 1, 3, 2, 3]
***** skip layer  3
[2, 2, 4, 2, 2, 3, 5, 0, 5, 2, 1, 3, 2, 3]
***** skip layer  4
[2, 2, 4, 2, 1, 3, 5, 0, 5, 2, 1, 3, 2, 3]
***** skip layer  5
[2, 2, 4, 2, 1, 2, 5, 0, 5, 2, 1, 3, 2, 3]
***** skip layer  6
[2, 2, 4, 2, 1, 2, 4, 0, 5, 2, 1, 3, 2, 3]
optimize layer  7
backward train epoch:  639
test acc:  0.0896
forward train acc:  0.9946  and loss:  7.9144991227658466
test acc:  0.9116
forward train acc:  0.99696  and loss:  4.028248333896045
test acc:  0.9131
forward train acc:  0.99756  and loss:  3.2597423072438687
test acc:  0.9144
forward train acc:  0.9984  and loss:  2.115440870868042
test acc:  0.9142
forward train acc:  0.9983  and loss:  2.323239960387582
test acc:  0.9149
forward train acc:  0.99844  and loss:  1.9061624180321814
test acc:  0.9156
forward train acc:  0.99866  and loss:  1.635540409537498
test acc:  0.9144
forward train acc:  0.99878  and loss:  1.5841983294376405
test acc:  0.9161
forward train acc:  0.9988  and loss:  1.6367015605501365
test acc:  0.9145
forward train acc:  0.99868  and loss:  1.5909628559311386
test acc:  0.9158
********** reverse layer  7  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5625  ==>  168 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  8
[2, 2, 4, 2, 1, 2, 4, 5, 4, 2, 1, 3, 2, 3]
***** skip layer  9
[2, 2, 4, 2, 1, 2, 4, 5, 4, 1, 1, 3, 2, 3]
***** skip layer  10
[2, 2, 4, 2, 1, 2, 4, 5, 4, 1, 0, 3, 2, 3]
***** skip layer  11
[2, 2, 4, 2, 1, 2, 4, 5, 4, 1, 0, 2, 2, 3]
***** skip layer  12
[2, 2, 4, 2, 1, 2, 4, 5, 4, 1, 0, 2, 1, 3]
***** skip layer  13
[2, 2, 4, 2, 1, 2, 4, 5, 4, 1, 0, 2, 1, 2]
***** skip layer  0
[1, 2, 4, 2, 1, 2, 4, 5, 4, 1, 0, 2, 1, 2]
***** skip layer  1
[1, 1, 4, 2, 1, 2, 4, 5, 4, 1, 0, 2, 1, 2]
***** skip layer  2
[1, 1, 3, 2, 1, 2, 4, 5, 4, 1, 0, 2, 1, 2]
***** skip layer  3
[1, 1, 3, 1, 1, 2, 4, 5, 4, 1, 0, 2, 1, 2]
***** skip layer  4
[1, 1, 3, 1, 0, 2, 4, 5, 4, 1, 0, 2, 1, 2]
***** skip layer  5
[1, 1, 3, 1, 0, 1, 4, 5, 4, 1, 0, 2, 1, 2]
***** skip layer  6
[1, 1, 3, 1, 0, 1, 3, 5, 4, 1, 0, 2, 1, 2]
***** skip layer  7
[1, 1, 3, 1, 0, 1, 3, 4, 4, 1, 0, 2, 1, 2]
***** skip layer  8
[1, 1, 3, 1, 0, 1, 3, 4, 3, 1, 0, 2, 1, 2]
***** skip layer  9
[1, 1, 3, 1, 0, 1, 3, 4, 3, 0, 0, 2, 1, 2]
optimize layer  10
backward train epoch:  88
test acc:  0.1067
forward train acc:  0.99934  and loss:  0.9092533419898245
test acc:  0.9194
forward train acc:  0.99932  and loss:  0.8383473473950289
test acc:  0.9184
forward train acc:  0.99954  and loss:  0.5025551958970027
test acc:  0.9177
forward train acc:  0.99956  and loss:  0.5890389968990348
test acc:  0.9192
forward train acc:  0.9997  and loss:  0.41545040826895274
test acc:  0.9178
forward train acc:  0.99968  and loss:  0.5140990476938896
test acc:  0.9184
forward train acc:  0.99972  and loss:  0.38523228261692566
test acc:  0.9175
forward train acc:  0.99974  and loss:  0.35399759282154264
test acc:  0.9182
forward train acc:  0.99964  and loss:  0.5143854824927985
test acc:  0.9183
forward train acc:  0.9997  and loss:  0.37930946092819795
test acc:  0.9179
********** reverse layer  10  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5625  ==>  168 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  11
[1, 1, 3, 1, 0, 1, 3, 4, 3, 0, 5, 1, 1, 2]
***** skip layer  12
[1, 1, 3, 1, 0, 1, 3, 4, 3, 0, 5, 1, 0, 2]
***** skip layer  13
[1, 1, 3, 1, 0, 1, 3, 4, 3, 0, 5, 1, 0, 1]
***** skip layer  0
[0, 1, 3, 1, 0, 1, 3, 4, 3, 0, 5, 1, 0, 1]
***** skip layer  1
[0, 0, 3, 1, 0, 1, 3, 4, 3, 0, 5, 1, 0, 1]
***** skip layer  2
[0, 0, 2, 1, 0, 1, 3, 4, 3, 0, 5, 1, 0, 1]
***** skip layer  3
[0, 0, 2, 0, 0, 1, 3, 4, 3, 0, 5, 1, 0, 1]
optimize layer  4
backward train epoch:  894
test acc:  0.0992
forward train acc:  0.99858  and loss:  1.8050920937675983
test acc:  0.9172
forward train acc:  0.99888  and loss:  1.4971752754354384
test acc:  0.9156
forward train acc:  0.99886  and loss:  1.5069368423428386
test acc:  0.9157
forward train acc:  0.99892  and loss:  1.407723389158491
test acc:  0.9159
forward train acc:  0.99938  and loss:  0.8492645499645732
test acc:  0.9179
forward train acc:  0.99928  and loss:  0.788840172055643
test acc:  0.9172
forward train acc:  0.99942  and loss:  0.7436312049685512
test acc:  0.9172
forward train acc:  0.99936  and loss:  0.7589794789528241
test acc:  0.9188
forward train acc:  0.99914  and loss:  0.9100068732732325
test acc:  0.9179
forward train acc:  0.99936  and loss:  0.8076330610201694
test acc:  0.9182
********** reverse layer  4  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5625  ==>  168 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  5
[0, 0, 2, 0, 5, 0, 3, 4, 3, 0, 5, 1, 0, 1]
***** skip layer  6
[0, 0, 2, 0, 5, 0, 2, 4, 3, 0, 5, 1, 0, 1]
***** skip layer  7
[0, 0, 2, 0, 5, 0, 2, 3, 3, 0, 5, 1, 0, 1]
***** skip layer  8
[0, 0, 2, 0, 5, 0, 2, 3, 2, 0, 5, 1, 0, 1]
optimize layer  9
backward train epoch:  111
test acc:  0.0999
forward train acc:  0.99938  and loss:  0.801804386148433
test acc:  0.9172
forward train acc:  0.99958  and loss:  0.5601619086955907
test acc:  0.9186
forward train acc:  0.99946  and loss:  0.580329054791946
test acc:  0.915
forward train acc:  0.99956  and loss:  0.63343059241015
test acc:  0.9189
forward train acc:  0.99964  and loss:  0.4688123333398835
test acc:  0.9178
forward train acc:  0.99964  and loss:  0.43866697110934183
test acc:  0.9175
forward train acc:  0.99946  and loss:  0.6815309714002069
test acc:  0.9186
forward train acc:  0.99962  and loss:  0.4218805730197346
test acc:  0.9183
forward train acc:  0.99966  and loss:  0.3567212457419373
test acc:  0.9172
forward train acc:  0.99984  and loss:  0.2750609276808973
test acc:  0.9189
********** reverse layer  9  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5625  ==>  168 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  10
[0, 0, 2, 0, 5, 0, 2, 3, 2, 5, 4, 1, 0, 1]
***** skip layer  11
[0, 0, 2, 0, 5, 0, 2, 3, 2, 5, 4, 0, 0, 1]
optimize layer  12
backward train epoch:  285
test acc:  0.1
forward train acc:  0.1  and loss:  941.0051348209381
test acc:  0.1
forward train acc:  0.1  and loss:  933.0996644496918
test acc:  0.1
forward train acc:  0.1  and loss:  926.5203619003296
test acc:  0.1
forward train acc:  0.1  and loss:  922.2860126495361
test acc:  0.1
forward train acc:  0.1  and loss:  919.7732467651367
test acc:  0.1
forward train acc:  0.1  and loss:  917.4564709663391
test acc:  0.1
forward train acc:  0.1  and loss:  915.3617568016052
test acc:  0.1
forward train acc:  0.1  and loss:  913.9114453792572
test acc:  0.1
forward train acc:  0.1  and loss:  912.9864983558655
test acc:  0.1
forward train acc:  0.1  and loss:  912.1175680160522
test acc:  0.1
********** reverse layer  12  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5625  ==>  168 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  13
[0, 0, 2, 0, 5, 0, 2, 3, 2, 5, 4, 0, 5, 0]
optimize layer  0
backward train epoch:  890
test acc:  0.0997
forward train acc:  0.73512  and loss:  454.58111131191254
test acc:  0.7667
forward train acc:  0.8181  and loss:  227.7211557328701
test acc:  0.8054
forward train acc:  0.84818  and loss:  187.87383198738098
test acc:  0.8245
forward train acc:  0.86834  and loss:  162.98478624224663
test acc:  0.8296
forward train acc:  0.87736  and loss:  151.36092492938042
test acc:  0.8349
forward train acc:  0.88678  and loss:  140.49459867179394
test acc:  0.8383
forward train acc:  0.89256  and loss:  130.61870224028826
test acc:  0.8445
forward train acc:  0.89854  and loss:  125.02071586251259
test acc:  0.8453
forward train acc:  0.90208  and loss:  118.33331601321697
test acc:  0.8464
forward train acc:  0.90436  and loss:  116.581960901618
test acc:  0.8499
********** reverse layer  0  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5625  ==>  168 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  1
backward train epoch:  194
test acc:  0.1
forward train acc:  0.9956  and loss:  6.061420474783517
test acc:  0.9151
forward train acc:  0.9982  and loss:  2.3611397805507295
test acc:  0.9178
forward train acc:  0.99868  and loss:  1.6637146363500506
test acc:  0.9156
forward train acc:  0.99892  and loss:  1.4135321151989046
test acc:  0.9162
forward train acc:  0.99912  and loss:  1.2157800265122205
test acc:  0.9173
forward train acc:  0.9993  and loss:  0.9871711642190348
test acc:  0.917
forward train acc:  0.99908  and loss:  1.0447342806728557
test acc:  0.9165
forward train acc:  0.99932  and loss:  0.8582474875147454
test acc:  0.9181
forward train acc:  0.99924  and loss:  0.9249719229701441
test acc:  0.9163
forward train acc:  0.9995  and loss:  0.6221508634334896
test acc:  0.917
********** reverse layer  1  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5625  ==>  168 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  2
[5, 5, 1, 0, 5, 0, 2, 3, 2, 5, 4, 0, 5, 0]
optimize layer  3
backward train epoch:  600
test acc:  0.1004
forward train acc:  0.9986  and loss:  1.6712850753974635
test acc:  0.9148
forward train acc:  0.99884  and loss:  1.662855472357478
test acc:  0.9172
forward train acc:  0.99892  and loss:  1.313398552709259
test acc:  0.9169
forward train acc:  0.99932  and loss:  0.9947798039647751
test acc:  0.9165
forward train acc:  0.9994  and loss:  0.750793492392404
test acc:  0.9171
forward train acc:  0.9994  and loss:  0.7826994701026706
test acc:  0.9188
forward train acc:  0.99926  and loss:  1.1034670358640142
test acc:  0.9189
forward train acc:  0.99942  and loss:  0.6736672782571986
test acc:  0.919
forward train acc:  0.9995  and loss:  0.6154698788595852
test acc:  0.9183
forward train acc:  0.99946  and loss:  0.6455154844443314
test acc:  0.9188
********** reverse layer  3  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5625  ==>  168 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  4
[5, 5, 1, 5, 4, 0, 2, 3, 2, 5, 4, 0, 5, 0]
optimize layer  5
backward train epoch:  1040
test acc:  0.1073
forward train acc:  0.99878  and loss:  1.2691651680506766
test acc:  0.9177
forward train acc:  0.99908  and loss:  1.157236071259831
test acc:  0.9186
forward train acc:  0.99926  and loss:  1.0569484106381424
test acc:  0.9192
forward train acc:  0.99932  and loss:  0.8732346698088804
test acc:  0.9203
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9010416666666666  ==>  76 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  6
[5, 5, 1, 5, 4, 0, 1, 3, 2, 5, 4, 0, 5, 0]
***** skip layer  7
[5, 5, 1, 5, 4, 0, 1, 2, 2, 5, 4, 0, 5, 0]
***** skip layer  8
[5, 5, 1, 5, 4, 0, 1, 2, 1, 5, 4, 0, 5, 0]
***** skip layer  9
[5, 5, 1, 5, 4, 0, 1, 2, 1, 4, 4, 0, 5, 0]
***** skip layer  10
[5, 5, 1, 5, 4, 0, 1, 2, 1, 4, 3, 0, 5, 0]
optimize layer  11
backward train epoch:  207
test acc:  0.1
forward train acc:  0.9991  and loss:  1.0891276603360893
test acc:  0.9175
forward train acc:  0.9992  and loss:  1.1488536841643509
test acc:  0.9171
forward train acc:  0.99926  and loss:  1.08041475340724
test acc:  0.9176
forward train acc:  0.99934  and loss:  0.8375501660921145
test acc:  0.9186
forward train acc:  0.9994  and loss:  0.7094650374929188
test acc:  0.9185
forward train acc:  0.9993  and loss:  0.785094837199722
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9088541666666666  ==>  70 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  12
[5, 5, 1, 5, 4, 0, 1, 2, 1, 4, 3, 0, 4, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.1
forward train acc:  0.7801  and loss:  337.5582038164139
test acc:  0.7278
forward train acc:  0.79904  and loss:  301.22200658917427
test acc:  0.7295
forward train acc:  0.79934  and loss:  290.3604830503464
test acc:  0.7298
forward train acc:  0.79918  and loss:  282.7078697681427
test acc:  0.729
forward train acc:  0.79932  and loss:  277.97693634033203
test acc:  0.7298
forward train acc:  0.79924  and loss:  273.11826848983765
test acc:  0.7294
forward train acc:  0.79928  and loss:  268.3133284151554
test acc:  0.7304
forward train acc:  0.79968  and loss:  264.2219235897064
test acc:  0.7291
forward train acc:  0.79968  and loss:  261.9570477902889
test acc:  0.7305
forward train acc:  0.79952  and loss:  259.8843592107296
test acc:  0.7302
********** reverse layer  13  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9088541666666666  ==>  70 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  0
[4, 5, 1, 5, 4, 0, 1, 2, 1, 4, 3, 0, 4, 5]
***** skip layer  1
[4, 4, 1, 5, 4, 0, 1, 2, 1, 4, 3, 0, 4, 5]
***** skip layer  2
[4, 4, 0, 5, 4, 0, 1, 2, 1, 4, 3, 0, 4, 5]
***** skip layer  3
[4, 4, 0, 4, 4, 0, 1, 2, 1, 4, 3, 0, 4, 5]
***** skip layer  4
[4, 4, 0, 4, 3, 0, 1, 2, 1, 4, 3, 0, 4, 5]
optimize layer  5
backward train epoch:  449
test acc:  0.1319
forward train acc:  0.9992  and loss:  1.2285182315099519
test acc:  0.9194
forward train acc:  0.99888  and loss:  1.5229565375775564
test acc:  0.9173
forward train acc:  0.99888  and loss:  1.4883478260017
test acc:  0.9164
forward train acc:  0.99888  and loss:  1.5339116545510478
test acc:  0.9159
forward train acc:  0.99906  and loss:  1.2162892566993833
test acc:  0.9169
forward train acc:  0.9993  and loss:  1.0799311783339363
test acc:  0.9195
forward train acc:  0.99906  and loss:  1.22346758338972
test acc:  0.9191
forward train acc:  0.9993  and loss:  0.845550216443371
test acc:  0.9187
forward train acc:  0.99936  and loss:  0.7272546034655534
test acc:  0.9188
forward train acc:  0.9993  and loss:  1.0055532524711452
test acc:  0.9185
********** reverse layer  5  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9088541666666666  ==>  70 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  6
[4, 4, 0, 4, 3, 5, 0, 2, 1, 4, 3, 0, 4, 5]
***** skip layer  7
[4, 4, 0, 4, 3, 5, 0, 1, 1, 4, 3, 0, 4, 5]
***** skip layer  8
[4, 4, 0, 4, 3, 5, 0, 1, 0, 4, 3, 0, 4, 5]
***** skip layer  9
[4, 4, 0, 4, 3, 5, 0, 1, 0, 3, 3, 0, 4, 5]
***** skip layer  10
[4, 4, 0, 4, 3, 5, 0, 1, 0, 3, 2, 0, 4, 5]
optimize layer  11
backward train epoch:  243
test acc:  0.0849
forward train acc:  0.99944  and loss:  0.8441764479794074
test acc:  0.9173
forward train acc:  0.99944  and loss:  0.8214529874967411
test acc:  0.9182
forward train acc:  0.99932  and loss:  0.848568487999728
test acc:  0.9163
forward train acc:  0.9993  and loss:  0.8679166216752492
test acc:  0.9192
forward train acc:  0.99952  and loss:  0.7807654316275148
test acc:  0.9185
forward train acc:  0.99936  and loss:  0.9095241314498708
test acc:  0.9199
forward train acc:  0.99958  and loss:  0.5552986217371654
test acc:  0.9197
forward train acc:  0.9996  and loss:  0.5459508169878973
test acc:  0.9182
forward train acc:  0.99968  and loss:  0.44543917807459366
test acc:  0.9179
forward train acc:  0.9996  and loss:  0.4803208408266073
test acc:  0.9189
********** reverse layer  11  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9088541666666666  ==>  70 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  12
[4, 4, 0, 4, 3, 5, 0, 1, 0, 3, 2, 5, 3, 5]
***** skip layer  13
[4, 4, 0, 4, 3, 5, 0, 1, 0, 3, 2, 5, 3, 4]
***** skip layer  0
[3, 4, 0, 4, 3, 5, 0, 1, 0, 3, 2, 5, 3, 4]
***** skip layer  1
[3, 3, 0, 4, 3, 5, 0, 1, 0, 3, 2, 5, 3, 4]
optimize layer  2
backward train epoch:  286
test acc:  0.1
forward train acc:  0.99928  and loss:  0.8381076999503421
test acc:  0.9156
forward train acc:  0.99914  and loss:  0.985747923143208
test acc:  0.9171
forward train acc:  0.9993  and loss:  0.973483882684377
test acc:  0.9179
forward train acc:  0.99926  and loss:  1.01801828268799
test acc:  0.9188
forward train acc:  0.99936  and loss:  0.8869000218110159
test acc:  0.9164
forward train acc:  0.9995  and loss:  0.6187370998086408
test acc:  0.9169
forward train acc:  0.99956  and loss:  0.5644872520933859
test acc:  0.9189
forward train acc:  0.99956  and loss:  0.6171016428852454
test acc:  0.9179
forward train acc:  0.9996  and loss:  0.5469406510237604
test acc:  0.9182
forward train acc:  0.99964  and loss:  0.5818019137368537
test acc:  0.9186
********** reverse layer  2  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9088541666666666  ==>  70 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  3
[3, 3, 5, 3, 3, 5, 0, 1, 0, 3, 2, 5, 3, 4]
***** skip layer  4
[3, 3, 5, 3, 2, 5, 0, 1, 0, 3, 2, 5, 3, 4]
***** skip layer  5
[3, 3, 5, 3, 2, 4, 0, 1, 0, 3, 2, 5, 3, 4]
optimize layer  6
backward train epoch:  109
test acc:  0.1101
forward train acc:  0.9991  and loss:  0.9300586600438692
test acc:  0.9181
forward train acc:  0.99926  and loss:  0.9251230087247677
test acc:  0.9159
forward train acc:  0.99918  and loss:  0.8719003638834693
test acc:  0.9156
forward train acc:  0.99938  and loss:  0.7994270859780954
test acc:  0.9171
forward train acc:  0.99954  and loss:  0.5308884886690066
test acc:  0.9186
forward train acc:  0.99956  and loss:  0.5494863020358025
test acc:  0.9179
forward train acc:  0.99946  and loss:  0.6476671031268779
test acc:  0.9176
forward train acc:  0.99954  and loss:  0.7025364009823534
test acc:  0.9173
forward train acc:  0.99954  and loss:  0.5878746652015252
test acc:  0.9183
forward train acc:  0.99962  and loss:  0.46126234789699083
test acc:  0.9187
********** reverse layer  6  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9088541666666666  ==>  70 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  7
[3, 3, 5, 3, 2, 4, 5, 0, 0, 3, 2, 5, 3, 4]
optimize layer  8
backward train epoch:  73
test acc:  0.1
forward train acc:  0.99746  and loss:  3.4495084378140746
test acc:  0.9129
forward train acc:  0.99846  and loss:  2.0897656582819764
test acc:  0.9146
forward train acc:  0.99856  and loss:  2.0123782430309802
test acc:  0.9154
forward train acc:  0.99872  and loss:  1.5985388085828163
test acc:  0.915
forward train acc:  0.99892  and loss:  1.2982612744672224
test acc:  0.9164
forward train acc:  0.99898  and loss:  1.1423996896774042
test acc:  0.9177
forward train acc:  0.99912  and loss:  1.3560149642144097
test acc:  0.9162
forward train acc:  0.99916  and loss:  1.3324084848718485
test acc:  0.9177
forward train acc:  0.99926  and loss:  1.031861403840594
test acc:  0.9155
forward train acc:  0.99926  and loss:  0.9247019878530409
test acc:  0.9157
********** reverse layer  8  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9088541666666666  ==>  70 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  9
[3, 3, 5, 3, 2, 4, 5, 0, 5, 2, 2, 5, 3, 4]
***** skip layer  10
[3, 3, 5, 3, 2, 4, 5, 0, 5, 2, 1, 5, 3, 4]
***** skip layer  11
[3, 3, 5, 3, 2, 4, 5, 0, 5, 2, 1, 4, 3, 4]
***** skip layer  12
[3, 3, 5, 3, 2, 4, 5, 0, 5, 2, 1, 4, 2, 4]
***** skip layer  13
[3, 3, 5, 3, 2, 4, 5, 0, 5, 2, 1, 4, 2, 3]
***** skip layer  0
[2, 3, 5, 3, 2, 4, 5, 0, 5, 2, 1, 4, 2, 3]
***** skip layer  1
[2, 2, 5, 3, 2, 4, 5, 0, 5, 2, 1, 4, 2, 3]
***** skip layer  2
[2, 2, 4, 3, 2, 4, 5, 0, 5, 2, 1, 4, 2, 3]
***** skip layer  3
[2, 2, 4, 2, 2, 4, 5, 0, 5, 2, 1, 4, 2, 3]
***** skip layer  4
[2, 2, 4, 2, 1, 4, 5, 0, 5, 2, 1, 4, 2, 3]
***** skip layer  5
[2, 2, 4, 2, 1, 3, 5, 0, 5, 2, 1, 4, 2, 3]
***** skip layer  6
[2, 2, 4, 2, 1, 3, 4, 0, 5, 2, 1, 4, 2, 3]
optimize layer  7
backward train epoch:  120
test acc:  0.1028
forward train acc:  0.99504  and loss:  7.235641849692911
test acc:  0.9119
forward train acc:  0.99774  and loss:  3.1164708393334877
test acc:  0.9139
forward train acc:  0.99812  and loss:  2.283061379712308
test acc:  0.9145
forward train acc:  0.99802  and loss:  2.2402872592792846
test acc:  0.9152
forward train acc:  0.99844  and loss:  1.9739726273110136
test acc:  0.9147
forward train acc:  0.99874  and loss:  1.5626053744053934
test acc:  0.9148
forward train acc:  0.99894  and loss:  1.3303276370861568
test acc:  0.9153
forward train acc:  0.99888  and loss:  1.607377249340061
test acc:  0.9154
forward train acc:  0.9991  and loss:  1.2557695210562088
test acc:  0.9147
forward train acc:  0.9989  and loss:  1.4256428180378862
test acc:  0.9138
********** reverse layer  7  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.84375  ==>  120 / 768
layer  11  :  0.9088541666666666  ==>  70 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  8
[2, 2, 4, 2, 1, 3, 4, 5, 4, 2, 1, 4, 2, 3]
***** skip layer  9
[2, 2, 4, 2, 1, 3, 4, 5, 4, 1, 1, 4, 2, 3]
***** skip layer  10
[2, 2, 4, 2, 1, 3, 4, 5, 4, 1, 0, 4, 2, 3]
***** skip layer  11
[2, 2, 4, 2, 1, 3, 4, 5, 4, 1, 0, 3, 2, 3]
***** skip layer  12
[2, 2, 4, 2, 1, 3, 4, 5, 4, 1, 0, 3, 1, 3]
***** skip layer  13
[2, 2, 4, 2, 1, 3, 4, 5, 4, 1, 0, 3, 1, 2]
***** skip layer  0
[1, 2, 4, 2, 1, 3, 4, 5, 4, 1, 0, 3, 1, 2]
***** skip layer  1
[1, 1, 4, 2, 1, 3, 4, 5, 4, 1, 0, 3, 1, 2]
***** skip layer  2
[1, 1, 3, 2, 1, 3, 4, 5, 4, 1, 0, 3, 1, 2]
***** skip layer  3
[1, 1, 3, 1, 1, 3, 4, 5, 4, 1, 0, 3, 1, 2]
***** skip layer  4
[1, 1, 3, 1, 0, 3, 4, 5, 4, 1, 0, 3, 1, 2]
***** skip layer  5
[1, 1, 3, 1, 0, 2, 4, 5, 4, 1, 0, 3, 1, 2]
***** skip layer  6
[1, 1, 3, 1, 0, 2, 3, 5, 4, 1, 0, 3, 1, 2]
***** skip layer  7
[1, 1, 3, 1, 0, 2, 3, 4, 4, 1, 0, 3, 1, 2]
***** skip layer  8
[1, 1, 3, 1, 0, 2, 3, 4, 3, 1, 0, 3, 1, 2]
***** skip layer  9
[1, 1, 3, 1, 0, 2, 3, 4, 3, 0, 0, 3, 1, 2]
optimize layer  10
backward train epoch:  316
test acc:  0.0981
forward train acc:  0.99908  and loss:  1.2369675970985554
test acc:  0.919
forward train acc:  0.99966  and loss:  0.4069744780426845
test acc:  0.9191
forward train acc:  0.99954  and loss:  0.5584364206297323
test acc:  0.9185
forward train acc:  0.99952  and loss:  0.6445687880041078
test acc:  0.9185
forward train acc:  0.9997  and loss:  0.4606455760804238
test acc:  0.9194
forward train acc:  0.99956  and loss:  0.6830682135769166
test acc:  0.9197
forward train acc:  0.9996  and loss:  0.3862965743173845
test acc:  0.9196
forward train acc:  0.99976  and loss:  0.3387440225924365
test acc:  0.9204
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8515625  ==>  114 / 768
layer  11  :  0.9088541666666666  ==>  70 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  11
[1, 1, 3, 1, 0, 2, 3, 4, 3, 0, 0, 2, 1, 2]
***** skip layer  12
[1, 1, 3, 1, 0, 2, 3, 4, 3, 0, 0, 2, 0, 2]
***** skip layer  13
[1, 1, 3, 1, 0, 2, 3, 4, 3, 0, 0, 2, 0, 1]
***** skip layer  0
[0, 1, 3, 1, 0, 2, 3, 4, 3, 0, 0, 2, 0, 1]
***** skip layer  1
[0, 0, 3, 1, 0, 2, 3, 4, 3, 0, 0, 2, 0, 1]
***** skip layer  2
[0, 0, 2, 1, 0, 2, 3, 4, 3, 0, 0, 2, 0, 1]
***** skip layer  3
[0, 0, 2, 0, 0, 2, 3, 4, 3, 0, 0, 2, 0, 1]
optimize layer  4
backward train epoch:  337
test acc:  0.1004
forward train acc:  0.99874  and loss:  1.7134548731410177
test acc:  0.916
forward train acc:  0.99918  and loss:  1.1508382452302612
test acc:  0.9165
forward train acc:  0.99904  and loss:  1.0967973475926556
test acc:  0.9172
forward train acc:  0.99922  and loss:  1.0277793038403615
test acc:  0.9174
forward train acc:  0.99928  and loss:  0.9087307495210553
test acc:  0.9184
forward train acc:  0.9994  and loss:  0.6658510644083435
test acc:  0.9191
forward train acc:  0.99914  and loss:  0.9503677083121147
test acc:  0.9176
forward train acc:  0.99956  and loss:  0.5326694731265889
test acc:  0.9194
forward train acc:  0.99956  and loss:  0.6410851151449606
test acc:  0.917
forward train acc:  0.9995  and loss:  0.4652244834105659
test acc:  0.9186
********** reverse layer  4  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8515625  ==>  114 / 768
layer  11  :  0.9088541666666666  ==>  70 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  5
[0, 0, 2, 0, 5, 1, 3, 4, 3, 0, 0, 2, 0, 1]
***** skip layer  6
[0, 0, 2, 0, 5, 1, 2, 4, 3, 0, 0, 2, 0, 1]
***** skip layer  7
[0, 0, 2, 0, 5, 1, 2, 3, 3, 0, 0, 2, 0, 1]
***** skip layer  8
[0, 0, 2, 0, 5, 1, 2, 3, 2, 0, 0, 2, 0, 1]
optimize layer  9
backward train epoch:  207
test acc:  0.1038
forward train acc:  0.99966  and loss:  0.5461638060514815
test acc:  0.9193
forward train acc:  0.99982  and loss:  0.25802431409829296
test acc:  0.9181
forward train acc:  0.99914  and loss:  1.1756124572893896
test acc:  0.9171
forward train acc:  0.99942  and loss:  0.7102614273899235
test acc:  0.9185
forward train acc:  0.99954  and loss:  0.5141113264653541
test acc:  0.9174
forward train acc:  0.9997  and loss:  0.39148836575986934
test acc:  0.9194
forward train acc:  0.99978  and loss:  0.3382737331703538
test acc:  0.9185
forward train acc:  0.99974  and loss:  0.45930038792357664
test acc:  0.9188
forward train acc:  0.99976  and loss:  0.3498185705393553
test acc:  0.9183
forward train acc:  0.99966  and loss:  0.3963407107294188
test acc:  0.9188
********** reverse layer  9  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8515625  ==>  114 / 768
layer  11  :  0.9088541666666666  ==>  70 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  10
backward train epoch:  271
test acc:  0.1039
forward train acc:  0.99956  and loss:  0.63600607283297
test acc:  0.9176
forward train acc:  0.9996  and loss:  0.5229864367502159
test acc:  0.9176
forward train acc:  0.9995  and loss:  0.5473695632535964
test acc:  0.9187
forward train acc:  0.9996  and loss:  0.49777548330894206
test acc:  0.9192
forward train acc:  0.99966  and loss:  0.4331124188611284
test acc:  0.9185
forward train acc:  0.9996  and loss:  0.5438165571831632
test acc:  0.9183
forward train acc:  0.9996  and loss:  0.5056214558244392
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.859375  ==>  108 / 768
layer  11  :  0.9088541666666666  ==>  70 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  11
[0, 0, 2, 0, 5, 1, 2, 3, 2, 5, 0, 1, 0, 1]
optimize layer  12
backward train epoch:  159
test acc:  0.1
forward train acc:  0.09944  and loss:  913.7648787498474
test acc:  0.1
forward train acc:  0.1  and loss:  910.0428419113159
test acc:  0.1
forward train acc:  0.1  and loss:  907.2701790332794
test acc:  0.1
forward train acc:  0.1  and loss:  905.642457485199
test acc:  0.1
forward train acc:  0.1  and loss:  904.7764542102814
test acc:  0.1
forward train acc:  0.1  and loss:  904.0163352489471
test acc:  0.1
forward train acc:  0.1  and loss:  903.3627910614014
test acc:  0.1
forward train acc:  0.1  and loss:  902.9466879367828
test acc:  0.1
forward train acc:  0.1  and loss:  902.6893870830536
test acc:  0.1
forward train acc:  0.1  and loss:  902.4497811794281
test acc:  0.1
********** reverse layer  12  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.859375  ==>  108 / 768
layer  11  :  0.9088541666666666  ==>  70 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  13
[0, 0, 2, 0, 5, 1, 2, 3, 2, 5, 0, 1, 5, 0]
optimize layer  0
backward train epoch:  65
test acc:  0.1093
forward train acc:  0.99808  and loss:  2.590897965186741
test acc:  0.9142
forward train acc:  0.99842  and loss:  2.2387934640137246
test acc:  0.9137
forward train acc:  0.9987  and loss:  1.8116709954338148
test acc:  0.9152
forward train acc:  0.99874  and loss:  1.3483157311566174
test acc:  0.916
forward train acc:  0.99884  and loss:  1.5385069077601656
test acc:  0.9163
forward train acc:  0.99898  and loss:  1.2034595698642079
test acc:  0.9145
forward train acc:  0.99918  and loss:  1.1070591905590845
test acc:  0.9152
forward train acc:  0.99926  and loss:  0.9969582561752759
test acc:  0.9172
forward train acc:  0.99922  and loss:  0.9636268355534412
test acc:  0.9177
forward train acc:  0.9994  and loss:  0.8762491109082475
test acc:  0.9161
********** reverse layer  0  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.859375  ==>  108 / 768
layer  11  :  0.9088541666666666  ==>  70 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  1
backward train epoch:  398
test acc:  0.1
forward train acc:  0.99706  and loss:  4.519514885410899
test acc:  0.9144
forward train acc:  0.9978  and loss:  2.82934259850299
test acc:  0.9148
forward train acc:  0.99874  and loss:  1.890118628856726
test acc:  0.9148
forward train acc:  0.99846  and loss:  2.0517091033398174
test acc:  0.9165
forward train acc:  0.99906  and loss:  1.4780166324562742
test acc:  0.9167
forward train acc:  0.99904  and loss:  1.1618733939249068
test acc:  0.9164
forward train acc:  0.9991  and loss:  1.134315082046669
test acc:  0.916
forward train acc:  0.9992  and loss:  1.0086228128056973
test acc:  0.9176
forward train acc:  0.99916  and loss:  1.0471243322244845
test acc:  0.9177
forward train acc:  0.99924  and loss:  1.0233916599972872
test acc:  0.9173
********** reverse layer  1  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.859375  ==>  108 / 768
layer  11  :  0.9088541666666666  ==>  70 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  2
[5, 5, 1, 0, 5, 1, 2, 3, 2, 5, 0, 1, 5, 0]
optimize layer  3
backward train epoch:  245
test acc:  0.1
forward train acc:  0.99906  and loss:  1.3709249327657744
test acc:  0.9178
forward train acc:  0.9991  and loss:  1.0995118379796622
test acc:  0.9188
forward train acc:  0.99904  and loss:  1.207909739430761
test acc:  0.9158
forward train acc:  0.99922  and loss:  0.9757471498451196
test acc:  0.9179
forward train acc:  0.9994  and loss:  0.8200557139789453
test acc:  0.9183
forward train acc:  0.99922  and loss:  1.0027564443298616
test acc:  0.9174
forward train acc:  0.99936  and loss:  0.9399231564893853
test acc:  0.9183
forward train acc:  0.99944  and loss:  0.8059486218407983
test acc:  0.918
forward train acc:  0.99952  and loss:  0.6790395544958301
test acc:  0.919
forward train acc:  0.99944  and loss:  0.6503222667088266
test acc:  0.9193
********** reverse layer  3  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.859375  ==>  108 / 768
layer  11  :  0.9088541666666666  ==>  70 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  4
[5, 5, 1, 5, 4, 1, 2, 3, 2, 5, 0, 1, 5, 0]
***** skip layer  5
[5, 5, 1, 5, 4, 0, 2, 3, 2, 5, 0, 1, 5, 0]
***** skip layer  6
[5, 5, 1, 5, 4, 0, 1, 3, 2, 5, 0, 1, 5, 0]
***** skip layer  7
[5, 5, 1, 5, 4, 0, 1, 2, 2, 5, 0, 1, 5, 0]
***** skip layer  8
[5, 5, 1, 5, 4, 0, 1, 2, 1, 5, 0, 1, 5, 0]
***** skip layer  9
[5, 5, 1, 5, 4, 0, 1, 2, 1, 4, 0, 1, 5, 0]
optimize layer  10
backward train epoch:  163
test acc:  0.1005
forward train acc:  0.99952  and loss:  0.5407979097581119
test acc:  0.9189
forward train acc:  0.99938  and loss:  0.720601198496297
test acc:  0.9162
forward train acc:  0.99968  and loss:  0.4603449796923087
test acc:  0.9181
forward train acc:  0.99954  and loss:  0.7240972989347938
test acc:  0.9207
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8671875  ==>  102 / 768
layer  11  :  0.9088541666666666  ==>  70 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  11
[5, 5, 1, 5, 4, 0, 1, 2, 1, 4, 0, 0, 5, 0]
***** skip layer  12
[5, 5, 1, 5, 4, 0, 1, 2, 1, 4, 0, 0, 4, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.0911
forward train acc:  0.69094  and loss:  399.11594808101654
test acc:  0.6427
forward train acc:  0.69894  and loss:  369.6762177348137
test acc:  0.6445
forward train acc:  0.69932  and loss:  359.22841614484787
test acc:  0.6433
forward train acc:  0.6993  and loss:  351.8030759692192
test acc:  0.6462
forward train acc:  0.6994  and loss:  347.48585218191147
test acc:  0.6461
forward train acc:  0.69956  and loss:  342.3584619164467
test acc:  0.6462
forward train acc:  0.69968  and loss:  337.87859013676643
test acc:  0.6464
forward train acc:  0.6996  and loss:  334.2905871272087
test acc:  0.6465
forward train acc:  0.69954  and loss:  332.2366414666176
test acc:  0.6477
forward train acc:  0.69962  and loss:  329.6344279050827
test acc:  0.6469
********** reverse layer  13  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8671875  ==>  102 / 768
layer  11  :  0.9088541666666666  ==>  70 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  0
[4, 5, 1, 5, 4, 0, 1, 2, 1, 4, 0, 0, 4, 5]
***** skip layer  1
[4, 4, 1, 5, 4, 0, 1, 2, 1, 4, 0, 0, 4, 5]
***** skip layer  2
[4, 4, 0, 5, 4, 0, 1, 2, 1, 4, 0, 0, 4, 5]
***** skip layer  3
[4, 4, 0, 4, 4, 0, 1, 2, 1, 4, 0, 0, 4, 5]
***** skip layer  4
[4, 4, 0, 4, 3, 0, 1, 2, 1, 4, 0, 0, 4, 5]
optimize layer  5
backward train epoch:  65
test acc:  0.1112
forward train acc:  0.99918  and loss:  1.2345794996799668
test acc:  0.9177
forward train acc:  0.99926  and loss:  0.9767963246704312
test acc:  0.9151
forward train acc:  0.9993  and loss:  1.032099944233778
test acc:  0.918
forward train acc:  0.99938  and loss:  0.761779302789364
test acc:  0.9175
forward train acc:  0.99964  and loss:  0.549684574289131
test acc:  0.9187
forward train acc:  0.99932  and loss:  0.7618313323037
test acc:  0.9191
forward train acc:  0.99956  and loss:  0.5613497690646909
test acc:  0.919
forward train acc:  0.99954  and loss:  0.6247849452483933
test acc:  0.9192
forward train acc:  0.99958  and loss:  0.43977417760470416
test acc:  0.9188
forward train acc:  0.9995  and loss:  0.4781277421207051
test acc:  0.9194
********** reverse layer  5  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8671875  ==>  102 / 768
layer  11  :  0.9088541666666666  ==>  70 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  6
[4, 4, 0, 4, 3, 5, 0, 2, 1, 4, 0, 0, 4, 5]
***** skip layer  7
[4, 4, 0, 4, 3, 5, 0, 1, 1, 4, 0, 0, 4, 5]
***** skip layer  8
[4, 4, 0, 4, 3, 5, 0, 1, 0, 4, 0, 0, 4, 5]
***** skip layer  9
[4, 4, 0, 4, 3, 5, 0, 1, 0, 3, 0, 0, 4, 5]
optimize layer  10
backward train epoch:  248
test acc:  0.1
forward train acc:  0.99958  and loss:  0.4927484937943518
test acc:  0.919
forward train acc:  0.9996  and loss:  0.6698300569332787
test acc:  0.9176
forward train acc:  0.99932  and loss:  0.9261256936952122
test acc:  0.9193
forward train acc:  0.99962  and loss:  0.5562088779406622
test acc:  0.9195
forward train acc:  0.99966  and loss:  0.42471534584183246
test acc:  0.9198
forward train acc:  0.99968  and loss:  0.46431408484932035
test acc:  0.9203
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.875  ==>  96 / 768
layer  11  :  0.9088541666666666  ==>  70 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  11
backward train epoch:  13
test acc:  0.1566
forward train acc:  0.99962  and loss:  0.6104227257092134
test acc:  0.9168
forward train acc:  0.99948  and loss:  0.7568982555021648
test acc:  0.9176
forward train acc:  0.9994  and loss:  0.8673121148895007
test acc:  0.9155
forward train acc:  0.99954  and loss:  0.8190410883107688
test acc:  0.9192
forward train acc:  0.99972  and loss:  0.43750739394454286
test acc:  0.9185
forward train acc:  0.99958  and loss:  0.5152771147259045
test acc:  0.9181
forward train acc:  0.99976  and loss:  0.3320353037343011
test acc:  0.9197
forward train acc:  0.99958  and loss:  0.5824360297701787
test acc:  0.9214
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.875  ==>  96 / 768
layer  11  :  0.9166666666666666  ==>  64 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  12
[4, 4, 0, 4, 3, 5, 0, 1, 0, 3, 0, 0, 3, 5]
***** skip layer  13
[4, 4, 0, 4, 3, 5, 0, 1, 0, 3, 0, 0, 3, 4]
***** skip layer  0
[3, 4, 0, 4, 3, 5, 0, 1, 0, 3, 0, 0, 3, 4]
***** skip layer  1
[3, 3, 0, 4, 3, 5, 0, 1, 0, 3, 0, 0, 3, 4]
optimize layer  2
backward train epoch:  642
test acc:  0.1
forward train acc:  0.94292  and loss:  97.97084244340658
test acc:  0.8818
forward train acc:  0.95826  and loss:  56.001623291522264
test acc:  0.8871
forward train acc:  0.96604  and loss:  43.01213281415403
test acc:  0.8911
forward train acc:  0.97222  and loss:  35.69236981868744
test acc:  0.8918
forward train acc:  0.9744  and loss:  32.67359676863998
test acc:  0.8944
forward train acc:  0.97684  and loss:  29.80566076003015
test acc:  0.897
forward train acc:  0.97898  and loss:  26.283026137854904
test acc:  0.8984
forward train acc:  0.9824  and loss:  22.596516511868685
test acc:  0.8978
forward train acc:  0.9816  and loss:  23.04609290882945
test acc:  0.8994
forward train acc:  0.98182  and loss:  22.43300680210814
test acc:  0.899
********** reverse layer  2  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6666666666666666  ==>  128 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.875  ==>  96 / 768
layer  11  :  0.9166666666666666  ==>  64 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  3
[3, 3, 5, 3, 3, 5, 0, 1, 0, 3, 0, 0, 3, 4]
***** skip layer  4
[3, 3, 5, 3, 2, 5, 0, 1, 0, 3, 0, 0, 3, 4]
***** skip layer  5
[3, 3, 5, 3, 2, 4, 0, 1, 0, 3, 0, 0, 3, 4]
optimize layer  6
backward train epoch:  85
test acc:  0.1
forward train acc:  0.99902  and loss:  1.5086502820049645
test acc:  0.9188
forward train acc:  0.9992  and loss:  1.1183026402140968
test acc:  0.9194
forward train acc:  0.99944  and loss:  0.7911829162039794
test acc:  0.9182
forward train acc:  0.9995  and loss:  0.8083169284946052
test acc:  0.9214
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6770833333333334  ==>  124 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.875  ==>  96 / 768
layer  11  :  0.9166666666666666  ==>  64 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  7
[3, 3, 5, 3, 2, 4, 0, 0, 0, 3, 0, 0, 3, 4]
optimize layer  8
backward train epoch:  21
test acc:  0.1006
forward train acc:  0.9969  and loss:  4.457479508477263
test acc:  0.9142
forward train acc:  0.99764  and loss:  3.051877562858863
test acc:  0.9126
forward train acc:  0.99832  and loss:  2.4482627281395253
test acc:  0.9156
forward train acc:  0.99846  and loss:  1.9972100326413056
test acc:  0.9158
forward train acc:  0.99846  and loss:  2.0101509730739053
test acc:  0.9161
forward train acc:  0.99866  and loss:  1.6903211000899319
test acc:  0.9183
forward train acc:  0.99886  and loss:  1.3849448460969143
test acc:  0.9184
forward train acc:  0.99904  and loss:  1.450417471525725
test acc:  0.9191
forward train acc:  0.99914  and loss:  1.0905123625416309
test acc:  0.9193
forward train acc:  0.9991  and loss:  1.1438462879741564
test acc:  0.9181
********** reverse layer  8  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6770833333333334  ==>  124 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.875  ==>  96 / 768
layer  11  :  0.9166666666666666  ==>  64 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  9
[3, 3, 5, 3, 2, 4, 0, 0, 5, 2, 0, 0, 3, 4]
optimize layer  10
backward train epoch:  132
test acc:  0.1
forward train acc:  0.99948  and loss:  0.7471273955598008
test acc:  0.9193
forward train acc:  0.99952  and loss:  0.7410258317395346
test acc:  0.9187
forward train acc:  0.99938  and loss:  0.7231880151302903
test acc:  0.9199
forward train acc:  0.99954  and loss:  0.628332389795105
test acc:  0.9193
forward train acc:  0.99952  and loss:  0.6074830258148722
test acc:  0.9188
forward train acc:  0.99958  and loss:  0.5614457800184027
test acc:  0.9194
forward train acc:  0.9998  and loss:  0.30510578693065327
test acc:  0.9206
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6770833333333334  ==>  124 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8828125  ==>  90 / 768
layer  11  :  0.9166666666666666  ==>  64 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  11
backward train epoch:  643
test acc:  0.1
forward train acc:  0.99958  and loss:  0.7087021949555492
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6770833333333334  ==>  124 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8828125  ==>  90 / 768
layer  11  :  0.9244791666666666  ==>  58 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  12
[3, 3, 5, 3, 2, 4, 0, 0, 5, 2, 0, 0, 2, 4]
***** skip layer  13
[3, 3, 5, 3, 2, 4, 0, 0, 5, 2, 0, 0, 2, 3]
***** skip layer  0
[2, 3, 5, 3, 2, 4, 0, 0, 5, 2, 0, 0, 2, 3]
***** skip layer  1
[2, 2, 5, 3, 2, 4, 0, 0, 5, 2, 0, 0, 2, 3]
***** skip layer  2
[2, 2, 4, 3, 2, 4, 0, 0, 5, 2, 0, 0, 2, 3]
***** skip layer  3
[2, 2, 4, 2, 2, 4, 0, 0, 5, 2, 0, 0, 2, 3]
***** skip layer  4
[2, 2, 4, 2, 1, 4, 0, 0, 5, 2, 0, 0, 2, 3]
***** skip layer  5
[2, 2, 4, 2, 1, 3, 0, 0, 5, 2, 0, 0, 2, 3]
optimize layer  6
backward train epoch:  139
test acc:  0.1
forward train acc:  0.99942  and loss:  0.6595250758109614
test acc:  0.918
forward train acc:  0.9993  and loss:  0.8810436964995461
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6875  ==>  120 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8828125  ==>  90 / 768
layer  11  :  0.9244791666666666  ==>  58 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  7
backward train epoch:  148
test acc:  0.1001
forward train acc:  0.99442  and loss:  7.620293819520157
test acc:  0.9127
forward train acc:  0.99714  and loss:  3.6910564287682064
test acc:  0.9127
forward train acc:  0.99786  and loss:  2.4653905043378472
test acc:  0.9161
forward train acc:  0.99848  and loss:  1.820169121521758
test acc:  0.9154
forward train acc:  0.99854  and loss:  1.9399350039893761
test acc:  0.9157
forward train acc:  0.99848  and loss:  1.7680051797942724
test acc:  0.915
forward train acc:  0.99874  and loss:  1.5702592584275408
test acc:  0.9161
forward train acc:  0.99884  and loss:  1.5351279073802289
test acc:  0.9165
forward train acc:  0.99874  and loss:  1.4793377482274082
test acc:  0.9161
forward train acc:  0.9991  and loss:  1.2120522143377457
test acc:  0.9161
********** reverse layer  7  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6875  ==>  120 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8828125  ==>  90 / 768
layer  11  :  0.9244791666666666  ==>  58 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  8
[2, 2, 4, 2, 1, 3, 0, 5, 4, 2, 0, 0, 2, 3]
***** skip layer  9
[2, 2, 4, 2, 1, 3, 0, 5, 4, 1, 0, 0, 2, 3]
optimize layer  10
backward train epoch:  221
test acc:  0.1006
forward train acc:  0.99902  and loss:  1.2992040535682463
test acc:  0.9191
forward train acc:  0.99944  and loss:  0.7622217691678088
test acc:  0.9197
forward train acc:  0.99948  and loss:  0.750585214846069
test acc:  0.9195
forward train acc:  0.99948  and loss:  0.5672251335636247
test acc:  0.9186
forward train acc:  0.9996  and loss:  0.49847712618066
test acc:  0.9204
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6875  ==>  120 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.890625  ==>  84 / 768
layer  11  :  0.9244791666666666  ==>  58 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  11
backward train epoch:  619
test acc:  0.1065
forward train acc:  0.99946  and loss:  0.799346223968314
test acc:  0.9193
forward train acc:  0.99952  and loss:  0.5820060464902781
test acc:  0.9188
forward train acc:  0.99916  and loss:  1.0338212293572724
test acc:  0.9191
forward train acc:  0.99946  and loss:  0.6530734128537006
test acc:  0.9194
forward train acc:  0.99948  and loss:  0.6239409730187617
test acc:  0.9186
forward train acc:  0.9996  and loss:  0.47969054235727526
test acc:  0.919
forward train acc:  0.99936  and loss:  0.8800654784572544
test acc:  0.9177
forward train acc:  0.99958  and loss:  0.49281819756288314
test acc:  0.9203
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6875  ==>  120 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.890625  ==>  84 / 768
layer  11  :  0.9322916666666666  ==>  52 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  12
[2, 2, 4, 2, 1, 3, 0, 5, 4, 1, 0, 0, 1, 3]
***** skip layer  13
[2, 2, 4, 2, 1, 3, 0, 5, 4, 1, 0, 0, 1, 2]
***** skip layer  0
[1, 2, 4, 2, 1, 3, 0, 5, 4, 1, 0, 0, 1, 2]
***** skip layer  1
[1, 1, 4, 2, 1, 3, 0, 5, 4, 1, 0, 0, 1, 2]
***** skip layer  2
[1, 1, 3, 2, 1, 3, 0, 5, 4, 1, 0, 0, 1, 2]
***** skip layer  3
[1, 1, 3, 1, 1, 3, 0, 5, 4, 1, 0, 0, 1, 2]
***** skip layer  4
[1, 1, 3, 1, 0, 3, 0, 5, 4, 1, 0, 0, 1, 2]
***** skip layer  5
[1, 1, 3, 1, 0, 2, 0, 5, 4, 1, 0, 0, 1, 2]
optimize layer  6
backward train epoch:  294
test acc:  0.1003
forward train acc:  0.99942  and loss:  0.7304883422984858
test acc:  0.9169
forward train acc:  0.99938  and loss:  0.8737611162068788
test acc:  0.9171
forward train acc:  0.99924  and loss:  0.9463043321447913
test acc:  0.917
forward train acc:  0.99922  and loss:  0.9138498935499229
test acc:  0.9175
forward train acc:  0.99958  and loss:  0.5719182600296335
test acc:  0.9185
forward train acc:  0.99954  and loss:  0.5870821054995758
test acc:  0.9189
forward train acc:  0.9997  and loss:  0.3829327226849273
test acc:  0.9179
forward train acc:  0.99958  and loss:  0.5064693369786255
test acc:  0.9184
forward train acc:  0.99966  and loss:  0.5023614483943675
test acc:  0.9193
forward train acc:  0.9996  and loss:  0.5624288795806933
test acc:  0.9194
********** reverse layer  6  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6875  ==>  120 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.890625  ==>  84 / 768
layer  11  :  0.9322916666666666  ==>  52 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  7
[1, 1, 3, 1, 0, 2, 5, 4, 4, 1, 0, 0, 1, 2]
***** skip layer  8
[1, 1, 3, 1, 0, 2, 5, 4, 3, 1, 0, 0, 1, 2]
***** skip layer  9
[1, 1, 3, 1, 0, 2, 5, 4, 3, 0, 0, 0, 1, 2]
optimize layer  10
backward train epoch:  467
test acc:  0.1017
forward train acc:  0.99956  and loss:  0.6752667853434104
test acc:  0.9199
forward train acc:  0.9994  and loss:  0.7772627355006989
test acc:  0.9182
forward train acc:  0.99934  and loss:  0.8164605513738934
test acc:  0.9191
forward train acc:  0.99948  and loss:  0.6261676357971737
test acc:  0.9198
forward train acc:  0.99956  and loss:  0.542599038741173
test acc:  0.9198
forward train acc:  0.99954  and loss:  0.5629863297072006
test acc:  0.9202
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6875  ==>  120 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8984375  ==>  78 / 768
layer  11  :  0.9322916666666666  ==>  52 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  11
backward train epoch:  498
test acc:  0.1027
forward train acc:  0.99956  and loss:  0.5008373387099709
test acc:  0.9197
forward train acc:  0.99934  and loss:  0.8130368189886212
test acc:  0.9168
forward train acc:  0.9994  and loss:  0.7168630800733808
test acc:  0.9189
forward train acc:  0.99954  and loss:  0.626635721695493
test acc:  0.9196
forward train acc:  0.99962  and loss:  0.5954370287581696
test acc:  0.9213
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6354166666666666  ==>  140 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6875  ==>  120 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8984375  ==>  78 / 768
layer  11  :  0.9401041666666666  ==>  46 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  12
[1, 1, 3, 1, 0, 2, 5, 4, 3, 0, 0, 0, 0, 2]
***** skip layer  13
[1, 1, 3, 1, 0, 2, 5, 4, 3, 0, 0, 0, 0, 1]
***** skip layer  0
[0, 1, 3, 1, 0, 2, 5, 4, 3, 0, 0, 0, 0, 1]
***** skip layer  1
[0, 0, 3, 1, 0, 2, 5, 4, 3, 0, 0, 0, 0, 1]
***** skip layer  2
[0, 0, 2, 1, 0, 2, 5, 4, 3, 0, 0, 0, 0, 1]
***** skip layer  3
[0, 0, 2, 0, 0, 2, 5, 4, 3, 0, 0, 0, 0, 1]
optimize layer  4
backward train epoch:  440
test acc:  0.1
forward train acc:  0.99886  and loss:  1.5739239975227974
test acc:  0.915
forward train acc:  0.99902  and loss:  1.4123655964795034
test acc:  0.9172
forward train acc:  0.9987  and loss:  1.6992889580578776
test acc:  0.9174
forward train acc:  0.999  and loss:  1.2088183923478937
test acc:  0.9173
forward train acc:  0.99926  and loss:  1.075484727116418
test acc:  0.9182
forward train acc:  0.9994  and loss:  0.6967939603055129
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6875  ==>  120 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8984375  ==>  78 / 768
layer  11  :  0.9401041666666666  ==>  46 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  5
[0, 0, 2, 0, 0, 1, 5, 4, 3, 0, 0, 0, 0, 1]
***** skip layer  6
[0, 0, 2, 0, 0, 1, 4, 4, 3, 0, 0, 0, 0, 1]
***** skip layer  7
[0, 0, 2, 0, 0, 1, 4, 3, 3, 0, 0, 0, 0, 1]
***** skip layer  8
[0, 0, 2, 0, 0, 1, 4, 3, 2, 0, 0, 0, 0, 1]
optimize layer  9
backward train epoch:  299
test acc:  0.1
forward train acc:  0.9992  and loss:  1.187874154340534
test acc:  0.9158
forward train acc:  0.9992  and loss:  1.3686894376878627
test acc:  0.9164
forward train acc:  0.99884  and loss:  1.5227972410793882
test acc:  0.9168
forward train acc:  0.99916  and loss:  1.0818734101048904
test acc:  0.9165
forward train acc:  0.99926  and loss:  1.0231925789703382
test acc:  0.9167
forward train acc:  0.9996  and loss:  0.5522858970798552
test acc:  0.9173
forward train acc:  0.99946  and loss:  0.8508613927770057
test acc:  0.9181
forward train acc:  0.99948  and loss:  0.7826806706580101
test acc:  0.9192
forward train acc:  0.99942  and loss:  0.8284109867236111
test acc:  0.9186
forward train acc:  0.99964  and loss:  0.5447665529864025
test acc:  0.9181
********** reverse layer  9  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6875  ==>  120 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8984375  ==>  78 / 768
layer  11  :  0.9401041666666666  ==>  46 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  10
backward train epoch:  36
test acc:  0.1
forward train acc:  0.9993  and loss:  0.9418004453546018
test acc:  0.9157
forward train acc:  0.99944  and loss:  0.8644190729246475
test acc:  0.916
forward train acc:  0.99926  and loss:  0.9373895416720188
test acc:  0.9172
forward train acc:  0.9994  and loss:  0.8145512402697932
test acc:  0.9173
forward train acc:  0.99926  and loss:  0.845281635876745
test acc:  0.9176
forward train acc:  0.99918  and loss:  0.9482375865336508
test acc:  0.9153
forward train acc:  0.99952  and loss:  0.6592997124098474
test acc:  0.9173
forward train acc:  0.99974  and loss:  0.36600564418768045
test acc:  0.9176
forward train acc:  0.99958  and loss:  0.6078424806983094
test acc:  0.9186
forward train acc:  0.99958  and loss:  0.6103322056005709
test acc:  0.9176
********** reverse layer  10  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6875  ==>  120 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8984375  ==>  78 / 768
layer  11  :  0.9401041666666666  ==>  46 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  11
backward train epoch:  248
test acc:  0.1
forward train acc:  0.99932  and loss:  1.0277856673346832
test acc:  0.9163
forward train acc:  0.9991  and loss:  1.1504458024573978
test acc:  0.9168
forward train acc:  0.99914  and loss:  1.0216269569646101
test acc:  0.9167
forward train acc:  0.99932  and loss:  0.7862563986855093
test acc:  0.9183
forward train acc:  0.99936  and loss:  0.8396513177867746
test acc:  0.9193
forward train acc:  0.99932  and loss:  0.8061165861436166
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6875  ==>  120 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8984375  ==>  78 / 768
layer  11  :  0.9479166666666666  ==>  40 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  12
backward train epoch:  119
test acc:  0.1
forward train acc:  0.1  and loss:  918.4218780994415
test acc:  0.1
forward train acc:  0.1  and loss:  913.8671040534973
test acc:  0.1
forward train acc:  0.1  and loss:  910.240161895752
test acc:  0.1
forward train acc:  0.1  and loss:  908.0619008541107
test acc:  0.1
forward train acc:  0.1  and loss:  906.8619017601013
test acc:  0.1
forward train acc:  0.1  and loss:  905.7997448444366
test acc:  0.1
forward train acc:  0.1  and loss:  904.891485452652
test acc:  0.1
forward train acc:  0.1  and loss:  904.2900700569153
test acc:  0.1
forward train acc:  0.1  and loss:  903.9167985916138
test acc:  0.1
forward train acc:  0.1  and loss:  903.5854160785675
test acc:  0.1
********** reverse layer  12  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6875  ==>  120 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8984375  ==>  78 / 768
layer  11  :  0.9479166666666666  ==>  40 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  13
[0, 0, 2, 0, 0, 1, 4, 3, 2, 5, 5, 0, 5, 0]
optimize layer  0
backward train epoch:  68
test acc:  0.1003
forward train acc:  0.9954  and loss:  6.6813961085281335
test acc:  0.9126
forward train acc:  0.99712  and loss:  4.2297309375717305
test acc:  0.9125
forward train acc:  0.99746  and loss:  3.589485111529939
test acc:  0.9156
forward train acc:  0.99772  and loss:  2.891791822214145
test acc:  0.9143
forward train acc:  0.99816  and loss:  2.569943902315572
test acc:  0.9145
forward train acc:  0.9981  and loss:  2.501688787364401
test acc:  0.9162
forward train acc:  0.99832  and loss:  2.034121194650652
test acc:  0.9146
forward train acc:  0.99844  and loss:  2.044376931153238
test acc:  0.9155
forward train acc:  0.9986  and loss:  1.685621315788012
test acc:  0.916
forward train acc:  0.99856  and loss:  1.7175798162643332
test acc:  0.9156
********** reverse layer  0  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6875  ==>  120 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8984375  ==>  78 / 768
layer  11  :  0.9479166666666666  ==>  40 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  1
backward train epoch:  66
test acc:  0.0991
forward train acc:  0.9971  and loss:  3.943305120919831
test acc:  0.9161
forward train acc:  0.99818  and loss:  2.1678619085869286
test acc:  0.9139
forward train acc:  0.99852  and loss:  1.9570611853851005
test acc:  0.9148
forward train acc:  0.99864  and loss:  1.788659536628984
test acc:  0.9165
forward train acc:  0.99912  and loss:  1.2908442683692556
test acc:  0.9169
forward train acc:  0.99902  and loss:  1.1461314945627237
test acc:  0.9165
forward train acc:  0.99898  and loss:  1.1701512451691087
test acc:  0.9168
forward train acc:  0.9993  and loss:  0.9830625768518075
test acc:  0.917
forward train acc:  0.99912  and loss:  1.0434853189217392
test acc:  0.9162
forward train acc:  0.9991  and loss:  1.0937285031250212
test acc:  0.9178
********** reverse layer  1  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6875  ==>  120 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8984375  ==>  78 / 768
layer  11  :  0.9479166666666666  ==>  40 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  2
[5, 5, 1, 0, 0, 1, 4, 3, 2, 5, 5, 0, 5, 0]
optimize layer  3
backward train epoch:  2
test acc:  0.1
forward train acc:  0.9985  and loss:  2.0198874723282643
test acc:  0.9156
forward train acc:  0.99888  and loss:  1.5233057583973277
test acc:  0.916
forward train acc:  0.99916  and loss:  1.156190546345897
test acc:  0.916
forward train acc:  0.99936  and loss:  0.987204453733284
test acc:  0.9184
forward train acc:  0.9992  and loss:  1.115556874196045
test acc:  0.9193
forward train acc:  0.99916  and loss:  0.9797690219420474
test acc:  0.9181
forward train acc:  0.9994  and loss:  0.7590705660986714
test acc:  0.9197
forward train acc:  0.9994  and loss:  0.9442266894329805
test acc:  0.9187
forward train acc:  0.9994  and loss:  0.8170335955510382
test acc:  0.9186
forward train acc:  0.99948  and loss:  0.6896121431855136
test acc:  0.9189
********** reverse layer  3  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6875  ==>  120 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8984375  ==>  78 / 768
layer  11  :  0.9479166666666666  ==>  40 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  4
backward train epoch:  118
test acc:  0.1
forward train acc:  0.99904  and loss:  1.3729100046912208
test acc:  0.9152
forward train acc:  0.99896  and loss:  1.4728968254930805
test acc:  0.9175
forward train acc:  0.99878  and loss:  1.4492187580908649
test acc:  0.9175
forward train acc:  0.99904  and loss:  1.1529251073370688
test acc:  0.9172
forward train acc:  0.99938  and loss:  0.9828524315380491
test acc:  0.917
forward train acc:  0.9994  and loss:  0.8807670317910379
test acc:  0.9167
forward train acc:  0.9992  and loss:  1.1032947319909
test acc:  0.9172
forward train acc:  0.99928  and loss:  0.8832852488849312
test acc:  0.9183
forward train acc:  0.99932  and loss:  1.0101143730717013
test acc:  0.9174
forward train acc:  0.99938  and loss:  0.9239235478744376
test acc:  0.9171
********** reverse layer  4  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6875  ==>  120 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8984375  ==>  78 / 768
layer  11  :  0.9479166666666666  ==>  40 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  5
[5, 5, 1, 5, 5, 0, 4, 3, 2, 5, 5, 0, 5, 0]
***** skip layer  6
[5, 5, 1, 5, 5, 0, 3, 3, 2, 5, 5, 0, 5, 0]
***** skip layer  7
[5, 5, 1, 5, 5, 0, 3, 2, 2, 5, 5, 0, 5, 0]
***** skip layer  8
[5, 5, 1, 5, 5, 0, 3, 2, 1, 5, 5, 0, 5, 0]
***** skip layer  9
[5, 5, 1, 5, 5, 0, 3, 2, 1, 4, 5, 0, 5, 0]
***** skip layer  10
[5, 5, 1, 5, 5, 0, 3, 2, 1, 4, 4, 0, 5, 0]
optimize layer  11
backward train epoch:  264
test acc:  0.1003
forward train acc:  0.99946  and loss:  0.9746121671050787
test acc:  0.9192
forward train acc:  0.99948  and loss:  0.7792761960590724
test acc:  0.9172
forward train acc:  0.99932  and loss:  0.7578577922977274
test acc:  0.9182
forward train acc:  0.99948  and loss:  0.6495719616941642
test acc:  0.9185
forward train acc:  0.99956  and loss:  0.5227197593922028
test acc:  0.9173
forward train acc:  0.99966  and loss:  0.5682282799389213
test acc:  0.9205
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6875  ==>  120 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8984375  ==>  78 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  12
[5, 5, 1, 5, 5, 0, 3, 2, 1, 4, 4, 0, 4, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.1
forward train acc:  0.99956  and loss:  4.078395516611636
test acc:  0.9152
forward train acc:  0.99948  and loss:  2.9433069895021617
test acc:  0.9161
forward train acc:  0.99932  and loss:  2.665741397999227
test acc:  0.9175
forward train acc:  0.99944  and loss:  2.442911548074335
test acc:  0.9177
forward train acc:  0.99944  and loss:  2.287284136749804
test acc:  0.9172
forward train acc:  0.99946  and loss:  2.0610722140409052
test acc:  0.9175
forward train acc:  0.99954  and loss:  2.124457262456417
test acc:  0.9176
forward train acc:  0.99956  and loss:  1.8634523146320134
test acc:  0.917
forward train acc:  0.9996  and loss:  1.7373113001231104
test acc:  0.9186
forward train acc:  0.99972  and loss:  1.5822598051745445
test acc:  0.9184
********** reverse layer  13  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6875  ==>  120 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8984375  ==>  78 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  0
[4, 5, 1, 5, 5, 0, 3, 2, 1, 4, 4, 0, 4, 5]
***** skip layer  1
[4, 4, 1, 5, 5, 0, 3, 2, 1, 4, 4, 0, 4, 5]
***** skip layer  2
[4, 4, 0, 5, 5, 0, 3, 2, 1, 4, 4, 0, 4, 5]
***** skip layer  3
[4, 4, 0, 4, 5, 0, 3, 2, 1, 4, 4, 0, 4, 5]
***** skip layer  4
[4, 4, 0, 4, 4, 0, 3, 2, 1, 4, 4, 0, 4, 5]
optimize layer  5
backward train epoch:  432
test acc:  0.1012
forward train acc:  0.9988  and loss:  1.8435093707739725
test acc:  0.9158
forward train acc:  0.99908  and loss:  1.0980672246660106
test acc:  0.9182
forward train acc:  0.99918  and loss:  1.188794725050684
test acc:  0.9156
forward train acc:  0.99938  and loss:  1.1638931046618382
test acc:  0.9164
forward train acc:  0.99932  and loss:  0.8991720052144956
test acc:  0.9168
forward train acc:  0.99918  and loss:  1.0899236903787823
test acc:  0.9169
forward train acc:  0.99956  and loss:  0.523913879529573
test acc:  0.917
forward train acc:  0.99926  and loss:  0.8870563107775524
test acc:  0.918
forward train acc:  0.99946  and loss:  0.878327691971208
test acc:  0.9184
forward train acc:  0.99938  and loss:  0.7738052286294987
test acc:  0.9179
********** reverse layer  5  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6875  ==>  120 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8984375  ==>  78 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  6
[4, 4, 0, 4, 4, 5, 2, 2, 1, 4, 4, 0, 4, 5]
***** skip layer  7
[4, 4, 0, 4, 4, 5, 2, 1, 1, 4, 4, 0, 4, 5]
***** skip layer  8
[4, 4, 0, 4, 4, 5, 2, 1, 0, 4, 4, 0, 4, 5]
***** skip layer  9
[4, 4, 0, 4, 4, 5, 2, 1, 0, 3, 4, 0, 4, 5]
***** skip layer  10
[4, 4, 0, 4, 4, 5, 2, 1, 0, 3, 3, 0, 4, 5]
optimize layer  11
backward train epoch:  127
test acc:  0.1001
forward train acc:  0.99944  and loss:  0.9090203874511644
test acc:  0.9177
forward train acc:  0.9994  and loss:  0.70657856935577
test acc:  0.919
forward train acc:  0.99932  and loss:  0.8944899393973174
test acc:  0.9178
forward train acc:  0.99954  and loss:  0.5507034699840005
test acc:  0.9169
forward train acc:  0.99944  and loss:  0.8146469961793628
test acc:  0.9176
forward train acc:  0.99966  and loss:  0.4583405937155476
test acc:  0.9189
forward train acc:  0.99956  and loss:  0.6001988626958337
test acc:  0.9186
forward train acc:  0.99992  and loss:  0.28072076247190125
test acc:  0.9186
forward train acc:  0.99958  and loss:  0.535410148302617
test acc:  0.9198
forward train acc:  0.99986  and loss:  0.24174928283900954
test acc:  0.919
********** reverse layer  11  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6875  ==>  120 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8984375  ==>  78 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  12
[4, 4, 0, 4, 4, 5, 2, 1, 0, 3, 3, 5, 3, 5]
***** skip layer  13
[4, 4, 0, 4, 4, 5, 2, 1, 0, 3, 3, 5, 3, 4]
***** skip layer  0
[3, 4, 0, 4, 4, 5, 2, 1, 0, 3, 3, 5, 3, 4]
***** skip layer  1
[3, 3, 0, 4, 4, 5, 2, 1, 0, 3, 3, 5, 3, 4]
optimize layer  2
backward train epoch:  205
test acc:  0.1088
forward train acc:  0.95308  and loss:  81.83438938111067
test acc:  0.89
forward train acc:  0.97004  and loss:  39.82137370668352
test acc:  0.8981
forward train acc:  0.97814  and loss:  28.285871010739356
test acc:  0.902
forward train acc:  0.98236  and loss:  22.51253369404003
test acc:  0.9049
forward train acc:  0.98564  and loss:  18.575206864392385
test acc:  0.9064
forward train acc:  0.98664  and loss:  16.69592308276333
test acc:  0.9067
forward train acc:  0.98924  and loss:  14.02735234471038
test acc:  0.9083
forward train acc:  0.98942  and loss:  13.663255929015577
test acc:  0.9081
forward train acc:  0.99058  and loss:  12.44341998337768
test acc:  0.9085
forward train acc:  0.99048  and loss:  12.02036002150271
test acc:  0.9101
********** reverse layer  2  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6875  ==>  120 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8984375  ==>  78 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  3
[3, 3, 5, 3, 4, 5, 2, 1, 0, 3, 3, 5, 3, 4]
***** skip layer  4
[3, 3, 5, 3, 3, 5, 2, 1, 0, 3, 3, 5, 3, 4]
***** skip layer  5
[3, 3, 5, 3, 3, 4, 2, 1, 0, 3, 3, 5, 3, 4]
***** skip layer  6
[3, 3, 5, 3, 3, 4, 1, 1, 0, 3, 3, 5, 3, 4]
***** skip layer  7
[3, 3, 5, 3, 3, 4, 1, 0, 0, 3, 3, 5, 3, 4]
optimize layer  8
backward train epoch:  59
test acc:  0.1
forward train acc:  0.99302  and loss:  10.061697344761342
test acc:  0.9116
forward train acc:  0.99696  and loss:  4.480842421296984
test acc:  0.9117
forward train acc:  0.99738  and loss:  3.5608165683806874
test acc:  0.9117
forward train acc:  0.99818  and loss:  2.74741600692505
test acc:  0.912
forward train acc:  0.99838  and loss:  2.109700717701344
test acc:  0.9126
forward train acc:  0.99836  and loss:  2.2377206847886555
test acc:  0.9129
forward train acc:  0.99846  and loss:  2.141194782423554
test acc:  0.9128
forward train acc:  0.99874  and loss:  1.7481674654700328
test acc:  0.9135
forward train acc:  0.99862  and loss:  1.7556676172243897
test acc:  0.9136
forward train acc:  0.9987  and loss:  1.6597845521755517
test acc:  0.9129
********** reverse layer  8  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6875  ==>  120 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8984375  ==>  78 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  9
[3, 3, 5, 3, 3, 4, 1, 0, 5, 2, 3, 5, 3, 4]
***** skip layer  10
[3, 3, 5, 3, 3, 4, 1, 0, 5, 2, 2, 5, 3, 4]
***** skip layer  11
[3, 3, 5, 3, 3, 4, 1, 0, 5, 2, 2, 4, 3, 4]
***** skip layer  12
[3, 3, 5, 3, 3, 4, 1, 0, 5, 2, 2, 4, 2, 4]
***** skip layer  13
[3, 3, 5, 3, 3, 4, 1, 0, 5, 2, 2, 4, 2, 3]
***** skip layer  0
[2, 3, 5, 3, 3, 4, 1, 0, 5, 2, 2, 4, 2, 3]
***** skip layer  1
[2, 2, 5, 3, 3, 4, 1, 0, 5, 2, 2, 4, 2, 3]
***** skip layer  2
[2, 2, 4, 3, 3, 4, 1, 0, 5, 2, 2, 4, 2, 3]
***** skip layer  3
[2, 2, 4, 2, 3, 4, 1, 0, 5, 2, 2, 4, 2, 3]
***** skip layer  4
[2, 2, 4, 2, 2, 4, 1, 0, 5, 2, 2, 4, 2, 3]
***** skip layer  5
[2, 2, 4, 2, 2, 3, 1, 0, 5, 2, 2, 4, 2, 3]
***** skip layer  6
[2, 2, 4, 2, 2, 3, 0, 0, 5, 2, 2, 4, 2, 3]
optimize layer  7
backward train epoch:  176
test acc:  0.1
forward train acc:  0.98334  and loss:  20.9340125345625
test acc:  0.9066
forward train acc:  0.99502  and loss:  5.758071850170381
test acc:  0.909
forward train acc:  0.99624  and loss:  4.534781672497047
test acc:  0.9124
forward train acc:  0.99692  and loss:  3.9134634574875236
test acc:  0.9125
forward train acc:  0.99784  and loss:  2.884577387740137
test acc:  0.9143
forward train acc:  0.9976  and loss:  2.9771071476861835
test acc:  0.9141
forward train acc:  0.99796  and loss:  2.6571286566613708
test acc:  0.9137
forward train acc:  0.99844  and loss:  2.3061924515350256
test acc:  0.9154
forward train acc:  0.99844  and loss:  2.00249261042336
test acc:  0.9142
forward train acc:  0.99832  and loss:  2.1306321589509025
test acc:  0.9155
********** reverse layer  7  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6875  ==>  120 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8984375  ==>  78 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  8
[2, 2, 4, 2, 2, 3, 0, 5, 4, 2, 2, 4, 2, 3]
***** skip layer  9
[2, 2, 4, 2, 2, 3, 0, 5, 4, 1, 2, 4, 2, 3]
***** skip layer  10
[2, 2, 4, 2, 2, 3, 0, 5, 4, 1, 1, 4, 2, 3]
***** skip layer  11
[2, 2, 4, 2, 2, 3, 0, 5, 4, 1, 1, 3, 2, 3]
***** skip layer  12
[2, 2, 4, 2, 2, 3, 0, 5, 4, 1, 1, 3, 1, 3]
***** skip layer  13
[2, 2, 4, 2, 2, 3, 0, 5, 4, 1, 1, 3, 1, 2]
***** skip layer  0
[1, 2, 4, 2, 2, 3, 0, 5, 4, 1, 1, 3, 1, 2]
***** skip layer  1
[1, 1, 4, 2, 2, 3, 0, 5, 4, 1, 1, 3, 1, 2]
***** skip layer  2
[1, 1, 3, 2, 2, 3, 0, 5, 4, 1, 1, 3, 1, 2]
***** skip layer  3
[1, 1, 3, 1, 2, 3, 0, 5, 4, 1, 1, 3, 1, 2]
***** skip layer  4
[1, 1, 3, 1, 1, 3, 0, 5, 4, 1, 1, 3, 1, 2]
***** skip layer  5
[1, 1, 3, 1, 1, 2, 0, 5, 4, 1, 1, 3, 1, 2]
optimize layer  6
backward train epoch:  819
test acc:  0.1001
forward train acc:  0.99886  and loss:  1.4683389368874487
test acc:  0.9181
forward train acc:  0.9993  and loss:  0.8705266074102838
test acc:  0.9176
forward train acc:  0.99954  and loss:  0.7775992900133133
test acc:  0.9188
forward train acc:  0.9996  and loss:  0.6302752352057723
test acc:  0.9182
forward train acc:  0.99948  and loss:  0.6597581788882962
test acc:  0.9197
forward train acc:  0.9996  and loss:  0.5508027836040128
test acc:  0.9187
forward train acc:  0.9996  and loss:  0.599639129028219
test acc:  0.9203
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8984375  ==>  78 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  7
[1, 1, 3, 1, 1, 2, 0, 4, 4, 1, 1, 3, 1, 2]
***** skip layer  8
[1, 1, 3, 1, 1, 2, 0, 4, 3, 1, 1, 3, 1, 2]
***** skip layer  9
[1, 1, 3, 1, 1, 2, 0, 4, 3, 0, 1, 3, 1, 2]
***** skip layer  10
[1, 1, 3, 1, 1, 2, 0, 4, 3, 0, 0, 3, 1, 2]
***** skip layer  11
[1, 1, 3, 1, 1, 2, 0, 4, 3, 0, 0, 2, 1, 2]
***** skip layer  12
[1, 1, 3, 1, 1, 2, 0, 4, 3, 0, 0, 2, 0, 2]
***** skip layer  13
[1, 1, 3, 1, 1, 2, 0, 4, 3, 0, 0, 2, 0, 1]
***** skip layer  0
[0, 1, 3, 1, 1, 2, 0, 4, 3, 0, 0, 2, 0, 1]
***** skip layer  1
[0, 0, 3, 1, 1, 2, 0, 4, 3, 0, 0, 2, 0, 1]
***** skip layer  2
[0, 0, 2, 1, 1, 2, 0, 4, 3, 0, 0, 2, 0, 1]
***** skip layer  3
[0, 0, 2, 0, 1, 2, 0, 4, 3, 0, 0, 2, 0, 1]
***** skip layer  4
[0, 0, 2, 0, 0, 2, 0, 4, 3, 0, 0, 2, 0, 1]
***** skip layer  5
[0, 0, 2, 0, 0, 1, 0, 4, 3, 0, 0, 2, 0, 1]
optimize layer  6
backward train epoch:  96
test acc:  0.1
forward train acc:  0.99942  and loss:  0.9803621910978109
test acc:  0.9193
forward train acc:  0.99942  and loss:  0.6862600466884032
test acc:  0.9169
forward train acc:  0.9991  and loss:  1.1102563989552436
test acc:  0.9186
forward train acc:  0.99928  and loss:  0.9487746395170689
test acc:  0.9187
forward train acc:  0.99928  and loss:  0.8738782779255416
test acc:  0.9188
forward train acc:  0.99952  and loss:  0.5553242447931552
test acc:  0.919
forward train acc:  0.99948  and loss:  0.7397042758457246
test acc:  0.9172
forward train acc:  0.99974  and loss:  0.41777927996008657
test acc:  0.9189
forward train acc:  0.99954  and loss:  0.6256505706260214
test acc:  0.9191
forward train acc:  0.99976  and loss:  0.39487359940540045
test acc:  0.9193
********** reverse layer  6  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8046875  ==>  150 / 768
layer  10  :  0.8984375  ==>  78 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  7
[0, 0, 2, 0, 0, 1, 5, 3, 3, 0, 0, 2, 0, 1]
***** skip layer  8
[0, 0, 2, 0, 0, 1, 5, 3, 2, 0, 0, 2, 0, 1]
optimize layer  9
backward train epoch:  66
test acc:  0.1007
forward train acc:  0.99954  and loss:  0.6351796896342421
test acc:  0.918
forward train acc:  0.99926  and loss:  0.9266229753993684
test acc:  0.9179
forward train acc:  0.99932  and loss:  0.784099754091585
test acc:  0.9176
forward train acc:  0.9996  and loss:  0.5357421457738383
test acc:  0.9189
forward train acc:  0.99958  and loss:  0.5769303695415147
test acc:  0.9198
forward train acc:  0.99956  and loss:  0.6667620219377568
test acc:  0.9184
forward train acc:  0.99966  and loss:  0.41527883877279237
test acc:  0.9193
forward train acc:  0.99964  and loss:  0.4064495514903683
test acc:  0.9179
forward train acc:  0.99976  and loss:  0.5138450941485644
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.8984375  ==>  78 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  10
backward train epoch:  166
test acc:  0.1031
forward train acc:  0.99938  and loss:  0.9296022587514017
test acc:  0.9181
forward train acc:  0.99974  and loss:  0.3763347987260204
test acc:  0.9189
forward train acc:  0.9995  and loss:  0.7811454966722522
test acc:  0.917
forward train acc:  0.9996  and loss:  0.4775990200287197
test acc:  0.9204
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.90625  ==>  72 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  11
[0, 0, 2, 0, 0, 1, 5, 3, 2, 0, 0, 1, 0, 1]
optimize layer  12
backward train epoch:  119
test acc:  0.1
forward train acc:  0.1  and loss:  917.2988555431366
test acc:  0.1
forward train acc:  0.1  and loss:  912.9623739719391
test acc:  0.1
forward train acc:  0.1  and loss:  909.6103653907776
test acc:  0.1
forward train acc:  0.1  and loss:  907.5771646499634
test acc:  0.1
forward train acc:  0.1  and loss:  906.4450781345367
test acc:  0.1
forward train acc:  0.1  and loss:  905.4314575195312
test acc:  0.1
forward train acc:  0.1  and loss:  904.5688743591309
test acc:  0.1
forward train acc:  0.1  and loss:  904.0015408992767
test acc:  0.1
forward train acc:  0.1  and loss:  903.6518187522888
test acc:  0.1
forward train acc:  0.1  and loss:  903.343955039978
test acc:  0.1
********** reverse layer  12  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.90625  ==>  72 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  13
[0, 0, 2, 0, 0, 1, 5, 3, 2, 0, 0, 1, 5, 0]
optimize layer  0
backward train epoch:  260
test acc:  0.1035
forward train acc:  0.99928  and loss:  0.8600484111229889
test acc:  0.9175
forward train acc:  0.99942  and loss:  0.7838969484437257
test acc:  0.9184
forward train acc:  0.99932  and loss:  1.0384478985215537
test acc:  0.9183
forward train acc:  0.99962  and loss:  0.5403898693912197
test acc:  0.9193
forward train acc:  0.99956  and loss:  0.4796026883414015
test acc:  0.9166
forward train acc:  0.99966  and loss:  0.5375227898839512
test acc:  0.9189
forward train acc:  0.99974  and loss:  0.32749275636160746
test acc:  0.9193
forward train acc:  0.9997  and loss:  0.37434872168523725
test acc:  0.9176
forward train acc:  0.99968  and loss:  0.4419845553857158
test acc:  0.9187
forward train acc:  0.99968  and loss:  0.38765649337437935
test acc:  0.9182
********** reverse layer  0  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.90625  ==>  72 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  1
backward train epoch:  433
test acc:  0.1
forward train acc:  0.99924  and loss:  0.9821397668347345
test acc:  0.916
forward train acc:  0.99922  and loss:  0.9308499924518401
test acc:  0.9164
forward train acc:  0.99954  and loss:  0.6729849831099273
test acc:  0.916
forward train acc:  0.99944  and loss:  0.6917845757561736
test acc:  0.9162
forward train acc:  0.99956  and loss:  0.7103388545947382
test acc:  0.9183
forward train acc:  0.9996  and loss:  0.5520651538390666
test acc:  0.9185
forward train acc:  0.99964  and loss:  0.5136463299859315
test acc:  0.9178
forward train acc:  0.9995  and loss:  0.6045891912071966
test acc:  0.9189
forward train acc:  0.99966  and loss:  0.41487149678869173
test acc:  0.9184
forward train acc:  0.99966  and loss:  0.3573085263778921
test acc:  0.9184
********** reverse layer  1  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.90625  ==>  72 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  2
[5, 5, 1, 0, 0, 1, 5, 3, 2, 0, 0, 1, 5, 0]
optimize layer  3
backward train epoch:  2
test acc:  0.1
forward train acc:  0.99896  and loss:  1.4547130602986726
test acc:  0.9157
forward train acc:  0.99896  and loss:  1.364397004188504
test acc:  0.9166
forward train acc:  0.99902  and loss:  1.2340339543479786
test acc:  0.9158
forward train acc:  0.9992  and loss:  0.9997334943909664
test acc:  0.9167
forward train acc:  0.99922  and loss:  1.094985514471773
test acc:  0.9161
forward train acc:  0.99946  and loss:  0.6622050181322265
test acc:  0.9174
forward train acc:  0.99942  and loss:  0.7333311795191548
test acc:  0.9162
forward train acc:  0.99942  and loss:  0.8467804204119602
test acc:  0.9166
forward train acc:  0.99954  and loss:  0.5798032687453087
test acc:  0.9174
forward train acc:  0.99954  and loss:  0.6619865666143596
test acc:  0.9175
********** reverse layer  3  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.90625  ==>  72 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  4
backward train epoch:  414
test acc:  0.1
forward train acc:  0.99846  and loss:  1.9958588072913699
test acc:  0.916
forward train acc:  0.99846  and loss:  1.9243854906526394
test acc:  0.9153
forward train acc:  0.99912  and loss:  1.157094830996357
test acc:  0.9154
forward train acc:  0.9991  and loss:  1.010301292699296
test acc:  0.9152
forward train acc:  0.99904  and loss:  1.1718951376969926
test acc:  0.9179
forward train acc:  0.9992  and loss:  1.1272955328167882
test acc:  0.9182
forward train acc:  0.99926  and loss:  0.8919448481174186
test acc:  0.9163
forward train acc:  0.9992  and loss:  0.941221302782651
test acc:  0.9189
forward train acc:  0.99952  and loss:  0.64582160115242
test acc:  0.9187
forward train acc:  0.99926  and loss:  0.9440524301026016
test acc:  0.9176
********** reverse layer  4  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.90625  ==>  72 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  5
[5, 5, 1, 5, 5, 0, 5, 3, 2, 0, 0, 1, 5, 0]
***** skip layer  6
[5, 5, 1, 5, 5, 0, 4, 3, 2, 0, 0, 1, 5, 0]
***** skip layer  7
[5, 5, 1, 5, 5, 0, 4, 2, 2, 0, 0, 1, 5, 0]
***** skip layer  8
[5, 5, 1, 5, 5, 0, 4, 2, 1, 0, 0, 1, 5, 0]
optimize layer  9
backward train epoch:  114
test acc:  0.1004
forward train acc:  0.9995  and loss:  0.5963495108881034
test acc:  0.9165
forward train acc:  0.99938  and loss:  0.7986539782432374
test acc:  0.915
forward train acc:  0.99924  and loss:  0.828707506734645
test acc:  0.917
forward train acc:  0.99956  and loss:  0.6081466267642099
test acc:  0.918
forward train acc:  0.99968  and loss:  0.47622006936580874
test acc:  0.9182
forward train acc:  0.9997  and loss:  0.3999180436367169
test acc:  0.9172
forward train acc:  0.99976  and loss:  0.3553601305175107
test acc:  0.918
forward train acc:  0.99966  and loss:  0.4935686060816806
test acc:  0.9173
forward train acc:  0.99974  and loss:  0.3514112670090981
test acc:  0.9171
forward train acc:  0.99976  and loss:  0.30263968242798
test acc:  0.9177
********** reverse layer  9  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.90625  ==>  72 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  10
backward train epoch:  84
test acc:  0.1002
forward train acc:  0.99962  and loss:  0.5502112167596351
test acc:  0.917
forward train acc:  0.99948  and loss:  0.5901518522878177
test acc:  0.9181
forward train acc:  0.9996  and loss:  0.5096413267528987
test acc:  0.9179
forward train acc:  0.99958  and loss:  0.4913083149876911
test acc:  0.9181
forward train acc:  0.99968  and loss:  0.4833987594865903
test acc:  0.9176
forward train acc:  0.99964  and loss:  0.5150199217532645
test acc:  0.9184
forward train acc:  0.99972  and loss:  0.37680197760346346
test acc:  0.9172
forward train acc:  0.9996  and loss:  0.5451637966325507
test acc:  0.9183
forward train acc:  0.99962  and loss:  0.4487145397506538
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  11
[5, 5, 1, 5, 5, 0, 4, 2, 1, 5, 0, 0, 5, 0]
***** skip layer  12
[5, 5, 1, 5, 5, 0, 4, 2, 1, 5, 0, 0, 4, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.1157
forward train acc:  0.97124  and loss:  89.24499990046024
test acc:  0.9102
forward train acc:  0.99788  and loss:  62.54934659600258
test acc:  0.9114
forward train acc:  0.9984  and loss:  56.949605129659176
test acc:  0.912
forward train acc:  0.999  and loss:  53.03008794039488
test acc:  0.9144
forward train acc:  0.9988  and loss:  50.917776957154274
test acc:  0.9146
forward train acc:  0.99882  and loss:  48.94287807494402
test acc:  0.9148
forward train acc:  0.99934  and loss:  46.86971478164196
test acc:  0.9154
forward train acc:  0.99914  and loss:  45.46965556591749
test acc:  0.916
forward train acc:  0.99914  and loss:  44.69394666701555
test acc:  0.9155
forward train acc:  0.99906  and loss:  43.84377497434616
test acc:  0.9156
********** reverse layer  13  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  0
[4, 5, 1, 5, 5, 0, 4, 2, 1, 5, 0, 0, 4, 5]
***** skip layer  1
[4, 4, 1, 5, 5, 0, 4, 2, 1, 5, 0, 0, 4, 5]
***** skip layer  2
[4, 4, 0, 5, 5, 0, 4, 2, 1, 5, 0, 0, 4, 5]
***** skip layer  3
[4, 4, 0, 4, 5, 0, 4, 2, 1, 5, 0, 0, 4, 5]
***** skip layer  4
[4, 4, 0, 4, 4, 0, 4, 2, 1, 5, 0, 0, 4, 5]
optimize layer  5
backward train epoch:  818
test acc:  0.1056
forward train acc:  0.9989  and loss:  1.6693670552922413
test acc:  0.9159
forward train acc:  0.99894  and loss:  1.7065290397586068
test acc:  0.9185
forward train acc:  0.99926  and loss:  1.0695121723983902
test acc:  0.9172
forward train acc:  0.99918  and loss:  1.1357997677841922
test acc:  0.9183
forward train acc:  0.99936  and loss:  0.8756459702999564
test acc:  0.9191
forward train acc:  0.99952  and loss:  0.6078564074705355
test acc:  0.9181
forward train acc:  0.9994  and loss:  1.0177765464177355
test acc:  0.9165
forward train acc:  0.99944  and loss:  0.8441099847259466
test acc:  0.9179
forward train acc:  0.99954  and loss:  0.7415996356648975
test acc:  0.9175
forward train acc:  0.99956  and loss:  0.7352884879364865
test acc:  0.9182
********** reverse layer  5  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  6
[4, 4, 0, 4, 4, 5, 3, 2, 1, 5, 0, 0, 4, 5]
***** skip layer  7
[4, 4, 0, 4, 4, 5, 3, 1, 1, 5, 0, 0, 4, 5]
***** skip layer  8
[4, 4, 0, 4, 4, 5, 3, 1, 0, 5, 0, 0, 4, 5]
***** skip layer  9
[4, 4, 0, 4, 4, 5, 3, 1, 0, 4, 0, 0, 4, 5]
optimize layer  10
backward train epoch:  279
test acc:  0.1007
forward train acc:  0.9995  and loss:  0.7795593462287798
test acc:  0.9163
forward train acc:  0.9995  and loss:  0.8372351335710846
test acc:  0.9159
forward train acc:  0.99966  and loss:  0.5696954198356252
test acc:  0.9175
forward train acc:  0.9997  and loss:  0.5644516224419931
test acc:  0.9168
forward train acc:  0.99946  and loss:  0.7034930719164549
test acc:  0.9161
forward train acc:  0.99962  and loss:  0.5346531869363389
test acc:  0.9168
forward train acc:  0.99964  and loss:  0.4542021759552881
test acc:  0.9169
forward train acc:  0.9996  and loss:  0.4954708523728186
test acc:  0.9186
forward train acc:  0.99978  and loss:  0.3072174646149506
test acc:  0.9171
forward train acc:  0.9997  and loss:  0.4343228301513591
test acc:  0.9185
********** reverse layer  10  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  11
backward train epoch:  131
test acc:  0.1048
forward train acc:  0.99958  and loss:  0.6155077940638876
test acc:  0.9178
forward train acc:  0.99922  and loss:  1.0840032187043107
test acc:  0.9159
forward train acc:  0.99946  and loss:  0.7292067268426763
test acc:  0.9164
forward train acc:  0.99946  and loss:  0.7815938256680965
test acc:  0.9163
forward train acc:  0.9994  and loss:  0.7699955218995456
test acc:  0.9172
forward train acc:  0.99972  and loss:  0.4077434015853214
test acc:  0.9176
forward train acc:  0.99958  and loss:  0.4728999354992993
test acc:  0.9175
forward train acc:  0.99966  and loss:  0.3833059386961395
test acc:  0.9175
forward train acc:  0.9997  and loss:  0.3907578395374003
test acc:  0.9186
forward train acc:  0.99978  and loss:  0.39437732863007113
test acc:  0.9177
********** reverse layer  11  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  12
[4, 4, 0, 4, 4, 5, 3, 1, 0, 4, 5, 5, 3, 5]
***** skip layer  13
[4, 4, 0, 4, 4, 5, 3, 1, 0, 4, 5, 5, 3, 4]
***** skip layer  0
[3, 4, 0, 4, 4, 5, 3, 1, 0, 4, 5, 5, 3, 4]
***** skip layer  1
[3, 3, 0, 4, 4, 5, 3, 1, 0, 4, 5, 5, 3, 4]
optimize layer  2
backward train epoch:  87
test acc:  0.1005
forward train acc:  0.99908  and loss:  1.318035894586501
test acc:  0.9151
forward train acc:  0.99886  and loss:  1.3453594316888484
test acc:  0.9154
forward train acc:  0.9989  and loss:  1.1412780183309224
test acc:  0.9139
forward train acc:  0.99914  and loss:  1.1035886215686332
test acc:  0.9144
forward train acc:  0.99952  and loss:  0.7430195465640281
test acc:  0.9158
forward train acc:  0.99948  and loss:  0.693511452365783
test acc:  0.9164
forward train acc:  0.99932  and loss:  0.8390237698768033
test acc:  0.9164
forward train acc:  0.99952  and loss:  0.6857690088036179
test acc:  0.917
forward train acc:  0.99952  and loss:  0.6037023860262707
test acc:  0.9158
forward train acc:  0.99952  and loss:  0.6099226168735186
test acc:  0.917
********** reverse layer  2  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  3
[3, 3, 5, 3, 4, 5, 3, 1, 0, 4, 5, 5, 3, 4]
***** skip layer  4
[3, 3, 5, 3, 3, 5, 3, 1, 0, 4, 5, 5, 3, 4]
***** skip layer  5
[3, 3, 5, 3, 3, 4, 3, 1, 0, 4, 5, 5, 3, 4]
***** skip layer  6
[3, 3, 5, 3, 3, 4, 2, 1, 0, 4, 5, 5, 3, 4]
***** skip layer  7
[3, 3, 5, 3, 3, 4, 2, 0, 0, 4, 5, 5, 3, 4]
optimize layer  8
backward train epoch:  83
test acc:  0.0819
forward train acc:  0.99474  and loss:  7.367506538052112
test acc:  0.9113
forward train acc:  0.99692  and loss:  4.009391667146701
test acc:  0.9132
forward train acc:  0.99804  and loss:  2.5406884810654446
test acc:  0.9117
forward train acc:  0.99816  and loss:  2.2376523724815343
test acc:  0.9134
forward train acc:  0.99834  and loss:  2.2099102753854822
test acc:  0.9143
forward train acc:  0.99874  and loss:  1.778649838095589
test acc:  0.9135
forward train acc:  0.99864  and loss:  1.7476974204764701
test acc:  0.9138
forward train acc:  0.99896  and loss:  1.464732053442276
test acc:  0.9147
forward train acc:  0.99886  and loss:  1.7004030675452668
test acc:  0.9142
forward train acc:  0.99882  and loss:  1.443790415796684
test acc:  0.9141
********** reverse layer  8  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  9
[3, 3, 5, 3, 3, 4, 2, 0, 5, 3, 5, 5, 3, 4]
***** skip layer  10
[3, 3, 5, 3, 3, 4, 2, 0, 5, 3, 4, 5, 3, 4]
***** skip layer  11
[3, 3, 5, 3, 3, 4, 2, 0, 5, 3, 4, 4, 3, 4]
***** skip layer  12
[3, 3, 5, 3, 3, 4, 2, 0, 5, 3, 4, 4, 2, 4]
***** skip layer  13
[3, 3, 5, 3, 3, 4, 2, 0, 5, 3, 4, 4, 2, 3]
***** skip layer  0
[2, 3, 5, 3, 3, 4, 2, 0, 5, 3, 4, 4, 2, 3]
***** skip layer  1
[2, 2, 5, 3, 3, 4, 2, 0, 5, 3, 4, 4, 2, 3]
***** skip layer  2
[2, 2, 4, 3, 3, 4, 2, 0, 5, 3, 4, 4, 2, 3]
***** skip layer  3
[2, 2, 4, 2, 3, 4, 2, 0, 5, 3, 4, 4, 2, 3]
***** skip layer  4
[2, 2, 4, 2, 2, 4, 2, 0, 5, 3, 4, 4, 2, 3]
***** skip layer  5
[2, 2, 4, 2, 2, 3, 2, 0, 5, 3, 4, 4, 2, 3]
***** skip layer  6
[2, 2, 4, 2, 2, 3, 1, 0, 5, 3, 4, 4, 2, 3]
optimize layer  7
backward train epoch:  975
test acc:  0.0732
forward train acc:  0.99534  and loss:  6.846234968979843
test acc:  0.9119
forward train acc:  0.99768  and loss:  2.8004828876291867
test acc:  0.9125
forward train acc:  0.99786  and loss:  2.529477733929525
test acc:  0.9123
forward train acc:  0.99856  and loss:  1.7283177810022607
test acc:  0.9157
forward train acc:  0.99884  and loss:  1.5125204750220291
test acc:  0.9151
forward train acc:  0.99876  and loss:  1.4887048545642756
test acc:  0.9142
forward train acc:  0.9989  and loss:  1.258294461571495
test acc:  0.9146
forward train acc:  0.99884  and loss:  1.5797853127587587
test acc:  0.9141
forward train acc:  0.9988  and loss:  1.3823878020921256
test acc:  0.9132
forward train acc:  0.99916  and loss:  1.1741047877585515
test acc:  0.9144
********** reverse layer  7  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  8
[2, 2, 4, 2, 2, 3, 1, 5, 4, 3, 4, 4, 2, 3]
***** skip layer  9
[2, 2, 4, 2, 2, 3, 1, 5, 4, 2, 4, 4, 2, 3]
***** skip layer  10
[2, 2, 4, 2, 2, 3, 1, 5, 4, 2, 3, 4, 2, 3]
***** skip layer  11
[2, 2, 4, 2, 2, 3, 1, 5, 4, 2, 3, 3, 2, 3]
***** skip layer  12
[2, 2, 4, 2, 2, 3, 1, 5, 4, 2, 3, 3, 1, 3]
***** skip layer  13
[2, 2, 4, 2, 2, 3, 1, 5, 4, 2, 3, 3, 1, 2]
***** skip layer  0
[1, 2, 4, 2, 2, 3, 1, 5, 4, 2, 3, 3, 1, 2]
***** skip layer  1
[1, 1, 4, 2, 2, 3, 1, 5, 4, 2, 3, 3, 1, 2]
***** skip layer  2
[1, 1, 3, 2, 2, 3, 1, 5, 4, 2, 3, 3, 1, 2]
***** skip layer  3
[1, 1, 3, 1, 2, 3, 1, 5, 4, 2, 3, 3, 1, 2]
***** skip layer  4
[1, 1, 3, 1, 1, 3, 1, 5, 4, 2, 3, 3, 1, 2]
***** skip layer  5
[1, 1, 3, 1, 1, 2, 1, 5, 4, 2, 3, 3, 1, 2]
***** skip layer  6
[1, 1, 3, 1, 1, 2, 0, 5, 4, 2, 3, 3, 1, 2]
***** skip layer  7
[1, 1, 3, 1, 1, 2, 0, 4, 4, 2, 3, 3, 1, 2]
***** skip layer  8
[1, 1, 3, 1, 1, 2, 0, 4, 3, 2, 3, 3, 1, 2]
***** skip layer  9
[1, 1, 3, 1, 1, 2, 0, 4, 3, 1, 3, 3, 1, 2]
***** skip layer  10
[1, 1, 3, 1, 1, 2, 0, 4, 3, 1, 2, 3, 1, 2]
***** skip layer  11
[1, 1, 3, 1, 1, 2, 0, 4, 3, 1, 2, 2, 1, 2]
***** skip layer  12
[1, 1, 3, 1, 1, 2, 0, 4, 3, 1, 2, 2, 0, 2]
***** skip layer  13
[1, 1, 3, 1, 1, 2, 0, 4, 3, 1, 2, 2, 0, 1]
***** skip layer  0
[0, 1, 3, 1, 1, 2, 0, 4, 3, 1, 2, 2, 0, 1]
***** skip layer  1
[0, 0, 3, 1, 1, 2, 0, 4, 3, 1, 2, 2, 0, 1]
***** skip layer  2
[0, 0, 2, 1, 1, 2, 0, 4, 3, 1, 2, 2, 0, 1]
***** skip layer  3
[0, 0, 2, 0, 1, 2, 0, 4, 3, 1, 2, 2, 0, 1]
***** skip layer  4
[0, 0, 2, 0, 0, 2, 0, 4, 3, 1, 2, 2, 0, 1]
***** skip layer  5
[0, 0, 2, 0, 0, 1, 0, 4, 3, 1, 2, 2, 0, 1]
optimize layer  6
backward train epoch:  95
test acc:  0.0927
forward train acc:  0.999  and loss:  1.4049710415347363
test acc:  0.9165
forward train acc:  0.99954  and loss:  0.5402174759219633
test acc:  0.9164
forward train acc:  0.99934  and loss:  0.8464495580046787
test acc:  0.9166
forward train acc:  0.99938  and loss:  0.7891570112115005
test acc:  0.9183
forward train acc:  0.99966  and loss:  0.49934834908344783
test acc:  0.9176
forward train acc:  0.99962  and loss:  0.44570720257252106
test acc:  0.9184
forward train acc:  0.99984  and loss:  0.25074500270420685
test acc:  0.9162
forward train acc:  0.99966  and loss:  0.46241944585199235
test acc:  0.9175
forward train acc:  0.99974  and loss:  0.332067740324419
test acc:  0.9167
forward train acc:  0.99974  and loss:  0.324710661006975
test acc:  0.917
********** reverse layer  6  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  7
[0, 0, 2, 0, 0, 1, 5, 3, 3, 1, 2, 2, 0, 1]
***** skip layer  8
[0, 0, 2, 0, 0, 1, 5, 3, 2, 1, 2, 2, 0, 1]
***** skip layer  9
[0, 0, 2, 0, 0, 1, 5, 3, 2, 0, 2, 2, 0, 1]
***** skip layer  10
[0, 0, 2, 0, 0, 1, 5, 3, 2, 0, 1, 2, 0, 1]
***** skip layer  11
[0, 0, 2, 0, 0, 1, 5, 3, 2, 0, 1, 1, 0, 1]
optimize layer  12
backward train epoch:  198
test acc:  0.1
forward train acc:  0.1  and loss:  915.6322250366211
test acc:  0.1
forward train acc:  0.1  and loss:  911.2814562320709
test acc:  0.1
forward train acc:  0.1  and loss:  908.0380187034607
test acc:  0.1
forward train acc:  0.1  and loss:  906.1372222900391
test acc:  0.1
forward train acc:  0.1  and loss:  905.1119132041931
test acc:  0.1
forward train acc:  0.1  and loss:  904.2199223041534
test acc:  0.1
forward train acc:  0.1  and loss:  903.4781908988953
test acc:  0.1
forward train acc:  0.1  and loss:  902.9885964393616
test acc:  0.1
forward train acc:  0.1  and loss:  902.702855348587
test acc:  0.1
forward train acc:  0.1  and loss:  902.4334454536438
test acc:  0.1
********** reverse layer  12  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  13
[0, 0, 2, 0, 0, 1, 5, 3, 2, 0, 1, 1, 5, 0]
optimize layer  0
backward train epoch:  147
test acc:  0.0962
forward train acc:  0.99972  and loss:  0.5070950779772829
test acc:  0.9151
forward train acc:  0.9995  and loss:  0.6233758122834843
test acc:  0.9149
forward train acc:  0.99944  and loss:  0.919075723431888
test acc:  0.9159
forward train acc:  0.99946  and loss:  0.7338222863036208
test acc:  0.9158
forward train acc:  0.99954  and loss:  0.5903687472891761
test acc:  0.9176
forward train acc:  0.99976  and loss:  0.30656525905942544
test acc:  0.9172
forward train acc:  0.99944  and loss:  0.8243095788639039
test acc:  0.9171
forward train acc:  0.99976  and loss:  0.3342509061731107
test acc:  0.9179
forward train acc:  0.99978  and loss:  0.3817039014029433
test acc:  0.9182
forward train acc:  0.99972  and loss:  0.3305138497817097
test acc:  0.918
********** reverse layer  0  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  1
backward train epoch:  659
test acc:  0.0891
forward train acc:  0.99748  and loss:  3.490038961288519
test acc:  0.9126
forward train acc:  0.9987  and loss:  1.8639231362030841
test acc:  0.9126
forward train acc:  0.99864  and loss:  1.760942010441795
test acc:  0.9115
forward train acc:  0.99902  and loss:  1.2708047159248963
test acc:  0.9138
forward train acc:  0.99928  and loss:  1.0747667353280121
test acc:  0.9121
forward train acc:  0.9989  and loss:  1.36634660193522
test acc:  0.9145
forward train acc:  0.9994  and loss:  0.8458813159595593
test acc:  0.9156
forward train acc:  0.99918  and loss:  1.143740994419204
test acc:  0.9154
forward train acc:  0.99948  and loss:  0.7245210588298505
test acc:  0.9151
forward train acc:  0.9995  and loss:  0.769882948610757
test acc:  0.9155
********** reverse layer  1  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  2
[5, 5, 1, 0, 0, 1, 5, 3, 2, 0, 1, 1, 5, 0]
optimize layer  3
backward train epoch:  93
test acc:  0.1
forward train acc:  0.99902  and loss:  1.4116747223306447
test acc:  0.9156
forward train acc:  0.99892  and loss:  1.372096523307846
test acc:  0.9155
forward train acc:  0.99936  and loss:  0.9169885192532092
test acc:  0.9164
forward train acc:  0.99944  and loss:  0.8135143703839276
test acc:  0.9182
forward train acc:  0.99954  and loss:  0.6692477160831913
test acc:  0.9177
forward train acc:  0.9994  and loss:  0.8039402675785823
test acc:  0.9161
forward train acc:  0.99962  and loss:  0.5119034576055128
test acc:  0.918
forward train acc:  0.9996  and loss:  0.5153264914843021
test acc:  0.9169
forward train acc:  0.99956  and loss:  0.4737937757090549
test acc:  0.9169
forward train acc:  0.9997  and loss:  0.419567120319698
test acc:  0.9193
********** reverse layer  3  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  4
backward train epoch:  658
test acc:  0.1
forward train acc:  0.99896  and loss:  1.41874953918159
test acc:  0.915
forward train acc:  0.99912  and loss:  1.2781434452044778
test acc:  0.9137
forward train acc:  0.99904  and loss:  1.2618967375310604
test acc:  0.9158
forward train acc:  0.99936  and loss:  0.7538406586536439
test acc:  0.9166
forward train acc:  0.9994  and loss:  0.9688418765726965
test acc:  0.9161
forward train acc:  0.99944  and loss:  0.7097736514406279
test acc:  0.9148
forward train acc:  0.9994  and loss:  0.7000432837812696
test acc:  0.9166
forward train acc:  0.9995  and loss:  0.712382639554562
test acc:  0.9167
forward train acc:  0.99958  and loss:  0.5347628708113916
test acc:  0.9161
forward train acc:  0.99952  and loss:  0.5661796599524678
test acc:  0.916
********** reverse layer  4  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  5
[5, 5, 1, 5, 5, 0, 5, 3, 2, 0, 1, 1, 5, 0]
***** skip layer  6
[5, 5, 1, 5, 5, 0, 4, 3, 2, 0, 1, 1, 5, 0]
***** skip layer  7
[5, 5, 1, 5, 5, 0, 4, 2, 2, 0, 1, 1, 5, 0]
***** skip layer  8
[5, 5, 1, 5, 5, 0, 4, 2, 1, 0, 1, 1, 5, 0]
optimize layer  9
backward train epoch:  98
test acc:  0.1
forward train acc:  0.99948  and loss:  0.6447965021361597
test acc:  0.9179
forward train acc:  0.99954  and loss:  0.6056008871237282
test acc:  0.9163
forward train acc:  0.99958  and loss:  0.5118719094316475
test acc:  0.9161
forward train acc:  0.99966  and loss:  0.45005530406342587
test acc:  0.9162
forward train acc:  0.9997  and loss:  0.35458869446301833
test acc:  0.9155
forward train acc:  0.9997  and loss:  0.3413884143810719
test acc:  0.9168
forward train acc:  0.99974  and loss:  0.2883651987358462
test acc:  0.9164
forward train acc:  0.99982  and loss:  0.2843587646711967
test acc:  0.9178
forward train acc:  0.99984  and loss:  0.29513552953721955
test acc:  0.918
forward train acc:  0.9999  and loss:  0.19402342591638444
test acc:  0.9175
********** reverse layer  9  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  10
[5, 5, 1, 5, 5, 0, 4, 2, 1, 5, 0, 1, 5, 0]
***** skip layer  11
[5, 5, 1, 5, 5, 0, 4, 2, 1, 5, 0, 0, 5, 0]
***** skip layer  12
[5, 5, 1, 5, 5, 0, 4, 2, 1, 5, 0, 0, 4, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.1002
forward train acc:  0.78376  and loss:  241.61332648992538
test acc:  0.7771
forward train acc:  0.86698  and loss:  179.82908779382706
test acc:  0.7997
forward train acc:  0.88554  and loss:  165.53616988658905
test acc:  0.8079
forward train acc:  0.89216  and loss:  158.3431654572487
test acc:  0.8083
forward train acc:  0.89446  and loss:  153.16288831830025
test acc:  0.8107
forward train acc:  0.89588  and loss:  149.44684040546417
test acc:  0.8129
forward train acc:  0.89662  and loss:  145.2242003083229
test acc:  0.8128
forward train acc:  0.89772  and loss:  142.96757704019547
test acc:  0.813
forward train acc:  0.89744  and loss:  140.9762861430645
test acc:  0.8143
forward train acc:  0.89738  and loss:  139.63285134732723
test acc:  0.8155
********** reverse layer  13  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  0
[4, 5, 1, 5, 5, 0, 4, 2, 1, 5, 0, 0, 4, 5]
***** skip layer  1
[4, 4, 1, 5, 5, 0, 4, 2, 1, 5, 0, 0, 4, 5]
***** skip layer  2
[4, 4, 0, 5, 5, 0, 4, 2, 1, 5, 0, 0, 4, 5]
***** skip layer  3
[4, 4, 0, 4, 5, 0, 4, 2, 1, 5, 0, 0, 4, 5]
***** skip layer  4
[4, 4, 0, 4, 4, 0, 4, 2, 1, 5, 0, 0, 4, 5]
optimize layer  5
backward train epoch:  253
test acc:  0.1
forward train acc:  0.99874  and loss:  2.1930907047353685
test acc:  0.918
forward train acc:  0.99922  and loss:  1.21141067481949
test acc:  0.917
forward train acc:  0.99902  and loss:  1.4869819478481077
test acc:  0.9159
forward train acc:  0.99932  and loss:  0.9887665158894379
test acc:  0.9177
forward train acc:  0.99946  and loss:  0.9102586157678161
test acc:  0.9174
forward train acc:  0.99948  and loss:  0.7875108543958049
test acc:  0.9166
forward train acc:  0.99948  and loss:  0.7438972724485211
test acc:  0.9158
forward train acc:  0.99966  and loss:  0.5194006774108857
test acc:  0.9165
forward train acc:  0.99966  and loss:  0.4571316488145385
test acc:  0.9173
forward train acc:  0.9994  and loss:  0.8303515488223638
test acc:  0.9176
********** reverse layer  5  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  6
[4, 4, 0, 4, 4, 5, 3, 2, 1, 5, 0, 0, 4, 5]
***** skip layer  7
[4, 4, 0, 4, 4, 5, 3, 1, 1, 5, 0, 0, 4, 5]
***** skip layer  8
[4, 4, 0, 4, 4, 5, 3, 1, 0, 5, 0, 0, 4, 5]
***** skip layer  9
[4, 4, 0, 4, 4, 5, 3, 1, 0, 4, 0, 0, 4, 5]
optimize layer  10
backward train epoch:  538
test acc:  0.1237
forward train acc:  0.99956  and loss:  0.4915768562059384
test acc:  0.9158
forward train acc:  0.99962  and loss:  0.48283449184236815
test acc:  0.9158
forward train acc:  0.99968  and loss:  0.55330867550947
test acc:  0.9158
forward train acc:  0.9995  and loss:  0.6691428666454158
test acc:  0.918
forward train acc:  0.99976  and loss:  0.2869954791603959
test acc:  0.9179
forward train acc:  0.9997  and loss:  0.4012343726135441
test acc:  0.9185
forward train acc:  0.99968  and loss:  0.39565687166759744
test acc:  0.9167
forward train acc:  0.99968  and loss:  0.3483119952434208
test acc:  0.9169
forward train acc:  0.99982  and loss:  0.29213159444043413
test acc:  0.9165
forward train acc:  0.99976  and loss:  0.37095886902534403
test acc:  0.9174
********** reverse layer  10  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  11
backward train epoch:  52
test acc:  0.1049
forward train acc:  0.99962  and loss:  0.43273041940119583
test acc:  0.9162
forward train acc:  0.99946  and loss:  0.8304363015413401
test acc:  0.9144
forward train acc:  0.9994  and loss:  0.6903460441244533
test acc:  0.916
forward train acc:  0.9996  and loss:  0.5925410906784236
test acc:  0.9162
forward train acc:  0.99966  and loss:  0.6353802861704025
test acc:  0.9171
forward train acc:  0.99968  and loss:  0.4675729434675304
test acc:  0.9161
forward train acc:  0.99982  and loss:  0.21767118378193118
test acc:  0.9171
forward train acc:  0.99982  and loss:  0.264691660337121
test acc:  0.9174
forward train acc:  0.99976  and loss:  0.3472427122287627
test acc:  0.9176
forward train acc:  0.99966  and loss:  0.48772104902309366
test acc:  0.9173
********** reverse layer  11  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  12
[4, 4, 0, 4, 4, 5, 3, 1, 0, 4, 5, 5, 3, 5]
***** skip layer  13
[4, 4, 0, 4, 4, 5, 3, 1, 0, 4, 5, 5, 3, 4]
***** skip layer  0
[3, 4, 0, 4, 4, 5, 3, 1, 0, 4, 5, 5, 3, 4]
***** skip layer  1
[3, 3, 0, 4, 4, 5, 3, 1, 0, 4, 5, 5, 3, 4]
optimize layer  2
backward train epoch:  348
test acc:  0.1087
forward train acc:  0.8949  and loss:  212.80930965393782
test acc:  0.8596
forward train acc:  0.93224  and loss:  95.7140672840178
test acc:  0.8727
forward train acc:  0.94938  and loss:  67.35482547804713
test acc:  0.881
forward train acc:  0.95996  and loss:  52.979712795466185
test acc:  0.8868
forward train acc:  0.9632  and loss:  48.11134000308812
test acc:  0.8878
forward train acc:  0.96864  and loss:  40.682328841648996
test acc:  0.8907
forward train acc:  0.97178  and loss:  35.28000380191952
test acc:  0.8921
forward train acc:  0.9746  and loss:  31.75017735362053
test acc:  0.8938
forward train acc:  0.9769  and loss:  29.557587534189224
test acc:  0.8949
forward train acc:  0.97792  and loss:  27.70890410337597
test acc:  0.8969
********** reverse layer  2  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  3
[3, 3, 5, 3, 4, 5, 3, 1, 0, 4, 5, 5, 3, 4]
***** skip layer  4
[3, 3, 5, 3, 3, 5, 3, 1, 0, 4, 5, 5, 3, 4]
***** skip layer  5
[3, 3, 5, 3, 3, 4, 3, 1, 0, 4, 5, 5, 3, 4]
***** skip layer  6
[3, 3, 5, 3, 3, 4, 2, 1, 0, 4, 5, 5, 3, 4]
***** skip layer  7
[3, 3, 5, 3, 3, 4, 2, 0, 0, 4, 5, 5, 3, 4]
optimize layer  8
backward train epoch:  42
test acc:  0.1055
forward train acc:  0.99564  and loss:  6.881369508220814
test acc:  0.9127
forward train acc:  0.99778  and loss:  3.318021153623704
test acc:  0.9143
forward train acc:  0.99808  and loss:  2.696801395912189
test acc:  0.9107
forward train acc:  0.99838  and loss:  2.0237042932931217
test acc:  0.9123
forward train acc:  0.9987  and loss:  2.053107154380996
test acc:  0.9127
forward train acc:  0.99896  and loss:  1.5433982303948142
test acc:  0.913
forward train acc:  0.99864  and loss:  1.7738180652813753
test acc:  0.9127
forward train acc:  0.99906  and loss:  1.304787373286672
test acc:  0.9125
forward train acc:  0.99924  and loss:  0.9605847019411158
test acc:  0.9142
forward train acc:  0.9991  and loss:  1.1716641566017643
test acc:  0.9135
********** reverse layer  8  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  9
[3, 3, 5, 3, 3, 4, 2, 0, 5, 3, 5, 5, 3, 4]
***** skip layer  10
[3, 3, 5, 3, 3, 4, 2, 0, 5, 3, 4, 5, 3, 4]
***** skip layer  11
[3, 3, 5, 3, 3, 4, 2, 0, 5, 3, 4, 4, 3, 4]
***** skip layer  12
[3, 3, 5, 3, 3, 4, 2, 0, 5, 3, 4, 4, 2, 4]
***** skip layer  13
[3, 3, 5, 3, 3, 4, 2, 0, 5, 3, 4, 4, 2, 3]
***** skip layer  0
[2, 3, 5, 3, 3, 4, 2, 0, 5, 3, 4, 4, 2, 3]
***** skip layer  1
[2, 2, 5, 3, 3, 4, 2, 0, 5, 3, 4, 4, 2, 3]
***** skip layer  2
[2, 2, 4, 3, 3, 4, 2, 0, 5, 3, 4, 4, 2, 3]
***** skip layer  3
[2, 2, 4, 2, 3, 4, 2, 0, 5, 3, 4, 4, 2, 3]
***** skip layer  4
[2, 2, 4, 2, 2, 4, 2, 0, 5, 3, 4, 4, 2, 3]
***** skip layer  5
[2, 2, 4, 2, 2, 3, 2, 0, 5, 3, 4, 4, 2, 3]
***** skip layer  6
[2, 2, 4, 2, 2, 3, 1, 0, 5, 3, 4, 4, 2, 3]
optimize layer  7
backward train epoch:  152
test acc:  0.1103
forward train acc:  0.99464  and loss:  7.410456475801766
test acc:  0.909
forward train acc:  0.99748  and loss:  3.5585978098679334
test acc:  0.9092
forward train acc:  0.9984  and loss:  1.9938637037412263
test acc:  0.9121
forward train acc:  0.9983  and loss:  2.254406370455399
test acc:  0.9134
forward train acc:  0.99874  and loss:  1.6531328171113273
test acc:  0.9143
forward train acc:  0.99856  and loss:  1.757102232571924
test acc:  0.9132
forward train acc:  0.99876  and loss:  1.6388384237652645
test acc:  0.9143
forward train acc:  0.9987  and loss:  1.549810701457318
test acc:  0.9151
forward train acc:  0.9992  and loss:  1.0673801976518007
test acc:  0.9153
forward train acc:  0.99904  and loss:  1.3450014353729784
test acc:  0.9145
********** reverse layer  7  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.6979166666666666  ==>  116 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  8
[2, 2, 4, 2, 2, 3, 1, 5, 4, 3, 4, 4, 2, 3]
***** skip layer  9
[2, 2, 4, 2, 2, 3, 1, 5, 4, 2, 4, 4, 2, 3]
***** skip layer  10
[2, 2, 4, 2, 2, 3, 1, 5, 4, 2, 3, 4, 2, 3]
***** skip layer  11
[2, 2, 4, 2, 2, 3, 1, 5, 4, 2, 3, 3, 2, 3]
***** skip layer  12
[2, 2, 4, 2, 2, 3, 1, 5, 4, 2, 3, 3, 1, 3]
***** skip layer  13
[2, 2, 4, 2, 2, 3, 1, 5, 4, 2, 3, 3, 1, 2]
***** skip layer  0
[1, 2, 4, 2, 2, 3, 1, 5, 4, 2, 3, 3, 1, 2]
***** skip layer  1
[1, 1, 4, 2, 2, 3, 1, 5, 4, 2, 3, 3, 1, 2]
***** skip layer  2
[1, 1, 3, 2, 2, 3, 1, 5, 4, 2, 3, 3, 1, 2]
***** skip layer  3
[1, 1, 3, 1, 2, 3, 1, 5, 4, 2, 3, 3, 1, 2]
***** skip layer  4
[1, 1, 3, 1, 1, 3, 1, 5, 4, 2, 3, 3, 1, 2]
***** skip layer  5
[1, 1, 3, 1, 1, 2, 1, 5, 4, 2, 3, 3, 1, 2]
***** skip layer  6
[1, 1, 3, 1, 1, 2, 0, 5, 4, 2, 3, 3, 1, 2]
***** skip layer  7
[1, 1, 3, 1, 1, 2, 0, 4, 4, 2, 3, 3, 1, 2]
***** skip layer  8
[1, 1, 3, 1, 1, 2, 0, 4, 3, 2, 3, 3, 1, 2]
***** skip layer  9
[1, 1, 3, 1, 1, 2, 0, 4, 3, 1, 3, 3, 1, 2]
***** skip layer  10
[1, 1, 3, 1, 1, 2, 0, 4, 3, 1, 2, 3, 1, 2]
***** skip layer  11
[1, 1, 3, 1, 1, 2, 0, 4, 3, 1, 2, 2, 1, 2]
***** skip layer  12
[1, 1, 3, 1, 1, 2, 0, 4, 3, 1, 2, 2, 0, 2]
***** skip layer  13
[1, 1, 3, 1, 1, 2, 0, 4, 3, 1, 2, 2, 0, 1]
***** skip layer  0
[0, 1, 3, 1, 1, 2, 0, 4, 3, 1, 2, 2, 0, 1]
***** skip layer  1
[0, 0, 3, 1, 1, 2, 0, 4, 3, 1, 2, 2, 0, 1]
***** skip layer  2
[0, 0, 2, 1, 1, 2, 0, 4, 3, 1, 2, 2, 0, 1]
***** skip layer  3
[0, 0, 2, 0, 1, 2, 0, 4, 3, 1, 2, 2, 0, 1]
***** skip layer  4
[0, 0, 2, 0, 0, 2, 0, 4, 3, 1, 2, 2, 0, 1]
***** skip layer  5
[0, 0, 2, 0, 0, 1, 0, 4, 3, 1, 2, 2, 0, 1]
optimize layer  6
backward train epoch:  126
test acc:  0.1015
forward train acc:  0.99888  and loss:  1.4919278111156018
test acc:  0.9187
forward train acc:  0.9997  and loss:  0.3654129515925888
test acc:  0.9199
forward train acc:  0.99978  and loss:  0.4294146786996862
test acc:  0.918
forward train acc:  0.99972  and loss:  0.46454417400673265
test acc:  0.9203
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  7
[0, 0, 2, 0, 0, 1, 0, 3, 3, 1, 2, 2, 0, 1]
***** skip layer  8
[0, 0, 2, 0, 0, 1, 0, 3, 2, 1, 2, 2, 0, 1]
***** skip layer  9
[0, 0, 2, 0, 0, 1, 0, 3, 2, 0, 2, 2, 0, 1]
***** skip layer  10
[0, 0, 2, 0, 0, 1, 0, 3, 2, 0, 1, 2, 0, 1]
***** skip layer  11
[0, 0, 2, 0, 0, 1, 0, 3, 2, 0, 1, 1, 0, 1]
optimize layer  12
backward train epoch:  127
test acc:  0.1
forward train acc:  0.1  and loss:  913.4900612831116
test acc:  0.1
forward train acc:  0.1  and loss:  909.9360611438751
test acc:  0.1
forward train acc:  0.1  and loss:  907.2917408943176
test acc:  0.1
forward train acc:  0.1  and loss:  905.7393617630005
test acc:  0.1
forward train acc:  0.1  and loss:  904.9006521701813
test acc:  0.1
forward train acc:  0.1  and loss:  904.1636080741882
test acc:  0.1
forward train acc:  0.1  and loss:  903.5307385921478
test acc:  0.1
forward train acc:  0.1  and loss:  903.1157343387604
test acc:  0.1
forward train acc:  0.1  and loss:  902.8557417392731
test acc:  0.1
forward train acc:  0.1  and loss:  902.6300263404846
test acc:  0.1
********** reverse layer  12  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  13
[0, 0, 2, 0, 0, 1, 0, 3, 2, 0, 1, 1, 5, 0]
optimize layer  0
backward train epoch:  476
test acc:  0.0992
forward train acc:  0.67204  and loss:  593.9027545750141
test acc:  0.7232
forward train acc:  0.75782  and loss:  307.6144250035286
test acc:  0.7679
forward train acc:  0.80238  and loss:  248.14881682395935
test acc:  0.7917
forward train acc:  0.82724  and loss:  213.58544421195984
test acc:  0.7999
forward train acc:  0.83598  and loss:  202.8054697215557
test acc:  0.8116
forward train acc:  0.84662  and loss:  188.26037016510963
test acc:  0.8159
forward train acc:  0.85706  and loss:  174.23080229759216
test acc:  0.8243
forward train acc:  0.86568  and loss:  166.45375362038612
test acc:  0.8272
forward train acc:  0.86848  and loss:  161.4702308923006
test acc:  0.8284
forward train acc:  0.87228  and loss:  155.85518546402454
test acc:  0.8337
********** reverse layer  0  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  1
backward train epoch:  316
test acc:  0.0997
forward train acc:  0.995  and loss:  7.4457283690571785
test acc:  0.9158
forward train acc:  0.99766  and loss:  3.080439104582183
test acc:  0.9166
forward train acc:  0.9984  and loss:  2.071385432907846
test acc:  0.9171
forward train acc:  0.99888  and loss:  1.5981156035559252
test acc:  0.9179
forward train acc:  0.99886  and loss:  1.5017386925464962
test acc:  0.9186
forward train acc:  0.99906  and loss:  1.1276936394278891
test acc:  0.9179
forward train acc:  0.99924  and loss:  0.9144990885979496
test acc:  0.9178
forward train acc:  0.99906  and loss:  1.116850831080228
test acc:  0.918
forward train acc:  0.99922  and loss:  1.1403322314145043
test acc:  0.9177
forward train acc:  0.9993  and loss:  0.9995616809173953
test acc:  0.919
********** reverse layer  1  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  2
[5, 5, 1, 0, 0, 1, 0, 3, 2, 0, 1, 1, 5, 0]
optimize layer  3
backward train epoch:  3
test acc:  0.1
forward train acc:  0.99864  and loss:  2.003142341854982
test acc:  0.916
forward train acc:  0.99922  and loss:  1.1138034932664596
test acc:  0.9166
forward train acc:  0.99928  and loss:  0.9753859333286528
test acc:  0.9159
forward train acc:  0.99916  and loss:  0.9778176629843074
test acc:  0.917
forward train acc:  0.99938  and loss:  0.9060799776634667
test acc:  0.9163
forward train acc:  0.99942  and loss:  0.7396196154877543
test acc:  0.9179
forward train acc:  0.99942  and loss:  0.7182294374797493
test acc:  0.9164
forward train acc:  0.99952  and loss:  0.5428765283431858
test acc:  0.9186
forward train acc:  0.9995  and loss:  0.6542939372884575
test acc:  0.9173
forward train acc:  0.9996  and loss:  0.586134559722268
test acc:  0.9179
********** reverse layer  3  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.6458333333333334  ==>  136 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  4
backward train epoch:  352
test acc:  0.1153
forward train acc:  0.9988  and loss:  1.635951778240269
test acc:  0.917
forward train acc:  0.99916  and loss:  1.2567635627638083
test acc:  0.9158
forward train acc:  0.9991  and loss:  1.100334000599105
test acc:  0.919
forward train acc:  0.99906  and loss:  1.225321277976036
test acc:  0.9199
forward train acc:  0.99908  and loss:  1.0649331235617865
test acc:  0.918
forward train acc:  0.99942  and loss:  0.7473860653699376
test acc:  0.9198
forward train acc:  0.99932  and loss:  0.8268577543349238
test acc:  0.9199
forward train acc:  0.99966  and loss:  0.4873309295217041
test acc:  0.9202
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  5
[5, 5, 1, 5, 0, 0, 0, 3, 2, 0, 1, 1, 5, 0]
optimize layer  6
backward train epoch:  202
test acc:  0.1146
forward train acc:  0.99896  and loss:  1.2944640457862988
test acc:  0.9178
forward train acc:  0.99892  and loss:  1.4753055820183363
test acc:  0.9184
forward train acc:  0.99916  and loss:  1.1139907910364855
test acc:  0.9191
forward train acc:  0.99932  and loss:  0.9096761359251104
test acc:  0.9185
forward train acc:  0.99934  and loss:  0.794752554356819
test acc:  0.9172
forward train acc:  0.9994  and loss:  0.7017498249988421
test acc:  0.9179
forward train acc:  0.99938  and loss:  0.9107508892775513
test acc:  0.9182
forward train acc:  0.99952  and loss:  0.5807182671742339
test acc:  0.9183
forward train acc:  0.99958  and loss:  0.618426375585841
test acc:  0.919
forward train acc:  0.99956  and loss:  0.7277822856704006
test acc:  0.9179
********** reverse layer  6  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8125  ==>  144 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  7
[5, 5, 1, 5, 0, 0, 5, 2, 2, 0, 1, 1, 5, 0]
***** skip layer  8
[5, 5, 1, 5, 0, 0, 5, 2, 1, 0, 1, 1, 5, 0]
optimize layer  9
backward train epoch:  90
test acc:  0.1007
forward train acc:  0.99932  and loss:  0.8488359851689893
test acc:  0.9193
forward train acc:  0.99946  and loss:  0.6564990705519449
test acc:  0.9185
forward train acc:  0.99908  and loss:  0.9813875377731165
test acc:  0.9169
forward train acc:  0.9993  and loss:  0.7061428145243553
test acc:  0.9188
forward train acc:  0.99944  and loss:  0.72950364719145
test acc:  0.9186
forward train acc:  0.9996  and loss:  0.5550103241257602
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8203125  ==>  138 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  10
[5, 5, 1, 5, 0, 0, 5, 2, 1, 0, 0, 1, 5, 0]
***** skip layer  11
[5, 5, 1, 5, 0, 0, 5, 2, 1, 0, 0, 0, 5, 0]
***** skip layer  12
[5, 5, 1, 5, 0, 0, 5, 2, 1, 0, 0, 0, 4, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.1021
forward train acc:  0.99912  and loss:  7.45710000442341
test acc:  0.9179
forward train acc:  0.99938  and loss:  5.117176252417266
test acc:  0.9175
forward train acc:  0.9991  and loss:  5.074931412469596
test acc:  0.9166
forward train acc:  0.99918  and loss:  4.757751016877592
test acc:  0.9161
forward train acc:  0.99952  and loss:  4.203870814293623
test acc:  0.9171
forward train acc:  0.99952  and loss:  4.039335979614407
test acc:  0.9172
forward train acc:  0.99948  and loss:  3.931776618119329
test acc:  0.9182
forward train acc:  0.99962  and loss:  3.6606282969005406
test acc:  0.9189
forward train acc:  0.99952  and loss:  3.949336742516607
test acc:  0.9175
forward train acc:  0.99964  and loss:  3.5556097766384482
test acc:  0.9175
********** reverse layer  13  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8203125  ==>  138 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  0
[4, 5, 1, 5, 0, 0, 5, 2, 1, 0, 0, 0, 4, 5]
***** skip layer  1
[4, 4, 1, 5, 0, 0, 5, 2, 1, 0, 0, 0, 4, 5]
***** skip layer  2
[4, 4, 0, 5, 0, 0, 5, 2, 1, 0, 0, 0, 4, 5]
***** skip layer  3
[4, 4, 0, 4, 0, 0, 5, 2, 1, 0, 0, 0, 4, 5]
optimize layer  4
backward train epoch:  556
test acc:  0.1069
forward train acc:  0.99894  and loss:  1.921467399370158
test acc:  0.9159
forward train acc:  0.99866  and loss:  2.168141250847839
test acc:  0.9147
forward train acc:  0.99856  and loss:  1.9840836773219053
test acc:  0.9156
forward train acc:  0.999  and loss:  1.3560941366595216
test acc:  0.9175
forward train acc:  0.99912  and loss:  1.1879487411279115
test acc:  0.9168
forward train acc:  0.9992  and loss:  1.0970124766899971
test acc:  0.9172
forward train acc:  0.99936  and loss:  1.018459260056261
test acc:  0.9175
forward train acc:  0.99936  and loss:  0.8605239068347146
test acc:  0.9189
forward train acc:  0.99924  and loss:  0.9299929128319491
test acc:  0.9181
forward train acc:  0.99956  and loss:  0.5032093041663757
test acc:  0.9172
********** reverse layer  4  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8203125  ==>  138 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  5
backward train epoch:  132
test acc:  0.1134
forward train acc:  0.99914  and loss:  1.2798934936581645
test acc:  0.9161
forward train acc:  0.99884  and loss:  1.5068593298638007
test acc:  0.918
forward train acc:  0.9992  and loss:  1.1316730641119648
test acc:  0.9175
forward train acc:  0.99932  and loss:  1.082722499515512
test acc:  0.918
forward train acc:  0.99928  and loss:  0.9453519180533476
test acc:  0.9182
forward train acc:  0.9993  and loss:  1.0061824164440623
test acc:  0.9174
forward train acc:  0.99948  and loss:  0.6856460733833956
test acc:  0.9177
forward train acc:  0.99952  and loss:  0.6348985078657279
test acc:  0.9173
forward train acc:  0.99956  and loss:  0.5943105449259747
test acc:  0.9188
forward train acc:  0.9995  and loss:  0.7330454992770683
test acc:  0.9193
********** reverse layer  5  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8203125  ==>  138 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  6
[4, 4, 0, 4, 5, 5, 4, 2, 1, 0, 0, 0, 4, 5]
***** skip layer  7
[4, 4, 0, 4, 5, 5, 4, 1, 1, 0, 0, 0, 4, 5]
***** skip layer  8
[4, 4, 0, 4, 5, 5, 4, 1, 0, 0, 0, 0, 4, 5]
optimize layer  9
backward train epoch:  134
test acc:  0.1104
forward train acc:  0.9994  and loss:  0.8412245349754812
test acc:  0.9196
forward train acc:  0.99948  and loss:  0.5936897841456812
test acc:  0.9196
forward train acc:  0.99932  and loss:  0.7913215841326746
test acc:  0.9203
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.828125  ==>  132 / 768
layer  10  :  0.9140625  ==>  66 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  10
backward train epoch:  192
test acc:  0.1094
forward train acc:  0.99942  and loss:  0.7330539182585198
test acc:  0.9183
forward train acc:  0.99946  and loss:  0.6812061451782938
test acc:  0.9202
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.828125  ==>  132 / 768
layer  10  :  0.921875  ==>  60 / 768
layer  11  :  0.9557291666666666  ==>  34 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  11
backward train epoch:  264
test acc:  0.1193
forward train acc:  0.99954  and loss:  0.8794898574415129
test acc:  0.9199
forward train acc:  0.99922  and loss:  1.0048389870498795
test acc:  0.9197
forward train acc:  0.99922  and loss:  0.8972488759027328
test acc:  0.921
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.828125  ==>  132 / 768
layer  10  :  0.921875  ==>  60 / 768
layer  11  :  0.9635416666666666  ==>  28 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  12
[4, 4, 0, 4, 5, 5, 4, 1, 0, 0, 0, 0, 3, 5]
***** skip layer  13
[4, 4, 0, 4, 5, 5, 4, 1, 0, 0, 0, 0, 3, 4]
***** skip layer  0
[3, 4, 0, 4, 5, 5, 4, 1, 0, 0, 0, 0, 3, 4]
***** skip layer  1
[3, 3, 0, 4, 5, 5, 4, 1, 0, 0, 0, 0, 3, 4]
optimize layer  2
backward train epoch:  1615
test acc:  0.0998
forward train acc:  0.9589  and loss:  66.3930762913078
test acc:  0.89
forward train acc:  0.97238  and loss:  36.974921743385494
test acc:  0.8967
forward train acc:  0.97886  and loss:  27.784785608761013
test acc:  0.8989
forward train acc:  0.98102  and loss:  24.909654365852475
test acc:  0.9014
forward train acc:  0.9825  and loss:  21.683687745127827
test acc:  0.9027
forward train acc:  0.98316  and loss:  21.06287275860086
test acc:  0.9033
forward train acc:  0.98586  and loss:  18.690483964048326
test acc:  0.9063
forward train acc:  0.98636  and loss:  17.277800998301245
test acc:  0.9051
forward train acc:  0.98668  and loss:  16.31612735823728
test acc:  0.906
forward train acc:  0.98806  and loss:  14.906500066863373
test acc:  0.9052
********** reverse layer  2  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.828125  ==>  132 / 768
layer  10  :  0.921875  ==>  60 / 768
layer  11  :  0.9635416666666666  ==>  28 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  3
[3, 3, 5, 3, 5, 5, 4, 1, 0, 0, 0, 0, 3, 4]
***** skip layer  4
[3, 3, 5, 3, 4, 5, 4, 1, 0, 0, 0, 0, 3, 4]
***** skip layer  5
[3, 3, 5, 3, 4, 4, 4, 1, 0, 0, 0, 0, 3, 4]
***** skip layer  6
[3, 3, 5, 3, 4, 4, 3, 1, 0, 0, 0, 0, 3, 4]
***** skip layer  7
[3, 3, 5, 3, 4, 4, 3, 0, 0, 0, 0, 0, 3, 4]
optimize layer  8
backward train epoch:  41
test acc:  0.0789
forward train acc:  0.99322  and loss:  10.48153408523649
test acc:  0.9124
forward train acc:  0.99676  and loss:  4.471614121866878
test acc:  0.9145
forward train acc:  0.99774  and loss:  3.130183113273233
test acc:  0.9152
forward train acc:  0.99756  and loss:  3.5468020695261657
test acc:  0.9169
forward train acc:  0.99788  and loss:  2.7406613856146578
test acc:  0.9159
forward train acc:  0.99842  and loss:  2.076514318032423
test acc:  0.9156
forward train acc:  0.9984  and loss:  2.110825249401387
test acc:  0.9161
forward train acc:  0.9988  and loss:  1.8439928549632896
test acc:  0.9168
forward train acc:  0.9987  and loss:  1.6392768317600712
test acc:  0.9157
forward train acc:  0.99866  and loss:  1.9918123175157234
test acc:  0.9156
********** reverse layer  8  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.828125  ==>  132 / 768
layer  10  :  0.921875  ==>  60 / 768
layer  11  :  0.9635416666666666  ==>  28 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  9
backward train epoch:  146
test acc:  0.1211
forward train acc:  0.99948  and loss:  0.7486352488631383
test acc:  0.919
forward train acc:  0.99916  and loss:  0.898095129916328
test acc:  0.9183
forward train acc:  0.99942  and loss:  0.6899081001465674
test acc:  0.9198
forward train acc:  0.99974  and loss:  0.4709289232996525
test acc:  0.9213
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8359375  ==>  126 / 768
layer  10  :  0.921875  ==>  60 / 768
layer  11  :  0.9635416666666666  ==>  28 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  10
backward train epoch:  338
test acc:  0.125
forward train acc:  0.99938  and loss:  0.7816733090148773
test acc:  0.918
forward train acc:  0.9994  and loss:  0.7933306009217631
test acc:  0.9196
forward train acc:  0.9995  and loss:  0.675904790230561
test acc:  0.9178
forward train acc:  0.99954  and loss:  0.6330691864204709
test acc:  0.9187
forward train acc:  0.99966  and loss:  0.46686102962848963
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8359375  ==>  126 / 768
layer  10  :  0.9296875  ==>  54 / 768
layer  11  :  0.9635416666666666  ==>  28 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  11
backward train epoch:  375
test acc:  0.1016
forward train acc:  0.99944  and loss:  0.7019009710638784
test acc:  0.9191
forward train acc:  0.99942  and loss:  0.7470374756376259
test acc:  0.9167
forward train acc:  0.99944  and loss:  0.8539865332350018
test acc:  0.9163
forward train acc:  0.99958  and loss:  0.5500792238453869
test acc:  0.9174
forward train acc:  0.99952  and loss:  0.5743251463863999
test acc:  0.9209
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8359375  ==>  126 / 768
layer  10  :  0.9296875  ==>  54 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  12
[3, 3, 5, 3, 4, 4, 3, 0, 5, 0, 0, 0, 2, 4]
***** skip layer  13
[3, 3, 5, 3, 4, 4, 3, 0, 5, 0, 0, 0, 2, 3]
***** skip layer  0
[2, 3, 5, 3, 4, 4, 3, 0, 5, 0, 0, 0, 2, 3]
***** skip layer  1
[2, 2, 5, 3, 4, 4, 3, 0, 5, 0, 0, 0, 2, 3]
***** skip layer  2
[2, 2, 4, 3, 4, 4, 3, 0, 5, 0, 0, 0, 2, 3]
***** skip layer  3
[2, 2, 4, 2, 4, 4, 3, 0, 5, 0, 0, 0, 2, 3]
***** skip layer  4
[2, 2, 4, 2, 3, 4, 3, 0, 5, 0, 0, 0, 2, 3]
***** skip layer  5
[2, 2, 4, 2, 3, 3, 3, 0, 5, 0, 0, 0, 2, 3]
***** skip layer  6
[2, 2, 4, 2, 3, 3, 2, 0, 5, 0, 0, 0, 2, 3]
optimize layer  7
backward train epoch:  576
test acc:  0.1138
forward train acc:  0.99646  and loss:  4.562266556575196
test acc:  0.9131
forward train acc:  0.99828  and loss:  2.2429445294546895
test acc:  0.9125
forward train acc:  0.99876  and loss:  1.716454020162928
test acc:  0.9136
forward train acc:  0.99852  and loss:  1.7735425072023645
test acc:  0.9134
forward train acc:  0.99846  and loss:  1.8600024925981415
test acc:  0.9158
forward train acc:  0.99882  and loss:  1.5500273854122497
test acc:  0.9158
forward train acc:  0.99908  and loss:  1.1224058391380822
test acc:  0.9153
forward train acc:  0.9992  and loss:  1.1116882628557505
test acc:  0.9152
forward train acc:  0.99882  and loss:  1.2849340276006842
test acc:  0.9151
forward train acc:  0.99918  and loss:  1.1168382553441916
test acc:  0.9163
********** reverse layer  7  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8359375  ==>  126 / 768
layer  10  :  0.9296875  ==>  54 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  8
[2, 2, 4, 2, 3, 3, 2, 5, 4, 0, 0, 0, 2, 3]
optimize layer  9
backward train epoch:  421
test acc:  0.102
forward train acc:  0.99934  and loss:  0.7305896234538523
test acc:  0.9194
forward train acc:  0.99966  and loss:  0.5214889399976528
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.84375  ==>  120 / 768
layer  10  :  0.9296875  ==>  54 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  10
backward train epoch:  483
test acc:  0.0955
forward train acc:  0.99888  and loss:  1.7766712450538762
test acc:  0.9187
forward train acc:  0.9997  and loss:  0.5870023730385583
test acc:  0.9194
forward train acc:  0.9995  and loss:  0.625520709756529
test acc:  0.9174
forward train acc:  0.99984  and loss:  0.45704395744542126
test acc:  0.9185
forward train acc:  0.99952  and loss:  0.5929835599963553
test acc:  0.9176
forward train acc:  0.99948  and loss:  0.7372863269847585
test acc:  0.9179
forward train acc:  0.99972  and loss:  0.39416398890898563
test acc:  0.9191
forward train acc:  0.99972  and loss:  0.4365713678780594
test acc:  0.9188
forward train acc:  0.9997  and loss:  0.46063205947575625
test acc:  0.919
forward train acc:  0.99978  and loss:  0.36417618865380064
test acc:  0.918
********** reverse layer  10  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.84375  ==>  120 / 768
layer  10  :  0.9296875  ==>  54 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  11
backward train epoch:  73
test acc:  0.104
forward train acc:  0.99962  and loss:  0.8006676261138637
test acc:  0.9188
forward train acc:  0.99956  and loss:  0.5929862965276698
test acc:  0.9157
forward train acc:  0.99914  and loss:  1.3257002263126196
test acc:  0.9143
forward train acc:  0.99942  and loss:  0.8876634940534132
test acc:  0.9173
forward train acc:  0.9996  and loss:  0.6231659533950733
test acc:  0.9166
forward train acc:  0.99932  and loss:  0.8441692070628051
test acc:  0.9168
forward train acc:  0.99964  and loss:  0.5262266895442735
test acc:  0.915
forward train acc:  0.99972  and loss:  0.33785148251627106
test acc:  0.9162
forward train acc:  0.99958  and loss:  0.49369428610953037
test acc:  0.9171
forward train acc:  0.99968  and loss:  0.47414188236143673
test acc:  0.9173
********** reverse layer  11  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.84375  ==>  120 / 768
layer  10  :  0.9296875  ==>  54 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  12
[2, 2, 4, 2, 3, 3, 2, 5, 4, 0, 5, 5, 1, 3]
***** skip layer  13
[2, 2, 4, 2, 3, 3, 2, 5, 4, 0, 5, 5, 1, 2]
***** skip layer  0
[1, 2, 4, 2, 3, 3, 2, 5, 4, 0, 5, 5, 1, 2]
***** skip layer  1
[1, 1, 4, 2, 3, 3, 2, 5, 4, 0, 5, 5, 1, 2]
***** skip layer  2
[1, 1, 3, 2, 3, 3, 2, 5, 4, 0, 5, 5, 1, 2]
***** skip layer  3
[1, 1, 3, 1, 3, 3, 2, 5, 4, 0, 5, 5, 1, 2]
***** skip layer  4
[1, 1, 3, 1, 2, 3, 2, 5, 4, 0, 5, 5, 1, 2]
***** skip layer  5
[1, 1, 3, 1, 2, 2, 2, 5, 4, 0, 5, 5, 1, 2]
***** skip layer  6
[1, 1, 3, 1, 2, 2, 1, 5, 4, 0, 5, 5, 1, 2]
***** skip layer  7
[1, 1, 3, 1, 2, 2, 1, 4, 4, 0, 5, 5, 1, 2]
***** skip layer  8
[1, 1, 3, 1, 2, 2, 1, 4, 3, 0, 5, 5, 1, 2]
optimize layer  9
backward train epoch:  346
test acc:  0.113
forward train acc:  0.99964  and loss:  0.5605931285244878
test acc:  0.9184
forward train acc:  0.99938  and loss:  0.7767828082651249
test acc:  0.9184
forward train acc:  0.99942  and loss:  0.6737100107056904
test acc:  0.9182
forward train acc:  0.99964  and loss:  0.5754791382933035
test acc:  0.9191
forward train acc:  0.99964  and loss:  0.42093511654093163
test acc:  0.9186
forward train acc:  0.99948  and loss:  0.675173361756606
test acc:  0.9193
forward train acc:  0.99968  and loss:  0.43101312968792627
test acc:  0.9178
forward train acc:  0.99956  and loss:  0.5429303211931256
test acc:  0.9186
forward train acc:  0.9997  and loss:  0.2961314200219931
test acc:  0.918
forward train acc:  0.99972  and loss:  0.41474157823540736
test acc:  0.9186
********** reverse layer  9  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.84375  ==>  120 / 768
layer  10  :  0.9296875  ==>  54 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  10
[1, 1, 3, 1, 2, 2, 1, 4, 3, 5, 4, 5, 1, 2]
***** skip layer  11
[1, 1, 3, 1, 2, 2, 1, 4, 3, 5, 4, 4, 1, 2]
***** skip layer  12
[1, 1, 3, 1, 2, 2, 1, 4, 3, 5, 4, 4, 0, 2]
***** skip layer  13
[1, 1, 3, 1, 2, 2, 1, 4, 3, 5, 4, 4, 0, 1]
***** skip layer  0
[0, 1, 3, 1, 2, 2, 1, 4, 3, 5, 4, 4, 0, 1]
***** skip layer  1
[0, 0, 3, 1, 2, 2, 1, 4, 3, 5, 4, 4, 0, 1]
***** skip layer  2
[0, 0, 2, 1, 2, 2, 1, 4, 3, 5, 4, 4, 0, 1]
***** skip layer  3
[0, 0, 2, 0, 2, 2, 1, 4, 3, 5, 4, 4, 0, 1]
***** skip layer  4
[0, 0, 2, 0, 1, 2, 1, 4, 3, 5, 4, 4, 0, 1]
***** skip layer  5
[0, 0, 2, 0, 1, 1, 1, 4, 3, 5, 4, 4, 0, 1]
***** skip layer  6
[0, 0, 2, 0, 1, 1, 0, 4, 3, 5, 4, 4, 0, 1]
***** skip layer  7
[0, 0, 2, 0, 1, 1, 0, 3, 3, 5, 4, 4, 0, 1]
***** skip layer  8
[0, 0, 2, 0, 1, 1, 0, 3, 2, 5, 4, 4, 0, 1]
***** skip layer  9
[0, 0, 2, 0, 1, 1, 0, 3, 2, 4, 4, 4, 0, 1]
***** skip layer  10
[0, 0, 2, 0, 1, 1, 0, 3, 2, 4, 3, 4, 0, 1]
***** skip layer  11
[0, 0, 2, 0, 1, 1, 0, 3, 2, 4, 3, 3, 0, 1]
optimize layer  12
backward train epoch:  107
test acc:  0.1
forward train acc:  0.1  and loss:  929.3234028816223
test acc:  0.1
forward train acc:  0.1  and loss:  922.7441620826721
test acc:  0.1
forward train acc:  0.1  and loss:  917.4592821598053
test acc:  0.1
forward train acc:  0.1  and loss:  914.2002964019775
test acc:  0.1
forward train acc:  0.1  and loss:  912.3262882232666
test acc:  0.1
forward train acc:  0.1  and loss:  910.6389925479889
test acc:  0.1
forward train acc:  0.1  and loss:  909.1286675930023
test acc:  0.1
forward train acc:  0.1  and loss:  908.1075706481934
test acc:  0.1
forward train acc:  0.1  and loss:  907.4869356155396
test acc:  0.1
forward train acc:  0.1  and loss:  906.9074199199677
test acc:  0.1
********** reverse layer  12  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.84375  ==>  120 / 768
layer  10  :  0.9296875  ==>  54 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  13
[0, 0, 2, 0, 1, 1, 0, 3, 2, 4, 3, 3, 5, 0]
optimize layer  0
backward train epoch:  97
test acc:  0.0984
forward train acc:  0.99856  and loss:  2.011473753998871
test acc:  0.9142
forward train acc:  0.99876  and loss:  1.6912794742092956
test acc:  0.9146
forward train acc:  0.99876  and loss:  1.6531252087734174
test acc:  0.9143
forward train acc:  0.99912  and loss:  1.2229017156932969
test acc:  0.9162
forward train acc:  0.99916  and loss:  1.2462174247484654
test acc:  0.9152
forward train acc:  0.99924  and loss:  1.0021468728373293
test acc:  0.9172
forward train acc:  0.99926  and loss:  0.9361520491365809
test acc:  0.918
forward train acc:  0.99942  and loss:  0.856624096428277
test acc:  0.918
forward train acc:  0.9994  and loss:  0.6906949449767126
test acc:  0.9176
forward train acc:  0.99926  and loss:  1.006896953585965
test acc:  0.9193
********** reverse layer  0  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.84375  ==>  120 / 768
layer  10  :  0.9296875  ==>  54 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  1
backward train epoch:  151
test acc:  0.0969
forward train acc:  0.99894  and loss:  1.3065640876011457
test acc:  0.9135
forward train acc:  0.99888  and loss:  1.3834632709622383
test acc:  0.916
forward train acc:  0.99894  and loss:  1.4120790464949096
test acc:  0.9168
forward train acc:  0.9994  and loss:  0.7516287186881527
test acc:  0.9174
forward train acc:  0.99952  and loss:  0.6020732518518344
test acc:  0.9164
forward train acc:  0.99938  and loss:  0.73173848510487
test acc:  0.9173
forward train acc:  0.99956  and loss:  0.5496806452283636
test acc:  0.919
forward train acc:  0.99962  and loss:  0.5620030855352525
test acc:  0.9179
forward train acc:  0.99958  and loss:  0.4526956971458276
test acc:  0.9176
forward train acc:  0.99944  and loss:  0.6133879163317033
test acc:  0.9183
********** reverse layer  1  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.84375  ==>  120 / 768
layer  10  :  0.9296875  ==>  54 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  2
[5, 5, 1, 0, 1, 1, 0, 3, 2, 4, 3, 3, 5, 0]
optimize layer  3
backward train epoch:  301
test acc:  0.0947
forward train acc:  0.99908  and loss:  1.085724616306834
test acc:  0.9169
forward train acc:  0.99938  and loss:  0.8519753183791181
test acc:  0.9153
forward train acc:  0.99944  and loss:  0.8564223879802739
test acc:  0.9154
forward train acc:  0.99942  and loss:  0.6526387866833829
test acc:  0.9166
forward train acc:  0.99954  and loss:  0.5512239702948136
test acc:  0.9173
forward train acc:  0.99966  and loss:  0.555116885836469
test acc:  0.9184
forward train acc:  0.9995  and loss:  0.5707142719475087
test acc:  0.9166
forward train acc:  0.99972  and loss:  0.4412760114937555
test acc:  0.9166
forward train acc:  0.99966  and loss:  0.51424691468128
test acc:  0.9183
forward train acc:  0.99954  and loss:  0.5882891158689745
test acc:  0.9179
********** reverse layer  3  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.84375  ==>  120 / 768
layer  10  :  0.9296875  ==>  54 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  4
[5, 5, 1, 5, 0, 1, 0, 3, 2, 4, 3, 3, 5, 0]
***** skip layer  5
[5, 5, 1, 5, 0, 0, 0, 3, 2, 4, 3, 3, 5, 0]
optimize layer  6
backward train epoch:  258
test acc:  0.1153
forward train acc:  0.99928  and loss:  0.8136645368504105
test acc:  0.9169
forward train acc:  0.99946  and loss:  0.7478571592946537
test acc:  0.9163
forward train acc:  0.99944  and loss:  0.690649003649014
test acc:  0.9162
forward train acc:  0.9994  and loss:  0.7363998288201401
test acc:  0.9184
forward train acc:  0.99972  and loss:  0.3840825540100923
test acc:  0.9183
forward train acc:  0.99972  and loss:  0.4601572035899153
test acc:  0.9184
forward train acc:  0.99954  and loss:  0.7368869363854174
test acc:  0.9195
forward train acc:  0.99974  and loss:  0.37391140426916536
test acc:  0.9189
forward train acc:  0.99964  and loss:  0.43632674086256884
test acc:  0.9181
forward train acc:  0.99974  and loss:  0.3622066356292635
test acc:  0.9183
********** reverse layer  6  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.84375  ==>  120 / 768
layer  10  :  0.9296875  ==>  54 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  7
[5, 5, 1, 5, 0, 0, 5, 2, 2, 4, 3, 3, 5, 0]
***** skip layer  8
[5, 5, 1, 5, 0, 0, 5, 2, 1, 4, 3, 3, 5, 0]
***** skip layer  9
[5, 5, 1, 5, 0, 0, 5, 2, 1, 3, 3, 3, 5, 0]
***** skip layer  10
[5, 5, 1, 5, 0, 0, 5, 2, 1, 3, 2, 3, 5, 0]
***** skip layer  11
[5, 5, 1, 5, 0, 0, 5, 2, 1, 3, 2, 2, 5, 0]
***** skip layer  12
[5, 5, 1, 5, 0, 0, 5, 2, 1, 3, 2, 2, 4, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.1169
forward train acc:  0.69788  and loss:  331.04248359799385
test acc:  0.6363
forward train acc:  0.69976  and loss:  303.263933300972
test acc:  0.6384
forward train acc:  0.69974  and loss:  295.2821059525013
test acc:  0.6375
forward train acc:  0.69982  and loss:  289.31776389479637
test acc:  0.6381
forward train acc:  0.69988  and loss:  285.5614827275276
test acc:  0.6376
forward train acc:  0.69984  and loss:  281.8371765911579
test acc:  0.637
forward train acc:  0.69978  and loss:  278.2829994857311
test acc:  0.6374
forward train acc:  0.69986  and loss:  275.32392477989197
test acc:  0.6369
forward train acc:  0.6997  and loss:  273.5097026526928
test acc:  0.6383
forward train acc:  0.69968  and loss:  271.6135075688362
test acc:  0.6374
********** reverse layer  13  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.84375  ==>  120 / 768
layer  10  :  0.9296875  ==>  54 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  0
[4, 5, 1, 5, 0, 0, 5, 2, 1, 3, 2, 2, 4, 5]
***** skip layer  1
[4, 4, 1, 5, 0, 0, 5, 2, 1, 3, 2, 2, 4, 5]
***** skip layer  2
[4, 4, 0, 5, 0, 0, 5, 2, 1, 3, 2, 2, 4, 5]
***** skip layer  3
[4, 4, 0, 4, 0, 0, 5, 2, 1, 3, 2, 2, 4, 5]
optimize layer  4
backward train epoch:  211
test acc:  0.0969
forward train acc:  0.99882  and loss:  1.948058200214291
test acc:  0.9172
forward train acc:  0.999  and loss:  1.5962334632349666
test acc:  0.9156
forward train acc:  0.99896  and loss:  1.4150540794071276
test acc:  0.9159
forward train acc:  0.99922  and loss:  0.9294280674075708
test acc:  0.9151
forward train acc:  0.99918  and loss:  1.0387006794917397
test acc:  0.9171
forward train acc:  0.99926  and loss:  1.0445989811705658
test acc:  0.9162
forward train acc:  0.9993  and loss:  0.8519121574936435
test acc:  0.9167
forward train acc:  0.99928  and loss:  0.8775109619891737
test acc:  0.9178
forward train acc:  0.99942  and loss:  0.7730568145925645
test acc:  0.9181
forward train acc:  0.99938  and loss:  0.7723416947483202
test acc:  0.9181
********** reverse layer  4  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.84375  ==>  120 / 768
layer  10  :  0.9296875  ==>  54 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  5
backward train epoch:  99
test acc:  0.1224
forward train acc:  0.99854  and loss:  2.0199290887685493
test acc:  0.9173
forward train acc:  0.99936  and loss:  0.8378900278839865
test acc:  0.9162
forward train acc:  0.99904  and loss:  1.3349666873691604
test acc:  0.9185
forward train acc:  0.99956  and loss:  0.6322340407932643
test acc:  0.9187
forward train acc:  0.99966  and loss:  0.49531581276096404
test acc:  0.919
forward train acc:  0.99954  and loss:  0.7086820857803104
test acc:  0.9173
forward train acc:  0.9994  and loss:  0.8010274233092787
test acc:  0.9181
forward train acc:  0.9994  and loss:  0.739146153209731
test acc:  0.9198
forward train acc:  0.9996  and loss:  0.5312056356633548
test acc:  0.9188
forward train acc:  0.99964  and loss:  0.4682245292206062
test acc:  0.9194
********** reverse layer  5  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.5833333333333334  ==>  80 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.84375  ==>  120 / 768
layer  10  :  0.9296875  ==>  54 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  6
[4, 4, 0, 4, 5, 5, 4, 2, 1, 3, 2, 2, 4, 5]
***** skip layer  7
[4, 4, 0, 4, 5, 5, 4, 1, 1, 3, 2, 2, 4, 5]
***** skip layer  8
[4, 4, 0, 4, 5, 5, 4, 1, 0, 3, 2, 2, 4, 5]
***** skip layer  9
[4, 4, 0, 4, 5, 5, 4, 1, 0, 2, 2, 2, 4, 5]
***** skip layer  10
[4, 4, 0, 4, 5, 5, 4, 1, 0, 2, 1, 2, 4, 5]
***** skip layer  11
[4, 4, 0, 4, 5, 5, 4, 1, 0, 2, 1, 1, 4, 5]
***** skip layer  12
[4, 4, 0, 4, 5, 5, 4, 1, 0, 2, 1, 1, 3, 5]
***** skip layer  13
[4, 4, 0, 4, 5, 5, 4, 1, 0, 2, 1, 1, 3, 4]
***** skip layer  0
[3, 4, 0, 4, 5, 5, 4, 1, 0, 2, 1, 1, 3, 4]
***** skip layer  1
[3, 3, 0, 4, 5, 5, 4, 1, 0, 2, 1, 1, 3, 4]
optimize layer  2
backward train epoch:  48
test acc:  0.092
forward train acc:  0.99958  and loss:  0.7027342115325155
test acc:  0.9187
forward train acc:  0.99934  and loss:  0.7933815216674702
test acc:  0.9171
forward train acc:  0.99958  and loss:  0.5935731396166375
test acc:  0.9187
forward train acc:  0.99928  and loss:  0.9235811146427295
test acc:  0.9186
forward train acc:  0.99962  and loss:  0.557143203484884
test acc:  0.9185
forward train acc:  0.99966  and loss:  0.4899834971947712
test acc:  0.9179
forward train acc:  0.99958  and loss:  0.5198742710563238
test acc:  0.9193
forward train acc:  0.9996  and loss:  0.5942918411674327
test acc:  0.9192
forward train acc:  0.99974  and loss:  0.3655786615126999
test acc:  0.9206
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.84375  ==>  120 / 768
layer  10  :  0.9296875  ==>  54 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  3
[3, 3, 0, 3, 5, 5, 4, 1, 0, 2, 1, 1, 3, 4]
***** skip layer  4
[3, 3, 0, 3, 4, 5, 4, 1, 0, 2, 1, 1, 3, 4]
***** skip layer  5
[3, 3, 0, 3, 4, 4, 4, 1, 0, 2, 1, 1, 3, 4]
***** skip layer  6
[3, 3, 0, 3, 4, 4, 3, 1, 0, 2, 1, 1, 3, 4]
***** skip layer  7
[3, 3, 0, 3, 4, 4, 3, 0, 0, 2, 1, 1, 3, 4]
optimize layer  8
backward train epoch:  872
test acc:  0.0975
forward train acc:  0.97794  and loss:  28.193867205642164
test acc:  0.9022
forward train acc:  0.99326  and loss:  8.832530913874507
test acc:  0.908
forward train acc:  0.99498  and loss:  6.772810617869254
test acc:  0.908
forward train acc:  0.99638  and loss:  4.505398387875175
test acc:  0.9119
forward train acc:  0.99656  and loss:  4.328879562672228
test acc:  0.9109
forward train acc:  0.9971  and loss:  3.663159946183441
test acc:  0.9121
forward train acc:  0.99756  and loss:  3.3999331089726184
test acc:  0.9116
forward train acc:  0.99752  and loss:  3.2457432296068873
test acc:  0.913
forward train acc:  0.9981  and loss:  2.6121009431954008
test acc:  0.9134
forward train acc:  0.99824  and loss:  2.3035454233468045
test acc:  0.9138
********** reverse layer  8  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.84375  ==>  120 / 768
layer  10  :  0.9296875  ==>  54 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  9
[3, 3, 0, 3, 4, 4, 3, 0, 5, 1, 1, 1, 3, 4]
***** skip layer  10
[3, 3, 0, 3, 4, 4, 3, 0, 5, 1, 0, 1, 3, 4]
***** skip layer  11
[3, 3, 0, 3, 4, 4, 3, 0, 5, 1, 0, 0, 3, 4]
***** skip layer  12
[3, 3, 0, 3, 4, 4, 3, 0, 5, 1, 0, 0, 2, 4]
***** skip layer  13
[3, 3, 0, 3, 4, 4, 3, 0, 5, 1, 0, 0, 2, 3]
***** skip layer  0
[2, 3, 0, 3, 4, 4, 3, 0, 5, 1, 0, 0, 2, 3]
***** skip layer  1
[2, 2, 0, 3, 4, 4, 3, 0, 5, 1, 0, 0, 2, 3]
optimize layer  2
backward train epoch:  379
test acc:  0.0893
forward train acc:  0.95304  and loss:  80.2153317630291
test acc:  0.8879
forward train acc:  0.9705  and loss:  39.377293379977345
test acc:  0.8937
forward train acc:  0.97674  and loss:  30.111487860791385
test acc:  0.8975
forward train acc:  0.98156  and loss:  23.0686272601597
test acc:  0.9001
forward train acc:  0.98268  and loss:  21.815615222323686
test acc:  0.9003
forward train acc:  0.98488  and loss:  19.546502163633704
test acc:  0.9022
forward train acc:  0.98634  and loss:  17.594306260347366
test acc:  0.9032
forward train acc:  0.98746  and loss:  15.97185081266798
test acc:  0.9025
forward train acc:  0.98736  and loss:  16.101080696797
test acc:  0.9036
forward train acc:  0.98824  and loss:  14.653904108214192
test acc:  0.9038
********** reverse layer  2  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.84375  ==>  120 / 768
layer  10  :  0.9296875  ==>  54 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  3
[2, 2, 5, 2, 4, 4, 3, 0, 5, 1, 0, 0, 2, 3]
***** skip layer  4
[2, 2, 5, 2, 3, 4, 3, 0, 5, 1, 0, 0, 2, 3]
***** skip layer  5
[2, 2, 5, 2, 3, 3, 3, 0, 5, 1, 0, 0, 2, 3]
***** skip layer  6
[2, 2, 5, 2, 3, 3, 2, 0, 5, 1, 0, 0, 2, 3]
optimize layer  7
backward train epoch:  24
test acc:  0.1088
forward train acc:  0.99688  and loss:  4.209119058039505
test acc:  0.9132
forward train acc:  0.99862  and loss:  1.8506469080457464
test acc:  0.913
forward train acc:  0.99864  and loss:  1.6757982839189935
test acc:  0.9151
forward train acc:  0.999  and loss:  1.3022522515966557
test acc:  0.9154
forward train acc:  0.9993  and loss:  0.950421997171361
test acc:  0.9151
forward train acc:  0.9992  and loss:  1.0470282383903395
test acc:  0.9158
forward train acc:  0.99922  and loss:  1.0905769713717746
test acc:  0.9157
forward train acc:  0.99924  and loss:  0.909779422916472
test acc:  0.9156
forward train acc:  0.99948  and loss:  0.6742071406915784
test acc:  0.9159
forward train acc:  0.9994  and loss:  0.7341018573642941
test acc:  0.9152
********** reverse layer  7  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.84375  ==>  120 / 768
layer  10  :  0.9296875  ==>  54 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  8
[2, 2, 5, 2, 3, 3, 2, 5, 4, 1, 0, 0, 2, 3]
***** skip layer  9
[2, 2, 5, 2, 3, 3, 2, 5, 4, 0, 0, 0, 2, 3]
optimize layer  10
backward train epoch:  170
test acc:  0.1251
forward train acc:  0.9995  and loss:  0.8592020319338189
test acc:  0.9192
forward train acc:  0.9997  and loss:  0.4149068657134194
test acc:  0.9191
forward train acc:  0.99948  and loss:  0.7769140617820085
test acc:  0.9179
forward train acc:  0.99958  and loss:  0.5552057432505535
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.84375  ==>  120 / 768
layer  10  :  0.9375  ==>  48 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  11
backward train epoch:  1072
test acc:  0.0998
forward train acc:  0.99686  and loss:  5.5594154495047405
test acc:  0.917
forward train acc:  0.99952  and loss:  1.2108968921820633
test acc:  0.9177
forward train acc:  0.99946  and loss:  1.030171042861184
test acc:  0.9175
forward train acc:  0.9995  and loss:  0.6841711656888947
test acc:  0.9182
forward train acc:  0.99952  and loss:  0.773456913389964
test acc:  0.9182
forward train acc:  0.99958  and loss:  0.7268370030506048
test acc:  0.9182
forward train acc:  0.9995  and loss:  0.7214025354769547
test acc:  0.9177
forward train acc:  0.99964  and loss:  0.581065862410469
test acc:  0.9183
forward train acc:  0.99954  and loss:  0.5569921023270581
test acc:  0.9178
forward train acc:  0.99968  and loss:  0.43536220031091943
test acc:  0.9187
********** reverse layer  11  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.84375  ==>  120 / 768
layer  10  :  0.9375  ==>  48 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  12
[2, 2, 5, 2, 3, 3, 2, 5, 4, 0, 0, 5, 1, 3]
***** skip layer  13
[2, 2, 5, 2, 3, 3, 2, 5, 4, 0, 0, 5, 1, 2]
***** skip layer  0
[1, 2, 5, 2, 3, 3, 2, 5, 4, 0, 0, 5, 1, 2]
***** skip layer  1
[1, 1, 5, 2, 3, 3, 2, 5, 4, 0, 0, 5, 1, 2]
***** skip layer  2
[1, 1, 4, 2, 3, 3, 2, 5, 4, 0, 0, 5, 1, 2]
***** skip layer  3
[1, 1, 4, 1, 3, 3, 2, 5, 4, 0, 0, 5, 1, 2]
***** skip layer  4
[1, 1, 4, 1, 2, 3, 2, 5, 4, 0, 0, 5, 1, 2]
***** skip layer  5
[1, 1, 4, 1, 2, 2, 2, 5, 4, 0, 0, 5, 1, 2]
***** skip layer  6
[1, 1, 4, 1, 2, 2, 1, 5, 4, 0, 0, 5, 1, 2]
***** skip layer  7
[1, 1, 4, 1, 2, 2, 1, 4, 4, 0, 0, 5, 1, 2]
***** skip layer  8
[1, 1, 4, 1, 2, 2, 1, 4, 3, 0, 0, 5, 1, 2]
optimize layer  9
backward train epoch:  232
test acc:  0.1091
forward train acc:  0.9997  and loss:  0.3739108885638416
test acc:  0.9172
forward train acc:  0.9996  and loss:  0.6016449481321615
test acc:  0.9192
forward train acc:  0.99944  and loss:  0.6605163046042435
test acc:  0.9177
forward train acc:  0.99952  and loss:  0.5858523877977859
test acc:  0.9183
forward train acc:  0.99978  and loss:  0.3191948696767213
test acc:  0.9191
forward train acc:  0.9995  and loss:  0.6183721643974422
test acc:  0.9183
forward train acc:  0.99966  and loss:  0.45835827171686105
test acc:  0.919
forward train acc:  0.99972  and loss:  0.4464962249985547
test acc:  0.9197
forward train acc:  0.99962  and loss:  0.4756077686470235
test acc:  0.921
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8515625  ==>  114 / 768
layer  10  :  0.9375  ==>  48 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  10
backward train epoch:  42
test acc:  0.0907
forward train acc:  0.9995  and loss:  0.8557058875012444
test acc:  0.9176
forward train acc:  0.99952  and loss:  0.6119763249444077
test acc:  0.9201
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8515625  ==>  114 / 768
layer  10  :  0.9453125  ==>  42 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  11
[1, 1, 4, 1, 2, 2, 1, 4, 3, 0, 0, 4, 1, 2]
***** skip layer  12
[1, 1, 4, 1, 2, 2, 1, 4, 3, 0, 0, 4, 0, 2]
***** skip layer  13
[1, 1, 4, 1, 2, 2, 1, 4, 3, 0, 0, 4, 0, 1]
***** skip layer  0
[0, 1, 4, 1, 2, 2, 1, 4, 3, 0, 0, 4, 0, 1]
***** skip layer  1
[0, 0, 4, 1, 2, 2, 1, 4, 3, 0, 0, 4, 0, 1]
***** skip layer  2
[0, 0, 3, 1, 2, 2, 1, 4, 3, 0, 0, 4, 0, 1]
***** skip layer  3
[0, 0, 3, 0, 2, 2, 1, 4, 3, 0, 0, 4, 0, 1]
***** skip layer  4
[0, 0, 3, 0, 1, 2, 1, 4, 3, 0, 0, 4, 0, 1]
***** skip layer  5
[0, 0, 3, 0, 1, 1, 1, 4, 3, 0, 0, 4, 0, 1]
***** skip layer  6
[0, 0, 3, 0, 1, 1, 0, 4, 3, 0, 0, 4, 0, 1]
***** skip layer  7
[0, 0, 3, 0, 1, 1, 0, 3, 3, 0, 0, 4, 0, 1]
***** skip layer  8
[0, 0, 3, 0, 1, 1, 0, 3, 2, 0, 0, 4, 0, 1]
optimize layer  9
backward train epoch:  76
test acc:  0.0959
forward train acc:  0.99946  and loss:  0.6337023765518097
test acc:  0.9186
forward train acc:  0.99952  and loss:  0.5862311475721071
test acc:  0.9187
forward train acc:  0.99952  and loss:  0.672348063410027
test acc:  0.9172
forward train acc:  0.99962  and loss:  0.5263395855872659
test acc:  0.92
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.859375  ==>  108 / 768
layer  10  :  0.9453125  ==>  42 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  10
backward train epoch:  166
test acc:  0.0903
forward train acc:  0.99956  and loss:  0.6244704279961297
test acc:  0.9165
forward train acc:  0.99932  and loss:  0.8613795272176503
test acc:  0.9159
forward train acc:  0.9994  and loss:  0.8589267921634018
test acc:  0.9182
forward train acc:  0.99956  and loss:  0.4255279551798594
test acc:  0.9192
forward train acc:  0.9996  and loss:  0.4565458757424494
test acc:  0.9184
forward train acc:  0.99972  and loss:  0.4220614583027782
test acc:  0.9178
forward train acc:  0.99946  and loss:  0.5335449843259994
test acc:  0.9177
forward train acc:  0.99966  and loss:  0.5310392257160856
test acc:  0.9182
forward train acc:  0.99968  and loss:  0.5028716845845338
test acc:  0.9189
forward train acc:  0.9997  and loss:  0.44125918768259
test acc:  0.9195
********** reverse layer  10  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.859375  ==>  108 / 768
layer  10  :  0.9453125  ==>  42 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  11
[0, 0, 3, 0, 1, 1, 0, 3, 2, 0, 5, 3, 0, 1]
optimize layer  12
backward train epoch:  118
test acc:  0.1
forward train acc:  0.1  and loss:  917.4942145347595
test acc:  0.1
forward train acc:  0.1  and loss:  912.5244815349579
test acc:  0.1
forward train acc:  0.1  and loss:  908.7932164669037
test acc:  0.1
forward train acc:  0.1  and loss:  906.6348547935486
test acc:  0.1
forward train acc:  0.1  and loss:  905.4695746898651
test acc:  0.1
forward train acc:  0.1  and loss:  904.4701261520386
test acc:  0.1
forward train acc:  0.1  and loss:  903.6363823413849
test acc:  0.1
forward train acc:  0.1  and loss:  903.1028528213501
test acc:  0.1
forward train acc:  0.1  and loss:  902.7926297187805
test acc:  0.1
forward train acc:  0.1  and loss:  902.5005941390991
test acc:  0.1
********** reverse layer  12  *********
layer  0  :  0.3333333333333333  ==>  64 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.859375  ==>  108 / 768
layer  10  :  0.9453125  ==>  42 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  13
[0, 0, 3, 0, 1, 1, 0, 3, 2, 0, 5, 3, 5, 0]
optimize layer  0
backward train epoch:  594
test acc:  0.1
forward train acc:  0.99956  and loss:  0.5538673956689308
test acc:  0.9184
forward train acc:  0.9997  and loss:  0.41464056784752756
test acc:  0.9196
forward train acc:  0.99948  and loss:  0.8728834649955388
test acc:  0.917
forward train acc:  0.99946  and loss:  0.751172422147647
test acc:  0.9186
forward train acc:  0.99956  and loss:  0.6275159108117805
test acc:  0.92
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.6458333333333334  ==>  34 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.859375  ==>  108 / 768
layer  10  :  0.9453125  ==>  42 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  1
backward train epoch:  150
test acc:  0.1117
forward train acc:  0.99896  and loss:  1.3113017318755738
test acc:  0.9186
forward train acc:  0.99914  and loss:  1.007998776214663
test acc:  0.918
forward train acc:  0.99926  and loss:  1.0137045181982103
test acc:  0.9174
forward train acc:  0.99958  and loss:  0.6964653090544743
test acc:  0.9187
forward train acc:  0.99924  and loss:  1.0481177220644895
test acc:  0.9186
forward train acc:  0.99952  and loss:  0.8122195151445339
test acc:  0.9204
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.859375  ==>  108 / 768
layer  10  :  0.9453125  ==>  42 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  2
[0, 0, 2, 0, 1, 1, 0, 3, 2, 0, 5, 3, 5, 0]
optimize layer  3
backward train epoch:  3
test acc:  0.7819
forward train acc:  0.99906  and loss:  1.30653341357538
test acc:  0.9167
forward train acc:  0.99914  and loss:  1.0688563357689418
test acc:  0.9172
forward train acc:  0.99898  and loss:  1.3914443440007744
test acc:  0.9173
forward train acc:  0.99926  and loss:  0.8137806750310119
test acc:  0.9174
forward train acc:  0.99914  and loss:  1.000511903250299
test acc:  0.9178
forward train acc:  0.9992  and loss:  1.1119126392586622
test acc:  0.9172
forward train acc:  0.99938  and loss:  0.8746329151181271
test acc:  0.9181
forward train acc:  0.99954  and loss:  0.6617062380464631
test acc:  0.917
forward train acc:  0.99944  and loss:  0.6089602881111205
test acc:  0.9189
forward train acc:  0.99942  and loss:  0.7842491613773745
test acc:  0.9176
********** reverse layer  3  *********
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7083333333333334  ==>  112 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.859375  ==>  108 / 768
layer  10  :  0.9453125  ==>  42 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  4
[0, 0, 2, 5, 0, 1, 0, 3, 2, 0, 5, 3, 5, 0]
***** skip layer  5
[0, 0, 2, 5, 0, 0, 0, 3, 2, 0, 5, 3, 5, 0]
optimize layer  6
backward train epoch:  345
test acc:  0.0965
forward train acc:  0.9993  and loss:  1.0230224844344775
test acc:  0.9176
forward train acc:  0.99926  and loss:  0.954060041854973
test acc:  0.9157
forward train acc:  0.9994  and loss:  0.7973322978068609
test acc:  0.9174
forward train acc:  0.99954  and loss:  0.6614786234131316
test acc:  0.9185
forward train acc:  0.99954  and loss:  0.541551777641871
test acc:  0.9186
forward train acc:  0.99956  and loss:  0.7949852559540886
test acc:  0.9199
forward train acc:  0.99956  and loss:  0.642519917433674
test acc:  0.9196
forward train acc:  0.99948  and loss:  0.6620316586922854
test acc:  0.9196
forward train acc:  0.99944  and loss:  0.6329543942410965
test acc:  0.9208
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.71875  ==>  108 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.859375  ==>  108 / 768
layer  10  :  0.9453125  ==>  42 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  7
[0, 0, 2, 5, 0, 0, 0, 2, 2, 0, 5, 3, 5, 0]
***** skip layer  8
[0, 0, 2, 5, 0, 0, 0, 2, 1, 0, 5, 3, 5, 0]
optimize layer  9
backward train epoch:  49
test acc:  0.0985
forward train acc:  0.99922  and loss:  0.8814247958362103
test acc:  0.9183
forward train acc:  0.99928  and loss:  0.9027932937606238
test acc:  0.9176
forward train acc:  0.99932  and loss:  0.689366246238933
test acc:  0.9161
forward train acc:  0.99932  and loss:  0.8522054266941268
test acc:  0.9178
forward train acc:  0.99952  and loss:  0.5279614624669193
test acc:  0.9167
forward train acc:  0.99934  and loss:  0.8457058389103622
test acc:  0.919
forward train acc:  0.99968  and loss:  0.34053386437153677
test acc:  0.9179
forward train acc:  0.9997  and loss:  0.4045853265852202
test acc:  0.917
forward train acc:  0.99962  and loss:  0.41740008231136017
test acc:  0.9182
forward train acc:  0.9996  and loss:  0.6069619772024453
test acc:  0.9183
********** reverse layer  9  *********
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.71875  ==>  108 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.859375  ==>  108 / 768
layer  10  :  0.9453125  ==>  42 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  10
[0, 0, 2, 5, 0, 0, 0, 2, 1, 5, 4, 3, 5, 0]
***** skip layer  11
[0, 0, 2, 5, 0, 0, 0, 2, 1, 5, 4, 2, 5, 0]
***** skip layer  12
[0, 0, 2, 5, 0, 0, 0, 2, 1, 5, 4, 2, 4, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.0958
forward train acc:  0.3906  and loss:  997.3031816482544
test acc:  0.3604
forward train acc:  0.39876  and loss:  958.9550100564957
test acc:  0.3628
forward train acc:  0.3989  and loss:  942.3178821802139
test acc:  0.3634
forward train acc:  0.39924  and loss:  931.076132774353
test acc:  0.3636
forward train acc:  0.39914  and loss:  923.6613819599152
test acc:  0.3637
forward train acc:  0.39946  and loss:  916.1574771404266
test acc:  0.3637
forward train acc:  0.39944  and loss:  908.8682178258896
test acc:  0.3646
forward train acc:  0.39938  and loss:  904.022499203682
test acc:  0.3644
forward train acc:  0.39966  and loss:  899.9534789323807
test acc:  0.365
forward train acc:  0.39954  and loss:  896.4314634799957
test acc:  0.3646
********** reverse layer  13  *********
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.71875  ==>  108 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.859375  ==>  108 / 768
layer  10  :  0.9453125  ==>  42 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  0
backward train epoch:  205
test acc:  0.1178
forward train acc:  0.8026  and loss:  363.94379472732544
test acc:  0.7933
forward train acc:  0.86128  and loss:  191.86686226725578
test acc:  0.8247
forward train acc:  0.88526  and loss:  152.71653392910957
test acc:  0.8382
forward train acc:  0.89994  and loss:  132.6556049734354
test acc:  0.8467
forward train acc:  0.9063  and loss:  121.8357277661562
test acc:  0.8511
forward train acc:  0.91514  and loss:  110.42368026077747
test acc:  0.8551
forward train acc:  0.9216  and loss:  102.92471268773079
test acc:  0.8609
forward train acc:  0.92618  and loss:  95.4475866407156
test acc:  0.8589
forward train acc:  0.92796  and loss:  92.8851460032165
test acc:  0.8625
forward train acc:  0.93152  and loss:  89.29290931299329
test acc:  0.8661
********** reverse layer  0  *********
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.71875  ==>  108 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.859375  ==>  108 / 768
layer  10  :  0.9453125  ==>  42 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  1
backward train epoch:  195
test acc:  0.104
forward train acc:  0.99606  and loss:  5.926896551158279
test acc:  0.9169
forward train acc:  0.9979  and loss:  2.757161558838561
test acc:  0.919
forward train acc:  0.99896  and loss:  1.4578435503644869
test acc:  0.9185
forward train acc:  0.99896  and loss:  1.2464477408793755
test acc:  0.9179
forward train acc:  0.99888  and loss:  1.3857267443090677
test acc:  0.9171
forward train acc:  0.99924  and loss:  1.0088753654272296
test acc:  0.9189
forward train acc:  0.99912  and loss:  1.0308740314503666
test acc:  0.9186
forward train acc:  0.99934  and loss:  0.8792020937835332
test acc:  0.9187
forward train acc:  0.99942  and loss:  0.6856392586196307
test acc:  0.919
forward train acc:  0.99952  and loss:  0.7091972305788659
test acc:  0.9194
********** reverse layer  1  *********
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.71875  ==>  108 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.859375  ==>  108 / 768
layer  10  :  0.9453125  ==>  42 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  2
[5, 5, 1, 5, 0, 0, 0, 2, 1, 5, 4, 2, 4, 5]
***** skip layer  3
[5, 5, 1, 4, 0, 0, 0, 2, 1, 5, 4, 2, 4, 5]
optimize layer  4
backward train epoch:  118
test acc:  0.0933
forward train acc:  0.99768  and loss:  2.930820412468165
test acc:  0.9156
forward train acc:  0.99866  and loss:  1.7310199041676242
test acc:  0.9153
forward train acc:  0.99856  and loss:  2.0110598220926477
test acc:  0.9153
forward train acc:  0.9988  and loss:  1.7249286572041456
test acc:  0.9155
forward train acc:  0.999  and loss:  1.300688706949586
test acc:  0.9163
forward train acc:  0.99876  and loss:  1.5841494322085055
test acc:  0.9159
forward train acc:  0.99912  and loss:  1.1847878292319365
test acc:  0.9169
forward train acc:  0.9993  and loss:  0.9314506071677897
test acc:  0.9164
forward train acc:  0.99934  and loss:  0.8635536322690314
test acc:  0.9163
forward train acc:  0.99934  and loss:  0.963733498880174
test acc:  0.9175
********** reverse layer  4  *********
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.71875  ==>  108 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.859375  ==>  108 / 768
layer  10  :  0.9453125  ==>  42 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  5
backward train epoch:  26
test acc:  0.1158
forward train acc:  0.99884  and loss:  1.3239450562105048
test acc:  0.9164
forward train acc:  0.99908  and loss:  1.245280436502071
test acc:  0.9173
forward train acc:  0.99928  and loss:  0.8841896781232208
test acc:  0.9177
forward train acc:  0.99942  and loss:  0.925322774448432
test acc:  0.9173
forward train acc:  0.99944  and loss:  0.7301877922727726
test acc:  0.9181
forward train acc:  0.9994  and loss:  0.7826049900904763
test acc:  0.9179
forward train acc:  0.99968  and loss:  0.49396282837551553
test acc:  0.918
forward train acc:  0.99964  and loss:  0.5852775187086081
test acc:  0.9193
forward train acc:  0.99942  and loss:  0.7549090861866716
test acc:  0.9185
forward train acc:  0.99954  and loss:  0.5816842945932876
test acc:  0.9192
********** reverse layer  5  *********
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.71875  ==>  108 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.859375  ==>  108 / 768
layer  10  :  0.9453125  ==>  42 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  6
backward train epoch:  343
test acc:  0.1218
forward train acc:  0.99936  and loss:  0.7808299447933678
test acc:  0.92
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.859375  ==>  108 / 768
layer  10  :  0.9453125  ==>  42 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  7
[5, 5, 1, 4, 5, 5, 0, 1, 1, 5, 4, 2, 4, 5]
***** skip layer  8
[5, 5, 1, 4, 5, 5, 0, 1, 0, 5, 4, 2, 4, 5]
***** skip layer  9
[5, 5, 1, 4, 5, 5, 0, 1, 0, 4, 4, 2, 4, 5]
***** skip layer  10
[5, 5, 1, 4, 5, 5, 0, 1, 0, 4, 3, 2, 4, 5]
***** skip layer  11
[5, 5, 1, 4, 5, 5, 0, 1, 0, 4, 3, 1, 4, 5]
***** skip layer  12
[5, 5, 1, 4, 5, 5, 0, 1, 0, 4, 3, 1, 3, 5]
***** skip layer  13
[5, 5, 1, 4, 5, 5, 0, 1, 0, 4, 3, 1, 3, 4]
***** skip layer  0
[4, 5, 1, 4, 5, 5, 0, 1, 0, 4, 3, 1, 3, 4]
***** skip layer  1
[4, 4, 1, 4, 5, 5, 0, 1, 0, 4, 3, 1, 3, 4]
***** skip layer  2
[4, 4, 0, 4, 5, 5, 0, 1, 0, 4, 3, 1, 3, 4]
***** skip layer  3
[4, 4, 0, 3, 5, 5, 0, 1, 0, 4, 3, 1, 3, 4]
***** skip layer  4
[4, 4, 0, 3, 4, 5, 0, 1, 0, 4, 3, 1, 3, 4]
***** skip layer  5
[4, 4, 0, 3, 4, 4, 0, 1, 0, 4, 3, 1, 3, 4]
optimize layer  6
backward train epoch:  36
test acc:  0.1069
forward train acc:  0.99904  and loss:  1.1783576577727217
test acc:  0.9178
forward train acc:  0.99922  and loss:  0.8962340926227625
test acc:  0.9173
forward train acc:  0.99928  and loss:  1.022456361213699
test acc:  0.9184
forward train acc:  0.99934  and loss:  1.0047037643234944
test acc:  0.9183
forward train acc:  0.99952  and loss:  0.7274726910836762
test acc:  0.9183
forward train acc:  0.99944  and loss:  0.8172269056231016
test acc:  0.9181
forward train acc:  0.99952  and loss:  0.7041682413473609
test acc:  0.918
forward train acc:  0.99942  and loss:  0.6613814792435733
test acc:  0.9174
forward train acc:  0.99958  and loss:  0.6295094676897861
test acc:  0.9178
forward train acc:  0.99956  and loss:  0.8075842716934858
test acc:  0.9172
********** reverse layer  6  *********
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.859375  ==>  108 / 768
layer  10  :  0.9453125  ==>  42 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  7
[4, 4, 0, 3, 4, 4, 5, 0, 0, 4, 3, 1, 3, 4]
optimize layer  8
backward train epoch:  338
test acc:  0.0988
forward train acc:  0.99282  and loss:  9.178831190394703
test acc:  0.9085
forward train acc:  0.99602  and loss:  5.221468075294979
test acc:  0.9103
forward train acc:  0.99684  and loss:  4.128167131799273
test acc:  0.912
forward train acc:  0.9975  and loss:  3.551365233026445
test acc:  0.9129
forward train acc:  0.9977  and loss:  3.017375560651999
test acc:  0.9139
forward train acc:  0.99794  and loss:  2.6976013601524755
test acc:  0.9138
forward train acc:  0.99802  and loss:  2.477060101169627
test acc:  0.9133
forward train acc:  0.99808  and loss:  2.3992897244461346
test acc:  0.9151
forward train acc:  0.99856  and loss:  2.072238397639012
test acc:  0.9143
forward train acc:  0.99866  and loss:  1.8474041394947562
test acc:  0.9151
********** reverse layer  8  *********
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.859375  ==>  108 / 768
layer  10  :  0.9453125  ==>  42 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  9
[4, 4, 0, 3, 4, 4, 5, 0, 5, 3, 3, 1, 3, 4]
***** skip layer  10
[4, 4, 0, 3, 4, 4, 5, 0, 5, 3, 2, 1, 3, 4]
***** skip layer  11
[4, 4, 0, 3, 4, 4, 5, 0, 5, 3, 2, 0, 3, 4]
***** skip layer  12
[4, 4, 0, 3, 4, 4, 5, 0, 5, 3, 2, 0, 2, 4]
***** skip layer  13
[4, 4, 0, 3, 4, 4, 5, 0, 5, 3, 2, 0, 2, 3]
***** skip layer  0
[3, 4, 0, 3, 4, 4, 5, 0, 5, 3, 2, 0, 2, 3]
***** skip layer  1
[3, 3, 0, 3, 4, 4, 5, 0, 5, 3, 2, 0, 2, 3]
optimize layer  2
backward train epoch:  328
test acc:  0.111
forward train acc:  0.99602  and loss:  5.730815198970959
test acc:  0.9113
forward train acc:  0.99708  and loss:  3.9645571228465997
test acc:  0.9157
forward train acc:  0.99746  and loss:  3.281047742260853
test acc:  0.9148
forward train acc:  0.99762  and loss:  3.0253436233324464
test acc:  0.9154
forward train acc:  0.99764  and loss:  2.703417040029308
test acc:  0.9165
forward train acc:  0.99852  and loss:  1.879390702699311
test acc:  0.916
forward train acc:  0.99826  and loss:  2.1130618350289296
test acc:  0.915
forward train acc:  0.99822  and loss:  2.1568765651027206
test acc:  0.9153
forward train acc:  0.9988  and loss:  1.7777609870827291
test acc:  0.9152
forward train acc:  0.99896  and loss:  1.2565667244634824
test acc:  0.9168
********** reverse layer  2  *********
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.859375  ==>  108 / 768
layer  10  :  0.9453125  ==>  42 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  3
[3, 3, 5, 2, 4, 4, 5, 0, 5, 3, 2, 0, 2, 3]
***** skip layer  4
[3, 3, 5, 2, 3, 4, 5, 0, 5, 3, 2, 0, 2, 3]
***** skip layer  5
[3, 3, 5, 2, 3, 3, 5, 0, 5, 3, 2, 0, 2, 3]
***** skip layer  6
[3, 3, 5, 2, 3, 3, 4, 0, 5, 3, 2, 0, 2, 3]
optimize layer  7
backward train epoch:  93
test acc:  0.104
forward train acc:  0.99434  and loss:  7.255447150440887
test acc:  0.9089
forward train acc:  0.99762  and loss:  3.0740991679485887
test acc:  0.9116
forward train acc:  0.9975  and loss:  3.023392254690407
test acc:  0.9122
forward train acc:  0.99832  and loss:  2.2362072370015085
test acc:  0.9134
forward train acc:  0.99796  and loss:  2.221694638079498
test acc:  0.9129
forward train acc:  0.99878  and loss:  1.4774199286475778
test acc:  0.9138
forward train acc:  0.9988  and loss:  1.5391818072093884
test acc:  0.9141
forward train acc:  0.99876  and loss:  1.678111386456294
test acc:  0.9144
forward train acc:  0.99858  and loss:  1.7166580870834878
test acc:  0.9136
forward train acc:  0.99882  and loss:  1.3710002280713525
test acc:  0.9155
********** reverse layer  7  *********
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.859375  ==>  108 / 768
layer  10  :  0.9453125  ==>  42 / 768
layer  11  :  0.9713541666666666  ==>  22 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  8
[3, 3, 5, 2, 3, 3, 4, 5, 4, 3, 2, 0, 2, 3]
***** skip layer  9
[3, 3, 5, 2, 3, 3, 4, 5, 4, 2, 2, 0, 2, 3]
***** skip layer  10
[3, 3, 5, 2, 3, 3, 4, 5, 4, 2, 1, 0, 2, 3]
optimize layer  11
backward train epoch:  249
test acc:  0.1
forward train acc:  0.9958  and loss:  7.442406784277409
test acc:  0.9173
forward train acc:  0.99956  and loss:  1.0934511052910239
test acc:  0.9182
forward train acc:  0.9995  and loss:  1.0041830322006717
test acc:  0.9183
forward train acc:  0.99962  and loss:  0.7528426073258743
test acc:  0.9197
forward train acc:  0.99954  and loss:  0.745807236089604
test acc:  0.9202
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.859375  ==>  108 / 768
layer  10  :  0.9453125  ==>  42 / 768
layer  11  :  0.9791666666666666  ==>  16 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  12
[3, 3, 5, 2, 3, 3, 4, 5, 4, 2, 1, 0, 1, 3]
***** skip layer  13
[3, 3, 5, 2, 3, 3, 4, 5, 4, 2, 1, 0, 1, 2]
***** skip layer  0
[2, 3, 5, 2, 3, 3, 4, 5, 4, 2, 1, 0, 1, 2]
***** skip layer  1
[2, 2, 5, 2, 3, 3, 4, 5, 4, 2, 1, 0, 1, 2]
***** skip layer  2
[2, 2, 4, 2, 3, 3, 4, 5, 4, 2, 1, 0, 1, 2]
***** skip layer  3
[2, 2, 4, 1, 3, 3, 4, 5, 4, 2, 1, 0, 1, 2]
***** skip layer  4
[2, 2, 4, 1, 2, 3, 4, 5, 4, 2, 1, 0, 1, 2]
***** skip layer  5
[2, 2, 4, 1, 2, 2, 4, 5, 4, 2, 1, 0, 1, 2]
***** skip layer  6
[2, 2, 4, 1, 2, 2, 3, 5, 4, 2, 1, 0, 1, 2]
***** skip layer  7
[2, 2, 4, 1, 2, 2, 3, 4, 4, 2, 1, 0, 1, 2]
***** skip layer  8
[2, 2, 4, 1, 2, 2, 3, 4, 3, 2, 1, 0, 1, 2]
***** skip layer  9
[2, 2, 4, 1, 2, 2, 3, 4, 3, 1, 1, 0, 1, 2]
***** skip layer  10
[2, 2, 4, 1, 2, 2, 3, 4, 3, 1, 0, 0, 1, 2]
optimize layer  11
backward train epoch:  85
test acc:  0.0989
forward train acc:  0.9985  and loss:  5.306949300458655
test acc:  0.9168
forward train acc:  0.99928  and loss:  1.6228882191935554
test acc:  0.9175
forward train acc:  0.9995  and loss:  1.260089033341501
test acc:  0.9182
forward train acc:  0.99952  and loss:  1.0176679555443116
test acc:  0.9199
forward train acc:  0.99964  and loss:  0.8213549376814626
test acc:  0.9187
forward train acc:  0.99952  and loss:  0.9667624367866665
test acc:  0.9183
forward train acc:  0.99954  and loss:  0.8618215526803397
test acc:  0.9189
forward train acc:  0.99956  and loss:  0.7543377067195252
test acc:  0.9187
forward train acc:  0.99964  and loss:  0.6823839173011947
test acc:  0.9193
forward train acc:  0.99946  and loss:  0.8742748773656785
test acc:  0.9187
********** reverse layer  11  *********
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.859375  ==>  108 / 768
layer  10  :  0.9453125  ==>  42 / 768
layer  11  :  0.9791666666666666  ==>  16 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  12
[2, 2, 4, 1, 2, 2, 3, 4, 3, 1, 0, 5, 0, 2]
***** skip layer  13
[2, 2, 4, 1, 2, 2, 3, 4, 3, 1, 0, 5, 0, 1]
***** skip layer  0
[1, 2, 4, 1, 2, 2, 3, 4, 3, 1, 0, 5, 0, 1]
***** skip layer  1
[1, 1, 4, 1, 2, 2, 3, 4, 3, 1, 0, 5, 0, 1]
***** skip layer  2
[1, 1, 3, 1, 2, 2, 3, 4, 3, 1, 0, 5, 0, 1]
***** skip layer  3
[1, 1, 3, 0, 2, 2, 3, 4, 3, 1, 0, 5, 0, 1]
***** skip layer  4
[1, 1, 3, 0, 1, 2, 3, 4, 3, 1, 0, 5, 0, 1]
***** skip layer  5
[1, 1, 3, 0, 1, 1, 3, 4, 3, 1, 0, 5, 0, 1]
***** skip layer  6
[1, 1, 3, 0, 1, 1, 2, 4, 3, 1, 0, 5, 0, 1]
***** skip layer  7
[1, 1, 3, 0, 1, 1, 2, 3, 3, 1, 0, 5, 0, 1]
***** skip layer  8
[1, 1, 3, 0, 1, 1, 2, 3, 2, 1, 0, 5, 0, 1]
***** skip layer  9
[1, 1, 3, 0, 1, 1, 2, 3, 2, 0, 0, 5, 0, 1]
optimize layer  10
backward train epoch:  141
test acc:  0.1059
forward train acc:  0.9995  and loss:  0.9503793947806116
test acc:  0.9188
forward train acc:  0.99954  and loss:  0.6073565564292949
test acc:  0.9195
forward train acc:  0.99942  and loss:  0.8002894025848946
test acc:  0.9206
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.859375  ==>  108 / 768
layer  10  :  0.953125  ==>  36 / 768
layer  11  :  0.9791666666666666  ==>  16 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  11
[1, 1, 3, 0, 1, 1, 2, 3, 2, 0, 0, 4, 0, 1]
optimize layer  12
backward train epoch:  107
test acc:  0.1
forward train acc:  0.1  and loss:  910.6605925559998
test acc:  0.1
forward train acc:  0.1  and loss:  907.5098052024841
test acc:  0.1
forward train acc:  0.1  and loss:  905.2223522663116
test acc:  0.1
forward train acc:  0.1  and loss:  903.9329578876495
test acc:  0.1
forward train acc:  0.1  and loss:  903.2514433860779
test acc:  0.1
forward train acc:  0.1  and loss:  902.6714265346527
test acc:  0.1
forward train acc:  0.1  and loss:  902.1964364051819
test acc:  0.1
forward train acc:  0.1  and loss:  901.8966150283813
test acc:  0.1
forward train acc:  0.1  and loss:  901.7227649688721
test acc:  0.1
forward train acc:  0.1  and loss:  901.5577783584595
test acc:  0.1
********** reverse layer  12  *********
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5625  ==>  84 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.859375  ==>  108 / 768
layer  10  :  0.953125  ==>  36 / 768
layer  11  :  0.9791666666666666  ==>  16 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  13
[1, 1, 3, 0, 1, 1, 2, 3, 2, 0, 0, 4, 5, 0]
***** skip layer  0
[0, 1, 3, 0, 1, 1, 2, 3, 2, 0, 0, 4, 5, 0]
***** skip layer  1
[0, 0, 3, 0, 1, 1, 2, 3, 2, 0, 0, 4, 5, 0]
***** skip layer  2
[0, 0, 2, 0, 1, 1, 2, 3, 2, 0, 0, 4, 5, 0]
optimize layer  3
backward train epoch:  3
test acc:  0.3105
forward train acc:  0.99904  and loss:  1.3243449841975234
test acc:  0.9167
forward train acc:  0.99892  and loss:  1.5485558943473734
test acc:  0.9181
forward train acc:  0.99902  and loss:  1.335803232010221
test acc:  0.9178
forward train acc:  0.99914  and loss:  1.0250916941877222
test acc:  0.9177
forward train acc:  0.99926  and loss:  0.9562750637560384
test acc:  0.9194
forward train acc:  0.9994  and loss:  0.8962576177291339
test acc:  0.9178
forward train acc:  0.9991  and loss:  1.0852437225548783
test acc:  0.9183
forward train acc:  0.9995  and loss:  0.6444007050740765
test acc:  0.9199
forward train acc:  0.99952  and loss:  0.6963140263396781
test acc:  0.9203
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5729166666666666  ==>  82 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.859375  ==>  108 / 768
layer  10  :  0.953125  ==>  36 / 768
layer  11  :  0.9791666666666666  ==>  16 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  4
[0, 0, 2, 0, 0, 1, 2, 3, 2, 0, 0, 4, 5, 0]
***** skip layer  5
[0, 0, 2, 0, 0, 0, 2, 3, 2, 0, 0, 4, 5, 0]
***** skip layer  6
[0, 0, 2, 0, 0, 0, 1, 3, 2, 0, 0, 4, 5, 0]
***** skip layer  7
[0, 0, 2, 0, 0, 0, 1, 2, 2, 0, 0, 4, 5, 0]
***** skip layer  8
[0, 0, 2, 0, 0, 0, 1, 2, 1, 0, 0, 4, 5, 0]
optimize layer  9
backward train epoch:  150
test acc:  0.1157
forward train acc:  0.99934  and loss:  0.8581306763226166
test acc:  0.9182
forward train acc:  0.99908  and loss:  1.2754131795809371
test acc:  0.9188
forward train acc:  0.99902  and loss:  1.217446714596008
test acc:  0.9191
forward train acc:  0.99936  and loss:  0.8690356208826415
test acc:  0.919
forward train acc:  0.99944  and loss:  0.7419566567405127
test acc:  0.9203
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5729166666666666  ==>  82 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8671875  ==>  102 / 768
layer  10  :  0.953125  ==>  36 / 768
layer  11  :  0.9791666666666666  ==>  16 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  10
backward train epoch:  173
test acc:  0.1037
forward train acc:  0.99878  and loss:  3.149398786306847
test acc:  0.9201
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5729166666666666  ==>  82 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8671875  ==>  102 / 768
layer  10  :  0.9609375  ==>  30 / 768
layer  11  :  0.9791666666666666  ==>  16 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  11
[0, 0, 2, 0, 0, 0, 1, 2, 1, 0, 0, 3, 5, 0]
***** skip layer  12
[0, 0, 2, 0, 0, 0, 1, 2, 1, 0, 0, 3, 4, 0]
optimize layer  13
backward train epoch:  102
test acc:  0.1085
forward train acc:  0.97088  and loss:  51.61336725577712
test acc:  0.913
forward train acc:  0.9969  and loss:  38.82151397317648
test acc:  0.9125
forward train acc:  0.99836  and loss:  36.36675362661481
test acc:  0.914
forward train acc:  0.9988  and loss:  34.43967467546463
test acc:  0.916
forward train acc:  0.99914  and loss:  33.42473394796252
test acc:  0.9161
forward train acc:  0.9989  and loss:  32.608939573168755
test acc:  0.9163
forward train acc:  0.99892  and loss:  31.606221886351705
test acc:  0.9155
forward train acc:  0.99914  and loss:  30.829765340313315
test acc:  0.916
forward train acc:  0.99944  and loss:  30.294587820768356
test acc:  0.9163
forward train acc:  0.99926  and loss:  29.901325274258852
test acc:  0.9166
********** reverse layer  13  *********
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5729166666666666  ==>  82 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8671875  ==>  102 / 768
layer  10  :  0.9609375  ==>  30 / 768
layer  11  :  0.9791666666666666  ==>  16 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  0
backward train epoch:  94
test acc:  0.0834
forward train acc:  0.99918  and loss:  1.2491800622665323
test acc:  0.9177
forward train acc:  0.9993  and loss:  1.2084963205561507
test acc:  0.9173
forward train acc:  0.99958  and loss:  0.65898085795925
test acc:  0.9161
forward train acc:  0.99938  and loss:  0.9468542514368892
test acc:  0.9166
forward train acc:  0.99928  and loss:  1.0875579373387154
test acc:  0.917
forward train acc:  0.9995  and loss:  0.8295161892456235
test acc:  0.918
forward train acc:  0.99928  and loss:  0.9518204627238447
test acc:  0.9151
forward train acc:  0.99952  and loss:  0.751589938547113
test acc:  0.9159
forward train acc:  0.99938  and loss:  0.7872958881489467
test acc:  0.9169
forward train acc:  0.99962  and loss:  0.6993308310338762
test acc:  0.9171
********** reverse layer  0  *********
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5729166666666666  ==>  82 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8671875  ==>  102 / 768
layer  10  :  0.9609375  ==>  30 / 768
layer  11  :  0.9791666666666666  ==>  16 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  1
backward train epoch:  147
test acc:  0.1
forward train acc:  0.9982  and loss:  2.813362917455379
test acc:  0.9157
forward train acc:  0.99888  and loss:  1.7905259211547673
test acc:  0.916
forward train acc:  0.99842  and loss:  2.1564432748418767
test acc:  0.9163
forward train acc:  0.99896  and loss:  1.3226720398233738
test acc:  0.9177
forward train acc:  0.9989  and loss:  1.569666600698838
test acc:  0.918
forward train acc:  0.9992  and loss:  1.0218860487511847
test acc:  0.9182
forward train acc:  0.99894  and loss:  1.194058693916304
test acc:  0.9184
forward train acc:  0.99916  and loss:  0.9955095469340449
test acc:  0.918
forward train acc:  0.99922  and loss:  1.1368811054562684
test acc:  0.9179
forward train acc:  0.99914  and loss:  1.2463964665221283
test acc:  0.9183
********** reverse layer  1  *********
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5729166666666666  ==>  82 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8671875  ==>  102 / 768
layer  10  :  0.9609375  ==>  30 / 768
layer  11  :  0.9791666666666666  ==>  16 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  2
[5, 5, 1, 0, 0, 0, 1, 2, 1, 0, 0, 3, 4, 5]
optimize layer  3
backward train epoch:  1
test acc:  0.1015
forward train acc:  0.99876  and loss:  1.6670296555676032
test acc:  0.9173
forward train acc:  0.99886  and loss:  1.3593766835110728
test acc:  0.9165
forward train acc:  0.99882  and loss:  1.3187492619472323
test acc:  0.9158
forward train acc:  0.99928  and loss:  1.296006994205527
test acc:  0.9178
forward train acc:  0.99926  and loss:  0.8452342937525827
test acc:  0.9166
forward train acc:  0.99924  and loss:  0.9834080901928246
test acc:  0.918
forward train acc:  0.99938  and loss:  0.6896150022366783
test acc:  0.9176
forward train acc:  0.99942  and loss:  0.9231139007752063
test acc:  0.9185
forward train acc:  0.99944  and loss:  0.7161272385128541
test acc:  0.9201
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5833333333333334  ==>  80 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8671875  ==>  102 / 768
layer  10  :  0.9609375  ==>  30 / 768
layer  11  :  0.9791666666666666  ==>  16 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  4
backward train epoch:  407
test acc:  0.1105
forward train acc:  0.99778  and loss:  2.867631812405307
test acc:  0.9155
forward train acc:  0.99814  and loss:  2.6347975636890624
test acc:  0.916
forward train acc:  0.99828  and loss:  2.2315085640293546
test acc:  0.9175
forward train acc:  0.99848  and loss:  1.9387148669338785
test acc:  0.9163
forward train acc:  0.99898  and loss:  1.5272556222917046
test acc:  0.9177
forward train acc:  0.99852  and loss:  1.7417416680546012
test acc:  0.9161
forward train acc:  0.9989  and loss:  1.48220232676249
test acc:  0.9172
forward train acc:  0.99886  and loss:  1.5153049827204086
test acc:  0.9177
forward train acc:  0.99866  and loss:  1.848398915521102
test acc:  0.9174
forward train acc:  0.99894  and loss:  1.486653737767483
test acc:  0.9168
********** reverse layer  4  *********
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5833333333333334  ==>  80 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8671875  ==>  102 / 768
layer  10  :  0.9609375  ==>  30 / 768
layer  11  :  0.9791666666666666  ==>  16 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  5
backward train epoch:  223
test acc:  0.0906
forward train acc:  0.99858  and loss:  2.037090995494509
test acc:  0.9172
forward train acc:  0.99858  and loss:  1.7619329432782251
test acc:  0.9163
forward train acc:  0.99892  and loss:  1.3870133614400402
test acc:  0.9173
forward train acc:  0.99906  and loss:  1.109092964004958
test acc:  0.9192
forward train acc:  0.99936  and loss:  0.8433031918248162
test acc:  0.9191
forward train acc:  0.99926  and loss:  1.0848354153131368
test acc:  0.9184
forward train acc:  0.99926  and loss:  1.0808879206742859
test acc:  0.9171
forward train acc:  0.99938  and loss:  0.8047206927585648
test acc:  0.9191
forward train acc:  0.99948  and loss:  0.7278515270154458
test acc:  0.9189
forward train acc:  0.9995  and loss:  0.757557252625702
test acc:  0.9181
********** reverse layer  5  *********
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5833333333333334  ==>  80 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8671875  ==>  102 / 768
layer  10  :  0.9609375  ==>  30 / 768
layer  11  :  0.9791666666666666  ==>  16 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  6
[5, 5, 1, 0, 5, 5, 0, 2, 1, 0, 0, 3, 4, 5]
***** skip layer  7
[5, 5, 1, 0, 5, 5, 0, 1, 1, 0, 0, 3, 4, 5]
***** skip layer  8
[5, 5, 1, 0, 5, 5, 0, 1, 0, 0, 0, 3, 4, 5]
optimize layer  9
backward train epoch:  79
test acc:  0.1008
forward train acc:  0.9994  and loss:  0.8609426812181482
test acc:  0.9186
forward train acc:  0.9993  and loss:  0.9664901387877762
test acc:  0.9191
forward train acc:  0.99926  and loss:  0.8162909787206445
test acc:  0.9167
forward train acc:  0.99936  and loss:  1.0196288789011305
test acc:  0.9177
forward train acc:  0.9996  and loss:  0.5832422067978769
test acc:  0.9185
forward train acc:  0.9996  and loss:  0.5662440003361553
test acc:  0.9183
forward train acc:  0.99924  and loss:  1.1242420015914831
test acc:  0.9193
forward train acc:  0.9996  and loss:  0.4903346459977911
test acc:  0.9184
forward train acc:  0.99952  and loss:  0.5286456705143792
test acc:  0.92
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5833333333333334  ==>  80 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.875  ==>  96 / 768
layer  10  :  0.9609375  ==>  30 / 768
layer  11  :  0.9791666666666666  ==>  16 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  10
backward train epoch:  13
test acc:  0.1628
forward train acc:  0.99948  and loss:  0.8364918837323785
test acc:  0.9183
forward train acc:  0.9991  and loss:  1.057221924274927
test acc:  0.917
forward train acc:  0.99914  and loss:  1.1612117151016719
test acc:  0.9181
forward train acc:  0.99938  and loss:  0.7001487279922003
test acc:  0.9198
forward train acc:  0.99924  and loss:  1.0625335864533554
test acc:  0.9195
forward train acc:  0.99936  and loss:  0.811315514765738
test acc:  0.9183
forward train acc:  0.99944  and loss:  0.7098231651470996
test acc:  0.9199
forward train acc:  0.99948  and loss:  0.7234636032226263
test acc:  0.9192
forward train acc:  0.99952  and loss:  0.5428206386131933
test acc:  0.9191
forward train acc:  0.99968  and loss:  0.5978518451447599
test acc:  0.9192
********** reverse layer  10  *********
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5833333333333334  ==>  80 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.875  ==>  96 / 768
layer  10  :  0.9609375  ==>  30 / 768
layer  11  :  0.9791666666666666  ==>  16 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  11
[5, 5, 1, 0, 5, 5, 0, 1, 0, 0, 5, 2, 4, 5]
***** skip layer  12
[5, 5, 1, 0, 5, 5, 0, 1, 0, 0, 5, 2, 3, 5]
***** skip layer  13
[5, 5, 1, 0, 5, 5, 0, 1, 0, 0, 5, 2, 3, 4]
***** skip layer  0
[4, 5, 1, 0, 5, 5, 0, 1, 0, 0, 5, 2, 3, 4]
***** skip layer  1
[4, 4, 1, 0, 5, 5, 0, 1, 0, 0, 5, 2, 3, 4]
***** skip layer  2
[4, 4, 0, 0, 5, 5, 0, 1, 0, 0, 5, 2, 3, 4]
optimize layer  3
backward train epoch:  1
test acc:  0.8333
forward train acc:  0.99894  and loss:  1.5744376620277762
test acc:  0.9159
forward train acc:  0.99896  and loss:  1.4225566246459493
test acc:  0.9159
forward train acc:  0.99888  and loss:  1.3970108231224003
test acc:  0.9158
forward train acc:  0.999  and loss:  1.3182756255264394
test acc:  0.9184
forward train acc:  0.99896  and loss:  1.3623014550976222
test acc:  0.9185
forward train acc:  0.99908  and loss:  1.0202605695521925
test acc:  0.9195
forward train acc:  0.99924  and loss:  1.0312138640729245
test acc:  0.917
forward train acc:  0.99924  and loss:  0.9679004241115763
test acc:  0.9193
forward train acc:  0.99932  and loss:  0.8942838960356312
test acc:  0.9186
forward train acc:  0.9994  and loss:  0.8365011081332341
test acc:  0.9186
********** reverse layer  3  *********
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5833333333333334  ==>  80 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.875  ==>  96 / 768
layer  10  :  0.9609375  ==>  30 / 768
layer  11  :  0.9791666666666666  ==>  16 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  4
[4, 4, 0, 5, 4, 5, 0, 1, 0, 0, 5, 2, 3, 4]
***** skip layer  5
[4, 4, 0, 5, 4, 4, 0, 1, 0, 0, 5, 2, 3, 4]
optimize layer  6
backward train epoch:  189
test acc:  0.1221
forward train acc:  0.99938  and loss:  0.7649658168666065
test acc:  0.9169
forward train acc:  0.99928  and loss:  0.8664652645093156
test acc:  0.916
forward train acc:  0.99898  and loss:  1.1961896413995419
test acc:  0.918
forward train acc:  0.99932  and loss:  0.7974557384004584
test acc:  0.9173
forward train acc:  0.99936  and loss:  0.9238259651465341
test acc:  0.9179
forward train acc:  0.99934  and loss:  0.9081011916568968
test acc:  0.9172
forward train acc:  0.99942  and loss:  0.650152029687888
test acc:  0.9181
forward train acc:  0.9992  and loss:  1.0320775146246888
test acc:  0.9186
forward train acc:  0.9996  and loss:  0.5299948444589972
test acc:  0.9168
forward train acc:  0.99942  and loss:  0.6183721679699374
test acc:  0.9188
********** reverse layer  6  *********
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5833333333333334  ==>  80 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.875  ==>  96 / 768
layer  10  :  0.9609375  ==>  30 / 768
layer  11  :  0.9791666666666666  ==>  16 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  7
[4, 4, 0, 5, 4, 4, 5, 0, 0, 0, 5, 2, 3, 4]
optimize layer  8
backward train epoch:  36
test acc:  0.0936
forward train acc:  0.98298  and loss:  23.96954119997099
test acc:  0.909
forward train acc:  0.99316  and loss:  9.302753628464416
test acc:  0.9121
forward train acc:  0.995  and loss:  6.378034870373085
test acc:  0.9116
forward train acc:  0.99652  and loss:  4.85470189829357
test acc:  0.9124
forward train acc:  0.9966  and loss:  4.69113809405826
test acc:  0.9123
forward train acc:  0.99684  and loss:  4.319799575896468
test acc:  0.9122
forward train acc:  0.9973  and loss:  3.669169885048177
test acc:  0.9134
forward train acc:  0.99754  and loss:  3.4017328131303657
test acc:  0.9129
forward train acc:  0.99782  and loss:  3.244972733518807
test acc:  0.9145
forward train acc:  0.99776  and loss:  3.3214817096304614
test acc:  0.914
********** reverse layer  8  *********
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5833333333333334  ==>  80 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.875  ==>  96 / 768
layer  10  :  0.9609375  ==>  30 / 768
layer  11  :  0.9791666666666666  ==>  16 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  9
backward train epoch:  197
test acc:  0.105
forward train acc:  0.99936  and loss:  0.7280578024219722
test acc:  0.9194
forward train acc:  0.99952  and loss:  0.6486827297776472
test acc:  0.9163
forward train acc:  0.99924  and loss:  1.0591985906503396
test acc:  0.9183
forward train acc:  0.9994  and loss:  0.6904307601071196
test acc:  0.9195
forward train acc:  0.99946  and loss:  0.6346981196402339
test acc:  0.9184
forward train acc:  0.99956  and loss:  0.649407406541286
test acc:  0.9201
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5833333333333334  ==>  80 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8828125  ==>  90 / 768
layer  10  :  0.9609375  ==>  30 / 768
layer  11  :  0.9791666666666666  ==>  16 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  10
[4, 4, 0, 5, 4, 4, 5, 0, 5, 0, 4, 2, 3, 4]
***** skip layer  11
[4, 4, 0, 5, 4, 4, 5, 0, 5, 0, 4, 1, 3, 4]
***** skip layer  12
[4, 4, 0, 5, 4, 4, 5, 0, 5, 0, 4, 1, 2, 4]
***** skip layer  13
[4, 4, 0, 5, 4, 4, 5, 0, 5, 0, 4, 1, 2, 3]
***** skip layer  0
[3, 4, 0, 5, 4, 4, 5, 0, 5, 0, 4, 1, 2, 3]
***** skip layer  1
[3, 3, 0, 5, 4, 4, 5, 0, 5, 0, 4, 1, 2, 3]
optimize layer  2
backward train epoch:  39
test acc:  0.1251
forward train acc:  0.99918  and loss:  1.298175357900618
test acc:  0.9153
forward train acc:  0.99914  and loss:  1.008562979681301
test acc:  0.916
forward train acc:  0.99916  and loss:  0.9429243349877652
test acc:  0.9158
forward train acc:  0.99908  and loss:  1.063715119526023
test acc:  0.9171
forward train acc:  0.99926  and loss:  1.0228782682315796
test acc:  0.9182
forward train acc:  0.9994  and loss:  0.8806293747620657
test acc:  0.9181
forward train acc:  0.99932  and loss:  0.7797511875396594
test acc:  0.9182
forward train acc:  0.99936  and loss:  0.779232499517093
test acc:  0.9181
forward train acc:  0.99944  and loss:  0.5961162052117288
test acc:  0.9188
forward train acc:  0.99952  and loss:  0.5855430193478242
test acc:  0.9189
********** reverse layer  2  *********
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5833333333333334  ==>  80 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8828125  ==>  90 / 768
layer  10  :  0.9609375  ==>  30 / 768
layer  11  :  0.9791666666666666  ==>  16 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  3
[3, 3, 5, 4, 4, 4, 5, 0, 5, 0, 4, 1, 2, 3]
***** skip layer  4
[3, 3, 5, 4, 3, 4, 5, 0, 5, 0, 4, 1, 2, 3]
***** skip layer  5
[3, 3, 5, 4, 3, 3, 5, 0, 5, 0, 4, 1, 2, 3]
***** skip layer  6
[3, 3, 5, 4, 3, 3, 4, 0, 5, 0, 4, 1, 2, 3]
optimize layer  7
backward train epoch:  25
test acc:  0.1136
forward train acc:  0.99484  and loss:  6.9649835414020345
test acc:  0.9145
forward train acc:  0.99758  and loss:  3.353220744727878
test acc:  0.9152
forward train acc:  0.99802  and loss:  2.5262721358740237
test acc:  0.9136
forward train acc:  0.99842  and loss:  2.248990379142924
test acc:  0.9154
forward train acc:  0.99848  and loss:  2.119480607827427
test acc:  0.9161
forward train acc:  0.99836  and loss:  1.960036734904861
test acc:  0.9152
forward train acc:  0.99844  and loss:  1.9064192146033747
test acc:  0.9158
forward train acc:  0.9989  and loss:  1.5732298830262152
test acc:  0.9169
forward train acc:  0.99894  and loss:  1.3994823104003444
test acc:  0.9175
forward train acc:  0.99876  and loss:  1.5493727125576697
test acc:  0.9167
********** reverse layer  7  *********
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5833333333333334  ==>  80 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8828125  ==>  90 / 768
layer  10  :  0.9609375  ==>  30 / 768
layer  11  :  0.9791666666666666  ==>  16 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  8
[3, 3, 5, 4, 3, 3, 4, 5, 4, 0, 4, 1, 2, 3]
optimize layer  9
backward train epoch:  77
test acc:  0.1071
forward train acc:  0.99902  and loss:  1.2198753239499638
test acc:  0.9204
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5833333333333334  ==>  80 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.890625  ==>  84 / 768
layer  10  :  0.9609375  ==>  30 / 768
layer  11  :  0.9791666666666666  ==>  16 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  10
[3, 3, 5, 4, 3, 3, 4, 5, 4, 0, 3, 1, 2, 3]
***** skip layer  11
[3, 3, 5, 4, 3, 3, 4, 5, 4, 0, 3, 0, 2, 3]
***** skip layer  12
[3, 3, 5, 4, 3, 3, 4, 5, 4, 0, 3, 0, 1, 3]
***** skip layer  13
[3, 3, 5, 4, 3, 3, 4, 5, 4, 0, 3, 0, 1, 2]
***** skip layer  0
[2, 3, 5, 4, 3, 3, 4, 5, 4, 0, 3, 0, 1, 2]
***** skip layer  1
[2, 2, 5, 4, 3, 3, 4, 5, 4, 0, 3, 0, 1, 2]
***** skip layer  2
[2, 2, 4, 4, 3, 3, 4, 5, 4, 0, 3, 0, 1, 2]
***** skip layer  3
[2, 2, 4, 3, 3, 3, 4, 5, 4, 0, 3, 0, 1, 2]
***** skip layer  4
[2, 2, 4, 3, 2, 3, 4, 5, 4, 0, 3, 0, 1, 2]
***** skip layer  5
[2, 2, 4, 3, 2, 2, 4, 5, 4, 0, 3, 0, 1, 2]
***** skip layer  6
[2, 2, 4, 3, 2, 2, 3, 5, 4, 0, 3, 0, 1, 2]
***** skip layer  7
[2, 2, 4, 3, 2, 2, 3, 4, 4, 0, 3, 0, 1, 2]
***** skip layer  8
[2, 2, 4, 3, 2, 2, 3, 4, 3, 0, 3, 0, 1, 2]
optimize layer  9
backward train epoch:  55
test acc:  0.1087
forward train acc:  0.99946  and loss:  0.829836972465273
test acc:  0.9211
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5833333333333334  ==>  80 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8984375  ==>  78 / 768
layer  10  :  0.9609375  ==>  30 / 768
layer  11  :  0.9791666666666666  ==>  16 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  10
[2, 2, 4, 3, 2, 2, 3, 4, 3, 0, 2, 0, 1, 2]
optimize layer  11
backward train epoch:  47
test acc:  0.112
forward train acc:  0.98598  and loss:  25.241835221182555
test acc:  0.9195
forward train acc:  0.99938  and loss:  3.1260624879505485
test acc:  0.9193
forward train acc:  0.99944  and loss:  1.9617850544163957
test acc:  0.9199
forward train acc:  0.99944  and loss:  1.6612427487270907
test acc:  0.9185
forward train acc:  0.99932  and loss:  1.696195338270627
test acc:  0.9189
forward train acc:  0.99952  and loss:  1.3865968828322366
test acc:  0.9206
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5833333333333334  ==>  80 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.8984375  ==>  78 / 768
layer  10  :  0.9609375  ==>  30 / 768
layer  11  :  0.9869791666666666  ==>  10 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  12
[2, 2, 4, 3, 2, 2, 3, 4, 3, 0, 2, 0, 0, 2]
***** skip layer  13
[2, 2, 4, 3, 2, 2, 3, 4, 3, 0, 2, 0, 0, 1]
***** skip layer  0
[1, 2, 4, 3, 2, 2, 3, 4, 3, 0, 2, 0, 0, 1]
***** skip layer  1
[1, 1, 4, 3, 2, 2, 3, 4, 3, 0, 2, 0, 0, 1]
***** skip layer  2
[1, 1, 3, 3, 2, 2, 3, 4, 3, 0, 2, 0, 0, 1]
***** skip layer  3
[1, 1, 3, 2, 2, 2, 3, 4, 3, 0, 2, 0, 0, 1]
***** skip layer  4
[1, 1, 3, 2, 1, 2, 3, 4, 3, 0, 2, 0, 0, 1]
***** skip layer  5
[1, 1, 3, 2, 1, 1, 3, 4, 3, 0, 2, 0, 0, 1]
***** skip layer  6
[1, 1, 3, 2, 1, 1, 2, 4, 3, 0, 2, 0, 0, 1]
***** skip layer  7
[1, 1, 3, 2, 1, 1, 2, 3, 3, 0, 2, 0, 0, 1]
***** skip layer  8
[1, 1, 3, 2, 1, 1, 2, 3, 2, 0, 2, 0, 0, 1]
optimize layer  9
backward train epoch:  118
test acc:  0.1156
forward train acc:  0.99942  and loss:  1.2756099920952693
test acc:  0.92
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5833333333333334  ==>  80 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.90625  ==>  72 / 768
layer  10  :  0.9609375  ==>  30 / 768
layer  11  :  0.9869791666666666  ==>  10 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  10
[1, 1, 3, 2, 1, 1, 2, 3, 2, 0, 1, 0, 0, 1]
optimize layer  11
backward train epoch:  296
test acc:  0.0956
forward train acc:  0.91172  and loss:  164.1859406903386
test acc:  0.9157
forward train acc:  0.99872  and loss:  25.218329045921564
test acc:  0.9176
forward train acc:  0.99918  and loss:  12.496528550982475
test acc:  0.9171
forward train acc:  0.99906  and loss:  8.654070382937789
test acc:  0.9196
forward train acc:  0.99924  and loss:  6.796780664473772
test acc:  0.9184
forward train acc:  0.99944  and loss:  5.487671690061688
test acc:  0.919
forward train acc:  0.99936  and loss:  4.554339893627912
test acc:  0.9193
forward train acc:  0.99956  and loss:  4.0369537132792175
test acc:  0.9205
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5833333333333334  ==>  80 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.90625  ==>  72 / 768
layer  10  :  0.9609375  ==>  30 / 768
layer  11  :  0.9947916666666666  ==>  4 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
optimize layer  12
backward train epoch:  109
test acc:  0.1
forward train acc:  0.1  and loss:  923.1060254573822
test acc:  0.1
forward train acc:  0.1  and loss:  917.37508893013
test acc:  0.1
forward train acc:  0.1  and loss:  912.8587920665741
test acc:  0.1
forward train acc:  0.1  and loss:  910.1056609153748
test acc:  0.1
forward train acc:  0.1  and loss:  908.5643393993378
test acc:  0.1
forward train acc:  0.1  and loss:  907.200231552124
test acc:  0.1
forward train acc:  0.1  and loss:  905.9997265338898
test acc:  0.1
forward train acc:  0.1  and loss:  905.220427274704
test acc:  0.1
forward train acc:  0.1  and loss:  904.7386705875397
test acc:  0.1
forward train acc:  0.1  and loss:  904.2932615280151
test acc:  0.1
********** reverse layer  12  *********
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5833333333333334  ==>  80 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.90625  ==>  72 / 768
layer  10  :  0.9609375  ==>  30 / 768
layer  11  :  0.9947916666666666  ==>  4 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  13
[1, 1, 3, 2, 1, 1, 2, 3, 2, 0, 1, 0, 5, 0]
***** skip layer  0
[0, 1, 3, 2, 1, 1, 2, 3, 2, 0, 1, 0, 5, 0]
***** skip layer  1
[0, 0, 3, 2, 1, 1, 2, 3, 2, 0, 1, 0, 5, 0]
***** skip layer  2
[0, 0, 2, 2, 1, 1, 2, 3, 2, 0, 1, 0, 5, 0]
***** skip layer  3
[0, 0, 2, 1, 1, 1, 2, 3, 2, 0, 1, 0, 5, 0]
***** skip layer  4
[0, 0, 2, 1, 0, 1, 2, 3, 2, 0, 1, 0, 5, 0]
***** skip layer  5
[0, 0, 2, 1, 0, 0, 2, 3, 2, 0, 1, 0, 5, 0]
***** skip layer  6
[0, 0, 2, 1, 0, 0, 1, 3, 2, 0, 1, 0, 5, 0]
***** skip layer  7
[0, 0, 2, 1, 0, 0, 1, 2, 2, 0, 1, 0, 5, 0]
***** skip layer  8
[0, 0, 2, 1, 0, 0, 1, 2, 1, 0, 1, 0, 5, 0]
optimize layer  9
backward train epoch:  216
test acc:  0.0956
forward train acc:  0.99928  and loss:  3.804255824536085
test acc:  0.918
forward train acc:  0.99898  and loss:  2.59489805274643
test acc:  0.9163
forward train acc:  0.99916  and loss:  1.9806972556980327
test acc:  0.9174
forward train acc:  0.99928  and loss:  1.6679187284316868
test acc:  0.9182
forward train acc:  0.99938  and loss:  1.387265394674614
test acc:  0.9176
forward train acc:  0.99948  and loss:  1.2405593457515351
test acc:  0.9186
forward train acc:  0.99948  and loss:  1.1240646669757552
test acc:  0.9171
forward train acc:  0.99946  and loss:  1.12686818651855
test acc:  0.9171
forward train acc:  0.99956  and loss:  1.01481485174736
test acc:  0.9175
forward train acc:  0.99926  and loss:  1.3299422418349423
test acc:  0.9158
********** reverse layer  9  *********
layer  0  :  0.34375  ==>  63 / 96
layer  1  :  0.65625  ==>  33 / 96
layer  2  :  0.59375  ==>  78 / 192
layer  3  :  0.5833333333333334  ==>  80 / 192
layer  4  :  0.65625  ==>  132 / 384
layer  5  :  0.5729166666666666  ==>  164 / 384
layer  6  :  0.7291666666666666  ==>  104 / 384
layer  7  :  0.96875  ==>  24 / 768
layer  8  :  0.9791666666666666  ==>  16 / 768
layer  9  :  0.90625  ==>  72 / 768
layer  10  :  0.9609375  ==>  30 / 768
layer  11  :  0.9947916666666666  ==>  4 / 768
layer  12  :  0.90625  ==>  72 / 768
layer  13  :  0.7083333333333334  ==>  224 / 768
***** skip layer  10
[0, 0, 2, 1, 0, 0, 1, 2, 1, 5, 0, 0, 5, 0]
optimize layer  11
backward train epoch:  270
Traceback (most recent call last):
  File "v4_vgg16_15.py", line 154, in <module>
    train_back(epoch)
  File "v4_vgg16_15.py", line 92, in train_back
    net.apply(tailer)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 232, in apply
    fn(self)
  File "/home/xingyu/spnn/Util.py", line 39, in __call__
    w[torch.topk(torch.abs(w),self.nclip_list[i],largest=False)[1]] = 0 # make abs(w) smallest k values to be zero
RuntimeError: invalid argument 5: k not in range for dimension at /pytorch/aten/src/THC/generic/THCTensorTopK.cu:21
