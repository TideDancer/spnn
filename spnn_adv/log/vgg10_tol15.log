Files already downloaded and verified
Files already downloaded and verified
CONV_Mask(
  (net): VGG(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace)
      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (9): ReLU(inplace)
      (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (12): ReLU(inplace)
      (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (16): ReLU(inplace)
      (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (19): ReLU(inplace)
      (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (22): ReLU(inplace)
      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (26): ReLU(inplace)
      (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (29): ReLU(inplace)
      (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (32): ReLU(inplace)
      (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (36): ReLU(inplace)
      (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (39): ReLU(inplace)
      (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (42): ReLU(inplace)
      (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (44): AvgPool2d(kernel_size=1, stride=1, padding=0)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=512, bias=True)
      (1): ReLU(inplace)
      (2): Linear(in_features=512, out_features=10, bias=True)
    )
  )
  (mask): ParameterList(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 128 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 128 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
      (5): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
      (6): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
      (7): Parameter containing: [torch.cuda.FloatTensor of size 512 (GPU 0)]
      (8): Parameter containing: [torch.cuda.FloatTensor of size 512 (GPU 0)]
      (9): Parameter containing: [torch.cuda.FloatTensor of size 512 (GPU 0)]
      (10): Parameter containing: [torch.cuda.FloatTensor of size 512 (GPU 0)]
      (11): Parameter containing: [torch.cuda.FloatTensor of size 512 (GPU 0)]
      (12): Parameter containing: [torch.cuda.FloatTensor of size 512 (GPU 0)]
      (13): Parameter containing: [torch.cuda.FloatTensor of size 512 (GPU 0)]
    
  )
)
$$$$$$$$$$$$$ epoch  0  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -0.033744567292160355 , diff:  0.033744567292160355
adv train loss:  -0.03807437467912678 , diff:  0.004329807386966422
layer  0  adv train finish, try to retain  32
test acc: top1 ->  91.85 ; top5 ->  99.16  and loss:  74.35130500793457
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.037896369392910856
test acc: top1 ->  91.8 ; top5 ->  99.14  and loss:  76.0182154327631
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.03374630964026437
test acc: top1 ->  91.83 ; top5 ->  99.12  and loss:  77.14006097614765
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.04549112549284473
test acc: top1 ->  91.81 ; top5 ->  99.05  and loss:  77.45304940640926
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.045117478439351544
test acc: top1 ->  91.82 ; top5 ->  99.13  and loss:  76.86446277797222
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  64 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -0.03401520043735218 , diff:  0.03401520043735218
adv train loss:  -0.03676205906958785 , diff:  0.0027468586322356714
layer  1  adv train finish, try to retain  55
test acc: top1 ->  91.78 ; top5 ->  99.12  and loss:  76.63763104379177
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.04772907917504199
test acc: top1 ->  91.71 ; top5 ->  99.13  and loss:  77.95632779598236
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.03928130785971007
test acc: top1 ->  91.69 ; top5 ->  99.06  and loss:  79.2988051623106
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.04664178327220725
test acc: top1 ->  91.74 ; top5 ->  99.04  and loss:  79.76384036242962
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.024688072473509237
test acc: top1 ->  91.85 ; top5 ->  99.09  and loss:  79.04456698894501
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.042848405515542254
test acc: top1 ->  91.87 ; top5 ->  99.1  and loss:  78.45919387042522
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.03794750804263458
test acc: top1 ->  91.86 ; top5 ->  99.08  and loss:  78.67925943434238
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.02932314031113492
test acc: top1 ->  91.91 ; top5 ->  99.08  and loss:  79.4127082079649
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.06316171311118524
test acc: top1 ->  91.92 ; top5 ->  99.12  and loss:  79.52136974036694
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.026173643294896465
test acc: top1 ->  91.89 ; top5 ->  99.09  and loss:  78.66248665750027
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.03290004840528127
test acc: top1 ->  91.83 ; top5 ->  99.09  and loss:  79.50278143584728
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  64 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -0.048945493006613106 , diff:  0.048945493006613106
adv train loss:  -0.045731656779480545 , diff:  0.0032138362271325605
layer  2  adv train finish, try to retain  100
test acc: top1 ->  91.84 ; top5 ->  99.11  and loss:  78.25842466950417
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.03797561809915351
test acc: top1 ->  91.79 ; top5 ->  99.14  and loss:  80.79527635872364
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.031455507705686614
test acc: top1 ->  91.88 ; top5 ->  99.14  and loss:  81.15692695975304
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.055729047008753696
test acc: top1 ->  91.79 ; top5 ->  99.1  and loss:  81.21083752810955
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.021484306127831587
test acc: top1 ->  91.82 ; top5 ->  99.06  and loss:  82.60078436136246
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.029160186992157833
test acc: top1 ->  91.73 ; top5 ->  99.05  and loss:  83.08650224655867
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.03912265228609613
test acc: top1 ->  91.75 ; top5 ->  99.04  and loss:  83.58593316376209
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03558777100624866
test acc: top1 ->  91.76 ; top5 ->  99.12  and loss:  82.00880309939384
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.03979148878761407
test acc: top1 ->  91.78 ; top5 ->  99.06  and loss:  82.93053536117077
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.04336312816485588
test acc: top1 ->  91.69 ; top5 ->  99.03  and loss:  82.8852352052927
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03080115521515836
test acc: top1 ->  91.77 ; top5 ->  99.1  and loss:  81.54463747888803
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  128 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -0.021899568671642555 , diff:  0.021899568671642555
adv train loss:  -0.042982746665074956 , diff:  0.0210831779934324
adv train loss:  -0.0384365835434437 , diff:  0.004546163121631253
layer  3  adv train finish, try to retain  87
test acc: top1 ->  91.82 ; top5 ->  99.08  and loss:  82.16876977682114
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.04506074428854845
test acc: top1 ->  91.89 ; top5 ->  99.1  and loss:  84.43661394715309
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03987073368307392
test acc: top1 ->  91.83 ; top5 ->  99.12  and loss:  84.36546708643436
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.03267344534833683
test acc: top1 ->  91.85 ; top5 ->  99.09  and loss:  85.66557738929987
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.028526239872007864
test acc: top1 ->  91.9 ; top5 ->  99.12  and loss:  85.4981365352869
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.048437080517715
test acc: top1 ->  91.87 ; top5 ->  99.1  and loss:  84.8005122244358
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.03838940070852459
test acc: top1 ->  91.88 ; top5 ->  99.07  and loss:  85.40758480131626
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.03837733357249817
test acc: top1 ->  91.86 ; top5 ->  99.07  and loss:  85.80098367482424
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  128 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -0.041298107532384165 , diff:  0.041298107532384165
adv train loss:  -0.01850675552213943 , diff:  0.022791352010244736
adv train loss:  -0.024631858756038127 , diff:  0.006125103233898699
layer  4  adv train finish, try to retain  154
test acc: top1 ->  91.88 ; top5 ->  98.94  and loss:  87.44306372851133
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.05796374849524
test acc: top1 ->  91.93 ; top5 ->  99.06  and loss:  87.75234226882458
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.03976138964208076
test acc: top1 ->  91.86 ; top5 ->  99.07  and loss:  86.8677595704794
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.07487966300482185
test acc: top1 ->  91.84 ; top5 ->  99.09  and loss:  86.32736891508102
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.049297473564365646
test acc: top1 ->  91.73 ; top5 ->  99.1  and loss:  86.29781410098076
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03540801621329592
test acc: top1 ->  91.73 ; top5 ->  99.09  and loss:  87.5089245736599
forward train acc: top1 ->  99.98399997558593 ; top5 ->  100.0  and loss:  0.05551316280616447
test acc: top1 ->  91.75 ; top5 ->  99.14  and loss:  86.50407399237156
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.04275711094305734
test acc: top1 ->  91.77 ; top5 ->  99.12  and loss:  86.01771801710129
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03388374928726989
test acc: top1 ->  91.74 ; top5 ->  99.08  and loss:  86.84963920712471
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.050199099375333844
test acc: top1 ->  91.76 ; top5 ->  99.09  and loss:  86.9509683996439
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03519065813816269
test acc: top1 ->  91.71 ; top5 ->  99.09  and loss:  86.97031995654106
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -0.027370605756459554 , diff:  0.027370605756459554
adv train loss:  -0.036757835346747925 , diff:  0.00938722959028837
layer  5  adv train finish, try to retain  149
test acc: top1 ->  91.86 ; top5 ->  99.11  and loss:  86.26708161830902
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.02131353421827953
test acc: top1 ->  91.79 ; top5 ->  99.1  and loss:  87.43401855230331
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.02835694070381578
test acc: top1 ->  91.79 ; top5 ->  99.02  and loss:  87.75631506741047
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.04091502145638515
test acc: top1 ->  91.74 ; top5 ->  99.11  and loss:  86.36659844219685
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.02689183645270532
test acc: top1 ->  91.86 ; top5 ->  99.06  and loss:  86.83963686227798
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.04492087744984019
test acc: top1 ->  91.9 ; top5 ->  99.07  and loss:  87.02361635118723
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.031200749374306724
test acc: top1 ->  91.87 ; top5 ->  99.04  and loss:  86.95714569091797
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.009690078150015324
test acc: top1 ->  91.89 ; top5 ->  99.0  and loss:  87.42110911011696
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.015589697559335036
test acc: top1 ->  91.86 ; top5 ->  99.01  and loss:  87.39188715815544
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.030084719366641366
test acc: top1 ->  91.92 ; top5 ->  99.09  and loss:  86.13262487947941
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.03976339582550281
test acc: top1 ->  91.89 ; top5 ->  99.03  and loss:  87.69355078041553
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -0.022986529453191906 , diff:  0.022986529453191906
adv train loss:  -0.031031978816827177 , diff:  0.00804544936363527
************ all values are small in this layer **********
layer  6  adv train finish, try to retain  255
test acc: top1 ->  91.89 ; top5 ->  99.04  and loss:  87.63851198554039
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.01948597302231292
test acc: top1 ->  91.96 ; top5 ->  99.06  and loss:  87.98410749435425
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.042331026088504586
test acc: top1 ->  91.91 ; top5 ->  99.05  and loss:  87.84303867816925
forward train acc: top1 ->  99.97999997558594 ; top5 ->  100.0  and loss:  0.05822181029361673
test acc: top1 ->  91.86 ; top5 ->  99.08  and loss:  88.0397427380085
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.03418811059600557
test acc: top1 ->  92.0 ; top5 ->  99.05  and loss:  88.81794279813766
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.04377869126619771
test acc: top1 ->  91.96 ; top5 ->  99.07  and loss:  87.09478928148746
forward train acc: top1 ->  99.98199997558594 ; top5 ->  100.0  and loss:  0.04045299356948817
test acc: top1 ->  92.02 ; top5 ->  99.13  and loss:  86.73534315824509
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.04003764169829083
test acc: top1 ->  91.9 ; top5 ->  99.06  and loss:  86.58753116428852
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -0.020766598006048298 , diff:  0.020766598006048298
adv train loss:  -0.013985082383442204 , diff:  0.0067815156226060935
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  511
test acc: top1 ->  91.97 ; top5 ->  99.08  and loss:  86.84866999089718
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.02226945104303013
test acc: top1 ->  91.92 ; top5 ->  99.09  and loss:  87.6722943931818
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.022194470555405132
test acc: top1 ->  91.94 ; top5 ->  99.06  and loss:  88.1503385156393
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -0.04242653814435471 , diff:  0.04242653814435471
adv train loss:  -0.032712141134652484 , diff:  0.009714397009702225
layer  8  adv train finish, try to retain  2
test acc: top1 ->  10.15 ; top5 ->  57.92  and loss:  1054.62922000885
forward train acc: top1 ->  30.869999996337892 ; top5 ->  84.08400000488281  and loss:  821.711371421814
test acc: top1 ->  37.44 ; top5 ->  88.58  and loss:  485.3920862674713
forward train acc: top1 ->  45.54199998779297 ; top5 ->  97.05399998046875  and loss:  225.37638986110687
test acc: top1 ->  53.68 ; top5 ->  96.46  and loss:  186.65614545345306
forward train acc: top1 ->  59.262000017089846 ; top5 ->  99.21199997558594  and loss:  101.03766644001007
test acc: top1 ->  61.78 ; top5 ->  96.91  and loss:  151.61960434913635
forward train acc: top1 ->  66.14200001220703 ; top5 ->  99.55  and loss:  80.8342456817627
test acc: top1 ->  64.81 ; top5 ->  96.92  and loss:  141.66819661855698
forward train acc: top1 ->  70.08999997314453 ; top5 ->  99.68199997558594  and loss:  71.45482403039932
test acc: top1 ->  68.43 ; top5 ->  97.03  and loss:  134.95689976215363
forward train acc: top1 ->  72.87200000732422 ; top5 ->  99.768  and loss:  64.76537257432938
test acc: top1 ->  68.59 ; top5 ->  97.14  and loss:  130.55271023511887
forward train acc: top1 ->  75.3920000024414 ; top5 ->  99.778  and loss:  60.767232835292816
test acc: top1 ->  70.52 ; top5 ->  97.18  and loss:  127.23985415697098
forward train acc: top1 ->  77.22199999511719 ; top5 ->  99.86  and loss:  56.589484095573425
test acc: top1 ->  72.64 ; top5 ->  97.2  and loss:  122.78099972009659
forward train acc: top1 ->  79.20799999023437 ; top5 ->  99.842  and loss:  52.71255147457123
test acc: top1 ->  74.22 ; top5 ->  97.23  and loss:  118.59279364347458
forward train acc: top1 ->  80.73599997802734 ; top5 ->  99.878  and loss:  48.57384940981865
test acc: top1 ->  75.24 ; top5 ->  97.35  and loss:  115.14264577627182
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -7.718641798943281 , diff:  7.718641798943281
adv train loss:  -7.3882616981863976 , diff:  0.3303801007568836
adv train loss:  -7.725928799947724 , diff:  0.3376671017613262
adv train loss:  -7.282225666567683 , diff:  0.4437031333800405
adv train loss:  -7.032710618339479 , diff:  0.24951504822820425
adv train loss:  -7.503808315843344 , diff:  0.47109769750386477
adv train loss:  -7.402667559683323 , diff:  0.10114075616002083
adv train loss:  -7.186394324526191 , diff:  0.21627323515713215
adv train loss:  -7.432810842059553 , diff:  0.2464165175333619
adv train loss:  -7.3139846650883555 , diff:  0.11882617697119713
layer  9  adv train finish, try to retain  32
test acc: top1 ->  34.7 ; top5 ->  87.27  and loss:  653.8478407859802
forward train acc: top1 ->  99.486 ; top5 ->  99.998  and loss:  1.8770280196331441
test acc: top1 ->  91.11 ; top5 ->  99.07  and loss:  86.84041588008404
forward train acc: top1 ->  99.87999997558593 ; top5 ->  100.0  and loss:  0.31057174105080776
test acc: top1 ->  91.37 ; top5 ->  99.05  and loss:  84.35680025070906
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.16026194036385277
test acc: top1 ->  91.33 ; top5 ->  99.11  and loss:  83.41965277493
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.1488186775968643
test acc: top1 ->  91.32 ; top5 ->  99.13  and loss:  83.07376302033663
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.16716766797981109
test acc: top1 ->  91.43 ; top5 ->  99.09  and loss:  83.93651304394007
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.1227110801373783
test acc: top1 ->  91.41 ; top5 ->  99.15  and loss:  82.42678397893906
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.051140253264748026
test acc: top1 ->  91.46 ; top5 ->  99.13  and loss:  83.01199744641781
forward train acc: top1 ->  99.9720000024414 ; top5 ->  100.0  and loss:  0.10008276464941446
test acc: top1 ->  91.44 ; top5 ->  99.12  and loss:  82.16113650798798
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.05248563750501489
test acc: top1 ->  91.39 ; top5 ->  99.13  and loss:  82.08146642893553
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.09704513636097545
test acc: top1 ->  91.49 ; top5 ->  99.13  and loss:  81.75465579330921
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -0.09196300015719316 , diff:  0.09196300015719316
adv train loss:  -0.06146875932245166 , diff:  0.030494240834741504
adv train loss:  -0.12507364768680418 , diff:  0.06360488836435252
adv train loss:  -0.07881667804940662 , diff:  0.04625696963739756
adv train loss:  -0.08822347019668086 , diff:  0.009406792147274246
layer  10  adv train finish, try to retain  11
test acc: top1 ->  11.4 ; top5 ->  70.32  and loss:  755.9946136474609
forward train acc: top1 ->  94.66200000488281 ; top5 ->  98.368  and loss:  38.7045564558357
test acc: top1 ->  90.17 ; top5 ->  98.81  and loss:  81.1495951116085
forward train acc: top1 ->  99.668 ; top5 ->  99.996  and loss:  1.183139833738096
test acc: top1 ->  90.87 ; top5 ->  98.89  and loss:  75.31762252748013
forward train acc: top1 ->  99.804 ; top5 ->  100.0  and loss:  0.6510469170752913
test acc: top1 ->  91.06 ; top5 ->  98.97  and loss:  73.84187088906765
forward train acc: top1 ->  99.864 ; top5 ->  100.0  and loss:  0.4358716439746786
test acc: top1 ->  91.19 ; top5 ->  98.97  and loss:  72.74140565097332
forward train acc: top1 ->  99.928 ; top5 ->  100.0  and loss:  0.25805366574786603
test acc: top1 ->  91.34 ; top5 ->  98.95  and loss:  73.0109991133213
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.24390217744803522
test acc: top1 ->  91.32 ; top5 ->  98.98  and loss:  72.74247401952744
forward train acc: top1 ->  99.92199997558593 ; top5 ->  100.0  and loss:  0.2521911013463978
test acc: top1 ->  91.39 ; top5 ->  98.98  and loss:  72.91373881697655
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.13758604468603153
test acc: top1 ->  91.41 ; top5 ->  98.97  and loss:  73.45823147892952
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.17705691937590018
test acc: top1 ->  91.37 ; top5 ->  99.0  and loss:  72.98544900119305
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.18076576697058044
test acc: top1 ->  91.43 ; top5 ->  98.98  and loss:  73.4380287528038
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -1.7610916000558063 , diff:  1.7610916000558063
adv train loss:  -1.626572282082634 , diff:  0.1345193179731723
adv train loss:  -1.9901376350753708 , diff:  0.36356535299273673
adv train loss:  -1.7208794293983374 , diff:  0.2692582056770334
adv train loss:  -1.7392428515013307 , diff:  0.018363422102993354
adv train loss:  -1.8290827158634784 , diff:  0.08983986436214764
adv train loss:  -1.9577063953620382 , diff:  0.1286236794985598
adv train loss:  -1.8565750551060773 , diff:  0.10113134025596082
adv train loss:  -1.8781637870706618 , diff:  0.02158873196458444
adv train loss:  -1.9278343496844172 , diff:  0.049670562613755465
layer  11  adv train finish, try to retain  42
test acc: top1 ->  64.3 ; top5 ->  96.28  and loss:  155.24477195739746
forward train acc: top1 ->  99.042 ; top5 ->  100.0  and loss:  3.268057180393953
test acc: top1 ->  90.88 ; top5 ->  98.78  and loss:  89.12906622886658
forward train acc: top1 ->  99.87 ; top5 ->  100.0  and loss:  0.3999565423146123
test acc: top1 ->  91.07 ; top5 ->  98.9  and loss:  85.31088940799236
forward train acc: top1 ->  99.918 ; top5 ->  100.0  and loss:  0.2211072150967084
test acc: top1 ->  91.24 ; top5 ->  98.98  and loss:  82.90109260380268
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.16643199232930783
test acc: top1 ->  91.35 ; top5 ->  99.02  and loss:  82.34501264989376
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.13757509681454394
test acc: top1 ->  91.45 ; top5 ->  99.0  and loss:  81.82507158815861
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.12564706471312093
test acc: top1 ->  91.5 ; top5 ->  98.99  and loss:  81.81039084494114
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.11627170097199269
test acc: top1 ->  91.6 ; top5 ->  98.95  and loss:  80.77795322239399
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.09528584867075551
test acc: top1 ->  91.59 ; top5 ->  98.97  and loss:  80.58010749518871
forward train acc: top1 ->  99.97399997558594 ; top5 ->  100.0  and loss:  0.09524275222793221
test acc: top1 ->  91.66 ; top5 ->  98.98  and loss:  80.88940542936325
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -1.6305497160647064 , diff:  1.6305497160647064
adv train loss:  -1.6719863041653298 , diff:  0.04143658810062334
adv train loss:  -1.499421360262204 , diff:  0.17256494390312582
adv train loss:  -1.506549763478688 , diff:  0.007128403216484003
layer  12  adv train finish, try to retain  43
test acc: top1 ->  83.09 ; top5 ->  98.67  and loss:  111.03491789102554
forward train acc: top1 ->  98.744 ; top5 ->  100.0  and loss:  3.8338206838816404
test acc: top1 ->  91.07 ; top5 ->  99.07  and loss:  60.65468391776085
forward train acc: top1 ->  99.90800000244141 ; top5 ->  100.0  and loss:  0.3735782441799529
test acc: top1 ->  91.35 ; top5 ->  99.13  and loss:  59.12279084324837
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.20376594009576365
test acc: top1 ->  91.62 ; top5 ->  99.17  and loss:  59.34082838892937
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.14653385392739438
test acc: top1 ->  91.65 ; top5 ->  99.19  and loss:  59.74593560397625
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.11766865404206328
test acc: top1 ->  91.76 ; top5 ->  99.17  and loss:  60.47971764206886
forward train acc: top1 ->  99.95999997558594 ; top5 ->  100.0  and loss:  0.12961948901647702
test acc: top1 ->  91.68 ; top5 ->  99.16  and loss:  61.030144453048706
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.08545328480249736
test acc: top1 ->  91.69 ; top5 ->  99.16  and loss:  61.34825213253498
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.08611868944717571
test acc: top1 ->  91.71 ; top5 ->  99.17  and loss:  62.33532449603081
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -0.16183848668879364 , diff:  0.16183848668879364
adv train loss:  -0.15458043136823107 , diff:  0.007258055320562562
layer  13  adv train finish, try to retain  56
test acc: top1 ->  90.65 ; top5 ->  98.99  and loss:  98.27432358264923
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.21902288373712508
test acc: top1 ->  91.66 ; top5 ->  99.0  and loss:  87.27817796170712
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.06077539035322843
test acc: top1 ->  91.81 ; top5 ->  99.0  and loss:  86.5654476583004
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.0754985958810721
test acc: top1 ->  91.78 ; top5 ->  99.06  and loss:  87.02356076985598
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.04433931025778293
test acc: top1 ->  91.83 ; top5 ->  99.05  and loss:  87.05441910028458
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.029870619227949646
test acc: top1 ->  91.96 ; top5 ->  99.04  and loss:  86.02813510596752
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.042723931230284506
test acc: top1 ->  91.99 ; top5 ->  99.06  and loss:  85.89749027788639
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.0743671675445512
test acc: top1 ->  91.95 ; top5 ->  99.05  and loss:  83.98804846405983
forward train acc: top1 ->  99.98199997558594 ; top5 ->  100.0  and loss:  0.05484117572996183
test acc: top1 ->  91.87 ; top5 ->  99.06  and loss:  84.0494440048933
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.024789740084088407
test acc: top1 ->  91.94 ; top5 ->  99.03  and loss:  84.44690871238708
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.03415214054257376
test acc: top1 ->  91.93 ; top5 ->  99.06  and loss:  85.04306015372276
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
layer  0  :  0  ==>  64 / 64 , inc:  1
layer  1  :  0  ==>  64 / 64 , inc:  1
layer  2  :  0  ==>  128 / 128 , inc:  1
layer  3  :  0  ==>  128 / 128 , inc:  1
layer  4  :  0  ==>  256 / 256 , inc:  1
layer  5  :  0  ==>  256 / 256 , inc:  1
layer  6  :  0  ==>  256 / 256 , inc:  1
layer  7  :  0  ==>  512 / 512 , inc:  1
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0  ==>  512 / 512 , inc:  1
layer  11  :  0  ==>  512 / 512 , inc:  1
layer  12  :  0  ==>  512 / 512 , inc:  1
layer  13  :  0  ==>  512 / 512 , inc:  1
eps [0.00075, 0.00075, 0.00075, 0.00075, 0.00075, 0.00075, 0.00075, 0.00075, 0.00075, 0.00075, 0.00075, 0.00075, 0.00075, 0.00075]  wait [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 0
$$$$$$$$$$$$$ epoch  1  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -0.09487102418734139 , diff:  0.09487102418734139
adv train loss:  -0.1586564115859801 , diff:  0.0637853873986387
adv train loss:  -0.07491023617694736 , diff:  0.08374617540903273
adv train loss:  -0.06935316873114061 , diff:  0.0055570674458067515
layer  0  adv train finish, try to retain  32
test acc: top1 ->  91.66 ; top5 ->  98.88  and loss:  102.5107773244381
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03232945005447618
test acc: top1 ->  91.8 ; top5 ->  99.1  and loss:  100.42059859633446
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03008292199956486
test acc: top1 ->  91.89 ; top5 ->  99.06  and loss:  99.45968949794769
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.036860840822555474
test acc: top1 ->  91.82 ; top5 ->  99.05  and loss:  99.7089083045721
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.039677684903210775
test acc: top1 ->  91.86 ; top5 ->  99.05  and loss:  97.96272076666355
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.021447543213525933
test acc: top1 ->  91.82 ; top5 ->  99.08  and loss:  97.5002444088459
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.027263235051577794
test acc: top1 ->  91.72 ; top5 ->  99.13  and loss:  97.45346716046333
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.020873170889615267
test acc: top1 ->  91.84 ; top5 ->  99.08  and loss:  97.22351728379726
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.032451927467263886
test acc: top1 ->  91.9 ; top5 ->  99.07  and loss:  96.94615459442139
forward train acc: top1 ->  99.99199997558594 ; top5 ->  100.0  and loss:  0.016084303686511703
test acc: top1 ->  91.92 ; top5 ->  99.09  and loss:  97.27135741710663
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.022631556775650097
test acc: top1 ->  91.86 ; top5 ->  99.06  and loss:  97.29702262580395
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  64 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -0.05743963591794454 , diff:  0.05743963591794454
adv train loss:  -0.027209507843508618 , diff:  0.030230128074435925
adv train loss:  -0.020531440292415937 , diff:  0.006678067551092681
layer  1  adv train finish, try to retain  55
test acc: top1 ->  91.84 ; top5 ->  99.07  and loss:  96.99578049778938
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.014076898606390387
test acc: top1 ->  91.74 ; top5 ->  99.06  and loss:  99.45404851436615
forward train acc: top1 ->  99.99199997558594 ; top5 ->  100.0  and loss:  0.04162480282684555
test acc: top1 ->  91.85 ; top5 ->  99.03  and loss:  99.96858201920986
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.027909341140912147
test acc: top1 ->  91.9 ; top5 ->  99.04  and loss:  101.73790200054646
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.031366801717013004
test acc: top1 ->  91.83 ; top5 ->  99.09  and loss:  98.78098459541798
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.06789006242769346
test acc: top1 ->  91.91 ; top5 ->  99.09  and loss:  98.46958427131176
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.03603051141544711
test acc: top1 ->  91.83 ; top5 ->  99.06  and loss:  97.92012506723404
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.041982770097092725
test acc: top1 ->  91.94 ; top5 ->  99.05  and loss:  97.56691718101501
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.027202738013954786
test acc: top1 ->  91.95 ; top5 ->  99.1  and loss:  97.82542514801025
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.04139965953072533
test acc: top1 ->  91.93 ; top5 ->  99.11  and loss:  96.99572429060936
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.025560441468542194
test acc: top1 ->  91.79 ; top5 ->  99.11  and loss:  97.52137045562267
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  64 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -0.02618598996195942 , diff:  0.02618598996195942
adv train loss:  -0.026908617950539337 , diff:  0.0007226279885799158
layer  2  adv train finish, try to retain  100
test acc: top1 ->  91.79 ; top5 ->  99.11  and loss:  97.31846217811108
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.016496242153152707
test acc: top1 ->  91.86 ; top5 ->  99.09  and loss:  99.31462617218494
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.02260632309889843
test acc: top1 ->  91.76 ; top5 ->  99.12  and loss:  99.7594079375267
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.018841661499209295
test acc: top1 ->  91.79 ; top5 ->  99.1  and loss:  100.77234515547752
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.04148155635994044
test acc: top1 ->  91.83 ; top5 ->  99.11  and loss:  101.24187061190605
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.021812638588016853
test acc: top1 ->  91.82 ; top5 ->  99.05  and loss:  99.61904826760292
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.08064072043634951
test acc: top1 ->  91.88 ; top5 ->  99.05  and loss:  97.22939936816692
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.03225488850148395
test acc: top1 ->  91.93 ; top5 ->  99.11  and loss:  96.79175174236298
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.026553994068308384
test acc: top1 ->  92.01 ; top5 ->  99.17  and loss:  96.81157532334328
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.02963526170606201
test acc: top1 ->  91.88 ; top5 ->  99.11  and loss:  95.81866329908371
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.020274127975653755
test acc: top1 ->  91.9 ; top5 ->  99.14  and loss:  97.1010770201683
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  128 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -0.021773757605842547 , diff:  0.021773757605842547
adv train loss:  -0.02814620316348737 , diff:  0.0063724455576448236
layer  3  adv train finish, try to retain  86
test acc: top1 ->  91.79 ; top5 ->  99.11  and loss:  96.63110149651766
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.07430332087096758
test acc: top1 ->  91.71 ; top5 ->  99.13  and loss:  99.30370630323887
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.03008851980757754
test acc: top1 ->  91.64 ; top5 ->  99.1  and loss:  100.03795705735683
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.03352826841000933
test acc: top1 ->  91.88 ; top5 ->  99.11  and loss:  99.94930963218212
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.033748010931503813
test acc: top1 ->  91.8 ; top5 ->  99.1  and loss:  97.90455412864685
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  128 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -0.035627313518489245 , diff:  0.035627313518489245
adv train loss:  -0.03397541306912899 , diff:  0.0016519004493602552
layer  4  adv train finish, try to retain  154
test acc: top1 ->  91.77 ; top5 ->  99.06  and loss:  99.89311516284943
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.03150949563132599
test acc: top1 ->  91.66 ; top5 ->  99.11  and loss:  100.62309071421623
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.013396974434726872
test acc: top1 ->  91.75 ; top5 ->  99.03  and loss:  99.93778922408819
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.0471971953229513
test acc: top1 ->  91.69 ; top5 ->  99.05  and loss:  99.59275119751692
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.05019187743891962
test acc: top1 ->  91.84 ; top5 ->  99.05  and loss:  98.34798163175583
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.055182737471113796
test acc: top1 ->  91.79 ; top5 ->  99.05  and loss:  99.19882971048355
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.046109105547657236
test acc: top1 ->  91.83 ; top5 ->  99.06  and loss:  98.8129613995552
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.060392140360363555
test acc: top1 ->  91.66 ; top5 ->  99.07  and loss:  97.81306867301464
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.020886196281935554
test acc: top1 ->  91.71 ; top5 ->  99.07  and loss:  98.22842179238796
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.05281769208522746
test acc: top1 ->  91.74 ; top5 ->  99.11  and loss:  97.69214551150799
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.0295339778313064
test acc: top1 ->  91.82 ; top5 ->  99.05  and loss:  97.21136665344238
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -0.027014866926947434 , diff:  0.027014866926947434
adv train loss:  -0.020574475181092566 , diff:  0.006440391745854868
layer  5  adv train finish, try to retain  149
test acc: top1 ->  91.98 ; top5 ->  99.08  and loss:  97.10798282921314
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.026604495629726443
test acc: top1 ->  91.94 ; top5 ->  99.09  and loss:  98.6268647313118
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.020168276940239593
test acc: top1 ->  91.95 ; top5 ->  99.1  and loss:  100.00798399746418
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.026559992900729412
test acc: top1 ->  91.91 ; top5 ->  99.08  and loss:  98.31128856539726
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.017556411015902995
test acc: top1 ->  91.94 ; top5 ->  99.08  and loss:  100.56137463450432
forward train acc: top1 ->  99.9800000024414 ; top5 ->  100.0  and loss:  0.061117419669244555
test acc: top1 ->  91.93 ; top5 ->  99.13  and loss:  101.32201547920704
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.030991236650152132
test acc: top1 ->  92.04 ; top5 ->  99.07  and loss:  102.17957165837288
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03470800980358035
test acc: top1 ->  91.95 ; top5 ->  99.07  and loss:  100.24779090285301
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.010687145675547072
test acc: top1 ->  91.9 ; top5 ->  99.1  and loss:  100.3348163664341
forward train acc: top1 ->  99.98599997558594 ; top5 ->  100.0  and loss:  0.028880765356007032
test acc: top1 ->  91.9 ; top5 ->  99.13  and loss:  100.03047388792038
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.015122906224860344
test acc: top1 ->  91.67 ; top5 ->  99.11  and loss:  99.84141778945923
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  6  ---------------
layer  6  adv train finish, try to retain  144
test acc: top1 ->  91.72 ; top5 ->  99.11  and loss:  99.99757535755634
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.04171917473041731
test acc: top1 ->  91.73 ; top5 ->  99.1  and loss:  101.96449840068817
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.030400526327866828
test acc: top1 ->  91.83 ; top5 ->  99.04  and loss:  101.9913607686758
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.013737153065790153
test acc: top1 ->  91.86 ; top5 ->  99.11  and loss:  100.45877979695797
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.015829913058269085
test acc: top1 ->  91.96 ; top5 ->  99.1  and loss:  100.14731833338737
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.02962597912119236
test acc: top1 ->  91.87 ; top5 ->  99.09  and loss:  100.2303778231144
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.024133639742757396
test acc: top1 ->  91.85 ; top5 ->  99.1  and loss:  100.92585597932339
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.048315477288269904
test acc: top1 ->  91.85 ; top5 ->  99.11  and loss:  98.24176901578903
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.01419091136131101
test acc: top1 ->  91.89 ; top5 ->  99.11  and loss:  98.99049152433872
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.01811327743109814
test acc: top1 ->  91.94 ; top5 ->  99.11  and loss:  99.03285147249699
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.013883697862013378
test acc: top1 ->  91.94 ; top5 ->  99.1  and loss:  98.8356283903122
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -0.017721686730510555 , diff:  0.017721686730510555
adv train loss:  -0.015810338386017975 , diff:  0.0019113483444925805
layer  7  adv train finish, try to retain  5
test acc: top1 ->  10.0 ; top5 ->  54.97  and loss:  956.3347988128662
forward train acc: top1 ->  57.90400001464844 ; top5 ->  97.16000001464843  and loss:  365.6374969482422
test acc: top1 ->  65.28 ; top5 ->  96.11  and loss:  276.09935212135315
forward train acc: top1 ->  75.59999999511719 ; top5 ->  98.86400000488281  and loss:  106.87126046419144
test acc: top1 ->  72.5 ; top5 ->  96.85  and loss:  182.39517533779144
forward train acc: top1 ->  81.47799998291016 ; top5 ->  99.23200000732422  and loss:  69.05945286154747
test acc: top1 ->  76.5 ; top5 ->  97.31  and loss:  147.92022264003754
forward train acc: top1 ->  85.35400000244141 ; top5 ->  99.5020000024414  and loss:  51.02878341078758
test acc: top1 ->  79.15 ; top5 ->  97.7  and loss:  125.70560663938522
forward train acc: top1 ->  87.37600001220703 ; top5 ->  99.63399997558594  and loss:  41.40786638855934
test acc: top1 ->  81.01 ; top5 ->  97.92  and loss:  108.52179631590843
forward train acc: top1 ->  89.40999999511719 ; top5 ->  99.67999997558594  and loss:  34.583229422569275
test acc: top1 ->  81.61 ; top5 ->  98.01  and loss:  104.21395534276962
forward train acc: top1 ->  89.87799997314453 ; top5 ->  99.71799997558594  and loss:  31.983300507068634
test acc: top1 ->  82.55 ; top5 ->  97.98  and loss:  98.93132027983665
forward train acc: top1 ->  90.59199998535156 ; top5 ->  99.76800000244141  and loss:  29.720330774784088
test acc: top1 ->  83.14 ; top5 ->  98.01  and loss:  93.06234687566757
forward train acc: top1 ->  91.03199997558593 ; top5 ->  99.778  and loss:  28.26352594792843
test acc: top1 ->  83.52 ; top5 ->  98.21  and loss:  90.20449805259705
forward train acc: top1 ->  91.97799998046875 ; top5 ->  99.75799997558593  and loss:  25.398716866970062
test acc: top1 ->  84.01 ; top5 ->  98.28  and loss:  87.34073945879936
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -18.47330678999424 , diff:  18.47330678999424
adv train loss:  -18.783226370811462 , diff:  0.3099195808172226
adv train loss:  -18.577237710356712 , diff:  0.20598866045475006
adv train loss:  -18.08685616403818 , diff:  0.49038154631853104
adv train loss:  -18.516862466931343 , diff:  0.4300063028931618
adv train loss:  -18.678501091897488 , diff:  0.16163862496614456
adv train loss:  -18.140308566391468 , diff:  0.5381925255060196
adv train loss:  -18.382392317056656 , diff:  0.24208375066518784
adv train loss:  -18.71413418650627 , diff:  0.3317418694496155
adv train loss:  -18.202873654663563 , diff:  0.5112605318427086
layer  8  adv train finish, try to retain  94
test acc: top1 ->  67.72 ; top5 ->  93.97  and loss:  280.7035229206085
forward train acc: top1 ->  99.464 ; top5 ->  99.99  and loss:  1.846557645512803
test acc: top1 ->  91.5 ; top5 ->  99.19  and loss:  76.68793289363384
forward train acc: top1 ->  99.924 ; top5 ->  99.998  and loss:  0.294771056927857
test acc: top1 ->  91.57 ; top5 ->  99.18  and loss:  74.80918318033218
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.1748818129490246
test acc: top1 ->  91.64 ; top5 ->  99.16  and loss:  75.12822590768337
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.14776413537038025
test acc: top1 ->  91.66 ; top5 ->  99.16  and loss:  74.38315708935261
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.127585697977338
test acc: top1 ->  91.72 ; top5 ->  99.16  and loss:  74.63374391198158
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.12322865720489062
test acc: top1 ->  91.78 ; top5 ->  99.19  and loss:  74.96452535688877
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.10323946357675595
test acc: top1 ->  91.77 ; top5 ->  99.15  and loss:  73.78737315535545
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.11074300848849816
test acc: top1 ->  91.78 ; top5 ->  99.19  and loss:  74.62097865343094
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.07744459931564052
test acc: top1 ->  91.78 ; top5 ->  99.2  and loss:  74.1480035930872
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.08963790752750356
test acc: top1 ->  91.73 ; top5 ->  99.18  and loss:  74.15336762368679
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -0.10288360965205356 , diff:  0.10288360965205356
adv train loss:  -0.14148066175403073 , diff:  0.03859705210197717
adv train loss:  -0.13380190257157665 , diff:  0.007678759182454087
layer  9  adv train finish, try to retain  14
test acc: top1 ->  19.77 ; top5 ->  66.8  and loss:  965.7006344795227
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.08945588130154647
test acc: top1 ->  91.58 ; top5 ->  99.18  and loss:  79.56160159409046
forward train acc: top1 ->  99.96799997558594 ; top5 ->  100.0  and loss:  0.10341029091432574
test acc: top1 ->  91.54 ; top5 ->  99.15  and loss:  80.37458960711956
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.08521606107387925
test acc: top1 ->  91.73 ; top5 ->  99.23  and loss:  78.50017692148685
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.0710870243201498
test acc: top1 ->  91.83 ; top5 ->  99.19  and loss:  79.4616537541151
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03416415884566959
test acc: top1 ->  91.76 ; top5 ->  99.16  and loss:  80.17801006138325
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.035748609730944736
test acc: top1 ->  91.69 ; top5 ->  99.13  and loss:  80.69878204166889
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.054669275516062044
test acc: top1 ->  91.82 ; top5 ->  99.18  and loss:  81.4613820463419
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.06965317123103887
test acc: top1 ->  91.8 ; top5 ->  99.2  and loss:  81.38458669185638
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.03476638369465945
test acc: top1 ->  91.85 ; top5 ->  99.2  and loss:  80.86825554072857
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.057950374456595455
test acc: top1 ->  91.8 ; top5 ->  99.14  and loss:  81.43143540620804
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -0.04647960004513152 , diff:  0.04647960004513152
adv train loss:  -0.052598706952267094 , diff:  0.006119106907135574
layer  10  adv train finish, try to retain  2
test acc: top1 ->  10.0 ; top5 ->  50.02  and loss:  1550.0507926940918
forward train acc: top1 ->  78.48799997802735 ; top5 ->  97.61199997558593  and loss:  121.94542776048183
test acc: top1 ->  79.08 ; top5 ->  98.3  and loss:  97.46151393651962
forward train acc: top1 ->  94.95000001953125 ; top5 ->  99.93399997558593  and loss:  17.2566806524992
test acc: top1 ->  85.52 ; top5 ->  98.44  and loss:  79.93469938635826
forward train acc: top1 ->  96.72800000976562 ; top5 ->  99.968  and loss:  11.909611850976944
test acc: top1 ->  86.71 ; top5 ->  98.61  and loss:  76.8424861729145
forward train acc: top1 ->  97.63800001464844 ; top5 ->  99.97999997558594  and loss:  8.615824170410633
test acc: top1 ->  87.12 ; top5 ->  98.63  and loss:  77.5755366384983
forward train acc: top1 ->  98.29400000244141 ; top5 ->  99.978  and loss:  6.495402082800865
test acc: top1 ->  87.62 ; top5 ->  98.69  and loss:  77.61367955803871
forward train acc: top1 ->  98.45600000732422 ; top5 ->  99.98  and loss:  5.612733330577612
test acc: top1 ->  87.94 ; top5 ->  98.7  and loss:  77.0292357802391
forward train acc: top1 ->  98.64399997802734 ; top5 ->  99.986  and loss:  4.984346028417349
test acc: top1 ->  87.94 ; top5 ->  98.73  and loss:  77.91172936558723
forward train acc: top1 ->  98.69399998046875 ; top5 ->  99.996  and loss:  4.597587948665023
test acc: top1 ->  88.16 ; top5 ->  98.72  and loss:  77.75343605875969
forward train acc: top1 ->  98.85400000732422 ; top5 ->  99.98599997558594  and loss:  4.212477488443255
test acc: top1 ->  88.47 ; top5 ->  98.75  and loss:  77.44420462846756
forward train acc: top1 ->  98.94200000732422 ; top5 ->  99.98599997558594  and loss:  3.782822825014591
test acc: top1 ->  88.53 ; top5 ->  98.71  and loss:  77.89811685681343
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -2.631193400826305 , diff:  2.631193400826305
adv train loss:  -2.784721943316981 , diff:  0.15352854249067605
adv train loss:  -2.781647125724703 , diff:  0.003074817592278123
layer  11  adv train finish, try to retain  66
test acc: top1 ->  86.43 ; top5 ->  98.8  and loss:  87.86430974304676
forward train acc: top1 ->  99.652 ; top5 ->  100.0  and loss:  1.0732634320302168
test acc: top1 ->  91.35 ; top5 ->  98.93  and loss:  90.35942889750004
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.11816101116346545
test acc: top1 ->  91.54 ; top5 ->  99.04  and loss:  86.71209979057312
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.08261357618357579
test acc: top1 ->  91.66 ; top5 ->  99.04  and loss:  85.43887268006802
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.06069998905877583
test acc: top1 ->  91.73 ; top5 ->  99.07  and loss:  84.65799604356289
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.06513348127646168
test acc: top1 ->  91.72 ; top5 ->  99.03  and loss:  85.27110709249973
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.03429423352372396
test acc: top1 ->  91.72 ; top5 ->  99.04  and loss:  85.559310272336
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.03578676168945094
test acc: top1 ->  91.83 ; top5 ->  99.04  and loss:  85.3588348031044
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.055102991969761206
test acc: top1 ->  91.72 ; top5 ->  99.12  and loss:  85.72399561107159
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.03432715098369954
test acc: top1 ->  91.8 ; top5 ->  99.14  and loss:  85.16609463095665
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.038133966829263954
test acc: top1 ->  91.83 ; top5 ->  99.11  and loss:  84.96960537135601
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -0.04424882087914739 , diff:  0.04424882087914739
adv train loss:  -0.048192001442657784 , diff:  0.003943180563510396
layer  12  adv train finish, try to retain  23
test acc: top1 ->  52.37 ; top5 ->  89.07  and loss:  276.90187907218933
forward train acc: top1 ->  88.23599998046875 ; top5 ->  99.708  and loss:  43.36339706927538
test acc: top1 ->  89.31 ; top5 ->  99.0  and loss:  51.8171152099967
forward train acc: top1 ->  99.53399997558594 ; top5 ->  100.0  and loss:  2.4788536624982953
test acc: top1 ->  91.06 ; top5 ->  98.93  and loss:  47.30988363176584
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.8059320035390556
test acc: top1 ->  91.45 ; top5 ->  98.93  and loss:  47.964476227760315
forward train acc: top1 ->  99.91599997558593 ; top5 ->  100.0  and loss:  0.5699678580276668
test acc: top1 ->  91.5 ; top5 ->  98.99  and loss:  48.80638737231493
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.4068094870308414
test acc: top1 ->  91.54 ; top5 ->  98.97  and loss:  49.74098303169012
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.29368295310996473
test acc: top1 ->  91.69 ; top5 ->  98.95  and loss:  50.470317251980305
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.24866230500629172
test acc: top1 ->  91.63 ; top5 ->  98.96  and loss:  51.622892588377
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.24091176665388048
test acc: top1 ->  91.64 ; top5 ->  98.95  and loss:  52.324163503944874
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.19404851546278223
test acc: top1 ->  91.75 ; top5 ->  99.0  and loss:  52.58313508331776
forward train acc: top1 ->  99.98199997558594 ; top5 ->  100.0  and loss:  0.1671750049572438
test acc: top1 ->  91.86 ; top5 ->  98.97  and loss:  53.63549456745386
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -0.7968007856688928 , diff:  0.7968007856688928
adv train loss:  -0.8274062126729405 , diff:  0.03060542700404767
adv train loss:  -0.7213367177173495 , diff:  0.10606949495559093
adv train loss:  -0.7977453336552571 , diff:  0.0764086159379076
adv train loss:  -0.7680731392611051 , diff:  0.02967219439415203
adv train loss:  -0.8064580055142869 , diff:  0.038384866253181826
adv train loss:  -0.5362050876719877 , diff:  0.2702529178422992
adv train loss:  -0.7414448733616155 , diff:  0.20523978568962775
adv train loss:  -0.7181662303337362 , diff:  0.023278643027879298
adv train loss:  -0.7195648638544299 , diff:  0.0013986335206936928
layer  13  adv train finish, try to retain  89
test acc: top1 ->  91.1 ; top5 ->  98.03  and loss:  134.2937076985836
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.19406907318762023
test acc: top1 ->  91.57 ; top5 ->  98.89  and loss:  112.42456051707268
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.042900769647985726
test acc: top1 ->  91.81 ; top5 ->  98.88  and loss:  112.1805479824543
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.07642665117782599
test acc: top1 ->  91.86 ; top5 ->  98.97  and loss:  106.02141532301903
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.040259044110143805
test acc: top1 ->  91.99 ; top5 ->  98.99  and loss:  105.05287837982178
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.024583532299402577
test acc: top1 ->  91.91 ; top5 ->  99.0  and loss:  103.81816774606705
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.02866923789588327
test acc: top1 ->  91.84 ; top5 ->  99.01  and loss:  104.93890728056431
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.02560201582582522
test acc: top1 ->  91.96 ; top5 ->  99.08  and loss:  103.10630305111408
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.03170738890548819
test acc: top1 ->  91.85 ; top5 ->  99.09  and loss:  104.4595665037632
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.025832429471847718
test acc: top1 ->  91.83 ; top5 ->  99.03  and loss:  105.54801459610462
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.04945823428033691
test acc: top1 ->  91.91 ; top5 ->  99.02  and loss:  105.20189188420773
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
layer  0  :  0  ==>  64 / 64 , inc:  1
layer  1  :  0  ==>  64 / 64 , inc:  1
layer  2  :  0  ==>  128 / 128 , inc:  1
layer  3  :  0  ==>  128 / 128 , inc:  1
layer  4  :  0  ==>  256 / 256 , inc:  1
layer  5  :  0  ==>  256 / 256 , inc:  1
layer  6  :  0  ==>  256 / 256 , inc:  1
layer  7  :  0  ==>  512 / 512 , inc:  1
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0  ==>  512 / 512 , inc:  1
layer  11  :  0  ==>  512 / 512 , inc:  1
layer  12  :  0  ==>  512 / 512 , inc:  1
layer  13  :  0  ==>  512 / 512 , inc:  1
eps [0.0005625000000000001, 0.0005625000000000001, 0.0005625000000000001, 0.0005625000000000001, 0.0005625000000000001, 0.0005625000000000001, 0.0005625000000000001, 0.0005625000000000001, 0.0005625000000000001, 0.0005625000000000001, 0.0005625000000000001, 0.0005625000000000001, 0.0005625000000000001, 0.0005625000000000001]  wait [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 1
$$$$$$$$$$$$$ epoch  2  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -0.023602597049375618 , diff:  0.023602597049375618
adv train loss:  -0.015569519819450761 , diff:  0.008033077229924857
layer  0  adv train finish, try to retain  32
test acc: top1 ->  91.82 ; top5 ->  99.05  and loss:  105.10197925567627
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.017398196383965114
test acc: top1 ->  91.85 ; top5 ->  99.04  and loss:  107.44980047643185
forward train acc: top1 ->  99.98199997558594 ; top5 ->  100.0  and loss:  0.04910586068581324
test acc: top1 ->  91.94 ; top5 ->  99.04  and loss:  107.28687083721161
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.08602118109638468
test acc: top1 ->  91.99 ; top5 ->  99.07  and loss:  106.62343564629555
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.036384133834872046
test acc: top1 ->  91.88 ; top5 ->  99.0  and loss:  105.60578082501888
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.029275246040924685
test acc: top1 ->  92.05 ; top5 ->  99.0  and loss:  104.3004052489996
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.037029707611509366
test acc: top1 ->  92.14 ; top5 ->  99.07  and loss:  102.61590804159641
==> this epoch:  32 / 64
---------------- start layer  1  ---------------
adv train loss:  -0.050315265682684185 , diff:  0.050315265682684185
adv train loss:  -0.02251638615689444 , diff:  0.027798879525789744
adv train loss:  -0.02106056242541854 , diff:  0.0014558237314759026
layer  1  adv train finish, try to retain  55
test acc: top1 ->  92.1 ; top5 ->  99.1  and loss:  101.41423372924328
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.040008514711416865
test acc: top1 ->  92.16 ; top5 ->  99.15  and loss:  101.71115290373564
==> this epoch:  55 / 64
---------------- start layer  2  ---------------
adv train loss:  -0.04881377210784876 , diff:  0.04881377210784876
adv train loss:  -0.049244359077420086 , diff:  0.0004305869695713227
layer  2  adv train finish, try to retain  100
test acc: top1 ->  92.14 ; top5 ->  99.13  and loss:  103.85053341090679
==> this epoch:  100 / 128
---------------- start layer  3  ---------------
adv train loss:  -0.045640306703717215 , diff:  0.045640306703717215
adv train loss:  -0.03127211719038314 , diff:  0.014368189513334073
adv train loss:  -0.03945102632860653 , diff:  0.00817890913822339
layer  3  adv train finish, try to retain  87
test acc: top1 ->  92.16 ; top5 ->  99.11  and loss:  103.45338007062674
==> this epoch:  87 / 128
---------------- start layer  4  ---------------
adv train loss:  -0.03843027718221492 , diff:  0.03843027718221492
adv train loss:  -0.021952697832858803 , diff:  0.01647757934935612
adv train loss:  -0.021051676800198038 , diff:  0.0009010210326607648
layer  4  adv train finish, try to retain  155
test acc: top1 ->  92.14 ; top5 ->  99.11  and loss:  103.49224816262722
==> this epoch:  155 / 256
---------------- start layer  5  ---------------
adv train loss:  -0.048212119243544294 , diff:  0.048212119243544294
adv train loss:  -0.041403985552278755 , diff:  0.006808133691265539
layer  5  adv train finish, try to retain  149
test acc: top1 ->  92.16 ; top5 ->  99.09  and loss:  103.94041193276644
==> this epoch:  149 / 256
---------------- start layer  6  ---------------
adv train loss:  -0.03634177341155009 , diff:  0.03634177341155009
adv train loss:  -0.042547096814988805 , diff:  0.006205323403438712
layer  6  adv train finish, try to retain  144
test acc: top1 ->  92.14 ; top5 ->  99.11  and loss:  103.51885561645031
==> this epoch:  144 / 256
---------------- start layer  7  ---------------
adv train loss:  -0.03441041267797118 , diff:  0.03441041267797118
adv train loss:  -0.013627167097183701 , diff:  0.02078324558078748
adv train loss:  -0.045298404027562356 , diff:  0.031671236930378655
adv train loss:  -0.03737692627237266 , diff:  0.007921477755189699
layer  7  adv train finish, try to retain  39
test acc: top1 ->  46.43 ; top5 ->  94.61  and loss:  238.26732468605042
forward train acc: top1 ->  99.716 ; top5 ->  99.996  and loss:  1.226383070461452
test acc: top1 ->  91.13 ; top5 ->  98.89  and loss:  97.65481125563383
forward train acc: top1 ->  99.828 ; top5 ->  99.998  and loss:  0.5779993060277775
test acc: top1 ->  91.16 ; top5 ->  99.0  and loss:  93.39191643893719
forward train acc: top1 ->  99.852 ; top5 ->  99.998  and loss:  0.46140773017759784
test acc: top1 ->  91.15 ; top5 ->  99.06  and loss:  91.61968518793583
forward train acc: top1 ->  99.89 ; top5 ->  100.0  and loss:  0.36790536973421695
test acc: top1 ->  91.25 ; top5 ->  99.01  and loss:  90.82018698751926
forward train acc: top1 ->  99.884 ; top5 ->  100.0  and loss:  0.35694400405918714
test acc: top1 ->  91.29 ; top5 ->  99.01  and loss:  88.17767841368914
forward train acc: top1 ->  99.924 ; top5 ->  100.0  and loss:  0.2012068806689058
test acc: top1 ->  91.28 ; top5 ->  99.05  and loss:  88.30659780651331
forward train acc: top1 ->  99.892 ; top5 ->  100.0  and loss:  0.2845736737290281
test acc: top1 ->  91.32 ; top5 ->  99.07  and loss:  88.27271494269371
forward train acc: top1 ->  99.92599997558594 ; top5 ->  99.998  and loss:  0.26844140978937503
test acc: top1 ->  91.26 ; top5 ->  99.05  and loss:  87.61390557140112
forward train acc: top1 ->  99.91399997558594 ; top5 ->  100.0  and loss:  0.24093942818581127
test acc: top1 ->  91.43 ; top5 ->  99.05  and loss:  87.31926833093166
forward train acc: top1 ->  99.94 ; top5 ->  100.0  and loss:  0.1692175674252212
test acc: top1 ->  91.28 ; top5 ->  99.03  and loss:  86.47435373812914
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -0.19569793004484382 , diff:  0.19569793004484382
adv train loss:  -0.12155141733455821 , diff:  0.0741465127102856
adv train loss:  -0.17099429677182343 , diff:  0.04944287943726522
adv train loss:  -0.11724441079422832 , diff:  0.05374988597759511
adv train loss:  -0.13744442182360217 , diff:  0.020200011029373854
adv train loss:  -0.15870413932134397 , diff:  0.021259717497741804
adv train loss:  -0.09690178268920135 , diff:  0.06180235663214262
adv train loss:  -0.1364081384454039 , diff:  0.03950635575620254
adv train loss:  -0.12605676695966395 , diff:  0.010351371485739946
adv train loss:  -0.15226970687217545 , diff:  0.0262129399125115
layer  8  adv train finish, try to retain  43
test acc: top1 ->  88.84 ; top5 ->  98.75  and loss:  74.6787740290165
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.08126997473664233
test acc: top1 ->  91.87 ; top5 ->  99.17  and loss:  85.59039673954248
forward train acc: top1 ->  99.97599997558594 ; top5 ->  100.0  and loss:  0.0590333428226586
test acc: top1 ->  91.8 ; top5 ->  99.17  and loss:  87.9918017461896
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.043382353404012974
test acc: top1 ->  91.77 ; top5 ->  99.17  and loss:  88.42970619350672
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.052260333961385186
test acc: top1 ->  91.87 ; top5 ->  99.19  and loss:  88.66616174578667
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.04122897321030905
test acc: top1 ->  91.82 ; top5 ->  99.2  and loss:  90.06756177544594
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.051022235453274334
test acc: top1 ->  91.77 ; top5 ->  99.19  and loss:  90.27372803539038
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.062221742862675455
test acc: top1 ->  91.78 ; top5 ->  99.12  and loss:  90.2797887250781
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.024414929575868882
test acc: top1 ->  91.84 ; top5 ->  99.12  and loss:  90.7239741384983
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.02758004711358808
test acc: top1 ->  91.91 ; top5 ->  99.21  and loss:  91.92174108326435
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.05781397768760144
test acc: top1 ->  91.85 ; top5 ->  99.15  and loss:  91.39227218180895
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -0.05170986352095497 , diff:  0.05170986352095497
adv train loss:  -0.04313002058506754 , diff:  0.008579842935887427
layer  9  adv train finish, try to retain  12
test acc: top1 ->  26.4 ; top5 ->  75.06  and loss:  944.0109643936157
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.09088336167496891
test acc: top1 ->  91.71 ; top5 ->  99.12  and loss:  96.92560462653637
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.04406975753045117
test acc: top1 ->  91.67 ; top5 ->  99.11  and loss:  96.62846229970455
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.05460921455232892
test acc: top1 ->  91.77 ; top5 ->  99.09  and loss:  97.97641299664974
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.04574262441656174
test acc: top1 ->  91.81 ; top5 ->  99.12  and loss:  96.15053644031286
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.01876433462530258
test acc: top1 ->  91.84 ; top5 ->  99.12  and loss:  98.65251000225544
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.025098271144088358
test acc: top1 ->  91.74 ; top5 ->  99.08  and loss:  98.77846817672253
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.014182626345245808
test acc: top1 ->  91.72 ; top5 ->  99.1  and loss:  99.12836914509535
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.0179560094147746
test acc: top1 ->  91.77 ; top5 ->  99.1  and loss:  98.11367265880108
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.02451997586831567
test acc: top1 ->  91.83 ; top5 ->  99.03  and loss:  98.35992267727852
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03756335406069411
test acc: top1 ->  91.76 ; top5 ->  99.05  and loss:  98.4056358486414
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -0.027714289995174113 , diff:  0.027714289995174113
adv train loss:  -0.03073909336842462 , diff:  0.0030248033732505064
layer  10  adv train finish, try to retain  10
test acc: top1 ->  18.07 ; top5 ->  59.01  and loss:  1189.3759660720825
forward train acc: top1 ->  98.78999997558594 ; top5 ->  100.0  and loss:  4.14028546854388
test acc: top1 ->  90.72 ; top5 ->  98.58  and loss:  115.32430064678192
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.22863656828849344
test acc: top1 ->  91.26 ; top5 ->  98.51  and loss:  111.67689229547977
forward train acc: top1 ->  99.936 ; top5 ->  100.0  and loss:  0.1825637478941644
test acc: top1 ->  91.49 ; top5 ->  98.5  and loss:  110.99275216460228
forward train acc: top1 ->  99.97399997558594 ; top5 ->  100.0  and loss:  0.0899970535247121
test acc: top1 ->  91.51 ; top5 ->  98.49  and loss:  109.544025182724
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.11746999537717784
test acc: top1 ->  91.67 ; top5 ->  98.5  and loss:  107.31858156621456
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.07566502429108368
test acc: top1 ->  91.6 ; top5 ->  98.52  and loss:  107.59125612676144
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.05971736506035086
test acc: top1 ->  91.55 ; top5 ->  98.56  and loss:  106.18364614248276
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.07357459039485548
test acc: top1 ->  91.47 ; top5 ->  98.54  and loss:  106.05991834402084
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.05542988807428628
test acc: top1 ->  91.59 ; top5 ->  98.53  and loss:  107.41507789492607
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.06281122354994295
test acc: top1 ->  91.63 ; top5 ->  98.54  and loss:  106.74213154613972
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -0.7851382180306246 , diff:  0.7851382180306246
adv train loss:  -0.8556728555849986 , diff:  0.07053463755437406
adv train loss:  -0.7487803318981605 , diff:  0.1068925236868381
adv train loss:  -0.6416393758772756 , diff:  0.10714095602088491
adv train loss:  -0.5474616926803719 , diff:  0.09417768319690367
adv train loss:  -0.6177981250875746 , diff:  0.07033643240720266
adv train loss:  -0.6503009402076714 , diff:  0.032502815120096784
adv train loss:  -0.6405838649225188 , diff:  0.009717075285152532
layer  11  adv train finish, try to retain  61
test acc: top1 ->  89.73 ; top5 ->  98.91  and loss:  71.71294015645981
forward train acc: top1 ->  99.72 ; top5 ->  100.0  and loss:  0.8536332600124297
test acc: top1 ->  91.39 ; top5 ->  99.07  and loss:  97.51042214781046
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.09795970331288117
test acc: top1 ->  91.56 ; top5 ->  98.97  and loss:  95.37427280843258
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.08030910771958588
test acc: top1 ->  91.82 ; top5 ->  98.96  and loss:  94.36811438947916
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.015618264540535165
test acc: top1 ->  91.87 ; top5 ->  98.96  and loss:  94.33166518807411
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.05029343025125854
test acc: top1 ->  91.74 ; top5 ->  98.9  and loss:  95.3652181327343
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.028468233968851564
test acc: top1 ->  91.8 ; top5 ->  98.89  and loss:  95.59233685582876
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.01734614719862293
test acc: top1 ->  91.8 ; top5 ->  98.9  and loss:  95.46206001192331
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.01445380073528213
test acc: top1 ->  91.83 ; top5 ->  98.91  and loss:  95.32808353751898
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.030264641291978478
test acc: top1 ->  91.93 ; top5 ->  98.89  and loss:  95.94845807552338
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.04581622894056636
test acc: top1 ->  91.86 ; top5 ->  98.86  and loss:  95.77024054527283
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -0.1013984942183015 , diff:  0.1013984942183015
adv train loss:  -0.10370078623236623 , diff:  0.002302292014064733
layer  12  adv train finish, try to retain  36
test acc: top1 ->  91.49 ; top5 ->  98.71  and loss:  68.59614140167832
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.06381708749904647
test acc: top1 ->  91.92 ; top5 ->  98.82  and loss:  71.57149535045028
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.023935816145240096
test acc: top1 ->  91.97 ; top5 ->  98.77  and loss:  73.9428995884955
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.02709390855852689
test acc: top1 ->  91.92 ; top5 ->  98.81  and loss:  76.30673877894878
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.02591224767093081
test acc: top1 ->  91.97 ; top5 ->  98.69  and loss:  78.02899920195341
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.02862168390674924
test acc: top1 ->  91.87 ; top5 ->  98.67  and loss:  77.21878238767385
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03328642870837939
test acc: top1 ->  91.9 ; top5 ->  98.66  and loss:  78.74441336095333
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.01935730773220712
test acc: top1 ->  92.08 ; top5 ->  98.69  and loss:  78.39081104844809
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.039246537980943685
test acc: top1 ->  91.87 ; top5 ->  98.7  and loss:  78.55845914781094
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.02113500205814489
test acc: top1 ->  91.92 ; top5 ->  98.7  and loss:  78.96592008322477
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.014499767011329823
test acc: top1 ->  91.91 ; top5 ->  98.63  and loss:  80.43526041507721
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -0.08893990502110682 , diff:  0.08893990502110682
adv train loss:  -0.07088432856835425 , diff:  0.018055576452752575
adv train loss:  -0.095297510837554 , diff:  0.02441318226919975
adv train loss:  -0.08205763473233674 , diff:  0.013239876105217263
adv train loss:  -0.1148553275543236 , diff:  0.03279769282198686
adv train loss:  -0.06674250732369558 , diff:  0.04811282023062802
adv train loss:  -0.05860355000731943 , diff:  0.00813895731637615
layer  13  adv train finish, try to retain  78
test acc: top1 ->  91.85 ; top5 ->  98.67  and loss:  117.39534918963909
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.01519898913875295
test acc: top1 ->  91.95 ; top5 ->  98.87  and loss:  114.38431125879288
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.010175695445013844
test acc: top1 ->  91.87 ; top5 ->  98.88  and loss:  115.6041634529829
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.01712334808689775
test acc: top1 ->  91.93 ; top5 ->  99.06  and loss:  115.70687535405159
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.012981324840211528
test acc: top1 ->  91.92 ; top5 ->  98.88  and loss:  116.92637765407562
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.04115577993383113
test acc: top1 ->  91.86 ; top5 ->  99.02  and loss:  111.9004767537117
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.0055946337579371175
test acc: top1 ->  92.04 ; top5 ->  99.03  and loss:  111.3524661809206
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.028312710957834497
test acc: top1 ->  91.92 ; top5 ->  99.04  and loss:  111.56559713184834
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03056152576442628
test acc: top1 ->  92.04 ; top5 ->  98.98  and loss:  111.0325965732336
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.011581264469512575
test acc: top1 ->  92.07 ; top5 ->  99.04  and loss:  111.2734767049551
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.0070806680689941
test acc: top1 ->  92.14 ; top5 ->  99.03  and loss:  111.0439801812172
==> this epoch:  78 / 512
layer  0  :  0.5  ==>  32 / 64 , inc:  2
layer  1  :  0.859375  ==>  55 / 64 , inc:  2
layer  2  :  0.78125  ==>  100 / 128 , inc:  2
layer  3  :  0.6796875  ==>  87 / 128 , inc:  2
layer  4  :  0.60546875  ==>  155 / 256 , inc:  2
layer  5  :  0.58203125  ==>  149 / 256 , inc:  2
layer  6  :  0.5625  ==>  144 / 256 , inc:  2
layer  7  :  0  ==>  512 / 512 , inc:  1
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0  ==>  512 / 512 , inc:  1
layer  11  :  0  ==>  512 / 512 , inc:  1
layer  12  :  0  ==>  512 / 512 , inc:  1
layer  13  :  0.15234375  ==>  78 / 512 , inc:  2
eps [0.0005625000000000001, 0.0005625000000000001, 0.0005625000000000001, 0.0005625000000000001, 0.0005625000000000001, 0.0005625000000000001, 0.0005625000000000001, 0.00042187500000000005, 0.00042187500000000005, 0.00042187500000000005, 0.00042187500000000005, 0.00042187500000000005, 0.00042187500000000005, 0.0005625000000000001]  wait [0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0]  inc [2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2]  tol: 2
$$$$$$$$$$$$$ epoch  3  $$$$$$$$$$$$
---------------- start layer  0  ---------------
layer  0  adv train finish, try to retain  64
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -0.005783262178510995 , diff:  0.005783262178510995
layer  2  adv train finish, try to retain  128
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -0.02059671997062651 , diff:  0.02059671997062651
adv train loss:  -0.0071410973541787826 , diff:  0.013455622616447727
adv train loss:  -0.02040319972868332 , diff:  0.013262102374504536
adv train loss:  -0.007622729421200347 , diff:  0.012780470307482972
adv train loss:  -0.017334428573597904 , diff:  0.009711699152397557
layer  3  adv train finish, try to retain  128
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
layer  4  adv train finish, try to retain  256
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
layer  5  adv train finish, try to retain  256
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -0.024408520257566124 , diff:  0.024408520257566124
adv train loss:  -0.010390422128352839 , diff:  0.014018098129213286
adv train loss:  -0.019421455176143354 , diff:  0.009031033047790515
************ all values are small in this layer **********
layer  6  adv train finish, try to retain  142
test acc: top1 ->  86.21 ; top5 ->  98.42  and loss:  157.35831087827682
forward train acc: top1 ->  99.5340000024414 ; top5 ->  100.0  and loss:  2.064999937021639
test acc: top1 ->  90.77 ; top5 ->  98.78  and loss:  111.92999026179314
forward train acc: top1 ->  99.60999997558594 ; top5 ->  99.996  and loss:  1.481321347411722
test acc: top1 ->  90.95 ; top5 ->  98.82  and loss:  103.54663659632206
forward train acc: top1 ->  99.69599997558593 ; top5 ->  100.0  and loss:  0.870430096751079
test acc: top1 ->  91.07 ; top5 ->  98.85  and loss:  98.42047175765038
forward train acc: top1 ->  99.728 ; top5 ->  100.0  and loss:  0.9939155887695961
test acc: top1 ->  91.13 ; top5 ->  98.93  and loss:  93.35701625049114
forward train acc: top1 ->  99.69999997802735 ; top5 ->  100.0  and loss:  0.8998093483387493
test acc: top1 ->  91.13 ; top5 ->  98.93  and loss:  90.40606325864792
forward train acc: top1 ->  99.77599997558593 ; top5 ->  99.99599997558593  and loss:  0.7659308510192204
test acc: top1 ->  91.08 ; top5 ->  99.01  and loss:  87.83227837085724
forward train acc: top1 ->  99.792 ; top5 ->  99.998  and loss:  0.7868959766929038
test acc: top1 ->  91.11 ; top5 ->  98.94  and loss:  86.99154658615589
forward train acc: top1 ->  99.81399997558594 ; top5 ->  100.0  and loss:  0.6430999813601375
test acc: top1 ->  91.07 ; top5 ->  98.98  and loss:  85.29684513807297
forward train acc: top1 ->  99.81 ; top5 ->  100.0  and loss:  0.5516813400899991
test acc: top1 ->  91.1 ; top5 ->  99.0  and loss:  84.76262088119984
forward train acc: top1 ->  99.808 ; top5 ->  100.0  and loss:  0.5733809664379805
test acc: top1 ->  91.11 ; top5 ->  99.0  and loss:  83.56685334444046
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  144 / 256 , inc:  2
---------------- start layer  7  ---------------
adv train loss:  -0.12782916236756137 , diff:  0.12782916236756137
adv train loss:  -0.17911174007167574 , diff:  0.051282577704114374
adv train loss:  -0.15281572261301335 , diff:  0.02629601745866239
adv train loss:  -0.15044420871709008 , diff:  0.0023715138959232718
layer  7  adv train finish, try to retain  53
test acc: top1 ->  81.92 ; top5 ->  98.23  and loss:  91.5004001557827
forward train acc: top1 ->  99.894 ; top5 ->  100.0  and loss:  0.3437949590588687
test acc: top1 ->  91.6 ; top5 ->  99.09  and loss:  81.21978789567947
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.1819931960417307
test acc: top1 ->  91.5 ; top5 ->  99.11  and loss:  81.12367831170559
forward train acc: top1 ->  99.936 ; top5 ->  100.0  and loss:  0.19003151522338158
test acc: top1 ->  91.66 ; top5 ->  99.13  and loss:  80.02556678652763
forward train acc: top1 ->  99.93999997558593 ; top5 ->  99.998  and loss:  0.19401531717085163
test acc: top1 ->  91.66 ; top5 ->  99.08  and loss:  81.24536538124084
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.11317217090982012
test acc: top1 ->  91.73 ; top5 ->  99.06  and loss:  80.80232746899128
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.13568437306093983
test acc: top1 ->  91.7 ; top5 ->  99.03  and loss:  80.6954495459795
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.10431077633256791
test acc: top1 ->  91.64 ; top5 ->  99.08  and loss:  80.70991203188896
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.08383858814340783
test acc: top1 ->  91.62 ; top5 ->  99.05  and loss:  80.98122107982635
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.10697654873365536
test acc: top1 ->  91.62 ; top5 ->  99.06  and loss:  80.57956433296204
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.08385360588727053
test acc: top1 ->  91.71 ; top5 ->  99.09  and loss:  81.29989545047283
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -0.03730261571763549 , diff:  0.03730261571763549
adv train loss:  -0.036242588956156396 , diff:  0.0010600267614790937
layer  8  adv train finish, try to retain  12
test acc: top1 ->  22.67 ; top5 ->  84.1  and loss:  664.4956560134888
forward train acc: top1 ->  98.6860000024414 ; top5 ->  99.992  and loss:  4.914412911748514
test acc: top1 ->  90.77 ; top5 ->  98.9  and loss:  90.84450462460518
forward train acc: top1 ->  99.678 ; top5 ->  99.998  and loss:  1.100489728152752
test acc: top1 ->  91.04 ; top5 ->  98.97  and loss:  88.20822882652283
forward train acc: top1 ->  99.80799997558594 ; top5 ->  100.0  and loss:  0.6369129135273397
test acc: top1 ->  91.07 ; top5 ->  99.04  and loss:  85.57983550429344
forward train acc: top1 ->  99.854 ; top5 ->  100.0  and loss:  0.4857938859204296
test acc: top1 ->  91.14 ; top5 ->  99.04  and loss:  85.4472541809082
forward train acc: top1 ->  99.866 ; top5 ->  100.0  and loss:  0.42805216029228177
test acc: top1 ->  91.23 ; top5 ->  99.06  and loss:  83.97440584003925
forward train acc: top1 ->  99.882 ; top5 ->  99.998  and loss:  0.40628678583016153
test acc: top1 ->  91.29 ; top5 ->  99.09  and loss:  84.54584063589573
forward train acc: top1 ->  99.912 ; top5 ->  100.0  and loss:  0.2629185834521195
test acc: top1 ->  91.34 ; top5 ->  99.08  and loss:  84.43350288271904
forward train acc: top1 ->  99.906 ; top5 ->  100.0  and loss:  0.2983881573600229
test acc: top1 ->  91.38 ; top5 ->  99.08  and loss:  84.5006124228239
forward train acc: top1 ->  99.91 ; top5 ->  100.0  and loss:  0.26034197909757495
test acc: top1 ->  91.38 ; top5 ->  99.07  and loss:  83.3704419285059
forward train acc: top1 ->  99.918 ; top5 ->  100.0  and loss:  0.27281857434718404
test acc: top1 ->  91.38 ; top5 ->  99.05  and loss:  84.1936205625534
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -0.8957642939203652 , diff:  0.8957642939203652
adv train loss:  -0.8761909470777027 , diff:  0.01957334684266243
adv train loss:  -0.7347189062857069 , diff:  0.14147204079199582
adv train loss:  -0.7689196225837804 , diff:  0.03420071629807353
adv train loss:  -0.8520062811730895 , diff:  0.08308665858930908
adv train loss:  -0.7939149548765272 , diff:  0.05809132629656233
adv train loss:  -0.8751864059595391 , diff:  0.08127145108301193
adv train loss:  -0.9408878639223985 , diff:  0.06570145796285942
adv train loss:  -0.7631327780254651 , diff:  0.17775508589693345
adv train loss:  -0.8335036833304912 , diff:  0.0703709053050261
layer  9  adv train finish, try to retain  40
test acc: top1 ->  32.92 ; top5 ->  91.48  and loss:  578.5935230255127
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.12503092141560046
test acc: top1 ->  92.06 ; top5 ->  99.25  and loss:  82.8243054524064
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.0172243203396647
test acc: top1 ->  92.08 ; top5 ->  99.19  and loss:  85.34725048393011
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.017028882779413834
test acc: top1 ->  92.04 ; top5 ->  99.19  and loss:  88.71522026509047
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -0.021071662929898594 , diff:  0.021071662929898594
adv train loss:  -0.031397462601944426 , diff:  0.010325799672045832
adv train loss:  -0.034176033339463174 , diff:  0.0027785707375187485
layer  10  adv train finish, try to retain  19
test acc: top1 ->  24.95 ; top5 ->  76.84  and loss:  845.2587327957153
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.054454951714433264
test acc: top1 ->  91.81 ; top5 ->  98.93  and loss:  95.50180710852146
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.06463491158319812
test acc: top1 ->  92.01 ; top5 ->  98.88  and loss:  95.94977135956287
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.027681211246090243
test acc: top1 ->  91.94 ; top5 ->  98.84  and loss:  97.54593621194363
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.03022053310451156
test acc: top1 ->  91.92 ; top5 ->  98.81  and loss:  95.90734653174877
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.01673285987089912
test acc: top1 ->  91.94 ; top5 ->  98.79  and loss:  98.07372260093689
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.00884182961544866
test acc: top1 ->  92.07 ; top5 ->  98.83  and loss:  96.64068519324064
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.024030071505137585
test acc: top1 ->  92.02 ; top5 ->  98.86  and loss:  96.0422403216362
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.018229502013127785
test acc: top1 ->  92.03 ; top5 ->  98.81  and loss:  96.63512879610062
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.012772806084740296
test acc: top1 ->  92.09 ; top5 ->  98.81  and loss:  97.25079394876957
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.030689321814861614
test acc: top1 ->  92.12 ; top5 ->  98.8  and loss:  97.09085102379322
==> this epoch:  19 / 512
---------------- start layer  11  ---------------
adv train loss:  -0.01606207332224585 , diff:  0.01606207332224585
adv train loss:  -0.01975094484805595 , diff:  0.0036888715258101
layer  11  adv train finish, try to retain  22
test acc: top1 ->  51.25 ; top5 ->  96.31  and loss:  353.21408915519714
forward train acc: top1 ->  99.878 ; top5 ->  100.0  and loss:  0.4172002466439153
test acc: top1 ->  91.8 ; top5 ->  98.66  and loss:  97.99441143870354
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.037176829147938406
test acc: top1 ->  91.94 ; top5 ->  98.7  and loss:  97.15722278505564
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03860614607765456
test acc: top1 ->  92.01 ; top5 ->  98.8  and loss:  96.55299437791109
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.025557799574016826
test acc: top1 ->  92.0 ; top5 ->  98.66  and loss:  98.80206227302551
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.058468977329539484
test acc: top1 ->  92.02 ; top5 ->  98.78  and loss:  96.69300166517496
forward train acc: top1 ->  99.97799997558593 ; top5 ->  100.0  and loss:  0.050450338661903515
test acc: top1 ->  92.06 ; top5 ->  98.82  and loss:  97.22068786621094
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.035827085779601475
test acc: top1 ->  91.96 ; top5 ->  98.76  and loss:  97.92726472020149
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.019284599547972903
test acc: top1 ->  92.05 ; top5 ->  98.79  and loss:  98.004031509161
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.02957807938946644
test acc: top1 ->  91.96 ; top5 ->  98.81  and loss:  98.80760633945465
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.02604613924813748
test acc: top1 ->  92.07 ; top5 ->  98.83  and loss:  98.72126290202141
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -0.3760179082819377 , diff:  0.3760179082819377
adv train loss:  -0.4043918887327891 , diff:  0.028373980450851377
adv train loss:  -0.3527060508058639 , diff:  0.05168583792692516
adv train loss:  -0.4569335016713012 , diff:  0.10422745086543728
adv train loss:  -0.3451393626019126 , diff:  0.11179413906938862
adv train loss:  -0.5031232767032634 , diff:  0.15798391410135082
adv train loss:  -0.4593053293574485 , diff:  0.0438179473458149
adv train loss:  -0.39527506504236953 , diff:  0.06403026431507897
adv train loss:  -0.48557111540139886 , diff:  0.09029605035902932
adv train loss:  -0.5437254945427412 , diff:  0.05815437914134236
layer  12  adv train finish, try to retain  62
test acc: top1 ->  74.45 ; top5 ->  98.51  and loss:  191.50463008880615
forward train acc: top1 ->  97.838 ; top5 ->  99.998  and loss:  7.968891483906191
test acc: top1 ->  91.75 ; top5 ->  99.08  and loss:  69.08640630170703
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.1837663210753817
test acc: top1 ->  91.86 ; top5 ->  99.05  and loss:  67.85467303916812
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.12375840067397803
test acc: top1 ->  91.97 ; top5 ->  99.03  and loss:  67.65027885138988
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.07775830537866568
test acc: top1 ->  91.91 ; top5 ->  99.05  and loss:  69.06145236641169
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.05937358479422983
test acc: top1 ->  91.93 ; top5 ->  99.04  and loss:  68.79479897022247
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.053907016197626945
test acc: top1 ->  91.97 ; top5 ->  99.08  and loss:  69.12164714932442
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.04369808277624543
test acc: top1 ->  91.99 ; top5 ->  99.07  and loss:  69.57800338417292
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.031012267845653696
test acc: top1 ->  92.04 ; top5 ->  99.04  and loss:  70.72611277550459
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03976456571035669
test acc: top1 ->  92.11 ; top5 ->  99.05  and loss:  70.45577251166105
==> this epoch:  62 / 512
---------------- start layer  13  ---------------
adv train loss:  -0.043011212306737434 , diff:  0.043011212306737434
adv train loss:  -0.0389998956634372 , diff:  0.004011316643300233
layer  13  adv train finish, try to retain  496
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  2
layer  1  :  0.859375  ==>  55 / 64 , inc:  2
layer  2  :  0.78125  ==>  100 / 128 , inc:  2
layer  3  :  0.6796875  ==>  87 / 128 , inc:  2
layer  4  :  0.60546875  ==>  155 / 256 , inc:  2
layer  5  :  0.58203125  ==>  149 / 256 , inc:  2
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0  ==>  512 / 512 , inc:  1
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0.037109375  ==>  19 / 512 , inc:  2
layer  11  :  0  ==>  512 / 512 , inc:  1
layer  12  :  0.12109375  ==>  62 / 512 , inc:  2
layer  13  :  0.15234375  ==>  78 / 512 , inc:  2
eps [0.0011250000000000001, 0.0011250000000000001, 0.0011250000000000001, 0.0011250000000000001, 0.0011250000000000001, 0.0011250000000000001, 0.00042187500000000005, 0.00031640625000000006, 0.00031640625000000006, 0.00031640625000000006, 0.00042187500000000005, 0.00031640625000000006, 0.00042187500000000005, 0.0011250000000000001]  wait [0, 0, 0, 0, 0, 0, 2, 4, 4, 4, 0, 4, 0, 0]  inc [2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 2, 2]  tol: 2
$$$$$$$$$$$$$ epoch  4  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -0.038405690731451614 , diff:  0.038405690731451614
adv train loss:  -0.04542711505564512 , diff:  0.007021424324193504
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  30
test acc: top1 ->  29.29 ; top5 ->  73.85  and loss:  742.3223958015442
forward train acc: top1 ->  90.79200000244141 ; top5 ->  99.0580000024414  and loss:  49.54439541697502
test acc: top1 ->  78.77 ; top5 ->  95.89  and loss:  144.23228430747986
forward train acc: top1 ->  93.0119999951172 ; top5 ->  99.4540000024414  and loss:  26.774350732564926
test acc: top1 ->  86.32 ; top5 ->  98.39  and loss:  63.554163813591
forward train acc: top1 ->  94.17599999755859 ; top5 ->  99.6120000024414  and loss:  19.908538684248924
test acc: top1 ->  87.19 ; top5 ->  98.6  and loss:  56.41295501589775
forward train acc: top1 ->  94.94799999023438 ; top5 ->  99.69999997558594  and loss:  16.69373221695423
test acc: top1 ->  87.51 ; top5 ->  98.57  and loss:  51.69959615170956
forward train acc: top1 ->  95.63999998535157 ; top5 ->  99.804  and loss:  13.667480736970901
test acc: top1 ->  87.96 ; top5 ->  98.67  and loss:  50.54537118971348
forward train acc: top1 ->  95.97000000976563 ; top5 ->  99.836  and loss:  12.414111338555813
test acc: top1 ->  88.19 ; top5 ->  98.69  and loss:  50.042684614658356
forward train acc: top1 ->  96.28400001220703 ; top5 ->  99.81599997558594  and loss:  11.688791289925575
test acc: top1 ->  88.27 ; top5 ->  98.75  and loss:  48.862018808722496
forward train acc: top1 ->  96.68199998535157 ; top5 ->  99.866  and loss:  10.594890177249908
test acc: top1 ->  88.56 ; top5 ->  98.7  and loss:  48.91622753441334
forward train acc: top1 ->  96.74799999023438 ; top5 ->  99.902  and loss:  9.972372382879257
test acc: top1 ->  88.71 ; top5 ->  98.73  and loss:  48.64898394048214
forward train acc: top1 ->  96.74200001220703 ; top5 ->  99.872  and loss:  9.982626602053642
test acc: top1 ->  88.83 ; top5 ->  98.77  and loss:  47.62371361255646
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  2
---------------- start layer  1  ---------------
adv train loss:  -0.857403954025358 , diff:  0.857403954025358
adv train loss:  -0.908680179156363 , diff:  0.05127622513100505
adv train loss:  -0.858610013499856 , diff:  0.050070165656507015
adv train loss:  -0.8428330407477915 , diff:  0.015776972752064466
adv train loss:  -0.8494294856209308 , diff:  0.006596444873139262
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  53
test acc: top1 ->  84.29 ; top5 ->  97.89  and loss:  68.66821011900902
forward train acc: top1 ->  99.21800000488281 ; top5 ->  99.986  and loss:  2.569965738337487
test acc: top1 ->  90.78 ; top5 ->  99.09  and loss:  46.35362636297941
forward train acc: top1 ->  99.58399997558594 ; top5 ->  99.998  and loss:  1.2910775918280706
test acc: top1 ->  90.98 ; top5 ->  99.09  and loss:  49.58553001284599
forward train acc: top1 ->  99.67799997558593 ; top5 ->  99.998  and loss:  1.0377960953628644
test acc: top1 ->  91.09 ; top5 ->  99.14  and loss:  49.45519068092108
forward train acc: top1 ->  99.73399997802734 ; top5 ->  100.0  and loss:  0.7853274659719318
test acc: top1 ->  91.05 ; top5 ->  99.14  and loss:  50.30055284500122
forward train acc: top1 ->  99.82399997558593 ; top5 ->  100.0  and loss:  0.6103731722105294
test acc: top1 ->  91.07 ; top5 ->  99.15  and loss:  52.22572249174118
forward train acc: top1 ->  99.842 ; top5 ->  99.998  and loss:  0.4573913121421356
test acc: top1 ->  91.26 ; top5 ->  99.09  and loss:  53.65611982345581
forward train acc: top1 ->  99.812 ; top5 ->  100.0  and loss:  0.580844935728237
test acc: top1 ->  91.3 ; top5 ->  99.1  and loss:  54.12396876513958
forward train acc: top1 ->  99.854 ; top5 ->  99.998  and loss:  0.4746396726986859
test acc: top1 ->  91.34 ; top5 ->  99.11  and loss:  54.413799710571766
forward train acc: top1 ->  99.84799997558594 ; top5 ->  100.0  and loss:  0.4588150651543401
test acc: top1 ->  91.33 ; top5 ->  99.06  and loss:  55.052134066820145
forward train acc: top1 ->  99.872 ; top5 ->  100.0  and loss:  0.42429171517142095
test acc: top1 ->  91.36 ; top5 ->  99.07  and loss:  55.67686641216278
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  55 / 64 , inc:  2
---------------- start layer  2  ---------------
adv train loss:  -0.2299720176961273 , diff:  0.2299720176961273
adv train loss:  -0.1905479339766316 , diff:  0.039424083719495684
adv train loss:  -0.26348098948801635 , diff:  0.07293305551138474
adv train loss:  -0.15175336525135208 , diff:  0.11172762423666427
adv train loss:  -0.18710185791860567 , diff:  0.035348492667253595
adv train loss:  -0.19626985615468584 , diff:  0.009167998236080166
************ all values are small in this layer **********
layer  2  adv train finish, try to retain  98
test acc: top1 ->  69.25 ; top5 ->  92.33  and loss:  250.6555609703064
forward train acc: top1 ->  99.2840000024414 ; top5 ->  99.99199997558594  and loss:  2.32046171836555
test acc: top1 ->  90.59 ; top5 ->  99.06  and loss:  56.62621283531189
forward train acc: top1 ->  99.44199997558594 ; top5 ->  99.992  and loss:  1.6244070339016616
test acc: top1 ->  90.8 ; top5 ->  99.02  and loss:  54.85758537054062
forward train acc: top1 ->  99.532 ; top5 ->  99.998  and loss:  1.3966085927095264
test acc: top1 ->  90.89 ; top5 ->  99.09  and loss:  55.38456444442272
forward train acc: top1 ->  99.64599997558594 ; top5 ->  99.996  and loss:  1.1112278731307015
test acc: top1 ->  90.96 ; top5 ->  99.14  and loss:  55.080435663461685
forward train acc: top1 ->  99.66400000244141 ; top5 ->  99.998  and loss:  0.9308809345820919
test acc: top1 ->  90.93 ; top5 ->  99.08  and loss:  56.03559158742428
forward train acc: top1 ->  99.74599997802734 ; top5 ->  100.0  and loss:  0.8127341376966797
test acc: top1 ->  91.03 ; top5 ->  99.19  and loss:  56.05979312956333
forward train acc: top1 ->  99.69399997558594 ; top5 ->  100.0  and loss:  0.8916221896652132
test acc: top1 ->  91.07 ; top5 ->  99.18  and loss:  55.067603170871735
forward train acc: top1 ->  99.708 ; top5 ->  100.0  and loss:  0.8562076593516394
test acc: top1 ->  91.11 ; top5 ->  99.19  and loss:  55.96751557290554
forward train acc: top1 ->  99.728 ; top5 ->  99.996  and loss:  0.800046538701281
test acc: top1 ->  91.05 ; top5 ->  99.13  and loss:  55.58681608736515
forward train acc: top1 ->  99.75399997558594 ; top5 ->  99.998  and loss:  0.7182725748280063
test acc: top1 ->  91.04 ; top5 ->  99.13  and loss:  56.11276689171791
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  2
---------------- start layer  3  ---------------
adv train loss:  -0.12651310258661397 , diff:  0.12651310258661397
adv train loss:  -0.1582513078028569 , diff:  0.03173820521624293
adv train loss:  -0.16645399689150508 , diff:  0.008202689088648185
************ all values are small in this layer **********
layer  3  adv train finish, try to retain  85
test acc: top1 ->  80.61 ; top5 ->  97.22  and loss:  123.61667221784592
forward train acc: top1 ->  97.40200000488281 ; top5 ->  99.93  and loss:  8.76863032951951
test acc: top1 ->  89.2 ; top5 ->  98.74  and loss:  53.28875561803579
forward train acc: top1 ->  98.12999998046875 ; top5 ->  99.956  and loss:  5.96472923271358
test acc: top1 ->  89.58 ; top5 ->  98.89  and loss:  49.71138644218445
forward train acc: top1 ->  98.39199998291015 ; top5 ->  99.974  and loss:  4.911509420722723
test acc: top1 ->  89.76 ; top5 ->  98.87  and loss:  48.682783260941505
forward train acc: top1 ->  98.55800000488281 ; top5 ->  99.984  and loss:  4.320477657951415
test acc: top1 ->  89.89 ; top5 ->  98.95  and loss:  48.63286530226469
forward train acc: top1 ->  98.77199998046875 ; top5 ->  99.97799997558593  and loss:  3.676169720478356
test acc: top1 ->  90.01 ; top5 ->  98.98  and loss:  48.34064669907093
forward train acc: top1 ->  98.85399997558594 ; top5 ->  99.984  and loss:  3.4487863862887025
test acc: top1 ->  90.09 ; top5 ->  98.96  and loss:  49.42889533191919
forward train acc: top1 ->  98.93 ; top5 ->  99.98  and loss:  3.1979736373759806
test acc: top1 ->  90.28 ; top5 ->  98.96  and loss:  49.12348044663668
forward train acc: top1 ->  98.86199998291016 ; top5 ->  99.984  and loss:  3.165747160091996
test acc: top1 ->  90.2 ; top5 ->  99.03  and loss:  48.7192222699523
forward train acc: top1 ->  98.98600000732422 ; top5 ->  99.992  and loss:  3.0470749558880925
test acc: top1 ->  90.28 ; top5 ->  99.04  and loss:  49.12410210072994
forward train acc: top1 ->  99.03400000488281 ; top5 ->  99.986  and loss:  2.8320023752748966
test acc: top1 ->  90.24 ; top5 ->  99.06  and loss:  49.4646962583065
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  2
---------------- start layer  4  ---------------
adv train loss:  -0.4485568933887407 , diff:  0.4485568933887407
adv train loss:  -0.4736050752690062 , diff:  0.025048181880265474
adv train loss:  -0.46275767910992727 , diff:  0.010847396159078926
adv train loss:  -0.5063199321739376 , diff:  0.04356225306401029
adv train loss:  -0.44697717946837656 , diff:  0.059342752705561
adv train loss:  -0.5240907603874803 , diff:  0.0771135809191037
adv train loss:  -0.42397553042974323 , diff:  0.10011522995773703
adv train loss:  -0.46231570886448026 , diff:  0.03834017843473703
adv train loss:  -0.44413259241264313 , diff:  0.018183116451837122
adv train loss:  -0.45598803041502833 , diff:  0.011855438002385199
************ all values are small in this layer **********
layer  4  adv train finish, try to retain  153
test acc: top1 ->  43.51 ; top5 ->  79.68  and loss:  257.81106400489807
forward train acc: top1 ->  93.09599997802735 ; top5 ->  99.736  and loss:  22.724335625767708
test acc: top1 ->  86.75 ; top5 ->  98.69  and loss:  54.18587175011635
forward train acc: top1 ->  94.42599998291016 ; top5 ->  99.87  and loss:  16.578061521053314
test acc: top1 ->  87.56 ; top5 ->  98.82  and loss:  50.451484099030495
forward train acc: top1 ->  95.146 ; top5 ->  99.85400000244141  and loss:  14.50265932828188
test acc: top1 ->  88.12 ; top5 ->  98.88  and loss:  48.23518028855324
forward train acc: top1 ->  95.69399999267578 ; top5 ->  99.922  and loss:  12.391413412988186
test acc: top1 ->  88.36 ; top5 ->  98.89  and loss:  47.72712850570679
forward train acc: top1 ->  95.96400001953126 ; top5 ->  99.926  and loss:  11.69617135077715
test acc: top1 ->  88.63 ; top5 ->  98.93  and loss:  46.58786155283451
forward train acc: top1 ->  96.25800001220703 ; top5 ->  99.92399997558594  and loss:  10.835362665355206
test acc: top1 ->  88.77 ; top5 ->  98.99  and loss:  46.30852361023426
forward train acc: top1 ->  96.44599999267578 ; top5 ->  99.936  and loss:  10.229762461036444
test acc: top1 ->  88.86 ; top5 ->  98.98  and loss:  46.758402809500694
forward train acc: top1 ->  96.48799998535156 ; top5 ->  99.954  and loss:  10.212474633008242
test acc: top1 ->  88.9 ; top5 ->  98.96  and loss:  45.66864486038685
forward train acc: top1 ->  96.6199999975586 ; top5 ->  99.93999997558593  and loss:  9.951218090951443
test acc: top1 ->  88.98 ; top5 ->  99.0  and loss:  45.92098133265972
forward train acc: top1 ->  96.69400000732422 ; top5 ->  99.952  and loss:  9.466679904609919
test acc: top1 ->  89.1 ; top5 ->  98.95  and loss:  45.65402866899967
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  2
---------------- start layer  5  ---------------
adv train loss:  -1.0668734286446124 , diff:  1.0668734286446124
adv train loss:  -1.0924844634719193 , diff:  0.025611034827306867
adv train loss:  -1.0347137651406229 , diff:  0.057770698331296444
adv train loss:  -1.0139618944376707 , diff:  0.020751870702952147
adv train loss:  -1.0976883126422763 , diff:  0.08372641820460558
adv train loss:  -1.0762518434785306 , diff:  0.02143646916374564
adv train loss:  -1.0504857054911554 , diff:  0.02576613798737526
adv train loss:  -1.007746521383524 , diff:  0.042739184107631445
adv train loss:  -1.0462159605231136 , diff:  0.03846943913958967
adv train loss:  -1.039132582489401 , diff:  0.007083378033712506
************ all values are small in this layer **********
layer  5  adv train finish, try to retain  147
test acc: top1 ->  64.6 ; top5 ->  87.72  and loss:  167.2095239162445
forward train acc: top1 ->  97.03799998291015 ; top5 ->  99.924  and loss:  8.638739630579948
test acc: top1 ->  88.79 ; top5 ->  99.04  and loss:  49.37766106426716
forward train acc: top1 ->  97.78199998046875 ; top5 ->  99.968  and loss:  6.471372339874506
test acc: top1 ->  89.45 ; top5 ->  99.13  and loss:  48.344304978847504
forward train acc: top1 ->  98.05199998046875 ; top5 ->  99.988  and loss:  5.588161343708634
test acc: top1 ->  89.59 ; top5 ->  99.13  and loss:  48.059741139411926
forward train acc: top1 ->  98.40799997802735 ; top5 ->  99.986  and loss:  4.6396104749292135
test acc: top1 ->  89.51 ; top5 ->  99.17  and loss:  49.202169097959995
forward train acc: top1 ->  98.53399998046875 ; top5 ->  99.99  and loss:  4.2789986515417695
test acc: top1 ->  89.74 ; top5 ->  99.15  and loss:  49.814487002789974
forward train acc: top1 ->  98.58000000244141 ; top5 ->  99.982  and loss:  4.034681007266045
test acc: top1 ->  89.75 ; top5 ->  99.16  and loss:  49.07503968477249
forward train acc: top1 ->  98.67800000732421 ; top5 ->  99.976  and loss:  3.874087791889906
test acc: top1 ->  89.77 ; top5 ->  99.12  and loss:  49.323057882487774
forward train acc: top1 ->  98.74799998291016 ; top5 ->  99.986  and loss:  3.7198082581162453
test acc: top1 ->  89.84 ; top5 ->  99.12  and loss:  49.322064988315105
forward train acc: top1 ->  98.79400000488282 ; top5 ->  99.982  and loss:  3.5720765460282564
test acc: top1 ->  89.9 ; top5 ->  99.1  and loss:  49.34107183665037
forward train acc: top1 ->  98.8420000048828 ; top5 ->  99.992  and loss:  3.4264753172174096
test acc: top1 ->  89.85 ; top5 ->  99.13  and loss:  49.619252040982246
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  2
---------------- start layer  6  ---------------
adv train loss:  -0.8579004228813574 , diff:  0.8579004228813574
adv train loss:  -0.8212158291134983 , diff:  0.03668459376785904
adv train loss:  -0.8343040706822649 , diff:  0.013088241568766534
adv train loss:  -0.8730257932911627 , diff:  0.038721722608897835
adv train loss:  -0.8671168562723324 , diff:  0.005908937018830329
layer  6  adv train finish, try to retain  256
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
### skip layer  8 wait:  4  ###
---------------- start layer  9  ---------------
### skip layer  9 wait:  4  ###
---------------- start layer  10  ---------------
adv train loss:  -1.660983955487609 , diff:  1.660983955487609
adv train loss:  -1.7573712365701795 , diff:  0.09638728108257055
adv train loss:  -1.731920478399843 , diff:  0.025450758170336485
adv train loss:  -1.6752205851953477 , diff:  0.05669989320449531
adv train loss:  -1.6921939724124968 , diff:  0.01697338721714914
adv train loss:  -1.7746091466397047 , diff:  0.0824151742272079
adv train loss:  -1.7296900011133403 , diff:  0.044919145526364446
adv train loss:  -1.7010640057269484 , diff:  0.028625995386391878
adv train loss:  -1.6626554024405777 , diff:  0.038408603286370635
adv train loss:  -1.7609957763925195 , diff:  0.09834037395194173
layer  10  adv train finish, try to retain  494
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
### skip layer  11 wait:  4  ###
---------------- start layer  12  ---------------
adv train loss:  -1.4008022827329114 , diff:  1.4008022827329114
adv train loss:  -1.6282394744921476 , diff:  0.22743719175923616
adv train loss:  -1.447731439722702 , diff:  0.18050803476944566
adv train loss:  -1.3646510187536478 , diff:  0.0830804209690541
adv train loss:  -1.2230852008215152 , diff:  0.14156581793213263
adv train loss:  -1.4661232482176274 , diff:  0.24303804739611223
adv train loss:  -1.5715504598338157 , diff:  0.10542721161618829
adv train loss:  -1.4673776617273688 , diff:  0.10417279810644686
adv train loss:  -1.4001298899529502 , diff:  0.06724777177441865
adv train loss:  -1.4729919868987054 , diff:  0.07286209694575518
layer  12  adv train finish, try to retain  500
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -0.8205776156391948 , diff:  0.8205776156391948
adv train loss:  -0.8106066988548264 , diff:  0.009970916784368455
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  76
test acc: top1 ->  14.09 ; top5 ->  76.43  and loss:  383.1510736942291
forward train acc: top1 ->  53.90799998779297 ; top5 ->  89.472  and loss:  171.10530585050583
test acc: top1 ->  82.82 ; top5 ->  98.16  and loss:  74.67138040065765
forward train acc: top1 ->  97.418 ; top5 ->  99.924  and loss:  30.5546992123127
test acc: top1 ->  90.72 ; top5 ->  99.02  and loss:  39.334938898682594
forward train acc: top1 ->  99.696 ; top5 ->  100.0  and loss:  10.162281095981598
test acc: top1 ->  91.34 ; top5 ->  99.01  and loss:  33.513587079942226
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  4.321050468832254
test acc: top1 ->  91.7 ; top5 ->  99.04  and loss:  33.09262406826019
forward train acc: top1 ->  99.93799997558594 ; top5 ->  100.0  and loss:  2.314403942786157
test acc: top1 ->  91.68 ; top5 ->  99.02  and loss:  33.62720309942961
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  1.5862005287781358
test acc: top1 ->  91.73 ; top5 ->  99.02  and loss:  34.23099835216999
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  1.2897595092654228
test acc: top1 ->  91.71 ; top5 ->  99.0  and loss:  35.093720361590385
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  1.0577601650729775
test acc: top1 ->  91.83 ; top5 ->  98.95  and loss:  35.438793167471886
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.8861230495385826
test acc: top1 ->  91.83 ; top5 ->  99.01  and loss:  35.940086752176285
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.7581869256682694
test acc: top1 ->  91.93 ; top5 ->  98.96  and loss:  36.82187172025442
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  78 / 512 , inc:  2
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0  ==>  512 / 512 , inc:  1
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0.037109375  ==>  19 / 512 , inc:  2
layer  11  :  0  ==>  512 / 512 , inc:  1
layer  12  :  0.12109375  ==>  62 / 512 , inc:  2
layer  13  :  0.15234375  ==>  78 / 512 , inc:  1
eps [0.0008437500000000001, 0.0008437500000000001, 0.0008437500000000001, 0.0008437500000000001, 0.0008437500000000001, 0.0008437500000000001, 0.0008437500000000001, 0.00031640625000000006, 0.00031640625000000006, 0.00031640625000000006, 0.0008437500000000001, 0.00031640625000000006, 0.0008437500000000001, 0.0008437500000000001]  wait [2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 0, 3, 0, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1]  tol: 2
$$$$$$$$$$$$$ epoch  5  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -0.46055907347181346 , diff:  0.46055907347181346
adv train loss:  -0.47914339193084743 , diff:  0.018584318459033966
adv train loss:  -0.49683422986709047 , diff:  0.017690837936243042
adv train loss:  -0.4379922973748762 , diff:  0.058841932492214255
adv train loss:  -0.5555934561270988 , diff:  0.11760115875222255
adv train loss:  -0.42018580029252917 , diff:  0.1354076558345696
adv train loss:  -0.4578203537821537 , diff:  0.03763455348962452
adv train loss:  -0.43491414371237624 , diff:  0.022906210069777444
adv train loss:  -0.5592944154923316 , diff:  0.12438027177995536
adv train loss:  -0.44898939586710185 , diff:  0.11030501962522976
layer  0  adv train finish, try to retain  64
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -0.5378348102676682 , diff:  0.5378348102676682
adv train loss:  -0.48450822923041414 , diff:  0.05332658103725407
adv train loss:  -0.5111519255515304 , diff:  0.026643696321116295
adv train loss:  -0.5576153154834174 , diff:  0.04646338993188692
adv train loss:  -0.5303866843460128 , diff:  0.02722863113740459
adv train loss:  -0.5791511249990435 , diff:  0.04876444065303076
adv train loss:  -0.40139521745732054 , diff:  0.177755907541723
adv train loss:  -0.4771580768720014 , diff:  0.07576285941468086
adv train loss:  -0.4684190978005063 , diff:  0.008738979071495123
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -0.5022033181303414 , diff:  0.5022033181303414
adv train loss:  -0.347896612896875 , diff:  0.15430670523346635
adv train loss:  -0.39326490816893056 , diff:  0.04536829527205555
adv train loss:  -0.45531128512811847 , diff:  0.06204637695918791
adv train loss:  -0.469555604053312 , diff:  0.014244318925193511
adv train loss:  -0.4668574437018833 , diff:  0.002698160351428669
layer  2  adv train finish, try to retain  128
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -0.37748546949296724 , diff:  0.37748546949296724
adv train loss:  -0.4432436730276095 , diff:  0.06575820353464223
adv train loss:  -0.5058169393596472 , diff:  0.06257326633203775
adv train loss:  -0.5163957752811257 , diff:  0.010578835921478458
adv train loss:  -0.48587627070810413 , diff:  0.03051950457302155
adv train loss:  -0.510832486776053 , diff:  0.024956216067948844
adv train loss:  -0.526650650324882 , diff:  0.015818163548829034
adv train loss:  -0.4403200820670463 , diff:  0.0863305682578357
adv train loss:  -0.5273615941114258 , diff:  0.08704151204437949
adv train loss:  -0.5667979657446267 , diff:  0.039436371633200906
layer  3  adv train finish, try to retain  128
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -0.513642705976963 , diff:  0.513642705976963
adv train loss:  -0.5306277030758793 , diff:  0.01698499709891621
adv train loss:  -0.49197260361688677 , diff:  0.03865509945899248
adv train loss:  -0.48504175935522653 , diff:  0.006930844261660241
layer  4  adv train finish, try to retain  256
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -0.4661909849673975 , diff:  0.4661909849673975
adv train loss:  -0.5465690678393003 , diff:  0.08037808287190273
adv train loss:  -0.44965088431490585 , diff:  0.09691818352439441
adv train loss:  -0.5363061471143737 , diff:  0.08665526279946789
adv train loss:  -0.5607247369043762 , diff:  0.024418589790002443
adv train loss:  -0.5112238378496841 , diff:  0.04950089905469213
adv train loss:  -0.4975337933283299 , diff:  0.013690044521354139
adv train loss:  -0.5199606304377085 , diff:  0.02242683710937854
adv train loss:  -0.46004828400327824 , diff:  0.05991234643443022
adv train loss:  -0.5684287938929629 , diff:  0.10838050988968462
layer  5  adv train finish, try to retain  256
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -0.4213168957794551 , diff:  0.4213168957794551
adv train loss:  -0.4480164347914979 , diff:  0.026699539012042806
adv train loss:  -0.44414876418886706 , diff:  0.003867670602630824
layer  6  adv train finish, try to retain  255
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
### skip layer  8 wait:  3  ###
---------------- start layer  9  ---------------
### skip layer  9 wait:  3  ###
---------------- start layer  10  ---------------
adv train loss:  -0.6074300624022726 , diff:  0.6074300624022726
adv train loss:  -0.7604582165949978 , diff:  0.1530281541927252
adv train loss:  -0.6442809838044923 , diff:  0.11617723279050551
adv train loss:  -0.5747280358918943 , diff:  0.06955294791259803
adv train loss:  -0.6336322122660931 , diff:  0.05890417637419887
adv train loss:  -0.577900006872369 , diff:  0.055732205393724144
adv train loss:  -0.5559672805247828 , diff:  0.02193272634758614
adv train loss:  -0.6716259540553438 , diff:  0.11565867353056092
adv train loss:  -0.6734000332071446 , diff:  0.0017740791518008336
layer  10  adv train finish, try to retain  497
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
### skip layer  11 wait:  3  ###
---------------- start layer  12  ---------------
adv train loss:  -0.9116515312343836 , diff:  0.9116515312343836
adv train loss:  -0.9003605634788983 , diff:  0.011290967755485326
adv train loss:  -0.9939314413786633 , diff:  0.09357087789976504
adv train loss:  -0.8681201133294962 , diff:  0.12581132804916706
adv train loss:  -1.0896257397253066 , diff:  0.2215056263958104
adv train loss:  -0.8979510677454527 , diff:  0.19167467197985388
adv train loss:  -0.8201988004075247 , diff:  0.07775226733792806
adv train loss:  -1.0554107441857923 , diff:  0.2352119437782676
adv train loss:  -0.88858512376828 , diff:  0.16682562041751225
adv train loss:  -1.0896208959456999 , diff:  0.20103577217741986
layer  12  adv train finish, try to retain  494
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -0.5256133792208857 , diff:  0.5256133792208857
adv train loss:  -0.5652847629244206 , diff:  0.03967138370353496
adv train loss:  -0.4097496003523702 , diff:  0.15553516257205047
adv train loss:  -0.4360278045205632 , diff:  0.026278204168193042
adv train loss:  -0.5384660371055361 , diff:  0.10243823258497287
adv train loss:  -0.5439480838540476 , diff:  0.005482046748511493
layer  13  adv train finish, try to retain  496
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0  ==>  512 / 512 , inc:  1
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0.037109375  ==>  19 / 512 , inc:  2
layer  11  :  0  ==>  512 / 512 , inc:  1
layer  12  :  0.12109375  ==>  62 / 512 , inc:  2
layer  13  :  0.15234375  ==>  78 / 512 , inc:  1
eps [0.0016875000000000002, 0.0016875000000000002, 0.0016875000000000002, 0.0016875000000000002, 0.0016875000000000002, 0.0016875000000000002, 0.0016875000000000002, 0.00031640625000000006, 0.00031640625000000006, 0.00031640625000000006, 0.0016875000000000002, 0.00031640625000000006, 0.0016875000000000002, 0.0016875000000000002]  wait [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1]  tol: 2
$$$$$$$$$$$$$ epoch  6  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -0.5066112661006628 , diff:  0.5066112661006628
adv train loss:  -0.4938510406500427 , diff:  0.012760225450620055
adv train loss:  -0.421390316230827 , diff:  0.0724607244192157
adv train loss:  -0.46302961905894335 , diff:  0.04163930282811634
adv train loss:  -0.41307227277138736 , diff:  0.04995734628755599
adv train loss:  -0.5171169774985174 , diff:  0.10404470472713001
adv train loss:  -0.42150690549169667 , diff:  0.0956100720068207
adv train loss:  -0.5290414600603981 , diff:  0.10753455456870142
adv train loss:  -0.49674695332942065 , diff:  0.032294506730977446
adv train loss:  -0.4188973262789659 , diff:  0.07784962705045473
layer  0  adv train finish, try to retain  64
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -0.5547616503608879 , diff:  0.5547616503608879
adv train loss:  -0.5068163951218594 , diff:  0.047945255239028484
adv train loss:  -0.5027817238151329 , diff:  0.004034671306726523
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -0.4917674702446675 , diff:  0.4917674702446675
adv train loss:  -0.49979745704331435 , diff:  0.00802998679864686
layer  2  adv train finish, try to retain  128
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -0.3761872425384354 , diff:  0.3761872425384354
adv train loss:  -0.4779988200752996 , diff:  0.1018115775368642
adv train loss:  -0.5169180686061736 , diff:  0.03891924853087403
adv train loss:  -0.3719434807644575 , diff:  0.14497458784171613
adv train loss:  -0.5026037190546049 , diff:  0.1306602382901474
adv train loss:  -0.4718223304953426 , diff:  0.030781388559262268
adv train loss:  -0.5704478357074549 , diff:  0.09862550521211233
adv train loss:  -0.5358680789649952 , diff:  0.03457975674245972
adv train loss:  -0.4189578619261738 , diff:  0.11691021703882143
adv train loss:  -0.43093802516523283 , diff:  0.011980163239059038
layer  3  adv train finish, try to retain  128
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -0.5534039154881611 , diff:  0.5534039154881611
adv train loss:  -0.5787659774796339 , diff:  0.02536206199147273
adv train loss:  -0.46246770085417666 , diff:  0.11629827662545722
adv train loss:  -0.5179932169703534 , diff:  0.05552551611617673
adv train loss:  -0.5679656914726365 , diff:  0.04997247450228315
adv train loss:  -0.4074422760604648 , diff:  0.16052341541217174
adv train loss:  -0.53355960350018 , diff:  0.12611732743971515
adv train loss:  -0.5062883343198337 , diff:  0.02727126918034628
adv train loss:  -0.5236424934410024 , diff:  0.017354159121168777
adv train loss:  -0.5385021555412095 , diff:  0.01485966210020706
layer  4  adv train finish, try to retain  255
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -0.5226576318091247 , diff:  0.5226576318091247
adv train loss:  -0.6333852068346459 , diff:  0.11072757502552122
adv train loss:  -0.512514283938799 , diff:  0.12087092289584689
adv train loss:  -0.4287954585888656 , diff:  0.08371882534993347
adv train loss:  -0.5340283066325355 , diff:  0.10523284804366995
adv train loss:  -0.4509353992907563 , diff:  0.08309290734177921
adv train loss:  -0.43908162211300805 , diff:  0.01185377717774827
adv train loss:  -0.4821717321901815 , diff:  0.04309011007717345
adv train loss:  -0.5478199743665755 , diff:  0.06564824217639398
adv train loss:  -0.5037252963666106 , diff:  0.044094677999964915
layer  5  adv train finish, try to retain  256
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -0.4857537021889584 , diff:  0.4857537021889584
adv train loss:  -0.4275300838344265 , diff:  0.058223618354531936
adv train loss:  -0.5164199413120514 , diff:  0.0888898574776249
adv train loss:  -0.5279781790159177 , diff:  0.011558237703866325
adv train loss:  -0.5656579394126311 , diff:  0.03767976039671339
adv train loss:  -0.47968858006061055 , diff:  0.08596935935202055
adv train loss:  -0.4731867041627993 , diff:  0.00650187589781126
layer  6  adv train finish, try to retain  256
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -0.5339028557209531 , diff:  0.5339028557209531
adv train loss:  -0.5863835515046958 , diff:  0.052480695783742703
adv train loss:  -0.45178466592915356 , diff:  0.13459888557554223
adv train loss:  -0.3662762308085803 , diff:  0.08550843512057327
adv train loss:  -0.50445118083735 , diff:  0.13817495002876967
adv train loss:  -0.4778771908895578 , diff:  0.026573989947792143
adv train loss:  -0.48379458498675376 , diff:  0.005917394097195938
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  511
test acc: top1 ->  91.36 ; top5 ->  98.43  and loss:  78.51655948162079
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.09652998704405036
test acc: top1 ->  92.02 ; top5 ->  98.74  and loss:  75.34861479699612
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.04309763599849248
test acc: top1 ->  91.99 ; top5 ->  98.97  and loss:  75.84751145541668
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.05007109519738151
test acc: top1 ->  91.9 ; top5 ->  98.89  and loss:  77.30034493654966
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.03507995375821338
test acc: top1 ->  91.92 ; top5 ->  98.94  and loss:  78.15694245696068
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.05625561795568501
test acc: top1 ->  92.06 ; top5 ->  98.92  and loss:  78.36103703081608
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03043188341689529
test acc: top1 ->  91.98 ; top5 ->  98.93  and loss:  78.36418204009533
forward train acc: top1 ->  99.99199997558594 ; top5 ->  100.0  and loss:  0.032677425753718126
test acc: top1 ->  92.06 ; top5 ->  98.89  and loss:  79.06499356031418
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.021577188115770696
test acc: top1 ->  92.05 ; top5 ->  99.01  and loss:  77.40110573172569
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.02084828518491122
test acc: top1 ->  92.12 ; top5 ->  98.91  and loss:  77.58910347521305
==> this epoch:  511 / 512
---------------- start layer  8  ---------------
adv train loss:  -0.05518123488946003 , diff:  0.05518123488946003
adv train loss:  -0.026732303458629758 , diff:  0.02844893143083027
adv train loss:  -0.02327097059423977 , diff:  0.0034613328643899877
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  511
test acc: top1 ->  92.2 ; top5 ->  98.87  and loss:  77.52668303251266
==> this epoch:  511 / 512
---------------- start layer  9  ---------------
adv train loss:  -0.03882650510968233 , diff:  0.03882650510968233
adv train loss:  -0.02033562168799108 , diff:  0.01849088342169125
adv train loss:  -0.03222345864560339 , diff:  0.011887836957612308
adv train loss:  -0.03905114393637632 , diff:  0.0068276852907729335
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  511
test acc: top1 ->  92.11 ; top5 ->  98.93  and loss:  77.52428479492664
==> this epoch:  511 / 512
---------------- start layer  10  ---------------
adv train loss:  -0.12328940333100036 , diff:  0.12328940333100036
adv train loss:  -0.08427038743684534 , diff:  0.03901901589415502
adv train loss:  -0.12293010340363253 , diff:  0.03865971596678719
adv train loss:  -0.12207483916427009 , diff:  0.000855264239362441
layer  10  adv train finish, try to retain  489
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -0.07017067666129151 , diff:  0.07017067666129151
adv train loss:  -0.042373434069304494 , diff:  0.027797242591987015
adv train loss:  -0.04062570154928835 , diff:  0.001747732520016143
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  511
test acc: top1 ->  92.16 ; top5 ->  98.91  and loss:  77.94869083166122
==> this epoch:  511 / 512
---------------- start layer  12  ---------------
adv train loss:  -0.4461365676661444 , diff:  0.4461365676661444
adv train loss:  -0.3651381679928818 , diff:  0.08099839967326261
adv train loss:  -0.4249441093562609 , diff:  0.059805941363379134
adv train loss:  -0.4212337323970132 , diff:  0.0037103769592476965
layer  12  adv train finish, try to retain  480
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -0.02850974328839584 , diff:  0.02850974328839584
adv train loss:  -0.021515764056402986 , diff:  0.006993979231992853
layer  13  adv train finish, try to retain  488
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.998046875  ==>  511 / 512 , inc:  2
layer  8  :  0.998046875  ==>  511 / 512 , inc:  2
layer  9  :  0.998046875  ==>  511 / 512 , inc:  2
layer  10  :  0.037109375  ==>  19 / 512 , inc:  2
layer  11  :  0.998046875  ==>  511 / 512 , inc:  2
layer  12  :  0.12109375  ==>  62 / 512 , inc:  2
layer  13  :  0.15234375  ==>  78 / 512 , inc:  1
eps [0.0033750000000000004, 0.0033750000000000004, 0.0033750000000000004, 0.0033750000000000004, 0.0033750000000000004, 0.0033750000000000004, 0.0033750000000000004, 0.00031640625000000006, 0.00031640625000000006, 0.00031640625000000006, 0.0033750000000000004, 0.00031640625000000006, 0.0033750000000000004, 0.0033750000000000004]  wait [2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2]  inc [1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1]  tol: 2
$$$$$$$$$$$$$ epoch  7  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -0.04567053793789455 , diff:  0.04567053793789455
adv train loss:  -0.022231439778806816 , diff:  0.023439098159087735
adv train loss:  -0.045017309646937065 , diff:  0.02278586986813025
adv train loss:  -0.027338403604517225 , diff:  0.01767890604241984
adv train loss:  -0.024056889287749073 , diff:  0.0032815143167681526
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  31
test acc: top1 ->  37.06 ; top5 ->  78.18  and loss:  711.4877824783325
forward train acc: top1 ->  94.35999997314453 ; top5 ->  99.644  and loss:  27.013343811035156
test acc: top1 ->  81.55 ; top5 ->  96.7  and loss:  104.70829993486404
forward train acc: top1 ->  95.53800001464843 ; top5 ->  99.788  and loss:  14.347337409853935
test acc: top1 ->  88.26 ; top5 ->  98.79  and loss:  49.74385739862919
forward train acc: top1 ->  96.15800000976563 ; top5 ->  99.842  and loss:  12.20600737631321
test acc: top1 ->  88.67 ; top5 ->  98.75  and loss:  47.863545283675194
forward train acc: top1 ->  96.85600000976562 ; top5 ->  99.9  and loss:  9.77312158048153
test acc: top1 ->  89.06 ; top5 ->  98.93  and loss:  46.43371370434761
forward train acc: top1 ->  97.14999999023438 ; top5 ->  99.894  and loss:  8.662677343934774
test acc: top1 ->  89.08 ; top5 ->  98.91  and loss:  46.39084404706955
forward train acc: top1 ->  97.42599998291016 ; top5 ->  99.926  and loss:  7.954945009201765
test acc: top1 ->  89.32 ; top5 ->  98.97  and loss:  46.105340003967285
forward train acc: top1 ->  97.50999999023438 ; top5 ->  99.938  and loss:  7.45891329459846
test acc: top1 ->  89.54 ; top5 ->  98.99  and loss:  45.4175606071949
forward train acc: top1 ->  97.68000000976562 ; top5 ->  99.95  and loss:  7.030728381127119
test acc: top1 ->  89.42 ; top5 ->  99.0  and loss:  46.18379205465317
forward train acc: top1 ->  97.78200001464843 ; top5 ->  99.948  and loss:  6.660539574921131
test acc: top1 ->  89.55 ; top5 ->  99.05  and loss:  46.26312054693699
forward train acc: top1 ->  97.91999998535157 ; top5 ->  99.95  and loss:  6.231489343568683
test acc: top1 ->  89.61 ; top5 ->  99.03  and loss:  46.0471001714468
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -0.497356082778424 , diff:  0.497356082778424
adv train loss:  -0.5117993599269539 , diff:  0.014443277148529887
adv train loss:  -0.49745624838396907 , diff:  0.014343111542984843
adv train loss:  -0.5098966317018494 , diff:  0.012440383317880332
adv train loss:  -0.497221089201048 , diff:  0.012675542500801384
adv train loss:  -0.47033638099674135 , diff:  0.026884708204306662
adv train loss:  -0.48328283126465976 , diff:  0.012946450267918408
adv train loss:  -0.5231950009474531 , diff:  0.03991216968279332
adv train loss:  -0.49835981195792556 , diff:  0.024835188989527524
adv train loss:  -0.5298672942444682 , diff:  0.031507482286542654
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -0.5029335783328861 , diff:  0.5029335783328861
adv train loss:  -0.4662559060379863 , diff:  0.03667767229489982
adv train loss:  -0.5134578250581399 , diff:  0.04720191902015358
adv train loss:  -0.4935123671311885 , diff:  0.01994545792695135
adv train loss:  -0.4509889318142086 , diff:  0.042523435316979885
adv train loss:  -0.48479451902676374 , diff:  0.03380558721255511
adv train loss:  -0.5106847662245855 , diff:  0.025890247197821736
adv train loss:  -0.5036266064271331 , diff:  0.00705815979745239
layer  2  adv train finish, try to retain  128
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -0.49026467942167073 , diff:  0.49026467942167073
adv train loss:  -0.4878321927972138 , diff:  0.002432486624456942
layer  3  adv train finish, try to retain  128
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -0.4806156218983233 , diff:  0.4806156218983233
adv train loss:  -0.4777518273331225 , diff:  0.0028637945652008057
layer  4  adv train finish, try to retain  255
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -0.5015303228283301 , diff:  0.5015303228283301
adv train loss:  -0.5222028989810497 , diff:  0.020672576152719557
adv train loss:  -0.5191290262155235 , diff:  0.0030738727655261755
layer  5  adv train finish, try to retain  256
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -0.4804994611768052 , diff:  0.4804994611768052
adv train loss:  -0.4870041310787201 , diff:  0.006504669901914895
************ all values are small in this layer **********
layer  6  adv train finish, try to retain  143
test acc: top1 ->  87.92 ; top5 ->  98.85  and loss:  46.9759900867939
forward train acc: top1 ->  99.55400000244141 ; top5 ->  100.0  and loss:  1.4278784455964342
test acc: top1 ->  91.17 ; top5 ->  99.04  and loss:  49.27281439304352
forward train acc: top1 ->  99.67 ; top5 ->  99.998  and loss:  0.9546624809736386
test acc: top1 ->  91.4 ; top5 ->  99.12  and loss:  51.378913164138794
forward train acc: top1 ->  99.724 ; top5 ->  100.0  and loss:  0.7534191262675449
test acc: top1 ->  91.28 ; top5 ->  99.05  and loss:  54.20676127076149
forward train acc: top1 ->  99.814 ; top5 ->  100.0  and loss:  0.6173671461292543
test acc: top1 ->  91.41 ; top5 ->  99.1  and loss:  55.871767446398735
forward train acc: top1 ->  99.862 ; top5 ->  100.0  and loss:  0.4535971222794615
test acc: top1 ->  91.51 ; top5 ->  99.12  and loss:  58.61036965996027
forward train acc: top1 ->  99.82999997558593 ; top5 ->  100.0  and loss:  0.48134343975107186
test acc: top1 ->  91.57 ; top5 ->  99.13  and loss:  58.501785188913345
forward train acc: top1 ->  99.85999997558594 ; top5 ->  100.0  and loss:  0.45101258117938414
test acc: top1 ->  91.53 ; top5 ->  99.11  and loss:  58.23926045000553
forward train acc: top1 ->  99.87 ; top5 ->  100.0  and loss:  0.40638450063124765
test acc: top1 ->  91.54 ; top5 ->  99.11  and loss:  58.89381114393473
forward train acc: top1 ->  99.874 ; top5 ->  100.0  and loss:  0.3843231748905964
test acc: top1 ->  91.43 ; top5 ->  99.13  and loss:  60.77509792894125
forward train acc: top1 ->  99.88999997558594 ; top5 ->  100.0  and loss:  0.3262630839162739
test acc: top1 ->  91.54 ; top5 ->  99.08  and loss:  61.96350786834955
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  144 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -0.17376864812104031 , diff:  0.17376864812104031
adv train loss:  -0.12897254018025706 , diff:  0.04479610794078326
adv train loss:  -0.17560424020484788 , diff:  0.04663170002459083
adv train loss:  -0.14394934083975386 , diff:  0.031654899365094025
adv train loss:  -0.19862749628373422 , diff:  0.05467815544398036
adv train loss:  -0.15983834727376234 , diff:  0.03878914900997188
adv train loss:  -0.1376362102892017 , diff:  0.02220213698456064
adv train loss:  -0.17741717437456828 , diff:  0.03978096408536658
adv train loss:  -0.1605679025669815 , diff:  0.016849271807586774
adv train loss:  -0.18110792739753379 , diff:  0.020540024830552284
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  509
test acc: top1 ->  91.71 ; top5 ->  99.21  and loss:  60.37696448713541
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.07268093915627105
test acc: top1 ->  92.03 ; top5 ->  99.09  and loss:  65.6383783146739
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.05664849100867286
test acc: top1 ->  92.14 ; top5 ->  99.17  and loss:  65.85259322822094
==> this epoch:  509 / 512
---------------- start layer  8  ---------------
adv train loss:  -0.03573394841623667 , diff:  0.03573394841623667
adv train loss:  -0.054653597457217984 , diff:  0.018919649040981312
adv train loss:  -0.04268981047607667 , diff:  0.011963786981141311
adv train loss:  -0.06164609914594621 , diff:  0.018956288669869537
adv train loss:  -0.04339449883082125 , diff:  0.01825160031512496
adv train loss:  -0.03622060526868154 , diff:  0.00717389356213971
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  509
test acc: top1 ->  92.1 ; top5 ->  99.21  and loss:  65.90366093069315
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.032843083713032684
test acc: top1 ->  92.11 ; top5 ->  99.08  and loss:  70.49262216687202
==> this epoch:  509 / 512
---------------- start layer  9  ---------------
adv train loss:  -0.059058700190689706 , diff:  0.059058700190689706
adv train loss:  -0.04437766044065938 , diff:  0.014681039750030322
adv train loss:  -0.026259251433657482 , diff:  0.0181184090070019
adv train loss:  -0.03760568482493909 , diff:  0.011346433391281607
adv train loss:  -0.06989008291930077 , diff:  0.03228439809436168
adv train loss:  -0.033798929051954474 , diff:  0.03609115386734629
adv train loss:  -0.026930662678296358 , diff:  0.006868266373658116
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  509
test acc: top1 ->  92.04 ; top5 ->  99.13  and loss:  70.34148979187012
forward train acc: top1 ->  99.9840000024414 ; top5 ->  100.0  and loss:  0.05810190541888005
test acc: top1 ->  92.14 ; top5 ->  99.09  and loss:  72.4024855569005
==> this epoch:  509 / 512
---------------- start layer  10  ---------------
adv train loss:  -0.13614675786084263 , diff:  0.13614675786084263
adv train loss:  -0.1449426850740565 , diff:  0.008795927213213872
layer  10  adv train finish, try to retain  489
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -0.03922859906015219 , diff:  0.03922859906015219
adv train loss:  -0.048819732554420625 , diff:  0.009591133494268433
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  509
test acc: top1 ->  92.23 ; top5 ->  99.06  and loss:  72.1415898501873
==> this epoch:  509 / 512
---------------- start layer  12  ---------------
adv train loss:  -0.5667873668389802 , diff:  0.5667873668389802
adv train loss:  -0.5813746421408723 , diff:  0.014587275301892078
adv train loss:  -0.6200318254523154 , diff:  0.03865718331144308
adv train loss:  -0.4563963273722038 , diff:  0.16363549808011157
adv train loss:  -0.596472697237914 , diff:  0.14007636986571015
adv train loss:  -0.4470326346527145 , diff:  0.14944006258519948
adv train loss:  -0.5387325830233749 , diff:  0.09169994837066042
adv train loss:  -0.5806921532384877 , diff:  0.04195957021511276
adv train loss:  -0.5977422798168845 , diff:  0.017050126578396885
adv train loss:  -0.42860891134114354 , diff:  0.169133368475741
layer  12  adv train finish, try to retain  491
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -0.020373517285406706 , diff:  0.020373517285406706
adv train loss:  -0.05650099685180976 , diff:  0.03612747956640305
adv train loss:  -0.0341012938545191 , diff:  0.02239970299729066
adv train loss:  -0.03794847454992123 , diff:  0.0038471806954021304
layer  13  adv train finish, try to retain  489
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.994140625  ==>  509 / 512 , inc:  4
layer  8  :  0.994140625  ==>  509 / 512 , inc:  4
layer  9  :  0.994140625  ==>  509 / 512 , inc:  4
layer  10  :  0.037109375  ==>  19 / 512 , inc:  2
layer  11  :  0.994140625  ==>  509 / 512 , inc:  4
layer  12  :  0.12109375  ==>  62 / 512 , inc:  2
layer  13  :  0.15234375  ==>  78 / 512 , inc:  1
eps [0.0025312500000000005, 0.006750000000000001, 0.006750000000000001, 0.006750000000000001, 0.006750000000000001, 0.006750000000000001, 0.0025312500000000005, 0.00031640625000000006, 0.00031640625000000006, 0.00031640625000000006, 0.006750000000000001, 0.00031640625000000006, 0.006750000000000001, 0.006750000000000001]  wait [4, 2, 2, 2, 2, 2, 4, 0, 0, 0, 0, 0, 0, 2]  inc [1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 2, 4, 2, 1]  tol: 2
$$$$$$$$$$$$$ epoch  8  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
adv train loss:  -0.03673676641210477 , diff:  0.03673676641210477
adv train loss:  -0.02995679071318591 , diff:  0.006779975698918861
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  54
test acc: top1 ->  89.47 ; top5 ->  98.59  and loss:  98.50947944819927
forward train acc: top1 ->  99.748 ; top5 ->  99.996  and loss:  0.8685489666167996
test acc: top1 ->  91.55 ; top5 ->  99.07  and loss:  70.20478631556034
forward train acc: top1 ->  99.81000000244141 ; top5 ->  99.998  and loss:  0.604239439155208
test acc: top1 ->  91.57 ; top5 ->  99.12  and loss:  66.06813430413604
forward train acc: top1 ->  99.822 ; top5 ->  100.0  and loss:  0.5141611969447695
test acc: top1 ->  91.75 ; top5 ->  99.16  and loss:  62.52799807861447
forward train acc: top1 ->  99.87199997558594 ; top5 ->  100.0  and loss:  0.3581812981865369
test acc: top1 ->  91.79 ; top5 ->  99.22  and loss:  62.56289241835475
forward train acc: top1 ->  99.878 ; top5 ->  100.0  and loss:  0.33062607476313133
test acc: top1 ->  91.88 ; top5 ->  99.18  and loss:  62.71605494618416
forward train acc: top1 ->  99.86599997558594 ; top5 ->  100.0  and loss:  0.39165218110429123
test acc: top1 ->  91.88 ; top5 ->  99.21  and loss:  62.7868497222662
forward train acc: top1 ->  99.89999997558594 ; top5 ->  99.998  and loss:  0.2783770878886571
test acc: top1 ->  91.82 ; top5 ->  99.2  and loss:  62.15049133077264
forward train acc: top1 ->  99.928 ; top5 ->  100.0  and loss:  0.24467613294837065
test acc: top1 ->  91.76 ; top5 ->  99.24  and loss:  62.58309731259942
forward train acc: top1 ->  99.918 ; top5 ->  100.0  and loss:  0.24240506488422398
test acc: top1 ->  91.8 ; top5 ->  99.21  and loss:  63.613881915807724
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.22350439970614389
test acc: top1 ->  91.89 ; top5 ->  99.23  and loss:  64.14995050430298
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  55 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -0.06679723376691982 , diff:  0.06679723376691982
adv train loss:  -0.07829058205061301 , diff:  0.011493348283693194
adv train loss:  -0.0486537297074392 , diff:  0.029636852343173814
adv train loss:  -0.07680501254344563 , diff:  0.02815128283600643
adv train loss:  -0.04912213766147033 , diff:  0.027682874881975295
adv train loss:  -0.05777998278972518 , diff:  0.008657845128254849
************ all values are small in this layer **********
layer  2  adv train finish, try to retain  99
test acc: top1 ->  79.18 ; top5 ->  96.62  and loss:  177.40867042541504
forward train acc: top1 ->  99.54599997558594 ; top5 ->  99.996  and loss:  1.5298196229850873
test acc: top1 ->  90.95 ; top5 ->  99.21  and loss:  62.64794081449509
forward train acc: top1 ->  99.656 ; top5 ->  100.0  and loss:  1.061952896998264
test acc: top1 ->  91.1 ; top5 ->  99.19  and loss:  59.91694211959839
forward train acc: top1 ->  99.73999997558593 ; top5 ->  99.998  and loss:  0.8152720049256459
test acc: top1 ->  91.2 ; top5 ->  99.22  and loss:  58.84610012173653
forward train acc: top1 ->  99.76199997802735 ; top5 ->  99.998  and loss:  0.7253242280567065
test acc: top1 ->  91.39 ; top5 ->  99.22  and loss:  60.67211492359638
forward train acc: top1 ->  99.75799997558593 ; top5 ->  99.998  and loss:  0.7500850044016261
test acc: top1 ->  91.32 ; top5 ->  99.22  and loss:  59.46024429798126
forward train acc: top1 ->  99.804 ; top5 ->  100.0  and loss:  0.6239302647300065
test acc: top1 ->  91.37 ; top5 ->  99.28  and loss:  58.372979179024696
forward train acc: top1 ->  99.786 ; top5 ->  99.998  and loss:  0.6105851359898224
test acc: top1 ->  91.36 ; top5 ->  99.26  and loss:  59.17833510041237
forward train acc: top1 ->  99.82 ; top5 ->  99.998  and loss:  0.5497065600357018
test acc: top1 ->  91.41 ; top5 ->  99.22  and loss:  59.195706740021706
forward train acc: top1 ->  99.80999997558594 ; top5 ->  100.0  and loss:  0.5527227171696723
test acc: top1 ->  91.45 ; top5 ->  99.28  and loss:  59.16446603834629
forward train acc: top1 ->  99.84399997558593 ; top5 ->  100.0  and loss:  0.4542250145750586
test acc: top1 ->  91.44 ; top5 ->  99.23  and loss:  59.54158541560173
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -0.08315860157017596 , diff:  0.08315860157017596
adv train loss:  -0.08190628040028969 , diff:  0.0012523211698862724
************ all values are small in this layer **********
layer  3  adv train finish, try to retain  86
test acc: top1 ->  82.22 ; top5 ->  97.67  and loss:  127.2060170173645
forward train acc: top1 ->  98.33000000976563 ; top5 ->  99.966  and loss:  5.723679531365633
test acc: top1 ->  89.75 ; top5 ->  98.94  and loss:  54.113198444247246
forward train acc: top1 ->  98.73599998291016 ; top5 ->  99.988  and loss:  3.769518113695085
test acc: top1 ->  90.12 ; top5 ->  99.0  and loss:  49.9477606639266
forward train acc: top1 ->  98.8460000024414 ; top5 ->  99.984  and loss:  3.2959074396640062
test acc: top1 ->  90.32 ; top5 ->  99.07  and loss:  48.896297320723534
forward train acc: top1 ->  99.06800000488282 ; top5 ->  99.994  and loss:  2.7205937169492245
test acc: top1 ->  90.34 ; top5 ->  99.05  and loss:  49.135553039610386
forward train acc: top1 ->  99.19400000488281 ; top5 ->  99.986  and loss:  2.514425734989345
test acc: top1 ->  90.46 ; top5 ->  99.06  and loss:  49.477220483124256
forward train acc: top1 ->  99.1480000024414 ; top5 ->  99.99  and loss:  2.4588655941188335
test acc: top1 ->  90.55 ; top5 ->  99.12  and loss:  48.8466612175107
forward train acc: top1 ->  99.27799997802734 ; top5 ->  99.998  and loss:  2.1042093806900084
test acc: top1 ->  90.58 ; top5 ->  99.16  and loss:  49.319527707993984
forward train acc: top1 ->  99.34 ; top5 ->  99.99  and loss:  2.060449102311395
test acc: top1 ->  90.57 ; top5 ->  99.11  and loss:  50.30132024735212
forward train acc: top1 ->  99.29799997558594 ; top5 ->  99.996  and loss:  2.004420199431479
test acc: top1 ->  90.69 ; top5 ->  99.11  and loss:  50.77270218729973
forward train acc: top1 ->  99.28999997558594 ; top5 ->  99.994  and loss:  2.010050696786493
test acc: top1 ->  90.6 ; top5 ->  99.12  and loss:  50.50615679472685
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -0.27821628854144365 , diff:  0.27821628854144365
adv train loss:  -0.22218998018070124 , diff:  0.056026308360742405
adv train loss:  -0.25377804480376653 , diff:  0.03158806462306529
adv train loss:  -0.26365554667427205 , diff:  0.009877501870505512
************ all values are small in this layer **********
layer  4  adv train finish, try to retain  154
test acc: top1 ->  48.8 ; top5 ->  81.03  and loss:  255.20897221565247
forward train acc: top1 ->  94.99000001953125 ; top5 ->  99.86399997558594  and loss:  15.80288141220808
test acc: top1 ->  88.02 ; top5 ->  98.92  and loss:  50.52392703294754
forward train acc: top1 ->  96.12200001220702 ; top5 ->  99.92  and loss:  11.621507663279772
test acc: top1 ->  88.49 ; top5 ->  99.06  and loss:  47.57011507451534
forward train acc: top1 ->  96.47600001953126 ; top5 ->  99.954  and loss:  10.390805684030056
test acc: top1 ->  88.9 ; top5 ->  99.04  and loss:  46.466341093182564
forward train acc: top1 ->  96.69599998779297 ; top5 ->  99.954  and loss:  9.454217240214348
test acc: top1 ->  89.03 ; top5 ->  99.05  and loss:  45.36598299443722
forward train acc: top1 ->  97.08400000976563 ; top5 ->  99.962  and loss:  8.449813969433308
test acc: top1 ->  89.1 ; top5 ->  99.07  and loss:  45.6844616830349
forward train acc: top1 ->  97.14000001220703 ; top5 ->  99.978  and loss:  8.119249742478132
test acc: top1 ->  89.29 ; top5 ->  99.04  and loss:  45.80802363157272
forward train acc: top1 ->  97.19599998535156 ; top5 ->  99.964  and loss:  8.120361533015966
test acc: top1 ->  89.43 ; top5 ->  99.06  and loss:  45.72727155685425
forward train acc: top1 ->  97.25799997802734 ; top5 ->  99.962  and loss:  7.8099868260324
test acc: top1 ->  89.37 ; top5 ->  99.05  and loss:  45.22943493723869
forward train acc: top1 ->  97.37999998535156 ; top5 ->  99.95  and loss:  7.555326044559479
test acc: top1 ->  89.46 ; top5 ->  99.05  and loss:  44.863712921738625
forward train acc: top1 ->  97.42599998535157 ; top5 ->  99.976  and loss:  7.418800473213196
test acc: top1 ->  89.53 ; top5 ->  99.06  and loss:  45.37109316885471
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -0.6582117946818471 , diff:  0.6582117946818471
adv train loss:  -0.5936283126939088 , diff:  0.06458348198793828
adv train loss:  -0.6090450461488217 , diff:  0.015416733454912901
adv train loss:  -0.6506111905910075 , diff:  0.04156614444218576
adv train loss:  -0.6050107616465539 , diff:  0.0456004289444536
adv train loss:  -0.6367151134181768 , diff:  0.031704351771622896
adv train loss:  -0.6199048149865121 , diff:  0.016810298431664705
adv train loss:  -0.6685916816350073 , diff:  0.0486868666484952
adv train loss:  -0.6359682376496494 , diff:  0.03262344398535788
adv train loss:  -0.6455649631097913 , diff:  0.009596725460141897
************ all values are small in this layer **********
layer  5  adv train finish, try to retain  148
test acc: top1 ->  68.32 ; top5 ->  89.65  and loss:  153.77484726905823
forward train acc: top1 ->  98.11199998535156 ; top5 ->  99.976  and loss:  5.711508771404624
test acc: top1 ->  89.65 ; top5 ->  99.12  and loss:  49.032466016709805
forward train acc: top1 ->  98.42000000732422 ; top5 ->  99.992  and loss:  4.41434290446341
test acc: top1 ->  89.97 ; top5 ->  99.1  and loss:  49.491937443614006
forward train acc: top1 ->  98.75600000488281 ; top5 ->  99.996  and loss:  3.6116267535835505
test acc: top1 ->  89.97 ; top5 ->  99.12  and loss:  49.39448185265064
forward train acc: top1 ->  98.85200000732422 ; top5 ->  99.988  and loss:  3.4027297040447593
test acc: top1 ->  90.13 ; top5 ->  99.15  and loss:  50.98892021179199
forward train acc: top1 ->  99.01999997558593 ; top5 ->  99.994  and loss:  2.7899864856153727
test acc: top1 ->  90.17 ; top5 ->  99.15  and loss:  51.817029647529125
forward train acc: top1 ->  99.04799998046875 ; top5 ->  99.996  and loss:  2.736542713828385
test acc: top1 ->  90.21 ; top5 ->  99.15  and loss:  52.12650477141142
forward train acc: top1 ->  98.99800000488281 ; top5 ->  99.994  and loss:  2.857502792030573
test acc: top1 ->  90.27 ; top5 ->  99.13  and loss:  51.814214281737804
forward train acc: top1 ->  99.07599997558594 ; top5 ->  99.998  and loss:  2.6195644098334014
test acc: top1 ->  90.35 ; top5 ->  99.2  and loss:  51.02681903541088
forward train acc: top1 ->  99.10999997802735 ; top5 ->  99.998  and loss:  2.523971423972398
test acc: top1 ->  90.35 ; top5 ->  99.17  and loss:  51.66138315200806
forward train acc: top1 ->  99.21799997802735 ; top5 ->  99.996  and loss:  2.2784538785926998
test acc: top1 ->  90.27 ; top5 ->  99.18  and loss:  52.815014116466045
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
adv train loss:  -0.3403934437665157 , diff:  0.3403934437665157
adv train loss:  -0.35109821968944743 , diff:  0.01070477592293173
adv train loss:  -0.3739550831960514 , diff:  0.022856863506603986
adv train loss:  -0.327396824315656 , diff:  0.04655825888039544
adv train loss:  -0.3528197345731314 , diff:  0.02542291025747545
adv train loss:  -0.3416126377414912 , diff:  0.011207096831640229
adv train loss:  -0.3823598496383056 , diff:  0.040747211896814406
adv train loss:  -0.3119100716430694 , diff:  0.07044977799523622
adv train loss:  -0.3262872200575657 , diff:  0.014377148414496332
adv train loss:  -0.36270793160656467 , diff:  0.03642071154899895
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  505
test acc: top1 ->  91.74 ; top5 ->  99.25  and loss:  45.250665582716465
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.16614146093343152
test acc: top1 ->  92.0 ; top5 ->  99.27  and loss:  53.754004027694464
forward train acc: top1 ->  99.9620000024414 ; top5 ->  100.0  and loss:  0.13390670371882152
test acc: top1 ->  92.08 ; top5 ->  99.25  and loss:  57.4549464173615
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.09354502598944237
test acc: top1 ->  92.21 ; top5 ->  99.29  and loss:  56.670708268880844
==> this epoch:  505 / 512
---------------- start layer  8  ---------------
adv train loss:  -0.035305274974234635 , diff:  0.035305274974234635
adv train loss:  -0.04056850136112189 , diff:  0.005263226386887254
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  505
test acc: top1 ->  92.26 ; top5 ->  99.3  and loss:  56.85312011837959
==> this epoch:  505 / 512
---------------- start layer  9  ---------------
adv train loss:  -0.04714550237986259 , diff:  0.04714550237986259
adv train loss:  -0.037484561289602425 , diff:  0.009660941090260167
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  505
test acc: top1 ->  92.26 ; top5 ->  99.25  and loss:  57.15783302485943
==> this epoch:  505 / 512
---------------- start layer  10  ---------------
adv train loss:  -0.23077322679455392 , diff:  0.23077322679455392
adv train loss:  -0.2235280232707737 , diff:  0.007245203523780219
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  17
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  900.4536728858948
forward train acc: top1 ->  9.880000001831055 ; top5 ->  50.140000001220706  and loss:  441.1748082637787
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  389.82330417633057
forward train acc: top1 ->  10.086000000915528 ; top5 ->  50.22599998657227  and loss:  226.39442133903503
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.5785961151123
forward train acc: top1 ->  9.84600000076294 ; top5 ->  49.966000002441405  and loss:  225.94252467155457
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.80405044555664
forward train acc: top1 ->  9.869999999542236 ; top5 ->  49.766  and loss:  225.89100337028503
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.70452117919922
forward train acc: top1 ->  10.086000000915528 ; top5 ->  49.8980000012207  and loss:  225.8900065422058
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.42053294181824
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  19 / 512 , inc:  2
---------------- start layer  11  ---------------
adv train loss:  -1.4034457972738892 , diff:  1.4034457972738892
adv train loss:  -1.518861294258386 , diff:  0.11541549698449671
adv train loss:  -1.4549743616953492 , diff:  0.06388693256303668
adv train loss:  -1.354136561974883 , diff:  0.10083779972046614
adv train loss:  -1.5287180808372796 , diff:  0.17458151886239648
adv train loss:  -1.3795942530268803 , diff:  0.14912382781039923
adv train loss:  -1.4291513389907777 , diff:  0.04955708596389741
adv train loss:  -1.4106716108508408 , diff:  0.018479728139936924
adv train loss:  -1.4034327189438045 , diff:  0.0072388919070363045
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  505
test acc: top1 ->  91.32 ; top5 ->  99.11  and loss:  60.8016954138875
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.10361413608916337
test acc: top1 ->  92.16 ; top5 ->  99.28  and loss:  58.90191336721182
==> this epoch:  505 / 512
---------------- start layer  12  ---------------
adv train loss:  -0.7739342675049556 , diff:  0.7739342675049556
adv train loss:  -0.7251431145996321 , diff:  0.04879115290532354
adv train loss:  -0.9322162088064943 , diff:  0.20707309420686215
adv train loss:  -0.7102293575590011 , diff:  0.22198685124749318
adv train loss:  -0.9013963761099149 , diff:  0.19116701855091378
adv train loss:  -0.8708522082451964 , diff:  0.030544167864718474
adv train loss:  -0.8525291962432675 , diff:  0.018323012001928873
adv train loss:  -0.8408163814165164 , diff:  0.011712814826751128
adv train loss:  -0.6981734692308237 , diff:  0.1426429121856927
adv train loss:  -0.7145091295678867 , diff:  0.016335660337063018
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  60
test acc: top1 ->  16.44 ; top5 ->  67.75  and loss:  452.2700889110565
forward train acc: top1 ->  59.43000000732422 ; top5 ->  89.282  and loss:  162.62595543265343
test acc: top1 ->  87.37 ; top5 ->  98.62  and loss:  66.13000056147575
forward train acc: top1 ->  99.616 ; top5 ->  99.998  and loss:  31.066501036286354
test acc: top1 ->  90.87 ; top5 ->  98.46  and loss:  46.74281373620033
forward train acc: top1 ->  99.888 ; top5 ->  100.0  and loss:  16.018874883651733
test acc: top1 ->  91.34 ; top5 ->  98.47  and loss:  39.180393144488335
forward train acc: top1 ->  99.93199997558594 ; top5 ->  100.0  and loss:  8.965174362063408
test acc: top1 ->  91.73 ; top5 ->  98.49  and loss:  36.07143811136484
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  5.459363639354706
test acc: top1 ->  91.77 ; top5 ->  98.4  and loss:  35.16095645725727
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  3.9420403987169266
test acc: top1 ->  91.93 ; top5 ->  98.42  and loss:  35.15158921480179
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  3.2219166830182076
test acc: top1 ->  91.8 ; top5 ->  98.36  and loss:  35.1719269528985
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  2.6577741857618093
test acc: top1 ->  91.93 ; top5 ->  98.38  and loss:  35.42143951356411
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  2.238893698900938
test acc: top1 ->  91.88 ; top5 ->  98.34  and loss:  35.68032314628363
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  1.8587158843874931
test acc: top1 ->  91.75 ; top5 ->  98.33  and loss:  35.92915812507272
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  62 / 512 , inc:  2
---------------- start layer  13  ---------------
adv train loss:  -2.2187821096740663 , diff:  2.2187821096740663
adv train loss:  -1.9131261233706027 , diff:  0.3056559863034636
adv train loss:  -1.91301951254718 , diff:  0.00010661082342267036
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  77
test acc: top1 ->  65.02 ; top5 ->  97.9  and loss:  154.20114010572433
forward train acc: top1 ->  97.15799997558594 ; top5 ->  99.986  and loss:  10.899288268759847
test acc: top1 ->  91.75 ; top5 ->  98.92  and loss:  39.816751416772604
forward train acc: top1 ->  99.942 ; top5 ->  100.0  and loss:  0.8695852276869118
test acc: top1 ->  92.04 ; top5 ->  98.95  and loss:  40.09366002678871
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.486290194792673
test acc: top1 ->  91.97 ; top5 ->  98.93  and loss:  41.472912669181824
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.3294939248589799
test acc: top1 ->  92.03 ; top5 ->  98.95  and loss:  42.552621714770794
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.2657398380106315
test acc: top1 ->  92.04 ; top5 ->  98.96  and loss:  43.95239393413067
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.23221384757198393
test acc: top1 ->  92.01 ; top5 ->  98.93  and loss:  44.26967941224575
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.1765230483142659
test acc: top1 ->  91.97 ; top5 ->  98.97  and loss:  45.11099521815777
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.16121032391674817
test acc: top1 ->  91.98 ; top5 ->  98.95  and loss:  45.63548345118761
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.1558229747461155
test acc: top1 ->  92.05 ; top5 ->  98.94  and loss:  45.67755796760321
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.13727701507741585
test acc: top1 ->  92.04 ; top5 ->  98.96  and loss:  46.718019746243954
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  78 / 512 , inc:  1
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.986328125  ==>  505 / 512 , inc:  8
layer  8  :  0.986328125  ==>  505 / 512 , inc:  8
layer  9  :  0.986328125  ==>  505 / 512 , inc:  8
layer  10  :  0.037109375  ==>  19 / 512 , inc:  1
layer  11  :  0.986328125  ==>  505 / 512 , inc:  8
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.15234375  ==>  78 / 512 , inc:  1
eps [0.0025312500000000005, 0.005062500000000001, 0.005062500000000001, 0.005062500000000001, 0.005062500000000001, 0.005062500000000001, 0.0025312500000000005, 0.00031640625000000006, 0.00031640625000000006, 0.00031640625000000006, 0.005062500000000001, 0.00031640625000000006, 0.005062500000000001, 0.005062500000000001]  wait [3, 4, 4, 4, 4, 4, 3, 0, 0, 0, 2, 0, 2, 4]  inc [1, 1, 1, 1, 1, 1, 1, 8, 8, 8, 1, 8, 1, 1]  tol: 2
$$$$$$$$$$$$$ epoch  9  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
### skip layer  1 wait:  4  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  3  ###
---------------- start layer  7  ---------------
adv train loss:  -0.08868577389739585 , diff:  0.08868577389739585
adv train loss:  -0.05351289300779172 , diff:  0.03517288088960413
adv train loss:  -0.07215276440183516 , diff:  0.01863987139404344
adv train loss:  -0.07533754130963644 , diff:  0.003184776907801279
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  497
test acc: top1 ->  92.02 ; top5 ->  98.71  and loss:  101.48323506116867
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.04724526298559795
test acc: top1 ->  92.24 ; top5 ->  98.9  and loss:  98.14350408315659
==> this epoch:  497 / 512
---------------- start layer  8  ---------------
adv train loss:  -0.05067089484555254 , diff:  0.05067089484555254
adv train loss:  -0.057990795851992516 , diff:  0.007319901006439977
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  497
test acc: top1 ->  92.23 ; top5 ->  98.9  and loss:  98.50538052618504
==> this epoch:  497 / 512
---------------- start layer  9  ---------------
adv train loss:  -0.044055769873921236 , diff:  0.044055769873921236
adv train loss:  -0.05786507740435809 , diff:  0.013809307530436854
adv train loss:  -0.056270018832947244 , diff:  0.0015950585714108456
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  497
test acc: top1 ->  92.14 ; top5 ->  98.89  and loss:  98.69742649793625
==> this epoch:  497 / 512
---------------- start layer  10  ---------------
adv train loss:  -0.05164361936795103 , diff:  0.05164361936795103
adv train loss:  -0.031724442173981515 , diff:  0.019919177193969517
adv train loss:  -0.04787578378522994 , diff:  0.016151341611248426
adv train loss:  -0.040946692703073495 , diff:  0.006929091082156447
layer  10  adv train finish, try to retain  491
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -0.024676548922798247 , diff:  0.024676548922798247
adv train loss:  -0.055415163765019315 , diff:  0.030738614842221068
adv train loss:  -0.04571084224335209 , diff:  0.009704321521667225
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  497
test acc: top1 ->  92.07 ; top5 ->  98.94  and loss:  98.1948684155941
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.038245318763415526
test acc: top1 ->  92.02 ; top5 ->  98.96  and loss:  95.61670085787773
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.017519694942166097
test acc: top1 ->  92.12 ; top5 ->  98.9  and loss:  95.264065310359
==> this epoch:  497 / 512
---------------- start layer  12  ---------------
adv train loss:  -0.13227379973977804 , diff:  0.13227379973977804
adv train loss:  -0.09801003599932301 , diff:  0.03426376374045503
adv train loss:  -0.08983634246806105 , diff:  0.00817369353126196
layer  12  adv train finish, try to retain  463
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
### skip layer  13 wait:  4  ###
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.970703125  ==>  497 / 512 , inc:  16
layer  8  :  0.970703125  ==>  497 / 512 , inc:  16
layer  9  :  0.970703125  ==>  497 / 512 , inc:  16
layer  10  :  0.037109375  ==>  19 / 512 , inc:  1
layer  11  :  0.970703125  ==>  497 / 512 , inc:  16
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.15234375  ==>  78 / 512 , inc:  1
eps [0.0025312500000000005, 0.005062500000000001, 0.005062500000000001, 0.005062500000000001, 0.005062500000000001, 0.005062500000000001, 0.0025312500000000005, 0.00031640625000000006, 0.00031640625000000006, 0.00031640625000000006, 0.010125000000000002, 0.00031640625000000006, 0.010125000000000002, 0.005062500000000001]  wait [2, 3, 3, 3, 3, 3, 2, 0, 0, 0, 2, 0, 2, 3]  inc [1, 1, 1, 1, 1, 1, 1, 16, 16, 16, 1, 16, 1, 1]  tol: 2
$$$$$$$$$$$$$ epoch  10  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -0.06940214804126299 , diff:  0.06940214804126299
adv train loss:  -0.022284954579845362 , diff:  0.04711719346141763
adv train loss:  -0.06426391587228864 , diff:  0.04197896129244327
adv train loss:  -0.03662142926555134 , diff:  0.027642486606737293
adv train loss:  -0.05734711402783432 , diff:  0.02072568476228298
adv train loss:  -0.04249678845280869 , diff:  0.014850325575025636
adv train loss:  -0.037343320258742096 , diff:  0.0051534681940665905
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  31
test acc: top1 ->  40.61 ; top5 ->  81.1  and loss:  823.5243530273438
forward train acc: top1 ->  95.60999997070313 ; top5 ->  99.784  and loss:  23.99607829004526
test acc: top1 ->  82.16 ; top5 ->  97.18  and loss:  124.42909914255142
forward train acc: top1 ->  96.67400001708984 ; top5 ->  99.88399997558594  and loss:  11.454748220741749
test acc: top1 ->  88.97 ; top5 ->  98.84  and loss:  54.660977721214294
forward train acc: top1 ->  96.92600000732422 ; top5 ->  99.916  and loss:  9.432503446936607
test acc: top1 ->  89.23 ; top5 ->  98.91  and loss:  49.926115572452545
forward train acc: top1 ->  97.49999998291015 ; top5 ->  99.952  and loss:  7.593857370316982
test acc: top1 ->  89.33 ; top5 ->  98.9  and loss:  49.64884527027607
forward train acc: top1 ->  97.76600000976562 ; top5 ->  99.94199997558594  and loss:  7.084205236285925
test acc: top1 ->  89.53 ; top5 ->  99.02  and loss:  48.08522142469883
forward train acc: top1 ->  97.91199998291016 ; top5 ->  99.95  and loss:  6.1950090900063515
test acc: top1 ->  89.64 ; top5 ->  99.04  and loss:  47.40667687356472
forward train acc: top1 ->  97.98600000488281 ; top5 ->  99.98  and loss:  5.895385602489114
test acc: top1 ->  89.77 ; top5 ->  99.0  and loss:  47.67626151442528
forward train acc: top1 ->  98.13999998291015 ; top5 ->  99.952  and loss:  5.69056873768568
test acc: top1 ->  89.76 ; top5 ->  99.01  and loss:  47.50082126259804
forward train acc: top1 ->  98.15000000732422 ; top5 ->  99.962  and loss:  5.48325059376657
test acc: top1 ->  89.83 ; top5 ->  99.02  and loss:  47.04674869775772
forward train acc: top1 ->  98.34800000244141 ; top5 ->  99.978  and loss:  4.8646124713122845
test acc: top1 ->  89.78 ; top5 ->  99.03  and loss:  48.14831407368183
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
### skip layer  1 wait:  3  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
adv train loss:  -0.32354150526225567 , diff:  0.32354150526225567
adv train loss:  -0.36315586790442467 , diff:  0.039614362642169
adv train loss:  -0.33543448871932924 , diff:  0.02772137918509543
adv train loss:  -0.3425915737170726 , diff:  0.007157084997743368
************ all values are small in this layer **********
layer  6  adv train finish, try to retain  143
test acc: top1 ->  88.94 ; top5 ->  98.99  and loss:  46.135361447930336
forward train acc: top1 ->  99.66399997558594 ; top5 ->  99.998  and loss:  1.0765514128142968
test acc: top1 ->  91.46 ; top5 ->  99.1  and loss:  54.13567201793194
forward train acc: top1 ->  99.78199997558593 ; top5 ->  100.0  and loss:  0.7277640608372167
test acc: top1 ->  91.52 ; top5 ->  99.14  and loss:  57.2066206112504
forward train acc: top1 ->  99.78999997558594 ; top5 ->  99.998  and loss:  0.5911020718631335
test acc: top1 ->  91.54 ; top5 ->  99.15  and loss:  58.48073993623257
forward train acc: top1 ->  99.85799997558594 ; top5 ->  100.0  and loss:  0.45181128819240257
test acc: top1 ->  91.52 ; top5 ->  99.11  and loss:  60.68695808202028
forward train acc: top1 ->  99.84399997558593 ; top5 ->  99.998  and loss:  0.45250143454177305
test acc: top1 ->  91.52 ; top5 ->  99.08  and loss:  61.836465172469616
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  144 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -0.3768488287751097 , diff:  0.3768488287751097
adv train loss:  -0.2883174108574167 , diff:  0.08853141791769303
adv train loss:  -0.3517259952350287 , diff:  0.06340858437761199
adv train loss:  -0.3631651470641373 , diff:  0.011439151829108596
adv train loss:  -0.35308621560398024 , diff:  0.010078931460157037
adv train loss:  -0.38393188567715697 , diff:  0.030845670073176734
adv train loss:  -0.37386015681840945 , diff:  0.01007172885874752
adv train loss:  -0.3819262238248484 , diff:  0.00806606700643897
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  481
test acc: top1 ->  91.78 ; top5 ->  99.1  and loss:  60.75693917274475
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.09363852689421037
test acc: top1 ->  92.27 ; top5 ->  99.09  and loss:  60.73540260642767
==> this epoch:  481 / 512
---------------- start layer  8  ---------------
adv train loss:  -0.03181251949354191 , diff:  0.03181251949354191
adv train loss:  -0.06989074394732597 , diff:  0.03807822445378406
adv train loss:  -0.04598089768842328 , diff:  0.02390984625890269
adv train loss:  -0.04735503043411882 , diff:  0.0013741327456955332
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  481
test acc: top1 ->  92.26 ; top5 ->  99.07  and loss:  60.63643270730972
==> this epoch:  481 / 512
---------------- start layer  9  ---------------
adv train loss:  -0.044278542194660986 , diff:  0.044278542194660986
adv train loss:  -0.04014006205761689 , diff:  0.004138480137044098
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  481
test acc: top1 ->  92.28 ; top5 ->  99.07  and loss:  60.680380411446095
==> this epoch:  481 / 512
---------------- start layer  10  ---------------
adv train loss:  -0.2489940907980781 , diff:  0.2489940907980781
adv train loss:  -0.2392656597075984 , diff:  0.009728431090479717
layer  10  adv train finish, try to retain  492
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -0.04457851519691758 , diff:  0.04457851519691758
adv train loss:  -0.048471237050762284 , diff:  0.003892721853844705
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  481
test acc: top1 ->  92.32 ; top5 ->  99.09  and loss:  60.38363569974899
==> this epoch:  481 / 512
---------------- start layer  12  ---------------
adv train loss:  -0.721229931266862 , diff:  0.721229931266862
adv train loss:  -0.734821212652605 , diff:  0.01359128138574306
adv train loss:  -0.7147346478195686 , diff:  0.02008656483303639
adv train loss:  -0.6967808817717014 , diff:  0.017953766047867248
adv train loss:  -0.860775873370585 , diff:  0.16399499159888364
adv train loss:  -0.6811954245786183 , diff:  0.17958044879196677
adv train loss:  -0.7643046063094516 , diff:  0.08310918173083337
adv train loss:  -0.767176594970806 , diff:  0.0028719886613544077
layer  12  adv train finish, try to retain  493
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
### skip layer  13 wait:  3  ###
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.939453125  ==>  481 / 512 , inc:  32
layer  8  :  0.939453125  ==>  481 / 512 , inc:  32
layer  9  :  0.939453125  ==>  481 / 512 , inc:  32
layer  10  :  0.037109375  ==>  19 / 512 , inc:  1
layer  11  :  0.939453125  ==>  481 / 512 , inc:  32
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.15234375  ==>  78 / 512 , inc:  1
eps [0.0018984375000000004, 0.005062500000000001, 0.005062500000000001, 0.005062500000000001, 0.005062500000000001, 0.005062500000000001, 0.0018984375000000004, 0.00031640625000000006, 0.00031640625000000006, 0.00031640625000000006, 0.020250000000000004, 0.00031640625000000006, 0.020250000000000004, 0.005062500000000001]  wait [4, 2, 2, 2, 2, 2, 4, 0, 0, 0, 2, 0, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 32, 32, 32, 1, 32, 1, 1]  tol: 2
$$$$$$$$$$$$$ epoch  11  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
adv train loss:  -0.04251333015054115 , diff:  0.04251333015054115
adv train loss:  -0.05487000279754284 , diff:  0.012356672647001687
adv train loss:  -0.07845900325264665 , diff:  0.023589000455103815
adv train loss:  -0.0738096571949427 , diff:  0.004649346057703951
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -0.031470849749894114 , diff:  0.031470849749894114
adv train loss:  -0.0384561925457092 , diff:  0.006985342795815086
layer  2  adv train finish, try to retain  128
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -0.04222890723031014 , diff:  0.04222890723031014
adv train loss:  -0.04790204727760283 , diff:  0.005673140047292691
layer  3  adv train finish, try to retain  128
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -0.05166429991913901 , diff:  0.05166429991913901
adv train loss:  -0.03644211019491195 , diff:  0.015222189724227064
adv train loss:  -0.05844101612092345 , diff:  0.021998905926011503
adv train loss:  -0.03620489551394712 , diff:  0.02223612060697633
adv train loss:  -0.06807979174118373 , diff:  0.031874896227236604
adv train loss:  -0.05120837487083918 , diff:  0.016871416870344547
adv train loss:  -0.06617730562356883 , diff:  0.014968930752729648
adv train loss:  -0.04214555634825956 , diff:  0.024031749275309267
adv train loss:  -0.035002187489226344 , diff:  0.007143368859033217
layer  4  adv train finish, try to retain  256
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -0.05194784177001566 , diff:  0.05194784177001566
adv train loss:  -0.050429492508555995 , diff:  0.0015183492614596616
layer  5  adv train finish, try to retain  256
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
adv train loss:  -0.052684535783555475 , diff:  0.052684535783555475
adv train loss:  -0.053176037516095676 , diff:  0.0004915017325402005
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  449
test acc: top1 ->  92.3 ; top5 ->  99.08  and loss:  60.97124545276165
==> this epoch:  449 / 512
---------------- start layer  8  ---------------
adv train loss:  -0.04564732877770439 , diff:  0.04564732877770439
adv train loss:  -0.029937653462184244 , diff:  0.015709675315520144
adv train loss:  -0.0425753289819113 , diff:  0.012637675519727054
adv train loss:  -0.05619247959111817 , diff:  0.013617150609206874
adv train loss:  -0.042381860832392704 , diff:  0.013810618758725468
adv train loss:  -0.03438119408747298 , diff:  0.008000666744919727
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  449
test acc: top1 ->  92.25 ; top5 ->  99.08  and loss:  61.665877498686314
==> this epoch:  449 / 512
---------------- start layer  9  ---------------
adv train loss:  -0.041418373944907216 , diff:  0.041418373944907216
adv train loss:  -0.05714595468089101 , diff:  0.015727580735983793
adv train loss:  -0.047261319727113005 , diff:  0.009884634953778004
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  449
test acc: top1 ->  92.32 ; top5 ->  99.06  and loss:  60.8965015783906
==> this epoch:  449 / 512
---------------- start layer  10  ---------------
adv train loss:  -0.21895336676971056 , diff:  0.21895336676971056
adv train loss:  -0.20251904995529912 , diff:  0.01643431681441143
adv train loss:  -0.23565865514683537 , diff:  0.03313960519153625
adv train loss:  -0.22671599089517258 , diff:  0.00894266425166279
layer  10  adv train finish, try to retain  493
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -0.04884170104560326 , diff:  0.04884170104560326
adv train loss:  -0.048757046592072584 , diff:  8.46544535306748e-05
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  449
test acc: top1 ->  92.26 ; top5 ->  99.07  and loss:  60.724053144454956
==> this epoch:  449 / 512
---------------- start layer  12  ---------------
adv train loss:  -0.6830329138610978 , diff:  0.6830329138610978
adv train loss:  -0.7050267296435777 , diff:  0.021993815782479942
adv train loss:  -0.7289906451260322 , diff:  0.023963915482454468
adv train loss:  -0.7699376249802299 , diff:  0.04094697985419771
adv train loss:  -0.6672866579028778 , diff:  0.1026509670773521
adv train loss:  -0.7162643192496034 , diff:  0.048977661346725654
adv train loss:  -0.7901095102133695 , diff:  0.07384519096376607
adv train loss:  -0.7819487269443925 , diff:  0.008160783268976957
layer  12  adv train finish, try to retain  485
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -0.041318266637972556 , diff:  0.041318266637972556
adv train loss:  -0.0574791402068513 , diff:  0.016160873568878742
adv train loss:  -0.04347523627438932 , diff:  0.01400390393246198
adv train loss:  -0.051018330988881644 , diff:  0.007543094714492327
layer  13  adv train finish, try to retain  493
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.876953125  ==>  449 / 512 , inc:  64
layer  8  :  0.876953125  ==>  449 / 512 , inc:  64
layer  9  :  0.876953125  ==>  449 / 512 , inc:  64
layer  10  :  0.037109375  ==>  19 / 512 , inc:  1
layer  11  :  0.876953125  ==>  449 / 512 , inc:  64
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.15234375  ==>  78 / 512 , inc:  1
eps [0.0018984375000000004, 0.010125000000000002, 0.010125000000000002, 0.010125000000000002, 0.010125000000000002, 0.010125000000000002, 0.0018984375000000004, 0.00031640625000000006, 0.00031640625000000006, 0.00031640625000000006, 0.04050000000000001, 0.00031640625000000006, 0.04050000000000001, 0.010125000000000002]  wait [3, 2, 2, 2, 2, 2, 3, 0, 0, 0, 2, 0, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 64, 64, 64, 1, 64, 1, 1]  tol: 2
$$$$$$$$$$$$$ epoch  12  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
adv train loss:  -0.06477160625581746 , diff:  0.06477160625581746
adv train loss:  -0.039882749693788355 , diff:  0.024888856562029105
adv train loss:  -0.04057610789095634 , diff:  0.0006933581971679814
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -0.032842225169588346 , diff:  0.032842225169588346
adv train loss:  -0.05701593547928496 , diff:  0.024173710309696617
adv train loss:  -0.054420320188000915 , diff:  0.002595615291284048
layer  2  adv train finish, try to retain  128
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -0.04957949178060517 , diff:  0.04957949178060517
adv train loss:  -0.04611693926926819 , diff:  0.0034625525113369804
layer  3  adv train finish, try to retain  128
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -0.05023363037253148 , diff:  0.05023363037253148
adv train loss:  -0.07106661062425701 , diff:  0.02083298025172553
adv train loss:  -0.0375050305192417 , diff:  0.03356158010501531
adv train loss:  -0.07179755617835326 , diff:  0.03429252565911156
adv train loss:  -0.04112411485948542 , diff:  0.030673441318867845
adv train loss:  -0.058144514718151186 , diff:  0.01702039985866577
adv train loss:  -0.038107424708869075 , diff:  0.02003709000928211
adv train loss:  -0.04799157048546476 , diff:  0.009884145776595687
layer  4  adv train finish, try to retain  256
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -0.062433714760118164 , diff:  0.062433714760118164
adv train loss:  -0.0517161260577268 , diff:  0.010717588702391367
adv train loss:  -0.03373104148704442 , diff:  0.017985084570682375
adv train loss:  -0.051038915966273635 , diff:  0.017307874479229213
adv train loss:  -0.07666518495170749 , diff:  0.025626268985433853
adv train loss:  -0.04212768741126638 , diff:  0.03453749754044111
adv train loss:  -0.05796065443792031 , diff:  0.015832967026653932
adv train loss:  -0.045854019263060763 , diff:  0.012106635174859548
adv train loss:  -0.05080713732604636 , diff:  0.004953118062985595
layer  5  adv train finish, try to retain  256
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
### skip layer  6 wait:  3  ###
---------------- start layer  7  ---------------
adv train loss:  -0.05300646205796511 , diff:  0.05300646205796511
adv train loss:  -0.05149663553311257 , diff:  0.0015098265248525422
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  385
test acc: top1 ->  92.25 ; top5 ->  99.08  and loss:  60.78188082575798
==> this epoch:  385 / 512
---------------- start layer  8  ---------------
adv train loss:  -0.05760293345883838 , diff:  0.05760293345883838
adv train loss:  -0.04809418801232823 , diff:  0.009508745446510147
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  385
test acc: top1 ->  92.26 ; top5 ->  99.08  and loss:  60.71783259510994
==> this epoch:  385 / 512
---------------- start layer  9  ---------------
adv train loss:  -0.05210577682373696 , diff:  0.05210577682373696
adv train loss:  -0.05581165974581381 , diff:  0.003705882922076853
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  385
test acc: top1 ->  92.26 ; top5 ->  99.08  and loss:  60.80450565367937
==> this epoch:  385 / 512
---------------- start layer  10  ---------------
adv train loss:  -0.22654028580291197 , diff:  0.22654028580291197
adv train loss:  -0.20229461387498304 , diff:  0.024245671927928925
adv train loss:  -0.19131122529506683 , diff:  0.010983388579916209
adv train loss:  -0.23978773603448644 , diff:  0.04847651073941961
adv train loss:  -0.19936467614024878 , diff:  0.04042305989423767
adv train loss:  -0.2036490009049885 , diff:  0.004284324764739722
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  18
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  715.4846291542053
forward train acc: top1 ->  9.951999996948242 ; top5 ->  49.89600000366211  and loss:  351.44422578811646
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  390.7393159866333
forward train acc: top1 ->  9.89999999786377 ; top5 ->  49.6119999987793  and loss:  225.86375260353088
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.85539031028748
forward train acc: top1 ->  9.973999997253419 ; top5 ->  49.709999989013674  and loss:  225.89156007766724
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.73099207878113
forward train acc: top1 ->  9.919999998474122 ; top5 ->  49.99800000488281  and loss:  226.100111246109
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.64830374717712
forward train acc: top1 ->  10.01000000213623 ; top5 ->  50.152000004882815  and loss:  225.9207408428192
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  231.27623581886292
forward train acc: top1 ->  9.80999999847412 ; top5 ->  49.84399999511719  and loss:  225.919579744339
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.62255454063416
forward train acc: top1 ->  10.059999996643066 ; top5 ->  50.218  and loss:  225.86325883865356
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.4150013923645
forward train acc: top1 ->  10.04799999938965 ; top5 ->  49.941999998779295  and loss:  225.83708596229553
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.47015953063965
forward train acc: top1 ->  9.842000001220702 ; top5 ->  49.711999990234375  and loss:  225.87186360359192
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.51813435554504
forward train acc: top1 ->  9.845999997253418 ; top5 ->  49.714  and loss:  225.81218886375427
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.3325457572937
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  19 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -0.2330061531974934 , diff:  0.2330061531974934
adv train loss:  -0.21763294088304974 , diff:  0.015373212314443663
adv train loss:  -0.2553834198915865 , diff:  0.037750479008536786
adv train loss:  -0.2134138778783381 , diff:  0.04196954201324843
adv train loss:  -0.20164253503025975 , diff:  0.011771342848078348
adv train loss:  -0.1864947375288466 , diff:  0.015147797501413152
adv train loss:  -0.22022619834751822 , diff:  0.03373146081867162
adv train loss:  -0.23713766087894328 , diff:  0.01691146253142506
adv train loss:  -0.20761047294945456 , diff:  0.02952718792948872
adv train loss:  -0.19358105574792717 , diff:  0.014029417201527394
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  385
test acc: top1 ->  91.94 ; top5 ->  99.09  and loss:  62.341776087880135
forward train acc: top1 ->  99.99199997558594 ; top5 ->  100.0  and loss:  0.05118474271512241
test acc: top1 ->  92.27 ; top5 ->  99.13  and loss:  68.32078497111797
==> this epoch:  385 / 512
---------------- start layer  12  ---------------
adv train loss:  -0.5122132995675202 , diff:  0.5122132995675202
adv train loss:  -0.6460985990634072 , diff:  0.133885299495887
adv train loss:  -0.6830441176716704 , diff:  0.03694551860826323
adv train loss:  -0.5077497709244199 , diff:  0.17529434674725053
adv train loss:  -0.4184281298657879 , diff:  0.08932164105863194
adv train loss:  -0.6209292766034196 , diff:  0.20250114673763164
adv train loss:  -0.5353509480482899 , diff:  0.0855783285551297
adv train loss:  -0.6333240436324559 , diff:  0.097973095584166
adv train loss:  -0.5844458996361936 , diff:  0.048878143996262224
adv train loss:  -0.5608804474759381 , diff:  0.023565452160255518
layer  12  adv train finish, try to retain  472
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -0.026221883960715786 , diff:  0.026221883960715786
adv train loss:  -0.02741774251080642 , diff:  0.0011958585500906338
layer  13  adv train finish, try to retain  479
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.751953125  ==>  385 / 512 , inc:  128
layer  8  :  0.751953125  ==>  385 / 512 , inc:  128
layer  9  :  0.751953125  ==>  385 / 512 , inc:  128
layer  10  :  0.037109375  ==>  19 / 512 , inc:  1
layer  11  :  0.751953125  ==>  385 / 512 , inc:  128
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.15234375  ==>  78 / 512 , inc:  1
eps [0.0018984375000000004, 0.020250000000000004, 0.020250000000000004, 0.020250000000000004, 0.020250000000000004, 0.020250000000000004, 0.0018984375000000004, 0.00031640625000000006, 0.00031640625000000006, 0.00031640625000000006, 0.030375000000000006, 0.00031640625000000006, 0.08100000000000002, 0.020250000000000004]  wait [2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 4, 0, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 128, 128, 128, 1, 128, 1, 1]  tol: 2
$$$$$$$$$$$$$ epoch  13  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -0.03267889391463541 , diff:  0.03267889391463541
adv train loss:  -0.032276745752824354 , diff:  0.0004021481618110556
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  31
test acc: top1 ->  44.09 ; top5 ->  83.86  and loss:  544.6215624809265
forward train acc: top1 ->  97.88800000732422 ; top5 ->  99.944  and loss:  8.01613600552082
test acc: top1 ->  83.62 ; top5 ->  97.7  and loss:  103.85765200853348
forward train acc: top1 ->  98.20199998046876 ; top5 ->  99.976  and loss:  5.293978264555335
test acc: top1 ->  89.75 ; top5 ->  99.09  and loss:  52.402861982584
forward train acc: top1 ->  98.44199998291016 ; top5 ->  99.976  and loss:  4.783699816092849
test acc: top1 ->  89.87 ; top5 ->  99.08  and loss:  50.99993035197258
forward train acc: top1 ->  98.65400000488282 ; top5 ->  99.982  and loss:  4.025028521195054
test acc: top1 ->  89.94 ; top5 ->  99.06  and loss:  50.22085466980934
forward train acc: top1 ->  98.7820000024414 ; top5 ->  99.98  and loss:  3.5590423606336117
test acc: top1 ->  90.08 ; top5 ->  99.1  and loss:  50.270520731806755
forward train acc: top1 ->  98.78800000244141 ; top5 ->  99.98  and loss:  3.593271730467677
test acc: top1 ->  90.12 ; top5 ->  99.07  and loss:  50.634709775447845
forward train acc: top1 ->  98.77599997802734 ; top5 ->  99.976  and loss:  3.636582954786718
test acc: top1 ->  90.15 ; top5 ->  99.13  and loss:  49.68063613772392
forward train acc: top1 ->  98.97199997802734 ; top5 ->  99.988  and loss:  3.036959843710065
test acc: top1 ->  90.21 ; top5 ->  99.05  and loss:  50.175278067588806
forward train acc: top1 ->  98.98599997558594 ; top5 ->  99.984  and loss:  3.090714869555086
test acc: top1 ->  90.05 ; top5 ->  99.12  and loss:  50.068452790379524
forward train acc: top1 ->  99.01799998046874 ; top5 ->  99.992  and loss:  2.7906748931854963
test acc: top1 ->  90.06 ; top5 ->  99.12  and loss:  50.58234655857086
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -0.24516156897880137 , diff:  0.24516156897880137
adv train loss:  -0.24663368158508092 , diff:  0.001472112606279552
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -0.25242479774169624 , diff:  0.25242479774169624
adv train loss:  -0.2341907810769044 , diff:  0.018234016664791852
adv train loss:  -0.2336480591329746 , diff:  0.0005427219439297915
layer  2  adv train finish, try to retain  128
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -0.24000028846785426 , diff:  0.24000028846785426
adv train loss:  -0.26034781901398674 , diff:  0.020347530546132475
adv train loss:  -0.2608770798542537 , diff:  0.000529260840266943
layer  3  adv train finish, try to retain  128
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -0.23188298172317445 , diff:  0.23188298172317445
adv train loss:  -0.24098001117818058 , diff:  0.009097029455006123
layer  4  adv train finish, try to retain  256
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -0.25165409591863863 , diff:  0.25165409591863863
adv train loss:  -0.247260564356111 , diff:  0.004393531562527642
layer  5  adv train finish, try to retain  256
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -0.2245195735595189 , diff:  0.2245195735595189
adv train loss:  -0.22020523488754407 , diff:  0.004314338671974838
************ all values are small in this layer **********
layer  6  adv train finish, try to retain  143
test acc: top1 ->  89.11 ; top5 ->  98.94  and loss:  50.12461939454079
forward train acc: top1 ->  99.73999997558593 ; top5 ->  99.998  and loss:  0.8224159047240391
test acc: top1 ->  91.41 ; top5 ->  99.16  and loss:  56.421315774321556
forward train acc: top1 ->  99.796 ; top5 ->  100.0  and loss:  0.5792478268849663
test acc: top1 ->  91.47 ; top5 ->  99.06  and loss:  59.02973201870918
forward train acc: top1 ->  99.794 ; top5 ->  100.0  and loss:  0.5995880033296999
test acc: top1 ->  91.51 ; top5 ->  99.02  and loss:  57.418473564088345
forward train acc: top1 ->  99.85199997558594 ; top5 ->  100.0  and loss:  0.4615003020153381
test acc: top1 ->  91.58 ; top5 ->  99.02  and loss:  59.54788635671139
forward train acc: top1 ->  99.906 ; top5 ->  100.0  and loss:  0.297119470895268
test acc: top1 ->  91.63 ; top5 ->  99.02  and loss:  63.22314466536045
forward train acc: top1 ->  99.86999997558594 ; top5 ->  99.998  and loss:  0.3863258346245857
test acc: top1 ->  91.55 ; top5 ->  99.0  and loss:  64.25969699025154
forward train acc: top1 ->  99.87199997558594 ; top5 ->  100.0  and loss:  0.3811484197722166
test acc: top1 ->  91.58 ; top5 ->  99.01  and loss:  63.53737060725689
forward train acc: top1 ->  99.93799997558594 ; top5 ->  100.0  and loss:  0.2194413484976394
test acc: top1 ->  91.53 ; top5 ->  99.04  and loss:  64.88145446777344
forward train acc: top1 ->  99.90599997558594 ; top5 ->  100.0  and loss:  0.3108533860067837
test acc: top1 ->  91.7 ; top5 ->  99.02  and loss:  65.023831397295
forward train acc: top1 ->  99.93799997558594 ; top5 ->  100.0  and loss:  0.2129322900727857
test acc: top1 ->  91.62 ; top5 ->  98.96  and loss:  66.8989333063364
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  144 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -0.1696821655932581 , diff:  0.1696821655932581
adv train loss:  -0.1700053145395941 , diff:  0.0003231489463360049
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  257
test acc: top1 ->  90.19 ; top5 ->  99.11  and loss:  53.830727122724056
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.10900757716080989
test acc: top1 ->  91.94 ; top5 ->  99.02  and loss:  63.88945709168911
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.06605303111609828
test acc: top1 ->  91.97 ; top5 ->  99.09  and loss:  67.9239572584629
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.04775327959941933
test acc: top1 ->  92.03 ; top5 ->  99.06  and loss:  69.55613557249308
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.07571010046376614
test acc: top1 ->  91.98 ; top5 ->  99.09  and loss:  70.35061543434858
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.03869294051764882
test acc: top1 ->  91.98 ; top5 ->  99.05  and loss:  71.47934889793396
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.06527038219064707
test acc: top1 ->  91.97 ; top5 ->  99.05  and loss:  72.39192917943001
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.04148546181386337
test acc: top1 ->  92.05 ; top5 ->  99.08  and loss:  73.80327935516834
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.03177012760716025
test acc: top1 ->  92.08 ; top5 ->  99.09  and loss:  74.59449822455645
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03500024556410608
test acc: top1 ->  92.04 ; top5 ->  99.07  and loss:  74.36726133525372
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.02913794665801106
test acc: top1 ->  92.06 ; top5 ->  99.06  and loss:  74.30736546963453
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  385 / 512 , inc:  128
---------------- start layer  8  ---------------
adv train loss:  -0.01765564527886454 , diff:  0.01765564527886454
adv train loss:  -0.04597185926468228 , diff:  0.02831621398581774
adv train loss:  -0.04948067420082225 , diff:  0.00350881493613997
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  257
test acc: top1 ->  92.16 ; top5 ->  98.93  and loss:  78.03907385468483
==> this epoch:  257 / 512
---------------- start layer  9  ---------------
adv train loss:  -0.03674256029808021 , diff:  0.03674256029808021
adv train loss:  -0.04130519349473616 , diff:  0.004562633196655952
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  257
test acc: top1 ->  92.22 ; top5 ->  98.96  and loss:  79.62014026194811
==> this epoch:  257 / 512
---------------- start layer  10  ---------------
### skip layer  10 wait:  4  ###
---------------- start layer  11  ---------------
adv train loss:  -0.01786108186661295 , diff:  0.01786108186661295
adv train loss:  -0.04530365586242624 , diff:  0.02744257399581329
adv train loss:  -0.04834245227903011 , diff:  0.003038796416603873
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  257
test acc: top1 ->  92.16 ; top5 ->  98.98  and loss:  79.33478972315788
==> this epoch:  257 / 512
---------------- start layer  12  ---------------
adv train loss:  -0.5037332732681534 , diff:  0.5037332732681534
adv train loss:  -0.3851883273673593 , diff:  0.11854494590079412
adv train loss:  -0.269158568782359 , diff:  0.11602975858500031
adv train loss:  -0.3223739910661152 , diff:  0.0532154222837562
adv train loss:  -0.3650690985177789 , diff:  0.042695107451663716
adv train loss:  -0.44758721528341994 , diff:  0.08251811676564103
adv train loss:  -0.33317345941759413 , diff:  0.1144137558658258
adv train loss:  -0.4173081296385135 , diff:  0.08413467022091936
adv train loss:  -0.42797757366497535 , diff:  0.01066944402646186
adv train loss:  -0.31344660670310986 , diff:  0.11453096696186549
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  61
test acc: top1 ->  30.26 ; top5 ->  70.51  and loss:  379.29062032699585
forward train acc: top1 ->  65.998 ; top5 ->  92.188  and loss:  128.06880079209805
test acc: top1 ->  89.24 ; top5 ->  98.72  and loss:  55.00802409648895
forward train acc: top1 ->  99.718 ; top5 ->  99.996  and loss:  11.16502083092928
test acc: top1 ->  91.24 ; top5 ->  98.94  and loss:  39.56393529474735
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  5.171744357794523
test acc: top1 ->  91.38 ; top5 ->  98.97  and loss:  37.14169055223465
forward train acc: top1 ->  99.892 ; top5 ->  100.0  and loss:  3.1605222038924694
test acc: top1 ->  91.53 ; top5 ->  98.97  and loss:  36.127950467169285
forward train acc: top1 ->  99.94399997558594 ; top5 ->  100.0  and loss:  2.0121318269521
test acc: top1 ->  91.67 ; top5 ->  99.0  and loss:  36.49942485243082
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  1.5153296506032348
test acc: top1 ->  91.76 ; top5 ->  99.03  and loss:  36.825632363557816
forward train acc: top1 ->  99.94199997558594 ; top5 ->  100.0  and loss:  1.2902345778420568
test acc: top1 ->  91.8 ; top5 ->  98.98  and loss:  37.06408067047596
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  1.09210888389498
test acc: top1 ->  91.78 ; top5 ->  99.04  and loss:  37.39790388196707
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.9206257346086204
test acc: top1 ->  91.82 ; top5 ->  99.0  and loss:  37.793927535414696
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.8189412457868457
test acc: top1 ->  91.98 ; top5 ->  98.98  and loss:  38.46876407414675
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  62 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -4.596131969708949 , diff:  4.596131969708949
adv train loss:  -4.465843529440463 , diff:  0.13028844026848674
adv train loss:  -4.4481942327693105 , diff:  0.017649296671152115
adv train loss:  -4.426373653113842 , diff:  0.021820579655468464
adv train loss:  -4.2230778485536575 , diff:  0.20329580456018448
adv train loss:  -4.689566842280328 , diff:  0.46648899372667074
adv train loss:  -4.460535364225507 , diff:  0.2290314780548215
adv train loss:  -4.260240946896374 , diff:  0.20029441732913256
adv train loss:  -4.403476764447987 , diff:  0.14323581755161285
adv train loss:  -4.629146927967668 , diff:  0.2256701635196805
layer  13  adv train finish, try to retain  498
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.751953125  ==>  385 / 512 , inc:  64
layer  8  :  0.501953125  ==>  257 / 512 , inc:  128
layer  9  :  0.501953125  ==>  257 / 512 , inc:  128
layer  10  :  0.037109375  ==>  19 / 512 , inc:  1
layer  11  :  0.501953125  ==>  257 / 512 , inc:  128
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.15234375  ==>  78 / 512 , inc:  1
eps [0.0014238281250000002, 0.04050000000000001, 0.04050000000000001, 0.04050000000000001, 0.04050000000000001, 0.04050000000000001, 0.0014238281250000002, 0.00023730468750000005, 0.00031640625000000006, 0.00031640625000000006, 0.030375000000000006, 0.00031640625000000006, 0.06075000000000001, 0.04050000000000001]  wait [4, 2, 2, 2, 2, 2, 4, 2, 0, 0, 3, 0, 4, 2]  inc [1, 1, 1, 1, 1, 1, 1, 64, 128, 128, 1, 128, 1, 1]  tol: 2
$$$$$$$$$$$$$ epoch  14  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
adv train loss:  -3.1426621929276735 , diff:  3.1426621929276735
adv train loss:  -3.1906334282830358 , diff:  0.047971235355362296
adv train loss:  -3.3439124766737223 , diff:  0.1532790483906865
adv train loss:  -3.4220888055860996 , diff:  0.07817632891237736
adv train loss:  -3.19641734380275 , diff:  0.22567146178334951
adv train loss:  -3.1857603657990694 , diff:  0.010656978003680706
adv train loss:  -3.010856593027711 , diff:  0.1749037727713585
adv train loss:  -3.3231932767666876 , diff:  0.3123366837389767
adv train loss:  -3.291991366073489 , diff:  0.031201910693198442
adv train loss:  -3.372924809344113 , diff:  0.08093344327062368
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -3.0436359182931483 , diff:  3.0436359182931483
adv train loss:  -3.230267898645252 , diff:  0.1866319803521037
adv train loss:  -3.1691020261496305 , diff:  0.06116587249562144
adv train loss:  -3.3161723483353853 , diff:  0.14707032218575478
adv train loss:  -3.3886263775639236 , diff:  0.07245402922853827
adv train loss:  -3.152176500298083 , diff:  0.23644987726584077
adv train loss:  -3.3267499948851764 , diff:  0.1745734945870936
adv train loss:  -3.1538514788262546 , diff:  0.17289851605892181
adv train loss:  -3.154095829464495 , diff:  0.0002443506382405758
layer  2  adv train finish, try to retain  128
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -3.144418449141085 , diff:  3.144418449141085
adv train loss:  -3.3758358827326447 , diff:  0.23141743359155953
adv train loss:  -3.3170485757291317 , diff:  0.05878730700351298
adv train loss:  -3.2063158184755594 , diff:  0.11073275725357234
adv train loss:  -3.231610056012869 , diff:  0.025294237537309527
adv train loss:  -3.3892686618492007 , diff:  0.15765860583633184
adv train loss:  -3.2485936656594276 , diff:  0.14067499618977308
adv train loss:  -3.2874055015854537 , diff:  0.038811835926026106
adv train loss:  -3.1137699764221907 , diff:  0.17363552516326308
adv train loss:  -3.1217103232629597 , diff:  0.007940346840769053
layer  3  adv train finish, try to retain  127
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -3.2497178679332137 , diff:  3.2497178679332137
adv train loss:  -3.336831171065569 , diff:  0.08711330313235521
adv train loss:  -3.286878955550492 , diff:  0.049952215515077114
adv train loss:  -3.0191397790331393 , diff:  0.26773917651735246
adv train loss:  -3.2138181116897613 , diff:  0.19467833265662193
adv train loss:  -3.1881413955707103 , diff:  0.02567671611905098
adv train loss:  -3.3922838629223406 , diff:  0.20414246735163033
adv train loss:  -3.18462576251477 , diff:  0.2076581004075706
adv train loss:  -3.049837000668049 , diff:  0.13478876184672117
adv train loss:  -3.1469683799659833 , diff:  0.09713137929793447
layer  4  adv train finish, try to retain  256
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -2.9969788130838424 , diff:  2.9969788130838424
adv train loss:  -3.0402640090323985 , diff:  0.043285195948556066
adv train loss:  -3.250719300005585 , diff:  0.2104552909731865
adv train loss:  -3.3246723818592727 , diff:  0.07395308185368776
adv train loss:  -3.5262892968021333 , diff:  0.2016169149428606
adv train loss:  -3.4692306858487427 , diff:  0.0570586109533906
adv train loss:  -3.1310101845301688 , diff:  0.33822050131857395
adv train loss:  -3.578519644914195 , diff:  0.44750946038402617
adv train loss:  -3.1568524583708495 , diff:  0.42166718654334545
adv train loss:  -3.0854987911880016 , diff:  0.07135366718284786
layer  5  adv train finish, try to retain  256
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
adv train loss:  -3.03873702371493 , diff:  3.03873702371493
adv train loss:  -3.02412909665145 , diff:  0.01460792706348002
adv train loss:  -3.258891526143998 , diff:  0.2347624294925481
adv train loss:  -3.0020844638347626 , diff:  0.25680706230923533
adv train loss:  -3.1861539939418435 , diff:  0.18406953010708094
adv train loss:  -3.322043971857056 , diff:  0.1358899779152125
adv train loss:  -3.2648998282384127 , diff:  0.057144143618643284
adv train loss:  -3.125221114140004 , diff:  0.13967871409840882
adv train loss:  -3.0752933803014457 , diff:  0.0499277338385582
adv train loss:  -3.1259553702548146 , diff:  0.0506619899533689
layer  7  adv train finish, try to retain  253
test acc: top1 ->  87.58 ; top5 ->  97.91  and loss:  102.19856044650078
forward train acc: top1 ->  99.908 ; top5 ->  100.0  and loss:  0.327240800470463
test acc: top1 ->  92.32 ; top5 ->  98.8  and loss:  90.7515454813838
==> this epoch:  253 / 512
---------------- start layer  8  ---------------
adv train loss:  -0.07408196656206201 , diff:  0.07408196656206201
adv train loss:  -0.13628981350666436 , diff:  0.06220784694460235
adv train loss:  -0.06431661122405785 , diff:  0.07197320228260651
adv train loss:  -0.08553601494986651 , diff:  0.02121940372580866
adv train loss:  -0.10296726352498808 , diff:  0.017431248575121572
adv train loss:  -0.06814919833777822 , diff:  0.03481806518720987
adv train loss:  -0.09863992211467121 , diff:  0.030490723776892992
adv train loss:  -0.0839262080971821 , diff:  0.01471371401748911
adv train loss:  -0.11257828520865587 , diff:  0.028652077111473773
adv train loss:  -0.10542227733094478 , diff:  0.00715600787771109
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  129
test acc: top1 ->  34.52 ; top5 ->  83.61  and loss:  541.3642892837524
forward train acc: top1 ->  99.8 ; top5 ->  100.0  and loss:  0.6935838281788165
test acc: top1 ->  91.61 ; top5 ->  98.78  and loss:  81.66955605428666
forward train acc: top1 ->  99.882 ; top5 ->  100.0  and loss:  0.334721813647775
test acc: top1 ->  91.68 ; top5 ->  98.88  and loss:  82.17144744470716
forward train acc: top1 ->  99.908 ; top5 ->  100.0  and loss:  0.23127320208004676
test acc: top1 ->  91.74 ; top5 ->  98.85  and loss:  81.50822256878018
forward train acc: top1 ->  99.93599997558594 ; top5 ->  100.0  and loss:  0.2018806785927154
test acc: top1 ->  91.79 ; top5 ->  98.83  and loss:  81.15447674319148
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.1566523952933494
test acc: top1 ->  91.82 ; top5 ->  98.95  and loss:  80.40797952469438
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.1638935171686171
test acc: top1 ->  91.82 ; top5 ->  98.97  and loss:  80.53447420336306
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.12389793221882428
test acc: top1 ->  91.84 ; top5 ->  98.99  and loss:  80.67813494056463
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.08923077915096655
test acc: top1 ->  91.95 ; top5 ->  98.96  and loss:  81.75754068791866
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.12308837190721533
test acc: top1 ->  91.89 ; top5 ->  98.91  and loss:  82.16598795168102
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.1167495845729718
test acc: top1 ->  91.82 ; top5 ->  98.89  and loss:  82.42341429367661
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  257 / 512 , inc:  128
---------------- start layer  9  ---------------
adv train loss:  -0.06923745108724688 , diff:  0.06923745108724688
adv train loss:  -0.10624291697604349 , diff:  0.03700546588879661
adv train loss:  -0.0792294052807847 , diff:  0.027013511695258785
adv train loss:  -0.12406145996646956 , diff:  0.04483205468568485
adv train loss:  -0.09984930608493414 , diff:  0.024212153881535414
adv train loss:  -0.10644049821166846 , diff:  0.006591192126734313
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  129
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  1491.0062580108643
forward train acc: top1 ->  98.024 ; top5 ->  100.0  and loss:  7.3368149772286415
test acc: top1 ->  91.44 ; top5 ->  98.62  and loss:  75.18828676640987
forward train acc: top1 ->  99.81799997558593 ; top5 ->  100.0  and loss:  0.6255621894961223
test acc: top1 ->  91.92 ; top5 ->  98.74  and loss:  73.97570164501667
forward train acc: top1 ->  99.884 ; top5 ->  100.0  and loss:  0.38367385062156245
test acc: top1 ->  92.1 ; top5 ->  98.76  and loss:  73.58483543992043
forward train acc: top1 ->  99.92999997558594 ; top5 ->  100.0  and loss:  0.26281295908847824
test acc: top1 ->  92.08 ; top5 ->  98.83  and loss:  74.18608063459396
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.19115072453860193
test acc: top1 ->  92.04 ; top5 ->  98.81  and loss:  74.97023497521877
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.17632025892089587
test acc: top1 ->  92.01 ; top5 ->  98.88  and loss:  74.33386190235615
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.1507466128678061
test acc: top1 ->  92.12 ; top5 ->  98.86  and loss:  75.29930010437965
==> this epoch:  129 / 512
---------------- start layer  10  ---------------
### skip layer  10 wait:  3  ###
---------------- start layer  11  ---------------
adv train loss:  -0.11353415282792412 , diff:  0.11353415282792412
adv train loss:  -0.13891603719093837 , diff:  0.02538188436301425
adv train loss:  -0.12433263294951757 , diff:  0.014583404241420794
adv train loss:  -0.1358515649480978 , diff:  0.011518931998580229
adv train loss:  -0.11489356157835573 , diff:  0.020958003369742073
adv train loss:  -0.15833778509841068 , diff:  0.04344422352005495
adv train loss:  -0.18219969322672114 , diff:  0.02386190812831046
adv train loss:  -0.1944845396646997 , diff:  0.012284846437978558
adv train loss:  -0.1339348284236621 , diff:  0.060549711241037585
adv train loss:  -0.13614858641813044 , diff:  0.002213757994468324
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  129
test acc: top1 ->  15.2 ; top5 ->  50.0  and loss:  7992.716049194336
forward train acc: top1 ->  91.24599997558593 ; top5 ->  97.958  and loss:  53.11368655227125
test acc: top1 ->  90.52 ; top5 ->  98.3  and loss:  57.545571371912956
forward train acc: top1 ->  99.53799997558593 ; top5 ->  100.0  and loss:  2.0206676218658686
test acc: top1 ->  91.35 ; top5 ->  98.56  and loss:  54.58104867488146
forward train acc: top1 ->  99.786 ; top5 ->  100.0  and loss:  1.1230463720858097
test acc: top1 ->  91.49 ; top5 ->  98.63  and loss:  55.014616169035435
forward train acc: top1 ->  99.84799997558594 ; top5 ->  100.0  and loss:  0.7679924441035837
test acc: top1 ->  91.56 ; top5 ->  98.65  and loss:  56.66730834543705
forward train acc: top1 ->  99.86599997558594 ; top5 ->  100.0  and loss:  0.5922608773689717
test acc: top1 ->  91.58 ; top5 ->  98.69  and loss:  57.474552758038044
forward train acc: top1 ->  99.9 ; top5 ->  99.998  and loss:  0.4784881486557424
test acc: top1 ->  91.68 ; top5 ->  98.72  and loss:  58.104786932468414
forward train acc: top1 ->  99.904 ; top5 ->  100.0  and loss:  0.45898686454165727
test acc: top1 ->  91.64 ; top5 ->  98.72  and loss:  58.428827077150345
forward train acc: top1 ->  99.936 ; top5 ->  100.0  and loss:  0.3503548647277057
test acc: top1 ->  91.7 ; top5 ->  98.75  and loss:  58.536189422011375
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.342259505065158
test acc: top1 ->  91.57 ; top5 ->  98.74  and loss:  59.35981250554323
forward train acc: top1 ->  99.92999997558594 ; top5 ->  100.0  and loss:  0.3136028185253963
test acc: top1 ->  91.71 ; top5 ->  98.83  and loss:  59.33847388625145
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  257 / 512 , inc:  128
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
adv train loss:  -8.515640752390027 , diff:  8.515640752390027
adv train loss:  -7.988340828567743 , diff:  0.5272999238222837
adv train loss:  -8.26866084523499 , diff:  0.2803200166672468
adv train loss:  -8.295423727482557 , diff:  0.026762882247567177
adv train loss:  -8.218967789784074 , diff:  0.07645593769848347
adv train loss:  -8.485930670052767 , diff:  0.26696288026869297
adv train loss:  -8.507339449599385 , diff:  0.02140877954661846
adv train loss:  -8.503942884504795 , diff:  0.003396565094590187
layer  13  adv train finish, try to retain  497
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  126
layer  8  :  0.501953125  ==>  257 / 512 , inc:  64
layer  9  :  0.251953125  ==>  129 / 512 , inc:  64
layer  10  :  0.037109375  ==>  19 / 512 , inc:  1
layer  11  :  0.501953125  ==>  257 / 512 , inc:  64
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.15234375  ==>  78 / 512 , inc:  1
eps [0.0014238281250000002, 0.08100000000000002, 0.08100000000000002, 0.08100000000000002, 0.08100000000000002, 0.08100000000000002, 0.0014238281250000002, 0.00023730468750000005, 0.00023730468750000005, 0.00031640625000000006, 0.030375000000000006, 0.00023730468750000005, 0.06075000000000001, 0.08100000000000002]  wait [3, 2, 2, 2, 2, 2, 3, 0, 2, 0, 2, 2, 3, 2]  inc [1, 1, 1, 1, 1, 1, 1, 126, 64, 64, 1, 64, 1, 1]  tol: 2
$$$$$$$$$$$$$ epoch  15  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
adv train loss:  -4.193343699909747 , diff:  4.193343699909747
adv train loss:  -3.878701590001583 , diff:  0.31464210990816355
adv train loss:  -4.14359200745821 , diff:  0.2648904174566269
adv train loss:  -3.9683747133240104 , diff:  0.17521729413419962
adv train loss:  -3.937152777798474 , diff:  0.031221935525536537
adv train loss:  -4.054900566115975 , diff:  0.11774778831750154
adv train loss:  -3.9362947177141905 , diff:  0.1186058484017849
adv train loss:  -4.280136961489916 , diff:  0.34384224377572536
adv train loss:  -3.98441303614527 , diff:  0.295723925344646
adv train loss:  -4.033963442314416 , diff:  0.0495504061691463
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  54
test acc: top1 ->  86.16 ; top5 ->  98.42  and loss:  132.97098502516747
forward train acc: top1 ->  99.6 ; top5 ->  99.996  and loss:  1.4776323105907068
test acc: top1 ->  91.14 ; top5 ->  98.84  and loss:  82.72108125686646
forward train acc: top1 ->  99.76 ; top5 ->  99.998  and loss:  0.8112080335849896
test acc: top1 ->  91.27 ; top5 ->  98.92  and loss:  80.39663847535849
forward train acc: top1 ->  99.814 ; top5 ->  100.0  and loss:  0.611753974633757
test acc: top1 ->  91.53 ; top5 ->  98.93  and loss:  77.59847272187471
forward train acc: top1 ->  99.838 ; top5 ->  99.998  and loss:  0.5431743446170003
test acc: top1 ->  91.5 ; top5 ->  98.97  and loss:  76.25150833278894
forward train acc: top1 ->  99.844 ; top5 ->  100.0  and loss:  0.558614634515834
test acc: top1 ->  91.55 ; top5 ->  98.98  and loss:  74.62229435145855
forward train acc: top1 ->  99.86599997558594 ; top5 ->  100.0  and loss:  0.43924407548911404
test acc: top1 ->  91.66 ; top5 ->  99.04  and loss:  73.5347465723753
forward train acc: top1 ->  99.85400000244141 ; top5 ->  100.0  and loss:  0.37550625674339244
test acc: top1 ->  91.76 ; top5 ->  99.09  and loss:  72.8410883396864
forward train acc: top1 ->  99.854 ; top5 ->  100.0  and loss:  0.44769848363648634
test acc: top1 ->  91.58 ; top5 ->  99.05  and loss:  74.43170638382435
forward train acc: top1 ->  99.89599997558594 ; top5 ->  99.998  and loss:  0.3030831890500849
test acc: top1 ->  91.65 ; top5 ->  99.01  and loss:  73.55246154218912
forward train acc: top1 ->  99.874 ; top5 ->  100.0  and loss:  0.3609785106527852
test acc: top1 ->  91.56 ; top5 ->  99.05  and loss:  74.81055065244436
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  55 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -0.11681389759178273 , diff:  0.11681389759178273
adv train loss:  -0.10479907484113937 , diff:  0.012014822750643361
adv train loss:  -0.0706095858658955 , diff:  0.034189488975243876
adv train loss:  -0.09542519474416622 , diff:  0.024815608878270723
adv train loss:  -0.08385203243960859 , diff:  0.011573162304557627
adv train loss:  -0.08288825199997518 , diff:  0.0009637804396334104
************ all values are small in this layer **********
layer  2  adv train finish, try to retain  99
test acc: top1 ->  78.81 ; top5 ->  95.87  and loss:  202.3273447751999
forward train acc: top1 ->  99.49999997558594 ; top5 ->  99.996  and loss:  1.791750645963475
test acc: top1 ->  90.88 ; top5 ->  98.96  and loss:  73.85654850304127
forward train acc: top1 ->  99.63 ; top5 ->  99.998  and loss:  1.2339212320221122
test acc: top1 ->  90.96 ; top5 ->  99.01  and loss:  69.7514410763979
forward train acc: top1 ->  99.666 ; top5 ->  100.0  and loss:  1.0012186305830255
test acc: top1 ->  91.13 ; top5 ->  99.02  and loss:  71.18862949311733
forward train acc: top1 ->  99.70599997558594 ; top5 ->  99.998  and loss:  0.9338859547278844
test acc: top1 ->  91.25 ; top5 ->  99.05  and loss:  68.33590087294579
forward train acc: top1 ->  99.756 ; top5 ->  99.994  and loss:  0.7168511003837921
test acc: top1 ->  91.17 ; top5 ->  99.1  and loss:  67.93674191832542
forward train acc: top1 ->  99.7280000024414 ; top5 ->  99.998  and loss:  0.7571849004307296
test acc: top1 ->  91.29 ; top5 ->  99.06  and loss:  68.0311901718378
forward train acc: top1 ->  99.774 ; top5 ->  99.998  and loss:  0.7430547826224938
test acc: top1 ->  91.31 ; top5 ->  99.08  and loss:  67.71113619208336
forward train acc: top1 ->  99.7900000024414 ; top5 ->  99.998  and loss:  0.7490548796777148
test acc: top1 ->  91.29 ; top5 ->  99.1  and loss:  66.7800310999155
forward train acc: top1 ->  99.74799997558594 ; top5 ->  99.998  and loss:  0.7781424995046109
test acc: top1 ->  91.26 ; top5 ->  99.08  and loss:  66.03364707529545
forward train acc: top1 ->  99.80600000244141 ; top5 ->  99.998  and loss:  0.5876229231944308
test acc: top1 ->  91.28 ; top5 ->  99.07  and loss:  66.55856432020664
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -0.10304596361675067 , diff:  0.10304596361675067
adv train loss:  -0.10698320544906892 , diff:  0.00393724183231825
************ all values are small in this layer **********
layer  3  adv train finish, try to retain  86
test acc: top1 ->  84.15 ; top5 ->  98.02  and loss:  122.93476855754852
forward train acc: top1 ->  98.3200000024414 ; top5 ->  99.966  and loss:  5.722734442912042
test acc: top1 ->  89.74 ; top5 ->  98.92  and loss:  60.41927141696215
forward train acc: top1 ->  98.78000000488281 ; top5 ->  99.982  and loss:  3.982609413564205
test acc: top1 ->  89.82 ; top5 ->  98.98  and loss:  56.38835483044386
forward train acc: top1 ->  98.95599997558594 ; top5 ->  99.996  and loss:  3.2218050165101886
test acc: top1 ->  90.13 ; top5 ->  98.98  and loss:  53.94705931097269
forward train acc: top1 ->  98.97199998046875 ; top5 ->  99.992  and loss:  3.0382370306178927
test acc: top1 ->  90.16 ; top5 ->  98.98  and loss:  54.512006506323814
forward train acc: top1 ->  99.11400000488281 ; top5 ->  99.988  and loss:  2.6528232023119926
test acc: top1 ->  90.32 ; top5 ->  99.02  and loss:  53.592698104679585
forward train acc: top1 ->  99.2640000024414 ; top5 ->  99.99  and loss:  2.211052136030048
test acc: top1 ->  90.25 ; top5 ->  99.04  and loss:  54.48050773143768
forward train acc: top1 ->  99.21200000488281 ; top5 ->  99.996  and loss:  2.4283181154169142
test acc: top1 ->  90.38 ; top5 ->  98.99  and loss:  54.326822213828564
forward train acc: top1 ->  99.30600000488282 ; top5 ->  100.0  and loss:  2.003313970984891
test acc: top1 ->  90.38 ; top5 ->  99.02  and loss:  55.0074742436409
forward train acc: top1 ->  99.28199998046875 ; top5 ->  99.998  and loss:  2.1277496996335685
test acc: top1 ->  90.32 ; top5 ->  99.0  and loss:  53.821253918111324
forward train acc: top1 ->  99.30199997558594 ; top5 ->  99.998  and loss:  2.136148071847856
test acc: top1 ->  90.39 ; top5 ->  99.06  and loss:  53.10611962527037
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -0.261802286375314 , diff:  0.261802286375314
adv train loss:  -0.29965872602770105 , diff:  0.03785643965238705
adv train loss:  -0.3094149472017307 , diff:  0.009756221174029633
************ all values are small in this layer **********
layer  4  adv train finish, try to retain  154
test acc: top1 ->  56.4 ; top5 ->  90.06  and loss:  186.19388329982758
forward train acc: top1 ->  95.02799999511718 ; top5 ->  99.846  and loss:  16.897458650171757
test acc: top1 ->  88.1 ; top5 ->  98.84  and loss:  54.1965678781271
forward train acc: top1 ->  96.07399997314454 ; top5 ->  99.896  and loss:  12.072235062718391
test acc: top1 ->  88.51 ; top5 ->  98.91  and loss:  50.77377246320248
forward train acc: top1 ->  96.44799999267578 ; top5 ->  99.938  and loss:  10.791838444769382
test acc: top1 ->  88.98 ; top5 ->  99.02  and loss:  49.50948831439018
forward train acc: top1 ->  96.64399998779297 ; top5 ->  99.93  and loss:  10.071778684854507
test acc: top1 ->  89.02 ; top5 ->  99.0  and loss:  47.84545433521271
forward train acc: top1 ->  96.98000001464844 ; top5 ->  99.952  and loss:  8.946437995880842
test acc: top1 ->  89.27 ; top5 ->  99.01  and loss:  48.331290140748024
forward train acc: top1 ->  97.09999999267578 ; top5 ->  99.954  and loss:  8.501981265842915
test acc: top1 ->  89.33 ; top5 ->  99.02  and loss:  47.325119361281395
forward train acc: top1 ->  97.18199998535157 ; top5 ->  99.968  and loss:  8.17212188616395
test acc: top1 ->  89.37 ; top5 ->  99.0  and loss:  47.449582636356354
forward train acc: top1 ->  97.30399998046875 ; top5 ->  99.986  and loss:  7.91489189863205
test acc: top1 ->  89.47 ; top5 ->  99.05  and loss:  46.824705854058266
forward train acc: top1 ->  97.26999998535156 ; top5 ->  99.96799997558594  and loss:  7.8464580103755
test acc: top1 ->  89.5 ; top5 ->  99.04  and loss:  46.913967341184616
forward train acc: top1 ->  97.39800001708984 ; top5 ->  99.972  and loss:  7.735908795148134
test acc: top1 ->  89.66 ; top5 ->  99.07  and loss:  46.737019434571266
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -0.7163291762117296 , diff:  0.7163291762117296
adv train loss:  -0.738478138577193 , diff:  0.022148962365463376
adv train loss:  -0.733144469326362 , diff:  0.005333669250831008
************ all values are small in this layer **********
layer  5  adv train finish, try to retain  148
test acc: top1 ->  69.4 ; top5 ->  92.69  and loss:  128.65500432252884
forward train acc: top1 ->  98.30600000976563 ; top5 ->  99.974  and loss:  5.068643806502223
test acc: top1 ->  89.9 ; top5 ->  99.08  and loss:  49.29880338162184
forward train acc: top1 ->  98.6800000024414 ; top5 ->  99.992  and loss:  3.8488762602210045
test acc: top1 ->  89.92 ; top5 ->  99.04  and loss:  51.09731964021921
forward train acc: top1 ->  98.83999997802735 ; top5 ->  99.994  and loss:  3.4976965049281716
test acc: top1 ->  90.11 ; top5 ->  99.04  and loss:  50.66052704304457
forward train acc: top1 ->  98.95400000732423 ; top5 ->  99.998  and loss:  3.102823437191546
test acc: top1 ->  90.17 ; top5 ->  99.08  and loss:  51.38997957855463
forward train acc: top1 ->  99.108 ; top5 ->  99.992  and loss:  2.68229503929615
test acc: top1 ->  90.22 ; top5 ->  99.1  and loss:  51.69169693440199
forward train acc: top1 ->  99.12799997802735 ; top5 ->  100.0  and loss:  2.43660157173872
test acc: top1 ->  90.19 ; top5 ->  99.12  and loss:  53.25354542955756
forward train acc: top1 ->  99.09800000732422 ; top5 ->  99.992  and loss:  2.60094178840518
test acc: top1 ->  90.22 ; top5 ->  99.08  and loss:  53.71857528015971
forward train acc: top1 ->  99.1860000024414 ; top5 ->  99.998  and loss:  2.3215023232623935
test acc: top1 ->  90.29 ; top5 ->  99.08  and loss:  53.08326865360141
forward train acc: top1 ->  99.156 ; top5 ->  99.996  and loss:  2.4253887026570737
test acc: top1 ->  90.29 ; top5 ->  99.09  and loss:  53.50597872957587
forward train acc: top1 ->  99.23599997558594 ; top5 ->  99.996  and loss:  2.157305696979165
test acc: top1 ->  90.32 ; top5 ->  99.13  and loss:  54.715309381484985
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
### skip layer  6 wait:  3  ###
---------------- start layer  7  ---------------
adv train loss:  -0.28801197596476413 , diff:  0.28801197596476413
adv train loss:  -0.31196534479386173 , diff:  0.0239533688290976
adv train loss:  -0.31084725420805626 , diff:  0.0011180905858054757
layer  7  adv train finish, try to retain  272
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -0.3620283417403698 , diff:  0.3620283417403698
adv train loss:  -0.3055386572377756 , diff:  0.05648968450259417
adv train loss:  -0.3620656461280305 , diff:  0.05652698889025487
adv train loss:  -0.3484914097643923 , diff:  0.013574236363638192
adv train loss:  -0.36883103783475235 , diff:  0.02033962807036005
adv train loss:  -0.3524305879836902 , diff:  0.01640044985106215
adv train loss:  -0.28113687434233725 , diff:  0.07129371364135295
adv train loss:  -0.33572646431275643 , diff:  0.05458958997041918
adv train loss:  -0.3599168333748821 , diff:  0.024190369062125683
adv train loss:  -0.3144492892024573 , diff:  0.04546754417242482
layer  8  adv train finish, try to retain  298
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -6.712430858053267 , diff:  6.712430858053267
adv train loss:  -6.52357355505228 , diff:  0.18885730300098658
adv train loss:  -6.578441192395985 , diff:  0.0548676373437047
adv train loss:  -6.393488840200007 , diff:  0.18495235219597816
adv train loss:  -6.387141494080424 , diff:  0.006347346119582653
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  65
test acc: top1 ->  35.02 ; top5 ->  83.14  and loss:  587.7814407348633
forward train acc: top1 ->  99.71999997558594 ; top5 ->  100.0  and loss:  1.0335155221619061
test acc: top1 ->  91.88 ; top5 ->  99.18  and loss:  65.13566323369741
forward train acc: top1 ->  99.94399997558594 ; top5 ->  100.0  and loss:  0.15630383379539126
test acc: top1 ->  92.04 ; top5 ->  99.17  and loss:  64.82182962447405
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.09615132144244853
test acc: top1 ->  92.1 ; top5 ->  99.12  and loss:  65.52598747611046
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.11689903943261015
test acc: top1 ->  92.12 ; top5 ->  99.17  and loss:  65.37013549357653
==> this epoch:  65 / 512
---------------- start layer  10  ---------------
adv train loss:  -1.1057851501973346 , diff:  1.1057851501973346
adv train loss:  -1.1487374453572556 , diff:  0.04295229515992105
adv train loss:  -1.0956240604864433 , diff:  0.0531133848708123
adv train loss:  -1.1620896866661496 , diff:  0.0664656261797063
adv train loss:  -1.1463246064959094 , diff:  0.015765080170240253
adv train loss:  -1.154169048415497 , diff:  0.007844441919587553
layer  10  adv train finish, try to retain  472
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -0.06951244638185017 , diff:  0.06951244638185017
adv train loss:  -0.09462848099428811 , diff:  0.025116034612437943
adv train loss:  -0.0656338142543973 , diff:  0.028994666739890818
adv train loss:  -0.09780867800145643 , diff:  0.032174863747059135
adv train loss:  -0.10971156710002106 , diff:  0.011902889098564629
adv train loss:  -0.07362749711683136 , diff:  0.03608406998318969
adv train loss:  -0.08054382192494813 , diff:  0.006916324808116769
layer  11  adv train finish, try to retain  317
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
### skip layer  12 wait:  3  ###
---------------- start layer  13  ---------------
adv train loss:  -0.0827681037626462 , diff:  0.0827681037626462
adv train loss:  -0.09571324248827295 , diff:  0.012945138725626748
adv train loss:  -0.11004271077763406 , diff:  0.014329468289361103
adv train loss:  -0.10350648663734319 , diff:  0.006536224140290869
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  77
test acc: top1 ->  67.68 ; top5 ->  98.53  and loss:  98.1431970000267
forward train acc: top1 ->  97.70399997558594 ; top5 ->  99.99  and loss:  7.356275424361229
test acc: top1 ->  91.38 ; top5 ->  98.61  and loss:  40.277934800833464
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  1.0913163544610143
test acc: top1 ->  91.9 ; top5 ->  98.67  and loss:  41.49969671666622
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.615023743826896
test acc: top1 ->  92.08 ; top5 ->  98.71  and loss:  43.274277187883854
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.3669850879814476
test acc: top1 ->  92.01 ; top5 ->  98.72  and loss:  44.8847878575325
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.26535078207962215
test acc: top1 ->  92.11 ; top5 ->  98.81  and loss:  46.19738610833883
==> this epoch:  77 / 512
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  126
layer  8  :  0.501953125  ==>  257 / 512 , inc:  64
layer  9  :  0.126953125  ==>  65 / 512 , inc:  32
layer  10  :  0.037109375  ==>  19 / 512 , inc:  1
layer  11  :  0.501953125  ==>  257 / 512 , inc:  64
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.150390625  ==>  77 / 512 , inc:  2
eps [0.0014238281250000002, 0.06075000000000001, 0.06075000000000001, 0.06075000000000001, 0.06075000000000001, 0.06075000000000001, 0.0014238281250000002, 0.0004746093750000001, 0.0004746093750000001, 0.00031640625000000006, 0.06075000000000001, 0.0004746093750000001, 0.06075000000000001, 0.08100000000000002]  wait [2, 4, 4, 4, 4, 4, 2, 0, 2, 0, 2, 2, 2, 0]  inc [1, 1, 1, 1, 1, 1, 1, 126, 64, 32, 1, 64, 1, 2]  tol: 2
$$$$$$$$$$$$$ epoch  16  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -0.25308712071273476 , diff:  0.25308712071273476
adv train loss:  -0.25114434747956693 , diff:  0.0019427732331678271
layer  0  adv train finish, try to retain  64
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
### skip layer  1 wait:  4  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
adv train loss:  -0.21931631257757545 , diff:  0.21931631257757545
adv train loss:  -0.25854860281106085 , diff:  0.0392322902334854
adv train loss:  -0.2698976189130917 , diff:  0.011349016102030873
adv train loss:  -0.23928705661091954 , diff:  0.030610562302172184
adv train loss:  -0.24428377859294415 , diff:  0.00499672198202461
layer  6  adv train finish, try to retain  256
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -0.33958332799375057 , diff:  0.33958332799375057
adv train loss:  -0.32310458726715297 , diff:  0.016478740726597607
adv train loss:  -0.3103601667098701 , diff:  0.012744420557282865
adv train loss:  -0.3326275296276435 , diff:  0.022267362917773426
adv train loss:  -0.3149674490559846 , diff:  0.01766008057165891
adv train loss:  -0.3462892717216164 , diff:  0.03132182266563177
adv train loss:  -0.2971477204700932 , diff:  0.0491415512515232
adv train loss:  -0.33813675364945084 , diff:  0.04098903317935765
adv train loss:  -0.32519831391982734 , diff:  0.012938439729623497
adv train loss:  -0.31097847991622984 , diff:  0.014219834003597498
layer  7  adv train finish, try to retain  262
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -0.2633529610466212 , diff:  0.2633529610466212
adv train loss:  -0.26212176866829395 , diff:  0.0012311923783272505
layer  8  adv train finish, try to retain  267
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -0.5365576413460076 , diff:  0.5365576413460076
adv train loss:  -0.5803061197511852 , diff:  0.04374847840517759
adv train loss:  -0.5553124079015106 , diff:  0.024993711849674582
adv train loss:  -0.5848877022508532 , diff:  0.029575294349342585
adv train loss:  -0.5475724274292588 , diff:  0.03731527482159436
adv train loss:  -0.5292744147591293 , diff:  0.018298012670129538
adv train loss:  -0.5671062534675002 , diff:  0.037831838708370924
adv train loss:  -0.5937337833456695 , diff:  0.026627529878169298
adv train loss:  -0.5429502741899341 , diff:  0.050783509155735373
adv train loss:  -0.5678261774592102 , diff:  0.024875903269276023
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  33
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  758.5530734062195
forward train acc: top1 ->  72.00000000732422 ; top5 ->  94.28799997558593  and loss:  105.03413335978985
test acc: top1 ->  69.66 ; top5 ->  97.49  and loss:  99.91136711835861
forward train acc: top1 ->  98.55400000488281 ; top5 ->  99.996  and loss:  11.784147679805756
test acc: top1 ->  89.08 ; top5 ->  98.46  and loss:  45.77651987969875
forward train acc: top1 ->  99.30000000244141 ; top5 ->  100.0  and loss:  5.853803627192974
test acc: top1 ->  89.67 ; top5 ->  98.55  and loss:  44.27154478430748
forward train acc: top1 ->  99.4560000024414 ; top5 ->  100.0  and loss:  4.097212221473455
test acc: top1 ->  90.07 ; top5 ->  98.6  and loss:  43.92638187110424
forward train acc: top1 ->  99.53200000488282 ; top5 ->  99.996  and loss:  3.1886375304311514
test acc: top1 ->  90.21 ; top5 ->  98.62  and loss:  44.34345254302025
forward train acc: top1 ->  99.612 ; top5 ->  100.0  and loss:  2.534204864874482
test acc: top1 ->  90.03 ; top5 ->  98.61  and loss:  45.11625921726227
forward train acc: top1 ->  99.6500000024414 ; top5 ->  99.998  and loss:  2.2697039833292365
test acc: top1 ->  90.2 ; top5 ->  98.64  and loss:  44.66587796807289
forward train acc: top1 ->  99.70599997558594 ; top5 ->  100.0  and loss:  2.016719378530979
test acc: top1 ->  90.18 ; top5 ->  98.64  and loss:  44.935204923152924
forward train acc: top1 ->  99.73399997802734 ; top5 ->  100.0  and loss:  1.8184445789083838
test acc: top1 ->  90.42 ; top5 ->  98.65  and loss:  44.83969621360302
forward train acc: top1 ->  99.766 ; top5 ->  100.0  and loss:  1.5905357459560037
test acc: top1 ->  90.51 ; top5 ->  98.72  and loss:  45.10724197328091
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  65 / 512 , inc:  32
---------------- start layer  10  ---------------
adv train loss:  -51.60838824510574 , diff:  51.60838824510574
adv train loss:  -51.9180970788002 , diff:  0.309708833694458
adv train loss:  -52.16664785146713 , diff:  0.24855077266693115
adv train loss:  -52.513598531484604 , diff:  0.3469506800174713
adv train loss:  -51.65843549370766 , diff:  0.855163037776947
adv train loss:  -51.94640111923218 , diff:  0.2879656255245209
adv train loss:  -51.6977436542511 , diff:  0.2486574649810791
adv train loss:  -52.19665774703026 , diff:  0.49891409277915955
adv train loss:  -52.17750549316406 , diff:  0.01915225386619568
layer  10  adv train finish, try to retain  480
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -12.351580202579498 , diff:  12.351580202579498
adv train loss:  -12.568231008946896 , diff:  0.2166508063673973
adv train loss:  -12.648047149181366 , diff:  0.07981614023447037
adv train loss:  -12.266049765050411 , diff:  0.38199738413095474
adv train loss:  -12.344852536916733 , diff:  0.07880277186632156
adv train loss:  -12.739217601716518 , diff:  0.3943650647997856
adv train loss:  -12.038065955042839 , diff:  0.7011516466736794
adv train loss:  -12.586573049426079 , diff:  0.5485070943832397
adv train loss:  -12.724587321281433 , diff:  0.1380142718553543
adv train loss:  -12.593060962855816 , diff:  0.13152635842561722
layer  11  adv train finish, try to retain  353
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -13.346304412931204 , diff:  13.346304412931204
adv train loss:  -13.195140473544598 , diff:  0.15116393938660622
adv train loss:  -13.20314447954297 , diff:  0.008004005998373032
layer  12  adv train finish, try to retain  453
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -9.104746067896485 , diff:  9.104746067896485
adv train loss:  -8.837779324501753 , diff:  0.2669667433947325
adv train loss:  -9.218877084553242 , diff:  0.3810977600514889
adv train loss:  -9.028171926736832 , diff:  0.19070515781641006
adv train loss:  -8.824326448142529 , diff:  0.20384547859430313
adv train loss:  -8.852662334218621 , diff:  0.02833588607609272
adv train loss:  -8.899193465709686 , diff:  0.046531131491065025
adv train loss:  -9.029905036091805 , diff:  0.13071157038211823
adv train loss:  -8.866162430495024 , diff:  0.16374260559678078
adv train loss:  -9.091381069272757 , diff:  0.22521863877773285
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  75
test acc: top1 ->  86.19 ; top5 ->  98.68  and loss:  81.17502121627331
forward train acc: top1 ->  99.654 ; top5 ->  99.998  and loss:  1.2316091868269723
test acc: top1 ->  91.85 ; top5 ->  99.16  and loss:  58.18919935822487
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.13213148187060142
test acc: top1 ->  92.1 ; top5 ->  99.21  and loss:  59.53729226440191
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.08898512680752901
test acc: top1 ->  92.15 ; top5 ->  99.19  and loss:  60.71644355356693
==> this epoch:  75 / 512
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  126
layer  8  :  0.501953125  ==>  257 / 512 , inc:  64
layer  9  :  0.126953125  ==>  65 / 512 , inc:  16
layer  10  :  0.037109375  ==>  19 / 512 , inc:  1
layer  11  :  0.501953125  ==>  257 / 512 , inc:  64
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.146484375  ==>  75 / 512 , inc:  4
eps [0.0028476562500000004, 0.06075000000000001, 0.06075000000000001, 0.06075000000000001, 0.06075000000000001, 0.06075000000000001, 0.0028476562500000004, 0.0009492187500000002, 0.0009492187500000002, 0.00023730468750000005, 0.12150000000000002, 0.0009492187500000002, 0.12150000000000002, 0.08100000000000002]  wait [2, 3, 3, 3, 3, 3, 2, 0, 2, 2, 2, 2, 2, 0]  inc [1, 1, 1, 1, 1, 1, 1, 126, 64, 16, 1, 64, 1, 4]  tol: 2
$$$$$$$$$$$$$ epoch  17  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -0.06886695163848344 , diff:  0.06886695163848344
adv train loss:  -0.08497477025230182 , diff:  0.01610781861381838
adv train loss:  -0.07553454075969057 , diff:  0.009440229492611252
layer  0  adv train finish, try to retain  64
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
### skip layer  1 wait:  3  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
adv train loss:  -0.06685867111082189 , diff:  0.06685867111082189
adv train loss:  -0.0646267176780384 , diff:  0.0022319534327834845
layer  6  adv train finish, try to retain  256
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -0.10386082165496191 , diff:  0.10386082165496191
adv train loss:  -0.10110806791635696 , diff:  0.0027527537386049516
layer  7  adv train finish, try to retain  238
test acc: top1 ->  19.45 ; top5 ->  76.46  and loss:  532.9971137046814
forward train acc: top1 ->  97.96399998291015 ; top5 ->  99.996  and loss:  6.616430353373289
test acc: top1 ->  89.82 ; top5 ->  98.76  and loss:  62.39912460744381
forward train acc: top1 ->  98.93999998046876 ; top5 ->  99.996  and loss:  3.114147815387696
test acc: top1 ->  90.42 ; top5 ->  98.87  and loss:  59.05451651662588
forward train acc: top1 ->  99.17600000244141 ; top5 ->  99.998  and loss:  2.502592603676021
test acc: top1 ->  90.69 ; top5 ->  98.9  and loss:  57.90163719654083
forward train acc: top1 ->  99.38799997558594 ; top5 ->  99.996  and loss:  1.780806150753051
test acc: top1 ->  90.74 ; top5 ->  98.95  and loss:  57.90922129154205
forward train acc: top1 ->  99.4920000024414 ; top5 ->  99.996  and loss:  1.5752541478723288
test acc: top1 ->  91.04 ; top5 ->  98.9  and loss:  57.71971610188484
forward train acc: top1 ->  99.53999997558594 ; top5 ->  99.998  and loss:  1.3649040351156145
test acc: top1 ->  91.12 ; top5 ->  98.95  and loss:  57.96727013587952
forward train acc: top1 ->  99.59599997802735 ; top5 ->  100.0  and loss:  1.2039886889979243
test acc: top1 ->  91.16 ; top5 ->  98.96  and loss:  57.72202353179455
forward train acc: top1 ->  99.64000000244141 ; top5 ->  99.998  and loss:  1.196908671525307
test acc: top1 ->  91.15 ; top5 ->  99.02  and loss:  57.672989912331104
forward train acc: top1 ->  99.64 ; top5 ->  99.998  and loss:  1.0041685245232657
test acc: top1 ->  91.1 ; top5 ->  98.98  and loss:  58.167615696787834
forward train acc: top1 ->  99.642 ; top5 ->  100.0  and loss:  1.097094145952724
test acc: top1 ->  91.25 ; top5 ->  99.01  and loss:  58.18681846559048
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  253 / 512 , inc:  126
---------------- start layer  8  ---------------
adv train loss:  -0.723794783858466 , diff:  0.723794783858466
adv train loss:  -0.68762034032261 , diff:  0.03617444353585597
adv train loss:  -0.7350903890910558 , diff:  0.04747004876844585
adv train loss:  -0.7132627953978954 , diff:  0.021827593693160452
adv train loss:  -0.6967266999708954 , diff:  0.016536095426999964
adv train loss:  -0.7442347369797062 , diff:  0.04750803700881079
adv train loss:  -0.6912981332279742 , diff:  0.052936603751732036
adv train loss:  -0.6685249798174482 , diff:  0.022773153410525993
adv train loss:  -0.7398995267576538 , diff:  0.07137454694020562
adv train loss:  -0.8071782350598369 , diff:  0.06727870830218308
layer  8  adv train finish, try to retain  299
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -0.6136607138614636 , diff:  0.6136607138614636
adv train loss:  -0.5247922676499002 , diff:  0.08886844621156342
adv train loss:  -0.4957871847145725 , diff:  0.029005082935327664
adv train loss:  -0.39581993693718687 , diff:  0.09996724777738564
adv train loss:  -0.5017997608811129 , diff:  0.10597982394392602
adv train loss:  -0.6779971185897011 , diff:  0.17619735770858824
adv train loss:  -0.6037435410544276 , diff:  0.07425357753527351
adv train loss:  -0.5057248624507338 , diff:  0.09801867860369384
adv train loss:  -0.5996362440346275 , diff:  0.09391138158389367
adv train loss:  -0.5551315657212399 , diff:  0.04450467831338756
layer  9  adv train finish, try to retain  455
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -3.029692112468183 , diff:  3.029692112468183
adv train loss:  -3.1357192369177938 , diff:  0.10602712444961071
adv train loss:  -3.1377635411918163 , diff:  0.002044304274022579
layer  10  adv train finish, try to retain  479
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -0.6637026292155497 , diff:  0.6637026292155497
adv train loss:  -0.522171884906129 , diff:  0.1415307443094207
adv train loss:  -0.5761744739720598 , diff:  0.054002589065930806
adv train loss:  -0.5598190753953531 , diff:  0.016355398576706648
adv train loss:  -0.6558034279732965 , diff:  0.09598435257794335
adv train loss:  -0.6107514356699539 , diff:  0.04505199230334256
adv train loss:  -0.7033242390316445 , diff:  0.0925728033616906
adv train loss:  -0.7107266538077965 , diff:  0.007402414776152
layer  11  adv train finish, try to retain  321
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -0.9128047289850656 , diff:  0.9128047289850656
adv train loss:  -0.7433055896253791 , diff:  0.16949913935968652
adv train loss:  -0.7319067312200787 , diff:  0.011398858405300416
adv train loss:  -0.6863658563815989 , diff:  0.045540874838479795
adv train loss:  -0.6379261428664904 , diff:  0.04843971351510845
adv train loss:  -0.8033361727138981 , diff:  0.1654100298474077
adv train loss:  -0.9249089224613272 , diff:  0.12157274974742904
adv train loss:  -0.9258101183950203 , diff:  0.0009011959336930886
layer  12  adv train finish, try to retain  458
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -0.864328413255862 , diff:  0.864328413255862
adv train loss:  -0.9713936816842761 , diff:  0.10706526842841413
adv train loss:  -0.9720883126719855 , diff:  0.0006946309877093881
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  71
test acc: top1 ->  87.63 ; top5 ->  98.84  and loss:  76.76760077476501
forward train acc: top1 ->  99.812 ; top5 ->  100.0  and loss:  0.7889291275641881
test acc: top1 ->  92.2 ; top5 ->  99.09  and loss:  51.47357591986656
==> this epoch:  71 / 512
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  63
layer  8  :  0.501953125  ==>  257 / 512 , inc:  64
layer  9  :  0.126953125  ==>  65 / 512 , inc:  16
layer  10  :  0.037109375  ==>  19 / 512 , inc:  1
layer  11  :  0.501953125  ==>  257 / 512 , inc:  64
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.138671875  ==>  71 / 512 , inc:  8
eps [0.005695312500000001, 0.06075000000000001, 0.06075000000000001, 0.06075000000000001, 0.06075000000000001, 0.06075000000000001, 0.005695312500000001, 0.0007119140625000001, 0.0018984375000000004, 0.0004746093750000001, 0.24300000000000005, 0.0018984375000000004, 0.24300000000000005, 0.08100000000000002]  wait [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]  inc [1, 1, 1, 1, 1, 1, 1, 63, 64, 16, 1, 64, 1, 8]  tol: 2
$$$$$$$$$$$$$ epoch  18  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -0.1909504136419855 , diff:  0.1909504136419855
adv train loss:  -0.13458365373662673 , diff:  0.056366759905358776
adv train loss:  -0.123389063810464 , diff:  0.011194589926162735
adv train loss:  -0.136560902610654 , diff:  0.013171838800190017
adv train loss:  -0.14355898508802056 , diff:  0.006998082477366552
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  31
test acc: top1 ->  47.55 ; top5 ->  87.11  and loss:  326.1129584312439
forward train acc: top1 ->  96.96000000488282 ; top5 ->  99.852  and loss:  11.37869480997324
test acc: top1 ->  81.85 ; top5 ->  97.28  and loss:  101.67242538928986
forward train acc: top1 ->  97.72199998535156 ; top5 ->  99.954  and loss:  7.354110701009631
test acc: top1 ->  89.6 ; top5 ->  99.03  and loss:  49.84421990811825
forward train acc: top1 ->  98.17600000732422 ; top5 ->  99.972  and loss:  5.583980975672603
test acc: top1 ->  89.77 ; top5 ->  99.08  and loss:  49.61519318819046
forward train acc: top1 ->  98.46399998046876 ; top5 ->  99.964  and loss:  4.84737079590559
test acc: top1 ->  89.93 ; top5 ->  99.04  and loss:  48.96723531186581
forward train acc: top1 ->  98.59799998291015 ; top5 ->  99.986  and loss:  4.330721207894385
test acc: top1 ->  89.91 ; top5 ->  99.12  and loss:  49.13383135199547
forward train acc: top1 ->  98.7480000024414 ; top5 ->  99.986  and loss:  3.8539315555244684
test acc: top1 ->  90.07 ; top5 ->  99.16  and loss:  48.369553312659264
forward train acc: top1 ->  98.7220000024414 ; top5 ->  99.98  and loss:  3.921686314046383
test acc: top1 ->  90.09 ; top5 ->  99.15  and loss:  47.93988877534866
forward train acc: top1 ->  98.88999997802735 ; top5 ->  99.976  and loss:  3.4250192660838366
test acc: top1 ->  90.08 ; top5 ->  99.16  and loss:  48.42488780617714
forward train acc: top1 ->  98.90000000732422 ; top5 ->  99.982  and loss:  3.3991087884642184
test acc: top1 ->  90.17 ; top5 ->  99.13  and loss:  48.70910958945751
forward train acc: top1 ->  98.9900000024414 ; top5 ->  99.986  and loss:  2.962810479104519
test acc: top1 ->  90.1 ; top5 ->  99.15  and loss:  49.05550119280815
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -0.22092773480108008 , diff:  0.22092773480108008
adv train loss:  -0.23941941210068762 , diff:  0.018491677299607545
adv train loss:  -0.2304155514575541 , diff:  0.009003860643133521
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -0.22353460907470435 , diff:  0.22353460907470435
adv train loss:  -0.2558817396638915 , diff:  0.032347130589187145
adv train loss:  -0.2665971741080284 , diff:  0.010715434444136918
adv train loss:  -0.255528257926926 , diff:  0.011068916181102395
adv train loss:  -0.23937929538078606 , diff:  0.016148962546139956
adv train loss:  -0.2392103661550209 , diff:  0.00016892922576516867
layer  2  adv train finish, try to retain  128
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -0.21710498887114227 , diff:  0.21710498887114227
adv train loss:  -0.25208463449962437 , diff:  0.0349796456284821
adv train loss:  -0.2418681812705472 , diff:  0.01021645322907716
adv train loss:  -0.21246830379823223 , diff:  0.029399877472314984
adv train loss:  -0.2247784334467724 , diff:  0.012310129648540169
adv train loss:  -0.22154755075462162 , diff:  0.0032308826921507716
layer  3  adv train finish, try to retain  128
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -0.2516650023753755 , diff:  0.2516650023753755
adv train loss:  -0.25138320546830073 , diff:  0.00028179690707474947
layer  4  adv train finish, try to retain  254
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -0.23643360973801464 , diff:  0.23643360973801464
adv train loss:  -0.25017485808348283 , diff:  0.013741248345468193
adv train loss:  -0.23588842491153628 , diff:  0.014286433171946555
adv train loss:  -0.2455846683587879 , diff:  0.009696243447251618
layer  5  adv train finish, try to retain  256
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -0.22280491678975523 , diff:  0.22280491678975523
adv train loss:  -0.2567235251190141 , diff:  0.03391860832925886
adv train loss:  -0.24050461064325646 , diff:  0.01621891447575763
adv train loss:  -0.2179477017489262 , diff:  0.022556908894330263
adv train loss:  -0.2305307854549028 , diff:  0.012583083705976605
adv train loss:  -0.26234950067009777 , diff:  0.03181871521519497
adv train loss:  -0.26867886423133314 , diff:  0.006329363561235368
************ all values are small in this layer **********
layer  6  adv train finish, try to retain  143
test acc: top1 ->  88.15 ; top5 ->  98.7  and loss:  52.18030969798565
forward train acc: top1 ->  99.73799997558594 ; top5 ->  100.0  and loss:  0.865127706550993
test acc: top1 ->  91.31 ; top5 ->  99.08  and loss:  49.57471136748791
forward train acc: top1 ->  99.836 ; top5 ->  100.0  and loss:  0.5519308971706778
test acc: top1 ->  91.39 ; top5 ->  99.06  and loss:  52.39696407318115
forward train acc: top1 ->  99.81799997558593 ; top5 ->  100.0  and loss:  0.5364874920924194
test acc: top1 ->  91.54 ; top5 ->  99.11  and loss:  53.35513637959957
forward train acc: top1 ->  99.854 ; top5 ->  100.0  and loss:  0.4150918259692844
test acc: top1 ->  91.54 ; top5 ->  99.1  and loss:  54.50047120451927
forward train acc: top1 ->  99.89599997558594 ; top5 ->  100.0  and loss:  0.3535831975750625
test acc: top1 ->  91.59 ; top5 ->  99.1  and loss:  56.501393884420395
forward train acc: top1 ->  99.874 ; top5 ->  100.0  and loss:  0.36739515930821653
test acc: top1 ->  91.56 ; top5 ->  99.07  and loss:  56.02325722575188
forward train acc: top1 ->  99.8560000024414 ; top5 ->  100.0  and loss:  0.4480672005447559
test acc: top1 ->  91.53 ; top5 ->  99.14  and loss:  56.38238409161568
forward train acc: top1 ->  99.89999997558594 ; top5 ->  100.0  and loss:  0.3195932664821157
test acc: top1 ->  91.69 ; top5 ->  99.08  and loss:  56.66474959254265
forward train acc: top1 ->  99.898 ; top5 ->  100.0  and loss:  0.315604917195742
test acc: top1 ->  91.66 ; top5 ->  99.14  and loss:  56.73506397008896
forward train acc: top1 ->  99.898 ; top5 ->  100.0  and loss:  0.3292039041989483
test acc: top1 ->  91.67 ; top5 ->  99.11  and loss:  57.61793199181557
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  144 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -0.12656976441212464 , diff:  0.12656976441212464
adv train loss:  -0.12218039105209755 , diff:  0.0043893733600270934
layer  7  adv train finish, try to retain  237
test acc: top1 ->  24.43 ; top5 ->  69.19  and loss:  374.513653755188
forward train acc: top1 ->  98.4680000024414 ; top5 ->  99.994  and loss:  4.743206800427288
test acc: top1 ->  90.76 ; top5 ->  98.94  and loss:  52.013035118579865
forward train acc: top1 ->  99.258 ; top5 ->  100.0  and loss:  2.337275537662208
test acc: top1 ->  90.93 ; top5 ->  98.95  and loss:  51.69985853135586
forward train acc: top1 ->  99.45799997558593 ; top5 ->  99.998  and loss:  1.6892522354610264
test acc: top1 ->  90.97 ; top5 ->  99.07  and loss:  53.16082862019539
forward train acc: top1 ->  99.59800000244141 ; top5 ->  99.998  and loss:  1.3119789778720587
test acc: top1 ->  91.09 ; top5 ->  99.09  and loss:  53.437863260507584
forward train acc: top1 ->  99.69000000244141 ; top5 ->  100.0  and loss:  1.114080282743089
test acc: top1 ->  91.19 ; top5 ->  99.15  and loss:  54.39120239019394
forward train acc: top1 ->  99.69399997558594 ; top5 ->  100.0  and loss:  0.9905935880960897
test acc: top1 ->  91.24 ; top5 ->  99.14  and loss:  54.43121610581875
forward train acc: top1 ->  99.68 ; top5 ->  100.0  and loss:  0.9623891387600452
test acc: top1 ->  91.28 ; top5 ->  99.17  and loss:  54.299860790371895
forward train acc: top1 ->  99.70799997558593 ; top5 ->  100.0  and loss:  0.8861857778392732
test acc: top1 ->  91.26 ; top5 ->  99.13  and loss:  54.650286480784416
forward train acc: top1 ->  99.75600000244141 ; top5 ->  100.0  and loss:  0.7820842748042196
test acc: top1 ->  91.31 ; top5 ->  99.15  and loss:  55.03958375751972
forward train acc: top1 ->  99.774 ; top5 ->  100.0  and loss:  0.6912375832907856
test acc: top1 ->  91.51 ; top5 ->  99.18  and loss:  55.16632807999849
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  253 / 512 , inc:  63
---------------- start layer  8  ---------------
adv train loss:  -0.1820553966317675 , diff:  0.1820553966317675
adv train loss:  -0.19265606589033268 , diff:  0.010600669258565176
adv train loss:  -0.16381714781164192 , diff:  0.028838918078690767
adv train loss:  -0.1820303798886016 , diff:  0.018213232076959684
adv train loss:  -0.1849602531437995 , diff:  0.00292987325519789
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  193
test acc: top1 ->  18.12 ; top5 ->  51.43  and loss:  845.0470724105835
forward train acc: top1 ->  79.60999997070313 ; top5 ->  99.502  and loss:  91.94747945666313
test acc: top1 ->  85.35 ; top5 ->  98.03  and loss:  72.49329423904419
forward train acc: top1 ->  96.24599998535156 ; top5 ->  99.9720000024414  and loss:  12.967481382191181
test acc: top1 ->  87.9 ; top5 ->  98.45  and loss:  65.33421149849892
forward train acc: top1 ->  98.01800000244141 ; top5 ->  99.994  and loss:  6.925777442753315
test acc: top1 ->  89.2 ; top5 ->  98.61  and loss:  61.29633843898773
forward train acc: top1 ->  98.86799997558593 ; top5 ->  99.992  and loss:  4.107185097411275
test acc: top1 ->  89.99 ; top5 ->  98.69  and loss:  58.588178247213364
forward train acc: top1 ->  99.25400000488281 ; top5 ->  99.994  and loss:  2.779014398343861
test acc: top1 ->  90.17 ; top5 ->  98.82  and loss:  57.13129186630249
forward train acc: top1 ->  99.49600000488282 ; top5 ->  100.0  and loss:  1.9648241167888045
test acc: top1 ->  90.39 ; top5 ->  98.81  and loss:  57.720125928521156
forward train acc: top1 ->  99.4980000024414 ; top5 ->  99.998  and loss:  1.829113522078842
test acc: top1 ->  90.42 ; top5 ->  98.88  and loss:  57.20435542613268
forward train acc: top1 ->  99.57399997558593 ; top5 ->  99.998  and loss:  1.4841908374801278
test acc: top1 ->  90.59 ; top5 ->  98.9  and loss:  56.21543211489916
forward train acc: top1 ->  99.65 ; top5 ->  99.996  and loss:  1.3208687277510762
test acc: top1 ->  90.66 ; top5 ->  98.89  and loss:  56.5211613625288
forward train acc: top1 ->  99.6680000024414 ; top5 ->  100.0  and loss:  1.2305912096053362
test acc: top1 ->  90.84 ; top5 ->  98.93  and loss:  56.85148334503174
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  257 / 512 , inc:  64
---------------- start layer  9  ---------------
adv train loss:  -4.280550926923752 , diff:  4.280550926923752
adv train loss:  -4.186480980366468 , diff:  0.0940699465572834
adv train loss:  -4.592049022205174 , diff:  0.40556804183870554
adv train loss:  -4.330716299824417 , diff:  0.26133272238075733
adv train loss:  -4.3428249433636665 , diff:  0.012108643539249897
adv train loss:  -4.284287221729755 , diff:  0.05853772163391113
adv train loss:  -4.385528594255447 , diff:  0.10124137252569199
adv train loss:  -4.569033485837281 , diff:  0.18350489158183336
adv train loss:  -4.544113412965089 , diff:  0.024920072872191668
adv train loss:  -4.443028490059078 , diff:  0.10108492290601134
layer  9  adv train finish, try to retain  457
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -14.247030779719353 , diff:  14.247030779719353
adv train loss:  -14.266966417431831 , diff:  0.019935637712478638
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  18
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  1443.7149505615234
forward train acc: top1 ->  9.87800000076294 ; top5 ->  49.95800000244141  and loss:  247.6802957057953
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  519.2989373207092
forward train acc: top1 ->  10.219999996643066 ; top5 ->  50.16999999633789  and loss:  226.07454466819763
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  231.73753595352173
forward train acc: top1 ->  9.920000001525878 ; top5 ->  50.0000000024414  and loss:  226.25710940361023
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  231.29143142700195
forward train acc: top1 ->  10.002000001831055 ; top5 ->  50.246000008544925  and loss:  226.15138268470764
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.4231448173523
forward train acc: top1 ->  10.239999997253419 ; top5 ->  50.149999993896486  and loss:  226.35658526420593
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.4936957359314
forward train acc: top1 ->  9.887999998168945 ; top5 ->  50.31199999267578  and loss:  225.9525625705719
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.57865452766418
forward train acc: top1 ->  10.11600000213623 ; top5 ->  49.80199999633789  and loss:  225.92429947853088
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  231.27954530715942
forward train acc: top1 ->  10.291999996948242 ; top5 ->  50.105999992675784  and loss:  226.0344479084015
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.70776081085205
forward train acc: top1 ->  9.884000002441406 ; top5 ->  49.703999998779295  and loss:  226.08672761917114
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.5989990234375
forward train acc: top1 ->  10.05599999786377 ; top5 ->  49.61799999511719  and loss:  225.96852111816406
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  231.35092163085938
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  19 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -15.92176966369152 , diff:  15.92176966369152
adv train loss:  -16.07991037517786 , diff:  0.15814071148633957
adv train loss:  -16.37212874740362 , diff:  0.2922183722257614
adv train loss:  -16.952083066105843 , diff:  0.5799543187022209
adv train loss:  -16.303546339273453 , diff:  0.6485367268323898
adv train loss:  -16.20880414545536 , diff:  0.09474219381809235
adv train loss:  -16.21435835957527 , diff:  0.005554214119911194
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  193
test acc: top1 ->  10.42 ; top5 ->  77.16  and loss:  828.1120533943176
forward train acc: top1 ->  96.038 ; top5 ->  100.0  and loss:  17.682305544614792
test acc: top1 ->  91.73 ; top5 ->  99.08  and loss:  38.39281019568443
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.9347412073984742
test acc: top1 ->  92.03 ; top5 ->  99.11  and loss:  38.076650738716125
forward train acc: top1 ->  99.96399997558593 ; top5 ->  100.0  and loss:  0.5648037348873913
test acc: top1 ->  92.0 ; top5 ->  99.13  and loss:  39.08062466979027
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.4302483971696347
test acc: top1 ->  92.03 ; top5 ->  99.12  and loss:  39.98631364107132
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.3320782012306154
test acc: top1 ->  92.18 ; top5 ->  99.13  and loss:  41.00877449661493
==> this epoch:  193 / 512
---------------- start layer  12  ---------------
adv train loss:  -37.915224224328995 , diff:  37.915224224328995
adv train loss:  -37.42823848128319 , diff:  0.4869857430458069
adv train loss:  -36.927205100655556 , diff:  0.5010333806276321
adv train loss:  -37.385278061032295 , diff:  0.4580729603767395
adv train loss:  -38.00025959312916 , diff:  0.6149815320968628
adv train loss:  -37.91721652448177 , diff:  0.08304306864738464
adv train loss:  -37.690248154103756 , diff:  0.22696837037801743
adv train loss:  -37.584212347865105 , diff:  0.10603580623865128
adv train loss:  -37.2107762247324 , diff:  0.3734361231327057
adv train loss:  -37.61659958958626 , diff:  0.40582336485385895
layer  12  adv train finish, try to retain  459
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -116.78717887401581 , diff:  116.78717887401581
adv train loss:  -116.92692393064499 , diff:  0.1397450566291809
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  63
test acc: top1 ->  91.18 ; top5 ->  99.0  and loss:  53.09439843893051
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.15883059236512054
test acc: top1 ->  92.37 ; top5 ->  99.21  and loss:  51.68711157888174
==> this epoch:  63 / 512
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  31
layer  8  :  0.501953125  ==>  257 / 512 , inc:  32
layer  9  :  0.126953125  ==>  65 / 512 , inc:  16
layer  10  :  0.037109375  ==>  19 / 512 , inc:  1
layer  11  :  0.376953125  ==>  193 / 512 , inc:  96
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.123046875  ==>  63 / 512 , inc:  16
eps [0.0042714843750000005, 0.12150000000000002, 0.12150000000000002, 0.12150000000000002, 0.12150000000000002, 0.12150000000000002, 0.0042714843750000005, 0.0005339355468750001, 0.0014238281250000002, 0.0009492187500000002, 0.18225000000000002, 0.0018984375000000004, 0.4860000000000001, 0.08100000000000002]  wait [4, 2, 2, 2, 2, 2, 4, 4, 4, 2, 4, 0, 2, 0]  inc [1, 1, 1, 1, 1, 1, 1, 31, 32, 16, 1, 96, 1, 16]  tol: 2
$$$$$$$$$$$$$ epoch  19  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
adv train loss:  -0.10096147989679594 , diff:  0.10096147989679594
adv train loss:  -0.10570096406445373 , diff:  0.004739484167657793
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -0.1271336224017432 , diff:  0.1271336224017432
adv train loss:  -0.10382563080929685 , diff:  0.023307991592446342
adv train loss:  -0.09962240526510868 , diff:  0.004203225544188172
layer  2  adv train finish, try to retain  128
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -0.1016182219464099 , diff:  0.1016182219464099
adv train loss:  -0.13657946628518403 , diff:  0.03496124433877412
adv train loss:  -0.08684363729844335 , diff:  0.04973582898674067
adv train loss:  -0.0970379594800761 , diff:  0.010194322181632742
adv train loss:  -0.1196640269481577 , diff:  0.0226260674680816
adv train loss:  -0.10506678149977233 , diff:  0.014597245448385365
adv train loss:  -0.11887898277200293 , diff:  0.013812201272230595
adv train loss:  -0.09969528297369834 , diff:  0.019183699798304588
adv train loss:  -0.11479343609244097 , diff:  0.01509815311874263
adv train loss:  -0.09890952627756633 , diff:  0.01588390981487464
layer  3  adv train finish, try to retain  128
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -0.10310240913531743 , diff:  0.10310240913531743
adv train loss:  -0.11630170613352675 , diff:  0.01319929699820932
adv train loss:  -0.10406073913327418 , diff:  0.012240967000252567
adv train loss:  -0.09521840221714228 , diff:  0.008842336916131899
layer  4  adv train finish, try to retain  254
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -0.09159469457517844 , diff:  0.09159469457517844
adv train loss:  -0.1266562537784921 , diff:  0.035061559203313664
adv train loss:  -0.12568265627487563 , diff:  0.0009735975036164746
layer  5  adv train finish, try to retain  256
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
### skip layer  8 wait:  4  ###
---------------- start layer  9  ---------------
adv train loss:  -0.7157065354986116 , diff:  0.7157065354986116
adv train loss:  -0.696752358257072 , diff:  0.018954177241539583
adv train loss:  -0.6893866795580834 , diff:  0.007365678698988631
layer  9  adv train finish, try to retain  456
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
### skip layer  10 wait:  4  ###
---------------- start layer  11  ---------------
adv train loss:  -1.3301968307932839 , diff:  1.3301968307932839
adv train loss:  -1.258253341075033 , diff:  0.07194348971825093
adv train loss:  -1.248985551763326 , diff:  0.00926778931170702
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  97
test acc: top1 ->  31.26 ; top5 ->  86.12  and loss:  596.8027291297913
forward train acc: top1 ->  99.89 ; top5 ->  100.0  and loss:  0.40323179859115044
test acc: top1 ->  92.02 ; top5 ->  99.23  and loss:  60.01215371489525
forward train acc: top1 ->  99.97399997558594 ; top5 ->  100.0  and loss:  0.08217271265311865
test acc: top1 ->  91.93 ; top5 ->  99.22  and loss:  60.2813435792923
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.07269391492081922
test acc: top1 ->  92.32 ; top5 ->  99.26  and loss:  61.05045507848263
==> this epoch:  97 / 512
---------------- start layer  12  ---------------
adv train loss:  -0.3793073362467112 , diff:  0.3793073362467112
adv train loss:  -0.49270243478531484 , diff:  0.11339509853860363
adv train loss:  -0.3813481946635875 , diff:  0.11135424012172734
adv train loss:  -0.4741017369669862 , diff:  0.09275354230339872
adv train loss:  -0.4345632529584691 , diff:  0.039538484008517116
adv train loss:  -0.3231635101546999 , diff:  0.11139974280376919
adv train loss:  -0.4343026620626915 , diff:  0.11113915190799162
adv train loss:  -0.42021268617827445 , diff:  0.014089975884417072
adv train loss:  -0.48664736869977787 , diff:  0.06643468252150342
adv train loss:  -0.4799469968274934 , diff:  0.00670037187228445
layer  12  adv train finish, try to retain  460
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -1.4344928342616186 , diff:  1.4344928342616186
adv train loss:  -1.6312167585128918 , diff:  0.19672392425127327
adv train loss:  -1.5981086068204604 , diff:  0.03310815169243142
adv train loss:  -1.6165847268421203 , diff:  0.01847612002165988
adv train loss:  -1.7214309950941242 , diff:  0.10484626825200394
adv train loss:  -1.4420419941307046 , diff:  0.2793890009634197
adv train loss:  -1.5425563618191518 , diff:  0.10051436768844724
adv train loss:  -1.5669155358045828 , diff:  0.02435917398543097
adv train loss:  -1.6439499635016546 , diff:  0.0770344276970718
adv train loss:  -1.7511941715492867 , diff:  0.10724420804763213
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  47
test acc: top1 ->  31.34 ; top5 ->  81.92  and loss:  433.4122967720032
forward train acc: top1 ->  70.23800000244141 ; top5 ->  95.31  and loss:  130.75533942133188
test acc: top1 ->  89.88 ; top5 ->  98.57  and loss:  41.65970513224602
forward train acc: top1 ->  99.706 ; top5 ->  100.0  and loss:  6.645067352801561
test acc: top1 ->  91.06 ; top5 ->  98.77  and loss:  35.89133758842945
forward train acc: top1 ->  99.87 ; top5 ->  100.0  and loss:  3.1191317811608315
test acc: top1 ->  91.32 ; top5 ->  98.82  and loss:  36.22846561670303
forward train acc: top1 ->  99.91799997558594 ; top5 ->  100.0  and loss:  1.9422356840223074
test acc: top1 ->  91.64 ; top5 ->  98.82  and loss:  36.465775310993195
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  1.3021479090675712
test acc: top1 ->  91.76 ; top5 ->  98.8  and loss:  37.31847155094147
forward train acc: top1 ->  99.94999997558594 ; top5 ->  100.0  and loss:  1.0119050606153905
test acc: top1 ->  91.77 ; top5 ->  98.8  and loss:  37.97374667227268
forward train acc: top1 ->  99.96199997558594 ; top5 ->  100.0  and loss:  0.8791651222854853
test acc: top1 ->  91.72 ; top5 ->  98.8  and loss:  38.331636875867844
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.7451004297472537
test acc: top1 ->  91.79 ; top5 ->  98.78  and loss:  38.980418264865875
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.6516116242855787
test acc: top1 ->  91.89 ; top5 ->  98.84  and loss:  39.216426216065884
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.5602552788332105
test acc: top1 ->  91.87 ; top5 ->  98.85  and loss:  39.895719565451145
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  63 / 512 , inc:  16
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  31
layer  8  :  0.501953125  ==>  257 / 512 , inc:  32
layer  9  :  0.126953125  ==>  65 / 512 , inc:  16
layer  10  :  0.037109375  ==>  19 / 512 , inc:  1
layer  11  :  0.189453125  ==>  97 / 512 , inc:  48
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.123046875  ==>  63 / 512 , inc:  8
eps [0.0042714843750000005, 0.24300000000000005, 0.24300000000000005, 0.24300000000000005, 0.24300000000000005, 0.24300000000000005, 0.0042714843750000005, 0.0005339355468750001, 0.0014238281250000002, 0.0018984375000000004, 0.18225000000000002, 0.0018984375000000004, 0.9720000000000002, 0.06075000000000001]  wait [3, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 0, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 31, 32, 16, 1, 48, 1, 8]  tol: 2
$$$$$$$$$$$$$ epoch  20  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
adv train loss:  -3.072151107247919 , diff:  3.072151107247919
adv train loss:  -2.88601005775854 , diff:  0.18614104948937893
adv train loss:  -2.6914776489138603 , diff:  0.1945324088446796
adv train loss:  -2.909804450813681 , diff:  0.21832680189982057
adv train loss:  -2.8848623684607446 , diff:  0.024942082352936268
adv train loss:  -2.8223789487965405 , diff:  0.06248341966420412
adv train loss:  -3.0813202280551195 , diff:  0.258941279258579
adv train loss:  -3.180156046990305 , diff:  0.09883581893518567
adv train loss:  -2.891677220351994 , diff:  0.28847882663831115
adv train loss:  -2.724869984900579 , diff:  0.16680723545141518
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -2.9868184933438897 , diff:  2.9868184933438897
adv train loss:  -3.0786451746243984 , diff:  0.09182668128050864
adv train loss:  -2.7591284406371415 , diff:  0.3195167339872569
adv train loss:  -2.6613357856404036 , diff:  0.09779265499673784
adv train loss:  -3.065944458823651 , diff:  0.40460867318324745
adv train loss:  -2.9677128135226667 , diff:  0.09823164530098438
adv train loss:  -2.865606221370399 , diff:  0.1021065921522677
adv train loss:  -2.929736672900617 , diff:  0.06413045153021812
adv train loss:  -2.960655625909567 , diff:  0.030918953008949757
adv train loss:  -3.1193834207952023 , diff:  0.15872779488563538
layer  2  adv train finish, try to retain  127
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -3.0775833753868937 , diff:  3.0775833753868937
adv train loss:  -3.1496897938195616 , diff:  0.07210641843266785
adv train loss:  -3.038160091266036 , diff:  0.11152970255352557
adv train loss:  -3.0220882641151547 , diff:  0.01607182715088129
adv train loss:  -2.882090430241078 , diff:  0.1399978338740766
adv train loss:  -2.979506860021502 , diff:  0.09741642978042364
adv train loss:  -3.049511311110109 , diff:  0.07000445108860731
adv train loss:  -3.0681515806354582 , diff:  0.01864026952534914
adv train loss:  -3.1817347020842135 , diff:  0.11358312144875526
adv train loss:  -2.904516854789108 , diff:  0.27721784729510546
layer  3  adv train finish, try to retain  128
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -2.869952745269984 , diff:  2.869952745269984
adv train loss:  -2.8265875892248005 , diff:  0.04336515604518354
adv train loss:  -2.9787368588149548 , diff:  0.1521492695901543
adv train loss:  -3.078890576492995 , diff:  0.10015371767804027
adv train loss:  -2.8455162204336375 , diff:  0.23337435605935752
adv train loss:  -2.9885128440801054 , diff:  0.14299662364646792
adv train loss:  -2.7734906170517206 , diff:  0.2150222270283848
adv train loss:  -2.889396687038243 , diff:  0.1159060699865222
adv train loss:  -2.9997785661835223 , diff:  0.11038187914527953
adv train loss:  -2.7289902437478304 , diff:  0.27078832243569195
layer  4  adv train finish, try to retain  256
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -2.891197493299842 , diff:  2.891197493299842
adv train loss:  -2.7827662364579737 , diff:  0.10843125684186816
adv train loss:  -3.0008146753534675 , diff:  0.21804843889549375
adv train loss:  -3.133019806118682 , diff:  0.13220513076521456
adv train loss:  -2.8008092117961496 , diff:  0.3322105943225324
adv train loss:  -2.9430202576331794 , diff:  0.14221104583702981
adv train loss:  -3.122258147224784 , diff:  0.17923788959160447
adv train loss:  -2.9562738267704844 , diff:  0.16598432045429945
adv train loss:  -2.958274256438017 , diff:  0.002000429667532444
layer  5  adv train finish, try to retain  256
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
### skip layer  6 wait:  3  ###
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
### skip layer  8 wait:  3  ###
---------------- start layer  9  ---------------
adv train loss:  -2.3301831879653037 , diff:  2.3301831879653037
adv train loss:  -2.158786092652008 , diff:  0.17139709531329572
adv train loss:  -2.058548688190058 , diff:  0.10023740446195006
adv train loss:  -2.1661181109957397 , diff:  0.10756942280568182
adv train loss:  -2.0611791512928903 , diff:  0.10493895970284939
adv train loss:  -2.101949132978916 , diff:  0.04076998168602586
adv train loss:  -1.9537311405874789 , diff:  0.1482179923914373
adv train loss:  -2.101947180228308 , diff:  0.14821603964082897
adv train loss:  -2.1931515785399824 , diff:  0.0912043983116746
adv train loss:  -2.192862174473703 , diff:  0.0002894040662795305
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  49
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  2081.8866634368896
forward train acc: top1 ->  94.30199997558594 ; top5 ->  99.902  and loss:  29.585621538572013
test acc: top1 ->  90.23 ; top5 ->  98.1  and loss:  80.11715227365494
forward train acc: top1 ->  99.832 ; top5 ->  100.0  and loss:  0.7483273819088936
test acc: top1 ->  91.31 ; top5 ->  98.43  and loss:  69.6792186498642
forward train acc: top1 ->  99.9 ; top5 ->  100.0  and loss:  0.408767192508094
test acc: top1 ->  91.34 ; top5 ->  98.43  and loss:  69.35336789488792
forward train acc: top1 ->  99.936 ; top5 ->  100.0  and loss:  0.2868064042413607
test acc: top1 ->  91.45 ; top5 ->  98.53  and loss:  68.96049559116364
forward train acc: top1 ->  99.936 ; top5 ->  100.0  and loss:  0.26296689943410456
test acc: top1 ->  91.71 ; top5 ->  98.56  and loss:  68.93215951323509
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.23096747358795255
test acc: top1 ->  91.59 ; top5 ->  98.46  and loss:  68.67615109682083
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.1984770028211642
test acc: top1 ->  91.71 ; top5 ->  98.58  and loss:  67.87915650010109
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.16441862724605016
test acc: top1 ->  91.67 ; top5 ->  98.54  and loss:  67.71139147877693
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.1318653351336252
test acc: top1 ->  91.78 ; top5 ->  98.6  and loss:  68.05418242514133
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.12165016739163548
test acc: top1 ->  91.67 ; top5 ->  98.57  and loss:  68.48067563772202
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  65 / 512 , inc:  16
---------------- start layer  10  ---------------
### skip layer  10 wait:  3  ###
---------------- start layer  11  ---------------
adv train loss:  -5.295024290680885 , diff:  5.295024290680885
adv train loss:  -5.344344667159021 , diff:  0.049320376478135586
adv train loss:  -5.248519952408969 , diff:  0.0958247147500515
adv train loss:  -5.072370415553451 , diff:  0.17614953685551882
adv train loss:  -5.065249754115939 , diff:  0.007120661437511444
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  49
test acc: top1 ->  10.0 ; top5 ->  53.22  and loss:  1318.1674671173096
forward train acc: top1 ->  96.62599997558594 ; top5 ->  99.882  and loss:  12.529860165901482
test acc: top1 ->  91.41 ; top5 ->  98.54  and loss:  43.73928542435169
forward train acc: top1 ->  99.924 ; top5 ->  99.998  and loss:  1.0722049702890217
test acc: top1 ->  91.82 ; top5 ->  98.62  and loss:  44.10257840901613
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.5852428546641022
test acc: top1 ->  91.85 ; top5 ->  98.69  and loss:  44.89747440069914
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.4164892337284982
test acc: top1 ->  91.79 ; top5 ->  98.71  and loss:  46.19465694576502
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.30062758119311184
test acc: top1 ->  91.87 ; top5 ->  98.66  and loss:  47.062317468225956
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.2705309592420235
test acc: top1 ->  91.97 ; top5 ->  98.7  and loss:  47.622710309922695
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.2199635518481955
test acc: top1 ->  91.93 ; top5 ->  98.76  and loss:  48.328032694756985
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.20351369737181813
test acc: top1 ->  91.94 ; top5 ->  98.71  and loss:  48.80123858898878
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.17907133046537638
test acc: top1 ->  91.96 ; top5 ->  98.76  and loss:  49.231219835579395
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.1597552050370723
test acc: top1 ->  91.96 ; top5 ->  98.79  and loss:  49.56394183635712
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  97 / 512 , inc:  48
---------------- start layer  12  ---------------
adv train loss:  -3.4996626707725227 , diff:  3.4996626707725227
adv train loss:  -3.947407162399031 , diff:  0.44774449162650853
adv train loss:  -3.4865855594398454 , diff:  0.46082160295918584
adv train loss:  -3.4177046295953915 , diff:  0.06888092984445393
adv train loss:  -4.049836581922136 , diff:  0.6321319523267448
adv train loss:  -3.2809474433306605 , diff:  0.7688891385914758
adv train loss:  -3.6244930336251855 , diff:  0.343545590294525
adv train loss:  -3.702890913351439 , diff:  0.07839787972625345
adv train loss:  -3.5838459874503314 , diff:  0.11904492590110749
adv train loss:  -3.9135909198084846 , diff:  0.32974493235815316
layer  12  adv train finish, try to retain  460
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -9.159832999110222 , diff:  9.159832999110222
adv train loss:  -9.882047805935144 , diff:  0.7222148068249226
adv train loss:  -9.944853923283517 , diff:  0.06280611734837294
adv train loss:  -9.603163737803698 , diff:  0.3416901854798198
adv train loss:  -9.408712392672896 , diff:  0.1944513451308012
adv train loss:  -9.481388637796044 , diff:  0.07267624512314796
adv train loss:  -9.2112500295043 , diff:  0.2701386082917452
adv train loss:  -9.745644811540842 , diff:  0.5343947820365429
adv train loss:  -9.015670197084546 , diff:  0.729974614456296
adv train loss:  -9.362308613024652 , diff:  0.3466384159401059
layer  13  adv train finish, try to retain  466
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  31
layer  8  :  0.501953125  ==>  257 / 512 , inc:  32
layer  9  :  0.126953125  ==>  65 / 512 , inc:  8
layer  10  :  0.037109375  ==>  19 / 512 , inc:  1
layer  11  :  0.189453125  ==>  97 / 512 , inc:  24
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.123046875  ==>  63 / 512 , inc:  8
eps [0.0042714843750000005, 0.4860000000000001, 0.4860000000000001, 0.4860000000000001, 0.4860000000000001, 0.4860000000000001, 0.0042714843750000005, 0.0005339355468750001, 0.0014238281250000002, 0.0014238281250000002, 0.18225000000000002, 0.0014238281250000002, 1.9440000000000004, 0.12150000000000002]  wait [2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 31, 32, 8, 1, 24, 1, 8]  tol: 2
$$$$$$$$$$$$$ epoch  21  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -4.877664160216227 , diff:  4.877664160216227
adv train loss:  -4.633212683722377 , diff:  0.24445147649385035
adv train loss:  -5.00885910843499 , diff:  0.3756464247126132
adv train loss:  -4.758341162407305 , diff:  0.25051794602768496
adv train loss:  -4.923359283944592 , diff:  0.1650181215372868
adv train loss:  -5.134316663723439 , diff:  0.2109573797788471
adv train loss:  -4.834493845701218 , diff:  0.2998228180222213
adv train loss:  -5.004104000516236 , diff:  0.16961015481501818
adv train loss:  -4.601413385942578 , diff:  0.4026906145736575
adv train loss:  -4.572226535528898 , diff:  0.029186850413680077
layer  0  adv train finish, try to retain  64
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -5.029454136732966 , diff:  5.029454136732966
adv train loss:  -4.88443590211682 , diff:  0.1450182346161455
adv train loss:  -4.572553162463009 , diff:  0.31188273965381086
adv train loss:  -4.760108340065926 , diff:  0.18755517760291696
adv train loss:  -4.610527312615886 , diff:  0.14958102745003998
adv train loss:  -4.609213147778064 , diff:  0.001314164837822318
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -5.085986696300097 , diff:  5.085986696300097
adv train loss:  -4.9017516118474305 , diff:  0.18423508445266634
adv train loss:  -4.691287976223975 , diff:  0.21046363562345505
adv train loss:  -5.445376384072006 , diff:  0.7540884078480303
adv train loss:  -4.696789538022131 , diff:  0.748586846049875
adv train loss:  -5.439314297633246 , diff:  0.7425247596111149
adv train loss:  -4.8952393042854965 , diff:  0.5440749933477491
adv train loss:  -4.804695745464414 , diff:  0.09054355882108212
adv train loss:  -4.69132462516427 , diff:  0.11337112030014396
adv train loss:  -4.872885822900571 , diff:  0.18156119773630053
layer  2  adv train finish, try to retain  126
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -5.0570960533223115 , diff:  5.0570960533223115
adv train loss:  -4.903971673455089 , diff:  0.15312437986722216
adv train loss:  -4.9616351393051445 , diff:  0.05766346585005522
adv train loss:  -5.089264744892716 , diff:  0.12762960558757186
adv train loss:  -4.921223889803514 , diff:  0.16804085508920252
adv train loss:  -4.979396538808942 , diff:  0.05817264900542796
adv train loss:  -4.972756145754829 , diff:  0.006640393054112792
layer  3  adv train finish, try to retain  128
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -4.9506135827396065 , diff:  4.9506135827396065
adv train loss:  -4.938405073015019 , diff:  0.012208509724587202
adv train loss:  -5.572917116805911 , diff:  0.6345120437908918
adv train loss:  -4.5064508453942835 , diff:  1.0664662714116275
adv train loss:  -4.908527108142152 , diff:  0.4020762627478689
adv train loss:  -5.081473482307047 , diff:  0.17294637416489422
adv train loss:  -4.743866251781583 , diff:  0.3376072305254638
adv train loss:  -4.945813089725561 , diff:  0.2019468379439786
adv train loss:  -5.131133539834991 , diff:  0.18532045010942966
adv train loss:  -4.759756988380104 , diff:  0.3713765514548868
layer  4  adv train finish, try to retain  254
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -4.672579799313098 , diff:  4.672579799313098
adv train loss:  -4.532568566151895 , diff:  0.14001123316120356
adv train loss:  -5.017128377221525 , diff:  0.4845598110696301
adv train loss:  -5.02898055780679 , diff:  0.01185218058526516
adv train loss:  -4.858817586558871 , diff:  0.1701629712479189
adv train loss:  -4.844421948539093 , diff:  0.014395638019777834
adv train loss:  -5.1323411809280515 , diff:  0.28791923238895833
adv train loss:  -5.039554531686008 , diff:  0.0927866492420435
adv train loss:  -4.851302963681519 , diff:  0.18825156800448895
adv train loss:  -5.2302167697343975 , diff:  0.3789138060528785
layer  5  adv train finish, try to retain  256
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -4.820681098848581 , diff:  4.820681098848581
adv train loss:  -4.806902709882706 , diff:  0.01377838896587491
adv train loss:  -5.1088147684931755 , diff:  0.3019120586104691
adv train loss:  -4.942008672252996 , diff:  0.1668060962401796
adv train loss:  -4.688348355237395 , diff:  0.2536603170156013
adv train loss:  -4.405721223913133 , diff:  0.2826271313242614
adv train loss:  -4.628045022953302 , diff:  0.22232379904016852
adv train loss:  -4.851584580726922 , diff:  0.2235395577736199
adv train loss:  -4.990148726385087 , diff:  0.13856414565816522
adv train loss:  -4.781601753085852 , diff:  0.2085469732992351
layer  6  adv train finish, try to retain  256
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -6.593941617757082 , diff:  6.593941617757082
adv train loss:  -6.923781990073621 , diff:  0.3298403723165393
adv train loss:  -6.279425277840346 , diff:  0.6443567122332752
adv train loss:  -6.539688415825367 , diff:  0.2602631379850209
adv train loss:  -6.365585607010871 , diff:  0.1741028088144958
adv train loss:  -6.333111890591681 , diff:  0.03247371641919017
adv train loss:  -6.457011619582772 , diff:  0.12389972899109125
adv train loss:  -6.224896348081529 , diff:  0.23211527150124311
adv train loss:  -6.773266914533451 , diff:  0.5483705664519221
adv train loss:  -6.0746745159849524 , diff:  0.6985923985484987
layer  7  adv train finish, try to retain  307
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -4.892210842110217 , diff:  4.892210842110217
adv train loss:  -4.786131414817646 , diff:  0.10607942729257047
adv train loss:  -4.765655475202948 , diff:  0.02047593961469829
adv train loss:  -4.494391872081906 , diff:  0.27126360312104225
adv train loss:  -4.5031517937313765 , diff:  0.008759921649470925
layer  8  adv train finish, try to retain  303
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -3.141479746787809 , diff:  3.141479746787809
adv train loss:  -3.2873221100308 , diff:  0.14584236324299127
adv train loss:  -2.9301132718101144 , diff:  0.3572088382206857
adv train loss:  -3.0173793262802064 , diff:  0.08726605447009206
adv train loss:  -3.174668263993226 , diff:  0.15728893771301955
adv train loss:  -3.1192220937809907 , diff:  0.0554461702122353
adv train loss:  -2.9514834993169643 , diff:  0.1677385944640264
adv train loss:  -2.9837934795068577 , diff:  0.032309980189893395
adv train loss:  -3.064778853324242 , diff:  0.08098537381738424
adv train loss:  -3.142142664291896 , diff:  0.07736381096765399
layer  9  adv train finish, try to retain  444
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -5.0739362658932805 , diff:  5.0739362658932805
adv train loss:  -5.071332089602947 , diff:  0.002604176290333271
layer  10  adv train finish, try to retain  484
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -12.804945953190327 , diff:  12.804945953190327
adv train loss:  -12.876083955168724 , diff:  0.07113800197839737
adv train loss:  -12.784595888108015 , diff:  0.091488067060709
adv train loss:  -12.782485213130713 , diff:  0.0021106749773025513
layer  11  adv train finish, try to retain  420
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -403.6931223869324 , diff:  403.6931223869324
adv train loss:  -418.5287940502167 , diff:  14.835671663284302
adv train loss:  -393.1358985900879 , diff:  25.392895460128784
adv train loss:  -417.13456320762634 , diff:  23.998664617538452
adv train loss:  -417.25660490989685 , diff:  0.12204170227050781
layer  12  adv train finish, try to retain  3
test acc: top1 ->  15.66 ; top5 ->  59.17  and loss:  673.6945848464966
forward train acc: top1 ->  44.727999992675784 ; top5 ->  71.66000001464843  and loss:  560.4465868473053
test acc: top1 ->  53.04 ; top5 ->  78.32  and loss:  349.94139218330383
forward train acc: top1 ->  67.32399997070313 ; top5 ->  90.19999997558594  and loss:  162.51828426122665
test acc: top1 ->  70.18 ; top5 ->  95.94  and loss:  108.39150393009186
forward train acc: top1 ->  94.77400000244141 ; top5 ->  99.994  and loss:  28.267015427350998
test acc: top1 ->  88.8 ; top5 ->  96.52  and loss:  66.23341050744057
forward train acc: top1 ->  99.59799997558594 ; top5 ->  99.998  and loss:  12.788918390870094
test acc: top1 ->  89.41 ; top5 ->  96.54  and loss:  62.63129696249962
forward train acc: top1 ->  99.718 ; top5 ->  99.994  and loss:  8.36916197091341
test acc: top1 ->  89.84 ; top5 ->  96.59  and loss:  61.34582135081291
forward train acc: top1 ->  99.74199997558594 ; top5 ->  100.0  and loss:  6.338126465678215
test acc: top1 ->  89.63 ; top5 ->  96.57  and loss:  61.354421496391296
forward train acc: top1 ->  99.78 ; top5 ->  99.994  and loss:  5.298989247530699
test acc: top1 ->  89.81 ; top5 ->  96.57  and loss:  61.2640078663826
forward train acc: top1 ->  99.79799997558594 ; top5 ->  100.0  and loss:  4.514695361256599
test acc: top1 ->  89.82 ; top5 ->  96.55  and loss:  61.59973952174187
forward train acc: top1 ->  99.848 ; top5 ->  99.998  and loss:  3.852458044886589
test acc: top1 ->  89.94 ; top5 ->  96.54  and loss:  62.021111607551575
forward train acc: top1 ->  99.816 ; top5 ->  99.994  and loss:  3.4263991955667734
test acc: top1 ->  90.15 ; top5 ->  96.64  and loss:  61.47903123497963
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  62 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -481.8088240623474 , diff:  481.8088240623474
adv train loss:  -481.7225570678711 , diff:  0.08626699447631836
layer  13  adv train finish, try to retain  467
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  31
layer  8  :  0.501953125  ==>  257 / 512 , inc:  32
layer  9  :  0.126953125  ==>  65 / 512 , inc:  8
layer  10  :  0.037109375  ==>  19 / 512 , inc:  1
layer  11  :  0.189453125  ==>  97 / 512 , inc:  24
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.123046875  ==>  63 / 512 , inc:  8
eps [0.008542968750000001, 0.9720000000000002, 0.9720000000000002, 0.9720000000000002, 0.9720000000000002, 0.9720000000000002, 0.008542968750000001, 0.0010678710937500001, 0.0028476562500000004, 0.0028476562500000004, 0.36450000000000005, 0.0028476562500000004, 1.4580000000000002, 0.24300000000000005]  wait [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0]  inc [1, 1, 1, 1, 1, 1, 1, 31, 32, 8, 1, 24, 1, 8]  tol: 3
$$$$$$$$$$$$$ epoch  22  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -366.96115350723267 , diff:  366.96115350723267
adv train loss:  -366.78366684913635 , diff:  0.17748665809631348
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  31
test acc: top1 ->  29.1 ; top5 ->  77.68  and loss:  1014.1179094314575
forward train acc: top1 ->  93.30800001220703 ; top5 ->  99.5840000024414  and loss:  48.68306661397219
test acc: top1 ->  81.75 ; top5 ->  94.74  and loss:  222.68518376350403
forward train acc: top1 ->  98.12399998291015 ; top5 ->  99.912  and loss:  9.380148097872734
test acc: top1 ->  89.52 ; top5 ->  98.14  and loss:  99.23544958233833
forward train acc: top1 ->  98.43400000488282 ; top5 ->  99.952  and loss:  6.855051325634122
test acc: top1 ->  89.75 ; top5 ->  98.36  and loss:  89.01491388678551
forward train acc: top1 ->  98.61399997802734 ; top5 ->  99.968  and loss:  5.496590057387948
test acc: top1 ->  89.88 ; top5 ->  98.51  and loss:  80.20208132266998
forward train acc: top1 ->  98.8080000024414 ; top5 ->  99.964  and loss:  4.622632192913443
test acc: top1 ->  89.82 ; top5 ->  98.6  and loss:  74.76805186271667
forward train acc: top1 ->  98.83800000488282 ; top5 ->  99.982  and loss:  3.9874641383066773
test acc: top1 ->  89.81 ; top5 ->  98.58  and loss:  71.7225307226181
forward train acc: top1 ->  98.87800000732422 ; top5 ->  99.98  and loss:  4.105474077165127
test acc: top1 ->  89.93 ; top5 ->  98.71  and loss:  69.43597057461739
forward train acc: top1 ->  98.92200000488282 ; top5 ->  99.982  and loss:  3.525308160111308
test acc: top1 ->  89.91 ; top5 ->  98.76  and loss:  67.98685565590858
forward train acc: top1 ->  99.00799997802734 ; top5 ->  99.97399997558594  and loss:  3.3407236519269645
test acc: top1 ->  89.93 ; top5 ->  98.76  and loss:  67.03201711177826
forward train acc: top1 ->  99.03600000732422 ; top5 ->  99.98399997558593  and loss:  3.254445970058441
test acc: top1 ->  89.84 ; top5 ->  98.76  and loss:  66.37148916721344
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -0.12182903915527277 , diff:  0.12182903915527277
adv train loss:  -0.14140114569454454 , diff:  0.019572106539271772
adv train loss:  -0.1556256284529809 , diff:  0.014224482758436352
adv train loss:  -0.12256333077675663 , diff:  0.03306229767622426
adv train loss:  -0.12773488846141845 , diff:  0.0051715576846618205
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -0.14287782076280564 , diff:  0.14287782076280564
adv train loss:  -0.11919280383153819 , diff:  0.023685016931267455
adv train loss:  -0.13211828016210347 , diff:  0.012925476330565289
adv train loss:  -0.1540370699513005 , diff:  0.021918789789197035
adv train loss:  -0.13508034593542106 , diff:  0.018956724015879445
adv train loss:  -0.13770547430613078 , diff:  0.0026251283707097173
layer  2  adv train finish, try to retain  123
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -0.116325854731258 , diff:  0.116325854731258
adv train loss:  -0.12380118481814861 , diff:  0.007475330086890608
layer  3  adv train finish, try to retain  122
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -0.11464075968251564 , diff:  0.11464075968251564
adv train loss:  -0.11880978201224934 , diff:  0.004169022329733707
layer  4  adv train finish, try to retain  229
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -0.11463016300695017 , diff:  0.11463016300695017
adv train loss:  -0.12904624422662891 , diff:  0.014416081219678745
adv train loss:  -0.12853746002656408 , diff:  0.0005087842000648379
layer  5  adv train finish, try to retain  249
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -0.15984429550007917 , diff:  0.15984429550007917
adv train loss:  -0.13148805452510715 , diff:  0.028356240974972025
adv train loss:  -0.14838035593857057 , diff:  0.01689230141346343
adv train loss:  -0.1412228687258903 , diff:  0.00715748721268028
layer  6  adv train finish, try to retain  256
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -0.32434149278560653 , diff:  0.32434149278560653
adv train loss:  -0.26731376713723876 , diff:  0.05702772564836778
adv train loss:  -0.33299696262110956 , diff:  0.0656831954838708
adv train loss:  -0.29832951875869185 , diff:  0.03466744386241771
adv train loss:  -0.3561525197874289 , diff:  0.05782300102873705
adv train loss:  -0.2877855548285879 , diff:  0.06836696495884098
adv train loss:  -0.2983136190159712 , diff:  0.01052806418738328
adv train loss:  -0.27950513214454986 , diff:  0.018808486871421337
adv train loss:  -0.32036901131505147 , diff:  0.040863879170501605
adv train loss:  -0.3174947580846492 , diff:  0.002874253230402246
layer  7  adv train finish, try to retain  270
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -0.13953949019196443 , diff:  0.13953949019196443
adv train loss:  -0.14117172244004905 , diff:  0.0016322322480846196
layer  8  adv train finish, try to retain  255
test acc: top1 ->  10.0 ; top5 ->  50.01  and loss:  610.2431802749634
forward train acc: top1 ->  87.47999997558594 ; top5 ->  98.492  and loss:  83.92033324018121
test acc: top1 ->  89.94 ; top5 ->  99.0  and loss:  44.64267772436142
forward train acc: top1 ->  99.32399997802735 ; top5 ->  99.998  and loss:  3.632283365353942
test acc: top1 ->  90.9 ; top5 ->  99.12  and loss:  43.193719655275345
forward train acc: top1 ->  99.52399997802735 ; top5 ->  99.998  and loss:  2.2151095028966665
test acc: top1 ->  91.2 ; top5 ->  99.17  and loss:  43.42210577428341
forward train acc: top1 ->  99.67399997802734 ; top5 ->  100.0  and loss:  1.5435200720094144
test acc: top1 ->  91.25 ; top5 ->  99.17  and loss:  44.06027575582266
forward train acc: top1 ->  99.76999997558593 ; top5 ->  99.998  and loss:  1.1428534374572337
test acc: top1 ->  91.44 ; top5 ->  99.15  and loss:  44.85306663066149
forward train acc: top1 ->  99.73999997558593 ; top5 ->  100.0  and loss:  1.0021484412718564
test acc: top1 ->  91.44 ; top5 ->  99.13  and loss:  46.19932729750872
forward train acc: top1 ->  99.81199997558593 ; top5 ->  99.998  and loss:  0.9106985076796263
test acc: top1 ->  91.4 ; top5 ->  99.14  and loss:  46.95104871690273
forward train acc: top1 ->  99.81799997558593 ; top5 ->  99.998  and loss:  0.8429732082877308
test acc: top1 ->  91.44 ; top5 ->  99.16  and loss:  47.42868646234274
forward train acc: top1 ->  99.814 ; top5 ->  100.0  and loss:  0.813049420947209
test acc: top1 ->  91.56 ; top5 ->  99.13  and loss:  47.454285711050034
forward train acc: top1 ->  99.794 ; top5 ->  100.0  and loss:  0.7982332021929324
test acc: top1 ->  91.58 ; top5 ->  99.12  and loss:  47.87496340274811
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  257 / 512 , inc:  32
---------------- start layer  9  ---------------
adv train loss:  -0.7136406665813411 , diff:  0.7136406665813411
adv train loss:  -0.5969435220031301 , diff:  0.11669714457821101
adv train loss:  -0.5714707079168875 , diff:  0.02547281408624258
adv train loss:  -0.4917598427273333 , diff:  0.07971086518955417
adv train loss:  -0.48190141260420205 , diff:  0.009858430123131257
layer  9  adv train finish, try to retain  448
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -15.858961507678032 , diff:  15.858961507678032
adv train loss:  -16.057080686092377 , diff:  0.1981191784143448
adv train loss:  -15.6994798630476 , diff:  0.3576008230447769
adv train loss:  -15.839948371052742 , diff:  0.1404685080051422
adv train loss:  -16.265024796128273 , diff:  0.425076425075531
adv train loss:  -15.941526740789413 , diff:  0.32349805533885956
adv train loss:  -15.96705200523138 , diff:  0.02552526444196701
layer  10  adv train finish, try to retain  472
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -3.3375347228138708 , diff:  3.3375347228138708
adv train loss:  -3.5484494669362903 , diff:  0.2109147441224195
adv train loss:  -3.37508227606304 , diff:  0.17336719087325037
adv train loss:  -3.1392738497816026 , diff:  0.23580842628143728
adv train loss:  -3.0032987688900903 , diff:  0.13597508089151233
adv train loss:  -3.1192250384483486 , diff:  0.11592626955825835
adv train loss:  -3.179703030269593 , diff:  0.06047799182124436
adv train loss:  -3.1296880214940757 , diff:  0.050015008775517344
adv train loss:  -3.0730571595486253 , diff:  0.056630861945450306
adv train loss:  -2.9257333970163018 , diff:  0.1473237625323236
layer  11  adv train finish, try to retain  410
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -509.65954263322055 , diff:  509.65954263322055
adv train loss:  -1218.290901184082 , diff:  708.6313585508615
adv train loss:  -1274.5422019958496 , diff:  56.25130081176758
adv train loss:  -1275.2101278305054 , diff:  0.6679258346557617
adv train loss:  -1275.0627889633179 , diff:  0.1473388671875
adv train loss:  -1569.300136566162 , diff:  294.23734760284424
adv train loss:  -1597.228178024292 , diff:  27.928041458129883
adv train loss:  -1596.2307262420654 , diff:  0.9974517822265625
adv train loss:  -1596.6867637634277 , diff:  0.4560375213623047
adv train loss:  -1596.4187936782837 , diff:  0.26797008514404297
layer  12  adv train finish, try to retain  1
test acc: top1 ->  16.76 ; top5 ->  50.0  and loss:  1383.9830474853516
forward train acc: top1 ->  20.649999994506835 ; top5 ->  59.55400000488281  and loss:  395.7647695541382
test acc: top1 ->  25.21 ; top5 ->  69.34  and loss:  243.67969405651093
forward train acc: top1 ->  41.69799998779297 ; top5 ->  89.464  and loss:  164.48976874351501
test acc: top1 ->  48.66 ; top5 ->  96.75  and loss:  153.51004934310913
forward train acc: top1 ->  58.877999995117186 ; top5 ->  99.92  and loss:  132.84128868579865
test acc: top1 ->  54.1 ; top5 ->  96.77  and loss:  144.1075303554535
forward train acc: top1 ->  62.01199999389648 ; top5 ->  99.94  and loss:  122.88041460514069
test acc: top1 ->  58.75 ; top5 ->  96.77  and loss:  138.74298417568207
forward train acc: top1 ->  63.94199997558594 ; top5 ->  99.932  and loss:  115.41950309276581
test acc: top1 ->  59.66 ; top5 ->  96.61  and loss:  135.0378656387329
forward train acc: top1 ->  66.10000001464844 ; top5 ->  99.956  and loss:  110.57042157649994
test acc: top1 ->  60.54 ; top5 ->  96.57  and loss:  133.42899882793427
forward train acc: top1 ->  67.5960000024414 ; top5 ->  99.964  and loss:  107.45619428157806
test acc: top1 ->  60.61 ; top5 ->  96.52  and loss:  132.50223910808563
forward train acc: top1 ->  68.56199999511719 ; top5 ->  99.966  and loss:  104.65259826183319
test acc: top1 ->  61.94 ; top5 ->  96.64  and loss:  131.2480616569519
forward train acc: top1 ->  68.96799998046875 ; top5 ->  99.962  and loss:  102.23747479915619
test acc: top1 ->  62.23 ; top5 ->  96.59  and loss:  130.36329066753387
forward train acc: top1 ->  69.32999997314454 ; top5 ->  99.95999997558594  and loss:  99.70218902826309
test acc: top1 ->  62.56 ; top5 ->  96.6  and loss:  129.61085093021393
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  62 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -144.11734133958817 , diff:  144.11734133958817
adv train loss:  -145.82466506958008 , diff:  1.7073237299919128
adv train loss:  -144.9709678888321 , diff:  0.8536971807479858
adv train loss:  -147.01654011011124 , diff:  2.0455722212791443
adv train loss:  -145.11047035455704 , diff:  1.9060697555541992
adv train loss:  -148.41861367225647 , diff:  3.3081433176994324
adv train loss:  -146.4830647110939 , diff:  1.9355489611625671
adv train loss:  -147.6140097975731 , diff:  1.130945086479187
adv train loss:  -146.69488966464996 , diff:  0.9191201329231262
adv train loss:  -146.4475160241127 , diff:  0.24737364053726196
layer  13  adv train finish, try to retain  465
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  31
layer  8  :  0.501953125  ==>  257 / 512 , inc:  16
layer  9  :  0.126953125  ==>  65 / 512 , inc:  8
layer  10  :  0.037109375  ==>  19 / 512 , inc:  1
layer  11  :  0.189453125  ==>  97 / 512 , inc:  24
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.123046875  ==>  63 / 512 , inc:  8
eps [0.006407226562500001, 1.9440000000000004, 1.9440000000000004, 1.9440000000000004, 1.9440000000000004, 1.9440000000000004, 0.017085937500000002, 0.0021357421875000003, 0.0021357421875000003, 0.005695312500000001, 0.7290000000000001, 0.005695312500000001, 1.0935000000000001, 0.4860000000000001]  wait [2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 4, 0]  inc [1, 1, 1, 1, 1, 1, 1, 31, 16, 8, 1, 24, 1, 8]  tol: 3
$$$$$$$$$$$$$ epoch  23  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -10.37436006590724 , diff:  10.37436006590724
adv train loss:  -10.545453330501914 , diff:  0.1710932645946741
adv train loss:  -10.438951794058084 , diff:  0.10650153644382954
adv train loss:  -10.023448012769222 , diff:  0.41550378128886223
adv train loss:  -10.126768826507032 , diff:  0.10332081373780966
adv train loss:  -10.134674344211817 , diff:  0.00790551770478487
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  31
test acc: top1 ->  48.97 ; top5 ->  85.4  and loss:  492.83641052246094
forward train acc: top1 ->  97.95399998046875 ; top5 ->  99.912  and loss:  9.731655990704894
test acc: top1 ->  82.03 ; top5 ->  96.34  and loss:  160.05573016405106
forward train acc: top1 ->  98.56800000488282 ; top5 ->  99.94999997558594  and loss:  5.53714613057673
test acc: top1 ->  89.82 ; top5 ->  98.62  and loss:  70.00442652404308
forward train acc: top1 ->  98.7600000024414 ; top5 ->  99.95399997558594  and loss:  4.589895281009376
test acc: top1 ->  90.14 ; top5 ->  98.77  and loss:  64.22385628521442
forward train acc: top1 ->  98.96600000488282 ; top5 ->  99.972  and loss:  3.4815060244873166
test acc: top1 ->  90.24 ; top5 ->  98.78  and loss:  62.63021567463875
forward train acc: top1 ->  98.998 ; top5 ->  99.996  and loss:  3.18126362003386
test acc: top1 ->  90.28 ; top5 ->  98.87  and loss:  60.37037645280361
forward train acc: top1 ->  99.10800000732422 ; top5 ->  99.988  and loss:  2.856641963124275
test acc: top1 ->  90.29 ; top5 ->  98.79  and loss:  59.56366936862469
forward train acc: top1 ->  99.10799997558594 ; top5 ->  99.988  and loss:  2.73118245578371
test acc: top1 ->  90.34 ; top5 ->  98.86  and loss:  58.40161682665348
forward train acc: top1 ->  99.12800000244141 ; top5 ->  99.978  and loss:  2.7987559167668223
test acc: top1 ->  90.34 ; top5 ->  98.89  and loss:  58.28685374557972
forward train acc: top1 ->  99.20199997558593 ; top5 ->  99.998  and loss:  2.4051520372740924
test acc: top1 ->  90.33 ; top5 ->  98.81  and loss:  58.99795167148113
forward train acc: top1 ->  99.19000000732422 ; top5 ->  99.994  and loss:  2.4420067579485476
test acc: top1 ->  90.4 ; top5 ->  98.91  and loss:  57.47745721042156
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -36.349390185496304 , diff:  36.349390185496304
adv train loss:  -47.98436766862869 , diff:  11.634977483132388
adv train loss:  -46.39609822630882 , diff:  1.58826944231987
adv train loss:  -47.21469044685364 , diff:  0.8185922205448151
adv train loss:  -45.73984073102474 , diff:  1.4748497158288956
adv train loss:  -45.17518386244774 , diff:  0.5646568685770035
adv train loss:  -44.60315836966038 , diff:  0.5720254927873611
adv train loss:  -45.248932272195816 , diff:  0.6457739025354385
adv train loss:  -44.420452773571014 , diff:  0.8284794986248016
adv train loss:  -45.321555227041245 , diff:  0.9011024534702301
layer  1  adv train finish, try to retain  37
test acc: top1 ->  10.98 ; top5 ->  53.66  and loss:  695.1314940452576
forward train acc: top1 ->  97.16800001220703 ; top5 ->  99.906  and loss:  9.71209754049778
test acc: top1 ->  89.67 ; top5 ->  98.83  and loss:  54.51557491719723
forward train acc: top1 ->  98.07799998046875 ; top5 ->  99.93799997558594  and loss:  6.284680439159274
test acc: top1 ->  90.09 ; top5 ->  98.87  and loss:  52.11631380021572
forward train acc: top1 ->  98.48399998291016 ; top5 ->  99.974  and loss:  4.735281475819647
test acc: top1 ->  90.23 ; top5 ->  99.02  and loss:  49.96071510016918
forward train acc: top1 ->  98.81800000732422 ; top5 ->  99.984  and loss:  3.673123479820788
test acc: top1 ->  90.49 ; top5 ->  99.03  and loss:  50.50634963810444
forward train acc: top1 ->  98.99 ; top5 ->  99.992  and loss:  3.063717412762344
test acc: top1 ->  90.67 ; top5 ->  99.11  and loss:  51.20879563689232
forward train acc: top1 ->  99.01600000488281 ; top5 ->  99.992  and loss:  2.9663347601890564
test acc: top1 ->  90.6 ; top5 ->  99.06  and loss:  50.376475155353546
forward train acc: top1 ->  99.11999997558594 ; top5 ->  99.992  and loss:  2.6062905294820666
test acc: top1 ->  90.75 ; top5 ->  99.01  and loss:  50.9500587284565
forward train acc: top1 ->  99.27800000488281 ; top5 ->  99.992  and loss:  2.322755455505103
test acc: top1 ->  90.66 ; top5 ->  99.03  and loss:  51.420922458171844
forward train acc: top1 ->  99.214 ; top5 ->  99.99  and loss:  2.256951874587685
test acc: top1 ->  90.83 ; top5 ->  99.09  and loss:  51.20327231287956
forward train acc: top1 ->  99.30600000488282 ; top5 ->  99.994  and loss:  2.090714579913765
test acc: top1 ->  90.84 ; top5 ->  99.08  and loss:  51.462280467152596
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  55 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -1005.9834732195013 , diff:  1005.9834732195013
adv train loss:  -1568.4591064453125 , diff:  562.4756332258112
adv train loss:  -1578.3792028427124 , diff:  9.920096397399902
adv train loss:  -1570.2863054275513 , diff:  8.092897415161133
adv train loss:  -1562.9385766983032 , diff:  7.347728729248047
adv train loss:  -1561.774634361267 , diff:  1.1639423370361328
adv train loss:  -1571.4167003631592 , diff:  9.64206600189209
adv train loss:  -1565.9193449020386 , diff:  5.4973554611206055
adv train loss:  -1564.3428983688354 , diff:  1.576446533203125
adv train loss:  -1562.8353567123413 , diff:  1.5075416564941406
layer  2  adv train finish, try to retain  36
test acc: top1 ->  10.0 ; top5 ->  49.84  and loss:  963.7375268936157
forward train acc: top1 ->  79.78800000244141 ; top5 ->  97.53200000976562  and loss:  80.91022047400475
test acc: top1 ->  77.45 ; top5 ->  96.85  and loss:  81.22429102659225
forward train acc: top1 ->  83.73799999755859 ; top5 ->  98.45999997802734  and loss:  52.32325887680054
test acc: top1 ->  79.82 ; top5 ->  97.41  and loss:  69.23578408360481
forward train acc: top1 ->  86.02600001464843 ; top5 ->  98.82000000732423  and loss:  43.71442240476608
test acc: top1 ->  81.46 ; top5 ->  97.83  and loss:  63.02197661995888
forward train acc: top1 ->  87.48999997802734 ; top5 ->  99.15200000488281  and loss:  38.236392349004745
test acc: top1 ->  82.96 ; top5 ->  98.05  and loss:  59.30406755208969
forward train acc: top1 ->  88.94599997070313 ; top5 ->  99.25599997802735  and loss:  34.05780340731144
test acc: top1 ->  83.49 ; top5 ->  98.15  and loss:  56.81532196700573
forward train acc: top1 ->  89.83399999511718 ; top5 ->  99.43  and loss:  30.839759036898613
test acc: top1 ->  83.92 ; top5 ->  98.25  and loss:  55.136737048625946
forward train acc: top1 ->  90.18599998291016 ; top5 ->  99.42599997558594  and loss:  29.914316564798355
test acc: top1 ->  84.38 ; top5 ->  98.37  and loss:  54.2009237408638
forward train acc: top1 ->  90.64799998046875 ; top5 ->  99.47000000244141  and loss:  28.501207306981087
test acc: top1 ->  84.76 ; top5 ->  98.4  and loss:  53.20978827774525
forward train acc: top1 ->  90.97199999023438 ; top5 ->  99.49400000244141  and loss:  27.47823838889599
test acc: top1 ->  85.08 ; top5 ->  98.51  and loss:  52.620014399290085
forward train acc: top1 ->  91.26799998046874 ; top5 ->  99.594  and loss:  26.40271344780922
test acc: top1 ->  85.37 ; top5 ->  98.54  and loss:  51.82390147447586
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -546.4874078035355 , diff:  546.4874078035355
adv train loss:  -909.9865522384644 , diff:  363.4991444349289
adv train loss:  -947.0950622558594 , diff:  37.10851001739502
adv train loss:  -986.74342918396 , diff:  39.648366928100586
adv train loss:  -1004.2312698364258 , diff:  17.48784065246582
adv train loss:  -1006.6135368347168 , diff:  2.3822669982910156
adv train loss:  -1005.1150846481323 , diff:  1.4984521865844727
adv train loss:  -1002.0555772781372 , diff:  3.059507369995117
adv train loss:  -1005.7418756484985 , diff:  3.686298370361328
adv train loss:  -1000.306450843811 , diff:  5.4354248046875
layer  3  adv train finish, try to retain  23
test acc: top1 ->  10.62 ; top5 ->  49.83  and loss:  613.5018548965454
forward train acc: top1 ->  70.71799998535157 ; top5 ->  96.44199998779297  and loss:  91.43616282939911
test acc: top1 ->  71.47 ; top5 ->  96.44  and loss:  90.78996962308884
forward train acc: top1 ->  75.66200001464844 ; top5 ->  97.52999998535157  and loss:  73.04052495956421
test acc: top1 ->  74.18 ; top5 ->  97.11  and loss:  81.5966647863388
forward train acc: top1 ->  78.3740000024414 ; top5 ->  98.02600000732421  and loss:  64.4689729809761
test acc: top1 ->  76.13 ; top5 ->  97.47  and loss:  75.74324533343315
forward train acc: top1 ->  80.11400000976562 ; top5 ->  98.39000000244141  and loss:  59.00306862592697
test acc: top1 ->  77.88 ; top5 ->  97.81  and loss:  71.39232370257378
forward train acc: top1 ->  81.69599999511719 ; top5 ->  98.54799997558594  and loss:  54.6693719625473
test acc: top1 ->  78.64 ; top5 ->  97.82  and loss:  68.91015622019768
forward train acc: top1 ->  82.73400001464844 ; top5 ->  98.73400000732421  and loss:  51.42741513252258
test acc: top1 ->  79.23 ; top5 ->  97.9  and loss:  67.28244507312775
forward train acc: top1 ->  83.00399998779297 ; top5 ->  98.80200000976562  and loss:  49.886474430561066
test acc: top1 ->  79.48 ; top5 ->  97.99  and loss:  66.04972144961357
forward train acc: top1 ->  83.66800001953125 ; top5 ->  98.84400000976562  and loss:  48.28468635678291
test acc: top1 ->  79.72 ; top5 ->  98.05  and loss:  65.07864317297935
forward train acc: top1 ->  84.14999999511718 ; top5 ->  98.87400000732421  and loss:  47.42298159003258
test acc: top1 ->  80.07 ; top5 ->  98.2  and loss:  64.09730911254883
forward train acc: top1 ->  84.26399997558593 ; top5 ->  98.97599997558594  and loss:  46.25560191273689
test acc: top1 ->  80.22 ; top5 ->  98.24  and loss:  63.25613313913345
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -95.94236436486244 , diff:  95.94236436486244
adv train loss:  -697.6022839546204 , diff:  601.6599195897579
adv train loss:  -784.6041264533997 , diff:  87.0018424987793
adv train loss:  -808.8234767913818 , diff:  24.219350337982178
adv train loss:  -855.8322057723999 , diff:  47.008728981018066
adv train loss:  -880.7754049301147 , diff:  24.943199157714844
adv train loss:  -886.4172248840332 , diff:  5.641819953918457
adv train loss:  -892.3859510421753 , diff:  5.96872615814209
adv train loss:  -893.0708427429199 , diff:  0.6848917007446289
adv train loss:  -889.2147417068481 , diff:  3.8561010360717773
layer  4  adv train finish, try to retain  26
test acc: top1 ->  10.15 ; top5 ->  50.68  and loss:  1242.8601121902466
forward train acc: top1 ->  68.86599998535156 ; top5 ->  95.53800001464843  and loss:  92.95386153459549
test acc: top1 ->  70.01 ; top5 ->  95.76  and loss:  93.04094165563583
forward train acc: top1 ->  74.10599999511719 ; top5 ->  97.27800000976562  and loss:  74.87401384115219
test acc: top1 ->  73.18 ; top5 ->  96.6  and loss:  82.04680317640305
forward train acc: top1 ->  76.97600001953126 ; top5 ->  97.76200001220703  and loss:  66.85484462976456
test acc: top1 ->  74.63 ; top5 ->  97.31  and loss:  76.29436355829239
forward train acc: top1 ->  78.78399997558594 ; top5 ->  98.17200000976563  and loss:  61.560588359832764
test acc: top1 ->  75.75 ; top5 ->  97.5  and loss:  73.49256247282028
forward train acc: top1 ->  80.19399999267578 ; top5 ->  98.38400000976563  and loss:  57.84040552377701
test acc: top1 ->  77.07 ; top5 ->  97.85  and loss:  69.73959594964981
forward train acc: top1 ->  81.19399998046875 ; top5 ->  98.53800000976563  and loss:  54.55064409971237
test acc: top1 ->  77.47 ; top5 ->  97.83  and loss:  68.64136692881584
forward train acc: top1 ->  81.45599997070312 ; top5 ->  98.67800000488282  and loss:  53.70048916339874
test acc: top1 ->  78.07 ; top5 ->  97.96  and loss:  66.94834598898888
forward train acc: top1 ->  81.88400000732422 ; top5 ->  98.67399998779297  and loss:  52.57807621359825
test acc: top1 ->  78.26 ; top5 ->  98.05  and loss:  66.1696073114872
forward train acc: top1 ->  82.16999998046875 ; top5 ->  98.72200000488282  and loss:  51.83297887444496
test acc: top1 ->  78.57 ; top5 ->  98.0  and loss:  65.1506755053997
forward train acc: top1 ->  82.53400001708984 ; top5 ->  98.80200000732422  and loss:  50.38202279806137
test acc: top1 ->  79.15 ; top5 ->  98.08  and loss:  64.46116611361504
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -354.96111197024584 , diff:  354.96111197024584
adv train loss:  -1033.3330011367798 , diff:  678.371889166534
adv train loss:  -1126.2393016815186 , diff:  92.90630054473877
adv train loss:  -1144.7008390426636 , diff:  18.46153736114502
adv train loss:  -1147.7838020324707 , diff:  3.082962989807129
adv train loss:  -1152.661720275879 , diff:  4.877918243408203
adv train loss:  -1152.049087524414 , diff:  0.6126327514648438
adv train loss:  -1154.9722862243652 , diff:  2.923198699951172
adv train loss:  -1160.115234375 , diff:  5.142948150634766
adv train loss:  -1161.4650783538818 , diff:  1.349843978881836
layer  5  adv train finish, try to retain  26
test acc: top1 ->  17.24 ; top5 ->  50.57  and loss:  454.7883050441742
forward train acc: top1 ->  77.38599998779297 ; top5 ->  97.62199998291015  and loss:  67.46321403980255
test acc: top1 ->  76.89 ; top5 ->  97.45  and loss:  73.763667345047
forward train acc: top1 ->  82.47799997802734 ; top5 ->  98.49199997802734  and loss:  51.96391847729683
test acc: top1 ->  80.06 ; top5 ->  97.98  and loss:  63.901287317276
forward train acc: top1 ->  85.09000001464844 ; top5 ->  98.99799997558594  and loss:  43.93235144019127
test acc: top1 ->  82.06 ; top5 ->  98.29  and loss:  58.51845368742943
forward train acc: top1 ->  86.83999997314453 ; top5 ->  99.264  and loss:  39.0110998749733
test acc: top1 ->  83.19 ; top5 ->  98.44  and loss:  55.70684948563576
forward train acc: top1 ->  87.89199997314454 ; top5 ->  99.38999997802735  and loss:  35.453253120183945
test acc: top1 ->  83.93 ; top5 ->  98.57  and loss:  53.33034759759903
forward train acc: top1 ->  88.64199999267578 ; top5 ->  99.40800000244141  and loss:  33.44817978143692
test acc: top1 ->  84.16 ; top5 ->  98.63  and loss:  52.03671273589134
forward train acc: top1 ->  88.94999999755859 ; top5 ->  99.4660000024414  and loss:  32.16406302154064
test acc: top1 ->  84.59 ; top5 ->  98.7  and loss:  51.221732407808304
forward train acc: top1 ->  89.49399997558594 ; top5 ->  99.51000000488281  and loss:  31.34241819381714
test acc: top1 ->  84.83 ; top5 ->  98.71  and loss:  50.571282625198364
forward train acc: top1 ->  89.71799998535157 ; top5 ->  99.51799997802735  and loss:  30.244866654276848
test acc: top1 ->  85.06 ; top5 ->  98.77  and loss:  49.80367471277714
forward train acc: top1 ->  90.05400001708985 ; top5 ->  99.61600000244141  and loss:  29.131362557411194
test acc: top1 ->  85.27 ; top5 ->  98.77  and loss:  49.62790460884571
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -5.034612666815519 , diff:  5.034612666815519
adv train loss:  -5.038291580975056 , diff:  0.0036789141595363617
************ all values are small in this layer **********
layer  6  adv train finish, try to retain  143
test acc: top1 ->  86.65 ; top5 ->  98.63  and loss:  43.69351829588413
forward train acc: top1 ->  98.9060000024414 ; top5 ->  99.99  and loss:  3.5297464691102505
test acc: top1 ->  91.24 ; top5 ->  99.33  and loss:  40.22200855612755
forward train acc: top1 ->  99.37999997802734 ; top5 ->  99.996  and loss:  1.9061725214123726
test acc: top1 ->  91.47 ; top5 ->  99.29  and loss:  43.84683296084404
forward train acc: top1 ->  99.53600000244141 ; top5 ->  100.0  and loss:  1.3822656630072743
test acc: top1 ->  91.58 ; top5 ->  99.26  and loss:  45.865971237421036
forward train acc: top1 ->  99.60799997558594 ; top5 ->  99.996  and loss:  1.1773848935263231
test acc: top1 ->  91.64 ; top5 ->  99.32  and loss:  46.569487020373344
forward train acc: top1 ->  99.724 ; top5 ->  100.0  and loss:  0.8406095898826607
test acc: top1 ->  91.68 ; top5 ->  99.25  and loss:  49.44828063249588
forward train acc: top1 ->  99.724 ; top5 ->  99.998  and loss:  0.7955843353993259
test acc: top1 ->  91.7 ; top5 ->  99.27  and loss:  50.15591770410538
forward train acc: top1 ->  99.798 ; top5 ->  100.0  and loss:  0.6178556915838271
test acc: top1 ->  91.71 ; top5 ->  99.27  and loss:  50.99207779765129
forward train acc: top1 ->  99.74199997558594 ; top5 ->  100.0  and loss:  0.7513371306122281
test acc: top1 ->  91.74 ; top5 ->  99.21  and loss:  50.901005640625954
forward train acc: top1 ->  99.816 ; top5 ->  100.0  and loss:  0.5738956728600897
test acc: top1 ->  91.76 ; top5 ->  99.26  and loss:  51.76005233824253
forward train acc: top1 ->  99.82399997558593 ; top5 ->  99.998  and loss:  0.5498668031068519
test acc: top1 ->  91.67 ; top5 ->  99.22  and loss:  52.825956270098686
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  144 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -0.4092686539806891 , diff:  0.4092686539806891
adv train loss:  -0.49206186924129725 , diff:  0.08279321526060812
adv train loss:  -0.4545472856552806 , diff:  0.03751458358601667
adv train loss:  -0.4738134371000342 , diff:  0.019266151444753632
adv train loss:  -0.4339528168347897 , diff:  0.039860620265244506
adv train loss:  -0.41628747136564925 , diff:  0.017665345469140448
adv train loss:  -0.4205244372715242 , diff:  0.004236965905874968
layer  7  adv train finish, try to retain  249
test acc: top1 ->  62.37 ; top5 ->  90.82  and loss:  156.56081837415695
forward train acc: top1 ->  99.59799997558594 ; top5 ->  99.998  and loss:  1.1937546416884288
test acc: top1 ->  91.34 ; top5 ->  99.26  and loss:  50.169717743992805
forward train acc: top1 ->  99.726 ; top5 ->  100.0  and loss:  0.8006658472586423
test acc: top1 ->  91.45 ; top5 ->  99.3  and loss:  51.62939465045929
forward train acc: top1 ->  99.7400000024414 ; top5 ->  99.998  and loss:  0.7397983534028754
test acc: top1 ->  91.59 ; top5 ->  99.29  and loss:  51.73458309099078
forward train acc: top1 ->  99.81 ; top5 ->  100.0  and loss:  0.5502241703215986
test acc: top1 ->  91.6 ; top5 ->  99.27  and loss:  53.478733509778976
forward train acc: top1 ->  99.83399997558594 ; top5 ->  100.0  and loss:  0.5025701423583087
test acc: top1 ->  91.62 ; top5 ->  99.23  and loss:  55.02363342791796
forward train acc: top1 ->  99.844 ; top5 ->  100.0  and loss:  0.44706152041908354
test acc: top1 ->  91.64 ; top5 ->  99.24  and loss:  54.49084962904453
forward train acc: top1 ->  99.884 ; top5 ->  100.0  and loss:  0.36944421447697096
test acc: top1 ->  91.65 ; top5 ->  99.21  and loss:  55.71375346183777
forward train acc: top1 ->  99.8640000024414 ; top5 ->  100.0  and loss:  0.3581584121566266
test acc: top1 ->  91.59 ; top5 ->  99.21  and loss:  55.95397078990936
forward train acc: top1 ->  99.8880000024414 ; top5 ->  100.0  and loss:  0.31712541286833584
test acc: top1 ->  91.64 ; top5 ->  99.22  and loss:  57.34392612427473
forward train acc: top1 ->  99.902 ; top5 ->  100.0  and loss:  0.30562731766258366
test acc: top1 ->  91.62 ; top5 ->  99.27  and loss:  57.45032922923565
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  253 / 512 , inc:  31
---------------- start layer  8  ---------------
adv train loss:  -0.4067871648003347 , diff:  0.4067871648003347
adv train loss:  -0.34098583288141526 , diff:  0.06580133191891946
adv train loss:  -0.38150829750520643 , diff:  0.040522464623791166
adv train loss:  -0.326667643035762 , diff:  0.054840654469444416
adv train loss:  -0.38390258977597114 , diff:  0.057234946740209125
adv train loss:  -0.2997089119744487 , diff:  0.08419367780152243
adv train loss:  -0.3987831716949586 , diff:  0.09907425972050987
adv train loss:  -0.38432690178160556 , diff:  0.014456269913353026
adv train loss:  -0.300980095162231 , diff:  0.08334680661937455
adv train loss:  -0.3976409075112315 , diff:  0.09666081234900048
layer  8  adv train finish, try to retain  271
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -0.5195535153034143 , diff:  0.5195535153034143
adv train loss:  -0.530648381478386 , diff:  0.011094866174971685
adv train loss:  -0.4537474854878383 , diff:  0.07690089599054772
adv train loss:  -0.4730801349505782 , diff:  0.019332649462739937
adv train loss:  -0.5480373142636381 , diff:  0.0749571793130599
adv train loss:  -0.4573395688930759 , diff:  0.09069774537056219
adv train loss:  -0.5009093897097046 , diff:  0.043569820816628635
adv train loss:  -0.45300179127661977 , diff:  0.047907598433084786
adv train loss:  -0.4839496462227544 , diff:  0.030947854946134612
adv train loss:  -0.4957417220866773 , diff:  0.011792075863922946
layer  9  adv train finish, try to retain  446
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -10.472873881459236 , diff:  10.472873881459236
adv train loss:  -10.525334633886814 , diff:  0.05246075242757797
adv train loss:  -10.458559188991785 , diff:  0.06677544489502907
adv train loss:  -10.634762302041054 , diff:  0.17620311304926872
adv train loss:  -10.519827369600534 , diff:  0.11493493244051933
adv train loss:  -10.62479692697525 , diff:  0.1049695573747158
adv train loss:  -10.255860071629286 , diff:  0.36893685534596443
adv train loss:  -10.37073240801692 , diff:  0.11487233638763428
adv train loss:  -10.34293315000832 , diff:  0.02779925800859928
adv train loss:  -10.502528339624405 , diff:  0.1595951896160841
layer  10  adv train finish, try to retain  489
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -0.7903731398400851 , diff:  0.7903731398400851
adv train loss:  -0.8545313524955418 , diff:  0.06415821265545674
adv train loss:  -0.5985867094568675 , diff:  0.25594464303867426
adv train loss:  -0.716597406368237 , diff:  0.11801069691136945
adv train loss:  -0.7950343087431975 , diff:  0.07843690237496048
adv train loss:  -0.6215526661835611 , diff:  0.17348164255963638
adv train loss:  -0.7086698412022088 , diff:  0.08711717501864769
adv train loss:  -0.8329131895734463 , diff:  0.12424334837123752
adv train loss:  -0.6691827692120569 , diff:  0.1637304203613894
adv train loss:  -0.8764994599914644 , diff:  0.2073166907794075
layer  11  adv train finish, try to retain  404
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
adv train loss:  -643.2791347503662 , diff:  643.2791347503662
adv train loss:  -646.9176287651062 , diff:  3.6384940147399902
adv train loss:  -646.9633660316467 , diff:  0.045737266540527344
layer  13  adv train finish, try to retain  465
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  15
layer  8  :  0.501953125  ==>  257 / 512 , inc:  16
layer  9  :  0.126953125  ==>  65 / 512 , inc:  8
layer  10  :  0.037109375  ==>  19 / 512 , inc:  1
layer  11  :  0.189453125  ==>  97 / 512 , inc:  24
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.123046875  ==>  63 / 512 , inc:  8
eps [0.004805419921875001, 1.4580000000000002, 1.4580000000000002, 1.4580000000000002, 1.4580000000000002, 1.4580000000000002, 0.012814453125000002, 0.0016018066406250002, 0.0042714843750000005, 0.011390625000000001, 1.4580000000000002, 0.011390625000000001, 1.0935000000000001, 0.9720000000000002]  wait [4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 3, 0]  inc [1, 1, 1, 1, 1, 1, 1, 15, 16, 8, 1, 24, 1, 8]  tol: 3
$$$$$$$$$$$$$ epoch  24  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
adv train loss:  -30.039188989554532 , diff:  30.039188989554532
adv train loss:  -55.45583000779152 , diff:  25.416641018236987
adv train loss:  -57.67540964484215 , diff:  2.2195796370506287
adv train loss:  -56.58610287308693 , diff:  1.0893067717552185
adv train loss:  -56.745129615068436 , diff:  0.15902674198150635
adv train loss:  -56.93883827328682 , diff:  0.1937086582183838
adv train loss:  -57.90160295367241 , diff:  0.9627646803855896
adv train loss:  -62.963473707437515 , diff:  5.061870753765106
adv train loss:  -63.97272723913193 , diff:  1.0092535316944122
adv train loss:  -62.84877362847328 , diff:  1.1239536106586456
layer  1  adv train finish, try to retain  41
test acc: top1 ->  15.94 ; top5 ->  53.31  and loss:  857.246829032898
forward train acc: top1 ->  98.10800000732422 ; top5 ->  99.928  and loss:  6.94528409326449
test acc: top1 ->  90.42 ; top5 ->  99.22  and loss:  56.12051612883806
forward train acc: top1 ->  98.73599997558594 ; top5 ->  99.98  and loss:  3.7771179834380746
test acc: top1 ->  90.73 ; top5 ->  99.23  and loss:  48.94484709575772
forward train acc: top1 ->  99.158 ; top5 ->  99.996  and loss:  2.5817202581092715
test acc: top1 ->  90.92 ; top5 ->  99.25  and loss:  48.40619596466422
forward train acc: top1 ->  99.16600000244141 ; top5 ->  99.99  and loss:  2.444390928838402
test acc: top1 ->  91.01 ; top5 ->  99.28  and loss:  47.50307108461857
forward train acc: top1 ->  99.32200000488281 ; top5 ->  99.996  and loss:  1.9949319671140984
test acc: top1 ->  91.25 ; top5 ->  99.28  and loss:  47.39200867712498
forward train acc: top1 ->  99.426 ; top5 ->  99.998  and loss:  1.6634336239658296
test acc: top1 ->  91.3 ; top5 ->  99.26  and loss:  47.60848252475262
forward train acc: top1 ->  99.41000000244141 ; top5 ->  99.99599997558593  and loss:  1.6313171181827784
test acc: top1 ->  91.34 ; top5 ->  99.25  and loss:  47.49460007250309
forward train acc: top1 ->  99.49599997558593 ; top5 ->  99.998  and loss:  1.4806318301707506
test acc: top1 ->  91.36 ; top5 ->  99.21  and loss:  48.37478129565716
forward train acc: top1 ->  99.54800000244141 ; top5 ->  100.0  and loss:  1.246620926540345
test acc: top1 ->  91.33 ; top5 ->  99.25  and loss:  48.55031734704971
forward train acc: top1 ->  99.544 ; top5 ->  99.996  and loss:  1.309312314959243
test acc: top1 ->  91.35 ; top5 ->  99.24  and loss:  48.15178030729294
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  55 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -923.2263640497113 , diff:  923.2263640497113
adv train loss:  -1541.4762296676636 , diff:  618.2498656179523
adv train loss:  -1545.995888710022 , diff:  4.519659042358398
adv train loss:  -1550.3041353225708 , diff:  4.308246612548828
adv train loss:  -1555.3339500427246 , diff:  5.029814720153809
adv train loss:  -1556.1892185211182 , diff:  0.8552684783935547
adv train loss:  -1556.9436874389648 , diff:  0.7544689178466797
adv train loss:  -1549.9073038101196 , diff:  7.036383628845215
adv train loss:  -1560.5428533554077 , diff:  10.635549545288086
adv train loss:  -1558.013705253601 , diff:  2.5291481018066406
layer  2  adv train finish, try to retain  54
test acc: top1 ->  21.04 ; top5 ->  69.31  and loss:  968.3899936676025
forward train acc: top1 ->  96.03200001464843 ; top5 ->  99.862  and loss:  13.115661211311817
test acc: top1 ->  88.74 ; top5 ->  99.07  and loss:  49.066016390919685
forward train acc: top1 ->  96.98999999267578 ; top5 ->  99.91399997558594  and loss:  9.015111975371838
test acc: top1 ->  89.27 ; top5 ->  99.08  and loss:  45.32209378480911
forward train acc: top1 ->  97.73799998535156 ; top5 ->  99.952  and loss:  6.865195535123348
test acc: top1 ->  89.48 ; top5 ->  99.1  and loss:  44.939480155706406
forward train acc: top1 ->  97.97999998291016 ; top5 ->  99.972  and loss:  6.090474281460047
test acc: top1 ->  89.69 ; top5 ->  99.13  and loss:  45.29985477030277
forward train acc: top1 ->  98.22400000732422 ; top5 ->  99.978  and loss:  5.279235091991723
test acc: top1 ->  89.75 ; top5 ->  99.18  and loss:  45.45894357562065
forward train acc: top1 ->  98.31199998535156 ; top5 ->  99.978  and loss:  4.770948315039277
test acc: top1 ->  89.88 ; top5 ->  99.16  and loss:  45.73248291015625
forward train acc: top1 ->  98.42199998535156 ; top5 ->  99.98  and loss:  4.550910282880068
test acc: top1 ->  89.94 ; top5 ->  99.19  and loss:  45.364641919732094
forward train acc: top1 ->  98.44000000732422 ; top5 ->  99.982  and loss:  4.584495672956109
test acc: top1 ->  90.07 ; top5 ->  99.22  and loss:  44.586994990706444
forward train acc: top1 ->  98.57799998046875 ; top5 ->  99.978  and loss:  4.067016403656453
test acc: top1 ->  90.09 ; top5 ->  99.26  and loss:  46.01339101791382
forward train acc: top1 ->  98.60399998046876 ; top5 ->  99.988  and loss:  4.050946921110153
test acc: top1 ->  90.15 ; top5 ->  99.24  and loss:  45.758948624134064
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -764.0221834213007 , diff:  764.0221834213007
adv train loss:  -1381.6397380828857 , diff:  617.617554661585
adv train loss:  -1411.6648797988892 , diff:  30.025141716003418
adv train loss:  -1414.6290254592896 , diff:  2.9641456604003906
adv train loss:  -1432.8157815933228 , diff:  18.186756134033203
adv train loss:  -1430.885747909546 , diff:  1.9300336837768555
adv train loss:  -1436.8004884719849 , diff:  5.914740562438965
adv train loss:  -1431.0858249664307 , diff:  5.714663505554199
adv train loss:  -1440.2644634246826 , diff:  9.178638458251953
adv train loss:  -1434.9294166564941 , diff:  5.335046768188477
layer  3  adv train finish, try to retain  33
test acc: top1 ->  13.5 ; top5 ->  58.03  and loss:  761.9098653793335
forward train acc: top1 ->  84.49999997802735 ; top5 ->  98.87199997802735  and loss:  51.367789536714554
test acc: top1 ->  80.62 ; top5 ->  97.99  and loss:  63.0163671374321
forward train acc: top1 ->  86.96000001708984 ; top5 ->  99.25599997558594  and loss:  38.84426945447922
test acc: top1 ->  82.33 ; top5 ->  98.27  and loss:  57.90707328915596
forward train acc: top1 ->  88.35599999267578 ; top5 ->  99.4  and loss:  34.699066400527954
test acc: top1 ->  83.25 ; top5 ->  98.49  and loss:  54.8997635692358
forward train acc: top1 ->  89.23800001220702 ; top5 ->  99.48399997558593  and loss:  31.422934651374817
test acc: top1 ->  84.01 ; top5 ->  98.58  and loss:  52.713849514722824
forward train acc: top1 ->  90.02 ; top5 ->  99.52000000488282  and loss:  29.377138793468475
test acc: top1 ->  84.54 ; top5 ->  98.68  and loss:  51.54995773732662
forward train acc: top1 ->  90.6620000024414 ; top5 ->  99.624  and loss:  27.48434929549694
test acc: top1 ->  84.67 ; top5 ->  98.72  and loss:  51.05738563835621
forward train acc: top1 ->  90.57999997558593 ; top5 ->  99.64000000488281  and loss:  27.29631793498993
test acc: top1 ->  84.85 ; top5 ->  98.8  and loss:  50.321345657110214
forward train acc: top1 ->  91.05399999023437 ; top5 ->  99.6300000024414  and loss:  26.35679703950882
test acc: top1 ->  85.16 ; top5 ->  98.74  and loss:  50.12848241627216
forward train acc: top1 ->  91.02799997802734 ; top5 ->  99.7100000024414  and loss:  25.429515093564987
test acc: top1 ->  85.21 ; top5 ->  98.76  and loss:  49.675168603658676
forward train acc: top1 ->  91.46200001220703 ; top5 ->  99.716  and loss:  24.832262441515923
test acc: top1 ->  85.37 ; top5 ->  98.83  and loss:  49.096515476703644
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -145.99391554109752 , diff:  145.99391554109752
adv train loss:  -875.2278342247009 , diff:  729.2339186836034
adv train loss:  -994.1221265792847 , diff:  118.89429235458374
adv train loss:  -1051.7493658065796 , diff:  57.62723922729492
adv train loss:  -1074.9095935821533 , diff:  23.16022777557373
adv train loss:  -1082.796838760376 , diff:  7.887245178222656
adv train loss:  -1084.7171230316162 , diff:  1.9202842712402344
adv train loss:  -1087.234398841858 , diff:  2.517275810241699
adv train loss:  -1087.1168575286865 , diff:  0.11754131317138672
adv train loss:  -1092.7300186157227 , diff:  5.613161087036133
layer  4  adv train finish, try to retain  57
test acc: top1 ->  38.96 ; top5 ->  73.95  and loss:  914.8883285522461
forward train acc: top1 ->  87.98199998535156 ; top5 ->  99.328  and loss:  35.712983682751656
test acc: top1 ->  83.07 ; top5 ->  98.63  and loss:  54.325014382600784
forward train acc: top1 ->  90.28000001464844 ; top5 ->  99.61399997558594  and loss:  28.247564181685448
test acc: top1 ->  84.34 ; top5 ->  98.83  and loss:  50.87592393159866
forward train acc: top1 ->  91.27000001708984 ; top5 ->  99.70199997558593  and loss:  24.978816404938698
test acc: top1 ->  85.1 ; top5 ->  98.9  and loss:  49.05142185091972
forward train acc: top1 ->  92.1199999975586 ; top5 ->  99.724  and loss:  22.688076585531235
test acc: top1 ->  85.68 ; top5 ->  98.97  and loss:  47.70688749849796
forward train acc: top1 ->  92.59600001464844 ; top5 ->  99.758  and loss:  21.180912494659424
test acc: top1 ->  86.03 ; top5 ->  99.04  and loss:  47.25962319970131
forward train acc: top1 ->  92.9920000024414 ; top5 ->  99.788  and loss:  19.91987606137991
test acc: top1 ->  86.22 ; top5 ->  99.07  and loss:  46.523575842380524
forward train acc: top1 ->  93.34799997802735 ; top5 ->  99.808  and loss:  19.112306773662567
test acc: top1 ->  86.3 ; top5 ->  99.03  and loss:  46.55633118748665
forward train acc: top1 ->  93.49000000488282 ; top5 ->  99.82  and loss:  18.50481128692627
test acc: top1 ->  86.41 ; top5 ->  99.06  and loss:  46.14693519473076
forward train acc: top1 ->  93.7159999975586 ; top5 ->  99.822  and loss:  18.005015276372433
test acc: top1 ->  86.57 ; top5 ->  99.11  and loss:  46.00728763639927
forward train acc: top1 ->  93.80199997314453 ; top5 ->  99.84799997558594  and loss:  17.744021840393543
test acc: top1 ->  86.55 ; top5 ->  99.06  and loss:  46.16427128016949
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -385.11283881776035 , diff:  385.11283881776035
adv train loss:  -1379.3489542007446 , diff:  994.2361153829843
adv train loss:  -1486.7299194335938 , diff:  107.38096523284912
adv train loss:  -1541.3213758468628 , diff:  54.59145641326904
adv train loss:  -1558.6226892471313 , diff:  17.301313400268555
adv train loss:  -1558.6161289215088 , diff:  0.006560325622558594
layer  5  adv train finish, try to retain  37
test acc: top1 ->  27.5 ; top5 ->  66.17  and loss:  1288.0314092636108
forward train acc: top1 ->  80.92200000488282 ; top5 ->  98.51799997802735  and loss:  58.822859704494476
test acc: top1 ->  79.08 ; top5 ->  98.13  and loss:  66.93731585144997
forward train acc: top1 ->  85.99999997314453 ; top5 ->  99.27999997802735  and loss:  41.202857315540314
test acc: top1 ->  81.35 ; top5 ->  98.48  and loss:  59.33181571960449
forward train acc: top1 ->  88.05199999267577 ; top5 ->  99.4240000024414  and loss:  35.1298545897007
test acc: top1 ->  82.92 ; top5 ->  98.64  and loss:  55.12263908982277
forward train acc: top1 ->  89.31599998535157 ; top5 ->  99.60999997558594  and loss:  31.02310973405838
test acc: top1 ->  83.86 ; top5 ->  98.8  and loss:  52.5458770096302
forward train acc: top1 ->  90.23399998535156 ; top5 ->  99.63399997558594  and loss:  28.165229558944702
test acc: top1 ->  84.47 ; top5 ->  98.97  and loss:  50.97058416903019
forward train acc: top1 ->  90.91600001953125 ; top5 ->  99.68  and loss:  26.14528140425682
test acc: top1 ->  84.72 ; top5 ->  98.96  and loss:  50.30951537191868
forward train acc: top1 ->  91.18400000976563 ; top5 ->  99.68199997558594  and loss:  25.354230031371117
test acc: top1 ->  85.16 ; top5 ->  98.97  and loss:  49.64029213786125
forward train acc: top1 ->  91.38799999023438 ; top5 ->  99.734  and loss:  24.705125346779823
test acc: top1 ->  85.32 ; top5 ->  99.03  and loss:  48.720324128866196
forward train acc: top1 ->  91.73200001220704 ; top5 ->  99.7300000024414  and loss:  23.938647225499153
test acc: top1 ->  85.53 ; top5 ->  99.02  and loss:  48.595694810152054
forward train acc: top1 ->  91.96800001708985 ; top5 ->  99.748  and loss:  22.966823413968086
test acc: top1 ->  85.6 ; top5 ->  99.07  and loss:  48.40589326620102
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -4.790414238348603 , diff:  4.790414238348603
adv train loss:  -4.663390150293708 , diff:  0.1270240880548954
adv train loss:  -4.8066633846610785 , diff:  0.1432732343673706
adv train loss:  -4.7672848384827375 , diff:  0.03937854617834091
adv train loss:  -4.7434339839965105 , diff:  0.023850854486227036
adv train loss:  -4.718417700380087 , diff:  0.025016283616423607
adv train loss:  -4.830576241016388 , diff:  0.11215854063630104
adv train loss:  -4.866483872756362 , diff:  0.03590763173997402
adv train loss:  -4.688827006146312 , diff:  0.1776568666100502
adv train loss:  -4.8562189768999815 , diff:  0.16739197075366974
layer  6  adv train finish, try to retain  256
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -3.9349261075258255 , diff:  3.9349261075258255
adv train loss:  -4.007899841293693 , diff:  0.07297373376786709
adv train loss:  -3.973055839538574 , diff:  0.03484400175511837
adv train loss:  -3.9508659839630127 , diff:  0.022189855575561523
adv train loss:  -3.9499652720987797 , diff:  0.000900711864233017
layer  7  adv train finish, try to retain  273
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -4.866924002766609 , diff:  4.866924002766609
adv train loss:  -4.744065323844552 , diff:  0.12285867892205715
adv train loss:  -4.930082207545638 , diff:  0.18601688370108604
adv train loss:  -4.828768867999315 , diff:  0.10131333954632282
adv train loss:  -4.979168448597193 , diff:  0.1503995805978775
adv train loss:  -4.816751888021827 , diff:  0.16241656057536602
adv train loss:  -4.889643123373389 , diff:  0.0728912353515625
adv train loss:  -4.912442417815328 , diff:  0.0227992944419384
adv train loss:  -4.8299739677459 , diff:  0.08246845006942749
adv train loss:  -4.793776655569673 , diff:  0.03619731217622757
layer  8  adv train finish, try to retain  291
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -4.404895894229412 , diff:  4.404895894229412
adv train loss:  -4.475186187773943 , diff:  0.07029029354453087
adv train loss:  -4.258149346336722 , diff:  0.21703684143722057
adv train loss:  -4.353281755000353 , diff:  0.09513240866363049
adv train loss:  -4.32761150971055 , diff:  0.02567024528980255
adv train loss:  -4.411684358492494 , diff:  0.08407284878194332
adv train loss:  -4.441341580823064 , diff:  0.02965722233057022
adv train loss:  -4.438710996881127 , diff:  0.002630583941936493
layer  9  adv train finish, try to retain  446
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -66.08424699306488 , diff:  66.08424699306488
adv train loss:  -66.24956941604614 , diff:  0.1653224229812622
adv train loss:  -66.3179714679718 , diff:  0.06840205192565918
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  18
test acc: top1 ->  10.0 ; top5 ->  66.53  and loss:  135159.642578125
forward train acc: top1 ->  96.89799997558593 ; top5 ->  99.896  and loss:  12.567353662103415
test acc: top1 ->  91.36 ; top5 ->  99.21  and loss:  32.59152094274759
forward train acc: top1 ->  99.64000000244141 ; top5 ->  99.998  and loss:  2.0799193866550922
test acc: top1 ->  91.75 ; top5 ->  99.24  and loss:  34.655512131750584
forward train acc: top1 ->  99.8020000024414 ; top5 ->  100.0  and loss:  1.085929624736309
test acc: top1 ->  91.98 ; top5 ->  99.22  and loss:  36.949029721319675
forward train acc: top1 ->  99.86 ; top5 ->  100.0  and loss:  0.7180695023853332
test acc: top1 ->  91.97 ; top5 ->  99.22  and loss:  38.84709721803665
forward train acc: top1 ->  99.908 ; top5 ->  100.0  and loss:  0.48226422257721424
test acc: top1 ->  92.13 ; top5 ->  99.18  and loss:  40.717551313340664
==> this epoch:  18 / 512
---------------- start layer  11  ---------------
adv train loss:  -3.7829263489693403 , diff:  3.7829263489693403
adv train loss:  -3.8329211371019483 , diff:  0.04999478813260794
adv train loss:  -3.765316323377192 , diff:  0.06760481372475624
adv train loss:  -3.8425591867417097 , diff:  0.07724286336451769
adv train loss:  -3.825714214704931 , diff:  0.016844972036778927
adv train loss:  -3.6837749145925045 , diff:  0.14193930011242628
adv train loss:  -3.6729379184544086 , diff:  0.010836996138095856
adv train loss:  -3.7260790364816785 , diff:  0.05314111802726984
adv train loss:  -3.746868761256337 , diff:  0.02078972477465868
adv train loss:  -3.709363623522222 , diff:  0.037505137734115124
layer  11  adv train finish, try to retain  407
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
### skip layer  12 wait:  3  ###
---------------- start layer  13  ---------------
adv train loss:  -4434.417003631592 , diff:  4434.417003631592
adv train loss:  -4896.093395233154 , diff:  461.6763916015625
adv train loss:  -4901.231342315674 , diff:  5.137947082519531
adv train loss:  -4902.707260131836 , diff:  1.4759178161621094
layer  13  adv train finish, try to retain  463
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  15
layer  8  :  0.501953125  ==>  257 / 512 , inc:  16
layer  9  :  0.126953125  ==>  65 / 512 , inc:  8
layer  10  :  0.03515625  ==>  18 / 512 , inc:  2
layer  11  :  0.189453125  ==>  97 / 512 , inc:  24
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.123046875  ==>  63 / 512 , inc:  8
eps [0.004805419921875001, 1.0935000000000001, 1.0935000000000001, 1.0935000000000001, 1.0935000000000001, 1.0935000000000001, 0.025628906250000003, 0.0032036132812500004, 0.008542968750000001, 0.022781250000000003, 1.4580000000000002, 0.022781250000000003, 1.0935000000000001, 1.9440000000000004]  wait [3, 4, 4, 4, 4, 4, 2, 2, 2, 2, 0, 0, 2, 0]  inc [1, 1, 1, 1, 1, 1, 1, 15, 16, 8, 2, 24, 1, 8]  tol: 3
$$$$$$$$$$$$$ epoch  25  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
### skip layer  1 wait:  4  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
adv train loss:  -0.4172493430087343 , diff:  0.4172493430087343
adv train loss:  -0.46907405846286565 , diff:  0.051824715454131365
adv train loss:  -0.39923169708345085 , diff:  0.0698423613794148
adv train loss:  -0.4314620887162164 , diff:  0.03223039163276553
adv train loss:  -0.4151769117452204 , diff:  0.016285176970995963
adv train loss:  -0.4181359256617725 , diff:  0.002959013916552067
layer  6  adv train finish, try to retain  256
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -0.5240710794460028 , diff:  0.5240710794460028
adv train loss:  -0.531175471842289 , diff:  0.00710439239628613
layer  7  adv train finish, try to retain  243
test acc: top1 ->  27.2 ; top5 ->  58.21  and loss:  397.239385843277
forward train acc: top1 ->  98.50599997558594 ; top5 ->  99.988  and loss:  4.83672159537673
test acc: top1 ->  90.69 ; top5 ->  99.07  and loss:  46.352892369031906
forward train acc: top1 ->  99.34200000244141 ; top5 ->  100.0  and loss:  2.117953820619732
test acc: top1 ->  90.94 ; top5 ->  99.18  and loss:  48.6823265850544
forward train acc: top1 ->  99.43399997558593 ; top5 ->  99.998  and loss:  1.7663764152675867
test acc: top1 ->  91.02 ; top5 ->  99.24  and loss:  49.81100517511368
forward train acc: top1 ->  99.58599997802735 ; top5 ->  99.998  and loss:  1.2241594078950584
test acc: top1 ->  91.27 ; top5 ->  99.18  and loss:  50.73491641879082
forward train acc: top1 ->  99.64799997558593 ; top5 ->  99.998  and loss:  1.0540912299184129
test acc: top1 ->  91.32 ; top5 ->  99.16  and loss:  52.4019393324852
forward train acc: top1 ->  99.72399997558594 ; top5 ->  100.0  and loss:  0.8567916166502982
test acc: top1 ->  91.26 ; top5 ->  99.17  and loss:  53.62242878973484
forward train acc: top1 ->  99.75 ; top5 ->  100.0  and loss:  0.8040023075882345
test acc: top1 ->  91.35 ; top5 ->  99.16  and loss:  53.301025941967964
forward train acc: top1 ->  99.766 ; top5 ->  100.0  and loss:  0.7260215124115348
test acc: top1 ->  91.41 ; top5 ->  99.14  and loss:  53.95166340470314
forward train acc: top1 ->  99.75399997558594 ; top5 ->  100.0  and loss:  0.7648933446034789
test acc: top1 ->  91.47 ; top5 ->  99.17  and loss:  54.56476466357708
forward train acc: top1 ->  99.8 ; top5 ->  99.998  and loss:  0.6275679146056063
test acc: top1 ->  91.48 ; top5 ->  99.14  and loss:  54.86186745762825
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  253 / 512 , inc:  15
---------------- start layer  8  ---------------
adv train loss:  -0.43458562024170533 , diff:  0.43458562024170533
adv train loss:  -0.4459884391690139 , diff:  0.011402818927308545
adv train loss:  -0.5268179263803177 , diff:  0.08082948721130379
adv train loss:  -0.4541081778879743 , diff:  0.07270974849234335
adv train loss:  -0.36247936182189733 , diff:  0.09162881606607698
adv train loss:  -0.42637329307035543 , diff:  0.0638939312484581
adv train loss:  -0.43129044368106406 , diff:  0.004917150610708632
layer  8  adv train finish, try to retain  256
test acc: top1 ->  18.95 ; top5 ->  62.86  and loss:  503.60377740859985
forward train acc: top1 ->  90.64800000488282 ; top5 ->  99.614  and loss:  35.81527279689908
test acc: top1 ->  89.61 ; top5 ->  98.84  and loss:  43.163153395056725
forward train acc: top1 ->  99.03200000488282 ; top5 ->  100.0  and loss:  4.2644112184643745
test acc: top1 ->  90.52 ; top5 ->  98.96  and loss:  42.52328749001026
forward train acc: top1 ->  99.43999997558593 ; top5 ->  100.0  and loss:  2.2627867376431823
test acc: top1 ->  91.01 ; top5 ->  99.05  and loss:  43.74700056016445
forward train acc: top1 ->  99.5160000024414 ; top5 ->  100.0  and loss:  1.746511133853346
test acc: top1 ->  91.5 ; top5 ->  99.04  and loss:  44.77504853159189
forward train acc: top1 ->  99.70599997558594 ; top5 ->  99.994  and loss:  1.1635457244701684
test acc: top1 ->  91.4 ; top5 ->  99.02  and loss:  46.31023174524307
forward train acc: top1 ->  99.762 ; top5 ->  100.0  and loss:  0.9230598011054099
test acc: top1 ->  91.57 ; top5 ->  99.04  and loss:  46.97366861999035
forward train acc: top1 ->  99.78199997558593 ; top5 ->  100.0  and loss:  0.8638893764000386
test acc: top1 ->  91.47 ; top5 ->  99.06  and loss:  48.08236088603735
forward train acc: top1 ->  99.79799997558594 ; top5 ->  100.0  and loss:  0.7784815593622625
test acc: top1 ->  91.5 ; top5 ->  99.07  and loss:  47.665128238499165
forward train acc: top1 ->  99.81999997558594 ; top5 ->  99.998  and loss:  0.6989446815568954
test acc: top1 ->  91.71 ; top5 ->  98.99  and loss:  48.955264039337635
forward train acc: top1 ->  99.79199997558594 ; top5 ->  100.0  and loss:  0.7041731650242582
test acc: top1 ->  91.48 ; top5 ->  99.08  and loss:  49.65030504018068
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  257 / 512 , inc:  16
---------------- start layer  9  ---------------
adv train loss:  -25.86356610059738 , diff:  25.86356610059738
adv train loss:  -26.147216267883778 , diff:  0.283650167286396
adv train loss:  -25.807520613074303 , diff:  0.33969565480947495
adv train loss:  -26.126591056585312 , diff:  0.3190704435110092
adv train loss:  -26.13155920803547 , diff:  0.0049681514501571655
layer  9  adv train finish, try to retain  450
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -1290.5114793777466 , diff:  1290.5114793777466
adv train loss:  -1291.4353952407837 , diff:  0.9239158630371094
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  16
test acc: top1 ->  14.24 ; top5 ->  58.56  and loss:  137169.5831298828
forward train acc: top1 ->  93.67999997802734 ; top5 ->  99.614  and loss:  31.754900965839624
test acc: top1 ->  91.06 ; top5 ->  98.8  and loss:  55.987924978137016
forward train acc: top1 ->  99.826 ; top5 ->  100.0  and loss:  1.0130326997023076
test acc: top1 ->  91.44 ; top5 ->  98.89  and loss:  55.61828030645847
forward train acc: top1 ->  99.906 ; top5 ->  100.0  and loss:  0.5288973737042397
test acc: top1 ->  91.6 ; top5 ->  98.88  and loss:  57.619789481163025
forward train acc: top1 ->  99.884 ; top5 ->  100.0  and loss:  0.47006961901206523
test acc: top1 ->  91.89 ; top5 ->  98.83  and loss:  59.03143745660782
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.3325700506102294
test acc: top1 ->  91.74 ; top5 ->  98.8  and loss:  59.324113100767136
forward train acc: top1 ->  99.94999997558594 ; top5 ->  100.0  and loss:  0.23902814387110993
test acc: top1 ->  91.83 ; top5 ->  98.83  and loss:  60.21494360268116
forward train acc: top1 ->  99.936 ; top5 ->  100.0  and loss:  0.2846843423321843
test acc: top1 ->  91.85 ; top5 ->  98.83  and loss:  60.675289273262024
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.20294664584798738
test acc: top1 ->  91.78 ; top5 ->  98.85  and loss:  60.8532392680645
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.17470608456642367
test acc: top1 ->  91.83 ; top5 ->  98.86  and loss:  61.20970506966114
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.18175197520758957
test acc: top1 ->  91.77 ; top5 ->  98.85  and loss:  61.231319174170494
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  18 / 512 , inc:  2
---------------- start layer  11  ---------------
adv train loss:  -45.85803136229515 , diff:  45.85803136229515
adv train loss:  -46.008285343647 , diff:  0.15025398135185242
adv train loss:  -45.967104345560074 , diff:  0.04118099808692932
layer  11  adv train finish, try to retain  406
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -200.44276881217957 , diff:  200.44276881217957
adv train loss:  -430.07897090911865 , diff:  229.6362020969391
adv train loss:  -722.3812346458435 , diff:  292.30226373672485
adv train loss:  -803.9017677307129 , diff:  81.52053308486938
adv train loss:  -974.8131046295166 , diff:  170.9113368988037
adv train loss:  -1099.0355310440063 , diff:  124.22242641448975
adv train loss:  -1102.5617790222168 , diff:  3.526247978210449
adv train loss:  -1102.2432594299316 , diff:  0.31851959228515625
adv train loss:  -1102.203914642334 , diff:  0.03934478759765625
layer  12  adv train finish, try to retain  461
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -7078.937732696533 , diff:  7078.937732696533
adv train loss:  -9777.78020477295 , diff:  2698.842472076416
adv train loss:  -10101.00089263916 , diff:  323.22068786621094
adv train loss:  -10135.042037963867 , diff:  34.04114532470703
adv train loss:  -10149.261138916016 , diff:  14.219100952148438
adv train loss:  -10155.873970031738 , diff:  6.612831115722656
adv train loss:  -10154.191268920898 , diff:  1.6827011108398438
layer  13  adv train finish, try to retain  56
test acc: top1 ->  57.16 ; top5 ->  88.6  and loss:  577.465943813324
forward train acc: top1 ->  95.27999997558594 ; top5 ->  99.252  and loss:  33.90568191674538
test acc: top1 ->  91.25 ; top5 ->  98.83  and loss:  81.832473680377
forward train acc: top1 ->  99.852 ; top5 ->  100.0  and loss:  0.480311159757548
test acc: top1 ->  91.54 ; top5 ->  98.84  and loss:  79.75422155857086
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.30804861737124156
test acc: top1 ->  91.71 ; top5 ->  98.93  and loss:  78.15192587673664
forward train acc: top1 ->  99.936 ; top5 ->  100.0  and loss:  0.2421624511916889
test acc: top1 ->  91.83 ; top5 ->  98.85  and loss:  78.6526792794466
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.225110612474964
test acc: top1 ->  91.95 ; top5 ->  98.89  and loss:  78.61104710400105
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.13751834216236603
test acc: top1 ->  92.0 ; top5 ->  98.88  and loss:  78.78959320485592
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.1487135697971098
test acc: top1 ->  91.96 ; top5 ->  98.84  and loss:  79.39647786319256
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.1036582281376468
test acc: top1 ->  91.95 ; top5 ->  98.91  and loss:  79.39987184107304
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.132116165215848
test acc: top1 ->  91.99 ; top5 ->  98.95  and loss:  79.01772373914719
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.09171712008537725
test acc: top1 ->  91.96 ; top5 ->  98.92  and loss:  78.87047179043293
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  63 / 512 , inc:  8
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  7
layer  8  :  0.501953125  ==>  257 / 512 , inc:  8
layer  9  :  0.126953125  ==>  65 / 512 , inc:  8
layer  10  :  0.03515625  ==>  18 / 512 , inc:  1
layer  11  :  0.189453125  ==>  97 / 512 , inc:  24
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.123046875  ==>  63 / 512 , inc:  4
eps [0.004805419921875001, 1.0935000000000001, 1.0935000000000001, 1.0935000000000001, 1.0935000000000001, 1.0935000000000001, 0.051257812500000006, 0.0024027099609375004, 0.006407226562500001, 0.045562500000000006, 1.0935000000000001, 0.045562500000000006, 2.1870000000000003, 1.4580000000000002]  wait [2, 3, 3, 3, 3, 3, 2, 4, 4, 2, 2, 0, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 7, 8, 8, 1, 24, 1, 4]  tol: 3
$$$$$$$$$$$$$ epoch  26  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -9.140364322811365 , diff:  9.140364322811365
adv train loss:  -9.233913663774729 , diff:  0.09354934096336365
adv train loss:  -9.135417787358165 , diff:  0.09849587641656399
adv train loss:  -9.136561192572117 , diff:  0.0011434052139520645
layer  0  adv train finish, try to retain  64
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
### skip layer  1 wait:  3  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
adv train loss:  -9.450956404209137 , diff:  9.450956404209137
adv train loss:  -9.187561627477407 , diff:  0.2633947767317295
adv train loss:  -9.180997531861067 , diff:  0.006564095616340637
layer  6  adv train finish, try to retain  256
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
### skip layer  8 wait:  4  ###
---------------- start layer  9  ---------------
adv train loss:  -13.699604481458664 , diff:  13.699604481458664
adv train loss:  -13.919652134180069 , diff:  0.22004765272140503
adv train loss:  -13.846747055649757 , diff:  0.07290507853031158
adv train loss:  -14.148716695606709 , diff:  0.30196963995695114
adv train loss:  -13.848192520439625 , diff:  0.30052417516708374
adv train loss:  -14.015664756298065 , diff:  0.1674722358584404
adv train loss:  -13.643886171281338 , diff:  0.37177858501672745
adv train loss:  -14.050065137445927 , diff:  0.40617896616458893
adv train loss:  -13.693791557103395 , diff:  0.3562735803425312
adv train loss:  -14.039264805614948 , diff:  0.3454732485115528
layer  9  adv train finish, try to retain  410
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -3047.0262184143066 , diff:  3047.0262184143066
adv train loss:  -3046.751474380493 , diff:  0.27474403381347656
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  17
test acc: top1 ->  24.97 ; top5 ->  67.34  and loss:  100525.4848022461
forward train acc: top1 ->  99.936 ; top5 ->  100.0  and loss:  0.23577048323932104
test acc: top1 ->  92.11 ; top5 ->  99.02  and loss:  68.29126147925854
==> this epoch:  17 / 512
---------------- start layer  11  ---------------
adv train loss:  -1.184989386529196 , diff:  1.184989386529196
adv train loss:  -1.2389670115953777 , diff:  0.053977625066181645
adv train loss:  -1.031949890195392 , diff:  0.20701712139998563
adv train loss:  -1.2004917662707157 , diff:  0.16854187607532367
adv train loss:  -1.0590516600350384 , diff:  0.14144010623567738
adv train loss:  -1.2181815923540853 , diff:  0.15912993231904693
adv train loss:  -1.116223396093119 , diff:  0.10195819626096636
adv train loss:  -1.1238460808526725 , diff:  0.007622684759553522
layer  11  adv train finish, try to retain  410
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -814.0145435333252 , diff:  814.0145435333252
adv train loss:  -1154.1854448318481 , diff:  340.17090129852295
adv train loss:  -1303.6924858093262 , diff:  149.50704097747803
adv train loss:  -1322.8307723999023 , diff:  19.138286590576172
adv train loss:  -1823.1504955291748 , diff:  500.31972312927246
adv train loss:  -2484.375909805298 , diff:  661.225414276123
adv train loss:  -2483.5851650238037 , diff:  0.7907447814941406
adv train loss:  -2769.282028198242 , diff:  285.6968631744385
adv train loss:  -2770.2213916778564 , diff:  0.9393634796142578
adv train loss:  -2770.492950439453 , diff:  0.2715587615966797
layer  12  adv train finish, try to retain  3
test acc: top1 ->  19.05 ; top5 ->  57.74  and loss:  1116.7601594924927
forward train acc: top1 ->  59.41400000976562 ; top5 ->  83.54  and loss:  235.1844639480114
test acc: top1 ->  68.96 ; top5 ->  97.63  and loss:  81.77101927995682
forward train acc: top1 ->  96.56400000488281 ; top5 ->  99.994  and loss:  22.045122161507607
test acc: top1 ->  88.41 ; top5 ->  97.4  and loss:  55.50794863700867
forward train acc: top1 ->  99.27999997558594 ; top5 ->  99.998  and loss:  10.765990898013115
test acc: top1 ->  89.22 ; top5 ->  97.31  and loss:  53.39262184500694
forward train acc: top1 ->  99.47399997558594 ; top5 ->  99.994  and loss:  7.468197498470545
test acc: top1 ->  89.56 ; top5 ->  97.32  and loss:  52.55863431096077
forward train acc: top1 ->  99.6560000024414 ; top5 ->  99.998  and loss:  5.463526211678982
test acc: top1 ->  89.85 ; top5 ->  97.37  and loss:  52.59058880805969
forward train acc: top1 ->  99.69199997802734 ; top5 ->  99.998  and loss:  4.514397792518139
test acc: top1 ->  89.95 ; top5 ->  97.38  and loss:  52.98660162091255
forward train acc: top1 ->  99.72399997558594 ; top5 ->  100.0  and loss:  3.9742700681090355
test acc: top1 ->  90.05 ; top5 ->  97.42  and loss:  53.228544399142265
forward train acc: top1 ->  99.744 ; top5 ->  99.996  and loss:  3.519792376086116
test acc: top1 ->  90.17 ; top5 ->  97.39  and loss:  53.239193722605705
forward train acc: top1 ->  99.76599997558594 ; top5 ->  99.998  and loss:  3.133704638108611
test acc: top1 ->  90.07 ; top5 ->  97.35  and loss:  53.51956833899021
forward train acc: top1 ->  99.794 ; top5 ->  99.998  and loss:  2.816475983709097
test acc: top1 ->  90.3 ; top5 ->  97.36  and loss:  53.939620926976204
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  62 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -6838.395774841309 , diff:  6838.395774841309
adv train loss:  -7842.364166259766 , diff:  1003.968391418457
adv train loss:  -7961.625076293945 , diff:  119.26091003417969
adv train loss:  -7977.543113708496 , diff:  15.918037414550781
adv train loss:  -7978.075698852539 , diff:  0.5325851440429688
layer  13  adv train finish, try to retain  56
test acc: top1 ->  59.84 ; top5 ->  97.91  and loss:  298.1184630393982
forward train acc: top1 ->  98.1060000024414 ; top5 ->  100.0  and loss:  6.785426983726211
test acc: top1 ->  91.75 ; top5 ->  98.99  and loss:  91.06735880672932
forward train acc: top1 ->  99.94399997558594 ; top5 ->  100.0  and loss:  0.20407823103232658
test acc: top1 ->  92.11 ; top5 ->  98.99  and loss:  87.91438837349415
==> this epoch:  56 / 512
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  7
layer  8  :  0.501953125  ==>  257 / 512 , inc:  8
layer  9  :  0.126953125  ==>  65 / 512 , inc:  8
layer  10  :  0.033203125  ==>  17 / 512 , inc:  2
layer  11  :  0.189453125  ==>  97 / 512 , inc:  24
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.109375  ==>  56 / 512 , inc:  8
eps [0.009610839843750002, 1.0935000000000001, 1.0935000000000001, 1.0935000000000001, 1.0935000000000001, 1.0935000000000001, 0.10251562500000001, 0.0024027099609375004, 0.006407226562500001, 0.09112500000000001, 1.0935000000000001, 0.09112500000000001, 1.6402500000000002, 1.4580000000000002]  wait [2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 0, 0, 4, 0]  inc [1, 1, 1, 1, 1, 1, 1, 7, 8, 8, 2, 24, 1, 8]  tol: 3
$$$$$$$$$$$$$ epoch  27  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -0.12071596239184146 , diff:  0.12071596239184146
adv train loss:  -0.15128281143552158 , diff:  0.030566849043680122
adv train loss:  -0.149254360665509 , diff:  0.0020284507700125687
layer  0  adv train finish, try to retain  64
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -21.957282585892244 , diff:  21.957282585892244
adv train loss:  -33.86624966561794 , diff:  11.908967079725699
adv train loss:  -34.11177469789982 , diff:  0.2455250322818756
adv train loss:  -38.15005286037922 , diff:  4.038278162479401
adv train loss:  -38.7979384213686 , diff:  0.6478855609893799
adv train loss:  -40.03697443008423 , diff:  1.2390360087156296
adv train loss:  -40.269357204437256 , diff:  0.23238277435302734
adv train loss:  -39.293856620788574 , diff:  0.9755005836486816
adv train loss:  -39.23483869433403 , diff:  0.05901792645454407
adv train loss:  -40.299340546131134 , diff:  1.0645018517971039
layer  1  adv train finish, try to retain  63
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -1901.4783009438834 , diff:  1901.4783009438834
adv train loss:  -2970.6057109832764 , diff:  1069.127410039393
adv train loss:  -3017.797035217285 , diff:  47.19132423400879
adv train loss:  -3020.5462436676025 , diff:  2.749208450317383
adv train loss:  -3028.739740371704 , diff:  8.193496704101562
adv train loss:  -3031.2505588531494 , diff:  2.5108184814453125
adv train loss:  -3035.4790477752686 , diff:  4.228488922119141
adv train loss:  -3017.196741104126 , diff:  18.282306671142578
adv train loss:  -3027.8093795776367 , diff:  10.612638473510742
adv train loss:  -3007.416778564453 , diff:  20.392601013183594
layer  2  adv train finish, try to retain  114
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -1403.171725216147 , diff:  1403.171725216147
adv train loss:  -2729.160207748413 , diff:  1325.988482532266
adv train loss:  -2767.093963623047 , diff:  37.93375587463379
adv train loss:  -2842.9429264068604 , diff:  75.84896278381348
adv train loss:  -2891.700033187866 , diff:  48.75710678100586
adv train loss:  -2893.2004985809326 , diff:  1.5004653930664062
adv train loss:  -2874.8118114471436 , diff:  18.388687133789062
adv train loss:  -2922.176420211792 , diff:  47.36460876464844
adv train loss:  -2956.05867767334 , diff:  33.88225746154785
adv train loss:  -3015.4364070892334 , diff:  59.377729415893555
layer  3  adv train finish, try to retain  120
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -680.3249794866206 , diff:  680.3249794866206
adv train loss:  -3344.367244720459 , diff:  2664.0422652338384
adv train loss:  -3582.6652870178223 , diff:  238.29804229736328
adv train loss:  -3693.065719604492 , diff:  110.40043258666992
adv train loss:  -3676.6557579040527 , diff:  16.409961700439453
adv train loss:  -3678.9231719970703 , diff:  2.267414093017578
adv train loss:  -3701.0447731018066 , diff:  22.121601104736328
adv train loss:  -3705.267276763916 , diff:  4.222503662109375
adv train loss:  -3728.307285308838 , diff:  23.040008544921875
adv train loss:  -3721.41495513916 , diff:  6.892330169677734
layer  4  adv train finish, try to retain  220
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -1311.7424201529357 , diff:  1311.7424201529357
adv train loss:  -4168.387298583984 , diff:  2856.6448784310487
adv train loss:  -4229.516574859619 , diff:  61.129276275634766
adv train loss:  -4277.144432067871 , diff:  47.62785720825195
adv train loss:  -4299.341667175293 , diff:  22.197235107421875
adv train loss:  -4318.300979614258 , diff:  18.959312438964844
adv train loss:  -4358.476173400879 , diff:  40.175193786621094
adv train loss:  -4358.220973968506 , diff:  0.2551994323730469
adv train loss:  -4368.489627838135 , diff:  10.268653869628906
adv train loss:  -4370.544399261475 , diff:  2.0547714233398438
layer  5  adv train finish, try to retain  224
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -0.09473818194965133 , diff:  0.09473818194965133
adv train loss:  -0.14507557475735666 , diff:  0.05033739280770533
adv train loss:  -0.09860386604850646 , diff:  0.0464717087088502
adv train loss:  -0.19838114202138968 , diff:  0.09977727597288322
adv train loss:  -0.1395028616352647 , diff:  0.05887828038612497
adv train loss:  -0.1334834497902193 , diff:  0.006019411845045397
layer  6  adv train finish, try to retain  256
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
### skip layer  8 wait:  3  ###
---------------- start layer  9  ---------------
adv train loss:  -0.246031890015729 , diff:  0.246031890015729
adv train loss:  -0.16605737927602604 , diff:  0.07997451073970296
adv train loss:  -0.25865664306184044 , diff:  0.0925992637858144
adv train loss:  -0.17947323582484387 , diff:  0.07918340723699657
adv train loss:  -0.24629928452486638 , diff:  0.06682604870002251
adv train loss:  -0.17054720554006053 , diff:  0.07575207898480585
adv train loss:  -0.23423594275664072 , diff:  0.0636887372165802
adv train loss:  -0.241587497486762 , diff:  0.007351554730121279
layer  9  adv train finish, try to retain  406
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -392.2028114795685 , diff:  392.2028114795685
adv train loss:  -391.85071206092834 , diff:  0.3520994186401367
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  15
test acc: top1 ->  24.86 ; top5 ->  59.4  and loss:  80226.6312866211
forward train acc: top1 ->  92.94000000244141 ; top5 ->  99.706  and loss:  59.74812775105238
test acc: top1 ->  90.85 ; top5 ->  98.52  and loss:  76.8429389744997
forward train acc: top1 ->  99.802 ; top5 ->  99.998  and loss:  0.7815354720223695
test acc: top1 ->  91.5 ; top5 ->  98.48  and loss:  72.87817262113094
forward train acc: top1 ->  99.88999997558594 ; top5 ->  100.0  and loss:  0.43863060005242005
test acc: top1 ->  91.6 ; top5 ->  98.54  and loss:  72.66355581581593
forward train acc: top1 ->  99.928 ; top5 ->  100.0  and loss:  0.300605938595254
test acc: top1 ->  91.78 ; top5 ->  98.6  and loss:  71.16598030924797
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.21790191167383455
test acc: top1 ->  91.75 ; top5 ->  98.62  and loss:  71.10550422221422
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.1480281029944308
test acc: top1 ->  91.74 ; top5 ->  98.61  and loss:  71.68717312812805
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.21322482617688365
test acc: top1 ->  91.8 ; top5 ->  98.61  and loss:  71.15233349055052
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.17839246877701953
test acc: top1 ->  91.78 ; top5 ->  98.69  and loss:  70.9384691119194
forward train acc: top1 ->  99.966 ; top5 ->  99.998  and loss:  0.16855869241408072
test acc: top1 ->  91.89 ; top5 ->  98.63  and loss:  71.08261680603027
forward train acc: top1 ->  99.94999997558594 ; top5 ->  100.0  and loss:  0.19910017316578887
test acc: top1 ->  91.84 ; top5 ->  98.69  and loss:  70.32763059437275
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  17 / 512 , inc:  2
---------------- start layer  11  ---------------
adv train loss:  -2.552540542674251 , diff:  2.552540542674251
adv train loss:  -2.5555262019624934 , diff:  0.0029856592882424593
layer  11  adv train finish, try to retain  402
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
adv train loss:  -7646.060859680176 , diff:  7646.060859680176
adv train loss:  -8577.433135986328 , diff:  931.3722763061523
adv train loss:  -8620.226089477539 , diff:  42.79295349121094
adv train loss:  -8631.881240844727 , diff:  11.6551513671875
adv train loss:  -8635.307739257812 , diff:  3.4264984130859375
adv train loss:  -8636.20620727539 , diff:  0.898468017578125
layer  13  adv train finish, try to retain  61
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  7
layer  8  :  0.501953125  ==>  257 / 512 , inc:  8
layer  9  :  0.126953125  ==>  65 / 512 , inc:  8
layer  10  :  0.033203125  ==>  17 / 512 , inc:  1
layer  11  :  0.189453125  ==>  97 / 512 , inc:  24
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.109375  ==>  56 / 512 , inc:  8
eps [0.019221679687500003, 2.1870000000000003, 2.1870000000000003, 2.1870000000000003, 2.1870000000000003, 2.1870000000000003, 0.20503125000000003, 0.0024027099609375004, 0.006407226562500001, 0.18225000000000002, 0.8201250000000001, 0.18225000000000002, 1.6402500000000002, 2.9160000000000004]  wait [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 3, 0]  inc [1, 1, 1, 1, 1, 1, 1, 7, 8, 8, 1, 24, 1, 8]  tol: 3
$$$$$$$$$$$$$ epoch  28  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -3.4773097027791664 , diff:  3.4773097027791664
adv train loss:  -3.6519212236162275 , diff:  0.1746115208370611
adv train loss:  -3.2692677655722946 , diff:  0.3826534580439329
adv train loss:  -2.870165364816785 , diff:  0.39910240075550973
adv train loss:  -3.4728810768574476 , diff:  0.6027157120406628
adv train loss:  -3.2939517514314502 , diff:  0.17892932542599738
adv train loss:  -3.215409526368603 , diff:  0.07854222506284714
adv train loss:  -3.2269532319623977 , diff:  0.011543705593794584
adv train loss:  -3.453492079861462 , diff:  0.22653884789906442
adv train loss:  -3.4840073375962675 , diff:  0.030515257734805346
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  31
test acc: top1 ->  46.17 ; top5 ->  85.46  and loss:  708.4308381080627
forward train acc: top1 ->  97.62000000976562 ; top5 ->  99.898  and loss:  12.93426913022995
test acc: top1 ->  82.09 ; top5 ->  97.77  and loss:  160.02732092142105
forward train acc: top1 ->  98.20399997802734 ; top5 ->  99.956  and loss:  6.759319586679339
test acc: top1 ->  89.89 ; top5 ->  99.22  and loss:  62.664680764079094
forward train acc: top1 ->  98.43200000732422 ; top5 ->  99.95799997558593  and loss:  5.472752196714282
test acc: top1 ->  90.09 ; top5 ->  99.25  and loss:  56.72527492046356
forward train acc: top1 ->  98.6380000024414 ; top5 ->  99.97  and loss:  4.52121296338737
test acc: top1 ->  90.19 ; top5 ->  99.24  and loss:  53.24296052753925
forward train acc: top1 ->  98.7200000048828 ; top5 ->  99.974  and loss:  4.117083496414125
test acc: top1 ->  90.24 ; top5 ->  99.19  and loss:  53.27790714800358
forward train acc: top1 ->  98.89200000488282 ; top5 ->  99.984  and loss:  3.382498468272388
test acc: top1 ->  90.35 ; top5 ->  99.25  and loss:  52.16626510024071
forward train acc: top1 ->  98.88199997802734 ; top5 ->  99.986  and loss:  3.171310367062688
test acc: top1 ->  90.34 ; top5 ->  99.25  and loss:  52.08397553861141
forward train acc: top1 ->  98.98399997558593 ; top5 ->  99.988  and loss:  3.1657348978333175
test acc: top1 ->  90.37 ; top5 ->  99.28  and loss:  51.90937899053097
forward train acc: top1 ->  99.02600000732421 ; top5 ->  99.98  and loss:  3.0333405630663037
test acc: top1 ->  90.4 ; top5 ->  99.27  and loss:  51.15578056871891
forward train acc: top1 ->  99.1140000024414 ; top5 ->  99.996  and loss:  2.5919822342693806
test acc: top1 ->  90.46 ; top5 ->  99.26  and loss:  51.33103759586811
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -6.750347024877556 , diff:  6.750347024877556
adv train loss:  -9.112477220594883 , diff:  2.3621301957173273
adv train loss:  -8.756792632862926 , diff:  0.35568458773195744
adv train loss:  -7.515685191377997 , diff:  1.2411074414849281
adv train loss:  -8.213840836659074 , diff:  0.6981556452810764
adv train loss:  -11.040023911744356 , diff:  2.8261830750852823
adv train loss:  -11.35528651624918 , diff:  0.3152626045048237
adv train loss:  -10.954337675124407 , diff:  0.400948841124773
adv train loss:  -11.395330391824245 , diff:  0.44099271669983864
adv train loss:  -11.199647277593613 , diff:  0.19568311423063278
layer  1  adv train finish, try to retain  42
test acc: top1 ->  10.4 ; top5 ->  53.5  and loss:  794.7118844985962
forward train acc: top1 ->  98.79400000244141 ; top5 ->  99.992  and loss:  3.7975656567141414
test acc: top1 ->  90.77 ; top5 ->  99.18  and loss:  48.574898689985275
forward train acc: top1 ->  99.30799997558594 ; top5 ->  99.992  and loss:  2.143534040544182
test acc: top1 ->  91.06 ; top5 ->  99.22  and loss:  47.97830222547054
forward train acc: top1 ->  99.44199998046875 ; top5 ->  99.996  and loss:  1.6787998271174729
test acc: top1 ->  91.08 ; top5 ->  99.24  and loss:  48.02582658827305
forward train acc: top1 ->  99.58 ; top5 ->  99.996  and loss:  1.1733496038941666
test acc: top1 ->  91.0 ; top5 ->  99.19  and loss:  50.81093370914459
forward train acc: top1 ->  99.67399997802734 ; top5 ->  100.0  and loss:  0.9839471117593348
test acc: top1 ->  91.18 ; top5 ->  99.26  and loss:  53.24945330619812
forward train acc: top1 ->  99.66999997558594 ; top5 ->  99.994  and loss:  0.8863009609631263
test acc: top1 ->  91.2 ; top5 ->  99.25  and loss:  53.18072175979614
forward train acc: top1 ->  99.722 ; top5 ->  100.0  and loss:  0.8777697727782652
test acc: top1 ->  91.22 ; top5 ->  99.28  and loss:  52.89243909716606
forward train acc: top1 ->  99.72 ; top5 ->  100.0  and loss:  0.8652828330523334
test acc: top1 ->  91.28 ; top5 ->  99.24  and loss:  52.74106162041426
forward train acc: top1 ->  99.74400000244141 ; top5 ->  100.0  and loss:  0.7457666132249869
test acc: top1 ->  91.29 ; top5 ->  99.19  and loss:  52.36748483777046
forward train acc: top1 ->  99.76999997558593 ; top5 ->  99.998  and loss:  0.7521344373817556
test acc: top1 ->  91.4 ; top5 ->  99.25  and loss:  52.20792402327061
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  55 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -1119.4681915076508 , diff:  1119.4681915076508
adv train loss:  -1686.541386604309 , diff:  567.0731950966583
adv train loss:  -1695.7617301940918 , diff:  9.220343589782715
adv train loss:  -1699.8292818069458 , diff:  4.067551612854004
adv train loss:  -1706.312536239624 , diff:  6.483254432678223
adv train loss:  -1707.2194623947144 , diff:  0.906926155090332
adv train loss:  -1697.4544429779053 , diff:  9.765019416809082
adv train loss:  -1699.716423034668 , diff:  2.2619800567626953
adv train loss:  -1698.4570264816284 , diff:  1.2593965530395508
adv train loss:  -1698.937747001648 , diff:  0.48072052001953125
layer  2  adv train finish, try to retain  32
test acc: top1 ->  10.0 ; top5 ->  51.17  and loss:  1345.5673742294312
forward train acc: top1 ->  81.17799999511719 ; top5 ->  97.96599997802734  and loss:  75.55364283919334
test acc: top1 ->  78.57 ; top5 ->  97.39  and loss:  74.51149314641953
forward train acc: top1 ->  84.03600000732422 ; top5 ->  98.66599997802734  and loss:  49.21309033036232
test acc: top1 ->  80.28 ; top5 ->  97.69  and loss:  66.11055991053581
forward train acc: top1 ->  85.942 ; top5 ->  98.91399997802735  and loss:  43.07112458348274
test acc: top1 ->  81.75 ; top5 ->  98.03  and loss:  61.6338204741478
forward train acc: top1 ->  87.43799997314453 ; top5 ->  99.1420000024414  and loss:  38.114151269197464
test acc: top1 ->  82.54 ; top5 ->  98.33  and loss:  58.90120401978493
forward train acc: top1 ->  88.43399997314454 ; top5 ->  99.23599997558594  and loss:  34.898764818906784
test acc: top1 ->  83.32 ; top5 ->  98.39  and loss:  56.090931206941605
forward train acc: top1 ->  89.1180000024414 ; top5 ->  99.36999997802734  and loss:  32.41900183260441
test acc: top1 ->  83.67 ; top5 ->  98.38  and loss:  55.56306937336922
forward train acc: top1 ->  89.692 ; top5 ->  99.3740000024414  and loss:  30.800161987543106
test acc: top1 ->  83.89 ; top5 ->  98.51  and loss:  54.60700178146362
forward train acc: top1 ->  90.07400001708984 ; top5 ->  99.41599997558593  and loss:  29.697511047124863
test acc: top1 ->  84.23 ; top5 ->  98.58  and loss:  53.36801481246948
forward train acc: top1 ->  90.31799998779297 ; top5 ->  99.468  and loss:  28.640239372849464
test acc: top1 ->  84.52 ; top5 ->  98.6  and loss:  53.37586286664009
forward train acc: top1 ->  90.68599998046875 ; top5 ->  99.53800000244141  and loss:  27.735975608229637
test acc: top1 ->  84.65 ; top5 ->  98.6  and loss:  52.780169516801834
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -554.2275698464364 , diff:  554.2275698464364
adv train loss:  -923.9636707305908 , diff:  369.73610088415444
adv train loss:  -918.74827003479 , diff:  5.215400695800781
adv train loss:  -949.3449783325195 , diff:  30.596708297729492
adv train loss:  -956.4494400024414 , diff:  7.104461669921875
adv train loss:  -955.5891580581665 , diff:  0.8602819442749023
adv train loss:  -957.4604845046997 , diff:  1.8713264465332031
adv train loss:  -954.9659204483032 , diff:  2.4945640563964844
adv train loss:  -950.5315761566162 , diff:  4.434344291687012
adv train loss:  -953.545163154602 , diff:  3.01358699798584
layer  3  adv train finish, try to retain  19
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  565.6419744491577
forward train acc: top1 ->  66.21600000732421 ; top5 ->  93.61599997314453  and loss:  107.87935811281204
test acc: top1 ->  68.04 ; top5 ->  95.12  and loss:  101.13123190402985
forward train acc: top1 ->  71.17399997802734 ; top5 ->  95.91600001708984  and loss:  87.07391893863678
test acc: top1 ->  70.87 ; top5 ->  96.22  and loss:  90.61448842287064
forward train acc: top1 ->  74.03400000732422 ; top5 ->  96.67399999023438  and loss:  78.05836749076843
test acc: top1 ->  72.87 ; top5 ->  96.76  and loss:  85.09626400470734
forward train acc: top1 ->  75.86200001220703 ; top5 ->  97.42200000976563  and loss:  71.5592143535614
test acc: top1 ->  74.61 ; top5 ->  97.13  and loss:  79.61736816167831
forward train acc: top1 ->  77.56799998291015 ; top5 ->  97.70399998291016  and loss:  67.01845961809158
test acc: top1 ->  75.64 ; top5 ->  97.27  and loss:  77.54797005653381
forward train acc: top1 ->  78.20200000244141 ; top5 ->  97.90399998291015  and loss:  64.61314862966537
test acc: top1 ->  76.23 ; top5 ->  97.45  and loss:  74.85207289457321
forward train acc: top1 ->  78.76000001708984 ; top5 ->  97.97599998291015  and loss:  63.34340238571167
test acc: top1 ->  76.78 ; top5 ->  97.43  and loss:  73.32554683089256
forward train acc: top1 ->  79.332 ; top5 ->  98.14999998046875  and loss:  60.74839228391647
test acc: top1 ->  77.14 ; top5 ->  97.57  and loss:  72.47645080089569
forward train acc: top1 ->  79.45200000732422 ; top5 ->  98.08999998046875  and loss:  60.710117876529694
test acc: top1 ->  77.39 ; top5 ->  97.56  and loss:  71.3177455663681
forward train acc: top1 ->  80.10999998535156 ; top5 ->  98.22400000732422  and loss:  59.06483060121536
test acc: top1 ->  77.72 ; top5 ->  97.71  and loss:  70.43734937906265
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -126.84253195673227 , diff:  126.84253195673227
adv train loss:  -639.7873306274414 , diff:  512.9447986707091
adv train loss:  -784.8478851318359 , diff:  145.06055450439453
adv train loss:  -812.2430863380432 , diff:  27.395201206207275
adv train loss:  -819.5103068351746 , diff:  7.267220497131348
adv train loss:  -841.2032823562622 , diff:  21.692975521087646
adv train loss:  -869.0280075073242 , diff:  27.82472515106201
adv train loss:  -883.827956199646 , diff:  14.799948692321777
adv train loss:  -881.4502782821655 , diff:  2.3776779174804688
adv train loss:  -882.5664758682251 , diff:  1.1161975860595703
layer  4  adv train finish, try to retain  18
test acc: top1 ->  12.64 ; top5 ->  52.05  and loss:  377.06218695640564
forward train acc: top1 ->  63.32599999267578 ; top5 ->  95.01399999267578  and loss:  104.73599338531494
test acc: top1 ->  65.59 ; top5 ->  96.06  and loss:  101.04871761798859
forward train acc: top1 ->  69.30600001464843 ; top5 ->  96.90000000976562  and loss:  86.90112435817719
test acc: top1 ->  69.31 ; top5 ->  96.72  and loss:  90.49845415353775
forward train acc: top1 ->  72.42399997070312 ; top5 ->  97.49599998046875  and loss:  78.24367713928223
test acc: top1 ->  71.77 ; top5 ->  97.16  and loss:  83.8899177312851
forward train acc: top1 ->  74.50000001953126 ; top5 ->  97.77999998046874  and loss:  72.63307476043701
test acc: top1 ->  73.36 ; top5 ->  97.33  and loss:  79.89092111587524
forward train acc: top1 ->  76.06399997802734 ; top5 ->  98.15799997802735  and loss:  68.17295092344284
test acc: top1 ->  74.44 ; top5 ->  97.56  and loss:  76.36818641424179
forward train acc: top1 ->  76.91800001464844 ; top5 ->  98.12400000732421  and loss:  65.93083500862122
test acc: top1 ->  74.95 ; top5 ->  97.71  and loss:  74.67228752374649
forward train acc: top1 ->  77.48999998535156 ; top5 ->  98.34400000488282  and loss:  64.24604886770248
test acc: top1 ->  75.5 ; top5 ->  97.7  and loss:  73.14687207341194
forward train acc: top1 ->  77.97800001708984 ; top5 ->  98.36999997802734  and loss:  62.75395184755325
test acc: top1 ->  75.81 ; top5 ->  97.84  and loss:  72.29594802856445
forward train acc: top1 ->  78.58800001220703 ; top5 ->  98.46399998046876  and loss:  61.623063921928406
test acc: top1 ->  76.16 ; top5 ->  97.94  and loss:  71.37411934137344
forward train acc: top1 ->  78.62600001220703 ; top5 ->  98.4820000048828  and loss:  61.22570437192917
test acc: top1 ->  76.62 ; top5 ->  97.98  and loss:  69.69817742705345
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -150.43864752352238 , diff:  150.43864752352238
adv train loss:  -831.2793869972229 , diff:  680.8407394737005
adv train loss:  -994.5036211013794 , diff:  163.2242341041565
adv train loss:  -1036.0128707885742 , diff:  41.509249687194824
adv train loss:  -1054.336814880371 , diff:  18.323944091796875
adv train loss:  -1055.9817819595337 , diff:  1.6449670791625977
adv train loss:  -1072.3953485488892 , diff:  16.41356658935547
adv train loss:  -1076.7045822143555 , diff:  4.309233665466309
adv train loss:  -1080.504388809204 , diff:  3.799806594848633
adv train loss:  -1075.8166408538818 , diff:  4.687747955322266
layer  5  adv train finish, try to retain  13
test acc: top1 ->  10.02 ; top5 ->  50.01  and loss:  506.21072149276733
forward train acc: top1 ->  61.36599998046875 ; top5 ->  95.41800000976562  and loss:  109.52141630649567
test acc: top1 ->  64.0 ; top5 ->  96.18  and loss:  104.25415045022964
forward train acc: top1 ->  68.80999998535157 ; top5 ->  97.29200001220703  and loss:  86.63144928216934
test acc: top1 ->  69.0 ; top5 ->  97.27  and loss:  90.92073494195938
forward train acc: top1 ->  72.45400000732423 ; top5 ->  97.95599998046875  and loss:  76.78874456882477
test acc: top1 ->  71.54 ; top5 ->  97.55  and loss:  84.53482592105865
forward train acc: top1 ->  74.89600001953124 ; top5 ->  98.32000000488281  and loss:  70.13227671384811
test acc: top1 ->  73.46 ; top5 ->  97.74  and loss:  79.13761115074158
forward train acc: top1 ->  76.77799999267579 ; top5 ->  98.58599998291015  and loss:  65.07743012905121
test acc: top1 ->  75.11 ; top5 ->  97.98  and loss:  74.75801715254784
forward train acc: top1 ->  78.15799999755859 ; top5 ->  98.70399998535156  and loss:  61.920196294784546
test acc: top1 ->  75.75 ; top5 ->  98.05  and loss:  73.3445753455162
forward train acc: top1 ->  78.64799998535156 ; top5 ->  98.78999997802734  and loss:  60.35873958468437
test acc: top1 ->  76.37 ; top5 ->  98.14  and loss:  71.37870129942894
forward train acc: top1 ->  79.26799997070313 ; top5 ->  98.86000000244141  and loss:  58.58853965997696
test acc: top1 ->  76.9 ; top5 ->  98.22  and loss:  69.96963346004486
forward train acc: top1 ->  80.12999998046875 ; top5 ->  98.9900000024414  and loss:  56.34573021531105
test acc: top1 ->  77.18 ; top5 ->  98.26  and loss:  68.94716110825539
forward train acc: top1 ->  80.21000001708984 ; top5 ->  98.90999997558593  and loss:  55.518241226673126
test acc: top1 ->  77.84 ; top5 ->  98.3  and loss:  67.9005837738514
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -10.960588730871677 , diff:  10.960588730871677
adv train loss:  -10.839343816041946 , diff:  0.12124491482973099
adv train loss:  -10.919342912733555 , diff:  0.07999909669160843
adv train loss:  -10.8570371940732 , diff:  0.062305718660354614
adv train loss:  -10.9536197707057 , diff:  0.0965825766324997
adv train loss:  -10.977643713355064 , diff:  0.02402394264936447
adv train loss:  -10.84662875533104 , diff:  0.13101495802402496
adv train loss:  -10.826830938458443 , diff:  0.01979781687259674
layer  6  adv train finish, try to retain  256
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -9.647173039615154 , diff:  9.647173039615154
adv train loss:  -9.659312471747398 , diff:  0.01213943213224411
layer  7  adv train finish, try to retain  254
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -10.670515485107899 , diff:  10.670515485107899
adv train loss:  -10.754130326211452 , diff:  0.08361484110355377
adv train loss:  -10.86518307030201 , diff:  0.1110527440905571
adv train loss:  -10.74372560530901 , diff:  0.12145746499300003
adv train loss:  -10.647650212049484 , diff:  0.0960753932595253
adv train loss:  -10.699418969452381 , diff:  0.05176875740289688
adv train loss:  -10.69051418453455 , diff:  0.008904784917831421
layer  8  adv train finish, try to retain  290
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -9.48623651266098 , diff:  9.48623651266098
adv train loss:  -9.752057150006294 , diff:  0.265820637345314
adv train loss:  -9.548405908048153 , diff:  0.20365124195814133
adv train loss:  -9.747028775513172 , diff:  0.19862286746501923
adv train loss:  -9.599337927997112 , diff:  0.14769084751605988
adv train loss:  -9.716515518724918 , diff:  0.11717759072780609
adv train loss:  -9.478479824960232 , diff:  0.23803569376468658
adv train loss:  -9.589732833206654 , diff:  0.11125300824642181
adv train loss:  -9.66235163807869 , diff:  0.07261880487203598
adv train loss:  -9.74355087429285 , diff:  0.08119923621416092
layer  9  adv train finish, try to retain  427
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -8.76064932718873 , diff:  8.76064932718873
adv train loss:  -8.702185120433569 , diff:  0.058464206755161285
adv train loss:  -8.943517994135618 , diff:  0.24133287370204926
adv train loss:  -8.67738251760602 , diff:  0.26613547652959824
adv train loss:  -8.884586118161678 , diff:  0.20720360055565834
adv train loss:  -8.752188686281443 , diff:  0.13239743188023567
adv train loss:  -9.006904479116201 , diff:  0.25471579283475876
adv train loss:  -8.704569138586521 , diff:  0.30233534052968025
adv train loss:  -8.842698346823454 , diff:  0.13812920823693275
adv train loss:  -8.68303506821394 , diff:  0.15966327860951424
layer  10  adv train finish, try to retain  493
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -22.353147596120834 , diff:  22.353147596120834
adv train loss:  -22.366068080067635 , diff:  0.012920483946800232
layer  11  adv train finish, try to retain  410
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
### skip layer  12 wait:  3  ###
---------------- start layer  13  ---------------
adv train loss:  -7711.848148345947 , diff:  7711.848148345947
adv train loss:  -12652.623840332031 , diff:  4940.775691986084
adv train loss:  -14284.854263305664 , diff:  1632.2304229736328
adv train loss:  -14524.289443969727 , diff:  239.4351806640625
adv train loss:  -14603.727966308594 , diff:  79.43852233886719
adv train loss:  -14633.280029296875 , diff:  29.55206298828125
adv train loss:  -14645.333724975586 , diff:  12.053695678710938
adv train loss:  -14653.089401245117 , diff:  7.75567626953125
adv train loss:  -14655.189727783203 , diff:  2.1003265380859375
layer  13  adv train finish, try to retain  61
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  7
layer  8  :  0.501953125  ==>  257 / 512 , inc:  8
layer  9  :  0.126953125  ==>  65 / 512 , inc:  8
layer  10  :  0.033203125  ==>  17 / 512 , inc:  1
layer  11  :  0.189453125  ==>  97 / 512 , inc:  24
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.109375  ==>  56 / 512 , inc:  8
eps [0.014416259765625002, 1.6402500000000002, 1.6402500000000002, 1.6402500000000002, 1.6402500000000002, 1.6402500000000002, 0.41006250000000005, 0.004805419921875001, 0.012814453125000002, 0.36450000000000005, 1.6402500000000002, 0.36450000000000005, 1.6402500000000002, 5.832000000000001]  wait [4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 0, 2, 0]  inc [1, 1, 1, 1, 1, 1, 1, 7, 8, 8, 1, 24, 1, 8]  tol: 3
$$$$$$$$$$$$$ epoch  29  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
### skip layer  1 wait:  4  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
adv train loss:  -10.785387344658375 , diff:  10.785387344658375
adv train loss:  -10.89665649831295 , diff:  0.11126915365457535
adv train loss:  -11.081946931779385 , diff:  0.18529043346643448
adv train loss:  -10.940900042653084 , diff:  0.1410468891263008
adv train loss:  -10.892110206186771 , diff:  0.04878983646631241
adv train loss:  -10.8138832077384 , diff:  0.07822699844837189
adv train loss:  -10.847347378730774 , diff:  0.03346417099237442
adv train loss:  -10.957781217992306 , diff:  0.11043383926153183
adv train loss:  -10.952869400382042 , diff:  0.0049118176102638245
layer  6  adv train finish, try to retain  256
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -9.775882996618748 , diff:  9.775882996618748
adv train loss:  -9.800312601029873 , diff:  0.024429604411125183
adv train loss:  -9.706369794905186 , diff:  0.0939428061246872
adv train loss:  -9.713749796152115 , diff:  0.007380001246929169
layer  7  adv train finish, try to retain  253
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -10.741201967000961 , diff:  10.741201967000961
adv train loss:  -10.73748130351305 , diff:  0.0037206634879112244
layer  8  adv train finish, try to retain  264
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -9.66111784428358 , diff:  9.66111784428358
adv train loss:  -9.728010565042496 , diff:  0.06689272075891495
adv train loss:  -9.531730972230434 , diff:  0.1962795928120613
adv train loss:  -9.536740727722645 , diff:  0.005009755492210388
layer  9  adv train finish, try to retain  438
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -504.42360162734985 , diff:  504.42360162734985
adv train loss:  -504.96641063690186 , diff:  0.542809009552002
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  16
test acc: top1 ->  21.81 ; top5 ->  68.21  and loss:  40605.95297241211
forward train acc: top1 ->  95.43599997802734 ; top5 ->  99.89  and loss:  15.809838093817234
test acc: top1 ->  91.31 ; top5 ->  99.17  and loss:  34.6447441726923
forward train acc: top1 ->  99.12800000244141 ; top5 ->  99.99  and loss:  3.1976159447804093
test acc: top1 ->  91.73 ; top5 ->  99.22  and loss:  36.048411183059216
forward train acc: top1 ->  99.46600000488282 ; top5 ->  99.996  and loss:  1.976920863147825
test acc: top1 ->  91.82 ; top5 ->  99.2  and loss:  38.505917221307755
forward train acc: top1 ->  99.61399997558594 ; top5 ->  99.998  and loss:  1.3028721956070513
test acc: top1 ->  91.81 ; top5 ->  99.18  and loss:  40.25128811597824
forward train acc: top1 ->  99.736 ; top5 ->  99.998  and loss:  0.9439875513780862
test acc: top1 ->  92.05 ; top5 ->  99.24  and loss:  41.844793900847435
forward train acc: top1 ->  99.75799997558593 ; top5 ->  100.0  and loss:  0.7974434534553438
test acc: top1 ->  92.02 ; top5 ->  99.22  and loss:  42.328845992684364
forward train acc: top1 ->  99.7780000024414 ; top5 ->  100.0  and loss:  0.7476125474786386
test acc: top1 ->  92.09 ; top5 ->  99.28  and loss:  43.22340141236782
forward train acc: top1 ->  99.83399997558594 ; top5 ->  100.0  and loss:  0.5555494797881693
test acc: top1 ->  92.12 ; top5 ->  99.25  and loss:  43.80698551237583
==> this epoch:  16 / 512
---------------- start layer  11  ---------------
adv train loss:  -27.419370785355568 , diff:  27.419370785355568
adv train loss:  -27.340450540184975 , diff:  0.07892024517059326
adv train loss:  -27.363595485687256 , diff:  0.02314494550228119
layer  11  adv train finish, try to retain  396
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -652.7562074661255 , diff:  652.7562074661255
adv train loss:  -683.6088533401489 , diff:  30.852645874023438
adv train loss:  -683.7764320373535 , diff:  0.16757869720458984
layer  12  adv train finish, try to retain  13
test acc: top1 ->  10.1 ; top5 ->  53.3  and loss:  858.8941292762756
forward train acc: top1 ->  64.58200001953125 ; top5 ->  94.092  and loss:  232.61681027710438
test acc: top1 ->  88.42 ; top5 ->  98.34  and loss:  46.475475788116455
forward train acc: top1 ->  98.91999997558594 ; top5 ->  99.994  and loss:  10.502244859933853
test acc: top1 ->  91.33 ; top5 ->  98.5  and loss:  34.71760065853596
forward train acc: top1 ->  99.66399997558594 ; top5 ->  99.998  and loss:  4.301404444500804
test acc: top1 ->  91.8 ; top5 ->  98.52  and loss:  33.357776425778866
forward train acc: top1 ->  99.812 ; top5 ->  100.0  and loss:  2.4728842973709106
test acc: top1 ->  91.9 ; top5 ->  98.58  and loss:  33.70369079709053
forward train acc: top1 ->  99.884 ; top5 ->  99.998  and loss:  1.6575758792459965
test acc: top1 ->  91.98 ; top5 ->  98.66  and loss:  34.53195918351412
forward train acc: top1 ->  99.86000000244141 ; top5 ->  100.0  and loss:  1.33034988027066
test acc: top1 ->  92.06 ; top5 ->  98.63  and loss:  35.176378063857555
forward train acc: top1 ->  99.896 ; top5 ->  99.998  and loss:  1.120903474278748
test acc: top1 ->  91.98 ; top5 ->  98.67  and loss:  35.69792325049639
forward train acc: top1 ->  99.88999997558594 ; top5 ->  99.998  and loss:  0.9926102408207953
test acc: top1 ->  92.0 ; top5 ->  98.71  and loss:  36.25919848680496
forward train acc: top1 ->  99.886 ; top5 ->  100.0  and loss:  0.8933983216993511
test acc: top1 ->  92.09 ; top5 ->  98.7  and loss:  36.57566053420305
forward train acc: top1 ->  99.936 ; top5 ->  100.0  and loss:  0.7570988568477333
test acc: top1 ->  92.07 ; top5 ->  98.7  and loss:  37.41644661128521
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  62 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -11454.058586120605 , diff:  11454.058586120605
adv train loss:  -20223.197509765625 , diff:  8769.13892364502
adv train loss:  -29130.68504333496 , diff:  8907.487533569336
adv train loss:  -37940.323974609375 , diff:  8809.638931274414
adv train loss:  -44240.64859008789 , diff:  6300.324615478516
adv train loss:  -46306.098693847656 , diff:  2065.4501037597656
adv train loss:  -46698.02795410156 , diff:  391.92926025390625
adv train loss:  -46795.57263183594 , diff:  97.544677734375
adv train loss:  -46844.638122558594 , diff:  49.06549072265625
adv train loss:  -46840.12438964844 , diff:  4.51373291015625
layer  13  adv train finish, try to retain  60
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  7
layer  8  :  0.501953125  ==>  257 / 512 , inc:  8
layer  9  :  0.126953125  ==>  65 / 512 , inc:  8
layer  10  :  0.03125  ==>  16 / 512 , inc:  2
layer  11  :  0.189453125  ==>  97 / 512 , inc:  24
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.109375  ==>  56 / 512 , inc:  8
eps [0.014416259765625002, 1.6402500000000002, 1.6402500000000002, 1.6402500000000002, 1.6402500000000002, 1.6402500000000002, 0.8201250000000001, 0.009610839843750002, 0.025628906250000003, 0.7290000000000001, 1.6402500000000002, 0.7290000000000001, 1.2301875000000002, 11.664000000000001]  wait [3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 0, 0, 4, 0]  inc [1, 1, 1, 1, 1, 1, 1, 7, 8, 8, 2, 24, 1, 8]  tol: 3
$$$$$$$$$$$$$ epoch  30  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
### skip layer  1 wait:  3  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
adv train loss:  -667.7281384468079 , diff:  667.7281384468079
adv train loss:  -667.2776098251343 , diff:  0.450528621673584
layer  6  adv train finish, try to retain  231
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -663.6558594703674 , diff:  663.6558594703674
adv train loss:  -663.8213033676147 , diff:  0.16544389724731445
layer  7  adv train finish, try to retain  293
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -661.928277015686 , diff:  661.928277015686
adv train loss:  -662.604866027832 , diff:  0.6765890121459961
layer  8  adv train finish, try to retain  293
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -568.9470143318176 , diff:  568.9470143318176
adv train loss:  -568.5120511054993 , diff:  0.4349632263183594
layer  9  adv train finish, try to retain  447
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -2684.754518508911 , diff:  2684.754518508911
adv train loss:  -2685.0816345214844 , diff:  0.3271160125732422
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  14
test acc: top1 ->  10.0 ; top5 ->  61.58  and loss:  139950.7117919922
forward train acc: top1 ->  89.908 ; top5 ->  99.878  and loss:  68.19036043202505
test acc: top1 ->  91.84 ; top5 ->  99.15  and loss:  52.101826161146164
forward train acc: top1 ->  99.83199997558594 ; top5 ->  99.996  and loss:  0.7674633418209851
test acc: top1 ->  91.88 ; top5 ->  99.15  and loss:  53.390979535877705
forward train acc: top1 ->  99.86999997558594 ; top5 ->  100.0  and loss:  0.5497621853137389
test acc: top1 ->  92.0 ; top5 ->  99.19  and loss:  53.15897737443447
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.31966270285192877
test acc: top1 ->  92.09 ; top5 ->  99.14  and loss:  54.0063152089715
forward train acc: top1 ->  99.908 ; top5 ->  100.0  and loss:  0.351677575672511
test acc: top1 ->  92.16 ; top5 ->  99.13  and loss:  54.21965750306845
==> this epoch:  14 / 512
---------------- start layer  11  ---------------
adv train loss:  -41.81835785508156 , diff:  41.81835785508156
adv train loss:  -41.81624886393547 , diff:  0.0021089911460876465
layer  11  adv train finish, try to retain  409
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
adv train loss:  -9298.644081115723 , diff:  9298.644081115723
adv train loss:  -19417.208404541016 , diff:  10118.564323425293
adv train loss:  -28144.827575683594 , diff:  8727.619171142578
adv train loss:  -36383.499603271484 , diff:  8238.67202758789
adv train loss:  -44456.798278808594 , diff:  8073.298675537109
adv train loss:  -52403.789489746094 , diff:  7946.9912109375
adv train loss:  -60237.515625 , diff:  7833.726135253906
adv train loss:  -68002.22387695312 , diff:  7764.708251953125
adv train loss:  -75698.89599609375 , diff:  7696.672119140625
adv train loss:  -81694.45178222656 , diff:  5995.5557861328125
layer  13  adv train finish, try to retain  39
test acc: top1 ->  35.72 ; top5 ->  95.94  and loss:  748.9029812812805
forward train acc: top1 ->  91.65200000488281 ; top5 ->  99.948  and loss:  53.90305056725629
test acc: top1 ->  91.04 ; top5 ->  98.83  and loss:  63.64970349520445
forward train acc: top1 ->  99.76599997558594 ; top5 ->  100.0  and loss:  0.795367163605988
test acc: top1 ->  91.51 ; top5 ->  98.89  and loss:  60.812889739871025
forward train acc: top1 ->  99.856 ; top5 ->  100.0  and loss:  0.4862987017259002
test acc: top1 ->  91.78 ; top5 ->  98.89  and loss:  60.91238421201706
forward train acc: top1 ->  99.88 ; top5 ->  100.0  and loss:  0.3970846269512549
test acc: top1 ->  91.83 ; top5 ->  98.96  and loss:  60.88612248748541
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.2670587942702696
test acc: top1 ->  91.93 ; top5 ->  98.95  and loss:  60.65030613541603
forward train acc: top1 ->  99.928 ; top5 ->  99.998  and loss:  0.2921022364171222
test acc: top1 ->  91.85 ; top5 ->  98.95  and loss:  60.99026448279619
forward train acc: top1 ->  99.924 ; top5 ->  100.0  and loss:  0.2602926653926261
test acc: top1 ->  91.89 ; top5 ->  98.94  and loss:  61.15837347507477
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.22432697156909853
test acc: top1 ->  91.86 ; top5 ->  98.98  and loss:  61.23112374544144
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.2023584136622958
test acc: top1 ->  91.86 ; top5 ->  98.93  and loss:  61.52300111949444
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.20961897398228757
test acc: top1 ->  91.89 ; top5 ->  99.0  and loss:  61.75619728863239
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  56 / 512 , inc:  8
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  7
layer  8  :  0.501953125  ==>  257 / 512 , inc:  8
layer  9  :  0.126953125  ==>  65 / 512 , inc:  8
layer  10  :  0.02734375  ==>  14 / 512 , inc:  4
layer  11  :  0.189453125  ==>  97 / 512 , inc:  24
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.109375  ==>  56 / 512 , inc:  4
eps [0.014416259765625002, 1.6402500000000002, 1.6402500000000002, 1.6402500000000002, 1.6402500000000002, 1.6402500000000002, 1.6402500000000002, 0.019221679687500003, 0.051257812500000006, 1.4580000000000002, 1.6402500000000002, 1.4580000000000002, 1.2301875000000002, 8.748000000000001]  wait [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 3, 2]  inc [1, 1, 1, 1, 1, 1, 1, 7, 8, 8, 4, 24, 1, 4]  tol: 3
$$$$$$$$$$$$$ epoch  31  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -71.97428178787231 , diff:  71.97428178787231
adv train loss:  -71.9722989499569 , diff:  0.0019828379154205322
layer  0  adv train finish, try to retain  64
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -101.09900277853012 , diff:  101.09900277853012
adv train loss:  -103.56975090503693 , diff:  2.4707481265068054
adv train loss:  -103.38379663228989 , diff:  0.1859542727470398
adv train loss:  -104.6837408542633 , diff:  1.2999442219734192
adv train loss:  -103.32063907384872 , diff:  1.3631017804145813
adv train loss:  -104.03150111436844 , diff:  0.7108620405197144
adv train loss:  -104.2271603345871 , diff:  0.19565922021865845
adv train loss:  -103.84776198863983 , diff:  0.3793983459472656
adv train loss:  -104.41964942216873 , diff:  0.5718874335289001
adv train loss:  -100.00766158103943 , diff:  4.411987841129303
layer  1  adv train finish, try to retain  45
test acc: top1 ->  8.88 ; top5 ->  48.69  and loss:  1423.7655515670776
forward train acc: top1 ->  98.16199997802734 ; top5 ->  99.95599997558594  and loss:  8.252731053624302
test acc: top1 ->  90.93 ; top5 ->  99.02  and loss:  71.90295465290546
forward train acc: top1 ->  99.43999997558593 ; top5 ->  99.992  and loss:  2.005163063062355
test acc: top1 ->  91.23 ; top5 ->  99.11  and loss:  63.39146065711975
forward train acc: top1 ->  99.51600000488281 ; top5 ->  99.994  and loss:  1.5473920097574592
test acc: top1 ->  91.19 ; top5 ->  99.15  and loss:  60.727854162454605
forward train acc: top1 ->  99.588 ; top5 ->  99.994  and loss:  1.2897792105795816
test acc: top1 ->  91.42 ; top5 ->  99.21  and loss:  56.01953649520874
forward train acc: top1 ->  99.6560000024414 ; top5 ->  99.996  and loss:  1.0344388521043584
test acc: top1 ->  91.45 ; top5 ->  99.21  and loss:  57.090007066726685
forward train acc: top1 ->  99.6620000024414 ; top5 ->  100.0  and loss:  1.0194075630279258
test acc: top1 ->  91.56 ; top5 ->  99.21  and loss:  55.27435252815485
forward train acc: top1 ->  99.696 ; top5 ->  100.0  and loss:  0.8711522366502322
test acc: top1 ->  91.52 ; top5 ->  99.21  and loss:  55.36026458442211
forward train acc: top1 ->  99.744 ; top5 ->  100.0  and loss:  0.7483660425059497
test acc: top1 ->  91.53 ; top5 ->  99.23  and loss:  55.95244789123535
forward train acc: top1 ->  99.734 ; top5 ->  100.0  and loss:  0.8198459483101033
test acc: top1 ->  91.46 ; top5 ->  99.2  and loss:  56.100170470774174
forward train acc: top1 ->  99.768 ; top5 ->  100.0  and loss:  0.7300372309982777
test acc: top1 ->  91.47 ; top5 ->  99.28  and loss:  54.645004615187645
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  55 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -1113.937699486909 , diff:  1113.937699486909
adv train loss:  -1681.684853553772 , diff:  567.747154066863
adv train loss:  -1683.2314910888672 , diff:  1.5466375350952148
adv train loss:  -1690.0093078613281 , diff:  6.7778167724609375
adv train loss:  -1692.4500064849854 , diff:  2.4406986236572266
adv train loss:  -1701.0621910095215 , diff:  8.612184524536133
adv train loss:  -1689.25417137146 , diff:  11.808019638061523
adv train loss:  -1688.9160232543945 , diff:  0.3381481170654297
adv train loss:  -1687.7334337234497 , diff:  1.1825895309448242
adv train loss:  -1690.5538311004639 , diff:  2.82039737701416
layer  2  adv train finish, try to retain  45
test acc: top1 ->  9.69 ; top5 ->  50.6  and loss:  1712.9738359451294
forward train acc: top1 ->  92.34199997558594 ; top5 ->  99.63999997558594  and loss:  26.867018043994904
test acc: top1 ->  86.02 ; top5 ->  98.58  and loss:  53.06752824783325
forward train acc: top1 ->  94.26200000244141 ; top5 ->  99.77399997558594  and loss:  17.591602347791195
test acc: top1 ->  86.94 ; top5 ->  98.8  and loss:  48.1848426759243
forward train acc: top1 ->  95.14600001220703 ; top5 ->  99.86999997558594  and loss:  14.380002357065678
test acc: top1 ->  87.63 ; top5 ->  98.99  and loss:  47.32059843838215
forward train acc: top1 ->  95.69600001220704 ; top5 ->  99.894  and loss:  13.099005162715912
test acc: top1 ->  87.82 ; top5 ->  98.98  and loss:  46.13683941960335
forward train acc: top1 ->  96.06999998535156 ; top5 ->  99.89199997558593  and loss:  11.610954724252224
test acc: top1 ->  88.09 ; top5 ->  99.03  and loss:  46.93479746580124
forward train acc: top1 ->  96.30400001464844 ; top5 ->  99.918  and loss:  10.903081361204386
test acc: top1 ->  88.3 ; top5 ->  99.04  and loss:  45.21604265272617
forward train acc: top1 ->  96.57199999023437 ; top5 ->  99.92999997558594  and loss:  9.977030210196972
test acc: top1 ->  88.36 ; top5 ->  99.07  and loss:  45.790504306554794
forward train acc: top1 ->  96.51600001464844 ; top5 ->  99.938  and loss:  10.195865899324417
test acc: top1 ->  88.53 ; top5 ->  99.07  and loss:  44.611614391207695
forward train acc: top1 ->  96.76800000732422 ; top5 ->  99.952  and loss:  9.455649331212044
test acc: top1 ->  88.58 ; top5 ->  99.11  and loss:  45.95283080637455
forward train acc: top1 ->  96.80199998535156 ; top5 ->  99.942  and loss:  9.058063961565495
test acc: top1 ->  88.61 ; top5 ->  99.11  and loss:  45.28290256857872
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -764.6664032069966 , diff:  764.6664032069966
adv train loss:  -1226.8481044769287 , diff:  462.1817012699321
adv train loss:  -1243.7105255126953 , diff:  16.8624210357666
adv train loss:  -1232.5910730361938 , diff:  11.119452476501465
adv train loss:  -1231.887077331543 , diff:  0.7039957046508789
adv train loss:  -1229.6075916290283 , diff:  2.2794857025146484
adv train loss:  -1240.5827207565308 , diff:  10.975129127502441
adv train loss:  -1227.15274143219 , diff:  13.42997932434082
adv train loss:  -1235.4051914215088 , diff:  8.252449989318848
adv train loss:  -1229.13529586792 , diff:  6.269895553588867
layer  3  adv train finish, try to retain  30
test acc: top1 ->  9.99 ; top5 ->  52.09  and loss:  715.027690410614
forward train acc: top1 ->  84.96999997802735 ; top5 ->  98.8860000024414  and loss:  46.38050910830498
test acc: top1 ->  81.22 ; top5 ->  98.01  and loss:  63.47860163450241
forward train acc: top1 ->  87.08399997314453 ; top5 ->  99.24799997558594  and loss:  37.748460203409195
test acc: top1 ->  82.49 ; top5 ->  98.47  and loss:  57.89158692955971
forward train acc: top1 ->  88.58799999511719 ; top5 ->  99.40200000244141  and loss:  33.6421237885952
test acc: top1 ->  83.38 ; top5 ->  98.64  and loss:  55.24428570270538
forward train acc: top1 ->  89.3819999975586 ; top5 ->  99.478  and loss:  30.98436050117016
test acc: top1 ->  84.03 ; top5 ->  98.6  and loss:  54.31494337320328
forward train acc: top1 ->  90.32399999755859 ; top5 ->  99.51599997558594  and loss:  28.836561933159828
test acc: top1 ->  84.39 ; top5 ->  98.75  and loss:  51.769715547561646
forward train acc: top1 ->  90.43799998779296 ; top5 ->  99.59600000488281  and loss:  27.77286909520626
test acc: top1 ->  84.58 ; top5 ->  98.69  and loss:  51.48346830904484
forward train acc: top1 ->  90.60000000488282 ; top5 ->  99.58999997802735  and loss:  27.24212808907032
test acc: top1 ->  84.69 ; top5 ->  98.77  and loss:  50.83475139737129
forward train acc: top1 ->  90.97799997070312 ; top5 ->  99.63399997558594  and loss:  26.174364387989044
test acc: top1 ->  84.81 ; top5 ->  98.77  and loss:  50.30104884505272
forward train acc: top1 ->  91.09399998291016 ; top5 ->  99.576  and loss:  25.815999180078506
test acc: top1 ->  84.89 ; top5 ->  98.75  and loss:  50.373591005802155
forward train acc: top1 ->  91.40799997314453 ; top5 ->  99.65399997558593  and loss:  25.143651098012924
test acc: top1 ->  84.94 ; top5 ->  98.79  and loss:  49.72025640308857
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -73.84769138507545 , diff:  73.84769138507545
adv train loss:  -801.8237190246582 , diff:  727.9760276395828
adv train loss:  -989.5928144454956 , diff:  187.7690954208374
adv train loss:  -1013.1420660018921 , diff:  23.549251556396484
adv train loss:  -1061.8416624069214 , diff:  48.6995964050293
adv train loss:  -1071.5496320724487 , diff:  9.707969665527344
adv train loss:  -1082.2784051895142 , diff:  10.72877311706543
adv train loss:  -1153.749976158142 , diff:  71.47157096862793
adv train loss:  -1153.0202598571777 , diff:  0.7297163009643555
adv train loss:  -1156.3246564865112 , diff:  3.304396629333496
layer  4  adv train finish, try to retain  34
test acc: top1 ->  17.81 ; top5 ->  59.49  and loss:  789.7951927185059
forward train acc: top1 ->  81.20599998046875 ; top5 ->  98.49999998291015  and loss:  55.373044312000275
test acc: top1 ->  79.27 ; top5 ->  98.12  and loss:  64.12045952677727
forward train acc: top1 ->  84.04400001220704 ; top5 ->  99.03599997802735  and loss:  45.68218120932579
test acc: top1 ->  80.91 ; top5 ->  98.35  and loss:  60.1058808863163
forward train acc: top1 ->  85.45800001220704 ; top5 ->  99.16599997558593  and loss:  41.75672370195389
test acc: top1 ->  82.34 ; top5 ->  98.55  and loss:  56.31868466734886
forward train acc: top1 ->  86.50800000976562 ; top5 ->  99.33199997558594  and loss:  38.613724648952484
test acc: top1 ->  82.58 ; top5 ->  98.71  and loss:  54.375548273324966
forward train acc: top1 ->  87.40999997314454 ; top5 ->  99.3520000024414  and loss:  36.553752064704895
test acc: top1 ->  83.31 ; top5 ->  98.7  and loss:  53.213839679956436
forward train acc: top1 ->  87.70599997558594 ; top5 ->  99.34999997802734  and loss:  35.39269259572029
test acc: top1 ->  83.54 ; top5 ->  98.71  and loss:  52.885624423623085
forward train acc: top1 ->  88.02399999755859 ; top5 ->  99.42200000244141  and loss:  34.355346858501434
test acc: top1 ->  83.67 ; top5 ->  98.73  and loss:  52.338008373975754
forward train acc: top1 ->  88.256 ; top5 ->  99.46599998291016  and loss:  33.96384420990944
test acc: top1 ->  84.06 ; top5 ->  98.83  and loss:  51.536052361130714
forward train acc: top1 ->  88.36600001220702 ; top5 ->  99.5000000048828  and loss:  33.24538251757622
test acc: top1 ->  84.11 ; top5 ->  98.9  and loss:  51.4135575145483
forward train acc: top1 ->  88.69000001953125 ; top5 ->  99.45000000488281  and loss:  32.65911614894867
test acc: top1 ->  84.16 ; top5 ->  98.95  and loss:  51.02115276455879
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -205.69821026921272 , diff:  205.69821026921272
adv train loss:  -1272.9865221977234 , diff:  1067.2883119285107
adv train loss:  -1451.4354982376099 , diff:  178.44897603988647
adv train loss:  -1458.1710662841797 , diff:  6.735568046569824
adv train loss:  -1455.1990823745728 , diff:  2.9719839096069336
adv train loss:  -1456.4069366455078 , diff:  1.2078542709350586
adv train loss:  -1453.9621467590332 , diff:  2.4447898864746094
adv train loss:  -1450.3800058364868 , diff:  3.5821409225463867
adv train loss:  -1452.5743322372437 , diff:  2.194326400756836
adv train loss:  -1453.140188217163 , diff:  0.5658559799194336
layer  5  adv train finish, try to retain  45
test acc: top1 ->  20.16 ; top5 ->  69.08  and loss:  2115.7204303741455
forward train acc: top1 ->  88.97600000244141 ; top5 ->  99.40599997558594  and loss:  31.856767520308495
test acc: top1 ->  84.89 ; top5 ->  98.77  and loss:  49.18023091554642
forward train acc: top1 ->  92.16799998046875 ; top5 ->  99.70399997558594  and loss:  22.39223052561283
test acc: top1 ->  86.09 ; top5 ->  98.92  and loss:  45.98603232204914
forward train acc: top1 ->  93.31800001953125 ; top5 ->  99.796  and loss:  19.467015877366066
test acc: top1 ->  86.9 ; top5 ->  99.08  and loss:  44.98057207465172
forward train acc: top1 ->  93.93999999267578 ; top5 ->  99.848  and loss:  17.225933134555817
test acc: top1 ->  87.54 ; top5 ->  99.14  and loss:  44.22529834508896
forward train acc: top1 ->  94.5699999975586 ; top5 ->  99.85199997558594  and loss:  15.758090175688267
test acc: top1 ->  87.83 ; top5 ->  99.1  and loss:  43.84661445021629
forward train acc: top1 ->  94.75399999511718 ; top5 ->  99.858  and loss:  14.906034961342812
test acc: top1 ->  87.94 ; top5 ->  99.14  and loss:  43.25644774734974
forward train acc: top1 ->  94.93599999511719 ; top5 ->  99.862  and loss:  14.842945270240307
test acc: top1 ->  87.96 ; top5 ->  99.11  and loss:  42.8286452293396
forward train acc: top1 ->  95.0379999975586 ; top5 ->  99.91  and loss:  14.018309749662876
test acc: top1 ->  88.15 ; top5 ->  99.16  and loss:  43.370400443673134
forward train acc: top1 ->  95.05399997802735 ; top5 ->  99.89399997558594  and loss:  13.93612913787365
test acc: top1 ->  88.17 ; top5 ->  99.16  and loss:  43.312808111310005
forward train acc: top1 ->  95.20399999511719 ; top5 ->  99.89999997558594  and loss:  13.72715049982071
test acc: top1 ->  88.4 ; top5 ->  99.13  and loss:  42.44712541997433
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -273.04558051750064 , diff:  273.04558051750064
adv train loss:  -1535.4107484817505 , diff:  1262.3651679642498
adv train loss:  -1719.288953781128 , diff:  183.87820529937744
adv train loss:  -1728.384599685669 , diff:  9.095645904541016
adv train loss:  -1748.4042339324951 , diff:  20.019634246826172
adv train loss:  -1793.6806888580322 , diff:  45.27645492553711
adv train loss:  -1801.384183883667 , diff:  7.703495025634766
adv train loss:  -1800.2170524597168 , diff:  1.1671314239501953
adv train loss:  -1797.8485431671143 , diff:  2.368509292602539
adv train loss:  -1793.8435287475586 , diff:  4.005014419555664
layer  6  adv train finish, try to retain  33
test acc: top1 ->  35.53 ; top5 ->  73.1  and loss:  479.3413758277893
forward train acc: top1 ->  94.73200001708985 ; top5 ->  99.91  and loss:  15.278231084346771
test acc: top1 ->  87.92 ; top5 ->  99.09  and loss:  45.52505864202976
forward train acc: top1 ->  96.52400001464844 ; top5 ->  99.942  and loss:  10.050698067992926
test acc: top1 ->  88.82 ; top5 ->  99.26  and loss:  44.84627257287502
forward train acc: top1 ->  97.12799999023437 ; top5 ->  99.98  and loss:  8.027598805725574
test acc: top1 ->  89.22 ; top5 ->  99.27  and loss:  45.55435013771057
forward train acc: top1 ->  97.61800000732421 ; top5 ->  99.974  and loss:  6.787621662020683
test acc: top1 ->  89.55 ; top5 ->  99.29  and loss:  45.742135763168335
forward train acc: top1 ->  97.91200000732422 ; top5 ->  99.966  and loss:  6.000025294721127
test acc: top1 ->  89.68 ; top5 ->  99.33  and loss:  45.09567029029131
forward train acc: top1 ->  98.01199998779298 ; top5 ->  99.97799997558593  and loss:  5.5571668446063995
test acc: top1 ->  89.93 ; top5 ->  99.31  and loss:  44.793314166367054
forward train acc: top1 ->  98.17800000732421 ; top5 ->  99.994  and loss:  5.176205234602094
test acc: top1 ->  89.85 ; top5 ->  99.33  and loss:  45.582377791404724
forward train acc: top1 ->  98.19999998046875 ; top5 ->  99.99  and loss:  5.192344509065151
test acc: top1 ->  89.87 ; top5 ->  99.28  and loss:  45.676734164357185
forward train acc: top1 ->  98.17999998046875 ; top5 ->  99.99  and loss:  4.968283537775278
test acc: top1 ->  90.01 ; top5 ->  99.32  and loss:  45.54034394770861
forward train acc: top1 ->  98.32200000976563 ; top5 ->  99.996  and loss:  4.796212527900934
test acc: top1 ->  89.98 ; top5 ->  99.29  and loss:  46.12286887317896
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  144 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -1.9392120325937867 , diff:  1.9392120325937867
adv train loss:  -1.8406431265175343 , diff:  0.09856890607625246
adv train loss:  -1.9882397977635264 , diff:  0.14759667124599218
adv train loss:  -1.9144934369251132 , diff:  0.07374636083841324
adv train loss:  -1.9505178923718631 , diff:  0.036024455446749926
adv train loss:  -2.0275774570181966 , diff:  0.07705956464633346
adv train loss:  -1.817067353054881 , diff:  0.2105101039633155
adv train loss:  -1.841835721861571 , diff:  0.024768368806689978
adv train loss:  -1.9747102269902825 , diff:  0.13287450512871146
adv train loss:  -2.0174422627314925 , diff:  0.042732035741209984
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  246
test acc: top1 ->  45.44 ; top5 ->  72.72  and loss:  229.08747506141663
forward train acc: top1 ->  99.13 ; top5 ->  99.998  and loss:  2.6563237099908292
test acc: top1 ->  90.92 ; top5 ->  99.3  and loss:  47.919432148337364
forward train acc: top1 ->  99.558 ; top5 ->  100.0  and loss:  1.3885886569041759
test acc: top1 ->  91.26 ; top5 ->  99.31  and loss:  50.559459306299686
forward train acc: top1 ->  99.68399997558593 ; top5 ->  100.0  and loss:  0.9617813325021416
test acc: top1 ->  91.22 ; top5 ->  99.29  and loss:  53.46089179068804
forward train acc: top1 ->  99.762 ; top5 ->  100.0  and loss:  0.6913784911157563
test acc: top1 ->  91.45 ; top5 ->  99.27  and loss:  54.652283884584904
forward train acc: top1 ->  99.7520000024414 ; top5 ->  100.0  and loss:  0.7488613414461724
test acc: top1 ->  91.35 ; top5 ->  99.21  and loss:  55.849499091506004
forward train acc: top1 ->  99.79199997558594 ; top5 ->  100.0  and loss:  0.5862870183773339
test acc: top1 ->  91.52 ; top5 ->  99.28  and loss:  56.289722949266434
forward train acc: top1 ->  99.83599997558593 ; top5 ->  100.0  and loss:  0.4742971546947956
test acc: top1 ->  91.47 ; top5 ->  99.29  and loss:  57.5503858178854
forward train acc: top1 ->  99.824 ; top5 ->  100.0  and loss:  0.469385049684206
test acc: top1 ->  91.44 ; top5 ->  99.27  and loss:  58.51878842711449
forward train acc: top1 ->  99.84 ; top5 ->  100.0  and loss:  0.4770174708100967
test acc: top1 ->  91.42 ; top5 ->  99.23  and loss:  58.99033959209919
forward train acc: top1 ->  99.838 ; top5 ->  100.0  and loss:  0.4452270165493246
test acc: top1 ->  91.58 ; top5 ->  99.28  and loss:  59.1919212192297
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  253 / 512 , inc:  7
---------------- start layer  8  ---------------
adv train loss:  -0.6346935626643244 , diff:  0.6346935626643244
adv train loss:  -0.6536570591561031 , diff:  0.018963496491778642
adv train loss:  -0.5864366879104637 , diff:  0.0672203712456394
adv train loss:  -0.621684690908296 , diff:  0.03524800299783237
adv train loss:  -0.5837131898733787 , diff:  0.03797150103491731
adv train loss:  -0.6303921481594443 , diff:  0.04667895828606561
adv train loss:  -0.6515248368959874 , diff:  0.02113268873654306
adv train loss:  -0.6020067079225555 , diff:  0.049518128973431885
adv train loss:  -0.6222846706514247 , diff:  0.02027796272886917
adv train loss:  -0.6844566997606307 , diff:  0.06217202910920605
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  249
test acc: top1 ->  51.03 ; top5 ->  93.35  and loss:  250.06659150123596
forward train acc: top1 ->  99.42599997558594 ; top5 ->  99.998  and loss:  1.947675982140936
test acc: top1 ->  91.4 ; top5 ->  99.11  and loss:  61.265788704156876
forward train acc: top1 ->  99.80199997558594 ; top5 ->  100.0  and loss:  0.5670193871774245
test acc: top1 ->  91.66 ; top5 ->  99.13  and loss:  61.448978781700134
forward train acc: top1 ->  99.89199997558593 ; top5 ->  100.0  and loss:  0.3552822903729975
test acc: top1 ->  91.78 ; top5 ->  99.2  and loss:  62.14083647727966
forward train acc: top1 ->  99.88399997558594 ; top5 ->  100.0  and loss:  0.3424774184823036
test acc: top1 ->  91.79 ; top5 ->  99.19  and loss:  63.18404829502106
forward train acc: top1 ->  99.902 ; top5 ->  100.0  and loss:  0.28489265928510576
test acc: top1 ->  91.93 ; top5 ->  99.26  and loss:  63.02805680036545
forward train acc: top1 ->  99.904 ; top5 ->  100.0  and loss:  0.2682970303867478
test acc: top1 ->  92.13 ; top5 ->  99.23  and loss:  63.30110338330269
==> this epoch:  249 / 512
---------------- start layer  9  ---------------
adv train loss:  -434.65368127822876 , diff:  434.65368127822876
adv train loss:  -432.98420810699463 , diff:  1.6694731712341309
adv train loss:  -435.159939289093 , diff:  2.1757311820983887
adv train loss:  -435.0469226837158 , diff:  0.11301660537719727
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  57
test acc: top1 ->  10.0 ; top5 ->  55.72  and loss:  704673.3032226562
forward train acc: top1 ->  98.17799997558593 ; top5 ->  99.984  and loss:  9.105969053925946
test acc: top1 ->  91.44 ; top5 ->  98.59  and loss:  58.21975776553154
forward train acc: top1 ->  99.782 ; top5 ->  99.998  and loss:  0.8524729332420975
test acc: top1 ->  91.68 ; top5 ->  98.63  and loss:  59.22851396352053
forward train acc: top1 ->  99.81199997558593 ; top5 ->  99.998  and loss:  0.6931028042454273
test acc: top1 ->  91.63 ; top5 ->  98.66  and loss:  60.528684705495834
forward train acc: top1 ->  99.854 ; top5 ->  100.0  and loss:  0.5429216293850914
test acc: top1 ->  91.69 ; top5 ->  98.67  and loss:  61.21590252220631
forward train acc: top1 ->  99.888 ; top5 ->  100.0  and loss:  0.39759928500279784
test acc: top1 ->  91.74 ; top5 ->  98.7  and loss:  62.60866980999708
forward train acc: top1 ->  99.89 ; top5 ->  100.0  and loss:  0.36014056572457775
test acc: top1 ->  91.87 ; top5 ->  98.77  and loss:  62.170141518116
forward train acc: top1 ->  99.916 ; top5 ->  100.0  and loss:  0.3206085783604067
test acc: top1 ->  91.81 ; top5 ->  98.77  and loss:  63.484984397888184
forward train acc: top1 ->  99.93199997558594 ; top5 ->  100.0  and loss:  0.2641965436923783
test acc: top1 ->  91.94 ; top5 ->  98.77  and loss:  63.57232256233692
forward train acc: top1 ->  99.908 ; top5 ->  100.0  and loss:  0.3038705256767571
test acc: top1 ->  91.82 ; top5 ->  98.76  and loss:  63.66485647857189
forward train acc: top1 ->  99.916 ; top5 ->  100.0  and loss:  0.2875188752368558
test acc: top1 ->  91.82 ; top5 ->  98.76  and loss:  64.10791224241257
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  65 / 512 , inc:  8
---------------- start layer  10  ---------------
adv train loss:  -1532.0219745635986 , diff:  1532.0219745635986
adv train loss:  -1532.635643005371 , diff:  0.6136684417724609
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  10
test acc: top1 ->  22.18 ; top5 ->  55.34  and loss:  75385.90557861328
forward train acc: top1 ->  96.08 ; top5 ->  99.98  and loss:  14.891101412009448
test acc: top1 ->  91.27 ; top5 ->  99.22  and loss:  53.691749691963196
forward train acc: top1 ->  99.80399997558594 ; top5 ->  100.0  and loss:  0.7692170559894294
test acc: top1 ->  91.64 ; top5 ->  99.19  and loss:  52.394237637519836
forward train acc: top1 ->  99.88199997558594 ; top5 ->  100.0  and loss:  0.5138488681986928
test acc: top1 ->  91.8 ; top5 ->  99.13  and loss:  53.24614480137825
forward train acc: top1 ->  99.906 ; top5 ->  100.0  and loss:  0.389967352617532
test acc: top1 ->  91.85 ; top5 ->  99.18  and loss:  53.85362918674946
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.32800222287187353
test acc: top1 ->  91.96 ; top5 ->  99.19  and loss:  53.8672553896904
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.2841514543106314
test acc: top1 ->  92.01 ; top5 ->  99.17  and loss:  54.80551280081272
forward train acc: top1 ->  99.92799997558593 ; top5 ->  100.0  and loss:  0.28353981210966595
test acc: top1 ->  91.88 ; top5 ->  99.19  and loss:  56.047567948699
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  14 / 512 , inc:  4
---------------- start layer  11  ---------------
adv train loss:  -311.2852371931076 , diff:  311.2852371931076
adv train loss:  -520.8401839733124 , diff:  209.55494678020477
adv train loss:  -612.8611207008362 , diff:  92.0209367275238
adv train loss:  -686.1133704185486 , diff:  73.2522497177124
adv train loss:  -702.2222995758057 , diff:  16.10892915725708
adv train loss:  -732.4839358329773 , diff:  30.26163625717163
adv train loss:  -827.411536693573 , diff:  94.9276008605957
adv train loss:  -1163.7057838439941 , diff:  336.29424715042114
adv train loss:  -1529.6409749984741 , diff:  365.93519115448
adv train loss:  -1902.3198127746582 , diff:  372.6788377761841
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  73
test acc: top1 ->  10.8 ; top5 ->  58.3  and loss:  11049903.5859375
forward train acc: top1 ->  98.6360000024414 ; top5 ->  99.996  and loss:  4.517853795667179
test acc: top1 ->  91.44 ; top5 ->  98.98  and loss:  72.91524827480316
forward train acc: top1 ->  99.896 ; top5 ->  100.0  and loss:  0.3836885379278101
test acc: top1 ->  91.58 ; top5 ->  98.96  and loss:  73.08472383022308
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.24663120300101582
test acc: top1 ->  91.79 ; top5 ->  98.93  and loss:  72.93291254341602
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.21825089072808623
test acc: top1 ->  91.82 ; top5 ->  99.02  and loss:  72.36123290657997
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.1661224099661922
test acc: top1 ->  91.86 ; top5 ->  98.94  and loss:  72.77639053016901
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.14877146912476746
test acc: top1 ->  91.99 ; top5 ->  99.0  and loss:  72.25416600704193
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.108506222422875
test acc: top1 ->  91.93 ; top5 ->  99.04  and loss:  73.08550328016281
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.12032738301786594
test acc: top1 ->  92.0 ; top5 ->  99.04  and loss:  73.21780002862215
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.1392190237675095
test acc: top1 ->  92.02 ; top5 ->  99.08  and loss:  73.53745544701815
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.12181522772880271
test acc: top1 ->  92.02 ; top5 ->  99.11  and loss:  74.5396090298891
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  97 / 512 , inc:  24
---------------- start layer  12  ---------------
### skip layer  12 wait:  3  ###
---------------- start layer  13  ---------------
adv train loss:  -8983.446258544922 , diff:  8983.446258544922
adv train loss:  -17844.939598083496 , diff:  8861.493339538574
adv train loss:  -28312.725463867188 , diff:  10467.785865783691
adv train loss:  -37886.34899902344 , diff:  9573.62353515625
adv train loss:  -46662.75109863281 , diff:  8776.402099609375
adv train loss:  -54546.553955078125 , diff:  7883.8028564453125
adv train loss:  -61497.96905517578 , diff:  6951.415100097656
adv train loss:  -65088.77020263672 , diff:  3590.8011474609375
adv train loss:  -66379.29327392578 , diff:  1290.5230712890625
adv train loss:  -66966.99908447266 , diff:  587.705810546875
layer  13  adv train finish, try to retain  54
test acc: top1 ->  53.74 ; top5 ->  96.61  and loss:  354.75311970710754
forward train acc: top1 ->  97.682 ; top5 ->  99.95  and loss:  9.773465057980502
test acc: top1 ->  92.01 ; top5 ->  99.05  and loss:  73.02101400494576
forward train acc: top1 ->  99.914 ; top5 ->  100.0  and loss:  0.25518601505609695
test acc: top1 ->  91.92 ; top5 ->  99.03  and loss:  71.04417245090008
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.14284952799062012
test acc: top1 ->  91.94 ; top5 ->  99.07  and loss:  70.77220423519611
forward train acc: top1 ->  99.9480000024414 ; top5 ->  100.0  and loss:  0.19019901614956325
test acc: top1 ->  92.05 ; top5 ->  99.04  and loss:  71.42249459028244
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.12316029872454237
test acc: top1 ->  91.91 ; top5 ->  99.09  and loss:  71.0354039222002
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.13500258455314906
test acc: top1 ->  91.96 ; top5 ->  99.07  and loss:  69.87656436860561
forward train acc: top1 ->  99.95800000244141 ; top5 ->  100.0  and loss:  0.11028890324632812
test acc: top1 ->  92.13 ; top5 ->  99.04  and loss:  69.72216407954693
==> this epoch:  54 / 512
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  3
layer  8  :  0.486328125  ==>  249 / 512 , inc:  16
layer  9  :  0.126953125  ==>  65 / 512 , inc:  4
layer  10  :  0.02734375  ==>  14 / 512 , inc:  2
layer  11  :  0.189453125  ==>  97 / 512 , inc:  12
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.10546875  ==>  54 / 512 , inc:  8
eps [0.028832519531250003, 1.2301875000000002, 1.2301875000000002, 1.2301875000000002, 1.2301875000000002, 1.2301875000000002, 1.2301875000000002, 0.014416259765625002, 0.051257812500000006, 1.0935000000000001, 1.2301875000000002, 1.0935000000000001, 1.2301875000000002, 8.748000000000001]  wait [2, 4, 4, 4, 4, 4, 4, 4, 0, 4, 2, 2, 2, 0]  inc [1, 1, 1, 1, 1, 1, 1, 3, 16, 4, 2, 12, 1, 8]  tol: 3
$$$$$$$$$$$$$ epoch  32  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -0.13476124907901976 , diff:  0.13476124907901976
adv train loss:  -0.13225361683726078 , diff:  0.0025076322417589836
layer  0  adv train finish, try to retain  63
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
### skip layer  1 wait:  4  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
adv train loss:  -0.6638072634523269 , diff:  0.6638072634523269
adv train loss:  -0.7526237439014949 , diff:  0.08881648044916801
adv train loss:  -0.8663307747337967 , diff:  0.11370703083230183
adv train loss:  -0.9856889636139385 , diff:  0.11935818888014182
adv train loss:  -0.7273817179666366 , diff:  0.2583072456473019
adv train loss:  -0.8486058755079284 , diff:  0.12122415754129179
adv train loss:  -0.8245026930817403 , diff:  0.02410318242618814
adv train loss:  -0.8083247882314026 , diff:  0.016177904850337654
adv train loss:  -0.7535737968864851 , diff:  0.054750991344917566
adv train loss:  -0.7533721393556334 , diff:  0.00020165753085166216
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  233
test acc: top1 ->  83.32 ; top5 ->  98.17  and loss:  111.04333513975143
forward train acc: top1 ->  99.92200000244141 ; top5 ->  100.0  and loss:  0.25376365373449516
test acc: top1 ->  92.04 ; top5 ->  99.17  and loss:  79.81755983829498
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.12556201683219115
test acc: top1 ->  92.1 ; top5 ->  99.15  and loss:  78.00663708895445
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.09980355552761466
test acc: top1 ->  92.11 ; top5 ->  99.32  and loss:  79.223956130445
==> this epoch:  233 / 512
---------------- start layer  9  ---------------
### skip layer  9 wait:  4  ###
---------------- start layer  10  ---------------
adv train loss:  -874.5876898765564 , diff:  874.5876898765564
adv train loss:  -874.4238767623901 , diff:  0.16381311416625977
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  12
test acc: top1 ->  26.33 ; top5 ->  71.32  and loss:  66950.92126464844
forward train acc: top1 ->  98.492 ; top5 ->  99.962  and loss:  9.494244144530967
test acc: top1 ->  91.69 ; top5 ->  99.06  and loss:  63.16136109828949
forward train acc: top1 ->  99.896 ; top5 ->  100.0  and loss:  0.38687800633488223
test acc: top1 ->  91.74 ; top5 ->  99.08  and loss:  65.09747290611267
forward train acc: top1 ->  99.8920000024414 ; top5 ->  100.0  and loss:  0.3644821198831778
test acc: top1 ->  91.86 ; top5 ->  99.14  and loss:  66.7766212373972
forward train acc: top1 ->  99.908 ; top5 ->  100.0  and loss:  0.31002727296436206
test acc: top1 ->  91.87 ; top5 ->  99.1  and loss:  66.95148387551308
forward train acc: top1 ->  99.936 ; top5 ->  100.0  and loss:  0.24937630428757984
test acc: top1 ->  92.09 ; top5 ->  99.12  and loss:  66.84580446779728
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.1378343022952322
test acc: top1 ->  91.96 ; top5 ->  99.09  and loss:  69.00739696621895
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.20421425109088887
test acc: top1 ->  92.04 ; top5 ->  99.08  and loss:  68.06089286506176
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.15373014916258398
test acc: top1 ->  92.11 ; top5 ->  99.16  and loss:  68.91694563627243
==> this epoch:  12 / 512
---------------- start layer  11  ---------------
adv train loss:  -1054.928949356079 , diff:  1054.928949356079
adv train loss:  -1139.0822343826294 , diff:  84.1532850265503
adv train loss:  -1208.691834449768 , diff:  69.60960006713867
adv train loss:  -1248.022970199585 , diff:  39.331135749816895
adv train loss:  -1571.1042766571045 , diff:  323.08130645751953
adv train loss:  -2638.108648300171 , diff:  1067.0043716430664
adv train loss:  -3019.5307655334473 , diff:  381.42211723327637
adv train loss:  -3145.701063156128 , diff:  126.17029762268066
adv train loss:  -3174.2603492736816 , diff:  28.55928611755371
adv train loss:  -3106.6899795532227 , diff:  67.57036972045898
layer  11  adv train finish, try to retain  435
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -1315.1202878952026 , diff:  1315.1202878952026
adv train loss:  -1538.282220840454 , diff:  223.16193294525146
adv train loss:  -1683.6847677230835 , diff:  145.4025468826294
adv train loss:  -1771.0299663543701 , diff:  87.34519863128662
adv train loss:  -2395.0214767456055 , diff:  623.9915103912354
adv train loss:  -2572.5416717529297 , diff:  177.52019500732422
adv train loss:  -2581.047445297241 , diff:  8.505773544311523
adv train loss:  -2689.59375 , diff:  108.54630470275879
adv train loss:  -2688.8304176330566 , diff:  0.7633323669433594
adv train loss:  -2690.037061691284 , diff:  1.206644058227539
layer  12  adv train finish, try to retain  32
test acc: top1 ->  32.15 ; top5 ->  69.53  and loss:  5077.348258972168
forward train acc: top1 ->  97.764 ; top5 ->  99.614  and loss:  14.451161391334608
test acc: top1 ->  91.43 ; top5 ->  98.77  and loss:  50.71727143228054
forward train acc: top1 ->  99.912 ; top5 ->  100.0  and loss:  0.5245426422916353
test acc: top1 ->  91.83 ; top5 ->  98.84  and loss:  50.993934482336044
forward train acc: top1 ->  99.94 ; top5 ->  100.0  and loss:  0.3693213388323784
test acc: top1 ->  91.87 ; top5 ->  98.87  and loss:  50.77951267361641
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.29205101873958483
test acc: top1 ->  91.87 ; top5 ->  98.9  and loss:  52.680505216121674
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.2270280012744479
test acc: top1 ->  91.85 ; top5 ->  98.93  and loss:  53.29136635363102
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.20930251735262573
test acc: top1 ->  91.87 ; top5 ->  98.91  and loss:  53.81518857181072
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.1974836708395742
test acc: top1 ->  91.92 ; top5 ->  98.91  and loss:  53.970628544688225
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.17939570182352327
test acc: top1 ->  91.92 ; top5 ->  98.94  and loss:  54.74536854028702
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.1315921281639021
test acc: top1 ->  92.01 ; top5 ->  98.98  and loss:  54.858824625611305
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.14191795064834878
test acc: top1 ->  91.92 ; top5 ->  98.93  and loss:  56.0230553150177
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  62 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -7583.666091918945 , diff:  7583.666091918945
adv train loss:  -12793.237937927246 , diff:  5209.571846008301
adv train loss:  -19061.568634033203 , diff:  6268.330696105957
adv train loss:  -25027.65168762207 , diff:  5966.083053588867
adv train loss:  -30854.019775390625 , diff:  5826.368087768555
adv train loss:  -36605.795013427734 , diff:  5751.775238037109
adv train loss:  -41413.007385253906 , diff:  4807.212371826172
adv train loss:  -43871.10217285156 , diff:  2458.0947875976562
adv train loss:  -44991.65072631836 , diff:  1120.5485534667969
adv train loss:  -45512.40197753906 , diff:  520.7512512207031
layer  13  adv train finish, try to retain  52
test acc: top1 ->  47.49 ; top5 ->  97.01  and loss:  408.3200035095215
forward train acc: top1 ->  97.17 ; top5 ->  99.996  and loss:  11.400058118975721
test acc: top1 ->  91.31 ; top5 ->  99.02  and loss:  87.2670131623745
forward train acc: top1 ->  99.906 ; top5 ->  100.0  and loss:  0.385993909323588
test acc: top1 ->  91.81 ; top5 ->  99.19  and loss:  78.92920444905758
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.2590882267104462
test acc: top1 ->  91.87 ; top5 ->  99.16  and loss:  77.62004604935646
forward train acc: top1 ->  99.936 ; top5 ->  100.0  and loss:  0.1934852788108401
test acc: top1 ->  91.98 ; top5 ->  99.17  and loss:  77.10988436639309
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.1596670239159721
test acc: top1 ->  92.0 ; top5 ->  99.13  and loss:  77.00199310481548
forward train acc: top1 ->  99.95399997558594 ; top5 ->  100.0  and loss:  0.17448447643982945
test acc: top1 ->  92.07 ; top5 ->  99.14  and loss:  76.3077726662159
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.13328974685282446
test acc: top1 ->  92.01 ; top5 ->  99.16  and loss:  77.02370908856392
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.14083855401258916
test acc: top1 ->  92.09 ; top5 ->  99.14  and loss:  75.87079387903214
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.12224339851672994
test acc: top1 ->  91.98 ; top5 ->  99.17  and loss:  75.40289476513863
forward train acc: top1 ->  99.9660000024414 ; top5 ->  100.0  and loss:  0.13623243389156414
test acc: top1 ->  91.97 ; top5 ->  99.09  and loss:  76.8229214400053
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  54 / 512 , inc:  8
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  3
layer  8  :  0.455078125  ==>  233 / 512 , inc:  32
layer  9  :  0.126953125  ==>  65 / 512 , inc:  4
layer  10  :  0.0234375  ==>  12 / 512 , inc:  4
layer  11  :  0.189453125  ==>  97 / 512 , inc:  12
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.10546875  ==>  54 / 512 , inc:  4
eps [0.057665039062500006, 1.2301875000000002, 1.2301875000000002, 1.2301875000000002, 1.2301875000000002, 1.2301875000000002, 1.2301875000000002, 0.014416259765625002, 0.051257812500000006, 1.0935000000000001, 1.2301875000000002, 2.1870000000000003, 0.9226406250000001, 6.561000000000001]  wait [2, 3, 3, 3, 3, 3, 3, 3, 0, 3, 0, 2, 4, 2]  inc [1, 1, 1, 1, 1, 1, 1, 3, 32, 4, 4, 12, 1, 4]  tol: 3
$$$$$$$$$$$$$ epoch  33  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -17.60504212975502 , diff:  17.60504212975502
adv train loss:  -17.548271872103214 , diff:  0.05677025765180588
adv train loss:  -17.210118271410465 , diff:  0.338153600692749
adv train loss:  -17.474043913185596 , diff:  0.2639256417751312
adv train loss:  -17.513546004891396 , diff:  0.0395020917057991
adv train loss:  -17.376945197582245 , diff:  0.1366008073091507
adv train loss:  -17.558445312082767 , diff:  0.1815001145005226
adv train loss:  -16.989582918584347 , diff:  0.5688623934984207
adv train loss:  -17.166579987853765 , diff:  0.17699706926941872
adv train loss:  -17.129519641399384 , diff:  0.03706034645438194
layer  0  adv train finish, try to retain  64
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
### skip layer  1 wait:  3  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  3  ###
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
adv train loss:  -11.731821186840534 , diff:  11.731821186840534
adv train loss:  -11.56998996809125 , diff:  0.16183121874928474
adv train loss:  -11.835785649716854 , diff:  0.26579568162560463
adv train loss:  -11.976571202278137 , diff:  0.1407855525612831
adv train loss:  -10.999618206173182 , diff:  0.9769529961049557
adv train loss:  -11.9556230828166 , diff:  0.9560048766434193
adv train loss:  -11.487979270517826 , diff:  0.4676438122987747
adv train loss:  -12.094770841300488 , diff:  0.6067915707826614
adv train loss:  -11.446796670556068 , diff:  0.6479741707444191
adv train loss:  -11.78206704929471 , diff:  0.33527037873864174
layer  8  adv train finish, try to retain  268
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
### skip layer  9 wait:  3  ###
---------------- start layer  10  ---------------
adv train loss:  -4741.460914611816 , diff:  4741.460914611816
adv train loss:  -4738.417797088623 , diff:  3.0431175231933594
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  8
test acc: top1 ->  19.46 ; top5 ->  67.69  and loss:  63344.718658447266
forward train acc: top1 ->  95.742 ; top5 ->  99.834  and loss:  21.389573744963855
test acc: top1 ->  90.94 ; top5 ->  98.67  and loss:  70.37082904577255
forward train acc: top1 ->  99.75399997802734 ; top5 ->  100.0  and loss:  0.8568534532096237
test acc: top1 ->  91.03 ; top5 ->  98.76  and loss:  69.14457154273987
forward train acc: top1 ->  99.85999997558594 ; top5 ->  100.0  and loss:  0.515831975499168
test acc: top1 ->  91.35 ; top5 ->  98.76  and loss:  67.16156849265099
forward train acc: top1 ->  99.906 ; top5 ->  100.0  and loss:  0.3733944086707197
test acc: top1 ->  91.29 ; top5 ->  98.75  and loss:  67.63354775309563
forward train acc: top1 ->  99.91199997558594 ; top5 ->  100.0  and loss:  0.31190363480709493
test acc: top1 ->  91.39 ; top5 ->  98.81  and loss:  66.93284663558006
forward train acc: top1 ->  99.93400000244141 ; top5 ->  100.0  and loss:  0.24869316321564838
test acc: top1 ->  91.32 ; top5 ->  98.82  and loss:  66.24990209937096
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.2621534236241132
test acc: top1 ->  91.46 ; top5 ->  98.85  and loss:  66.52680721879005
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.25565675215329975
test acc: top1 ->  91.61 ; top5 ->  98.77  and loss:  67.19881999492645
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.2103919158980716
test acc: top1 ->  91.51 ; top5 ->  98.85  and loss:  66.4158305823803
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.17915053220349364
test acc: top1 ->  91.5 ; top5 ->  98.84  and loss:  66.48786889016628
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  12 / 512 , inc:  4
---------------- start layer  11  ---------------
adv train loss:  -358.43178272247314 , diff:  358.43178272247314
adv train loss:  -385.6133608818054 , diff:  27.181578159332275
adv train loss:  -411.1945450305939 , diff:  25.581184148788452
adv train loss:  -411.65118169784546 , diff:  0.4566366672515869
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  85
test acc: top1 ->  10.2 ; top5 ->  50.01  and loss:  135693070.5
forward train acc: top1 ->  92.76999997558593 ; top5 ->  99.982  and loss:  47.44132475648075
test acc: top1 ->  90.59 ; top5 ->  98.29  and loss:  70.99649310112
forward train acc: top1 ->  99.79200000244141 ; top5 ->  100.0  and loss:  0.7964215534739196
test acc: top1 ->  91.3 ; top5 ->  98.35  and loss:  67.25608049333096
forward train acc: top1 ->  99.85 ; top5 ->  99.998  and loss:  0.5956835595425218
test acc: top1 ->  91.45 ; top5 ->  98.42  and loss:  65.78223460912704
forward train acc: top1 ->  99.89399997558594 ; top5 ->  100.0  and loss:  0.37716750451363623
test acc: top1 ->  91.62 ; top5 ->  98.44  and loss:  64.68257895112038
forward train acc: top1 ->  99.91999997558594 ; top5 ->  100.0  and loss:  0.28794397704768926
test acc: top1 ->  91.62 ; top5 ->  98.46  and loss:  65.228859603405
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.2627414787421003
test acc: top1 ->  91.68 ; top5 ->  98.44  and loss:  64.77568180859089
forward train acc: top1 ->  99.94999997558594 ; top5 ->  100.0  and loss:  0.2190441716229543
test acc: top1 ->  91.66 ; top5 ->  98.51  and loss:  65.421821013093
forward train acc: top1 ->  99.95599997558594 ; top5 ->  100.0  and loss:  0.18376961798639968
test acc: top1 ->  91.69 ; top5 ->  98.53  and loss:  65.42855553328991
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.18201809001038782
test acc: top1 ->  91.76 ; top5 ->  98.5  and loss:  65.24516180157661
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.16792161855846643
test acc: top1 ->  91.69 ; top5 ->  98.62  and loss:  65.36654552817345
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  97 / 512 , inc:  12
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
adv train loss:  -11947.864616394043 , diff:  11947.864616394043
adv train loss:  -20486.592239379883 , diff:  8538.72762298584
adv train loss:  -28466.077041625977 , diff:  7979.484802246094
adv train loss:  -36197.408599853516 , diff:  7731.331558227539
adv train loss:  -43517.024017333984 , diff:  7319.615417480469
adv train loss:  -47670.43728637695 , diff:  4153.413269042969
adv train loss:  -48157.96691894531 , diff:  487.5296325683594
adv train loss:  -48262.43283081055 , diff:  104.46591186523438
adv train loss:  -48339.36151123047 , diff:  76.92868041992188
adv train loss:  -48376.67370605469 , diff:  37.31219482421875
layer  13  adv train finish, try to retain  61
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  3
layer  8  :  0.455078125  ==>  233 / 512 , inc:  32
layer  9  :  0.126953125  ==>  65 / 512 , inc:  4
layer  10  :  0.0234375  ==>  12 / 512 , inc:  2
layer  11  :  0.189453125  ==>  97 / 512 , inc:  6
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.10546875  ==>  54 / 512 , inc:  4
eps [0.11533007812500001, 1.2301875000000002, 1.2301875000000002, 1.2301875000000002, 1.2301875000000002, 1.2301875000000002, 1.2301875000000002, 0.014416259765625002, 0.10251562500000001, 1.0935000000000001, 0.9226406250000001, 1.6402500000000002, 0.9226406250000001, 13.122000000000002]  wait [2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 4, 3, 2]  inc [1, 1, 1, 1, 1, 1, 1, 3, 32, 4, 2, 6, 1, 4]  tol: 3
$$$$$$$$$$$$$ epoch  34  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -23.788346510380507 , diff:  23.788346510380507
adv train loss:  -24.636214949190617 , diff:  0.8478684388101101
adv train loss:  -24.669727370142937 , diff:  0.0335124209523201
layer  0  adv train finish, try to retain  64
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -37.308952659368515 , diff:  37.308952659368515
adv train loss:  -36.93283562362194 , diff:  0.3761170357465744
adv train loss:  -38.166560128331184 , diff:  1.2337245047092438
adv train loss:  -41.956273406744 , diff:  3.789713278412819
adv train loss:  -46.91524887084961 , diff:  4.958975464105606
adv train loss:  -47.36398395895958 , diff:  0.4487350881099701
adv train loss:  -47.62056639790535 , diff:  0.25658243894577026
adv train loss:  -48.086029037833214 , diff:  0.4654626399278641
adv train loss:  -47.64718508720398 , diff:  0.4388439506292343
adv train loss:  -46.722975581884384 , diff:  0.9242095053195953
layer  1  adv train finish, try to retain  53
test acc: top1 ->  18.07 ; top5 ->  60.08  and loss:  1198.291657447815
forward train acc: top1 ->  99.762 ; top5 ->  99.998  and loss:  1.0574508794743451
test acc: top1 ->  91.78 ; top5 ->  98.94  and loss:  89.74114640057087
forward train acc: top1 ->  99.938 ; top5 ->  99.998  and loss:  0.2132011314843112
test acc: top1 ->  92.05 ; top5 ->  98.98  and loss:  87.64870975911617
forward train acc: top1 ->  99.912 ; top5 ->  99.998  and loss:  0.31991153546914575
test acc: top1 ->  91.94 ; top5 ->  98.99  and loss:  86.27712760865688
forward train acc: top1 ->  99.95199997558593 ; top5 ->  100.0  and loss:  0.1697387635922496
test acc: top1 ->  91.9 ; top5 ->  98.99  and loss:  87.57990653812885
forward train acc: top1 ->  99.95399997558594 ; top5 ->  100.0  and loss:  0.13780522474826284
test acc: top1 ->  91.92 ; top5 ->  99.02  and loss:  87.7576834410429
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.20040548437464167
test acc: top1 ->  91.91 ; top5 ->  98.99  and loss:  87.31321755051613
forward train acc: top1 ->  99.94799997558594 ; top5 ->  99.996  and loss:  0.16960961466247682
test acc: top1 ->  92.01 ; top5 ->  99.02  and loss:  86.41571718454361
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.12764423575754336
test acc: top1 ->  91.92 ; top5 ->  99.0  and loss:  85.31779325008392
forward train acc: top1 ->  99.9500000024414 ; top5 ->  100.0  and loss:  0.1387712323521555
test acc: top1 ->  91.87 ; top5 ->  98.97  and loss:  86.34991535544395
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.13016923447048612
test acc: top1 ->  92.0 ; top5 ->  99.02  and loss:  86.7323159724474
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  55 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -1923.334372508929 , diff:  1923.334372508929
adv train loss:  -3050.5114459991455 , diff:  1127.1770734902166
adv train loss:  -3040.995460510254 , diff:  9.515985488891602
adv train loss:  -3055.9619579315186 , diff:  14.966497421264648
adv train loss:  -3095.1546783447266 , diff:  39.19272041320801
adv train loss:  -3093.2908153533936 , diff:  1.8638629913330078
adv train loss:  -3092.210298538208 , diff:  1.0805168151855469
adv train loss:  -3083.9729537963867 , diff:  8.237344741821289
adv train loss:  -3101.0009021759033 , diff:  17.0279483795166
adv train loss:  -3092.03804397583 , diff:  8.962858200073242
layer  2  adv train finish, try to retain  84
test acc: top1 ->  35.15 ; top5 ->  80.21  and loss:  3827.3981037139893
forward train acc: top1 ->  99.656 ; top5 ->  100.0  and loss:  1.3679435388257843
test acc: top1 ->  91.02 ; top5 ->  99.05  and loss:  90.79298806190491
forward train acc: top1 ->  99.73799997558594 ; top5 ->  100.0  and loss:  0.9380136316176504
test acc: top1 ->  91.17 ; top5 ->  99.11  and loss:  86.22720044851303
forward train acc: top1 ->  99.77999997558594 ; top5 ->  100.0  and loss:  0.6814547575777397
test acc: top1 ->  91.28 ; top5 ->  99.1  and loss:  82.73834383487701
forward train acc: top1 ->  99.804 ; top5 ->  100.0  and loss:  0.6302562586351996
test acc: top1 ->  91.41 ; top5 ->  99.05  and loss:  80.09756916761398
forward train acc: top1 ->  99.8080000024414 ; top5 ->  100.0  and loss:  0.6327804858738091
test acc: top1 ->  91.19 ; top5 ->  99.15  and loss:  79.18424794077873
forward train acc: top1 ->  99.83799997558594 ; top5 ->  99.998  and loss:  0.5342778842459666
test acc: top1 ->  91.3 ; top5 ->  99.17  and loss:  77.22644105553627
forward train acc: top1 ->  99.852 ; top5 ->  100.0  and loss:  0.4571325682773022
test acc: top1 ->  91.46 ; top5 ->  99.16  and loss:  76.08945631980896
forward train acc: top1 ->  99.826 ; top5 ->  100.0  and loss:  0.5762906519448734
test acc: top1 ->  91.43 ; top5 ->  99.2  and loss:  73.7679100483656
forward train acc: top1 ->  99.864 ; top5 ->  100.0  and loss:  0.38374213644419797
test acc: top1 ->  91.39 ; top5 ->  99.19  and loss:  76.29309253394604
forward train acc: top1 ->  99.858 ; top5 ->  100.0  and loss:  0.4960949437081581
test acc: top1 ->  91.27 ; top5 ->  99.16  and loss:  73.58755128085613
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -1315.3908165038374 , diff:  1315.3908165038374
adv train loss:  -2261.7224254608154 , diff:  946.331608956978
adv train loss:  -2273.7153930664062 , diff:  11.99296760559082
adv train loss:  -2322.610855102539 , diff:  48.89546203613281
adv train loss:  -2606.524066925049 , diff:  283.91321182250977
adv train loss:  -2629.5625381469727 , diff:  23.038471221923828
adv train loss:  -2632.6486892700195 , diff:  3.086151123046875
adv train loss:  -2630.9819927215576 , diff:  1.666696548461914
adv train loss:  -2627.6816997528076 , diff:  3.30029296875
adv train loss:  -2629.4720134735107 , diff:  1.790313720703125
layer  3  adv train finish, try to retain  78
test acc: top1 ->  20.33 ; top5 ->  64.37  and loss:  5585.7730140686035
forward train acc: top1 ->  99.74399997558594 ; top5 ->  99.998  and loss:  0.9455992626026273
test acc: top1 ->  91.79 ; top5 ->  99.16  and loss:  68.42575983703136
forward train acc: top1 ->  99.774 ; top5 ->  100.0  and loss:  0.7174164354219101
test acc: top1 ->  91.88 ; top5 ->  99.25  and loss:  65.71160526573658
forward train acc: top1 ->  99.80599997558593 ; top5 ->  100.0  and loss:  0.5940481592842843
test acc: top1 ->  91.89 ; top5 ->  99.23  and loss:  66.9982139468193
forward train acc: top1 ->  99.842 ; top5 ->  99.998  and loss:  0.45679203198233154
test acc: top1 ->  91.78 ; top5 ->  99.2  and loss:  67.11763340234756
forward train acc: top1 ->  99.862 ; top5 ->  99.996  and loss:  0.40827384850854287
test acc: top1 ->  91.88 ; top5 ->  99.24  and loss:  68.62604670226574
forward train acc: top1 ->  99.86 ; top5 ->  100.0  and loss:  0.39105591268162243
test acc: top1 ->  91.89 ; top5 ->  99.2  and loss:  67.79721876978874
forward train acc: top1 ->  99.9 ; top5 ->  100.0  and loss:  0.31810739712091163
test acc: top1 ->  91.95 ; top5 ->  99.32  and loss:  67.39829930663109
forward train acc: top1 ->  99.87999997558593 ; top5 ->  100.0  and loss:  0.400660420069471
test acc: top1 ->  91.91 ; top5 ->  99.17  and loss:  67.37516023218632
forward train acc: top1 ->  99.88399997558594 ; top5 ->  100.0  and loss:  0.33468055517732864
test acc: top1 ->  91.74 ; top5 ->  99.18  and loss:  69.01728349924088
forward train acc: top1 ->  99.88799997558594 ; top5 ->  100.0  and loss:  0.362868189044093
test acc: top1 ->  91.82 ; top5 ->  99.22  and loss:  67.16918037831783
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -419.40189676910813 , diff:  419.40189676910813
adv train loss:  -2550.4147415161133 , diff:  2131.012844747005
adv train loss:  -2837.440553665161 , diff:  287.02581214904785
adv train loss:  -2884.584312438965 , diff:  47.14375877380371
adv train loss:  -2917.8406829833984 , diff:  33.256370544433594
adv train loss:  -2921.8996810913086 , diff:  4.058998107910156
adv train loss:  -2929.325351715088 , diff:  7.425670623779297
adv train loss:  -2933.7720470428467 , diff:  4.446695327758789
adv train loss:  -2962.6016178131104 , diff:  28.829570770263672
adv train loss:  -2984.9389152526855 , diff:  22.337297439575195
layer  4  adv train finish, try to retain  110
test acc: top1 ->  44.27 ; top5 ->  84.48  and loss:  3279.012035369873
forward train acc: top1 ->  97.81800000488282 ; top5 ->  99.956  and loss:  8.579104829579592
test acc: top1 ->  89.65 ; top5 ->  99.16  and loss:  60.95669423043728
forward train acc: top1 ->  98.44799998291016 ; top5 ->  99.986  and loss:  4.902890091761947
test acc: top1 ->  89.89 ; top5 ->  99.21  and loss:  52.80403183400631
forward train acc: top1 ->  98.55200000732422 ; top5 ->  99.99  and loss:  4.2196882562711835
test acc: top1 ->  90.22 ; top5 ->  99.2  and loss:  49.95934072136879
forward train acc: top1 ->  98.7440000048828 ; top5 ->  99.99  and loss:  3.7134994445368648
test acc: top1 ->  90.32 ; top5 ->  99.28  and loss:  50.69869866967201
forward train acc: top1 ->  98.91400000488281 ; top5 ->  99.996  and loss:  3.0649806573055685
test acc: top1 ->  90.38 ; top5 ->  99.23  and loss:  52.46458511054516
forward train acc: top1 ->  99.05999997802735 ; top5 ->  99.996  and loss:  2.8724689655937254
test acc: top1 ->  90.28 ; top5 ->  99.2  and loss:  52.5542277097702
forward train acc: top1 ->  98.97599998291015 ; top5 ->  99.998  and loss:  2.925154473632574
test acc: top1 ->  90.3 ; top5 ->  99.2  and loss:  51.68251855671406
forward train acc: top1 ->  99.12999998046875 ; top5 ->  99.99400000244141  and loss:  2.651287286542356
test acc: top1 ->  90.56 ; top5 ->  99.27  and loss:  50.6127427816391
forward train acc: top1 ->  99.06200000244141 ; top5 ->  99.992  and loss:  2.6468079243786633
test acc: top1 ->  90.46 ; top5 ->  99.24  and loss:  50.73018208146095
forward train acc: top1 ->  99.19799997558594 ; top5 ->  100.0  and loss:  2.358292526565492
test acc: top1 ->  90.45 ; top5 ->  99.25  and loss:  51.06837385892868
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -641.3282675336814 , diff:  641.3282675336814
adv train loss:  -2504.0886554718018 , diff:  1862.7603879381204
adv train loss:  -2650.951675415039 , diff:  146.8630199432373
adv train loss:  -2665.7122802734375 , diff:  14.760604858398438
adv train loss:  -2689.912353515625 , diff:  24.2000732421875
adv train loss:  -2710.2667598724365 , diff:  20.354406356811523
adv train loss:  -2708.870756149292 , diff:  1.3960037231445312
adv train loss:  -2732.7740726470947 , diff:  23.903316497802734
adv train loss:  -2733.6127796173096 , diff:  0.8387069702148438
adv train loss:  -2744.879686355591 , diff:  11.26690673828125
layer  5  adv train finish, try to retain  119
test acc: top1 ->  48.79 ; top5 ->  90.99  and loss:  3387.061756134033
forward train acc: top1 ->  99.48799997558594 ; top5 ->  99.998  and loss:  1.5611746995709836
test acc: top1 ->  91.19 ; top5 ->  99.23  and loss:  52.055590122938156
forward train acc: top1 ->  99.59199997558594 ; top5 ->  99.998  and loss:  1.1812417343608104
test acc: top1 ->  91.12 ; top5 ->  99.25  and loss:  54.32420486211777
forward train acc: top1 ->  99.712 ; top5 ->  100.0  and loss:  0.8051247280091047
test acc: top1 ->  91.42 ; top5 ->  99.28  and loss:  55.68874027580023
forward train acc: top1 ->  99.7400000024414 ; top5 ->  100.0  and loss:  0.7881860200432129
test acc: top1 ->  91.35 ; top5 ->  99.28  and loss:  58.90482284873724
forward train acc: top1 ->  99.76 ; top5 ->  100.0  and loss:  0.6615978794870898
test acc: top1 ->  91.38 ; top5 ->  99.27  and loss:  59.05402198433876
forward train acc: top1 ->  99.814 ; top5 ->  100.0  and loss:  0.5732842354336753
test acc: top1 ->  91.42 ; top5 ->  99.31  and loss:  58.762990310788155
forward train acc: top1 ->  99.802 ; top5 ->  100.0  and loss:  0.5964493080973625
test acc: top1 ->  91.4 ; top5 ->  99.27  and loss:  58.86658784747124
forward train acc: top1 ->  99.824 ; top5 ->  100.0  and loss:  0.5099343967158347
test acc: top1 ->  91.46 ; top5 ->  99.26  and loss:  59.97926779836416
forward train acc: top1 ->  99.83199997558594 ; top5 ->  100.0  and loss:  0.4965924611897208
test acc: top1 ->  91.39 ; top5 ->  99.33  and loss:  60.24971540272236
forward train acc: top1 ->  99.83999997558594 ; top5 ->  100.0  and loss:  0.4588601640716661
test acc: top1 ->  91.29 ; top5 ->  99.29  and loss:  60.611557736992836
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -815.876283516176 , diff:  815.876283516176
adv train loss:  -2903.3505115509033 , diff:  2087.4742280347273
adv train loss:  -2977.2351455688477 , diff:  73.88463401794434
adv train loss:  -2983.2741203308105 , diff:  6.038974761962891
adv train loss:  -3007.6575679779053 , diff:  24.383447647094727
adv train loss:  -3042.5403900146484 , diff:  34.882822036743164
adv train loss:  -3052.7861881256104 , diff:  10.245798110961914
adv train loss:  -3057.8835315704346 , diff:  5.097343444824219
adv train loss:  -3097.2901096343994 , diff:  39.406578063964844
adv train loss:  -3131.5424880981445 , diff:  34.25237846374512
layer  6  adv train finish, try to retain  127
test acc: top1 ->  68.66 ; top5 ->  98.22  and loss:  1286.2056412696838
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.21948142515611835
test acc: top1 ->  91.62 ; top5 ->  99.42  and loss:  62.87081031501293
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.16602069589862367
test acc: top1 ->  91.78 ; top5 ->  99.31  and loss:  63.81255751848221
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.10994861847575521
test acc: top1 ->  91.91 ; top5 ->  99.4  and loss:  66.05702416598797
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.08128455661062617
test acc: top1 ->  91.85 ; top5 ->  99.34  and loss:  67.74070265889168
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.1136595826537814
test acc: top1 ->  91.95 ; top5 ->  99.29  and loss:  70.39808082580566
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.07994619115925161
test acc: top1 ->  92.02 ; top5 ->  99.4  and loss:  70.00595954060555
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.09852841049359995
test acc: top1 ->  91.99 ; top5 ->  99.37  and loss:  70.16088984906673
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.09274378333248023
test acc: top1 ->  91.85 ; top5 ->  99.31  and loss:  72.01539169251919
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.052362450824148254
test acc: top1 ->  92.0 ; top5 ->  99.35  and loss:  70.07645655423403
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.05434015453738539
test acc: top1 ->  91.89 ; top5 ->  99.35  and loss:  72.16471299529076
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  144 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -0.10678058371740917 , diff:  0.10678058371740917
adv train loss:  -0.1300411561933288 , diff:  0.023260572475919616
adv train loss:  -0.13759503553592367 , diff:  0.0075538793425948825
layer  7  adv train finish, try to retain  255
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -0.12459681447944604 , diff:  0.12459681447944604
adv train loss:  -0.1666687040560646 , diff:  0.04207188957661856
adv train loss:  -0.1078350214047532 , diff:  0.05883368265131139
adv train loss:  -0.14056731372147624 , diff:  0.03273229231672303
adv train loss:  -0.0964628138790431 , diff:  0.04410449984243314
adv train loss:  -0.13150122454680968 , diff:  0.035038410667766584
adv train loss:  -0.15865687507903203 , diff:  0.027155650532222353
adv train loss:  -0.1868881535192486 , diff:  0.028231278440216556
adv train loss:  -0.1476293248560978 , diff:  0.03925882866315078
adv train loss:  -0.10224394379838486 , diff:  0.04538538105771295
layer  8  adv train finish, try to retain  263
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -0.6774217051570304 , diff:  0.6774217051570304
adv train loss:  -0.5893898721551523 , diff:  0.08803183300187811
adv train loss:  -0.6156473874580115 , diff:  0.026257515302859247
adv train loss:  -0.6658732835785486 , diff:  0.05022589612053707
adv train loss:  -0.6664615484769456 , diff:  0.0005882648983970284
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  61
test acc: top1 ->  10.6 ; top5 ->  52.59  and loss:  426654.05029296875
forward train acc: top1 ->  91.93999998046876 ; top5 ->  99.918  and loss:  56.78493116796017
test acc: top1 ->  89.21 ; top5 ->  98.51  and loss:  79.42702738940716
forward train acc: top1 ->  99.466 ; top5 ->  100.0  and loss:  1.8123741429299116
test acc: top1 ->  90.05 ; top5 ->  98.56  and loss:  72.53304570913315
forward train acc: top1 ->  99.64 ; top5 ->  100.0  and loss:  1.183179740095511
test acc: top1 ->  90.48 ; top5 ->  98.57  and loss:  70.15777732431889
forward train acc: top1 ->  99.772 ; top5 ->  99.998  and loss:  0.8140419743722305
test acc: top1 ->  90.87 ; top5 ->  98.62  and loss:  68.75273931026459
forward train acc: top1 ->  99.86199997558593 ; top5 ->  100.0  and loss:  0.55644815065898
test acc: top1 ->  90.93 ; top5 ->  98.63  and loss:  69.01919128000736
forward train acc: top1 ->  99.854 ; top5 ->  100.0  and loss:  0.4942966070957482
test acc: top1 ->  91.02 ; top5 ->  98.65  and loss:  69.32279506325722
forward train acc: top1 ->  99.89199997558593 ; top5 ->  100.0  and loss:  0.3884882600978017
test acc: top1 ->  91.15 ; top5 ->  98.61  and loss:  69.35352654755116
forward train acc: top1 ->  99.90200000244141 ; top5 ->  100.0  and loss:  0.38211923761991784
test acc: top1 ->  91.19 ; top5 ->  98.65  and loss:  70.18784557282925
forward train acc: top1 ->  99.864 ; top5 ->  100.0  and loss:  0.41081065632170066
test acc: top1 ->  91.22 ; top5 ->  98.66  and loss:  68.9573830217123
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.2796954801888205
test acc: top1 ->  91.23 ; top5 ->  98.68  and loss:  68.71952003240585
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  65 / 512 , inc:  4
---------------- start layer  10  ---------------
adv train loss:  -3.756970800925046 , diff:  3.756970800925046
adv train loss:  -3.4740397296845913 , diff:  0.2829310712404549
adv train loss:  -3.582409145310521 , diff:  0.10836941562592983
adv train loss:  -3.5997504009865224 , diff:  0.01734125567600131
adv train loss:  -3.846447369083762 , diff:  0.24669696809723973
adv train loss:  -3.476573134539649 , diff:  0.3698742345441133
adv train loss:  -3.615161055698991 , diff:  0.13858792115934193
adv train loss:  -3.484747104346752 , diff:  0.13041395135223866
adv train loss:  -3.7150314189493656 , diff:  0.23028431460261345
adv train loss:  -3.4601765563711524 , diff:  0.2548548625782132
layer  10  adv train finish, try to retain  493
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
### skip layer  11 wait:  4  ###
---------------- start layer  12  ---------------
### skip layer  12 wait:  3  ###
---------------- start layer  13  ---------------
adv train loss:  -11370.342346191406 , diff:  11370.342346191406
adv train loss:  -19777.755264282227 , diff:  8407.41291809082
adv train loss:  -27638.504348754883 , diff:  7860.749084472656
adv train loss:  -35265.73944091797 , diff:  7627.235092163086
adv train loss:  -42782.54699707031 , diff:  7516.807556152344
adv train loss:  -50193.93209838867 , diff:  7411.385101318359
adv train loss:  -57589.840576171875 , diff:  7395.908477783203
adv train loss:  -64942.69494628906 , diff:  7352.8543701171875
adv train loss:  -72256.15203857422 , diff:  7313.457092285156
adv train loss:  -79571.90307617188 , diff:  7315.751037597656
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  50
test acc: top1 ->  87.04 ; top5 ->  98.47  and loss:  128.276479691267
forward train acc: top1 ->  99.84999997558593 ; top5 ->  100.0  and loss:  0.5094245608743222
test acc: top1 ->  91.74 ; top5 ->  99.17  and loss:  79.05780480802059
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.06283290142164333
test acc: top1 ->  91.77 ; top5 ->  99.19  and loss:  78.82172881066799
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.05985165772290202
test acc: top1 ->  91.86 ; top5 ->  99.2  and loss:  79.57847581803799
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.03548071885597892
test acc: top1 ->  91.99 ; top5 ->  99.15  and loss:  79.11760248243809
forward train acc: top1 ->  99.98599997558594 ; top5 ->  100.0  and loss:  0.0357736532605486
test acc: top1 ->  91.9 ; top5 ->  99.2  and loss:  81.96131254732609
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  54 / 512 , inc:  4
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  3
layer  8  :  0.455078125  ==>  233 / 512 , inc:  32
layer  9  :  0.126953125  ==>  65 / 512 , inc:  2
layer  10  :  0.0234375  ==>  12 / 512 , inc:  2
layer  11  :  0.189453125  ==>  97 / 512 , inc:  6
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.10546875  ==>  54 / 512 , inc:  2
eps [0.23066015625000003, 0.9226406250000001, 0.9226406250000001, 0.9226406250000001, 0.9226406250000001, 0.9226406250000001, 0.9226406250000001, 0.028832519531250003, 0.20503125000000003, 0.8201250000000001, 1.8452812500000002, 1.6402500000000002, 0.9226406250000001, 9.841500000000002]  wait [2, 4, 4, 4, 4, 4, 4, 2, 0, 4, 2, 3, 2, 4]  inc [1, 1, 1, 1, 1, 1, 1, 3, 32, 2, 2, 6, 1, 2]  tol: 3
$$$$$$$$$$$$$ epoch  35  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -0.3239622366872936 , diff:  0.3239622366872936
adv train loss:  -0.31990089088503737 , diff:  0.004061345802256255
layer  0  adv train finish, try to retain  64
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
### skip layer  1 wait:  4  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
adv train loss:  -0.3427898935187841 , diff:  0.3427898935187841
adv train loss:  -0.3504090092537808 , diff:  0.00761911573499674
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  250
test acc: top1 ->  31.57 ; top5 ->  74.88  and loss:  464.21985697746277
forward train acc: top1 ->  97.01199997558594 ; top5 ->  99.98  and loss:  12.831703970208764
test acc: top1 ->  89.71 ; top5 ->  98.73  and loss:  83.74060475826263
forward train acc: top1 ->  99.27999997558594 ; top5 ->  99.998  and loss:  2.396828797645867
test acc: top1 ->  90.53 ; top5 ->  98.82  and loss:  77.39886094629765
forward train acc: top1 ->  99.52399997802735 ; top5 ->  100.0  and loss:  1.5415433761663735
test acc: top1 ->  90.58 ; top5 ->  98.97  and loss:  74.64267033338547
forward train acc: top1 ->  99.66399997558594 ; top5 ->  100.0  and loss:  1.0295122872339562
test acc: top1 ->  90.76 ; top5 ->  99.0  and loss:  73.07156610488892
forward train acc: top1 ->  99.742 ; top5 ->  99.998  and loss:  0.8550820582313463
test acc: top1 ->  90.79 ; top5 ->  99.03  and loss:  71.48432625830173
forward train acc: top1 ->  99.76 ; top5 ->  100.0  and loss:  0.8073591245338321
test acc: top1 ->  90.91 ; top5 ->  99.01  and loss:  71.41507606208324
forward train acc: top1 ->  99.81 ; top5 ->  100.0  and loss:  0.600023970589973
test acc: top1 ->  90.91 ; top5 ->  99.05  and loss:  72.08583375811577
forward train acc: top1 ->  99.80999997558594 ; top5 ->  99.998  and loss:  0.600715851818677
test acc: top1 ->  91.01 ; top5 ->  99.06  and loss:  72.31793561577797
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  253 / 512 , inc:  3
---------------- start layer  8  ---------------
adv train loss:  -1.1349205810111016 , diff:  1.1349205810111016
adv train loss:  -1.201536904554814 , diff:  0.0666163235437125
adv train loss:  -1.1560066228266805 , diff:  0.04553028172813356
adv train loss:  -1.3402127975132316 , diff:  0.1842061746865511
adv train loss:  -1.2586090568220243 , diff:  0.08160374069120735
adv train loss:  -1.289200919214636 , diff:  0.0305918623926118
adv train loss:  -1.2956292796880007 , diff:  0.006428360473364592
layer  8  adv train finish, try to retain  256
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
### skip layer  9 wait:  4  ###
---------------- start layer  10  ---------------
adv train loss:  -636.8274731636047 , diff:  636.8274731636047
adv train loss:  -637.584273815155 , diff:  0.756800651550293
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  10
test acc: top1 ->  28.87 ; top5 ->  67.03  and loss:  44364.95233154297
forward train acc: top1 ->  95.03799997558593 ; top5 ->  99.992  and loss:  31.274558193748817
test acc: top1 ->  91.51 ; top5 ->  98.72  and loss:  82.14716257154942
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.3059940261300653
test acc: top1 ->  91.64 ; top5 ->  98.63  and loss:  81.14250241219997
forward train acc: top1 ->  99.88999997558594 ; top5 ->  100.0  and loss:  0.361156705301255
test acc: top1 ->  91.77 ; top5 ->  98.66  and loss:  79.05916304886341
forward train acc: top1 ->  99.94999997558594 ; top5 ->  100.0  and loss:  0.17869718791916966
test acc: top1 ->  91.97 ; top5 ->  98.66  and loss:  79.27869673073292
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.16582705650944263
test acc: top1 ->  91.93 ; top5 ->  98.65  and loss:  79.42085656523705
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.16289373728795908
test acc: top1 ->  92.01 ; top5 ->  98.7  and loss:  79.07220609486103
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.097009337390773
test acc: top1 ->  92.0 ; top5 ->  98.72  and loss:  78.92547233402729
forward train acc: top1 ->  99.97199997558593 ; top5 ->  100.0  and loss:  0.09529192768968642
test acc: top1 ->  92.04 ; top5 ->  98.76  and loss:  79.19229955971241
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.11704923029174097
test acc: top1 ->  92.09 ; top5 ->  98.76  and loss:  78.15729223191738
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.11418145967763849
test acc: top1 ->  92.09 ; top5 ->  98.79  and loss:  78.85392938554287
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  12 / 512 , inc:  2
---------------- start layer  11  ---------------
### skip layer  11 wait:  3  ###
---------------- start layer  12  ---------------
adv train loss:  -12.641517776995897 , diff:  12.641517776995897
adv train loss:  -13.351053282618523 , diff:  0.7095355056226254
adv train loss:  -12.72057131305337 , diff:  0.6304819695651531
adv train loss:  -12.86771024018526 , diff:  0.14713892713189125
adv train loss:  -12.599844977259636 , diff:  0.26786526292562485
adv train loss:  -12.703776778653264 , diff:  0.10393180139362812
adv train loss:  -12.604530476033688 , diff:  0.09924630261957645
adv train loss:  -13.469923250377178 , diff:  0.8653927743434906
adv train loss:  -13.099992237985134 , diff:  0.36993101239204407
adv train loss:  -12.560676079243422 , diff:  0.5393161587417126
layer  12  adv train finish, try to retain  483
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
### skip layer  13 wait:  4  ###
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.455078125  ==>  233 / 512 , inc:  32
layer  9  :  0.126953125  ==>  65 / 512 , inc:  2
layer  10  :  0.0234375  ==>  12 / 512 , inc:  1
layer  11  :  0.189453125  ==>  97 / 512 , inc:  6
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.10546875  ==>  54 / 512 , inc:  2
eps [0.46132031250000005, 0.9226406250000001, 0.9226406250000001, 0.9226406250000001, 0.9226406250000001, 0.9226406250000001, 0.9226406250000001, 0.021624389648437502, 0.41006250000000005, 0.8201250000000001, 1.3839609375000002, 1.6402500000000002, 1.8452812500000002, 9.841500000000002]  wait [2, 3, 3, 3, 3, 3, 3, 4, 0, 3, 4, 2, 2, 3]  inc [1, 1, 1, 1, 1, 1, 1, 1, 32, 2, 1, 6, 1, 2]  tol: 3
$$$$$$$$$$$$$ epoch  36  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -8.268778340891004 , diff:  8.268778340891004
adv train loss:  -8.164885401725769 , diff:  0.10389293916523457
adv train loss:  -8.630458937957883 , diff:  0.46557353623211384
adv train loss:  -8.255287120118737 , diff:  0.37517181783914566
adv train loss:  -8.490228222683072 , diff:  0.23494110256433487
adv train loss:  -8.392327120527625 , diff:  0.097901102155447
adv train loss:  -8.409371538087726 , diff:  0.017044417560100555
adv train loss:  -8.380873024463654 , diff:  0.028498513624072075
adv train loss:  -7.841007173061371 , diff:  0.5398658514022827
adv train loss:  -8.034641467034817 , diff:  0.1936342939734459
layer  0  adv train finish, try to retain  64
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
### skip layer  1 wait:  3  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  3  ###
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
adv train loss:  -12.54061172902584 , diff:  12.54061172902584
adv train loss:  -12.22956708446145 , diff:  0.3110446445643902
adv train loss:  -12.52828162908554 , diff:  0.2987145446240902
adv train loss:  -12.082096930593252 , diff:  0.4461846984922886
adv train loss:  -12.59366051480174 , diff:  0.5115635842084885
adv train loss:  -12.435178965330124 , diff:  0.15848154947161674
adv train loss:  -12.467161867767572 , diff:  0.0319829024374485
adv train loss:  -13.091195531189442 , diff:  0.6240336634218693
adv train loss:  -12.79228476062417 , diff:  0.2989107705652714
adv train loss:  -11.875912304967642 , diff:  0.9163724556565285
layer  8  adv train finish, try to retain  261
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
### skip layer  9 wait:  3  ###
---------------- start layer  10  ---------------
### skip layer  10 wait:  4  ###
---------------- start layer  11  ---------------
adv train loss:  -332.35129165649414 , diff:  332.35129165649414
adv train loss:  -463.5873579978943 , diff:  131.23606634140015
adv train loss:  -541.4011073112488 , diff:  77.81374931335449
adv train loss:  -566.6892647743225 , diff:  25.28815746307373
adv train loss:  -651.6414103507996 , diff:  84.95214557647705
adv train loss:  -651.5524706840515 , diff:  0.08893966674804688
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  91
test acc: top1 ->  10.27 ; top5 ->  50.0  and loss:  93564232.4375
forward train acc: top1 ->  99.192 ; top5 ->  99.996  and loss:  2.776051183580421
test acc: top1 ->  92.04 ; top5 ->  98.96  and loss:  61.32323735952377
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.12759783562796656
test acc: top1 ->  92.01 ; top5 ->  98.94  and loss:  60.7117300927639
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.09411572289536707
test acc: top1 ->  92.06 ; top5 ->  98.97  and loss:  62.21025983989239
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.07599768308864441
test acc: top1 ->  92.17 ; top5 ->  99.01  and loss:  62.09180203080177
==> this epoch:  91 / 512
---------------- start layer  12  ---------------
adv train loss:  -454.79505552351475 , diff:  454.79505552351475
adv train loss:  -877.1301116943359 , diff:  422.3350561708212
adv train loss:  -1011.7705030441284 , diff:  134.64039134979248
adv train loss:  -1151.2482872009277 , diff:  139.47778415679932
adv train loss:  -1227.9260520935059 , diff:  76.67776489257812
adv train loss:  -1251.2064571380615 , diff:  23.280405044555664
adv train loss:  -1349.2223720550537 , diff:  98.01591491699219
adv train loss:  -1324.2309846878052 , diff:  24.991387367248535
adv train loss:  -1324.4154682159424 , diff:  0.18448352813720703
adv train loss:  -1323.448634147644 , diff:  0.9668340682983398
layer  12  adv train finish, try to retain  18
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  2359.0985565185547
forward train acc: top1 ->  76.44799997802734 ; top5 ->  90.612  and loss:  182.24886605143547
test acc: top1 ->  90.51 ; top5 ->  98.46  and loss:  40.8905334174633
forward train acc: top1 ->  99.86999997558594 ; top5 ->  100.0  and loss:  6.269539307802916
test acc: top1 ->  91.29 ; top5 ->  98.69  and loss:  35.97895531356335
forward train acc: top1 ->  99.912 ; top5 ->  100.0  and loss:  3.0167798828333616
test acc: top1 ->  91.59 ; top5 ->  98.78  and loss:  35.24234339594841
forward train acc: top1 ->  99.94999997558594 ; top5 ->  100.0  and loss:  1.7367433598265052
test acc: top1 ->  91.78 ; top5 ->  98.83  and loss:  35.28093422949314
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  1.1226993226446211
test acc: top1 ->  91.9 ; top5 ->  98.84  and loss:  35.73173774778843
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.8232259154319763
test acc: top1 ->  91.87 ; top5 ->  98.96  and loss:  36.14000768959522
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.7094540614634752
test acc: top1 ->  91.93 ; top5 ->  98.89  and loss:  36.45411107689142
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.643905519740656
test acc: top1 ->  91.9 ; top5 ->  98.92  and loss:  36.869267486035824
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.5204161182045937
test acc: top1 ->  92.01 ; top5 ->  98.92  and loss:  37.19925609976053
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.44534764741547406
test acc: top1 ->  92.02 ; top5 ->  98.89  and loss:  37.418502911925316
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  62 / 512 , inc:  1
---------------- start layer  13  ---------------
### skip layer  13 wait:  3  ###
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.455078125  ==>  233 / 512 , inc:  32
layer  9  :  0.126953125  ==>  65 / 512 , inc:  2
layer  10  :  0.0234375  ==>  12 / 512 , inc:  1
layer  11  :  0.177734375  ==>  91 / 512 , inc:  12
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.10546875  ==>  54 / 512 , inc:  2
eps [0.9226406250000001, 0.9226406250000001, 0.9226406250000001, 0.9226406250000001, 0.9226406250000001, 0.9226406250000001, 0.9226406250000001, 0.021624389648437502, 0.8201250000000001, 0.8201250000000001, 1.3839609375000002, 1.6402500000000002, 1.3839609375000002, 9.841500000000002]  wait [2, 2, 2, 2, 2, 2, 2, 3, 0, 2, 3, 0, 4, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 32, 2, 1, 12, 1, 2]  tol: 3
$$$$$$$$$$$$$ epoch  37  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -349.4735219478607 , diff:  349.4735219478607
adv train loss:  -350.9410901069641 , diff:  1.4675681591033936
adv train loss:  -349.676057100296 , diff:  1.2650330066680908
adv train loss:  -349.64174818992615 , diff:  0.03430891036987305
layer  0  adv train finish, try to retain  63
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -349.7829749584198 , diff:  349.7829749584198
adv train loss:  -349.14917612075806 , diff:  0.6337988376617432
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -350.3712146282196 , diff:  350.3712146282196
adv train loss:  -351.00566601753235 , diff:  0.6344513893127441
layer  2  adv train finish, try to retain  123
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -350.32440423965454 , diff:  350.32440423965454
adv train loss:  -350.5035831928253 , diff:  0.17917895317077637
layer  3  adv train finish, try to retain  123
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -348.44825553894043 , diff:  348.44825553894043
adv train loss:  -350.27260088920593 , diff:  1.824345350265503
adv train loss:  -349.9667944908142 , diff:  0.30580639839172363
layer  4  adv train finish, try to retain  226
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -350.1867735385895 , diff:  350.1867735385895
adv train loss:  -350.37327551841736 , diff:  0.18650197982788086
layer  5  adv train finish, try to retain  225
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -349.67679476737976 , diff:  349.67679476737976
adv train loss:  -350.3549544811249 , diff:  0.6781597137451172
layer  6  adv train finish, try to retain  207
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
adv train loss:  -282.3235720396042 , diff:  282.3235720396042
adv train loss:  -281.2126352787018 , diff:  1.1109367609024048
adv train loss:  -282.2384617328644 , diff:  1.0258264541625977
adv train loss:  -281.56311333179474 , diff:  0.6753484010696411
adv train loss:  -282.72775530815125 , diff:  1.1646419763565063
adv train loss:  -279.7824127674103 , diff:  2.945342540740967
adv train loss:  -282.1276453733444 , diff:  2.345232605934143
adv train loss:  -281.08725130558014 , diff:  1.0403940677642822
adv train loss:  -282.30277609825134 , diff:  1.2155247926712036
adv train loss:  -282.59584045410156 , diff:  0.2930643558502197
layer  8  adv train finish, try to retain  263
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -406.77742290496826 , diff:  406.77742290496826
adv train loss:  -407.33169293403625 , diff:  0.5542700290679932
layer  9  adv train finish, try to retain  444
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
### skip layer  10 wait:  3  ###
---------------- start layer  11  ---------------
adv train loss:  -3345.7776679992676 , diff:  3345.7776679992676
adv train loss:  -3515.546527862549 , diff:  169.76885986328125
adv train loss:  -3584.12056350708 , diff:  68.57403564453125
adv train loss:  -3626.0471725463867 , diff:  41.92660903930664
adv train loss:  -3705.2779579162598 , diff:  79.23078536987305
adv train loss:  -4327.916854858398 , diff:  622.6388969421387
adv train loss:  -4857.735916137695 , diff:  529.8190612792969
adv train loss:  -4862.44197845459 , diff:  4.706062316894531
adv train loss:  -4861.070804595947 , diff:  1.3711738586425781
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  79
test acc: top1 ->  18.33 ; top5 ->  50.82  and loss:  56699657.0
forward train acc: top1 ->  98.018 ; top5 ->  99.994  and loss:  17.753388672543224
test acc: top1 ->  91.78 ; top5 ->  98.91  and loss:  97.99689638614655
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.18770823499107792
test acc: top1 ->  91.91 ; top5 ->  99.04  and loss:  92.3246051222086
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.097858900526262
test acc: top1 ->  92.02 ; top5 ->  99.12  and loss:  91.74351498484612
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.06084087332783383
test acc: top1 ->  92.14 ; top5 ->  99.16  and loss:  88.59913775324821
==> this epoch:  79 / 512
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
adv train loss:  -8120.5082149505615 , diff:  8120.5082149505615
adv train loss:  -16633.60108947754 , diff:  8513.092874526978
adv train loss:  -23548.733139038086 , diff:  6915.132049560547
adv train loss:  -30127.30532836914 , diff:  6578.572189331055
adv train loss:  -36570.598876953125 , diff:  6443.293548583984
adv train loss:  -42935.71206665039 , diff:  6365.113189697266
adv train loss:  -49275.57434082031 , diff:  6339.862274169922
adv train loss:  -55263.62536621094 , diff:  5988.051025390625
adv train loss:  -58646.438537597656 , diff:  3382.8131713867188
adv train loss:  -59247.647399902344 , diff:  601.2088623046875
layer  13  adv train finish, try to retain  53
test acc: top1 ->  64.78 ; top5 ->  98.3  and loss:  322.81696486473083
forward train acc: top1 ->  98.494 ; top5 ->  100.0  and loss:  6.803371871908894
test acc: top1 ->  91.69 ; top5 ->  98.95  and loss:  97.38530173897743
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.0773705095925834
test acc: top1 ->  91.98 ; top5 ->  99.04  and loss:  95.2461142539978
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.06760606849184114
test acc: top1 ->  91.94 ; top5 ->  99.08  and loss:  95.77616167068481
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.04326249455334619
test acc: top1 ->  92.14 ; top5 ->  99.08  and loss:  94.85521823167801
==> this epoch:  53 / 512
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.455078125  ==>  233 / 512 , inc:  32
layer  9  :  0.126953125  ==>  65 / 512 , inc:  2
layer  10  :  0.0234375  ==>  12 / 512 , inc:  1
layer  11  :  0.154296875  ==>  79 / 512 , inc:  24
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.103515625  ==>  53 / 512 , inc:  4
eps [1.8452812500000002, 1.8452812500000002, 1.8452812500000002, 1.8452812500000002, 1.8452812500000002, 1.8452812500000002, 1.8452812500000002, 0.021624389648437502, 1.6402500000000002, 1.6402500000000002, 1.3839609375000002, 1.6402500000000002, 1.3839609375000002, 9.841500000000002]  wait [2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 3, 0]  inc [1, 1, 1, 1, 1, 1, 1, 1, 32, 2, 1, 24, 1, 4]  tol: 3
$$$$$$$$$$$$$ epoch  38  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1422.6772607604507 , diff:  1422.6772607604507
adv train loss:  -1808.7319984436035 , diff:  386.0547376831528
adv train loss:  -2128.7603549957275 , diff:  320.028356552124
adv train loss:  -2400.4891662597656 , diff:  271.7288112640381
adv train loss:  -2579.5455265045166 , diff:  179.05636024475098
adv train loss:  -2585.20112991333 , diff:  5.655603408813477
adv train loss:  -2582.569719314575 , diff:  2.631410598754883
adv train loss:  -2576.4050159454346 , diff:  6.164703369140625
adv train loss:  -2580.1691150665283 , diff:  3.76409912109375
adv train loss:  -2591.0324382781982 , diff:  10.863323211669922
layer  0  adv train finish, try to retain  5
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  8786.665565490723
forward train acc: top1 ->  71.38399998535156 ; top5 ->  94.94400001464844  and loss:  230.48937624692917
test acc: top1 ->  33.38 ; top5 ->  72.41  and loss:  404.7161660194397
forward train acc: top1 ->  74.65 ; top5 ->  96.13999999023437  and loss:  86.89161241054535
test acc: top1 ->  74.22 ; top5 ->  96.13  and loss:  86.72668826580048
forward train acc: top1 ->  76.91000000488282 ; top5 ->  96.82399998046876  and loss:  72.87235879898071
test acc: top1 ->  75.79 ; top5 ->  96.77  and loss:  78.71261179447174
forward train acc: top1 ->  78.32000000488281 ; top5 ->  97.45599998291016  and loss:  66.76542180776596
test acc: top1 ->  76.72 ; top5 ->  97.07  and loss:  74.1044490635395
forward train acc: top1 ->  79.83600000976563 ; top5 ->  97.80999998046875  and loss:  61.92910146713257
test acc: top1 ->  78.1 ; top5 ->  97.44  and loss:  70.49870690703392
forward train acc: top1 ->  80.90599997314453 ; top5 ->  97.99799997802734  and loss:  58.35687631368637
test acc: top1 ->  78.39 ; top5 ->  97.5  and loss:  69.23768475651741
forward train acc: top1 ->  81.432 ; top5 ->  98.05199997802734  and loss:  56.998409539461136
test acc: top1 ->  79.16 ; top5 ->  97.59  and loss:  67.53041353821754
forward train acc: top1 ->  82.30399997558594 ; top5 ->  98.16199997802734  and loss:  54.64607709646225
test acc: top1 ->  79.68 ; top5 ->  97.66  and loss:  66.01646867394447
forward train acc: top1 ->  82.79599999023438 ; top5 ->  98.32200000732422  and loss:  52.842756658792496
test acc: top1 ->  80.18 ; top5 ->  97.59  and loss:  65.31081920862198
forward train acc: top1 ->  83.13599997070312 ; top5 ->  98.39600000732422  and loss:  51.456619560718536
test acc: top1 ->  80.75 ; top5 ->  97.79  and loss:  63.60563763976097
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -9.6969930306077 , diff:  9.6969930306077
adv train loss:  -9.88311130553484 , diff:  0.18611827492713928
adv train loss:  -9.873931884765625 , diff:  0.00917942076921463
layer  1  adv train finish, try to retain  41
test acc: top1 ->  35.32 ; top5 ->  79.87  and loss:  202.79594254493713
forward train acc: top1 ->  98.40200000732422 ; top5 ->  99.95599997558594  and loss:  5.544802226126194
test acc: top1 ->  90.64 ; top5 ->  99.2  and loss:  40.24287198483944
forward train acc: top1 ->  99.16600000244141 ; top5 ->  99.992  and loss:  2.6019281297922134
test acc: top1 ->  91.12 ; top5 ->  99.21  and loss:  42.1178312599659
forward train acc: top1 ->  99.40199997558594 ; top5 ->  99.984  and loss:  1.813250343548134
test acc: top1 ->  91.41 ; top5 ->  99.24  and loss:  43.28712669014931
forward train acc: top1 ->  99.52399997802735 ; top5 ->  99.992  and loss:  1.4852025522850454
test acc: top1 ->  91.34 ; top5 ->  99.23  and loss:  45.07018029689789
forward train acc: top1 ->  99.59999998046875 ; top5 ->  99.992  and loss:  1.3462300830287859
test acc: top1 ->  91.44 ; top5 ->  99.17  and loss:  46.57757868617773
forward train acc: top1 ->  99.6360000024414 ; top5 ->  100.0  and loss:  1.1187380560440943
test acc: top1 ->  91.37 ; top5 ->  99.19  and loss:  45.96899975091219
forward train acc: top1 ->  99.67800000244141 ; top5 ->  99.998  and loss:  1.0712011705618352
test acc: top1 ->  91.43 ; top5 ->  99.24  and loss:  45.83926045894623
forward train acc: top1 ->  99.66 ; top5 ->  99.998  and loss:  1.0274170555640012
test acc: top1 ->  91.47 ; top5 ->  99.25  and loss:  46.04876434057951
forward train acc: top1 ->  99.69400000732422 ; top5 ->  99.998  and loss:  0.8846405984368175
test acc: top1 ->  91.61 ; top5 ->  99.25  and loss:  46.80615197867155
forward train acc: top1 ->  99.70199997558593 ; top5 ->  100.0  and loss:  0.8660922928247601
test acc: top1 ->  91.56 ; top5 ->  99.24  and loss:  47.31321685016155
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  55 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -931.3092293478549 , diff:  931.3092293478549
adv train loss:  -1505.075590133667 , diff:  573.7663607858121
adv train loss:  -1511.0440864562988 , diff:  5.968496322631836
adv train loss:  -1509.1598081588745 , diff:  1.8842782974243164
adv train loss:  -1515.0864191055298 , diff:  5.926610946655273
adv train loss:  -1512.3154916763306 , diff:  2.7709274291992188
adv train loss:  -1511.140124320984 , diff:  1.1753673553466797
adv train loss:  -1516.117681503296 , diff:  4.977557182312012
adv train loss:  -1514.5268688201904 , diff:  1.5908126831054688
adv train loss:  -1554.8061418533325 , diff:  40.27927303314209
layer  2  adv train finish, try to retain  37
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  1160.3346948623657
forward train acc: top1 ->  88.90000001708984 ; top5 ->  99.2120000024414  and loss:  39.2569992095232
test acc: top1 ->  84.12 ; top5 ->  98.68  and loss:  55.58475998044014
forward train acc: top1 ->  90.84800001953126 ; top5 ->  99.47000000244141  and loss:  27.68863332271576
test acc: top1 ->  85.11 ; top5 ->  98.75  and loss:  51.636905908584595
forward train acc: top1 ->  92.14599998535157 ; top5 ->  99.63199997558594  and loss:  23.707290142774582
test acc: top1 ->  85.9 ; top5 ->  98.81  and loss:  49.56832432746887
forward train acc: top1 ->  92.96999997558594 ; top5 ->  99.64199997558593  and loss:  21.256066218018532
test acc: top1 ->  86.18 ; top5 ->  98.93  and loss:  48.68583503365517
forward train acc: top1 ->  93.624 ; top5 ->  99.75599997558594  and loss:  18.972671821713448
test acc: top1 ->  86.8 ; top5 ->  98.86  and loss:  47.36504429578781
forward train acc: top1 ->  93.908 ; top5 ->  99.76399997558593  and loss:  18.04985883831978
test acc: top1 ->  87.01 ; top5 ->  98.98  and loss:  46.403164610266685
forward train acc: top1 ->  94.156 ; top5 ->  99.786  and loss:  17.23074695467949
test acc: top1 ->  87.32 ; top5 ->  98.92  and loss:  45.964260056614876
forward train acc: top1 ->  94.394 ; top5 ->  99.79399997558593  and loss:  16.545339092612267
test acc: top1 ->  87.51 ; top5 ->  98.96  and loss:  45.96925009787083
forward train acc: top1 ->  94.6119999975586 ; top5 ->  99.806  and loss:  15.815040938556194
test acc: top1 ->  87.46 ; top5 ->  99.04  and loss:  45.572927594184875
forward train acc: top1 ->  94.67199997314454 ; top5 ->  99.81399997558594  and loss:  15.775117486715317
test acc: top1 ->  87.69 ; top5 ->  98.97  and loss:  45.26222710311413
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -639.8526464784518 , diff:  639.8526464784518
adv train loss:  -986.9343242645264 , diff:  347.0816777860746
adv train loss:  -993.8529920578003 , diff:  6.918667793273926
adv train loss:  -994.3501033782959 , diff:  0.49711132049560547
adv train loss:  -1015.5127172470093 , diff:  21.16261386871338
adv train loss:  -1009.5332012176514 , diff:  5.97951602935791
adv train loss:  -1013.9460029602051 , diff:  4.412801742553711
adv train loss:  -1011.3496580123901 , diff:  2.5963449478149414
adv train loss:  -1013.6446485519409 , diff:  2.2949905395507812
adv train loss:  -1011.0529642105103 , diff:  2.591684341430664
layer  3  adv train finish, try to retain  29
test acc: top1 ->  10.0 ; top5 ->  50.47  and loss:  648.3522262573242
forward train acc: top1 ->  83.31800000244141 ; top5 ->  98.73200000244141  and loss:  50.80082604289055
test acc: top1 ->  80.46 ; top5 ->  98.27  and loss:  62.702367544174194
forward train acc: top1 ->  85.95599999267579 ; top5 ->  99.02199997558594  and loss:  42.036902874708176
test acc: top1 ->  82.07 ; top5 ->  98.45  and loss:  58.28873807191849
forward train acc: top1 ->  87.0960000024414 ; top5 ->  99.21799997558594  and loss:  38.14673888683319
test acc: top1 ->  82.9 ; top5 ->  98.48  and loss:  55.48993161320686
forward train acc: top1 ->  88.16999997070313 ; top5 ->  99.33399997558594  and loss:  35.08050313591957
test acc: top1 ->  83.46 ; top5 ->  98.55  and loss:  54.33096545934677
forward train acc: top1 ->  88.7759999975586 ; top5 ->  99.40799997558594  and loss:  33.07195794582367
test acc: top1 ->  83.93 ; top5 ->  98.62  and loss:  52.996824979782104
forward train acc: top1 ->  89.37999997314454 ; top5 ->  99.43799997558594  and loss:  31.548035338521004
test acc: top1 ->  84.18 ; top5 ->  98.72  and loss:  51.95015501976013
forward train acc: top1 ->  89.64599997314453 ; top5 ->  99.43200000488281  and loss:  30.526986688375473
test acc: top1 ->  84.42 ; top5 ->  98.7  and loss:  51.51921612024307
forward train acc: top1 ->  89.74400001220702 ; top5 ->  99.4540000024414  and loss:  30.144530713558197
test acc: top1 ->  84.54 ; top5 ->  98.75  and loss:  51.27720421552658
forward train acc: top1 ->  90.17000001953124 ; top5 ->  99.4980000024414  and loss:  29.207387536764145
test acc: top1 ->  84.67 ; top5 ->  98.74  and loss:  50.72684356570244
forward train acc: top1 ->  90.28000000488281 ; top5 ->  99.58999997558594  and loss:  28.26016601920128
test acc: top1 ->  84.76 ; top5 ->  98.65  and loss:  50.886738896369934
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -80.28133318759501 , diff:  80.28133318759501
adv train loss:  -671.968939781189 , diff:  591.687606593594
adv train loss:  -844.5145435333252 , diff:  172.54560375213623
adv train loss:  -942.1671533584595 , diff:  97.65260982513428
adv train loss:  -984.2957696914673 , diff:  42.12861633300781
adv train loss:  -1007.7476024627686 , diff:  23.45183277130127
adv train loss:  -1007.1832838058472 , diff:  0.5643186569213867
adv train loss:  -1005.7028703689575 , diff:  1.4804134368896484
adv train loss:  -1008.5492868423462 , diff:  2.846416473388672
adv train loss:  -1008.7512397766113 , diff:  0.20195293426513672
layer  4  adv train finish, try to retain  19
test acc: top1 ->  9.81 ; top5 ->  50.0  and loss:  2043.9822273254395
forward train acc: top1 ->  60.19399998779297 ; top5 ->  94.56799999023437  and loss:  118.72658258676529
test acc: top1 ->  64.27 ; top5 ->  95.65  and loss:  106.70678550004959
forward train acc: top1 ->  66.92599997314453 ; top5 ->  96.71000000976562  and loss:  92.90299654006958
test acc: top1 ->  67.85 ; top5 ->  96.4  and loss:  95.53102684020996
forward train acc: top1 ->  70.44799998046875 ; top5 ->  97.50400001220703  and loss:  83.07286649942398
test acc: top1 ->  70.42 ; top5 ->  97.14  and loss:  86.64151704311371
forward train acc: top1 ->  73.08199998046875 ; top5 ->  97.79999998535156  and loss:  76.13491708040237
test acc: top1 ->  72.3 ; top5 ->  97.37  and loss:  82.599130153656
forward train acc: top1 ->  74.74800001220703 ; top5 ->  98.07199997558594  and loss:  71.82685887813568
test acc: top1 ->  73.72 ; top5 ->  97.71  and loss:  77.7511100769043
forward train acc: top1 ->  76.14799997314454 ; top5 ->  98.25400000976562  and loss:  67.73697817325592
test acc: top1 ->  74.4 ; top5 ->  97.74  and loss:  76.2176256775856
forward train acc: top1 ->  76.42800000244141 ; top5 ->  98.28199997802734  and loss:  66.62397646903992
test acc: top1 ->  74.9 ; top5 ->  97.8  and loss:  74.38099956512451
forward train acc: top1 ->  77.0299999951172 ; top5 ->  98.32399998046876  and loss:  64.76591390371323
test acc: top1 ->  75.26 ; top5 ->  97.88  and loss:  73.7726996243
forward train acc: top1 ->  77.81399999023438 ; top5 ->  98.39200000976562  and loss:  63.47076952457428
test acc: top1 ->  75.86 ; top5 ->  97.88  and loss:  72.10057079792023
forward train acc: top1 ->  77.97800001464844 ; top5 ->  98.46800000732422  and loss:  62.30670648813248
test acc: top1 ->  76.21 ; top5 ->  98.14  and loss:  71.15560525655746
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -184.99397575110197 , diff:  184.99397575110197
adv train loss:  -829.2811393737793 , diff:  644.2871636226773
adv train loss:  -960.9171447753906 , diff:  131.63600540161133
adv train loss:  -982.7956409454346 , diff:  21.878496170043945
adv train loss:  -1006.9167404174805 , diff:  24.1210994720459
adv train loss:  -1024.271650314331 , diff:  17.354909896850586
adv train loss:  -1023.7298936843872 , diff:  0.5417566299438477
adv train loss:  -1024.7116165161133 , diff:  0.9817228317260742
adv train loss:  -1022.855447769165 , diff:  1.8561687469482422
adv train loss:  -1026.4512948989868 , diff:  3.5958471298217773
layer  5  adv train finish, try to retain  23
test acc: top1 ->  11.0 ; top5 ->  53.49  and loss:  750.9925866127014
forward train acc: top1 ->  76.50399999511718 ; top5 ->  98.02  and loss:  67.65217396616936
test acc: top1 ->  76.2 ; top5 ->  97.62  and loss:  74.15072220563889
forward train acc: top1 ->  81.646 ; top5 ->  98.89399998046875  and loss:  52.390222668647766
test acc: top1 ->  78.7 ; top5 ->  98.14  and loss:  66.85724949836731
forward train acc: top1 ->  84.18800000244141 ; top5 ->  99.15999997802734  and loss:  45.66938489675522
test acc: top1 ->  80.28 ; top5 ->  98.47  and loss:  62.811564803123474
forward train acc: top1 ->  85.63199999511718 ; top5 ->  99.3260000024414  and loss:  41.25866350531578
test acc: top1 ->  81.37 ; top5 ->  98.62  and loss:  58.76500955224037
forward train acc: top1 ->  86.502 ; top5 ->  99.40800000244141  and loss:  38.80176445841789
test acc: top1 ->  82.71 ; top5 ->  98.61  and loss:  57.191357880830765
forward train acc: top1 ->  87.2399999975586 ; top5 ->  99.46999997802735  and loss:  36.31296122074127
test acc: top1 ->  82.94 ; top5 ->  98.73  and loss:  55.94769832491875
forward train acc: top1 ->  87.5419999951172 ; top5 ->  99.4860000024414  and loss:  35.1656237244606
test acc: top1 ->  83.28 ; top5 ->  98.72  and loss:  55.251267194747925
forward train acc: top1 ->  87.90599998779297 ; top5 ->  99.49399998046874  and loss:  34.35369589924812
test acc: top1 ->  83.63 ; top5 ->  98.84  and loss:  54.04469406604767
forward train acc: top1 ->  88.32599998779297 ; top5 ->  99.54199997802735  and loss:  33.382123082876205
test acc: top1 ->  83.67 ; top5 ->  98.82  and loss:  53.744824558496475
forward train acc: top1 ->  88.77799999511718 ; top5 ->  99.55400000244141  and loss:  32.57637266814709
test acc: top1 ->  83.87 ; top5 ->  98.86  and loss:  52.602632254362106
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -198.69863533973694 , diff:  198.69863533973694
adv train loss:  -1158.7405767440796 , diff:  960.0419414043427
adv train loss:  -1200.59450340271 , diff:  41.85392665863037
adv train loss:  -1250.9586582183838 , diff:  50.36415481567383
adv train loss:  -1255.4309854507446 , diff:  4.47232723236084
adv train loss:  -1259.3149509429932 , diff:  3.883965492248535
adv train loss:  -1268.9410104751587 , diff:  9.626059532165527
adv train loss:  -1307.2903213500977 , diff:  38.349310874938965
adv train loss:  -1320.9536800384521 , diff:  13.663358688354492
adv train loss:  -1316.699299812317 , diff:  4.254380226135254
layer  6  adv train finish, try to retain  17
test acc: top1 ->  10.23 ; top5 ->  63.66  and loss:  887.6117300987244
forward train acc: top1 ->  85.01399998291015 ; top5 ->  98.79799997802735  and loss:  45.920118778944016
test acc: top1 ->  81.56 ; top5 ->  98.36  and loss:  62.7643326818943
forward train acc: top1 ->  90.44199999023438 ; top5 ->  99.596  and loss:  28.708152681589127
test acc: top1 ->  83.89 ; top5 ->  98.64  and loss:  55.88096712529659
forward train acc: top1 ->  92.14199998046875 ; top5 ->  99.722  and loss:  22.91940727829933
test acc: top1 ->  85.11 ; top5 ->  98.79  and loss:  53.29048836231232
forward train acc: top1 ->  93.22000000244141 ; top5 ->  99.78399997558594  and loss:  20.02834962308407
test acc: top1 ->  86.13 ; top5 ->  98.91  and loss:  50.1168075799942
forward train acc: top1 ->  94.01800000488281 ; top5 ->  99.838  and loss:  17.764838948845863
test acc: top1 ->  86.66 ; top5 ->  98.9  and loss:  49.634906232357025
forward train acc: top1 ->  94.43600001953125 ; top5 ->  99.838  and loss:  16.240875102579594
test acc: top1 ->  86.89 ; top5 ->  98.96  and loss:  49.08539251983166
forward train acc: top1 ->  94.70399999267578 ; top5 ->  99.85  and loss:  15.461310230195522
test acc: top1 ->  87.0 ; top5 ->  98.99  and loss:  48.90656501054764
forward train acc: top1 ->  94.89199999023438 ; top5 ->  99.88199997558594  and loss:  15.043326921761036
test acc: top1 ->  87.12 ; top5 ->  98.95  and loss:  48.18075501918793
forward train acc: top1 ->  95.138 ; top5 ->  99.904  and loss:  14.23077592626214
test acc: top1 ->  87.31 ; top5 ->  99.07  and loss:  48.378751665353775
forward train acc: top1 ->  95.35999999023437 ; top5 ->  99.894  and loss:  13.466188877820969
test acc: top1 ->  87.48 ; top5 ->  99.02  and loss:  47.667725682258606
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  144 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -3.7154434956610203 , diff:  3.7154434956610203
adv train loss:  -3.8899515625089407 , diff:  0.17450806684792042
adv train loss:  -3.865864994004369 , diff:  0.024086568504571915
adv train loss:  -3.9219559896737337 , diff:  0.05609099566936493
adv train loss:  -3.874495342373848 , diff:  0.04746064729988575
adv train loss:  -3.6916883382946253 , diff:  0.18280700407922268
adv train loss:  -3.7975558023899794 , diff:  0.10586746409535408
adv train loss:  -3.907488953322172 , diff:  0.1099331509321928
adv train loss:  -3.8414936531335115 , diff:  0.06599530018866062
adv train loss:  -3.804043964482844 , diff:  0.03744968865066767
layer  7  adv train finish, try to retain  249
test acc: top1 ->  67.21 ; top5 ->  87.93  and loss:  118.8643165230751
forward train acc: top1 ->  98.92999997802734 ; top5 ->  99.992  and loss:  3.440576657652855
test acc: top1 ->  90.85 ; top5 ->  99.28  and loss:  45.261260971426964
forward train acc: top1 ->  99.45 ; top5 ->  99.996  and loss:  1.6299163687508553
test acc: top1 ->  91.01 ; top5 ->  99.24  and loss:  48.92038656771183
forward train acc: top1 ->  99.57999997558593 ; top5 ->  99.998  and loss:  1.2488757020328194
test acc: top1 ->  91.12 ; top5 ->  99.31  and loss:  52.047931268811226
forward train acc: top1 ->  99.69 ; top5 ->  100.0  and loss:  0.9159739960450679
test acc: top1 ->  91.18 ; top5 ->  99.29  and loss:  55.545504823327065
forward train acc: top1 ->  99.6980000024414 ; top5 ->  100.0  and loss:  0.9109445190988481
test acc: top1 ->  91.39 ; top5 ->  99.38  and loss:  53.74859493970871
forward train acc: top1 ->  99.78999997558594 ; top5 ->  100.0  and loss:  0.6775267618941143
test acc: top1 ->  91.44 ; top5 ->  99.32  and loss:  54.59442798793316
forward train acc: top1 ->  99.77 ; top5 ->  100.0  and loss:  0.6268792533082888
test acc: top1 ->  91.36 ; top5 ->  99.35  and loss:  55.23265168070793
forward train acc: top1 ->  99.83600000244141 ; top5 ->  100.0  and loss:  0.5103173592942767
test acc: top1 ->  91.38 ; top5 ->  99.35  and loss:  57.21321552991867
forward train acc: top1 ->  99.82599997558594 ; top5 ->  100.0  and loss:  0.5044066595728509
test acc: top1 ->  91.41 ; top5 ->  99.36  and loss:  57.32301923632622
forward train acc: top1 ->  99.82199997558594 ; top5 ->  99.998  and loss:  0.5062470643315464
test acc: top1 ->  91.45 ; top5 ->  99.38  and loss:  57.674105152487755
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  253 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -4.83544083032757 , diff:  4.83544083032757
adv train loss:  -5.010421226732433 , diff:  0.1749803964048624
adv train loss:  -5.209105622023344 , diff:  0.1986843952909112
adv train loss:  -5.325929632876068 , diff:  0.11682401085272431
adv train loss:  -4.961897135712206 , diff:  0.364032497163862
adv train loss:  -5.183126797899604 , diff:  0.22122966218739748
adv train loss:  -5.24395105522126 , diff:  0.06082425732165575
adv train loss:  -5.0213529812172055 , diff:  0.22259807400405407
adv train loss:  -5.180977962911129 , diff:  0.15962498169392347
adv train loss:  -5.070015984587371 , diff:  0.11096197832375765
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  201
test acc: top1 ->  29.14 ; top5 ->  65.9  and loss:  21524698.125
forward train acc: top1 ->  99.58799997558594 ; top5 ->  100.0  and loss:  1.303263702429831
test acc: top1 ->  91.57 ; top5 ->  99.09  and loss:  55.50518870353699
forward train acc: top1 ->  99.85199997558594 ; top5 ->  100.0  and loss:  0.5100986653123982
test acc: top1 ->  91.83 ; top5 ->  99.1  and loss:  56.4220006018877
forward train acc: top1 ->  99.896 ; top5 ->  100.0  and loss:  0.3586390517302789
test acc: top1 ->  92.03 ; top5 ->  99.11  and loss:  57.56321455538273
forward train acc: top1 ->  99.894 ; top5 ->  99.998  and loss:  0.33941900086938404
test acc: top1 ->  91.92 ; top5 ->  99.2  and loss:  57.5425958186388
forward train acc: top1 ->  99.916 ; top5 ->  100.0  and loss:  0.2658005211269483
test acc: top1 ->  91.99 ; top5 ->  99.12  and loss:  60.26049265265465
forward train acc: top1 ->  99.92399997558594 ; top5 ->  100.0  and loss:  0.23213675006991252
test acc: top1 ->  91.98 ; top5 ->  99.17  and loss:  60.04822474718094
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.23161128920764895
test acc: top1 ->  92.07 ; top5 ->  99.12  and loss:  60.287148505449295
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  233 / 512 , inc:  32
---------------- start layer  9  ---------------
adv train loss:  -1.407477349275723 , diff:  1.407477349275723
adv train loss:  -1.540472696069628 , diff:  0.1329953467939049
adv train loss:  -1.339329031528905 , diff:  0.20114366454072297
adv train loss:  -1.3475237898528576 , diff:  0.008194758323952556
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  63
test acc: top1 ->  10.0 ; top5 ->  49.99  and loss:  1206282.5927734375
forward train acc: top1 ->  98.99399997558594 ; top5 ->  99.998  and loss:  4.554878961411305
test acc: top1 ->  91.56 ; top5 ->  99.3  and loss:  56.173648454248905
forward train acc: top1 ->  99.83999997558594 ; top5 ->  100.0  and loss:  0.5246455664746463
test acc: top1 ->  91.99 ; top5 ->  99.4  and loss:  56.367638647556305
forward train acc: top1 ->  99.86999997558594 ; top5 ->  100.0  and loss:  0.40159813972422853
test acc: top1 ->  92.1 ; top5 ->  99.37  and loss:  56.39015594124794
forward train acc: top1 ->  99.93199997558594 ; top5 ->  99.998  and loss:  0.26784520149521995
test acc: top1 ->  92.21 ; top5 ->  99.45  and loss:  57.63940630853176
==> this epoch:  63 / 512
---------------- start layer  10  ---------------
adv train loss:  -2133.3100090026855 , diff:  2133.3100090026855
adv train loss:  -2132.3657035827637 , diff:  0.944305419921875
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  11
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  603212.7021484375
forward train acc: top1 ->  96.13 ; top5 ->  99.938  and loss:  16.357059102971107
test acc: top1 ->  91.09 ; top5 ->  99.21  and loss:  59.72174263000488
forward train acc: top1 ->  99.79999997558593 ; top5 ->  100.0  and loss:  0.7076445561251603
test acc: top1 ->  91.82 ; top5 ->  99.31  and loss:  57.66723047196865
forward train acc: top1 ->  99.828 ; top5 ->  100.0  and loss:  0.5215269474429078
test acc: top1 ->  91.81 ; top5 ->  99.33  and loss:  58.73289605975151
forward train acc: top1 ->  99.9 ; top5 ->  100.0  and loss:  0.34357916202861816
test acc: top1 ->  91.97 ; top5 ->  99.36  and loss:  59.280527263879776
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.26480752759380266
test acc: top1 ->  91.97 ; top5 ->  99.38  and loss:  60.15900021791458
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.23469252900395077
test acc: top1 ->  92.12 ; top5 ->  99.32  and loss:  60.754844546318054
==> this epoch:  11 / 512
---------------- start layer  11  ---------------
adv train loss:  -102.77807807922363 , diff:  102.77807807922363
adv train loss:  -154.92581522464752 , diff:  52.14773714542389
adv train loss:  -301.61256289482117 , diff:  146.68674767017365
adv train loss:  -445.98890709877014 , diff:  144.37634420394897
adv train loss:  -516.2158880233765 , diff:  70.22698092460632
adv train loss:  -653.9586324691772 , diff:  137.74274444580078
adv train loss:  -654.6418976783752 , diff:  0.683265209197998
adv train loss:  -901.7426567077637 , diff:  247.10075902938843
adv train loss:  -1070.4407539367676 , diff:  168.6980972290039
adv train loss:  -1108.2410049438477 , diff:  37.80025100708008
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  55
test acc: top1 ->  10.0 ; top5 ->  50.06  and loss:  9432756.2109375
forward train acc: top1 ->  95.93999997558593 ; top5 ->  99.998  and loss:  27.534288921859115
test acc: top1 ->  91.69 ; top5 ->  99.11  and loss:  54.709530025720596
forward train acc: top1 ->  99.876 ; top5 ->  100.0  and loss:  0.4751814567716792
test acc: top1 ->  92.0 ; top5 ->  99.06  and loss:  54.66370194405317
forward train acc: top1 ->  99.89399997802734 ; top5 ->  99.998  and loss:  0.3803344847401604
test acc: top1 ->  92.04 ; top5 ->  99.1  and loss:  55.36140387505293
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.290945001936052
test acc: top1 ->  92.09 ; top5 ->  99.17  and loss:  55.468890339136124
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.22622990020317957
test acc: top1 ->  92.12 ; top5 ->  99.17  and loss:  55.95442980527878
==> this epoch:  55 / 512
---------------- start layer  12  ---------------
### skip layer  12 wait:  3  ###
---------------- start layer  13  ---------------
adv train loss:  -11627.07064819336 , diff:  11627.07064819336
adv train loss:  -20267.169815063477 , diff:  8640.099166870117
adv train loss:  -30789.09585571289 , diff:  10521.926040649414
adv train loss:  -42526.18685913086 , diff:  11737.091003417969
adv train loss:  -52950.21273803711 , diff:  10424.02587890625
adv train loss:  -62892.660217285156 , diff:  9942.447479248047
adv train loss:  -72471.28308105469 , diff:  9578.622863769531
adv train loss:  -80196.39239501953 , diff:  7725.109313964844
adv train loss:  -84469.03198242188 , diff:  4272.639587402344
adv train loss:  -86021.00616455078 , diff:  1551.9741821289062
layer  13  adv train finish, try to retain  46
test acc: top1 ->  36.96 ; top5 ->  86.22  and loss:  1475.3239459991455
forward train acc: top1 ->  89.102 ; top5 ->  99.694  and loss:  125.94563459930941
test acc: top1 ->  91.12 ; top5 ->  98.66  and loss:  63.275127217173576
forward train acc: top1 ->  99.812 ; top5 ->  100.0  and loss:  0.7141848679166287
test acc: top1 ->  91.6 ; top5 ->  98.76  and loss:  61.32986584305763
forward train acc: top1 ->  99.88 ; top5 ->  100.0  and loss:  0.42451290623284876
test acc: top1 ->  91.57 ; top5 ->  98.8  and loss:  61.82533559203148
forward train acc: top1 ->  99.904 ; top5 ->  100.0  and loss:  0.3593354885233566
test acc: top1 ->  91.83 ; top5 ->  98.87  and loss:  60.47371093928814
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.2996718962676823
test acc: top1 ->  91.84 ; top5 ->  98.85  and loss:  60.35124845802784
forward train acc: top1 ->  99.90999997558593 ; top5 ->  100.0  and loss:  0.2814019094221294
test acc: top1 ->  91.94 ; top5 ->  98.88  and loss:  60.42008715867996
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.2250958955846727
test acc: top1 ->  91.98 ; top5 ->  98.9  and loss:  60.423930287361145
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.22917414631228894
test acc: top1 ->  91.92 ; top5 ->  98.88  and loss:  61.050926342606544
forward train acc: top1 ->  99.94 ; top5 ->  100.0  and loss:  0.24515650537796319
test acc: top1 ->  92.12 ; top5 ->  98.88  and loss:  60.202125921845436
==> this epoch:  46 / 512
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.455078125  ==>  233 / 512 , inc:  16
layer  9  :  0.123046875  ==>  63 / 512 , inc:  4
layer  10  :  0.021484375  ==>  11 / 512 , inc:  2
layer  11  :  0.107421875  ==>  55 / 512 , inc:  27
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.08984375  ==>  46 / 512 , inc:  8
eps [1.3839609375000002, 1.3839609375000002, 1.3839609375000002, 1.3839609375000002, 1.3839609375000002, 1.3839609375000002, 1.3839609375000002, 0.016218292236328126, 1.2301875000000002, 1.6402500000000002, 1.3839609375000002, 1.6402500000000002, 1.3839609375000002, 9.841500000000002]  wait [4, 4, 4, 4, 4, 4, 4, 4, 2, 0, 0, 0, 2, 0]  inc [1, 1, 1, 1, 1, 1, 1, 1, 16, 4, 2, 27, 1, 8]  tol: 3
$$$$$$$$$$$$$ epoch  39  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
### skip layer  1 wait:  4  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
adv train loss:  -0.37384764885064214 , diff:  0.37384764885064214
adv train loss:  -0.33282765629701316 , diff:  0.04101999255362898
adv train loss:  -0.2929335144581273 , diff:  0.039894141838885844
adv train loss:  -0.3154273561667651 , diff:  0.022493841708637774
adv train loss:  -0.37388228438794613 , diff:  0.058454928221181035
adv train loss:  -0.2890530447475612 , diff:  0.08482923964038491
adv train loss:  -0.3367156202439219 , diff:  0.04766257549636066
adv train loss:  -0.27810670871986076 , diff:  0.058608911524061114
adv train loss:  -0.31981689494568855 , diff:  0.04171018622582778
adv train loss:  -0.346688864286989 , diff:  0.026871969341300428
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  217
test acc: top1 ->  28.22 ; top5 ->  76.9  and loss:  18296673.3125
forward train acc: top1 ->  99.486 ; top5 ->  99.996  and loss:  2.2480857581831515
test acc: top1 ->  91.23 ; top5 ->  98.84  and loss:  64.27891992032528
forward train acc: top1 ->  99.862 ; top5 ->  99.998  and loss:  0.5366430440917611
test acc: top1 ->  91.85 ; top5 ->  98.8  and loss:  64.45127633213997
forward train acc: top1 ->  99.9040000024414 ; top5 ->  100.0  and loss:  0.35916577954776585
test acc: top1 ->  91.84 ; top5 ->  98.83  and loss:  67.6985186189413
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.23737430153414607
test acc: top1 ->  92.11 ; top5 ->  98.84  and loss:  68.53629240393639
==> this epoch:  217 / 512
---------------- start layer  9  ---------------
adv train loss:  -1015.5112762451172 , diff:  1015.5112762451172
adv train loss:  -1012.9933252334595 , diff:  2.517951011657715
adv train loss:  -1015.389741897583 , diff:  2.396416664123535
adv train loss:  -1016.1222085952759 , diff:  0.7324666976928711
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  59
test acc: top1 ->  13.99 ; top5 ->  50.0  and loss:  1015389.8784179688
forward train acc: top1 ->  93.53199997802734 ; top5 ->  99.316  and loss:  36.25354960747063
test acc: top1 ->  89.14 ; top5 ->  98.19  and loss:  66.59899416565895
forward train acc: top1 ->  99.4 ; top5 ->  99.998  and loss:  2.840446727350354
test acc: top1 ->  90.31 ; top5 ->  98.29  and loss:  61.96741038560867
forward train acc: top1 ->  99.58399997558594 ; top5 ->  99.998  and loss:  1.8096651379019022
test acc: top1 ->  90.95 ; top5 ->  98.31  and loss:  60.69250248372555
forward train acc: top1 ->  99.69799997558594 ; top5 ->  99.998  and loss:  1.2940214909613132
test acc: top1 ->  91.16 ; top5 ->  98.38  and loss:  60.26537328958511
forward train acc: top1 ->  99.82199997558594 ; top5 ->  99.998  and loss:  0.9130506627261639
test acc: top1 ->  91.23 ; top5 ->  98.39  and loss:  61.6619104295969
forward train acc: top1 ->  99.80599997558593 ; top5 ->  100.0  and loss:  0.7922285995446146
test acc: top1 ->  91.51 ; top5 ->  98.46  and loss:  60.063917458057404
forward train acc: top1 ->  99.826 ; top5 ->  100.0  and loss:  0.7296201253775507
test acc: top1 ->  91.6 ; top5 ->  98.47  and loss:  59.93411457538605
forward train acc: top1 ->  99.8580000024414 ; top5 ->  100.0  and loss:  0.6048669517040253
test acc: top1 ->  91.52 ; top5 ->  98.44  and loss:  61.290307477116585
forward train acc: top1 ->  99.84199997802735 ; top5 ->  100.0  and loss:  0.649064586032182
test acc: top1 ->  91.64 ; top5 ->  98.5  and loss:  60.95603011548519
forward train acc: top1 ->  99.872 ; top5 ->  100.0  and loss:  0.5082156949210912
test acc: top1 ->  91.67 ; top5 ->  98.48  and loss:  62.195226937532425
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  63 / 512 , inc:  4
---------------- start layer  10  ---------------
adv train loss:  -1448.4675540924072 , diff:  1448.4675540924072
adv train loss:  -1450.9186735153198 , diff:  2.4511194229125977
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  9
test acc: top1 ->  24.06 ; top5 ->  58.42  and loss:  167246.5850830078
forward train acc: top1 ->  92.12800000488281 ; top5 ->  99.92  and loss:  40.25377518311143
test acc: top1 ->  81.57 ; top5 ->  98.32  and loss:  92.16783991456032
forward train acc: top1 ->  99.55199997802734 ; top5 ->  99.998  and loss:  2.7565711261704564
test acc: top1 ->  90.95 ; top5 ->  98.55  and loss:  60.21293318271637
forward train acc: top1 ->  99.72599997558594 ; top5 ->  100.0  and loss:  1.4542410904541612
test acc: top1 ->  91.33 ; top5 ->  98.65  and loss:  58.95444881170988
forward train acc: top1 ->  99.83799997558594 ; top5 ->  99.998  and loss:  0.8961223121732473
test acc: top1 ->  91.61 ; top5 ->  98.78  and loss:  58.33398324996233
forward train acc: top1 ->  99.87599997558594 ; top5 ->  99.998  and loss:  0.6166294906288385
test acc: top1 ->  91.76 ; top5 ->  98.81  and loss:  59.404273100197315
forward train acc: top1 ->  99.90999997558593 ; top5 ->  100.0  and loss:  0.4470040826126933
test acc: top1 ->  91.79 ; top5 ->  98.83  and loss:  60.28302678465843
forward train acc: top1 ->  99.89799997558593 ; top5 ->  99.998  and loss:  0.41781097650527954
test acc: top1 ->  91.79 ; top5 ->  98.83  and loss:  61.750156588852406
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.3492323216050863
test acc: top1 ->  91.93 ; top5 ->  98.85  and loss:  61.53458931297064
forward train acc: top1 ->  99.914 ; top5 ->  100.0  and loss:  0.3745130698662251
test acc: top1 ->  91.86 ; top5 ->  98.86  and loss:  61.24743275344372
forward train acc: top1 ->  99.91799997558594 ; top5 ->  99.998  and loss:  0.31191157875582576
test acc: top1 ->  91.88 ; top5 ->  98.87  and loss:  62.67454947531223
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  11 / 512 , inc:  2
---------------- start layer  11  ---------------
adv train loss:  -2926.9176387786865 , diff:  2926.9176387786865
adv train loss:  -3047.485710144043 , diff:  120.56807136535645
adv train loss:  -3051.081174850464 , diff:  3.5954647064208984
adv train loss:  -3047.5909004211426 , diff:  3.490274429321289
adv train loss:  -3048.8372173309326 , diff:  1.246316909790039
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  28
test acc: top1 ->  10.02 ; top5 ->  68.33  and loss:  116191.14544677734
forward train acc: top1 ->  96.504 ; top5 ->  99.982  and loss:  20.1145789254806
test acc: top1 ->  91.5 ; top5 ->  98.69  and loss:  83.96904328465462
forward train acc: top1 ->  99.86399997558594 ; top5 ->  100.0  and loss:  0.44036807562224567
test acc: top1 ->  91.75 ; top5 ->  98.74  and loss:  81.21665754914284
forward train acc: top1 ->  99.91199997558594 ; top5 ->  100.0  and loss:  0.30074039683677256
test acc: top1 ->  91.81 ; top5 ->  98.79  and loss:  79.44429340958595
forward train acc: top1 ->  99.936 ; top5 ->  100.0  and loss:  0.18553104159946088
test acc: top1 ->  91.93 ; top5 ->  98.81  and loss:  78.55454811453819
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.16869322455022484
test acc: top1 ->  91.9 ; top5 ->  98.78  and loss:  78.71890154480934
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.16331564623396844
test acc: top1 ->  91.79 ; top5 ->  98.89  and loss:  78.86754703521729
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.15207112711505033
test acc: top1 ->  91.9 ; top5 ->  98.83  and loss:  78.68487258255482
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.1466161604039371
test acc: top1 ->  91.96 ; top5 ->  98.82  and loss:  77.22460159659386
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.12308876652969047
test acc: top1 ->  91.95 ; top5 ->  98.84  and loss:  77.68964110314846
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.13313475781251327
test acc: top1 ->  91.94 ; top5 ->  98.86  and loss:  77.62926259636879
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  55 / 512 , inc:  27
---------------- start layer  12  ---------------
adv train loss:  -1467.632869720459 , diff:  1467.632869720459
adv train loss:  -1518.3666162490845 , diff:  50.73374652862549
adv train loss:  -1544.7546291351318 , diff:  26.388012886047363
adv train loss:  -1572.9345998764038 , diff:  28.179970741271973
adv train loss:  -1657.0900106430054 , diff:  84.15541076660156
adv train loss:  -1656.4924955368042 , diff:  0.5975151062011719
layer  12  adv train finish, try to retain  15
test acc: top1 ->  10.0 ; top5 ->  56.01  and loss:  1439.2933931350708
forward train acc: top1 ->  77.41599997558593 ; top5 ->  89.474  and loss:  266.9034969881177
test acc: top1 ->  89.65 ; top5 ->  97.8  and loss:  50.76777610182762
forward train acc: top1 ->  99.54999997558593 ; top5 ->  99.996  and loss:  3.325948426499963
test acc: top1 ->  90.79 ; top5 ->  98.13  and loss:  47.58513042330742
forward train acc: top1 ->  99.77599997558593 ; top5 ->  99.998  and loss:  1.796592891216278
test acc: top1 ->  91.06 ; top5 ->  98.22  and loss:  47.6290333122015
forward train acc: top1 ->  99.81 ; top5 ->  99.998  and loss:  1.3133056331425905
test acc: top1 ->  91.32 ; top5 ->  98.18  and loss:  48.07090422511101
forward train acc: top1 ->  99.87599997558594 ; top5 ->  100.0  and loss:  0.9771005213260651
test acc: top1 ->  91.4 ; top5 ->  98.3  and loss:  47.545448794960976
forward train acc: top1 ->  99.86 ; top5 ->  100.0  and loss:  0.8978560287505388
test acc: top1 ->  91.53 ; top5 ->  98.2  and loss:  48.27133621275425
forward train acc: top1 ->  99.90399997558593 ; top5 ->  99.998  and loss:  0.7383762178942561
test acc: top1 ->  91.65 ; top5 ->  98.2  and loss:  48.262764006853104
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.6392758530564606
test acc: top1 ->  91.65 ; top5 ->  98.23  and loss:  48.85704459249973
forward train acc: top1 ->  99.924 ; top5 ->  100.0  and loss:  0.5854110179934651
test acc: top1 ->  91.68 ; top5 ->  98.26  and loss:  49.3962322846055
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.5370548944920301
test acc: top1 ->  91.73 ; top5 ->  98.25  and loss:  49.488536171615124
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  62 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -11663.156616210938 , diff:  11663.156616210938
adv train loss:  -18675.70132446289 , diff:  7012.544708251953
adv train loss:  -25660.200714111328 , diff:  6984.4993896484375
adv train loss:  -32637.067138671875 , diff:  6976.866424560547
adv train loss:  -39625.47058105469 , diff:  6988.4034423828125
adv train loss:  -46600.157806396484 , diff:  6974.687225341797
adv train loss:  -53597.69647216797 , diff:  6997.538665771484
adv train loss:  -60512.147521972656 , diff:  6914.4510498046875
adv train loss:  -66678.98327636719 , diff:  6166.835754394531
adv train loss:  -70195.4521484375 , diff:  3516.4688720703125
layer  13  adv train finish, try to retain  52
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.423828125  ==>  217 / 512 , inc:  32
layer  9  :  0.123046875  ==>  63 / 512 , inc:  2
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.107421875  ==>  55 / 512 , inc:  13
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.08984375  ==>  46 / 512 , inc:  8
eps [1.3839609375000002, 1.3839609375000002, 1.3839609375000002, 1.3839609375000002, 1.3839609375000002, 1.3839609375000002, 1.3839609375000002, 0.016218292236328126, 1.2301875000000002, 1.2301875000000002, 1.037970703125, 1.2301875000000002, 1.037970703125, 19.683000000000003]  wait [3, 3, 3, 3, 3, 3, 3, 3, 0, 2, 2, 2, 4, 0]  inc [1, 1, 1, 1, 1, 1, 1, 1, 32, 2, 1, 13, 1, 8]  tol: 3
$$$$$$$$$$$$$ epoch  40  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
### skip layer  1 wait:  3  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  3  ###
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
adv train loss:  -1928.8792247772217 , diff:  1928.8792247772217
adv train loss:  -2013.9571647644043 , diff:  85.07793998718262
adv train loss:  -2028.1906070709229 , diff:  14.233442306518555
adv train loss:  -2027.0509243011475 , diff:  1.1396827697753906
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  185
test acc: top1 ->  19.9 ; top5 ->  76.37  and loss:  111441057.875
forward train acc: top1 ->  98.992 ; top5 ->  99.998  and loss:  4.46165003842907
test acc: top1 ->  91.92 ; top5 ->  98.89  and loss:  89.59755581617355
forward train acc: top1 ->  99.928 ; top5 ->  100.0  and loss:  0.2432237417815486
test acc: top1 ->  92.04 ; top5 ->  98.94  and loss:  87.54157494008541
forward train acc: top1 ->  99.94 ; top5 ->  100.0  and loss:  0.18489428369139205
test acc: top1 ->  92.1 ; top5 ->  98.99  and loss:  87.29364520311356
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.14831850430164195
test acc: top1 ->  92.25 ; top5 ->  98.96  and loss:  85.90460087358952
==> this epoch:  185 / 512
---------------- start layer  9  ---------------
adv train loss:  -1606.7598524093628 , diff:  1606.7598524093628
adv train loss:  -1611.5587711334229 , diff:  4.798918724060059
adv train loss:  -1618.9112844467163 , diff:  7.352513313293457
adv train loss:  -1617.9430150985718 , diff:  0.9682693481445312
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  61
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  1883017.3349609375
forward train acc: top1 ->  99.31199997558593 ; top5 ->  100.0  and loss:  3.214799503708491
test acc: top1 ->  91.7 ; top5 ->  98.74  and loss:  90.8341907709837
forward train acc: top1 ->  99.93399997558593 ; top5 ->  100.0  and loss:  0.20205760496901348
test acc: top1 ->  91.91 ; top5 ->  98.81  and loss:  89.68202611804008
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.13924934914030018
test acc: top1 ->  91.87 ; top5 ->  98.87  and loss:  88.71726417541504
forward train acc: top1 ->  99.94799997558594 ; top5 ->  100.0  and loss:  0.1280905102903489
test acc: top1 ->  91.99 ; top5 ->  98.77  and loss:  88.39157858490944
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.08620894401610713
test acc: top1 ->  91.99 ; top5 ->  98.87  and loss:  88.70163272321224
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.08119986172096105
test acc: top1 ->  91.98 ; top5 ->  98.83  and loss:  88.85074500739574
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.10866655525751412
test acc: top1 ->  92.08 ; top5 ->  98.88  and loss:  89.05248010158539
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.10986647989921039
test acc: top1 ->  92.07 ; top5 ->  98.89  and loss:  89.313277348876
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.07591064312509843
test acc: top1 ->  92.19 ; top5 ->  98.86  and loss:  88.82743188738823
==> this epoch:  61 / 512
---------------- start layer  10  ---------------
adv train loss:  -618.2380661964417 , diff:  618.2380661964417
adv train loss:  -617.5905776023865 , diff:  0.6474885940551758
layer  10  adv train finish, try to retain  501
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -1578.3550310134888 , diff:  1578.3550310134888
adv train loss:  -2008.2157936096191 , diff:  429.86076259613037
adv train loss:  -2127.280511856079 , diff:  119.06471824645996
adv train loss:  -2165.272285461426 , diff:  37.99177360534668
adv train loss:  -2210.317316055298 , diff:  45.04503059387207
adv train loss:  -2485.5763874053955 , diff:  275.25907135009766
adv train loss:  -3196.1887760162354 , diff:  710.6123886108398
adv train loss:  -3610.4219398498535 , diff:  414.23316383361816
adv train loss:  -4128.816921234131 , diff:  518.3949813842773
adv train loss:  -4240.919677734375 , diff:  112.10275650024414
layer  11  adv train finish, try to retain  23
test acc: top1 ->  28.94 ; top5 ->  50.02  and loss:  33796.463638305664
forward train acc: top1 ->  99.72 ; top5 ->  100.0  and loss:  1.0213131944474299
test acc: top1 ->  91.9 ; top5 ->  98.9  and loss:  85.8208879083395
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.10979817659244873
test acc: top1 ->  92.21 ; top5 ->  98.95  and loss:  84.74096401035786
==> this epoch:  23 / 512
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
adv train loss:  -16802.21371459961 , diff:  16802.21371459961
adv train loss:  -28608.764556884766 , diff:  11806.550842285156
adv train loss:  -39664.18209838867 , diff:  11055.417541503906
adv train loss:  -50470.687896728516 , diff:  10806.505798339844
adv train loss:  -61168.06530761719 , diff:  10697.377410888672
adv train loss:  -71776.49157714844 , diff:  10608.42626953125
adv train loss:  -82355.96868896484 , diff:  10579.477111816406
adv train loss:  -92904.478515625 , diff:  10548.509826660156
adv train loss:  -103409.57330322266 , diff:  10505.094787597656
adv train loss:  -113938.51330566406 , diff:  10528.940002441406
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  38
test acc: top1 ->  72.21 ; top5 ->  98.55  and loss:  233.4876925945282
forward train acc: top1 ->  98.366 ; top5 ->  100.0  and loss:  6.647403990791645
test acc: top1 ->  91.66 ; top5 ->  99.11  and loss:  63.98083855211735
forward train acc: top1 ->  99.942 ; top5 ->  100.0  and loss:  0.2345745523343794
test acc: top1 ->  91.77 ; top5 ->  99.1  and loss:  63.22405543923378
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.14401776535669342
test acc: top1 ->  91.97 ; top5 ->  99.14  and loss:  63.82746596634388
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.13818543471279554
test acc: top1 ->  92.0 ; top5 ->  99.17  and loss:  64.41180402040482
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.09850851625378709
test acc: top1 ->  92.02 ; top5 ->  99.19  and loss:  65.28748242557049
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.08839369242195971
test acc: top1 ->  92.07 ; top5 ->  99.17  and loss:  65.37663488090038
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.08342299272044329
test acc: top1 ->  92.17 ; top5 ->  99.16  and loss:  65.3024667352438
==> this epoch:  38 / 512
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.361328125  ==>  185 / 512 , inc:  64
layer  9  :  0.119140625  ==>  61 / 512 , inc:  4
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.044921875  ==>  23 / 512 , inc:  11
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.07421875  ==>  38 / 512 , inc:  16
eps [1.3839609375000002, 1.3839609375000002, 1.3839609375000002, 1.3839609375000002, 1.3839609375000002, 1.3839609375000002, 1.3839609375000002, 0.016218292236328126, 1.2301875000000002, 1.2301875000000002, 2.07594140625, 1.2301875000000002, 1.037970703125, 19.683000000000003]  wait [2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0, 3, 0]  inc [1, 1, 1, 1, 1, 1, 1, 1, 64, 4, 1, 11, 1, 16]  tol: 3
$$$$$$$$$$$$$ epoch  41  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1170.2058932296932 , diff:  1170.2058932296932
adv train loss:  -1325.4577236175537 , diff:  155.25183038786054
adv train loss:  -1264.9508380889893 , diff:  60.50688552856445
adv train loss:  -1170.060064315796 , diff:  94.89077377319336
adv train loss:  -1172.1371898651123 , diff:  2.0771255493164062
adv train loss:  -1172.8139305114746 , diff:  0.6767406463623047
adv train loss:  -1176.9373264312744 , diff:  4.123395919799805
adv train loss:  -1177.876591682434 , diff:  0.939265251159668
adv train loss:  -1169.3793468475342 , diff:  8.497244834899902
adv train loss:  -1172.2933368682861 , diff:  2.913990020751953
layer  0  adv train finish, try to retain  12
test acc: top1 ->  10.0 ; top5 ->  50.6  and loss:  6775.937461853027
forward train acc: top1 ->  86.43000000488281 ; top5 ->  98.55599998291015  and loss:  70.18620938062668
test acc: top1 ->  75.9 ; top5 ->  96.52  and loss:  110.20367980003357
forward train acc: top1 ->  89.14600001464844 ; top5 ->  99.02800000488281  and loss:  36.961462661623955
test acc: top1 ->  84.26 ; top5 ->  98.05  and loss:  58.89450827240944
forward train acc: top1 ->  90.76799998046874 ; top5 ->  99.356  and loss:  29.165940657258034
test acc: top1 ->  85.42 ; top5 ->  98.41  and loss:  52.90804798901081
forward train acc: top1 ->  91.78999997314453 ; top5 ->  99.47000000244141  and loss:  25.621225029230118
test acc: top1 ->  85.87 ; top5 ->  98.59  and loss:  51.25088474154472
forward train acc: top1 ->  92.69399998046875 ; top5 ->  99.56799997558593  and loss:  22.32888601720333
test acc: top1 ->  86.62 ; top5 ->  98.69  and loss:  49.46196296811104
forward train acc: top1 ->  93.41799997314453 ; top5 ->  99.69599997558593  and loss:  20.183874756097794
test acc: top1 ->  86.8 ; top5 ->  98.81  and loss:  48.17302146553993
forward train acc: top1 ->  93.91999997558594 ; top5 ->  99.6560000024414  and loss:  19.28061217814684
test acc: top1 ->  86.74 ; top5 ->  98.78  and loss:  47.84164960682392
forward train acc: top1 ->  94.15599997070312 ; top5 ->  99.7160000024414  and loss:  17.88825510442257
test acc: top1 ->  87.08 ; top5 ->  98.82  and loss:  47.836666852235794
forward train acc: top1 ->  94.39199999511719 ; top5 ->  99.73399997558593  and loss:  17.02515434473753
test acc: top1 ->  87.15 ; top5 ->  98.9  and loss:  46.80459047853947
forward train acc: top1 ->  94.70399997070312 ; top5 ->  99.74399997558594  and loss:  16.304956316947937
test acc: top1 ->  87.38 ; top5 ->  98.85  and loss:  46.890594348311424
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -5.583134964108467 , diff:  5.583134964108467
adv train loss:  -6.101259417831898 , diff:  0.5181244537234306
adv train loss:  -6.054504282772541 , diff:  0.04675513505935669
adv train loss:  -5.310232795774937 , diff:  0.7442714869976044
adv train loss:  -5.0696564223617315 , diff:  0.24057637341320515
adv train loss:  -4.887545220553875 , diff:  0.18211120180785656
adv train loss:  -4.803908698260784 , diff:  0.08363652229309082
adv train loss:  -5.468521142378449 , diff:  0.6646124441176653
adv train loss:  -5.3801322020590305 , diff:  0.08838894031941891
adv train loss:  -5.2306212447583675 , diff:  0.149510957300663
layer  1  adv train finish, try to retain  48
test acc: top1 ->  21.56 ; top5 ->  53.02  and loss:  504.9800019264221
forward train acc: top1 ->  99.6680000024414 ; top5 ->  99.996  and loss:  1.3273749153595418
test acc: top1 ->  91.66 ; top5 ->  99.29  and loss:  44.640528462827206
forward train acc: top1 ->  99.828 ; top5 ->  100.0  and loss:  0.6113986251875758
test acc: top1 ->  91.88 ; top5 ->  99.33  and loss:  48.55441362410784
forward train acc: top1 ->  99.844 ; top5 ->  99.998  and loss:  0.5073513933457434
test acc: top1 ->  91.74 ; top5 ->  99.23  and loss:  52.079948268830776
forward train acc: top1 ->  99.868 ; top5 ->  100.0  and loss:  0.35244610274094157
test acc: top1 ->  91.76 ; top5 ->  99.27  and loss:  54.14395193755627
forward train acc: top1 ->  99.902 ; top5 ->  100.0  and loss:  0.27647444751346484
test acc: top1 ->  91.58 ; top5 ->  99.29  and loss:  56.432979077100754
forward train acc: top1 ->  99.908 ; top5 ->  100.0  and loss:  0.2973982616676949
test acc: top1 ->  91.76 ; top5 ->  99.33  and loss:  56.74464562535286
forward train acc: top1 ->  99.914 ; top5 ->  99.998  and loss:  0.28762036317493767
test acc: top1 ->  91.77 ; top5 ->  99.31  and loss:  57.11741913855076
forward train acc: top1 ->  99.914 ; top5 ->  100.0  and loss:  0.26185294706374407
test acc: top1 ->  91.91 ; top5 ->  99.29  and loss:  58.31568902730942
forward train acc: top1 ->  99.928 ; top5 ->  100.0  and loss:  0.2293281292077154
test acc: top1 ->  91.84 ; top5 ->  99.27  and loss:  57.801464304327965
forward train acc: top1 ->  99.93199997558594 ; top5 ->  100.0  and loss:  0.19254476903006434
test acc: top1 ->  91.96 ; top5 ->  99.26  and loss:  59.14224515855312
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  55 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -1077.829629441956 , diff:  1077.829629441956
adv train loss:  -1757.2956924438477 , diff:  679.4660630018916
adv train loss:  -1759.2686233520508 , diff:  1.972930908203125
adv train loss:  -1756.4801864624023 , diff:  2.7884368896484375
adv train loss:  -1760.117597579956 , diff:  3.637411117553711
adv train loss:  -1761.2271537780762 , diff:  1.1095561981201172
adv train loss:  -1765.131980895996 , diff:  3.904827117919922
adv train loss:  -1763.4227313995361 , diff:  1.709249496459961
adv train loss:  -1766.0238933563232 , diff:  2.6011619567871094
adv train loss:  -1761.8734493255615 , diff:  4.150444030761719
layer  2  adv train finish, try to retain  59
test acc: top1 ->  36.34 ; top5 ->  73.75  and loss:  1343.2774095535278
forward train acc: top1 ->  96.94600001220704 ; top5 ->  99.902  and loss:  11.214365936815739
test acc: top1 ->  88.86 ; top5 ->  99.06  and loss:  52.46957424283028
forward train acc: top1 ->  97.47199999267578 ; top5 ->  99.94399997558594  and loss:  7.5600581634789705
test acc: top1 ->  89.28 ; top5 ->  99.09  and loss:  46.94212782382965
forward train acc: top1 ->  97.96799998291016 ; top5 ->  99.964  and loss:  6.187588999047875
test acc: top1 ->  89.61 ; top5 ->  99.19  and loss:  45.79179498553276
forward train acc: top1 ->  98.05600000976563 ; top5 ->  99.972  and loss:  5.604202967137098
test acc: top1 ->  89.85 ; top5 ->  99.19  and loss:  45.13147009909153
forward train acc: top1 ->  98.30799997802734 ; top5 ->  99.974  and loss:  4.9163831770420074
test acc: top1 ->  89.98 ; top5 ->  99.19  and loss:  45.78679269552231
forward train acc: top1 ->  98.49200000976562 ; top5 ->  99.99  and loss:  4.5107818162068725
test acc: top1 ->  90.04 ; top5 ->  99.22  and loss:  45.503945514559746
forward train acc: top1 ->  98.52400000976563 ; top5 ->  99.974  and loss:  4.25016718916595
test acc: top1 ->  90.23 ; top5 ->  99.16  and loss:  45.39117346704006
forward train acc: top1 ->  98.65199997558594 ; top5 ->  99.986  and loss:  3.939134239219129
test acc: top1 ->  90.29 ; top5 ->  99.15  and loss:  45.512634962797165
forward train acc: top1 ->  98.68200000244141 ; top5 ->  99.986  and loss:  3.9792891619727015
test acc: top1 ->  90.28 ; top5 ->  99.24  and loss:  45.66859646141529
forward train acc: top1 ->  98.78000000732422 ; top5 ->  99.99  and loss:  3.7783397482708097
test acc: top1 ->  90.37 ; top5 ->  99.2  and loss:  45.44562038779259
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -785.1537811923772 , diff:  785.1537811923772
adv train loss:  -1307.4546155929565 , diff:  522.3008344005793
adv train loss:  -1320.8752813339233 , diff:  13.420665740966797
adv train loss:  -1325.7318687438965 , diff:  4.8565874099731445
adv train loss:  -1321.7126502990723 , diff:  4.019218444824219
adv train loss:  -1327.0650939941406 , diff:  5.352443695068359
adv train loss:  -1320.4991779327393 , diff:  6.565916061401367
adv train loss:  -1330.2190647125244 , diff:  9.719886779785156
adv train loss:  -1348.740556716919 , diff:  18.52149200439453
adv train loss:  -1351.263638496399 , diff:  2.5230817794799805
layer  3  adv train finish, try to retain  48
test acc: top1 ->  36.92 ; top5 ->  79.65  and loss:  752.5610027313232
forward train acc: top1 ->  95.43000001953125 ; top5 ->  99.852  and loss:  14.624500572681427
test acc: top1 ->  88.12 ; top5 ->  98.99  and loss:  46.86599529534578
forward train acc: top1 ->  96.45799998779297 ; top5 ->  99.92799997558593  and loss:  10.559305645525455
test acc: top1 ->  88.43 ; top5 ->  99.0  and loss:  46.26946623623371
forward train acc: top1 ->  96.88400001220703 ; top5 ->  99.93  and loss:  9.023056242614985
test acc: top1 ->  88.7 ; top5 ->  99.14  and loss:  45.597068548202515
forward train acc: top1 ->  97.14599998535157 ; top5 ->  99.95799997558593  and loss:  8.175875011831522
test acc: top1 ->  89.08 ; top5 ->  99.11  and loss:  45.38880334794521
forward train acc: top1 ->  97.50800001464843 ; top5 ->  99.94799997558594  and loss:  7.534913443028927
test acc: top1 ->  89.23 ; top5 ->  99.09  and loss:  45.21195111423731
forward train acc: top1 ->  97.54400001708984 ; top5 ->  99.964  and loss:  7.108488090336323
test acc: top1 ->  89.34 ; top5 ->  99.18  and loss:  45.30656013637781
forward train acc: top1 ->  97.75600001220702 ; top5 ->  99.984  and loss:  6.593613704666495
test acc: top1 ->  89.3 ; top5 ->  99.08  and loss:  45.56997400522232
forward train acc: top1 ->  97.75599998291015 ; top5 ->  99.978  and loss:  6.444170389324427
test acc: top1 ->  89.36 ; top5 ->  99.12  and loss:  45.8035284280777
forward train acc: top1 ->  97.87199998291015 ; top5 ->  99.972  and loss:  6.168036861345172
test acc: top1 ->  89.5 ; top5 ->  99.1  and loss:  45.78000585734844
forward train acc: top1 ->  97.79999998046875 ; top5 ->  99.972  and loss:  6.224294921383262
test acc: top1 ->  89.51 ; top5 ->  99.17  and loss:  45.60170180350542
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -192.43853864492849 , diff:  192.43853864492849
adv train loss:  -1154.6852169036865 , diff:  962.246678258758
adv train loss:  -1402.3604440689087 , diff:  247.67522716522217
adv train loss:  -1437.1890306472778 , diff:  34.82858657836914
adv train loss:  -1446.424572944641 , diff:  9.235542297363281
adv train loss:  -1465.5531692504883 , diff:  19.128596305847168
adv train loss:  -1473.6358308792114 , diff:  8.082661628723145
adv train loss:  -1478.9641199111938 , diff:  5.328289031982422
adv train loss:  -1471.5143947601318 , diff:  7.449725151062012
adv train loss:  -1476.4969663619995 , diff:  4.982571601867676
layer  4  adv train finish, try to retain  70
test acc: top1 ->  35.44 ; top5 ->  81.62  and loss:  983.2976789474487
forward train acc: top1 ->  93.2840000024414 ; top5 ->  99.784  and loss:  19.990615159273148
test acc: top1 ->  86.62 ; top5 ->  98.96  and loss:  48.11812935769558
forward train acc: top1 ->  94.64400001953125 ; top5 ->  99.83199997558594  and loss:  15.6723081022501
test acc: top1 ->  87.41 ; top5 ->  99.02  and loss:  46.38733007013798
forward train acc: top1 ->  95.12799999755859 ; top5 ->  99.882  and loss:  14.0730464681983
test acc: top1 ->  87.67 ; top5 ->  99.03  and loss:  44.845642656087875
forward train acc: top1 ->  95.41799999267577 ; top5 ->  99.90599997558594  and loss:  13.220734603703022
test acc: top1 ->  87.92 ; top5 ->  99.18  and loss:  44.94326175749302
forward train acc: top1 ->  95.89000000244141 ; top5 ->  99.908  and loss:  11.811665564775467
test acc: top1 ->  88.09 ; top5 ->  99.14  and loss:  44.72936753928661
forward train acc: top1 ->  96.08400001464844 ; top5 ->  99.91999997558594  and loss:  11.31878487020731
test acc: top1 ->  88.48 ; top5 ->  99.18  and loss:  43.748726680874825
forward train acc: top1 ->  96.15599997070312 ; top5 ->  99.93  and loss:  11.08681434392929
test acc: top1 ->  88.48 ; top5 ->  99.21  and loss:  44.0654279589653
forward train acc: top1 ->  96.18599998535156 ; top5 ->  99.934  and loss:  10.805334933102131
test acc: top1 ->  88.55 ; top5 ->  99.2  and loss:  43.70675106346607
forward train acc: top1 ->  96.38599998535156 ; top5 ->  99.932  and loss:  10.670412451028824
test acc: top1 ->  88.62 ; top5 ->  99.27  and loss:  42.97698096930981
forward train acc: top1 ->  96.63599998291015 ; top5 ->  99.946  and loss:  9.670297145843506
test acc: top1 ->  88.88 ; top5 ->  99.29  and loss:  43.47967255115509
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -367.332905465737 , diff:  367.332905465737
adv train loss:  -1417.130208015442 , diff:  1049.797302549705
adv train loss:  -1511.5461072921753 , diff:  94.4158992767334
adv train loss:  -1537.8434715270996 , diff:  26.297364234924316
adv train loss:  -1548.513807296753 , diff:  10.67033576965332
adv train loss:  -1542.018398284912 , diff:  6.49540901184082
adv train loss:  -1528.6344566345215 , diff:  13.383941650390625
adv train loss:  -1531.813081741333 , diff:  3.1786251068115234
adv train loss:  -1528.9854946136475 , diff:  2.827587127685547
adv train loss:  -1534.0772724151611 , diff:  5.091777801513672
layer  5  adv train finish, try to retain  55
test acc: top1 ->  41.42 ; top5 ->  86.65  and loss:  679.6732831001282
forward train acc: top1 ->  92.00400000488281 ; top5 ->  99.77999997558594  and loss:  23.470367923378944
test acc: top1 ->  86.72 ; top5 ->  99.07  and loss:  48.186009272933006
forward train acc: top1 ->  94.27599999267578 ; top5 ->  99.882  and loss:  16.912873715162277
test acc: top1 ->  87.58 ; top5 ->  99.16  and loss:  46.09559868276119
forward train acc: top1 ->  95.11800001464844 ; top5 ->  99.90399997558593  and loss:  14.384077161550522
test acc: top1 ->  88.04 ; top5 ->  99.19  and loss:  44.668327420949936
forward train acc: top1 ->  95.51999998779297 ; top5 ->  99.938  and loss:  12.992597937583923
test acc: top1 ->  88.48 ; top5 ->  99.29  and loss:  43.85680288076401
forward train acc: top1 ->  95.87199999267578 ; top5 ->  99.95  and loss:  11.69789545238018
test acc: top1 ->  88.79 ; top5 ->  99.27  and loss:  44.99443654716015
forward train acc: top1 ->  96.13800001220703 ; top5 ->  99.926  and loss:  10.947068311274052
test acc: top1 ->  88.85 ; top5 ->  99.26  and loss:  43.98919315636158
forward train acc: top1 ->  96.28000000488281 ; top5 ->  99.96  and loss:  10.642678633332253
test acc: top1 ->  88.85 ; top5 ->  99.23  and loss:  44.03676964342594
forward train acc: top1 ->  96.37000001708985 ; top5 ->  99.954  and loss:  10.33988806605339
test acc: top1 ->  88.97 ; top5 ->  99.27  and loss:  43.71873651444912
forward train acc: top1 ->  96.4679999975586 ; top5 ->  99.9600000024414  and loss:  10.251199945807457
test acc: top1 ->  88.99 ; top5 ->  99.29  and loss:  43.51437009871006
forward train acc: top1 ->  96.65800000732422 ; top5 ->  99.966  and loss:  9.692240215837955
test acc: top1 ->  89.25 ; top5 ->  99.27  and loss:  44.28825372457504
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -178.5977082606405 , diff:  178.5977082606405
adv train loss:  -1409.9403972625732 , diff:  1231.3426890019327
adv train loss:  -1569.540548324585 , diff:  159.60015106201172
adv train loss:  -1605.698558807373 , diff:  36.158010482788086
adv train loss:  -1620.172194480896 , diff:  14.47363567352295
adv train loss:  -1627.0700178146362 , diff:  6.897823333740234
adv train loss:  -1648.6727600097656 , diff:  21.602742195129395
adv train loss:  -1646.9033908843994 , diff:  1.769369125366211
adv train loss:  -1647.6311388015747 , diff:  0.727747917175293
adv train loss:  -1646.2388515472412 , diff:  1.392287254333496
layer  6  adv train finish, try to retain  56
test acc: top1 ->  66.93 ; top5 ->  93.46  and loss:  410.87441182136536
forward train acc: top1 ->  98.2760000024414 ; top5 ->  99.988  and loss:  5.218864441849291
test acc: top1 ->  90.66 ; top5 ->  99.39  and loss:  43.50602646172047
forward train acc: top1 ->  98.98799997802735 ; top5 ->  99.992  and loss:  2.8523382740095258
test acc: top1 ->  90.83 ; top5 ->  99.4  and loss:  45.74649652838707
forward train acc: top1 ->  99.15599998291016 ; top5 ->  99.992  and loss:  2.5115840672515333
test acc: top1 ->  90.93 ; top5 ->  99.39  and loss:  45.780621126294136
forward train acc: top1 ->  99.26799997558594 ; top5 ->  99.998  and loss:  2.0672817081212997
test acc: top1 ->  91.11 ; top5 ->  99.34  and loss:  46.621219485998154
forward train acc: top1 ->  99.35399997558594 ; top5 ->  99.998  and loss:  1.80563087714836
test acc: top1 ->  91.24 ; top5 ->  99.37  and loss:  47.325237929821014
forward train acc: top1 ->  99.468 ; top5 ->  99.998  and loss:  1.5652426851447672
test acc: top1 ->  91.27 ; top5 ->  99.41  and loss:  47.558016151189804
forward train acc: top1 ->  99.37799997558594 ; top5 ->  99.998  and loss:  1.711954114260152
test acc: top1 ->  91.13 ; top5 ->  99.4  and loss:  47.26514793932438
forward train acc: top1 ->  99.49199997558594 ; top5 ->  100.0  and loss:  1.51891301269643
test acc: top1 ->  91.17 ; top5 ->  99.38  and loss:  48.91490940749645
forward train acc: top1 ->  99.53800000488282 ; top5 ->  100.0  and loss:  1.2460806935559958
test acc: top1 ->  91.28 ; top5 ->  99.38  and loss:  49.00619573891163
forward train acc: top1 ->  99.50999997558594 ; top5 ->  99.998  and loss:  1.3463427745737135
test acc: top1 ->  91.17 ; top5 ->  99.36  and loss:  49.350913405418396
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  144 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -0.4367714188992977 , diff:  0.4367714188992977
adv train loss:  -0.40597241463547107 , diff:  0.030799004263826646
adv train loss:  -0.39703673601616174 , diff:  0.008935678619309328
layer  7  adv train finish, try to retain  257
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -0.8175512906163931 , diff:  0.8175512906163931
adv train loss:  -0.6834908777382225 , diff:  0.1340604128781706
adv train loss:  -0.8173001620452851 , diff:  0.13380928430706263
adv train loss:  -0.7809780854731798 , diff:  0.03632207657210529
adv train loss:  -0.7673836522735655 , diff:  0.013594433199614286
adv train loss:  -0.6620574865955859 , diff:  0.10532616567797959
adv train loss:  -0.7266327980905771 , diff:  0.06457531149499118
adv train loss:  -0.7219121244270355 , diff:  0.004720673663541675
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  121
test acc: top1 ->  26.34 ; top5 ->  62.16  and loss:  8467606.24609375
forward train acc: top1 ->  99.07999997558593 ; top5 ->  99.992  and loss:  3.570151279680431
test acc: top1 ->  91.24 ; top5 ->  98.93  and loss:  47.74363912642002
forward train acc: top1 ->  99.78199997558593 ; top5 ->  99.996  and loss:  0.8750556251034141
test acc: top1 ->  91.32 ; top5 ->  99.09  and loss:  50.385431587696075
forward train acc: top1 ->  99.87799997558594 ; top5 ->  100.0  and loss:  0.5221867267973721
test acc: top1 ->  91.71 ; top5 ->  99.09  and loss:  52.02514934539795
forward train acc: top1 ->  99.91 ; top5 ->  100.0  and loss:  0.33394899033010006
test acc: top1 ->  91.59 ; top5 ->  99.13  and loss:  55.09775234758854
forward train acc: top1 ->  99.8920000024414 ; top5 ->  100.0  and loss:  0.35578883811831474
test acc: top1 ->  91.81 ; top5 ->  99.14  and loss:  54.65116649866104
forward train acc: top1 ->  99.91599997558593 ; top5 ->  100.0  and loss:  0.3077671127393842
test acc: top1 ->  91.95 ; top5 ->  99.11  and loss:  55.29154533147812
forward train acc: top1 ->  99.924 ; top5 ->  100.0  and loss:  0.2515349338937085
test acc: top1 ->  92.04 ; top5 ->  99.15  and loss:  55.65156088769436
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.2020279517164454
test acc: top1 ->  91.94 ; top5 ->  99.17  and loss:  56.88931950926781
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.24239844037219882
test acc: top1 ->  92.0 ; top5 ->  99.11  and loss:  57.64894935488701
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.18118751198926475
test acc: top1 ->  91.82 ; top5 ->  99.12  and loss:  57.87032674252987
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  185 / 512 , inc:  64
---------------- start layer  9  ---------------
adv train loss:  -45.77505660057068 , diff:  45.77505660057068
adv train loss:  -54.09858909249306 , diff:  8.323532491922379
adv train loss:  -54.67409020662308 , diff:  0.5755011141300201
adv train loss:  -53.66756051778793 , diff:  1.006529688835144
adv train loss:  -54.82565239071846 , diff:  1.1580918729305267
adv train loss:  -55.3410904109478 , diff:  0.5154380202293396
adv train loss:  -54.40702688694 , diff:  0.9340635240077972
adv train loss:  -54.67271354794502 , diff:  0.26568666100502014
adv train loss:  -54.644835382699966 , diff:  0.027878165245056152
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  57
test acc: top1 ->  10.0 ; top5 ->  59.32  and loss:  541682.3823242188
forward train acc: top1 ->  95.92 ; top5 ->  99.994  and loss:  19.43943351972848
test acc: top1 ->  91.28 ; top5 ->  98.84  and loss:  54.5062460899353
forward train acc: top1 ->  99.864 ; top5 ->  100.0  and loss:  0.6355149520095438
test acc: top1 ->  91.69 ; top5 ->  98.85  and loss:  52.3043742030859
forward train acc: top1 ->  99.912 ; top5 ->  100.0  and loss:  0.4228900078451261
test acc: top1 ->  91.85 ; top5 ->  98.93  and loss:  51.8500963896513
forward train acc: top1 ->  99.93599997558594 ; top5 ->  100.0  and loss:  0.31658761529251933
test acc: top1 ->  92.0 ; top5 ->  98.96  and loss:  52.04230371117592
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.2121589886955917
test acc: top1 ->  92.06 ; top5 ->  98.93  and loss:  53.53536342084408
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.19648479018360376
test acc: top1 ->  92.11 ; top5 ->  98.94  and loss:  53.09032253921032
==> this epoch:  57 / 512
---------------- start layer  10  ---------------
adv train loss:  -1873.610704421997 , diff:  1873.610704421997
adv train loss:  -1876.023738861084 , diff:  2.413034439086914
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  10
test acc: top1 ->  28.51 ; top5 ->  69.8  and loss:  70241.58666992188
forward train acc: top1 ->  94.62399997558593 ; top5 ->  100.0  and loss:  26.43811645358801
test acc: top1 ->  90.72 ; top5 ->  98.72  and loss:  52.47726231813431
forward train acc: top1 ->  99.84399997558593 ; top5 ->  99.998  and loss:  0.9216568651609123
test acc: top1 ->  91.33 ; top5 ->  98.75  and loss:  51.086609452962875
forward train acc: top1 ->  99.86999997558594 ; top5 ->  100.0  and loss:  0.6983922263607383
test acc: top1 ->  91.46 ; top5 ->  98.78  and loss:  51.36408290266991
forward train acc: top1 ->  99.89799997558593 ; top5 ->  100.0  and loss:  0.478941791690886
test acc: top1 ->  91.57 ; top5 ->  98.86  and loss:  52.098703786730766
forward train acc: top1 ->  99.91799997558594 ; top5 ->  100.0  and loss:  0.4370507560670376
test acc: top1 ->  91.8 ; top5 ->  98.87  and loss:  52.75223122537136
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.300942555652
test acc: top1 ->  91.81 ; top5 ->  98.93  and loss:  52.508312091231346
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.2847647095331922
test acc: top1 ->  91.88 ; top5 ->  98.89  and loss:  53.53961105644703
forward train acc: top1 ->  99.928 ; top5 ->  100.0  and loss:  0.2668957430869341
test acc: top1 ->  91.85 ; top5 ->  98.97  and loss:  53.855213552713394
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.2541944438125938
test acc: top1 ->  91.97 ; top5 ->  98.9  and loss:  53.825349137187004
forward train acc: top1 ->  99.94 ; top5 ->  100.0  and loss:  0.2510053808509838
test acc: top1 ->  91.82 ; top5 ->  98.91  and loss:  54.20716907083988
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  11 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -972.3875007629395 , diff:  972.3875007629395
adv train loss:  -999.4009313583374 , diff:  27.01343059539795
adv train loss:  -1000.1396532058716 , diff:  0.7387218475341797
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  12
test acc: top1 ->  18.86 ; top5 ->  50.0  and loss:  25709.138916015625
forward train acc: top1 ->  94.734 ; top5 ->  99.688  and loss:  34.48017677851021
test acc: top1 ->  91.08 ; top5 ->  98.84  and loss:  41.45054343342781
forward train acc: top1 ->  99.888 ; top5 ->  100.0  and loss:  0.966042039450258
test acc: top1 ->  91.64 ; top5 ->  98.91  and loss:  39.812382742762566
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.5907526207156479
test acc: top1 ->  91.81 ; top5 ->  98.95  and loss:  39.89016514271498
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.4107118877582252
test acc: top1 ->  91.89 ; top5 ->  99.01  and loss:  40.499041587114334
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.30081413872539997
test acc: top1 ->  91.89 ; top5 ->  99.06  and loss:  40.97278382629156
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.24993651872500777
test acc: top1 ->  92.04 ; top5 ->  99.09  and loss:  41.3107992708683
forward train acc: top1 ->  99.96799997558594 ; top5 ->  100.0  and loss:  0.2319483612664044
test acc: top1 ->  91.95 ; top5 ->  99.03  and loss:  41.76035653054714
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.21559168631210923
test acc: top1 ->  91.97 ; top5 ->  99.09  and loss:  41.96996773034334
forward train acc: top1 ->  99.97599997558594 ; top5 ->  100.0  and loss:  0.19846533611416817
test acc: top1 ->  92.06 ; top5 ->  99.07  and loss:  42.3101355060935
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.18890250870026648
test acc: top1 ->  92.07 ; top5 ->  99.11  and loss:  42.99922998249531
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  23 / 512 , inc:  11
---------------- start layer  12  ---------------
### skip layer  12 wait:  3  ###
---------------- start layer  13  ---------------
adv train loss:  -15772.26586151123 , diff:  15772.26586151123
adv train loss:  -29746.376586914062 , diff:  13974.110725402832
adv train loss:  -42498.252197265625 , diff:  12751.875610351562
adv train loss:  -54637.75018310547 , diff:  12139.497985839844
adv train loss:  -66531.65539550781 , diff:  11893.905212402344
adv train loss:  -78286.55419921875 , diff:  11754.898803710938
adv train loss:  -89940.87310791016 , diff:  11654.318908691406
adv train loss:  -101532.59167480469 , diff:  11591.718566894531
adv train loss:  -113065.48400878906 , diff:  11532.892333984375
adv train loss:  -124576.59057617188 , diff:  11511.106567382812
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  22
test acc: top1 ->  46.0 ; top5 ->  88.25  and loss:  694.3719415664673
forward train acc: top1 ->  93.18800000244141 ; top5 ->  99.482  and loss:  48.56069372873753
test acc: top1 ->  91.72 ; top5 ->  98.92  and loss:  43.94155691564083
forward train acc: top1 ->  99.90999997558593 ; top5 ->  100.0  and loss:  0.5645802514627576
test acc: top1 ->  91.89 ; top5 ->  98.96  and loss:  43.2229829877615
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.35404513753019273
test acc: top1 ->  91.92 ; top5 ->  98.91  and loss:  44.13059623539448
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.24069319502450526
test acc: top1 ->  91.99 ; top5 ->  98.96  and loss:  45.301297552883625
forward train acc: top1 ->  99.97599997558594 ; top5 ->  100.0  and loss:  0.20266809919849038
test acc: top1 ->  92.15 ; top5 ->  98.95  and loss:  45.83407724648714
==> this epoch:  22 / 512
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.361328125  ==>  185 / 512 , inc:  32
layer  9  :  0.111328125  ==>  57 / 512 , inc:  8
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.044921875  ==>  23 / 512 , inc:  5
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.04296875  ==>  22 / 512 , inc:  11
eps [1.037970703125, 1.037970703125, 1.037970703125, 1.037970703125, 1.037970703125, 1.037970703125, 1.037970703125, 0.03243658447265625, 0.9226406250000001, 1.2301875000000002, 1.5569560546875, 0.9226406250000001, 1.037970703125, 19.683000000000003]  wait [4, 4, 4, 4, 4, 4, 4, 2, 2, 0, 4, 2, 2, 0]  inc [1, 1, 1, 1, 1, 1, 1, 1, 32, 8, 1, 5, 1, 11]  tol: 3
$$$$$$$$$$$$$ epoch  42  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
### skip layer  1 wait:  4  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
adv train loss:  -0.2044697439414449 , diff:  0.2044697439414449
adv train loss:  -0.24115391832310706 , diff:  0.03668417438166216
adv train loss:  -0.23561602557310835 , diff:  0.0055378927499987185
layer  7  adv train finish, try to retain  258
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -0.9278065576218069 , diff:  0.9278065576218069
adv train loss:  -0.941948845051229 , diff:  0.01414228742942214
adv train loss:  -0.9380825986154377 , diff:  0.003866246435791254
layer  8  adv train finish, try to retain  327
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -1151.7218255996704 , diff:  1151.7218255996704
adv train loss:  -1154.2981996536255 , diff:  2.576374053955078
adv train loss:  -1156.0217008590698 , diff:  1.723501205444336
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  49
test acc: top1 ->  15.58 ; top5 ->  59.96  and loss:  15437.822219848633
forward train acc: top1 ->  99.484 ; top5 ->  100.0  and loss:  1.8774501460138708
test acc: top1 ->  92.27 ; top5 ->  98.95  and loss:  49.91532437503338
==> this epoch:  49 / 512
---------------- start layer  10  ---------------
### skip layer  10 wait:  4  ###
---------------- start layer  11  ---------------
adv train loss:  -144.24692690372467 , diff:  144.24692690372467
adv train loss:  -144.61858224868774 , diff:  0.37165534496307373
adv train loss:  -144.95959603786469 , diff:  0.3410137891769409
adv train loss:  -144.2473601102829 , diff:  0.7122359275817871
adv train loss:  -145.19369864463806 , diff:  0.9463385343551636
adv train loss:  -145.06786024570465 , diff:  0.12583839893341064
layer  11  adv train finish, try to retain  449
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -1343.907320022583 , diff:  1343.907320022583
adv train loss:  -1576.4436693191528 , diff:  232.53634929656982
adv train loss:  -1590.1714630126953 , diff:  13.72779369354248
adv train loss:  -1636.0490827560425 , diff:  45.87761974334717
adv train loss:  -1641.10262966156 , diff:  5.053546905517578
adv train loss:  -1639.7080879211426 , diff:  1.3945417404174805
adv train loss:  -1640.522560119629 , diff:  0.8144721984863281
layer  12  adv train finish, try to retain  468
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -15053.921661376953 , diff:  15053.921661376953
adv train loss:  -25501.277862548828 , diff:  10447.356201171875
adv train loss:  -35582.83319091797 , diff:  10081.55532836914
adv train loss:  -45535.223388671875 , diff:  9952.390197753906
adv train loss:  -55453.04507446289 , diff:  9917.821685791016
adv train loss:  -65324.890625 , diff:  9871.84555053711
adv train loss:  -75164.00787353516 , diff:  9839.117248535156
adv train loss:  -84979.82482910156 , diff:  9815.816955566406
adv train loss:  -94825.86358642578 , diff:  9846.038757324219
adv train loss:  -104663.18731689453 , diff:  9837.32373046875
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  11
test acc: top1 ->  22.91 ; top5 ->  65.26  and loss:  709.94944190979
forward train acc: top1 ->  73.03199998046875 ; top5 ->  90.522  and loss:  160.14547970891
test acc: top1 ->  88.22 ; top5 ->  97.45  and loss:  51.64008465409279
forward train acc: top1 ->  98.89799997802734 ; top5 ->  99.93  and loss:  7.660216320306063
test acc: top1 ->  91.09 ; top5 ->  98.84  and loss:  36.21856155991554
forward train acc: top1 ->  99.81399997558594 ; top5 ->  100.0  and loss:  2.7089248094707727
test acc: top1 ->  91.59 ; top5 ->  98.98  and loss:  33.668962091207504
forward train acc: top1 ->  99.93199997558594 ; top5 ->  100.0  and loss:  1.4340958008542657
test acc: top1 ->  91.84 ; top5 ->  99.0  and loss:  34.02767699956894
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.8894281107932329
test acc: top1 ->  91.88 ; top5 ->  98.97  and loss:  34.59913282096386
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.7118446081876755
test acc: top1 ->  92.02 ; top5 ->  99.0  and loss:  34.86290846765041
forward train acc: top1 ->  99.93799997558594 ; top5 ->  100.0  and loss:  0.6638731919229031
test acc: top1 ->  92.04 ; top5 ->  99.01  and loss:  35.35554265975952
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.5434458474628627
test acc: top1 ->  92.09 ; top5 ->  99.02  and loss:  35.55634942650795
forward train acc: top1 ->  99.942 ; top5 ->  100.0  and loss:  0.5291036353446543
test acc: top1 ->  92.07 ; top5 ->  99.02  and loss:  35.86355848610401
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.4168820008635521
test acc: top1 ->  92.15 ; top5 ->  99.02  and loss:  36.14440310001373
==> this epoch:  11 / 512
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.361328125  ==>  185 / 512 , inc:  32
layer  9  :  0.095703125  ==>  49 / 512 , inc:  16
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.044921875  ==>  23 / 512 , inc:  5
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.021484375  ==>  11 / 512 , inc:  5
eps [1.037970703125, 1.037970703125, 1.037970703125, 1.037970703125, 1.037970703125, 1.037970703125, 1.037970703125, 0.0648731689453125, 1.8452812500000002, 1.2301875000000002, 1.5569560546875, 1.8452812500000002, 2.07594140625, 19.683000000000003]  wait [3, 3, 3, 3, 3, 3, 3, 2, 2, 0, 3, 2, 2, 0]  inc [1, 1, 1, 1, 1, 1, 1, 1, 32, 16, 1, 5, 1, 5]  tol: 3
$$$$$$$$$$$$$ epoch  43  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
### skip layer  1 wait:  3  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  3  ###
---------------- start layer  7  ---------------
adv train loss:  -0.6070505958050489 , diff:  0.6070505958050489
adv train loss:  -0.5549457410816103 , diff:  0.05210485472343862
adv train loss:  -0.5433187396265566 , diff:  0.011627001455053687
adv train loss:  -0.58002576418221 , diff:  0.036707024555653334
adv train loss:  -0.5714719640091062 , diff:  0.00855380017310381
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  252
test acc: top1 ->  24.84 ; top5 ->  64.71  and loss:  392.983868598938
forward train acc: top1 ->  99.348 ; top5 ->  99.992  and loss:  2.7159778838977218
test acc: top1 ->  90.91 ; top5 ->  98.91  and loss:  43.23825454711914
forward train acc: top1 ->  99.67999997558594 ; top5 ->  99.998  and loss:  1.2612480837851763
test acc: top1 ->  91.25 ; top5 ->  98.99  and loss:  45.060684226453304
forward train acc: top1 ->  99.80000000244141 ; top5 ->  100.0  and loss:  0.8040075385943055
test acc: top1 ->  91.36 ; top5 ->  99.06  and loss:  47.654229655861855
forward train acc: top1 ->  99.846 ; top5 ->  100.0  and loss:  0.62116266367957
test acc: top1 ->  91.39 ; top5 ->  99.05  and loss:  47.985544323921204
forward train acc: top1 ->  99.89199997558593 ; top5 ->  100.0  and loss:  0.44012690149247646
test acc: top1 ->  91.44 ; top5 ->  99.04  and loss:  49.0690438747406
forward train acc: top1 ->  99.864 ; top5 ->  100.0  and loss:  0.4461764208972454
test acc: top1 ->  91.48 ; top5 ->  99.14  and loss:  49.907563887536526
forward train acc: top1 ->  99.896 ; top5 ->  100.0  and loss:  0.40498558804392815
test acc: top1 ->  91.46 ; top5 ->  99.15  and loss:  50.01163078099489
forward train acc: top1 ->  99.874 ; top5 ->  100.0  and loss:  0.4266210838686675
test acc: top1 ->  91.39 ; top5 ->  99.13  and loss:  50.47579977661371
forward train acc: top1 ->  99.91599997558593 ; top5 ->  100.0  and loss:  0.3448194975499064
test acc: top1 ->  91.46 ; top5 ->  99.15  and loss:  51.14121524244547
forward train acc: top1 ->  99.928 ; top5 ->  100.0  and loss:  0.2591869398020208
test acc: top1 ->  91.52 ; top5 ->  99.12  and loss:  51.60973843932152
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  253 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -0.33442094933707267 , diff:  0.33442094933707267
adv train loss:  -0.3445099601522088 , diff:  0.010089010815136135
adv train loss:  -0.3161589172668755 , diff:  0.0283510428853333
adv train loss:  -0.32180719706229866 , diff:  0.00564827979542315
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  153
test acc: top1 ->  10.19 ; top5 ->  51.37  and loss:  11260675.0078125
forward train acc: top1 ->  98.55 ; top5 ->  99.968  and loss:  4.718656582990661
test acc: top1 ->  91.42 ; top5 ->  98.68  and loss:  50.5076614767313
forward train acc: top1 ->  99.81599997558594 ; top5 ->  100.0  and loss:  0.7139785895124078
test acc: top1 ->  91.79 ; top5 ->  98.75  and loss:  50.144792690873146
forward train acc: top1 ->  99.91799997558594 ; top5 ->  100.0  and loss:  0.42450446216389537
test acc: top1 ->  91.87 ; top5 ->  98.81  and loss:  50.47333613038063
forward train acc: top1 ->  99.902 ; top5 ->  100.0  and loss:  0.34828266000840813
test acc: top1 ->  91.85 ; top5 ->  98.9  and loss:  51.695422634482384
forward train acc: top1 ->  99.936 ; top5 ->  100.0  and loss:  0.2634959506103769
test acc: top1 ->  91.82 ; top5 ->  98.86  and loss:  52.29304435104132
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.2136281655402854
test acc: top1 ->  91.91 ; top5 ->  98.86  and loss:  52.91980550438166
forward train acc: top1 ->  99.942 ; top5 ->  100.0  and loss:  0.23234850575681776
test acc: top1 ->  91.85 ; top5 ->  98.84  and loss:  53.38897478580475
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.1883898755768314
test acc: top1 ->  92.02 ; top5 ->  98.9  and loss:  53.47639747709036
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.1792947662761435
test acc: top1 ->  92.1 ; top5 ->  98.86  and loss:  53.37991550564766
forward train acc: top1 ->  99.942 ; top5 ->  100.0  and loss:  0.1901133864885196
test acc: top1 ->  92.0 ; top5 ->  98.92  and loss:  54.10235057771206
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  185 / 512 , inc:  32
---------------- start layer  9  ---------------
adv train loss:  -49.38648436963558 , diff:  49.38648436963558
adv train loss:  -49.68000453710556 , diff:  0.29352016746997833
adv train loss:  -49.75508072972298 , diff:  0.07507619261741638
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  33
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  830696.015625
forward train acc: top1 ->  96.99 ; top5 ->  99.992  and loss:  11.371969633735716
test acc: top1 ->  91.04 ; top5 ->  98.89  and loss:  47.70548915863037
forward train acc: top1 ->  99.838 ; top5 ->  99.998  and loss:  1.1862687775865197
test acc: top1 ->  91.47 ; top5 ->  98.92  and loss:  45.78432358801365
forward train acc: top1 ->  99.888 ; top5 ->  99.998  and loss:  0.7225092113949358
test acc: top1 ->  91.5 ; top5 ->  98.95  and loss:  47.20975057780743
forward train acc: top1 ->  99.92399997558594 ; top5 ->  100.0  and loss:  0.5127805939409882
test acc: top1 ->  91.6 ; top5 ->  99.0  and loss:  46.942612662911415
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.3961807368323207
test acc: top1 ->  91.67 ; top5 ->  99.02  and loss:  48.34510410577059
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.370117565151304
test acc: top1 ->  91.76 ; top5 ->  99.03  and loss:  48.49258770048618
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.29978752217721194
test acc: top1 ->  91.68 ; top5 ->  99.01  and loss:  48.65828551352024
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.26089878036873415
test acc: top1 ->  91.79 ; top5 ->  99.03  and loss:  49.418718464672565
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.24289560865145177
test acc: top1 ->  91.78 ; top5 ->  99.06  and loss:  49.508100122213364
forward train acc: top1 ->  99.95399997558594 ; top5 ->  100.0  and loss:  0.23314425838179886
test acc: top1 ->  91.83 ; top5 ->  99.04  and loss:  50.19632697105408
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  49 / 512 , inc:  16
---------------- start layer  10  ---------------
### skip layer  10 wait:  3  ###
---------------- start layer  11  ---------------
adv train loss:  -523.8988285064697 , diff:  523.8988285064697
adv train loss:  -634.3513674736023 , diff:  110.45253896713257
adv train loss:  -745.9004521369934 , diff:  111.54908466339111
adv train loss:  -879.0385608673096 , diff:  133.13810873031616
adv train loss:  -1028.1171436309814 , diff:  149.07858276367188
adv train loss:  -1027.3407182693481 , diff:  0.7764253616333008
adv train loss:  -1051.2982444763184 , diff:  23.957526206970215
adv train loss:  -1330.1840677261353 , diff:  278.8858232498169
adv train loss:  -1632.0410614013672 , diff:  301.85699367523193
adv train loss:  -1726.5393161773682 , diff:  94.49825477600098
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  18
test acc: top1 ->  10.95 ; top5 ->  69.08  and loss:  15768.237480163574
forward train acc: top1 ->  99.492 ; top5 ->  100.0  and loss:  1.4155849895905703
test acc: top1 ->  92.06 ; top5 ->  99.02  and loss:  50.99309200048447
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.13445139722898602
test acc: top1 ->  92.12 ; top5 ->  99.0  and loss:  51.4879414588213
==> this epoch:  18 / 512
---------------- start layer  12  ---------------
adv train loss:  -0.10987727524479851 , diff:  0.10987727524479851
adv train loss:  -0.12609247383079492 , diff:  0.016215198585996404
adv train loss:  -0.12347726155712735 , diff:  0.0026152122736675665
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  61
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  825.3186230659485
forward train acc: top1 ->  10.000000001220704 ; top5 ->  49.986  and loss:  608.2119369506836
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  382.67641615867615
forward train acc: top1 ->  10.00000000213623 ; top5 ->  49.89999998657227  and loss:  285.8794515132904
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  240.74651670455933
forward train acc: top1 ->  9.791999999084473 ; top5 ->  50.00400000610352  and loss:  227.50143432617188
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.30107188224792
forward train acc: top1 ->  9.843999998779298 ; top5 ->  49.776000006103516  and loss:  225.69864630699158
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.27563309669495
forward train acc: top1 ->  9.703999998168944 ; top5 ->  49.83799999511719  and loss:  225.7185981273651
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.270489692688
forward train acc: top1 ->  9.85400000213623 ; top5 ->  50.079999991455075  and loss:  225.6772575378418
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.26770901679993
forward train acc: top1 ->  9.948000002136231 ; top5 ->  49.80800000976563  and loss:  225.68268203735352
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.28617906570435
forward train acc: top1 ->  10.094000001831054 ; top5 ->  49.73799999267578  and loss:  225.67790293693542
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.2836308479309
forward train acc: top1 ->  9.935999996948242 ; top5 ->  49.83399999511719  and loss:  225.68885946273804
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.26381373405457
forward train acc: top1 ->  9.902 ; top5 ->  49.768  and loss:  225.68001103401184
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.27885913848877
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  62 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -12862.977760314941 , diff:  12862.977760314941
adv train loss:  -20858.901931762695 , diff:  7995.924171447754
adv train loss:  -28797.150680541992 , diff:  7938.248748779297
adv train loss:  -36734.58563232422 , diff:  7937.434951782227
adv train loss:  -44664.03918457031 , diff:  7929.453552246094
adv train loss:  -52592.56231689453 , diff:  7928.523132324219
adv train loss:  -60512.73193359375 , diff:  7920.169616699219
adv train loss:  -68393.30236816406 , diff:  7880.5704345703125
adv train loss:  -76355.12646484375 , diff:  7961.8240966796875
adv train loss:  -84236.2138671875 , diff:  7881.08740234375
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  6
test acc: top1 ->  15.3 ; top5 ->  59.18  and loss:  614.8941006660461
forward train acc: top1 ->  48.58600000244141 ; top5 ->  87.872  and loss:  229.09940093755722
test acc: top1 ->  80.66 ; top5 ->  98.54  and loss:  75.30450463294983
forward train acc: top1 ->  99.222 ; top5 ->  99.994  and loss:  24.864180028438568
test acc: top1 ->  91.45 ; top5 ->  98.9  and loss:  38.686263263225555
forward train acc: top1 ->  99.924 ; top5 ->  99.998  and loss:  8.154708500951529
test acc: top1 ->  91.69 ; top5 ->  99.04  and loss:  34.14315463602543
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  4.358338870108128
test acc: top1 ->  91.8 ; top5 ->  99.09  and loss:  33.35135155916214
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  2.699111146852374
test acc: top1 ->  92.07 ; top5 ->  99.06  and loss:  33.362352162599564
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  1.9684860911220312
test acc: top1 ->  92.02 ; top5 ->  99.04  and loss:  33.43851152062416
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  1.641883703880012
test acc: top1 ->  91.99 ; top5 ->  99.07  and loss:  33.68135778605938
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  1.371505538932979
test acc: top1 ->  91.99 ; top5 ->  99.08  and loss:  34.03738933801651
forward train acc: top1 ->  99.97399997558594 ; top5 ->  100.0  and loss:  1.1969541320577264
test acc: top1 ->  91.96 ; top5 ->  99.05  and loss:  34.58087846636772
forward train acc: top1 ->  99.96999997558594 ; top5 ->  100.0  and loss:  0.996809521690011
test acc: top1 ->  92.08 ; top5 ->  99.02  and loss:  34.618761986494064
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  11 / 512 , inc:  5
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.361328125  ==>  185 / 512 , inc:  16
layer  9  :  0.095703125  ==>  49 / 512 , inc:  8
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.03515625  ==>  18 / 512 , inc:  9
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.021484375  ==>  11 / 512 , inc:  2
eps [1.037970703125, 1.037970703125, 1.037970703125, 1.037970703125, 1.037970703125, 1.037970703125, 1.037970703125, 0.04865487670898438, 1.3839609375000002, 0.9226406250000001, 1.5569560546875, 1.8452812500000002, 1.5569560546875, 14.762250000000002]  wait [2, 2, 2, 2, 2, 2, 2, 4, 4, 2, 2, 0, 4, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 16, 8, 1, 9, 1, 2]  tol: 3
$$$$$$$$$$$$$ epoch  44  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1405.954360961914 , diff:  1405.954360961914
adv train loss:  -1594.1786842346191 , diff:  188.22432327270508
adv train loss:  -1592.2422151565552 , diff:  1.9364690780639648
adv train loss:  -1588.818962097168 , diff:  3.423253059387207
adv train loss:  -1596.8000326156616 , diff:  7.981070518493652
adv train loss:  -1587.827236175537 , diff:  8.972796440124512
adv train loss:  -1592.843334197998 , diff:  5.0160980224609375
adv train loss:  -1592.7783498764038 , diff:  0.06498432159423828
layer  0  adv train finish, try to retain  59
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -393.6729726791382 , diff:  393.6729726791382
adv train loss:  -393.89582109451294 , diff:  0.22284841537475586
layer  1  adv train finish, try to retain  62
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -1455.797738313675 , diff:  1455.797738313675
adv train loss:  -1945.4290370941162 , diff:  489.6312987804413
adv train loss:  -1948.0632400512695 , diff:  2.6342029571533203
adv train loss:  -1949.237361907959 , diff:  1.1741218566894531
adv train loss:  -1949.2697257995605 , diff:  0.0323638916015625
layer  2  adv train finish, try to retain  118
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -1405.1613318920135 , diff:  1405.1613318920135
adv train loss:  -1928.3559551239014 , diff:  523.1946232318878
adv train loss:  -1945.3037128448486 , diff:  16.947757720947266
adv train loss:  -1953.6568660736084 , diff:  8.353153228759766
adv train loss:  -1970.1335525512695 , diff:  16.476686477661133
adv train loss:  -1973.8705444335938 , diff:  3.7369918823242188
adv train loss:  -1987.378360748291 , diff:  13.507816314697266
adv train loss:  -1994.363531112671 , diff:  6.985170364379883
adv train loss:  -1992.8876304626465 , diff:  1.475900650024414
adv train loss:  -1997.000841140747 , diff:  4.113210678100586
layer  3  adv train finish, try to retain  119
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -667.2407104969025 , diff:  667.2407104969025
adv train loss:  -2006.270887374878 , diff:  1339.0301768779755
adv train loss:  -2165.9975605010986 , diff:  159.7266731262207
adv train loss:  -2206.7697467803955 , diff:  40.772186279296875
adv train loss:  -2212.0095710754395 , diff:  5.239824295043945
adv train loss:  -2221.0440464019775 , diff:  9.034475326538086
adv train loss:  -2237.5142192840576 , diff:  16.470172882080078
adv train loss:  -2254.0562419891357 , diff:  16.542022705078125
adv train loss:  -2266.7459506988525 , diff:  12.689708709716797
adv train loss:  -2253.703893661499 , diff:  13.042057037353516
layer  4  adv train finish, try to retain  213
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -706.9986038208008 , diff:  706.9986038208008
adv train loss:  -2232.176664352417 , diff:  1525.1780605316162
adv train loss:  -2422.959426879883 , diff:  190.78276252746582
adv train loss:  -2449.587574005127 , diff:  26.62814712524414
adv train loss:  -2485.486747741699 , diff:  35.899173736572266
adv train loss:  -2481.330373764038 , diff:  4.156373977661133
adv train loss:  -2456.1705074310303 , diff:  25.159866333007812
adv train loss:  -2446.239137649536 , diff:  9.93136978149414
adv train loss:  -2447.6269912719727 , diff:  1.3878536224365234
adv train loss:  -2442.0529823303223 , diff:  5.574008941650391
layer  5  adv train finish, try to retain  221
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -466.27056860923767 , diff:  466.27056860923767
adv train loss:  -1297.3044261932373 , diff:  831.0338575839996
adv train loss:  -1884.1320266723633 , diff:  586.827600479126
adv train loss:  -2248.0946979522705 , diff:  363.9626712799072
adv train loss:  -2390.644899368286 , diff:  142.55020141601562
adv train loss:  -2390.3863487243652 , diff:  0.25855064392089844
layer  6  adv train finish, try to retain  215
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
### skip layer  8 wait:  4  ###
---------------- start layer  9  ---------------
adv train loss:  -346.0071425437927 , diff:  346.0071425437927
adv train loss:  -345.85771679878235 , diff:  0.14942574501037598
layer  9  adv train finish, try to retain  461
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -1350.5718202590942 , diff:  1350.5718202590942
adv train loss:  -1351.3977966308594 , diff:  0.8259763717651367
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  10
test acc: top1 ->  21.22 ; top5 ->  59.6  and loss:  71917.28369140625
forward train acc: top1 ->  80.93800000244141 ; top5 ->  91.074  and loss:  176.19265285506845
test acc: top1 ->  90.24 ; top5 ->  97.25  and loss:  60.486386090517044
forward train acc: top1 ->  99.698 ; top5 ->  99.996  and loss:  3.1251259334385395
test acc: top1 ->  91.4 ; top5 ->  97.95  and loss:  52.63086558878422
forward train acc: top1 ->  99.88 ; top5 ->  99.994  and loss:  1.3586158966645598
test acc: top1 ->  91.6 ; top5 ->  98.29  and loss:  52.53856021165848
forward train acc: top1 ->  99.908 ; top5 ->  99.998  and loss:  0.8179515851661563
test acc: top1 ->  91.69 ; top5 ->  98.4  and loss:  51.358751848340034
forward train acc: top1 ->  99.938 ; top5 ->  99.998  and loss:  0.590531135443598
test acc: top1 ->  91.81 ; top5 ->  98.5  and loss:  50.608749613165855
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.4645712268538773
test acc: top1 ->  91.82 ; top5 ->  98.49  and loss:  51.350046053528786
forward train acc: top1 ->  99.938 ; top5 ->  99.998  and loss:  0.4408886239398271
test acc: top1 ->  91.91 ; top5 ->  98.57  and loss:  51.16468858718872
forward train acc: top1 ->  99.942 ; top5 ->  100.0  and loss:  0.37718960316851735
test acc: top1 ->  91.9 ; top5 ->  98.56  and loss:  51.75528483092785
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.2935068204533309
test acc: top1 ->  91.94 ; top5 ->  98.55  and loss:  51.739861100912094
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.2880595359019935
test acc: top1 ->  91.94 ; top5 ->  98.65  and loss:  52.06924711167812
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  11 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -1829.3648738861084 , diff:  1829.3648738861084
adv train loss:  -1947.1232242584229 , diff:  117.75835037231445
adv train loss:  -1951.5546836853027 , diff:  4.431459426879883
adv train loss:  -1966.3939323425293 , diff:  14.839248657226562
adv train loss:  -2124.8749656677246 , diff:  158.4810333251953
adv train loss:  -2173.322660446167 , diff:  48.44769477844238
adv train loss:  -2174.411853790283 , diff:  1.089193344116211
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  9
test acc: top1 ->  10.06 ; top5 ->  50.0  and loss:  16011.189361572266
forward train acc: top1 ->  91.704 ; top5 ->  98.262  and loss:  43.40032747806981
test acc: top1 ->  91.28 ; top5 ->  97.44  and loss:  62.22127118706703
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.5775806615129113
test acc: top1 ->  91.82 ; top5 ->  97.74  and loss:  60.04879128187895
forward train acc: top1 ->  99.956 ; top5 ->  99.996  and loss:  0.357871140120551
test acc: top1 ->  91.91 ; top5 ->  97.92  and loss:  60.73965576291084
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.2616716860793531
test acc: top1 ->  92.1 ; top5 ->  98.12  and loss:  59.32402852922678
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.20522435766179115
test acc: top1 ->  92.16 ; top5 ->  98.14  and loss:  60.33729262650013
==> this epoch:  9 / 512
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
adv train loss:  -13572.917068481445 , diff:  13572.917068481445
adv train loss:  -22818.792907714844 , diff:  9245.875839233398
adv train loss:  -31815.002410888672 , diff:  8996.209503173828
adv train loss:  -40701.07876586914 , diff:  8886.076354980469
adv train loss:  -49553.96408081055 , diff:  8852.885314941406
adv train loss:  -58374.85711669922 , diff:  8820.893035888672
adv train loss:  -67150.74542236328 , diff:  8775.888305664062
adv train loss:  -75889.90246582031 , diff:  8739.157043457031
adv train loss:  -84651.83489990234 , diff:  8761.932434082031
adv train loss:  -93430.52429199219 , diff:  8778.689392089844
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  9
test acc: top1 ->  33.81 ; top5 ->  74.15  and loss:  890.7737393379211
forward train acc: top1 ->  67.52000000976562 ; top5 ->  91.76  and loss:  241.5040691792965
test acc: top1 ->  88.94 ; top5 ->  97.7  and loss:  60.01757775247097
forward train acc: top1 ->  99.736 ; top5 ->  99.996  and loss:  2.9458483131602407
test acc: top1 ->  91.05 ; top5 ->  98.17  and loss:  47.81999862194061
forward train acc: top1 ->  99.924 ; top5 ->  99.998  and loss:  1.0176949696615338
test acc: top1 ->  91.56 ; top5 ->  98.27  and loss:  45.73493246734142
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.5756583197508007
test acc: top1 ->  91.99 ; top5 ->  98.32  and loss:  46.21753178536892
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.39816811587661505
test acc: top1 ->  92.03 ; top5 ->  98.39  and loss:  47.10543595254421
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.29328480339609087
test acc: top1 ->  92.14 ; top5 ->  98.45  and loss:  47.57651434838772
==> this epoch:  9 / 512
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.361328125  ==>  185 / 512 , inc:  16
layer  9  :  0.095703125  ==>  49 / 512 , inc:  8
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.017578125  ==>  9 / 512 , inc:  4
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  4
eps [2.07594140625, 2.07594140625, 2.07594140625, 2.07594140625, 2.07594140625, 2.07594140625, 2.07594140625, 0.04865487670898438, 1.3839609375000002, 1.8452812500000002, 1.167717041015625, 1.8452812500000002, 1.5569560546875, 14.762250000000002]  wait [2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 4, 0, 3, 0]  inc [1, 1, 1, 1, 1, 1, 1, 1, 16, 8, 1, 4, 1, 4]  tol: 3
$$$$$$$$$$$$$ epoch  45  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -704.5179524999112 , diff:  704.5179524999112
adv train loss:  -748.3730401992798 , diff:  43.855087699368596
adv train loss:  -745.8450484275818 , diff:  2.527991771697998
adv train loss:  -744.6346645355225 , diff:  1.2103838920593262
adv train loss:  -747.3534832000732 , diff:  2.7188186645507812
adv train loss:  -747.9705190658569 , diff:  0.6170358657836914
adv train loss:  -747.6874737739563 , diff:  0.28304529190063477
adv train loss:  -750.8622050285339 , diff:  3.1747312545776367
adv train loss:  -751.3884119987488 , diff:  0.5262069702148438
adv train loss:  -744.3302536010742 , diff:  7.0581583976745605
layer  0  adv train finish, try to retain  7
test acc: top1 ->  10.0 ; top5 ->  52.4  and loss:  1857.1820621490479
forward train acc: top1 ->  23.172000003051757 ; top5 ->  68.12199998046874  and loss:  323.724826335907
test acc: top1 ->  10.26 ; top5 ->  50.2  and loss:  500.6786913871765
forward train acc: top1 ->  29.69199998901367 ; top5 ->  77.78000000732422  and loss:  194.89445459842682
test acc: top1 ->  35.97 ; top5 ->  81.64  and loss:  187.20919334888458
forward train acc: top1 ->  35.93799998901367 ; top5 ->  82.94399998291016  and loss:  179.720538854599
test acc: top1 ->  40.34 ; top5 ->  85.64  and loss:  172.86128675937653
forward train acc: top1 ->  40.731999998779294 ; top5 ->  85.87599997558594  and loss:  167.46598982810974
test acc: top1 ->  44.63 ; top5 ->  88.4  and loss:  160.251567363739
forward train acc: top1 ->  45.03000000732422 ; top5 ->  88.00799999511719  and loss:  156.44199562072754
test acc: top1 ->  48.6 ; top5 ->  90.18  and loss:  149.5013688802719
forward train acc: top1 ->  48.387999997558595 ; top5 ->  89.49400000732422  and loss:  148.25214624404907
test acc: top1 ->  50.47 ; top5 ->  90.83  and loss:  145.1576282978058
forward train acc: top1 ->  50.17199998901367 ; top5 ->  90.24799998046875  and loss:  143.4410057067871
test acc: top1 ->  52.27 ; top5 ->  91.85  and loss:  139.49950456619263
forward train acc: top1 ->  51.978 ; top5 ->  91.03400001464844  and loss:  138.1402152776718
test acc: top1 ->  53.86 ; top5 ->  92.19  and loss:  135.01755619049072
forward train acc: top1 ->  53.95599999755859 ; top5 ->  91.87  and loss:  132.87242078781128
test acc: top1 ->  55.65 ; top5 ->  92.76  and loss:  130.2414070367813
forward train acc: top1 ->  55.78800000366211 ; top5 ->  92.52600000732421  and loss:  127.98484575748444
test acc: top1 ->  57.63 ; top5 ->  93.21  and loss:  125.66184091567993
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -38.70357048511505 , diff:  38.70357048511505
adv train loss:  -40.045827120542526 , diff:  1.342256635427475
adv train loss:  -39.65902742743492 , diff:  0.386799693107605
adv train loss:  -40.422381073236465 , diff:  0.7633536458015442
adv train loss:  -41.00583890080452 , diff:  0.5834578275680542
adv train loss:  -40.945294827222824 , diff:  0.06054407358169556
layer  1  adv train finish, try to retain  46
test acc: top1 ->  34.01 ; top5 ->  75.2  and loss:  194.96488618850708
forward train acc: top1 ->  99.16999998046875 ; top5 ->  99.994  and loss:  6.268535585608333
test acc: top1 ->  91.72 ; top5 ->  99.14  and loss:  36.90543130040169
forward train acc: top1 ->  99.6860000024414 ; top5 ->  100.0  and loss:  1.1266246989835054
test acc: top1 ->  91.78 ; top5 ->  99.07  and loss:  42.00187075138092
forward train acc: top1 ->  99.7760000024414 ; top5 ->  99.998  and loss:  0.763444654410705
test acc: top1 ->  91.87 ; top5 ->  99.12  and loss:  43.467965587973595
forward train acc: top1 ->  99.828 ; top5 ->  99.998  and loss:  0.6470348564907908
test acc: top1 ->  92.04 ; top5 ->  99.09  and loss:  45.140074357390404
forward train acc: top1 ->  99.852 ; top5 ->  99.998  and loss:  0.49488891661167145
test acc: top1 ->  92.03 ; top5 ->  99.1  and loss:  45.97118017822504
forward train acc: top1 ->  99.884 ; top5 ->  100.0  and loss:  0.41653597581898794
test acc: top1 ->  92.1 ; top5 ->  99.07  and loss:  46.52205255627632
forward train acc: top1 ->  99.86399997558594 ; top5 ->  100.0  and loss:  0.41079576371703297
test acc: top1 ->  92.04 ; top5 ->  99.06  and loss:  46.914224445819855
forward train acc: top1 ->  99.882 ; top5 ->  100.0  and loss:  0.37572675716364756
test acc: top1 ->  92.06 ; top5 ->  99.11  and loss:  47.906430438160896
forward train acc: top1 ->  99.88999997558594 ; top5 ->  100.0  and loss:  0.3915733264875598
test acc: top1 ->  92.01 ; top5 ->  98.98  and loss:  47.583146423101425
forward train acc: top1 ->  99.918 ; top5 ->  100.0  and loss:  0.2743157158838585
test acc: top1 ->  92.06 ; top5 ->  99.04  and loss:  48.900639936327934
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  55 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -686.7918473249883 , diff:  686.7918473249883
adv train loss:  -1091.310341835022 , diff:  404.51849451003363
adv train loss:  -1091.1302108764648 , diff:  0.1801309585571289
adv train loss:  -1093.6195163726807 , diff:  2.4893054962158203
adv train loss:  -1096.572693824768 , diff:  2.9531774520874023
adv train loss:  -1101.158525466919 , diff:  4.585831642150879
adv train loss:  -1101.6592235565186 , diff:  0.5006980895996094
adv train loss:  -1097.2847414016724 , diff:  4.374482154846191
adv train loss:  -1103.0076313018799 , diff:  5.7228899002075195
adv train loss:  -1102.899154663086 , diff:  0.10847663879394531
layer  2  adv train finish, try to retain  41
test acc: top1 ->  10.12 ; top5 ->  51.07  and loss:  1174.3087120056152
forward train acc: top1 ->  91.68599997070312 ; top5 ->  99.452  and loss:  29.211019963026047
test acc: top1 ->  84.91 ; top5 ->  98.31  and loss:  52.96002638339996
forward train acc: top1 ->  93.31800001953125 ; top5 ->  99.6980000024414  and loss:  20.626302868127823
test acc: top1 ->  86.14 ; top5 ->  98.61  and loss:  49.233281686902046
forward train acc: top1 ->  94.33399997314453 ; top5 ->  99.77199997558594  and loss:  17.635903611779213
test acc: top1 ->  86.69 ; top5 ->  98.8  and loss:  47.379375234246254
forward train acc: top1 ->  94.90199999267578 ; top5 ->  99.80399997558594  and loss:  15.503612995147705
test acc: top1 ->  87.1 ; top5 ->  98.83  and loss:  46.77551120519638
forward train acc: top1 ->  95.51200001220703 ; top5 ->  99.848  and loss:  13.579582817852497
test acc: top1 ->  87.39 ; top5 ->  98.9  and loss:  46.99797861278057
forward train acc: top1 ->  95.71399997070313 ; top5 ->  99.86599997558594  and loss:  12.848130531609058
test acc: top1 ->  87.69 ; top5 ->  99.0  and loss:  45.71691446006298
forward train acc: top1 ->  96.09999998535156 ; top5 ->  99.89  and loss:  11.66976834833622
test acc: top1 ->  87.83 ; top5 ->  98.96  and loss:  46.22100938856602
forward train acc: top1 ->  96.20600000976563 ; top5 ->  99.89999997558594  and loss:  11.2758819013834
test acc: top1 ->  87.9 ; top5 ->  98.98  and loss:  45.40310700237751
forward train acc: top1 ->  96.25800001708984 ; top5 ->  99.92  and loss:  11.350797556340694
test acc: top1 ->  88.09 ; top5 ->  99.02  and loss:  44.37083761394024
forward train acc: top1 ->  96.49200001708985 ; top5 ->  99.928  and loss:  10.650079742074013
test acc: top1 ->  88.17 ; top5 ->  99.03  and loss:  45.04491910338402
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -482.9752200427465 , diff:  482.9752200427465
adv train loss:  -788.9699330329895 , diff:  305.994712990243
adv train loss:  -842.2630462646484 , diff:  53.293113231658936
adv train loss:  -838.4829821586609 , diff:  3.780064105987549
adv train loss:  -849.8457164764404 , diff:  11.362734317779541
adv train loss:  -853.549090385437 , diff:  3.703373908996582
adv train loss:  -854.2435760498047 , diff:  0.6944856643676758
adv train loss:  -858.1855049133301 , diff:  3.9419288635253906
adv train loss:  -867.2616310119629 , diff:  9.076126098632812
adv train loss:  -863.8967952728271 , diff:  3.364835739135742
layer  3  adv train finish, try to retain  21
test acc: top1 ->  10.09 ; top5 ->  50.26  and loss:  639.6323318481445
forward train acc: top1 ->  74.18399997314454 ; top5 ->  97.37799998535156  and loss:  79.55371445417404
test acc: top1 ->  73.73 ; top5 ->  97.13  and loss:  81.59798073768616
forward train acc: top1 ->  78.63999998046874 ; top5 ->  98.27999998291016  and loss:  62.78778123855591
test acc: top1 ->  76.43 ; top5 ->  97.73  and loss:  72.85222363471985
forward train acc: top1 ->  81.00999998535156 ; top5 ->  98.57399998291015  and loss:  55.648938953876495
test acc: top1 ->  78.08 ; top5 ->  97.83  and loss:  69.08111557364464
forward train acc: top1 ->  82.45600001953125 ; top5 ->  98.76600000732422  and loss:  51.47613766789436
test acc: top1 ->  79.12 ; top5 ->  98.15  and loss:  65.34780415892601
forward train acc: top1 ->  83.78799998046875 ; top5 ->  98.94199997802734  and loss:  47.535833179950714
test acc: top1 ->  79.66 ; top5 ->  98.22  and loss:  63.1033411026001
forward train acc: top1 ->  84.50000000976563 ; top5 ->  99.04199997802735  and loss:  45.74861919879913
test acc: top1 ->  80.26 ; top5 ->  98.24  and loss:  61.90472358465195
forward train acc: top1 ->  84.73800000732422 ; top5 ->  99.168  and loss:  44.33792459964752
test acc: top1 ->  80.46 ; top5 ->  98.27  and loss:  60.80301705002785
forward train acc: top1 ->  85.29199997314453 ; top5 ->  99.156  and loss:  43.241427183151245
test acc: top1 ->  80.86 ; top5 ->  98.36  and loss:  59.99466896057129
forward train acc: top1 ->  85.64599997558594 ; top5 ->  99.16399997558594  and loss:  42.68737322092056
test acc: top1 ->  81.07 ; top5 ->  98.37  and loss:  59.56690916419029
forward train acc: top1 ->  85.92199998046875 ; top5 ->  99.26  and loss:  41.28508362174034
test acc: top1 ->  81.46 ; top5 ->  98.41  and loss:  58.80374300479889
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -139.12530364468694 , diff:  139.12530364468694
adv train loss:  -550.0578918457031 , diff:  410.9325882010162
adv train loss:  -630.9350457191467 , diff:  80.8771538734436
adv train loss:  -652.38419008255 , diff:  21.44914436340332
adv train loss:  -672.2519035339355 , diff:  19.867713451385498
adv train loss:  -681.0502367019653 , diff:  8.798333168029785
adv train loss:  -734.6274199485779 , diff:  53.57718324661255
adv train loss:  -743.4548587799072 , diff:  8.827438831329346
adv train loss:  -744.0463361740112 , diff:  0.5914773941040039
adv train loss:  -744.0382742881775 , diff:  0.008061885833740234
layer  4  adv train finish, try to retain  20
test acc: top1 ->  11.12 ; top5 ->  51.03  and loss:  638.239978313446
forward train acc: top1 ->  71.51600001953125 ; top5 ->  97.02400001220703  and loss:  83.00658106803894
test acc: top1 ->  71.86 ; top5 ->  96.93  and loss:  83.2251855134964
forward train acc: top1 ->  76.40600000488281 ; top5 ->  98.04600000488281  and loss:  67.883893430233
test acc: top1 ->  74.59 ; top5 ->  97.44  and loss:  75.60062411427498
forward train acc: top1 ->  78.88600000976562 ; top5 ->  98.40400000488282  and loss:  61.01630961894989
test acc: top1 ->  76.37 ; top5 ->  97.81  and loss:  70.9761281311512
forward train acc: top1 ->  80.34200001464843 ; top5 ->  98.61199997802734  and loss:  56.94529810547829
test acc: top1 ->  77.56 ; top5 ->  97.92  and loss:  67.5042313337326
forward train acc: top1 ->  81.31000001220703 ; top5 ->  98.80199998291016  and loss:  53.851284593343735
test acc: top1 ->  78.42 ; top5 ->  98.11  and loss:  65.32684022188187
forward train acc: top1 ->  81.93200000976563 ; top5 ->  98.90599997558594  and loss:  51.876831114292145
test acc: top1 ->  78.64 ; top5 ->  98.1  and loss:  64.17658793926239
forward train acc: top1 ->  82.20999999511719 ; top5 ->  98.90400000488282  and loss:  51.081479996442795
test acc: top1 ->  78.88 ; top5 ->  98.12  and loss:  63.23829919099808
forward train acc: top1 ->  82.33 ; top5 ->  98.92799997558593  and loss:  50.71313118934631
test acc: top1 ->  79.24 ; top5 ->  98.15  and loss:  62.69904714822769
forward train acc: top1 ->  82.79799999023437 ; top5 ->  98.92799998291015  and loss:  49.68588429689407
test acc: top1 ->  79.54 ; top5 ->  98.2  and loss:  61.86786323785782
forward train acc: top1 ->  83.08600001464843 ; top5 ->  99.00199997558593  and loss:  48.40810617804527
test acc: top1 ->  79.65 ; top5 ->  98.33  and loss:  60.92577901482582
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -224.12031876295805 , diff:  224.12031876295805
adv train loss:  -761.3153214454651 , diff:  537.195002682507
adv train loss:  -783.0102133750916 , diff:  21.694891929626465
adv train loss:  -830.7241039276123 , diff:  47.71389055252075
adv train loss:  -857.4067087173462 , diff:  26.682604789733887
adv train loss:  -883.3268060684204 , diff:  25.92009735107422
adv train loss:  -894.4021854400635 , diff:  11.075379371643066
adv train loss:  -895.6429052352905 , diff:  1.2407197952270508
adv train loss:  -894.3105087280273 , diff:  1.3323965072631836
adv train loss:  -892.9868679046631 , diff:  1.3236408233642578
layer  5  adv train finish, try to retain  17
test acc: top1 ->  12.18 ; top5 ->  55.73  and loss:  345.7045121192932
forward train acc: top1 ->  73.78400001953125 ; top5 ->  98.17800000244141  and loss:  72.82407194375992
test acc: top1 ->  73.62 ; top5 ->  98.03  and loss:  76.12030610442162
forward train acc: top1 ->  78.80599997070313 ; top5 ->  98.81399997558594  and loss:  58.743872702121735
test acc: top1 ->  76.6 ; top5 ->  98.28  and loss:  68.04593446850777
forward train acc: top1 ->  81.66799998535156 ; top5 ->  99.12199997802735  and loss:  51.30394425988197
test acc: top1 ->  78.28 ; top5 ->  98.49  and loss:  64.03122928738594
forward train acc: top1 ->  83.01000000976562 ; top5 ->  99.16999997802735  and loss:  47.59455552697182
test acc: top1 ->  79.3 ; top5 ->  98.57  and loss:  61.787110418081284
forward train acc: top1 ->  84.27399999755859 ; top5 ->  99.26199998046874  and loss:  44.62398463487625
test acc: top1 ->  80.41 ; top5 ->  98.71  and loss:  59.03754213452339
forward train acc: top1 ->  85.01799999267578 ; top5 ->  99.40200000244141  and loss:  42.30552661418915
test acc: top1 ->  81.05 ; top5 ->  98.83  and loss:  57.2411690056324
forward train acc: top1 ->  85.54400000244141 ; top5 ->  99.33599997558593  and loss:  41.055482000112534
test acc: top1 ->  81.29 ; top5 ->  98.74  and loss:  56.57999736070633
forward train acc: top1 ->  86.03799999511719 ; top5 ->  99.36799997558593  and loss:  39.864054441452026
test acc: top1 ->  81.81 ; top5 ->  98.85  and loss:  55.35719519853592
forward train acc: top1 ->  86.29999997558593 ; top5 ->  99.44400000244141  and loss:  39.12799817323685
test acc: top1 ->  82.16 ; top5 ->  98.85  and loss:  54.59378984570503
forward train acc: top1 ->  86.65199998046874 ; top5 ->  99.46200000488281  and loss:  37.937734335660934
test acc: top1 ->  82.26 ; top5 ->  98.77  and loss:  54.97134706377983
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -238.32784853503108 , diff:  238.32784853503108
adv train loss:  -938.0734434127808 , diff:  699.7455948777497
adv train loss:  -1007.4359064102173 , diff:  69.36246299743652
adv train loss:  -1008.453293800354 , diff:  1.0173873901367188
adv train loss:  -1011.0730905532837 , diff:  2.6197967529296875
adv train loss:  -1016.9830408096313 , diff:  5.909950256347656
adv train loss:  -1022.0550031661987 , diff:  5.071962356567383
adv train loss:  -1022.6543531417847 , diff:  0.5993499755859375
adv train loss:  -1019.4751224517822 , diff:  3.1792306900024414
adv train loss:  -1030.4980611801147 , diff:  11.02293872833252
layer  6  adv train finish, try to retain  6
test acc: top1 ->  9.54 ; top5 ->  51.16  and loss:  444.21244859695435
forward train acc: top1 ->  68.11800001464844 ; top5 ->  98.01600000488281  and loss:  86.47503864765167
test acc: top1 ->  71.14 ; top5 ->  97.74  and loss:  83.97462868690491
forward train acc: top1 ->  76.70199997070313 ; top5 ->  98.904  and loss:  63.91536873579025
test acc: top1 ->  75.59 ; top5 ->  98.12  and loss:  73.84855729341507
forward train acc: top1 ->  80.08000000976563 ; top5 ->  99.15000000488281  and loss:  55.292003840208054
test acc: top1 ->  77.74 ; top5 ->  98.31  and loss:  68.95034158229828
forward train acc: top1 ->  82.50200001220703 ; top5 ->  99.30399997558594  and loss:  49.32290679216385
test acc: top1 ->  79.35 ; top5 ->  98.32  and loss:  65.23389485478401
forward train acc: top1 ->  84.12800000732422 ; top5 ->  99.4240000024414  and loss:  44.68601778149605
test acc: top1 ->  80.34 ; top5 ->  98.37  and loss:  63.43939337134361
forward train acc: top1 ->  85.2299999975586 ; top5 ->  99.374  and loss:  42.071959406137466
test acc: top1 ->  80.95 ; top5 ->  98.47  and loss:  61.6413671374321
forward train acc: top1 ->  85.77600000976562 ; top5 ->  99.53599997802735  and loss:  40.08086875081062
test acc: top1 ->  81.39 ; top5 ->  98.47  and loss:  60.62081006169319
forward train acc: top1 ->  86.39799997802734 ; top5 ->  99.53599997802735  and loss:  38.60634794831276
test acc: top1 ->  81.65 ; top5 ->  98.47  and loss:  60.22485634684563
forward train acc: top1 ->  86.71400000244141 ; top5 ->  99.514  and loss:  37.655699729919434
test acc: top1 ->  81.8 ; top5 ->  98.55  and loss:  59.377534702420235
forward train acc: top1 ->  87.4139999951172 ; top5 ->  99.566  and loss:  36.25433611869812
test acc: top1 ->  82.38 ; top5 ->  98.58  and loss:  59.01215164363384
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  144 / 256 , inc:  1
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
### skip layer  8 wait:  3  ###
---------------- start layer  9  ---------------
adv train loss:  -24.431536927819252 , diff:  24.431536927819252
adv train loss:  -24.12692156434059 , diff:  0.3046153634786606
adv train loss:  -24.163240402936935 , diff:  0.036318838596343994
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  41
test acc: top1 ->  10.0 ; top5 ->  50.04  and loss:  577167.8408203125
forward train acc: top1 ->  94.76799997802735 ; top5 ->  99.972  and loss:  22.020939644426107
test acc: top1 ->  91.19 ; top5 ->  99.29  and loss:  34.08031526207924
forward train acc: top1 ->  99.26200000244141 ; top5 ->  99.998  and loss:  3.4067560909315944
test acc: top1 ->  91.47 ; top5 ->  99.25  and loss:  35.342843011021614
forward train acc: top1 ->  99.57399997558593 ; top5 ->  100.0  and loss:  1.9293800555169582
test acc: top1 ->  91.57 ; top5 ->  99.22  and loss:  37.29096931964159
forward train acc: top1 ->  99.7360000024414 ; top5 ->  100.0  and loss:  1.2555138217285275
test acc: top1 ->  91.61 ; top5 ->  99.17  and loss:  39.34621647745371
forward train acc: top1 ->  99.79399997558593 ; top5 ->  99.998  and loss:  0.9532162942923605
test acc: top1 ->  91.75 ; top5 ->  99.19  and loss:  39.904177606105804
forward train acc: top1 ->  99.828 ; top5 ->  100.0  and loss:  0.7676923065446317
test acc: top1 ->  92.03 ; top5 ->  99.21  and loss:  40.079121984541416
forward train acc: top1 ->  99.84599997558594 ; top5 ->  100.0  and loss:  0.64603225979954
test acc: top1 ->  91.88 ; top5 ->  99.16  and loss:  40.79498754441738
forward train acc: top1 ->  99.87999997558593 ; top5 ->  100.0  and loss:  0.5538509970065206
test acc: top1 ->  91.96 ; top5 ->  99.18  and loss:  41.30377534776926
forward train acc: top1 ->  99.892 ; top5 ->  100.0  and loss:  0.5105857590679079
test acc: top1 ->  91.99 ; top5 ->  99.24  and loss:  41.86872960627079
forward train acc: top1 ->  99.896 ; top5 ->  99.998  and loss:  0.46440724411513656
test acc: top1 ->  92.14 ; top5 ->  99.17  and loss:  42.39022620767355
==> this epoch:  41 / 512
---------------- start layer  10  ---------------
### skip layer  10 wait:  4  ###
---------------- start layer  11  ---------------
adv train loss:  -1062.2041463851929 , diff:  1062.2041463851929
adv train loss:  -1087.9823389053345 , diff:  25.7781925201416
adv train loss:  -1125.7351007461548 , diff:  37.75276184082031
adv train loss:  -1150.3784608840942 , diff:  24.643360137939453
adv train loss:  -1236.1403646469116 , diff:  85.76190376281738
adv train loss:  -1311.9452180862427 , diff:  75.80485343933105
adv train loss:  -1318.2351455688477 , diff:  6.2899274826049805
adv train loss:  -1344.4163970947266 , diff:  26.181251525878906
adv train loss:  -1358.778260231018 , diff:  14.361863136291504
adv train loss:  -1359.2865629196167 , diff:  0.5083026885986328
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  5
test acc: top1 ->  10.0 ; top5 ->  73.96  and loss:  7573.256637573242
forward train acc: top1 ->  89.90399997802734 ; top5 ->  99.474  and loss:  45.31093052774668
test acc: top1 ->  90.81 ; top5 ->  98.76  and loss:  40.066501438617706
forward train acc: top1 ->  99.57 ; top5 ->  100.0  and loss:  4.099313693121076
test acc: top1 ->  91.25 ; top5 ->  98.89  and loss:  38.33431465923786
forward train acc: top1 ->  99.714 ; top5 ->  100.0  and loss:  2.315675988793373
test acc: top1 ->  91.44 ; top5 ->  98.91  and loss:  38.96325419843197
forward train acc: top1 ->  99.73 ; top5 ->  99.998  and loss:  1.6166056105867028
test acc: top1 ->  91.57 ; top5 ->  98.98  and loss:  40.140227034687996
forward train acc: top1 ->  99.84 ; top5 ->  100.0  and loss:  1.132304665632546
test acc: top1 ->  91.64 ; top5 ->  98.98  and loss:  41.09277890622616
forward train acc: top1 ->  99.844 ; top5 ->  100.0  and loss:  0.978688039816916
test acc: top1 ->  91.68 ; top5 ->  98.99  and loss:  41.33049117773771
forward train acc: top1 ->  99.854 ; top5 ->  99.998  and loss:  0.8919005347415805
test acc: top1 ->  91.62 ; top5 ->  98.98  and loss:  42.096089228987694
forward train acc: top1 ->  99.85999997558594 ; top5 ->  100.0  and loss:  0.7963453656993806
test acc: top1 ->  91.73 ; top5 ->  99.02  and loss:  42.3147664219141
forward train acc: top1 ->  99.874 ; top5 ->  100.0  and loss:  0.7038342130836099
test acc: top1 ->  91.65 ; top5 ->  98.97  and loss:  43.19013483822346
forward train acc: top1 ->  99.896 ; top5 ->  100.0  and loss:  0.6417925630230457
test acc: top1 ->  91.64 ; top5 ->  99.04  and loss:  43.46399676799774
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  9 / 512 , inc:  4
---------------- start layer  12  ---------------
### skip layer  12 wait:  3  ###
---------------- start layer  13  ---------------
adv train loss:  -21137.17984008789 , diff:  21137.17984008789
adv train loss:  -35007.825592041016 , diff:  13870.645751953125
adv train loss:  -48743.233001708984 , diff:  13735.407409667969
adv train loss:  -62441.78857421875 , diff:  13698.555572509766
adv train loss:  -76107.6664428711 , diff:  13665.877868652344
adv train loss:  -89733.43585205078 , diff:  13625.769409179688
adv train loss:  -103362.04699707031 , diff:  13628.611145019531
adv train loss:  -116979.14123535156 , diff:  13617.09423828125
adv train loss:  -130534.14196777344 , diff:  13555.000732421875
adv train loss:  -144137.23291015625 , diff:  13603.090942382812
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  5
test acc: top1 ->  10.0 ; top5 ->  44.15  and loss:  1363.270664215088
forward train acc: top1 ->  28.23999999633789 ; top5 ->  66.88999997314453  and loss:  513.2476514577866
test acc: top1 ->  61.52 ; top5 ->  80.22  and loss:  147.68887293338776
forward train acc: top1 ->  88.91600000732421 ; top5 ->  97.214  and loss:  59.199960827827454
test acc: top1 ->  90.14 ; top5 ->  98.95  and loss:  47.250833451747894
forward train acc: top1 ->  99.43000000488281 ; top5 ->  99.994  and loss:  17.974288776516914
test acc: top1 ->  90.82 ; top5 ->  99.04  and loss:  36.66663393378258
forward train acc: top1 ->  99.69 ; top5 ->  99.996  and loss:  9.549189232289791
test acc: top1 ->  91.3 ; top5 ->  98.99  and loss:  33.41880360245705
forward train acc: top1 ->  99.83399997558594 ; top5 ->  100.0  and loss:  5.970519836992025
test acc: top1 ->  91.5 ; top5 ->  99.04  and loss:  32.32800230383873
forward train acc: top1 ->  99.886 ; top5 ->  100.0  and loss:  4.358625486493111
test acc: top1 ->  91.55 ; top5 ->  99.0  and loss:  32.160616651177406
forward train acc: top1 ->  99.86399997558594 ; top5 ->  100.0  and loss:  3.767872866243124
test acc: top1 ->  91.68 ; top5 ->  99.0  and loss:  32.38836017251015
forward train acc: top1 ->  99.912 ; top5 ->  100.0  and loss:  3.130935449153185
test acc: top1 ->  91.66 ; top5 ->  99.02  and loss:  32.40087591856718
forward train acc: top1 ->  99.936 ; top5 ->  99.998  and loss:  2.671418771147728
test acc: top1 ->  91.64 ; top5 ->  98.97  and loss:  32.61570695787668
forward train acc: top1 ->  99.916 ; top5 ->  100.0  and loss:  2.304080644622445
test acc: top1 ->  91.74 ; top5 ->  98.95  and loss:  32.84316708147526
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  9 / 512 , inc:  4
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.361328125  ==>  185 / 512 , inc:  16
layer  9  :  0.080078125  ==>  41 / 512 , inc:  16
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.017578125  ==>  9 / 512 , inc:  2
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  2
eps [1.5569560546875, 1.5569560546875, 1.5569560546875, 1.5569560546875, 1.5569560546875, 1.5569560546875, 1.5569560546875, 0.04865487670898438, 1.3839609375000002, 1.8452812500000002, 1.167717041015625, 1.3839609375000002, 1.5569560546875, 11.071687500000001]  wait [4, 4, 4, 4, 4, 4, 4, 2, 2, 0, 3, 2, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 16, 16, 1, 2, 1, 2]  tol: 3
$$$$$$$$$$$$$ epoch  46  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
### skip layer  1 wait:  4  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
adv train loss:  -259.53308391571045 , diff:  259.53308391571045
adv train loss:  -259.4549136161804 , diff:  0.0781702995300293
layer  7  adv train finish, try to retain  269
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -287.51155376434326 , diff:  287.51155376434326
adv train loss:  -287.49008655548096 , diff:  0.021467208862304688
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  169
test acc: top1 ->  10.11 ; top5 ->  54.99  and loss:  14338369.1015625
forward train acc: top1 ->  90.3580000024414 ; top5 ->  98.724  and loss:  48.043728871271014
test acc: top1 ->  90.53 ; top5 ->  98.68  and loss:  45.245644241571426
forward train acc: top1 ->  99.48799997802735 ; top5 ->  100.0  and loss:  2.9034121679142118
test acc: top1 ->  90.9 ; top5 ->  98.78  and loss:  45.02895174920559
forward train acc: top1 ->  99.67000000244141 ; top5 ->  99.996  and loss:  1.916660132817924
test acc: top1 ->  91.1 ; top5 ->  98.91  and loss:  45.43383827805519
forward train acc: top1 ->  99.7340000024414 ; top5 ->  100.0  and loss:  1.403914799913764
test acc: top1 ->  91.41 ; top5 ->  98.94  and loss:  46.34604475647211
forward train acc: top1 ->  99.77999997558594 ; top5 ->  100.0  and loss:  1.1007856191135943
test acc: top1 ->  91.45 ; top5 ->  99.02  and loss:  46.957860082387924
forward train acc: top1 ->  99.83199997558594 ; top5 ->  99.998  and loss:  0.8840390858240426
test acc: top1 ->  91.45 ; top5 ->  98.98  and loss:  47.66931187361479
forward train acc: top1 ->  99.78999997558594 ; top5 ->  100.0  and loss:  0.9094715160317719
test acc: top1 ->  91.61 ; top5 ->  99.04  and loss:  47.48181003332138
forward train acc: top1 ->  99.848 ; top5 ->  100.0  and loss:  0.782780144829303
test acc: top1 ->  91.69 ; top5 ->  99.07  and loss:  47.463575422763824
forward train acc: top1 ->  99.8140000024414 ; top5 ->  99.998  and loss:  0.8023063093423843
test acc: top1 ->  91.61 ; top5 ->  99.05  and loss:  48.17219849675894
forward train acc: top1 ->  99.872 ; top5 ->  100.0  and loss:  0.6637929007411003
test acc: top1 ->  91.69 ; top5 ->  99.06  and loss:  48.20484425127506
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  185 / 512 , inc:  16
---------------- start layer  9  ---------------
adv train loss:  -599.5150737762451 , diff:  599.5150737762451
adv train loss:  -596.8842859268188 , diff:  2.6307878494262695
adv train loss:  -599.0121178627014 , diff:  2.1278319358825684
adv train loss:  -598.9591164588928 , diff:  0.05300140380859375
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  25
test acc: top1 ->  13.49 ; top5 ->  74.84  and loss:  2537.1275672912598
forward train acc: top1 ->  97.24399997558594 ; top5 ->  99.992  and loss:  10.846769347321242
test acc: top1 ->  91.61 ; top5 ->  98.95  and loss:  45.99946475028992
forward train acc: top1 ->  99.86999997558594 ; top5 ->  99.998  and loss:  0.6427216400625184
test acc: top1 ->  91.91 ; top5 ->  98.93  and loss:  46.61080753803253
forward train acc: top1 ->  99.916 ; top5 ->  100.0  and loss:  0.40663898864295334
test acc: top1 ->  91.89 ; top5 ->  99.01  and loss:  47.66933535784483
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.3005148165393621
test acc: top1 ->  91.94 ; top5 ->  99.06  and loss:  49.11759449541569
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.27069393353303894
test acc: top1 ->  91.92 ; top5 ->  99.1  and loss:  49.66503269225359
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.22989265742944553
test acc: top1 ->  92.01 ; top5 ->  99.11  and loss:  49.76337453722954
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.21489935944555327
test acc: top1 ->  92.03 ; top5 ->  99.11  and loss:  49.939905397593975
forward train acc: top1 ->  99.94 ; top5 ->  100.0  and loss:  0.21632731950376183
test acc: top1 ->  91.97 ; top5 ->  99.16  and loss:  49.84513735771179
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.17672535014571622
test acc: top1 ->  92.05 ; top5 ->  99.12  and loss:  50.299035489559174
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.16279376010061242
test acc: top1 ->  92.02 ; top5 ->  99.14  and loss:  51.01589724421501
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  41 / 512 , inc:  16
---------------- start layer  10  ---------------
### skip layer  10 wait:  3  ###
---------------- start layer  11  ---------------
adv train loss:  -1416.2250604629517 , diff:  1416.2250604629517
adv train loss:  -1471.0323505401611 , diff:  54.80729007720947
adv train loss:  -1502.2087106704712 , diff:  31.17636013031006
adv train loss:  -1501.797441482544 , diff:  0.4112691879272461
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  7
test acc: top1 ->  19.75 ; top5 ->  61.34  and loss:  11825.323875427246
forward train acc: top1 ->  99.454 ; top5 ->  100.0  and loss:  2.1446967718657106
test acc: top1 ->  91.55 ; top5 ->  99.05  and loss:  48.306880220770836
forward train acc: top1 ->  99.94999997558594 ; top5 ->  100.0  and loss:  0.29564648657105863
test acc: top1 ->  92.07 ; top5 ->  99.1  and loss:  48.90690241008997
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.19429057405795902
test acc: top1 ->  91.9 ; top5 ->  99.07  and loss:  51.45510397851467
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.15239175216993317
test acc: top1 ->  91.88 ; top5 ->  99.13  and loss:  53.50952062010765
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.1701500363415107
test acc: top1 ->  92.06 ; top5 ->  99.07  and loss:  55.06626608222723
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.157917022384936
test acc: top1 ->  92.14 ; top5 ->  99.11  and loss:  54.28945417702198
==> this epoch:  7 / 512
---------------- start layer  12  ---------------
adv train loss:  -0.13317169294168707 , diff:  0.13317169294168707
adv train loss:  -0.12006688315887004 , diff:  0.013104809782817028
adv train loss:  -0.4632764745038003 , diff:  0.34320959134493023
adv train loss:  -0.10401622537756339 , diff:  0.3592602491262369
adv train loss:  -0.09797184744093101 , diff:  0.0060443779366323724
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  61
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  673.7062368392944
forward train acc: top1 ->  10.109999999389649 ; top5 ->  49.85999999267578  and loss:  356.70407795906067
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  251.90780782699585
forward train acc: top1 ->  10.00799999786377 ; top5 ->  49.861999986572265  and loss:  229.05181574821472
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.2812397480011
forward train acc: top1 ->  9.961999998168945 ; top5 ->  49.79400000854492  and loss:  225.69872617721558
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.27446126937866
forward train acc: top1 ->  10.022000002441406 ; top5 ->  50.18999998535156  and loss:  225.72932887077332
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.32814741134644
forward train acc: top1 ->  10.12799999938965 ; top5 ->  50.272000001220704  and loss:  225.71972036361694
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.32447385787964
forward train acc: top1 ->  9.931999996643066 ; top5 ->  49.603999991455076  and loss:  225.70407843589783
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.28652691841125
forward train acc: top1 ->  9.971999996643067 ; top5 ->  49.86199999145508  and loss:  225.6933629512787
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.27864980697632
forward train acc: top1 ->  10.073999999084473 ; top5 ->  49.432000008544925  and loss:  225.70303177833557
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.27452182769775
forward train acc: top1 ->  9.683999998321534 ; top5 ->  49.60599999633789  and loss:  225.70836353302002
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.27560663223267
forward train acc: top1 ->  9.852000001831055 ; top5 ->  49.65999998901367  and loss:  225.69698524475098
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.28764748573303
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  62 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -14281.229476928711 , diff:  14281.229476928711
adv train loss:  -26966.0558013916 , diff:  12684.82632446289
adv train loss:  -38678.42980957031 , diff:  11712.374008178711
adv train loss:  -49992.373138427734 , diff:  11313.943328857422
adv train loss:  -61129.556579589844 , diff:  11137.18344116211
adv train loss:  -72068.80938720703 , diff:  10939.252807617188
adv train loss:  -82984.99877929688 , diff:  10916.189392089844
adv train loss:  -93821.40155029297 , diff:  10836.402770996094
adv train loss:  -104578.47875976562 , diff:  10757.077209472656
adv train loss:  -114629.49584960938 , diff:  10051.01708984375
layer  13  adv train finish, try to retain  39
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.361328125  ==>  185 / 512 , inc:  8
layer  9  :  0.080078125  ==>  41 / 512 , inc:  8
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.013671875  ==>  7 / 512 , inc:  3
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  2
eps [1.5569560546875, 1.5569560546875, 1.5569560546875, 1.5569560546875, 1.5569560546875, 1.5569560546875, 1.5569560546875, 0.09730975341796876, 1.037970703125, 1.3839609375000002, 1.167717041015625, 1.3839609375000002, 1.167717041015625, 22.143375000000002]  wait [3, 3, 3, 3, 3, 3, 3, 2, 4, 2, 2, 0, 4, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 8, 8, 1, 3, 1, 2]  tol: 3
$$$$$$$$$$$$$ epoch  47  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
### skip layer  1 wait:  3  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  3  ###
---------------- start layer  7  ---------------
adv train loss:  -128.85703378915787 , diff:  128.85703378915787
adv train loss:  -128.96654057502747 , diff:  0.10950678586959839
layer  7  adv train finish, try to retain  250
test acc: top1 ->  21.42 ; top5 ->  74.85  and loss:  316.23873019218445
forward train acc: top1 ->  94.38999998291015 ; top5 ->  99.71  and loss:  21.44568622112274
test acc: top1 ->  89.45 ; top5 ->  98.29  and loss:  65.90838424861431
forward train acc: top1 ->  98.81599997558594 ; top5 ->  99.994  and loss:  3.7435010643675923
test acc: top1 ->  90.24 ; top5 ->  98.6  and loss:  60.739919155836105
forward train acc: top1 ->  99.32399997558593 ; top5 ->  99.988  and loss:  2.282044432591647
test acc: top1 ->  90.53 ; top5 ->  98.72  and loss:  59.976902678608894
forward train acc: top1 ->  99.40799997558594 ; top5 ->  99.994  and loss:  1.938476923853159
test acc: top1 ->  90.72 ; top5 ->  98.84  and loss:  60.288361333310604
forward train acc: top1 ->  99.5280000024414 ; top5 ->  99.998  and loss:  1.5154047878459096
test acc: top1 ->  90.94 ; top5 ->  98.78  and loss:  58.57988695055246
forward train acc: top1 ->  99.64399997558594 ; top5 ->  100.0  and loss:  1.1424483021255583
test acc: top1 ->  91.04 ; top5 ->  98.87  and loss:  59.478083692491055
forward train acc: top1 ->  99.6620000024414 ; top5 ->  100.0  and loss:  1.0926100802607834
test acc: top1 ->  91.27 ; top5 ->  98.87  and loss:  59.359985794872046
forward train acc: top1 ->  99.67599997558594 ; top5 ->  99.998  and loss:  0.9599391273222864
test acc: top1 ->  91.25 ; top5 ->  98.9  and loss:  58.57643564790487
forward train acc: top1 ->  99.706 ; top5 ->  99.998  and loss:  0.9023794922977686
test acc: top1 ->  91.14 ; top5 ->  98.87  and loss:  59.74367755651474
forward train acc: top1 ->  99.73 ; top5 ->  99.996  and loss:  0.8695370862260461
test acc: top1 ->  91.14 ; top5 ->  98.82  and loss:  59.45969953760505
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  253 / 512 , inc:  1
---------------- start layer  8  ---------------
### skip layer  8 wait:  4  ###
---------------- start layer  9  ---------------
adv train loss:  -1516.9199075698853 , diff:  1516.9199075698853
adv train loss:  -1517.1278820037842 , diff:  0.20797443389892578
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  33
test acc: top1 ->  11.26 ; top5 ->  50.04  and loss:  48453.88754272461
forward train acc: top1 ->  99.702 ; top5 ->  99.998  and loss:  1.0143922596471384
test acc: top1 ->  92.29 ; top5 ->  98.96  and loss:  57.695232417434454
==> this epoch:  33 / 512
---------------- start layer  10  ---------------
adv train loss:  -241.7499862909317 , diff:  241.7499862909317
adv train loss:  -242.4649624824524 , diff:  0.7149761915206909
adv train loss:  -242.21125268936157 , diff:  0.2537097930908203
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  10
test acc: top1 ->  10.0 ; top5 ->  53.98  and loss:  38870.6252746582
forward train acc: top1 ->  88.128 ; top5 ->  97.548  and loss:  101.5968802832067
test acc: top1 ->  91.16 ; top5 ->  98.48  and loss:  41.785476207733154
forward train acc: top1 ->  99.818 ; top5 ->  99.998  and loss:  1.2541391253471375
test acc: top1 ->  91.48 ; top5 ->  98.63  and loss:  41.3261643871665
forward train acc: top1 ->  99.90799997558594 ; top5 ->  100.0  and loss:  0.7568994415923953
test acc: top1 ->  91.6 ; top5 ->  98.66  and loss:  42.42320051789284
forward train acc: top1 ->  99.89999997558594 ; top5 ->  99.998  and loss:  0.6096126530319452
test acc: top1 ->  91.7 ; top5 ->  98.66  and loss:  42.9011260047555
forward train acc: top1 ->  99.914 ; top5 ->  99.998  and loss:  0.526510902447626
test acc: top1 ->  91.74 ; top5 ->  98.69  and loss:  43.7344541400671
forward train acc: top1 ->  99.942 ; top5 ->  100.0  and loss:  0.3971416358835995
test acc: top1 ->  91.77 ; top5 ->  98.73  and loss:  44.233839482069016
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.3481321793515235
test acc: top1 ->  91.68 ; top5 ->  98.76  and loss:  44.54881316423416
forward train acc: top1 ->  99.956 ; top5 ->  99.998  and loss:  0.32506797928363085
test acc: top1 ->  91.78 ; top5 ->  98.71  and loss:  44.92409811913967
forward train acc: top1 ->  99.95199997558593 ; top5 ->  100.0  and loss:  0.32773151621222496
test acc: top1 ->  91.84 ; top5 ->  98.74  and loss:  44.872339710593224
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.2625876651145518
test acc: top1 ->  91.89 ; top5 ->  98.75  and loss:  45.41950298845768
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  11 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -1309.5216436386108 , diff:  1309.5216436386108
adv train loss:  -1497.8168573379517 , diff:  188.29521369934082
adv train loss:  -1497.7455463409424 , diff:  0.07131099700927734
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  4
test acc: top1 ->  10.0 ; top5 ->  50.28  and loss:  18189.006942749023
forward train acc: top1 ->  96.57 ; top5 ->  99.972  and loss:  12.82098773540929
test acc: top1 ->  91.26 ; top5 ->  98.71  and loss:  48.76977502182126
forward train acc: top1 ->  99.9 ; top5 ->  100.0  and loss:  0.48892539646476507
test acc: top1 ->  91.9 ; top5 ->  98.92  and loss:  46.51115075498819
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.3576233258936554
test acc: top1 ->  91.99 ; top5 ->  98.98  and loss:  47.69154626131058
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.22269944637082517
test acc: top1 ->  92.12 ; top5 ->  98.92  and loss:  49.57702458649874
==> this epoch:  4 / 512
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
adv train loss:  -19592.755157470703 , diff:  19592.755157470703
adv train loss:  -32634.422271728516 , diff:  13041.667114257812
adv train loss:  -45515.92419433594 , diff:  12881.501922607422
adv train loss:  -58337.22735595703 , diff:  12821.303161621094
adv train loss:  -71138.66491699219 , diff:  12801.437561035156
adv train loss:  -83927.85552978516 , diff:  12789.190612792969
adv train loss:  -96720.74212646484 , diff:  12792.886596679688
adv train loss:  -109477.17413330078 , diff:  12756.432006835938
adv train loss:  -122254.35961914062 , diff:  12777.185485839844
adv train loss:  -135037.97314453125 , diff:  12783.613525390625
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  7
test acc: top1 ->  10.0 ; top5 ->  59.56  and loss:  1762.5040159225464
forward train acc: top1 ->  34.814000001220705 ; top5 ->  60.97399998046875  and loss:  792.6486372947693
test acc: top1 ->  52.59 ; top5 ->  76.34  and loss:  286.43201196193695
forward train acc: top1 ->  76.21400000976563 ; top5 ->  91.488  and loss:  106.14510694146156
test acc: top1 ->  81.79 ; top5 ->  98.84  and loss:  58.91675540804863
forward train acc: top1 ->  99.37799997558594 ; top5 ->  100.0  and loss:  22.776017904281616
test acc: top1 ->  91.21 ; top5 ->  98.98  and loss:  39.280597910284996
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  9.936050176620483
test acc: top1 ->  91.54 ; top5 ->  99.1  and loss:  34.094151720404625
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  5.329898335039616
test acc: top1 ->  91.66 ; top5 ->  99.1  and loss:  32.47872997820377
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  3.6119953244924545
test acc: top1 ->  91.71 ; top5 ->  99.07  and loss:  32.311482310295105
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  2.829677000641823
test acc: top1 ->  91.65 ; top5 ->  99.13  and loss:  32.36121855676174
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  2.2808069065213203
test acc: top1 ->  91.76 ; top5 ->  99.15  and loss:  32.38043212890625
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  1.8077754490077496
test acc: top1 ->  91.89 ; top5 ->  99.11  and loss:  32.654612109065056
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  1.4863771107047796
test acc: top1 ->  91.82 ; top5 ->  99.14  and loss:  33.1137206107378
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  9 / 512 , inc:  2
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.361328125  ==>  185 / 512 , inc:  8
layer  9  :  0.064453125  ==>  33 / 512 , inc:  16
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.0078125  ==>  4 / 512 , inc:  2
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [1.5569560546875, 1.5569560546875, 1.5569560546875, 1.5569560546875, 1.5569560546875, 1.5569560546875, 1.5569560546875, 0.07298231506347656, 1.037970703125, 1.3839609375000002, 0.8757877807617187, 1.3839609375000002, 1.167717041015625, 16.60753125]  wait [2, 2, 2, 2, 2, 2, 2, 4, 3, 0, 4, 0, 3, 4]  inc [1, 1, 1, 1, 1, 1, 1, 1, 8, 16, 1, 2, 1, 1]  tol: 3
$$$$$$$$$$$$$ epoch  48  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1197.1958494186401 , diff:  1197.1958494186401
adv train loss:  -1214.3362302780151 , diff:  17.140380859375
adv train loss:  -1215.9573545455933 , diff:  1.621124267578125
adv train loss:  -1210.8359422683716 , diff:  5.12141227722168
adv train loss:  -1208.5495643615723 , diff:  2.2863779067993164
adv train loss:  -1210.9207916259766 , diff:  2.371227264404297
adv train loss:  -1210.5075950622559 , diff:  0.4131965637207031
layer  0  adv train finish, try to retain  7
test acc: top1 ->  9.49 ; top5 ->  50.01  and loss:  3348.7588787078857
forward train acc: top1 ->  52.651999986572264 ; top5 ->  88.88399998046874  and loss:  253.05215978622437
test acc: top1 ->  41.72 ; top5 ->  79.55  and loss:  261.971382021904
forward train acc: top1 ->  64.89799998535156 ; top5 ->  93.77399997558594  and loss:  115.131791472435
test acc: top1 ->  67.32 ; top5 ->  94.69  and loss:  105.52429807186127
forward train acc: top1 ->  69.53399997314453 ; top5 ->  95.40000001708984  and loss:  95.71396780014038
test acc: top1 ->  70.09 ; top5 ->  96.02  and loss:  93.62995600700378
forward train acc: top1 ->  72.84399998046875 ; top5 ->  96.23600001220703  and loss:  84.9889822602272
test acc: top1 ->  72.33 ; top5 ->  96.62  and loss:  86.24307060241699
forward train acc: top1 ->  75.12999999511719 ; top5 ->  96.85599998779297  and loss:  77.16298413276672
test acc: top1 ->  74.28 ; top5 ->  96.94  and loss:  80.29183030128479
forward train acc: top1 ->  76.87599998046875 ; top5 ->  97.23799998535156  and loss:  71.75058925151825
test acc: top1 ->  75.3 ; top5 ->  97.06  and loss:  77.40960440039635
forward train acc: top1 ->  77.71799999023438 ; top5 ->  97.51199998291015  and loss:  68.5409345626831
test acc: top1 ->  76.02 ; top5 ->  97.23  and loss:  75.4238530099392
forward train acc: top1 ->  78.63800001220703 ; top5 ->  97.53599997802735  and loss:  66.36009931564331
test acc: top1 ->  76.65 ; top5 ->  97.36  and loss:  73.71584358811378
forward train acc: top1 ->  79.54999999023437 ; top5 ->  97.82000000488281  and loss:  63.09181386232376
test acc: top1 ->  77.43 ; top5 ->  97.62  and loss:  71.47525861859322
forward train acc: top1 ->  80.27200000488281 ; top5 ->  97.90199997802735  and loss:  60.92083328962326
test acc: top1 ->  77.82 ; top5 ->  97.74  and loss:  69.4989520907402
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -10.686941288411617 , diff:  10.686941288411617
adv train loss:  -11.14271143823862 , diff:  0.4557701498270035
adv train loss:  -11.689329244196415 , diff:  0.5466178059577942
adv train loss:  -11.729400798678398 , diff:  0.040071554481983185
adv train loss:  -11.745326817035675 , diff:  0.015926018357276917
layer  1  adv train finish, try to retain  47
test acc: top1 ->  41.77 ; top5 ->  74.91  and loss:  221.87889695167542
forward train acc: top1 ->  99.38400000244141 ; top5 ->  99.994  and loss:  2.760522184893489
test acc: top1 ->  91.58 ; top5 ->  99.27  and loss:  40.39460676535964
forward train acc: top1 ->  99.634 ; top5 ->  100.0  and loss:  1.0967848342843354
test acc: top1 ->  91.63 ; top5 ->  99.27  and loss:  43.70139006525278
forward train acc: top1 ->  99.776 ; top5 ->  99.996  and loss:  0.7429394606733695
test acc: top1 ->  91.83 ; top5 ->  99.24  and loss:  48.029060147702694
forward train acc: top1 ->  99.81199997558593 ; top5 ->  99.998  and loss:  0.5853384113870561
test acc: top1 ->  91.57 ; top5 ->  99.23  and loss:  49.749560952186584
forward train acc: top1 ->  99.806 ; top5 ->  100.0  and loss:  0.5733960324432701
test acc: top1 ->  91.93 ; top5 ->  99.11  and loss:  52.479892291128635
forward train acc: top1 ->  99.85399997558594 ; top5 ->  100.0  and loss:  0.42494530929252505
test acc: top1 ->  91.96 ; top5 ->  99.24  and loss:  51.80940065532923
forward train acc: top1 ->  99.866 ; top5 ->  100.0  and loss:  0.40233852446544915
test acc: top1 ->  91.87 ; top5 ->  99.2  and loss:  51.51287258416414
forward train acc: top1 ->  99.86599997558594 ; top5 ->  100.0  and loss:  0.34660630486905575
test acc: top1 ->  91.83 ; top5 ->  99.27  and loss:  52.54832797497511
forward train acc: top1 ->  99.886 ; top5 ->  99.998  and loss:  0.3727329885587096
test acc: top1 ->  91.78 ; top5 ->  99.23  and loss:  52.51471671462059
forward train acc: top1 ->  99.894 ; top5 ->  100.0  and loss:  0.31544397911056876
test acc: top1 ->  91.72 ; top5 ->  99.24  and loss:  53.68677854537964
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  55 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -788.7820544079877 , diff:  788.7820544079877
adv train loss:  -1290.791696548462 , diff:  502.00964214047417
adv train loss:  -1294.548095703125 , diff:  3.756399154663086
adv train loss:  -1301.4616231918335 , diff:  6.913527488708496
adv train loss:  -1299.6905117034912 , diff:  1.7711114883422852
adv train loss:  -1299.8726425170898 , diff:  0.1821308135986328
adv train loss:  -1297.0989255905151 , diff:  2.773716926574707
adv train loss:  -1301.020453453064 , diff:  3.921527862548828
adv train loss:  -1303.118543624878 , diff:  2.098090171813965
adv train loss:  -1302.016092300415 , diff:  1.1024513244628906
layer  2  adv train finish, try to retain  58
test acc: top1 ->  14.55 ; top5 ->  65.08  and loss:  1093.3080739974976
forward train acc: top1 ->  97.03400000732422 ; top5 ->  99.92399997558594  and loss:  10.26297963783145
test acc: top1 ->  88.69 ; top5 ->  99.07  and loss:  46.811015233397484
forward train acc: top1 ->  97.73200000244141 ; top5 ->  99.958  and loss:  6.891250381246209
test acc: top1 ->  89.12 ; top5 ->  99.09  and loss:  45.467475950717926
forward train acc: top1 ->  98.15599997558594 ; top5 ->  99.968  and loss:  5.356346159242094
test acc: top1 ->  89.47 ; top5 ->  99.15  and loss:  46.529485151171684
forward train acc: top1 ->  98.39399998046875 ; top5 ->  99.982  and loss:  4.744963470846415
test acc: top1 ->  89.92 ; top5 ->  99.17  and loss:  46.00264313817024
forward train acc: top1 ->  98.53000000488281 ; top5 ->  99.988  and loss:  4.251533363945782
test acc: top1 ->  89.96 ; top5 ->  99.21  and loss:  45.93674610555172
forward train acc: top1 ->  98.81800000244141 ; top5 ->  99.98199997558594  and loss:  3.680029415525496
test acc: top1 ->  90.14 ; top5 ->  99.15  and loss:  45.258284121751785
forward train acc: top1 ->  98.81200000732422 ; top5 ->  99.984  and loss:  3.573678845539689
test acc: top1 ->  90.24 ; top5 ->  99.15  and loss:  45.56073623895645
forward train acc: top1 ->  98.82600000488281 ; top5 ->  99.976  and loss:  3.6430660309270024
test acc: top1 ->  90.07 ; top5 ->  99.19  and loss:  45.55351719260216
forward train acc: top1 ->  98.84799998046876 ; top5 ->  99.988  and loss:  3.4175353571772575
test acc: top1 ->  90.26 ; top5 ->  99.18  and loss:  45.40498396009207
forward train acc: top1 ->  98.91399997558594 ; top5 ->  99.99  and loss:  3.2501016799360514
test acc: top1 ->  90.22 ; top5 ->  99.17  and loss:  45.76309962570667
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -529.5172684686258 , diff:  529.5172684686258
adv train loss:  -953.8755435943604 , diff:  424.3582751257345
adv train loss:  -973.9999074935913 , diff:  20.124363899230957
adv train loss:  -975.0487957000732 , diff:  1.0488882064819336
adv train loss:  -992.8660278320312 , diff:  17.817232131958008
adv train loss:  -1026.993863105774 , diff:  34.127835273742676
adv train loss:  -1032.3977146148682 , diff:  5.403851509094238
adv train loss:  -1029.9456768035889 , diff:  2.452037811279297
adv train loss:  -1035.862401008606 , diff:  5.91672420501709
adv train loss:  -1046.182728767395 , diff:  10.320327758789062
layer  3  adv train finish, try to retain  38
test acc: top1 ->  12.57 ; top5 ->  57.37  and loss:  941.7469539642334
forward train acc: top1 ->  89.87799999023437 ; top5 ->  99.3880000024414  and loss:  32.39557226002216
test acc: top1 ->  84.79 ; top5 ->  98.58  and loss:  52.60068288445473
forward train acc: top1 ->  92.08000000244141 ; top5 ->  99.6240000024414  and loss:  24.027971401810646
test acc: top1 ->  85.77 ; top5 ->  98.87  and loss:  48.62759667634964
forward train acc: top1 ->  93.13000000732421 ; top5 ->  99.69599997558593  and loss:  20.635344564914703
test acc: top1 ->  86.45 ; top5 ->  99.08  and loss:  46.45756413042545
forward train acc: top1 ->  93.72800001953125 ; top5 ->  99.7700000024414  and loss:  18.787193790078163
test acc: top1 ->  86.82 ; top5 ->  98.95  and loss:  45.6986446082592
forward train acc: top1 ->  94.3639999975586 ; top5 ->  99.83599997558593  and loss:  16.721587494015694
test acc: top1 ->  87.05 ; top5 ->  99.05  and loss:  45.28829789161682
forward train acc: top1 ->  94.66600001953125 ; top5 ->  99.854  and loss:  15.899062775075436
test acc: top1 ->  87.32 ; top5 ->  99.09  and loss:  44.75473664700985
forward train acc: top1 ->  94.88400001464844 ; top5 ->  99.87199997558594  and loss:  15.382550276815891
test acc: top1 ->  87.53 ; top5 ->  99.07  and loss:  44.769359946250916
forward train acc: top1 ->  94.918 ; top5 ->  99.864  and loss:  14.973126739263535
test acc: top1 ->  87.56 ; top5 ->  99.07  and loss:  44.77234487235546
forward train acc: top1 ->  94.94200000976562 ; top5 ->  99.854  and loss:  15.013170197606087
test acc: top1 ->  87.66 ; top5 ->  99.13  and loss:  44.15343925356865
forward train acc: top1 ->  95.21399999755859 ; top5 ->  99.894  and loss:  14.038281388580799
test acc: top1 ->  87.7 ; top5 ->  99.13  and loss:  44.436373233795166
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -91.41258867830038 , diff:  91.41258867830038
adv train loss:  -697.5956120491028 , diff:  606.1830233708024
adv train loss:  -815.493350982666 , diff:  117.89773893356323
adv train loss:  -863.9733791351318 , diff:  48.48002815246582
adv train loss:  -898.5333423614502 , diff:  34.55996322631836
adv train loss:  -902.7334041595459 , diff:  4.200061798095703
adv train loss:  -916.258171081543 , diff:  13.52476692199707
adv train loss:  -948.1348695755005 , diff:  31.87669849395752
adv train loss:  -960.067440032959 , diff:  11.932570457458496
adv train loss:  -961.716549873352 , diff:  1.6491098403930664
layer  4  adv train finish, try to retain  42
test acc: top1 ->  25.28 ; top5 ->  67.77  and loss:  737.1522493362427
forward train acc: top1 ->  83.23200001220704 ; top5 ->  98.80199997802734  and loss:  51.19709089398384
test acc: top1 ->  80.3 ; top5 ->  97.91  and loss:  64.52854689955711
forward train acc: top1 ->  86.47200000488282 ; top5 ->  99.28799997558593  and loss:  39.880094945430756
test acc: top1 ->  82.21 ; top5 ->  98.24  and loss:  58.87133875489235
forward train acc: top1 ->  87.73199997314453 ; top5 ->  99.40800000488281  and loss:  35.765049040317535
test acc: top1 ->  83.04 ; top5 ->  98.48  and loss:  55.6859675347805
forward train acc: top1 ->  88.88199999267579 ; top5 ->  99.49199997558594  and loss:  32.468955144286156
test acc: top1 ->  83.58 ; top5 ->  98.62  and loss:  54.19276964664459
forward train acc: top1 ->  89.4179999975586 ; top5 ->  99.57400000244141  and loss:  30.75335183739662
test acc: top1 ->  84.07 ; top5 ->  98.64  and loss:  52.88998183608055
forward train acc: top1 ->  89.86800000488282 ; top5 ->  99.598  and loss:  29.35706129670143
test acc: top1 ->  84.49 ; top5 ->  98.82  and loss:  51.50204321742058
forward train acc: top1 ->  90.14399997070312 ; top5 ->  99.6560000024414  and loss:  28.384900897741318
test acc: top1 ->  84.56 ; top5 ->  98.74  and loss:  50.834307461977005
forward train acc: top1 ->  90.40199999755859 ; top5 ->  99.60799997558594  and loss:  27.728102773427963
test acc: top1 ->  84.77 ; top5 ->  98.82  and loss:  50.379545509815216
forward train acc: top1 ->  90.60800001464844 ; top5 ->  99.658  and loss:  27.298717722296715
test acc: top1 ->  84.99 ; top5 ->  98.83  and loss:  50.01146340370178
forward train acc: top1 ->  90.66599997070313 ; top5 ->  99.694  and loss:  26.708092749118805
test acc: top1 ->  85.17 ; top5 ->  98.81  and loss:  49.822292268276215
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -162.79348167404532 , diff:  162.79348167404532
adv train loss:  -872.5674529075623 , diff:  709.7739712335169
adv train loss:  -981.5679817199707 , diff:  109.00052881240845
adv train loss:  -1018.0134229660034 , diff:  36.445441246032715
adv train loss:  -1023.3096723556519 , diff:  5.2962493896484375
adv train loss:  -1033.2845401763916 , diff:  9.974867820739746
adv train loss:  -1037.9349632263184 , diff:  4.650423049926758
adv train loss:  -1040.2181692123413 , diff:  2.283205986022949
adv train loss:  -1039.8326396942139 , diff:  0.3855295181274414
adv train loss:  -1048.0203714370728 , diff:  8.187731742858887
layer  5  adv train finish, try to retain  50
test acc: top1 ->  29.54 ; top5 ->  65.17  and loss:  621.4540243148804
forward train acc: top1 ->  91.51999997802734 ; top5 ->  99.728  and loss:  24.489341378211975
test acc: top1 ->  86.01 ; top5 ->  99.09  and loss:  46.07459142804146
forward train acc: top1 ->  93.33999999267579 ; top5 ->  99.83999997558594  and loss:  18.80026100575924
test acc: top1 ->  87.29 ; top5 ->  99.11  and loss:  43.86418674886227
forward train acc: top1 ->  94.3400000024414 ; top5 ->  99.89199997558593  and loss:  16.267563313245773
test acc: top1 ->  87.64 ; top5 ->  99.17  and loss:  43.3721471875906
forward train acc: top1 ->  94.84200001464843 ; top5 ->  99.916  and loss:  14.766815558075905
test acc: top1 ->  88.0 ; top5 ->  99.28  and loss:  42.30294689536095
forward train acc: top1 ->  95.28799999023437 ; top5 ->  99.922  and loss:  13.314961396157742
test acc: top1 ->  88.31 ; top5 ->  99.23  and loss:  41.58978587388992
forward train acc: top1 ->  95.70199999267578 ; top5 ->  99.92  and loss:  12.440727457404137
test acc: top1 ->  88.39 ; top5 ->  99.26  and loss:  41.4351274818182
forward train acc: top1 ->  95.68800001953124 ; top5 ->  99.92199997558593  and loss:  12.009856313467026
test acc: top1 ->  88.42 ; top5 ->  99.31  and loss:  41.44687609374523
forward train acc: top1 ->  95.79999998779297 ; top5 ->  99.942  and loss:  11.998165383934975
test acc: top1 ->  88.61 ; top5 ->  99.25  and loss:  41.03671170771122
forward train acc: top1 ->  96.02999999755859 ; top5 ->  99.952  and loss:  11.282153017818928
test acc: top1 ->  88.51 ; top5 ->  99.25  and loss:  41.483770579099655
forward train acc: top1 ->  96.01200001953126 ; top5 ->  99.936  and loss:  11.410947792232037
test acc: top1 ->  88.73 ; top5 ->  99.24  and loss:  41.091682240366936
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -224.93322333041579 , diff:  224.93322333041579
adv train loss:  -1169.1652841567993 , diff:  944.2320608263835
adv train loss:  -1274.1705169677734 , diff:  105.00523281097412
adv train loss:  -1293.8320875167847 , diff:  19.66157054901123
adv train loss:  -1316.0430727005005 , diff:  22.21098518371582
adv train loss:  -1330.3948230743408 , diff:  14.351750373840332
adv train loss:  -1338.939847946167 , diff:  8.545024871826172
adv train loss:  -1337.796573638916 , diff:  1.1432743072509766
adv train loss:  -1347.056324005127 , diff:  9.259750366210938
adv train loss:  -1347.8792562484741 , diff:  0.822932243347168
layer  6  adv train finish, try to retain  26
test acc: top1 ->  21.39 ; top5 ->  75.23  and loss:  566.740083694458
forward train acc: top1 ->  92.43199997314453 ; top5 ->  99.84399997558593  and loss:  21.64228066802025
test acc: top1 ->  86.8 ; top5 ->  99.01  and loss:  48.4823746830225
forward train acc: top1 ->  94.78199999267578 ; top5 ->  99.908  and loss:  14.80180136859417
test acc: top1 ->  87.84 ; top5 ->  98.99  and loss:  47.427883356809616
forward train acc: top1 ->  95.71999997314452 ; top5 ->  99.928  and loss:  12.349261987954378
test acc: top1 ->  87.92 ; top5 ->  99.06  and loss:  47.682068675756454
forward train acc: top1 ->  96.16399997070313 ; top5 ->  99.94599997558593  and loss:  10.985345665365458
test acc: top1 ->  88.62 ; top5 ->  99.15  and loss:  45.69161257147789
forward train acc: top1 ->  96.49200000976562 ; top5 ->  99.96  and loss:  9.973785240203142
test acc: top1 ->  88.6 ; top5 ->  99.09  and loss:  46.02380308508873
forward train acc: top1 ->  96.81799998291015 ; top5 ->  99.976  and loss:  8.775696001946926
test acc: top1 ->  88.93 ; top5 ->  99.15  and loss:  45.97627115249634
forward train acc: top1 ->  96.97399998535157 ; top5 ->  99.958  and loss:  8.609738528728485
test acc: top1 ->  89.08 ; top5 ->  99.1  and loss:  45.8072230219841
forward train acc: top1 ->  97.01800001464844 ; top5 ->  99.98599997558594  and loss:  8.318264078348875
test acc: top1 ->  89.19 ; top5 ->  99.16  and loss:  45.478382125496864
forward train acc: top1 ->  97.11400000732422 ; top5 ->  99.974  and loss:  7.997259642928839
test acc: top1 ->  89.2 ; top5 ->  99.05  and loss:  46.33547185361385
forward train acc: top1 ->  97.31399998291016 ; top5 ->  99.962  and loss:  7.744639914482832
test acc: top1 ->  89.17 ; top5 ->  99.11  and loss:  45.6963599473238
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  144 / 256 , inc:  1
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
### skip layer  8 wait:  3  ###
---------------- start layer  9  ---------------
adv train loss:  -34.22596649825573 , diff:  34.22596649825573
adv train loss:  -34.29542076587677 , diff:  0.06945426762104034
adv train loss:  -34.1981313675642 , diff:  0.09728939831256866
adv train loss:  -33.80140322446823 , diff:  0.39672814309597015
adv train loss:  -34.12511259317398 , diff:  0.3237093687057495
adv train loss:  -38.97677978873253 , diff:  4.851667195558548
adv train loss:  -40.13624057173729 , diff:  1.1594607830047607
adv train loss:  -76.91165021061897 , diff:  36.77540963888168
adv train loss:  -122.63343107700348 , diff:  45.721780866384506
adv train loss:  -123.2254011631012 , diff:  0.5919700860977173
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  17
test acc: top1 ->  10.0 ; top5 ->  53.88  and loss:  60259.332763671875
forward train acc: top1 ->  99.178 ; top5 ->  99.992  and loss:  3.344204332213849
test acc: top1 ->  92.0 ; top5 ->  99.01  and loss:  39.3654757887125
forward train acc: top1 ->  99.868 ; top5 ->  99.998  and loss:  0.6004583507310599
test acc: top1 ->  92.25 ; top5 ->  99.04  and loss:  44.01311969012022
==> this epoch:  17 / 512
---------------- start layer  10  ---------------
### skip layer  10 wait:  4  ###
---------------- start layer  11  ---------------
adv train loss:  -1475.612328529358 , diff:  1475.612328529358
adv train loss:  -1608.121587753296 , diff:  132.509259223938
adv train loss:  -1886.3808193206787 , diff:  278.2592315673828
adv train loss:  -2136.6596546173096 , diff:  250.27883529663086
adv train loss:  -2145.011308670044 , diff:  8.351654052734375
adv train loss:  -2143.6009521484375 , diff:  1.4103565216064453
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  2
test acc: top1 ->  12.96 ; top5 ->  59.7  and loss:  6728.746334075928
forward train acc: top1 ->  93.95199997802735 ; top5 ->  99.524  and loss:  29.102884899824858
test acc: top1 ->  91.0 ; top5 ->  98.71  and loss:  41.50948731601238
forward train acc: top1 ->  99.616 ; top5 ->  99.998  and loss:  3.0672645727172494
test acc: top1 ->  91.35 ; top5 ->  98.91  and loss:  42.13537375628948
forward train acc: top1 ->  99.762 ; top5 ->  100.0  and loss:  1.466036566067487
test acc: top1 ->  91.55 ; top5 ->  98.99  and loss:  44.049695163965225
forward train acc: top1 ->  99.84399997558593 ; top5 ->  99.998  and loss:  0.9825882902368903
test acc: top1 ->  91.5 ; top5 ->  99.02  and loss:  46.16859449446201
forward train acc: top1 ->  99.8560000024414 ; top5 ->  100.0  and loss:  0.7706019338220358
test acc: top1 ->  91.7 ; top5 ->  98.96  and loss:  47.338963106274605
forward train acc: top1 ->  99.91199997558594 ; top5 ->  100.0  and loss:  0.5673862025141716
test acc: top1 ->  91.73 ; top5 ->  98.94  and loss:  47.7918451577425
forward train acc: top1 ->  99.872 ; top5 ->  100.0  and loss:  0.5762603962793946
test acc: top1 ->  91.7 ; top5 ->  98.99  and loss:  48.63085001707077
forward train acc: top1 ->  99.88999997558594 ; top5 ->  100.0  and loss:  0.5016734953969717
test acc: top1 ->  91.78 ; top5 ->  98.98  and loss:  48.63016818463802
forward train acc: top1 ->  99.894 ; top5 ->  100.0  and loss:  0.4867027101572603
test acc: top1 ->  91.8 ; top5 ->  98.96  and loss:  48.94359336793423
forward train acc: top1 ->  99.90999997558593 ; top5 ->  100.0  and loss:  0.42172356974333525
test acc: top1 ->  91.82 ; top5 ->  99.01  and loss:  49.83888432383537
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  4 / 512 , inc:  2
---------------- start layer  12  ---------------
### skip layer  12 wait:  3  ###
---------------- start layer  13  ---------------
### skip layer  13 wait:  4  ###
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.361328125  ==>  185 / 512 , inc:  8
layer  9  :  0.033203125  ==>  17 / 512 , inc:  8
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.0078125  ==>  4 / 512 , inc:  1
layer  12  :  0.12109375  ==>  62 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [1.167717041015625, 1.167717041015625, 1.167717041015625, 1.167717041015625, 1.167717041015625, 1.167717041015625, 1.167717041015625, 0.07298231506347656, 1.037970703125, 1.3839609375000002, 0.8757877807617187, 1.037970703125, 1.167717041015625, 16.60753125]  wait [4, 4, 4, 4, 4, 4, 4, 3, 2, 0, 3, 2, 2, 3]  inc [1, 1, 1, 1, 1, 1, 1, 1, 8, 8, 1, 1, 1, 1]  tol: 3
$$$$$$$$$$$$$ epoch  49  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
### skip layer  1 wait:  4  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
adv train loss:  -91.21676850318909 , diff:  91.21676850318909
adv train loss:  -91.15068686008453 , diff:  0.06608164310455322
layer  8  adv train finish, try to retain  327
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -716.4083905220032 , diff:  716.4083905220032
adv train loss:  -717.9974489212036 , diff:  1.5890583992004395
adv train loss:  -717.9325985908508 , diff:  0.0648503303527832
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  9
test acc: top1 ->  10.0 ; top5 ->  50.07  and loss:  9133.706535339355
forward train acc: top1 ->  98.66799997558594 ; top5 ->  99.956  and loss:  4.849147038999945
test acc: top1 ->  91.95 ; top5 ->  98.99  and loss:  47.76222076267004
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.36172150645870715
test acc: top1 ->  92.26 ; top5 ->  98.97  and loss:  48.678165569901466
==> this epoch:  9 / 512
---------------- start layer  10  ---------------
### skip layer  10 wait:  3  ###
---------------- start layer  11  ---------------
adv train loss:  -2620.76381111145 , diff:  2620.76381111145
adv train loss:  -2643.3678131103516 , diff:  22.604001998901367
adv train loss:  -2577.971570968628 , diff:  65.39624214172363
adv train loss:  -2523.336093902588 , diff:  54.63547706604004
adv train loss:  -2450.9106426239014 , diff:  72.42545127868652
adv train loss:  -2452.157251358032 , diff:  1.2466087341308594
layer  11  adv train finish, try to retain  511
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -0.8924816265935078 , diff:  0.8924816265935078
adv train loss:  -0.27727290301118046 , diff:  0.6152087235823274
adv train loss:  -210.78553980449215 , diff:  210.50826690148097
adv train loss:  -1529.9434041976929 , diff:  1319.1578643932007
adv train loss:  -1598.1831283569336 , diff:  68.23972415924072
adv train loss:  -1597.3067045211792 , diff:  0.8764238357543945
adv train loss:  -1598.0506744384766 , diff:  0.7439699172973633
adv train loss:  -1598.4838161468506 , diff:  0.43314170837402344
adv train loss:  -1597.8234796524048 , diff:  0.6603364944458008
adv train loss:  -1587.4295148849487 , diff:  10.393964767456055
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  61
test acc: top1 ->  10.21 ; top5 ->  57.87  and loss:  35909.246032714844
forward train acc: top1 ->  91.346 ; top5 ->  99.836  and loss:  38.95451262034476
test acc: top1 ->  91.55 ; top5 ->  98.73  and loss:  46.53289891779423
forward train acc: top1 ->  99.884 ; top5 ->  100.0  and loss:  1.3306403439491987
test acc: top1 ->  91.85 ; top5 ->  98.79  and loss:  44.16063316911459
forward train acc: top1 ->  99.924 ; top5 ->  100.0  and loss:  0.7631304548121989
test acc: top1 ->  91.9 ; top5 ->  98.87  and loss:  44.9976594299078
forward train acc: top1 ->  99.942 ; top5 ->  100.0  and loss:  0.5135656860657036
test acc: top1 ->  92.01 ; top5 ->  98.9  and loss:  45.6147206351161
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.3672537369420752
test acc: top1 ->  92.13 ; top5 ->  98.96  and loss:  45.547748640179634
==> this epoch:  61 / 512
---------------- start layer  13  ---------------
### skip layer  13 wait:  3  ###
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.361328125  ==>  185 / 512 , inc:  8
layer  9  :  0.017578125  ==>  9 / 512 , inc:  4
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.0078125  ==>  4 / 512 , inc:  1
layer  12  :  0.119140625  ==>  61 / 512 , inc:  2
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [1.167717041015625, 1.167717041015625, 1.167717041015625, 1.167717041015625, 1.167717041015625, 1.167717041015625, 1.167717041015625, 0.07298231506347656, 2.07594140625, 1.3839609375000002, 0.8757877807617187, 2.07594140625, 1.167717041015625, 16.60753125]  wait [3, 3, 3, 3, 3, 3, 3, 2, 2, 0, 2, 2, 0, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 8, 4, 1, 1, 2, 1]  tol: 3
$$$$$$$$$$$$$ epoch  50  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
### skip layer  1 wait:  3  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  3  ###
---------------- start layer  7  ---------------
adv train loss:  -0.7735244915820658 , diff:  0.7735244915820658
adv train loss:  -0.7502891083713621 , diff:  0.02323538321070373
adv train loss:  -0.7634473000653088 , diff:  0.01315819169394672
adv train loss:  -0.72685736999847 , diff:  0.03658993006683886
adv train loss:  -0.7487352925818413 , diff:  0.0218779225833714
adv train loss:  -0.7735065270680934 , diff:  0.02477123448625207
adv train loss:  -0.760785503545776 , diff:  0.01272102352231741
adv train loss:  -0.7434673639945686 , diff:  0.017318139551207423
adv train loss:  -0.7509999799076468 , diff:  0.007532615913078189
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  252
test acc: top1 ->  67.29 ; top5 ->  96.98  and loss:  99.28698021173477
forward train acc: top1 ->  99.61199997558593 ; top5 ->  99.998  and loss:  1.5055712764151394
test acc: top1 ->  90.91 ; top5 ->  98.99  and loss:  49.226075530052185
forward train acc: top1 ->  99.814 ; top5 ->  100.0  and loss:  0.7088497106451541
test acc: top1 ->  91.26 ; top5 ->  98.99  and loss:  51.98823583871126
forward train acc: top1 ->  99.85599997558593 ; top5 ->  100.0  and loss:  0.5607358306297101
test acc: top1 ->  91.43 ; top5 ->  99.16  and loss:  52.32409270107746
forward train acc: top1 ->  99.85999997558594 ; top5 ->  100.0  and loss:  0.45387419703183696
test acc: top1 ->  91.65 ; top5 ->  99.13  and loss:  54.74788425862789
forward train acc: top1 ->  99.876 ; top5 ->  100.0  and loss:  0.40290282119531184
test acc: top1 ->  91.44 ; top5 ->  99.12  and loss:  54.92409427464008
forward train acc: top1 ->  99.89 ; top5 ->  100.0  and loss:  0.3428680670331232
test acc: top1 ->  91.72 ; top5 ->  99.21  and loss:  55.0443902015686
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.28548859391594306
test acc: top1 ->  91.7 ; top5 ->  99.26  and loss:  55.77687905728817
forward train acc: top1 ->  99.916 ; top5 ->  100.0  and loss:  0.27303603378823027
test acc: top1 ->  91.71 ; top5 ->  99.29  and loss:  55.968311205506325
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.22194607523852028
test acc: top1 ->  91.74 ; top5 ->  99.16  and loss:  57.56916104257107
forward train acc: top1 ->  99.92199997558593 ; top5 ->  100.0  and loss:  0.2340099816210568
test acc: top1 ->  91.73 ; top5 ->  99.11  and loss:  57.39490665495396
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  253 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -0.506559660192579 , diff:  0.506559660192579
adv train loss:  -0.38376345788128674 , diff:  0.12279620231129229
adv train loss:  -0.39547845275956206 , diff:  0.01171499487827532
adv train loss:  -0.4853438182035461 , diff:  0.08986536544398405
adv train loss:  -0.45943875377997756 , diff:  0.025905064423568547
adv train loss:  -0.48738065152429044 , diff:  0.027941897744312882
adv train loss:  -0.3864134468603879 , diff:  0.10096720466390252
adv train loss:  -0.450489045237191 , diff:  0.0640755983768031
adv train loss:  -0.4296578287612647 , diff:  0.02083121647592634
adv train loss:  -0.4424635121249594 , diff:  0.012805683363694698
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  177
test acc: top1 ->  24.17 ; top5 ->  55.8  and loss:  26328603.640625
forward train acc: top1 ->  99.164 ; top5 ->  100.0  and loss:  2.7920143250375986
test acc: top1 ->  91.8 ; top5 ->  98.94  and loss:  55.4800011664629
forward train acc: top1 ->  99.872 ; top5 ->  100.0  and loss:  0.4985512107377872
test acc: top1 ->  91.96 ; top5 ->  98.96  and loss:  55.854706838727
forward train acc: top1 ->  99.9040000024414 ; top5 ->  99.998  and loss:  0.3621585024520755
test acc: top1 ->  92.15 ; top5 ->  99.03  and loss:  56.60503448545933
==> this epoch:  177 / 512
---------------- start layer  9  ---------------
adv train loss:  -439.76343297958374 , diff:  439.76343297958374
adv train loss:  -438.7838704586029 , diff:  0.979562520980835
adv train loss:  -439.36836099624634 , diff:  0.5844905376434326
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  5
test acc: top1 ->  10.02 ; top5 ->  58.5  and loss:  7286.379802703857
forward train acc: top1 ->  83.13599999023438 ; top5 ->  98.616  and loss:  77.17396637052298
test acc: top1 ->  88.37 ; top5 ->  99.01  and loss:  49.27343125641346
forward train acc: top1 ->  98.15400000488282 ; top5 ->  99.994  and loss:  8.163660556077957
test acc: top1 ->  90.26 ; top5 ->  99.23  and loss:  44.3170220553875
forward train acc: top1 ->  99.18399997558593 ; top5 ->  99.998  and loss:  3.989682398736477
test acc: top1 ->  91.06 ; top5 ->  99.23  and loss:  44.94677014648914
forward train acc: top1 ->  99.48399998046875 ; top5 ->  100.0  and loss:  2.3069934472441673
test acc: top1 ->  91.46 ; top5 ->  99.2  and loss:  46.05612209439278
forward train acc: top1 ->  99.67800000244141 ; top5 ->  100.0  and loss:  1.540826603770256
test acc: top1 ->  91.72 ; top5 ->  99.15  and loss:  47.1914182305336
forward train acc: top1 ->  99.74599997558593 ; top5 ->  99.998  and loss:  1.1525525422766805
test acc: top1 ->  91.75 ; top5 ->  99.19  and loss:  47.60951487720013
forward train acc: top1 ->  99.81399997558594 ; top5 ->  99.998  and loss:  0.92990318313241
test acc: top1 ->  91.81 ; top5 ->  99.19  and loss:  48.31079000234604
forward train acc: top1 ->  99.826 ; top5 ->  100.0  and loss:  0.8157837793696672
test acc: top1 ->  91.88 ; top5 ->  99.13  and loss:  48.49269342422485
forward train acc: top1 ->  99.83199997558594 ; top5 ->  100.0  and loss:  0.7225280366837978
test acc: top1 ->  91.87 ; top5 ->  99.14  and loss:  49.08962495625019
forward train acc: top1 ->  99.86 ; top5 ->  100.0  and loss:  0.6658051335252821
test acc: top1 ->  91.92 ; top5 ->  99.11  and loss:  49.69499270617962
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  9 / 512 , inc:  4
---------------- start layer  10  ---------------
adv train loss:  -340.2847764492035 , diff:  340.2847764492035
adv train loss:  -341.2788202762604 , diff:  0.9940438270568848
adv train loss:  -340.5657410621643 , diff:  0.7130792140960693
adv train loss:  -340.556517124176 , diff:  0.00922393798828125
layer  10  adv train finish, try to retain  480
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -1305.3917999267578 , diff:  1305.3917999267578
adv train loss:  -1304.5010824203491 , diff:  0.8907175064086914
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  3
test acc: top1 ->  10.0 ; top5 ->  56.52  and loss:  25970.19776916504
forward train acc: top1 ->  98.926 ; top5 ->  99.988  and loss:  3.6830131063470617
test acc: top1 ->  91.49 ; top5 ->  98.75  and loss:  55.43161898851395
forward train acc: top1 ->  99.896 ; top5 ->  100.0  and loss:  0.4404148443136364
test acc: top1 ->  91.83 ; top5 ->  98.94  and loss:  55.1571458876133
forward train acc: top1 ->  99.914 ; top5 ->  100.0  and loss:  0.3181774191907607
test acc: top1 ->  91.91 ; top5 ->  98.96  and loss:  56.18306717276573
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.26328986836597323
test acc: top1 ->  92.0 ; top5 ->  98.97  and loss:  56.12294600903988
forward train acc: top1 ->  99.94 ; top5 ->  100.0  and loss:  0.22112289245706052
test acc: top1 ->  92.04 ; top5 ->  98.98  and loss:  56.68601469695568
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.14291278773453087
test acc: top1 ->  92.04 ; top5 ->  98.93  and loss:  57.11143101751804
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.13791351846884936
test acc: top1 ->  91.96 ; top5 ->  98.98  and loss:  57.440903291106224
forward train acc: top1 ->  99.96999997558594 ; top5 ->  100.0  and loss:  0.1388107556849718
test acc: top1 ->  92.07 ; top5 ->  98.93  and loss:  57.61442017555237
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  4 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -1361.8697338104248 , diff:  1361.8697338104248
adv train loss:  -1354.2098636627197 , diff:  7.659870147705078
adv train loss:  -1558.4682693481445 , diff:  204.2584056854248
adv train loss:  -2185.788642883301 , diff:  627.3203735351562
adv train loss:  -2185.0156955718994 , diff:  0.7729473114013672
adv train loss:  -2183.7971630096436 , diff:  1.2185325622558594
adv train loss:  -2182.924072265625 , diff:  0.8730907440185547
adv train loss:  -2184.4529933929443 , diff:  1.528921127319336
adv train loss:  -1857.2227764129639 , diff:  327.23021697998047
adv train loss:  -1752.1313667297363 , diff:  105.09140968322754
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  59
test acc: top1 ->  35.39 ; top5 ->  59.94  and loss:  17547.085159301758
forward train acc: top1 ->  97.986 ; top5 ->  99.768  and loss:  9.127327298047021
test acc: top1 ->  91.93 ; top5 ->  99.16  and loss:  46.92544788122177
forward train acc: top1 ->  99.94599997558593 ; top5 ->  99.998  and loss:  0.3309708056040108
test acc: top1 ->  92.16 ; top5 ->  99.14  and loss:  48.71793778985739
==> this epoch:  59 / 512
---------------- start layer  13  ---------------
adv train loss:  -11309.944931030273 , diff:  11309.944931030273
adv train loss:  -18550.354598999023 , diff:  7240.40966796875
adv train loss:  -25753.37255859375 , diff:  7203.017959594727
adv train loss:  -32979.33416748047 , diff:  7225.961608886719
adv train loss:  -40201.42221069336 , diff:  7222.088043212891
adv train loss:  -47427.830017089844 , diff:  7226.407806396484
adv train loss:  -54617.06076049805 , diff:  7189.230743408203
adv train loss:  -61845.952087402344 , diff:  7228.891326904297
adv train loss:  -69059.73565673828 , diff:  7213.7835693359375
adv train loss:  -76243.40936279297 , diff:  7183.6737060546875
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  8
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  1539.554307937622
forward train acc: top1 ->  22.343999993896485 ; top5 ->  75.13399997558594  and loss:  602.9617943763733
test acc: top1 ->  51.39 ; top5 ->  97.28  and loss:  134.84343647956848
forward train acc: top1 ->  89.72799997558593 ; top5 ->  99.948  and loss:  69.66914561390877
test acc: top1 ->  88.51 ; top5 ->  98.64  and loss:  67.60463863611221
forward train acc: top1 ->  99.3520000024414 ; top5 ->  99.998  and loss:  29.50267332792282
test acc: top1 ->  90.34 ; top5 ->  98.7  and loss:  47.58774530887604
forward train acc: top1 ->  99.6860000024414 ; top5 ->  100.0  and loss:  16.488357111811638
test acc: top1 ->  90.85 ; top5 ->  98.69  and loss:  41.35579797625542
forward train acc: top1 ->  99.768 ; top5 ->  100.0  and loss:  10.948963686823845
test acc: top1 ->  91.04 ; top5 ->  98.78  and loss:  38.903680577874184
forward train acc: top1 ->  99.84399997558593 ; top5 ->  100.0  and loss:  8.289464876055717
test acc: top1 ->  90.87 ; top5 ->  98.69  and loss:  38.31753146648407
forward train acc: top1 ->  99.82799997558594 ; top5 ->  100.0  and loss:  7.157445099204779
test acc: top1 ->  91.15 ; top5 ->  98.7  and loss:  37.94718670845032
forward train acc: top1 ->  99.878 ; top5 ->  100.0  and loss:  6.044794835150242
test acc: top1 ->  91.14 ; top5 ->  98.68  and loss:  37.70293912291527
forward train acc: top1 ->  99.86 ; top5 ->  100.0  and loss:  5.18645990267396
test acc: top1 ->  91.18 ; top5 ->  98.66  and loss:  37.84782263636589
forward train acc: top1 ->  99.878 ; top5 ->  100.0  and loss:  4.473073545843363
test acc: top1 ->  91.3 ; top5 ->  98.65  and loss:  37.954894468188286
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  9 / 512 , inc:  1
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.345703125  ==>  177 / 512 , inc:  16
layer  9  :  0.017578125  ==>  9 / 512 , inc:  2
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.0078125  ==>  4 / 512 , inc:  1
layer  12  :  0.115234375  ==>  59 / 512 , inc:  4
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [1.167717041015625, 1.167717041015625, 1.167717041015625, 1.167717041015625, 1.167717041015625, 1.167717041015625, 1.167717041015625, 0.05473673629760742, 2.07594140625, 1.037970703125, 1.7515755615234374, 1.5569560546875, 1.167717041015625, 12.4556484375]  wait [2, 2, 2, 2, 2, 2, 2, 4, 0, 2, 2, 4, 0, 4]  inc [1, 1, 1, 1, 1, 1, 1, 1, 16, 2, 1, 1, 4, 1]  tol: 3
$$$$$$$$$$$$$ epoch  51  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1252.5590658187866 , diff:  1252.5590658187866
adv train loss:  -1256.7936096191406 , diff:  4.234543800354004
adv train loss:  -1258.5768032073975 , diff:  1.783193588256836
adv train loss:  -1270.794017791748 , diff:  12.217214584350586
adv train loss:  -1278.6325721740723 , diff:  7.838554382324219
adv train loss:  -1278.3415327072144 , diff:  0.29103946685791016
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  31
test acc: top1 ->  10.18 ; top5 ->  50.0  and loss:  13041.377517700195
forward train acc: top1 ->  90.02800000488281 ; top5 ->  98.25  and loss:  57.25587127637118
test acc: top1 ->  90.01 ; top5 ->  98.88  and loss:  62.47196018695831
forward train acc: top1 ->  99.31599998046875 ; top5 ->  99.99  and loss:  2.4646819438785315
test acc: top1 ->  90.71 ; top5 ->  99.06  and loss:  58.245541378855705
forward train acc: top1 ->  99.51799997558594 ; top5 ->  99.996  and loss:  1.7552126792725176
test acc: top1 ->  91.17 ; top5 ->  99.16  and loss:  55.865827202796936
forward train acc: top1 ->  99.69199997558594 ; top5 ->  99.998  and loss:  1.1473583234474063
test acc: top1 ->  91.26 ; top5 ->  99.09  and loss:  55.4175493568182
forward train acc: top1 ->  99.77999997558594 ; top5 ->  100.0  and loss:  0.9028294829186052
test acc: top1 ->  91.39 ; top5 ->  99.24  and loss:  54.29535160958767
forward train acc: top1 ->  99.78 ; top5 ->  100.0  and loss:  0.8285902530187741
test acc: top1 ->  91.42 ; top5 ->  99.22  and loss:  53.93879245221615
forward train acc: top1 ->  99.77799997558594 ; top5 ->  99.996  and loss:  0.8276125950505957
test acc: top1 ->  91.43 ; top5 ->  99.2  and loss:  54.397276237607
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -2.22778083011508 , diff:  2.22778083011508
adv train loss:  -2.5148428212851286 , diff:  0.2870619911700487
adv train loss:  -2.812892454210669 , diff:  0.2980496329255402
adv train loss:  -2.431519469479099 , diff:  0.3813729847315699
adv train loss:  -2.2793816183693707 , diff:  0.15213785110972822
adv train loss:  -2.5558937969617546 , diff:  0.27651217859238386
adv train loss:  -2.838604921940714 , diff:  0.28271112497895956
adv train loss:  -2.84370100335218 , diff:  0.0050960814114660025
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  54
test acc: top1 ->  11.9 ; top5 ->  52.43  and loss:  752.0459518432617
forward train acc: top1 ->  99.89599997558594 ; top5 ->  100.0  and loss:  0.39667447766987607
test acc: top1 ->  92.07 ; top5 ->  99.14  and loss:  54.04842193424702
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.1961901574395597
test acc: top1 ->  92.04 ; top5 ->  99.17  and loss:  56.319309800863266
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.2341878544248175
test acc: top1 ->  92.12 ; top5 ->  99.24  and loss:  54.2755344286561
==> this epoch:  54 / 64
---------------- start layer  2  ---------------
adv train loss:  -895.4691044126521 , diff:  895.4691044126521
adv train loss:  -1459.3003129959106 , diff:  563.8312085832586
adv train loss:  -1476.848295211792 , diff:  17.547982215881348
adv train loss:  -1492.100028038025 , diff:  15.25173282623291
adv train loss:  -1489.7200078964233 , diff:  2.3800201416015625
adv train loss:  -1499.7687225341797 , diff:  10.048714637756348
adv train loss:  -1500.0864458084106 , diff:  0.31772327423095703
adv train loss:  -1500.5003442764282 , diff:  0.4138984680175781
adv train loss:  -1515.179588317871 , diff:  14.679244041442871
adv train loss:  -1520.7069053649902 , diff:  5.527317047119141
************ all values are small in this layer **********
layer  2  adv train finish, try to retain  99
test acc: top1 ->  21.5 ; top5 ->  66.03  and loss:  1907.435362815857
forward train acc: top1 ->  99.836 ; top5 ->  100.0  and loss:  0.4869148026336916
test acc: top1 ->  91.31 ; top5 ->  99.19  and loss:  63.39272032678127
forward train acc: top1 ->  99.834 ; top5 ->  99.998  and loss:  0.6216106016072445
test acc: top1 ->  91.34 ; top5 ->  99.2  and loss:  61.40734423696995
forward train acc: top1 ->  99.87199997558594 ; top5 ->  100.0  and loss:  0.45943256793543696
test acc: top1 ->  91.83 ; top5 ->  99.17  and loss:  59.626359812915325
forward train acc: top1 ->  99.89199997558593 ; top5 ->  100.0  and loss:  0.3532479233108461
test acc: top1 ->  91.72 ; top5 ->  99.13  and loss:  60.88336427882314
forward train acc: top1 ->  99.916 ; top5 ->  100.0  and loss:  0.27951230737380683
test acc: top1 ->  91.54 ; top5 ->  99.07  and loss:  62.25096182525158
forward train acc: top1 ->  99.91 ; top5 ->  100.0  and loss:  0.27215259429067373
test acc: top1 ->  91.72 ; top5 ->  99.23  and loss:  59.56981898099184
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.23160907987039536
test acc: top1 ->  91.92 ; top5 ->  99.23  and loss:  59.48504799976945
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.24436584989598487
test acc: top1 ->  91.73 ; top5 ->  99.2  and loss:  62.152429439127445
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.24477958577335812
test acc: top1 ->  91.83 ; top5 ->  99.26  and loss:  59.76891540363431
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -884.4508513431065 , diff:  884.4508513431065
adv train loss:  -1454.1448574066162 , diff:  569.6940060635097
adv train loss:  -1464.1750507354736 , diff:  10.030193328857422
adv train loss:  -1467.50257396698 , diff:  3.3275232315063477
adv train loss:  -1466.5731172561646 , diff:  0.9294567108154297
adv train loss:  -1473.6727294921875 , diff:  7.099612236022949
adv train loss:  -1486.5397663116455 , diff:  12.867036819458008
adv train loss:  -1507.4395027160645 , diff:  20.899736404418945
adv train loss:  -1515.9338159561157 , diff:  8.49431324005127
adv train loss:  -1518.1769819259644 , diff:  2.243165969848633
************ all values are small in this layer **********
layer  3  adv train finish, try to retain  86
test acc: top1 ->  9.88 ; top5 ->  52.42  and loss:  21999.675857543945
forward train acc: top1 ->  99.886 ; top5 ->  100.0  and loss:  0.31181506195571274
test acc: top1 ->  91.72 ; top5 ->  99.31  and loss:  59.94625888764858
forward train acc: top1 ->  99.902 ; top5 ->  100.0  and loss:  0.33218817797023803
test acc: top1 ->  91.68 ; top5 ->  99.12  and loss:  61.102032259106636
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.23495057335821912
test acc: top1 ->  91.86 ; top5 ->  99.2  and loss:  61.15918758511543
forward train acc: top1 ->  99.91199997558594 ; top5 ->  100.0  and loss:  0.31173337949439883
test acc: top1 ->  91.79 ; top5 ->  99.21  and loss:  59.96722785383463
forward train acc: top1 ->  99.916 ; top5 ->  100.0  and loss:  0.2631752108572982
test acc: top1 ->  91.93 ; top5 ->  99.26  and loss:  59.39331176131964
forward train acc: top1 ->  99.908 ; top5 ->  100.0  and loss:  0.2247650987628731
test acc: top1 ->  91.88 ; top5 ->  99.3  and loss:  59.44102120399475
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.13788602012209594
test acc: top1 ->  91.77 ; top5 ->  99.3  and loss:  60.08202749490738
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.1299794499645941
test acc: top1 ->  91.99 ; top5 ->  99.26  and loss:  60.90216326713562
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.14006620337022468
test acc: top1 ->  91.95 ; top5 ->  99.29  and loss:  60.20817365497351
forward train acc: top1 ->  99.95199997558593 ; top5 ->  100.0  and loss:  0.16273200570140034
test acc: top1 ->  92.0 ; top5 ->  99.2  and loss:  61.73681317269802
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -291.9692071704194 , diff:  291.9692071704194
adv train loss:  -1712.5134048461914 , diff:  1420.544197675772
adv train loss:  -1914.4824962615967 , diff:  201.96909141540527
adv train loss:  -1958.1161766052246 , diff:  43.63368034362793
adv train loss:  -1971.0050964355469 , diff:  12.888919830322266
adv train loss:  -1970.282211303711 , diff:  0.7228851318359375
adv train loss:  -1974.0386695861816 , diff:  3.756458282470703
adv train loss:  -1975.563247680664 , diff:  1.5245780944824219
adv train loss:  -1979.8443012237549 , diff:  4.28105354309082
adv train loss:  -1968.9747486114502 , diff:  10.869552612304688
************ all values are small in this layer **********
layer  4  adv train finish, try to retain  154
test acc: top1 ->  11.6 ; top5 ->  58.14  and loss:  16043.50220489502
forward train acc: top1 ->  98.748 ; top5 ->  99.974  and loss:  4.7889269147999585
test acc: top1 ->  90.52 ; top5 ->  99.34  and loss:  55.77790096402168
forward train acc: top1 ->  99.01999998046875 ; top5 ->  99.99399997558594  and loss:  3.0630898885428905
test acc: top1 ->  90.69 ; top5 ->  99.3  and loss:  50.3880270421505
forward train acc: top1 ->  99.10399997802735 ; top5 ->  99.992  and loss:  2.789910123217851
test acc: top1 ->  90.69 ; top5 ->  99.21  and loss:  50.02597691863775
forward train acc: top1 ->  99.2720000024414 ; top5 ->  99.98999997558593  and loss:  2.2830282635986805
test acc: top1 ->  90.97 ; top5 ->  99.31  and loss:  48.96427573263645
forward train acc: top1 ->  99.29000000488281 ; top5 ->  99.996  and loss:  2.2025674441829324
test acc: top1 ->  90.92 ; top5 ->  99.05  and loss:  50.150222793221474
forward train acc: top1 ->  99.34799997558594 ; top5 ->  99.994  and loss:  1.997015627566725
test acc: top1 ->  91.09 ; top5 ->  99.06  and loss:  48.68091695010662
forward train acc: top1 ->  99.3820000024414 ; top5 ->  100.0  and loss:  1.9026574278250337
test acc: top1 ->  91.05 ; top5 ->  99.23  and loss:  48.73820559680462
forward train acc: top1 ->  99.3920000024414 ; top5 ->  99.998  and loss:  1.7720797713845968
test acc: top1 ->  91.16 ; top5 ->  99.22  and loss:  48.88851177692413
forward train acc: top1 ->  99.5020000024414 ; top5 ->  100.0  and loss:  1.5426158281043172
test acc: top1 ->  91.17 ; top5 ->  99.28  and loss:  49.87624664604664
forward train acc: top1 ->  99.43400000488282 ; top5 ->  99.998  and loss:  1.7062336418312043
test acc: top1 ->  91.19 ; top5 ->  99.19  and loss:  49.90914985537529
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -375.46623385953717 , diff:  375.46623385953717
adv train loss:  -1607.352544784546 , diff:  1231.8863109250087
adv train loss:  -1668.7088623046875 , diff:  61.3563175201416
adv train loss:  -1675.4764003753662 , diff:  6.767538070678711
adv train loss:  -1687.9859199523926 , diff:  12.509519577026367
adv train loss:  -1701.3034439086914 , diff:  13.317523956298828
adv train loss:  -1729.907314300537 , diff:  28.603870391845703
adv train loss:  -1737.359727859497 , diff:  7.452413558959961
adv train loss:  -1760.6134777069092 , diff:  23.25374984741211
adv train loss:  -1767.788345336914 , diff:  7.174867630004883
************ all values are small in this layer **********
layer  5  adv train finish, try to retain  148
test acc: top1 ->  26.91 ; top5 ->  74.08  and loss:  2063.3022089004517
forward train acc: top1 ->  99.60400000244141 ; top5 ->  99.998  and loss:  1.173527270089835
test acc: top1 ->  91.12 ; top5 ->  99.23  and loss:  52.09458478540182
forward train acc: top1 ->  99.76399997558593 ; top5 ->  100.0  and loss:  0.6957114092074335
test acc: top1 ->  91.4 ; top5 ->  99.26  and loss:  54.296003356575966
forward train acc: top1 ->  99.788 ; top5 ->  100.0  and loss:  0.6010419421363622
test acc: top1 ->  91.59 ; top5 ->  99.17  and loss:  55.01760493218899
forward train acc: top1 ->  99.818 ; top5 ->  100.0  and loss:  0.5550371709396131
test acc: top1 ->  91.64 ; top5 ->  99.26  and loss:  54.745423540472984
forward train acc: top1 ->  99.854 ; top5 ->  99.998  and loss:  0.4354774282546714
test acc: top1 ->  91.63 ; top5 ->  99.33  and loss:  55.46114081889391
forward train acc: top1 ->  99.86599997558594 ; top5 ->  100.0  and loss:  0.3587703821249306
test acc: top1 ->  91.74 ; top5 ->  99.27  and loss:  56.518794029951096
forward train acc: top1 ->  99.862 ; top5 ->  100.0  and loss:  0.37038966119871475
test acc: top1 ->  91.71 ; top5 ->  99.33  and loss:  56.668691262602806
forward train acc: top1 ->  99.87999997558593 ; top5 ->  100.0  and loss:  0.3464477004017681
test acc: top1 ->  91.6 ; top5 ->  99.17  and loss:  58.14220079779625
forward train acc: top1 ->  99.90399997558593 ; top5 ->  100.0  and loss:  0.2928088663611561
test acc: top1 ->  91.85 ; top5 ->  99.24  and loss:  56.41918873786926
forward train acc: top1 ->  99.90799997558594 ; top5 ->  100.0  and loss:  0.2782149456907064
test acc: top1 ->  91.72 ; top5 ->  99.26  and loss:  57.57244949042797
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -299.2476091431454 , diff:  299.2476091431454
adv train loss:  -1727.0614824295044 , diff:  1427.813873286359
adv train loss:  -1906.149263381958 , diff:  179.0877809524536
adv train loss:  -1955.2405033111572 , diff:  49.09123992919922
adv train loss:  -1961.3245067596436 , diff:  6.084003448486328
adv train loss:  -1970.895866394043 , diff:  9.571359634399414
adv train loss:  -1970.9891338348389 , diff:  0.09326744079589844
adv train loss:  -1968.326644897461 , diff:  2.6624889373779297
adv train loss:  -1984.071050643921 , diff:  15.744405746459961
adv train loss:  -2002.641092300415 , diff:  18.57004165649414
************ all values are small in this layer **********
layer  6  adv train finish, try to retain  143
test acc: top1 ->  16.79 ; top5 ->  54.52  and loss:  10152.875015258789
forward train acc: top1 ->  99.906 ; top5 ->  100.0  and loss:  0.26288871734868735
test acc: top1 ->  91.94 ; top5 ->  99.32  and loss:  56.4233278632164
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.159924908017274
test acc: top1 ->  92.13 ; top5 ->  99.35  and loss:  55.24771589040756
==> this epoch:  143 / 256
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
adv train loss:  -10.796482190489769 , diff:  10.796482190489769
adv train loss:  -10.789454497396946 , diff:  0.0070276930928230286
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  161
test acc: top1 ->  12.11 ; top5 ->  67.82  and loss:  26921466.46875
forward train acc: top1 ->  99.4 ; top5 ->  100.0  and loss:  1.8149146446958184
test acc: top1 ->  92.01 ; top5 ->  99.24  and loss:  52.599892027676105
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.24454000469995663
test acc: top1 ->  92.08 ; top5 ->  99.36  and loss:  53.12739676237106
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.21133477636612952
test acc: top1 ->  92.19 ; top5 ->  99.33  and loss:  54.377681985497475
==> this epoch:  161 / 512
---------------- start layer  9  ---------------
adv train loss:  -1521.0574054718018 , diff:  1521.0574054718018
adv train loss:  -1517.6359310150146 , diff:  3.4214744567871094
adv train loss:  -1517.1596956253052 , diff:  0.47623538970947266
layer  9  adv train finish, try to retain  503
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -655.2922472953796 , diff:  655.2922472953796
adv train loss:  -654.9832706451416 , diff:  0.3089766502380371
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  10
test acc: top1 ->  17.06 ; top5 ->  59.64  and loss:  44260.73440551758
forward train acc: top1 ->  75.25800000732421 ; top5 ->  96.026  and loss:  112.7406971603632
test acc: top1 ->  83.45 ; top5 ->  98.14  and loss:  70.47555556893349
forward train acc: top1 ->  98.68199997802735 ; top5 ->  99.992  and loss:  7.957651808857918
test acc: top1 ->  89.34 ; top5 ->  98.65  and loss:  49.283637419342995
forward train acc: top1 ->  99.432 ; top5 ->  99.998  and loss:  3.304936544969678
test acc: top1 ->  90.23 ; top5 ->  98.76  and loss:  46.760881409049034
forward train acc: top1 ->  99.642 ; top5 ->  99.998  and loss:  2.17650774307549
test acc: top1 ->  90.48 ; top5 ->  98.84  and loss:  46.216380417346954
forward train acc: top1 ->  99.70999997558594 ; top5 ->  100.0  and loss:  1.5184573847800493
test acc: top1 ->  90.84 ; top5 ->  98.89  and loss:  45.75677581131458
forward train acc: top1 ->  99.82799997558594 ; top5 ->  99.998  and loss:  1.1683179512619972
test acc: top1 ->  91.04 ; top5 ->  98.89  and loss:  45.464460119605064
forward train acc: top1 ->  99.8500000024414 ; top5 ->  99.998  and loss:  1.0613656407222152
test acc: top1 ->  91.1 ; top5 ->  98.85  and loss:  45.52805304527283
forward train acc: top1 ->  99.8400000024414 ; top5 ->  100.0  and loss:  0.9424074022099376
test acc: top1 ->  91.01 ; top5 ->  98.9  and loss:  45.56904283165932
forward train acc: top1 ->  99.87999997558593 ; top5 ->  100.0  and loss:  0.800880498252809
test acc: top1 ->  91.02 ; top5 ->  98.91  and loss:  45.206216871738434
forward train acc: top1 ->  99.88 ; top5 ->  100.0  and loss:  0.7131643588654697
test acc: top1 ->  91.1 ; top5 ->  98.93  and loss:  46.2611198425293
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  11 / 512 , inc:  1
---------------- start layer  11  ---------------
### skip layer  11 wait:  4  ###
---------------- start layer  12  ---------------
adv train loss:  -1670.468903541565 , diff:  1670.468903541565
adv train loss:  -1960.713430404663 , diff:  290.24452686309814
adv train loss:  -1958.489860534668 , diff:  2.223569869995117
adv train loss:  -1962.377140045166 , diff:  3.887279510498047
adv train loss:  -1960.2258892059326 , diff:  2.1512508392333984
adv train loss:  -1961.395107269287 , diff:  1.1692180633544922
adv train loss:  -1960.2918739318848 , diff:  1.1032333374023438
adv train loss:  -1961.202127456665 , diff:  0.9102535247802734
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  55
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  369883.6472167969
forward train acc: top1 ->  98.706 ; top5 ->  100.0  and loss:  3.92303834942868
test acc: top1 ->  91.69 ; top5 ->  98.8  and loss:  76.53828340768814
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.2822696417570114
test acc: top1 ->  92.11 ; top5 ->  98.76  and loss:  74.30650843679905
==> this epoch:  55 / 512
---------------- start layer  13  ---------------
### skip layer  13 wait:  4  ###
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.84375  ==>  54 / 64 , inc:  2
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.55859375  ==>  143 / 256 , inc:  2
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.314453125  ==>  161 / 512 , inc:  32
layer  9  :  0.017578125  ==>  9 / 512 , inc:  2
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.0078125  ==>  4 / 512 , inc:  1
layer  12  :  0.107421875  ==>  55 / 512 , inc:  8
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [0.8757877807617187, 1.167717041015625, 0.8757877807617187, 0.8757877807617187, 0.8757877807617187, 0.8757877807617187, 1.167717041015625, 0.05473673629760742, 2.07594140625, 2.07594140625, 1.313681671142578, 1.5569560546875, 1.167717041015625, 12.4556484375]  wait [4, 0, 4, 4, 4, 4, 0, 3, 0, 2, 4, 3, 0, 3]  inc [1, 2, 1, 1, 1, 1, 2, 1, 32, 2, 1, 1, 8, 1]  tol: 3
$$$$$$$$$$$$$ epoch  52  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
adv train loss:  -0.6640055262687383 , diff:  0.6640055262687383
adv train loss:  -0.5838104824069887 , diff:  0.08019504386174958
adv train loss:  -0.6251776458811946 , diff:  0.04136716347420588
adv train loss:  -0.8930162682663649 , diff:  0.2678386223851703
adv train loss:  -1.0020275372080505 , diff:  0.10901126894168556
adv train loss:  -1.6272404761984944 , diff:  0.625212938990444
adv train loss:  -1.7109247271437198 , diff:  0.08368425094522536
adv train loss:  -1.6463042553514242 , diff:  0.06462047179229558
adv train loss:  -2.2848175442777574 , diff:  0.6385132889263332
adv train loss:  -1.7711462217848748 , diff:  0.5136713224928826
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  52
test acc: top1 ->  13.36 ; top5 ->  52.13  and loss:  1772.5725145339966
forward train acc: top1 ->  99.93199997558594 ; top5 ->  100.0  and loss:  0.26792292296886444
test acc: top1 ->  92.36 ; top5 ->  98.72  and loss:  72.5343346297741
==> this epoch:  52 / 64
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
adv train loss:  -709.4056308157742 , diff:  709.4056308157742
adv train loss:  -2391.748769760132 , diff:  1682.3431389443576
adv train loss:  -2451.320171356201 , diff:  59.571401596069336
adv train loss:  -2476.6488246917725 , diff:  25.32865333557129
adv train loss:  -2489.3321952819824 , diff:  12.683370590209961
adv train loss:  -2482.7944622039795 , diff:  6.53773307800293
adv train loss:  -2488.1504306793213 , diff:  5.355968475341797
adv train loss:  -2487.180591583252 , diff:  0.9698390960693359
adv train loss:  -2485.2464923858643 , diff:  1.9340991973876953
adv train loss:  -2486.9465255737305 , diff:  1.700033187866211
************ all values are small in this layer **********
layer  6  adv train finish, try to retain  141
test acc: top1 ->  10.0 ; top5 ->  49.78  and loss:  755219.4775390625
forward train acc: top1 ->  99.89600000244141 ; top5 ->  100.0  and loss:  0.2927161739207804
test acc: top1 ->  92.23 ; top5 ->  98.76  and loss:  72.25961723923683
==> this epoch:  141 / 256
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
adv train loss:  -11.75620961561799 , diff:  11.75620961561799
adv train loss:  -11.493166457861662 , diff:  0.2630431577563286
adv train loss:  -11.252711277455091 , diff:  0.24045518040657043
adv train loss:  -11.640127453953028 , diff:  0.38741617649793625
adv train loss:  -11.385243218392134 , diff:  0.254884235560894
adv train loss:  -11.373964864760637 , diff:  0.01127835363149643
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  129
test acc: top1 ->  10.85 ; top5 ->  54.93  and loss:  4550418.427734375
forward train acc: top1 ->  96.26600000488281 ; top5 ->  99.594  and loss:  24.35680063208565
test acc: top1 ->  91.32 ; top5 ->  98.84  and loss:  70.12942712008953
forward train acc: top1 ->  99.75199997558593 ; top5 ->  100.0  and loss:  0.8512429215479642
test acc: top1 ->  91.57 ; top5 ->  98.82  and loss:  68.14971399307251
forward train acc: top1 ->  99.794 ; top5 ->  100.0  and loss:  0.6752422014251351
test acc: top1 ->  91.79 ; top5 ->  98.89  and loss:  67.79603725671768
forward train acc: top1 ->  99.864 ; top5 ->  100.0  and loss:  0.41758050315547734
test acc: top1 ->  91.87 ; top5 ->  98.88  and loss:  67.33415254577994
forward train acc: top1 ->  99.878 ; top5 ->  100.0  and loss:  0.37956193392165005
test acc: top1 ->  91.94 ; top5 ->  98.85  and loss:  66.6705909371376
forward train acc: top1 ->  99.878 ; top5 ->  100.0  and loss:  0.336685007205233
test acc: top1 ->  91.89 ; top5 ->  98.93  and loss:  66.60391350090504
forward train acc: top1 ->  99.9120000024414 ; top5 ->  100.0  and loss:  0.3083896229509264
test acc: top1 ->  91.88 ; top5 ->  98.85  and loss:  67.12081327289343
forward train acc: top1 ->  99.91 ; top5 ->  100.0  and loss:  0.272603910183534
test acc: top1 ->  91.86 ; top5 ->  98.9  and loss:  67.43023397400975
forward train acc: top1 ->  99.918 ; top5 ->  100.0  and loss:  0.30837690259795636
test acc: top1 ->  91.96 ; top5 ->  98.87  and loss:  66.60964781790972
forward train acc: top1 ->  99.918 ; top5 ->  100.0  and loss:  0.25664469168987125
test acc: top1 ->  91.97 ; top5 ->  98.92  and loss:  66.60601994395256
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  161 / 512 , inc:  32
---------------- start layer  9  ---------------
adv train loss:  -1371.7010278701782 , diff:  1371.7010278701782
adv train loss:  -1376.466950416565 , diff:  4.765922546386719
adv train loss:  -1385.4118041992188 , diff:  8.944853782653809
adv train loss:  -1381.2751874923706 , diff:  4.1366167068481445
adv train loss:  -1383.4220848083496 , diff:  2.146897315979004
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  7
test acc: top1 ->  8.55 ; top5 ->  45.45  and loss:  3370.6121559143066
forward train acc: top1 ->  98.844 ; top5 ->  99.992  and loss:  3.50187826983165
test acc: top1 ->  90.99 ; top5 ->  98.08  and loss:  67.45779724419117
forward train acc: top1 ->  99.856 ; top5 ->  100.0  and loss:  0.4786844120826572
test acc: top1 ->  91.76 ; top5 ->  98.64  and loss:  62.84923121333122
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.29702636000001803
test acc: top1 ->  91.94 ; top5 ->  98.72  and loss:  64.07729726657271
forward train acc: top1 ->  99.912 ; top5 ->  100.0  and loss:  0.3127879613894038
test acc: top1 ->  91.91 ; top5 ->  98.72  and loss:  64.98940566927195
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.22525450252578594
test acc: top1 ->  91.94 ; top5 ->  98.74  and loss:  64.75510677322745
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.15392404270824045
test acc: top1 ->  91.92 ; top5 ->  98.8  and loss:  64.87939243763685
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.15505236195167527
test acc: top1 ->  92.02 ; top5 ->  98.75  and loss:  65.81045565754175
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.17565779061987996
test acc: top1 ->  91.91 ; top5 ->  98.83  and loss:  66.33676867932081
forward train acc: top1 ->  99.96199997558594 ; top5 ->  100.0  and loss:  0.12829186324961483
test acc: top1 ->  92.02 ; top5 ->  98.79  and loss:  66.33618583157659
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.10562218910490628
test acc: top1 ->  92.06 ; top5 ->  98.86  and loss:  67.06469052284956
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  9 / 512 , inc:  2
---------------- start layer  10  ---------------
### skip layer  10 wait:  4  ###
---------------- start layer  11  ---------------
### skip layer  11 wait:  3  ###
---------------- start layer  12  ---------------
adv train loss:  -1871.5639190673828 , diff:  1871.5639190673828
adv train loss:  -2058.4203300476074 , diff:  186.8564109802246
adv train loss:  -2058.50341796875 , diff:  0.08308792114257812
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  47
test acc: top1 ->  10.3 ; top5 ->  68.89  and loss:  31399.2138671875
forward train acc: top1 ->  99.85199997558594 ; top5 ->  100.0  and loss:  0.45988589015905745
test acc: top1 ->  91.95 ; top5 ->  98.94  and loss:  72.72617814689875
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.11719510967668612
test acc: top1 ->  92.16 ; top5 ->  98.96  and loss:  72.45430770516396
==> this epoch:  47 / 512
---------------- start layer  13  ---------------
### skip layer  13 wait:  3  ###
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.8125  ==>  52 / 64 , inc:  4
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.55078125  ==>  141 / 256 , inc:  4
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.314453125  ==>  161 / 512 , inc:  16
layer  9  :  0.017578125  ==>  9 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.0078125  ==>  4 / 512 , inc:  1
layer  12  :  0.091796875  ==>  47 / 512 , inc:  16
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [0.8757877807617187, 1.167717041015625, 0.8757877807617187, 0.8757877807617187, 0.8757877807617187, 0.8757877807617187, 1.167717041015625, 0.05473673629760742, 1.5569560546875, 1.5569560546875, 1.313681671142578, 1.5569560546875, 1.167717041015625, 12.4556484375]  wait [3, 0, 3, 3, 3, 3, 0, 2, 2, 4, 3, 2, 0, 2]  inc [1, 4, 1, 1, 1, 1, 4, 1, 16, 1, 1, 1, 16, 1]  tol: 3
$$$$$$$$$$$$$ epoch  53  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
adv train loss:  -0.8165065539069474 , diff:  0.8165065539069474
adv train loss:  -1.4825596758164465 , diff:  0.6660531219094992
adv train loss:  -1.6136333881877363 , diff:  0.13107371237128973
adv train loss:  -1.7407086903695017 , diff:  0.12707530218176544
adv train loss:  -1.2449167954619043 , diff:  0.49579189490759745
adv train loss:  -1.6090074686799198 , diff:  0.3640906732180156
adv train loss:  -1.4584455100120977 , diff:  0.15056195866782218
adv train loss:  -1.4818049327877816 , diff:  0.023359422775683925
adv train loss:  -1.4712475184351206 , diff:  0.010557414352660999
adv train loss:  -1.5394755627494305 , diff:  0.06822804431430995
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  48
test acc: top1 ->  11.64 ; top5 ->  50.72  and loss:  1777.335069656372
forward train acc: top1 ->  99.862 ; top5 ->  100.0  and loss:  0.43717742591979913
test acc: top1 ->  91.63 ; top5 ->  98.9  and loss:  77.45593716576695
forward train acc: top1 ->  99.912 ; top5 ->  100.0  and loss:  0.2966640082813683
test acc: top1 ->  91.68 ; top5 ->  99.18  and loss:  75.75629027187824
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.26344621842872584
test acc: top1 ->  91.79 ; top5 ->  99.07  and loss:  74.37469087913632
forward train acc: top1 ->  99.9 ; top5 ->  100.0  and loss:  0.356398872234422
test acc: top1 ->  91.9 ; top5 ->  98.94  and loss:  71.93606626987457
forward train acc: top1 ->  99.924 ; top5 ->  99.998  and loss:  0.26358754048123956
test acc: top1 ->  92.04 ; top5 ->  99.0  and loss:  71.00177778303623
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.2228645696741296
test acc: top1 ->  92.16 ; top5 ->  99.1  and loss:  68.42893220484257
==> this epoch:  48 / 64
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
adv train loss:  -644.3980544020887 , diff:  644.3980544020887
adv train loss:  -2281.474599838257 , diff:  1637.076545436168
adv train loss:  -2317.209030151367 , diff:  35.73443031311035
adv train loss:  -2321.6856727600098 , diff:  4.476642608642578
adv train loss:  -2333.1004390716553 , diff:  11.414766311645508
adv train loss:  -2334.879135131836 , diff:  1.778696060180664
adv train loss:  -2331.6097927093506 , diff:  3.2693424224853516
adv train loss:  -2330.817449569702 , diff:  0.7923431396484375
adv train loss:  -2325.728096008301 , diff:  5.089353561401367
adv train loss:  -2328.31245803833 , diff:  2.584362030029297
************ all values are small in this layer **********
layer  6  adv train finish, try to retain  137
test acc: top1 ->  9.95 ; top5 ->  50.66  and loss:  337349.47509765625
forward train acc: top1 ->  99.882 ; top5 ->  100.0  and loss:  0.38481313154261443
test acc: top1 ->  91.85 ; top5 ->  99.16  and loss:  70.19521384686232
forward train acc: top1 ->  99.87 ; top5 ->  99.998  and loss:  0.3919081308413297
test acc: top1 ->  91.58 ; top5 ->  99.13  and loss:  69.85140408575535
forward train acc: top1 ->  99.886 ; top5 ->  100.0  and loss:  0.3541633727727458
test acc: top1 ->  91.85 ; top5 ->  99.25  and loss:  67.5392064973712
forward train acc: top1 ->  99.916 ; top5 ->  100.0  and loss:  0.25807268725475296
test acc: top1 ->  91.83 ; top5 ->  99.15  and loss:  67.96135659515858
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.21513657469768077
test acc: top1 ->  91.85 ; top5 ->  99.15  and loss:  68.37807617336512
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.23365380070754327
test acc: top1 ->  91.88 ; top5 ->  99.16  and loss:  66.71233811974525
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.17414457030099584
test acc: top1 ->  91.95 ; top5 ->  99.23  and loss:  67.20135025680065
forward train acc: top1 ->  99.93599997558594 ; top5 ->  100.0  and loss:  0.18336750863818452
test acc: top1 ->  91.98 ; top5 ->  99.26  and loss:  69.35451632738113
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.17145063914358616
test acc: top1 ->  91.94 ; top5 ->  99.22  and loss:  68.1921367123723
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.18971569021232426
test acc: top1 ->  91.88 ; top5 ->  99.22  and loss:  68.52260599285364
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  141 / 256 , inc:  4
---------------- start layer  7  ---------------
adv train loss:  -0.2619771244353615 , diff:  0.2619771244353615
adv train loss:  -0.20988346898229793 , diff:  0.05209365545306355
adv train loss:  -0.20760626276023686 , diff:  0.002277206222061068
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  252
test acc: top1 ->  54.03 ; top5 ->  83.78  and loss:  280.18872797489166
forward train acc: top1 ->  99.558 ; top5 ->  99.998  and loss:  1.3108611392090097
test acc: top1 ->  90.97 ; top5 ->  99.19  and loss:  66.19766902923584
forward train acc: top1 ->  99.71799997558594 ; top5 ->  100.0  and loss:  0.8061070265248418
test acc: top1 ->  91.17 ; top5 ->  99.3  and loss:  65.26768684387207
forward train acc: top1 ->  99.76399997558593 ; top5 ->  100.0  and loss:  0.6572934833820909
test acc: top1 ->  91.25 ; top5 ->  99.21  and loss:  64.30776171386242
forward train acc: top1 ->  99.818 ; top5 ->  100.0  and loss:  0.589195333654061
test acc: top1 ->  91.53 ; top5 ->  99.27  and loss:  63.68192322552204
forward train acc: top1 ->  99.80999997558594 ; top5 ->  100.0  and loss:  0.5119250155694317
test acc: top1 ->  91.48 ; top5 ->  99.26  and loss:  65.16020598262548
forward train acc: top1 ->  99.828 ; top5 ->  99.998  and loss:  0.4961646248702891
test acc: top1 ->  91.45 ; top5 ->  99.34  and loss:  65.34760715812445
forward train acc: top1 ->  99.82999997558593 ; top5 ->  100.0  and loss:  0.4796870582504198
test acc: top1 ->  91.35 ; top5 ->  99.29  and loss:  64.93143332004547
forward train acc: top1 ->  99.856 ; top5 ->  100.0  and loss:  0.49238330812659115
test acc: top1 ->  91.62 ; top5 ->  99.29  and loss:  63.217977821826935
forward train acc: top1 ->  99.844 ; top5 ->  99.998  and loss:  0.44301553536206484
test acc: top1 ->  91.54 ; top5 ->  99.3  and loss:  64.14900655299425
forward train acc: top1 ->  99.864 ; top5 ->  100.0  and loss:  0.4335032369126566
test acc: top1 ->  91.41 ; top5 ->  99.3  and loss:  63.196846932172775
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  253 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -5.1614103047177196 , diff:  5.1614103047177196
adv train loss:  -5.3660988211631775 , diff:  0.20468851644545794
adv train loss:  -5.527852733619511 , diff:  0.16175391245633364
adv train loss:  -5.585553205572069 , diff:  0.057700471952557564
adv train loss:  -5.447374250739813 , diff:  0.13817895483225584
adv train loss:  -5.324289451353252 , diff:  0.12308479938656092
adv train loss:  -5.298975831829011 , diff:  0.025313619524240494
adv train loss:  -5.233262397348881 , diff:  0.06571343448013067
adv train loss:  -5.26201930269599 , diff:  0.02875690534710884
adv train loss:  -5.014243757352233 , diff:  0.24777554534375668
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  145
test acc: top1 ->  17.19 ; top5 ->  51.32  and loss:  6948759.6875
forward train acc: top1 ->  99.5400000024414 ; top5 ->  100.0  and loss:  1.5006312711630017
test acc: top1 ->  91.83 ; top5 ->  99.23  and loss:  62.60436328500509
forward train acc: top1 ->  99.894 ; top5 ->  100.0  and loss:  0.3582171468879096
test acc: top1 ->  91.8 ; top5 ->  99.13  and loss:  64.42215248942375
forward train acc: top1 ->  99.916 ; top5 ->  100.0  and loss:  0.25805557635612786
test acc: top1 ->  91.89 ; top5 ->  99.22  and loss:  64.58475720137358
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.18740818781225244
test acc: top1 ->  92.04 ; top5 ->  99.11  and loss:  64.20427211374044
forward train acc: top1 ->  99.94 ; top5 ->  100.0  and loss:  0.16613838799821679
test acc: top1 ->  91.98 ; top5 ->  99.17  and loss:  66.53936694562435
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.20174464301089756
test acc: top1 ->  92.09 ; top5 ->  99.21  and loss:  65.4961621016264
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.157415852198028
test acc: top1 ->  92.03 ; top5 ->  99.21  and loss:  66.96200077980757
forward train acc: top1 ->  99.93599997558594 ; top5 ->  100.0  and loss:  0.20131312072044238
test acc: top1 ->  91.88 ; top5 ->  99.17  and loss:  67.68728122860193
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.1696350505844748
test acc: top1 ->  91.92 ; top5 ->  99.19  and loss:  67.92211720347404
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.1428177069319645
test acc: top1 ->  92.11 ; top5 ->  99.25  and loss:  66.83799623697996
==> this epoch:  145 / 512
---------------- start layer  9  ---------------
### skip layer  9 wait:  4  ###
---------------- start layer  10  ---------------
### skip layer  10 wait:  3  ###
---------------- start layer  11  ---------------
adv train loss:  -2133.1373958587646 , diff:  2133.1373958587646
adv train loss:  -2471.2147483825684 , diff:  338.0773525238037
adv train loss:  -2447.727331161499 , diff:  23.487417221069336
adv train loss:  -2361.7287788391113 , diff:  85.9985523223877
adv train loss:  -2438.9859504699707 , diff:  77.25717163085938
adv train loss:  -2595.2936305999756 , diff:  156.30768013000488
adv train loss:  -2708.922166824341 , diff:  113.62853622436523
adv train loss:  -2730.487886428833 , diff:  21.565719604492188
adv train loss:  -2731.3858013153076 , diff:  0.8979148864746094
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  3
test acc: top1 ->  10.0 ; top5 ->  59.62  and loss:  21154.02407836914
forward train acc: top1 ->  93.484 ; top5 ->  99.974  and loss:  23.061619432643056
test acc: top1 ->  91.01 ; top5 ->  99.04  and loss:  41.16958840936422
forward train acc: top1 ->  99.79 ; top5 ->  99.998  and loss:  1.5464664213359356
test acc: top1 ->  91.23 ; top5 ->  99.1  and loss:  41.04390973597765
forward train acc: top1 ->  99.84599997558594 ; top5 ->  100.0  and loss:  0.9425464635714889
test acc: top1 ->  91.34 ; top5 ->  99.16  and loss:  42.69297034293413
forward train acc: top1 ->  99.87999997558593 ; top5 ->  100.0  and loss:  0.6888386253267527
test acc: top1 ->  91.61 ; top5 ->  99.21  and loss:  44.14320866018534
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.48909141262993217
test acc: top1 ->  91.64 ; top5 ->  99.2  and loss:  45.11198980733752
forward train acc: top1 ->  99.92799997558593 ; top5 ->  100.0  and loss:  0.444980563595891
test acc: top1 ->  91.75 ; top5 ->  99.13  and loss:  45.89974094927311
forward train acc: top1 ->  99.916 ; top5 ->  100.0  and loss:  0.4323075960855931
test acc: top1 ->  91.72 ; top5 ->  99.18  and loss:  46.63154622539878
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.3634778717532754
test acc: top1 ->  91.72 ; top5 ->  99.18  and loss:  46.85621163249016
forward train acc: top1 ->  99.93399997558593 ; top5 ->  100.0  and loss:  0.35259237606078386
test acc: top1 ->  91.69 ; top5 ->  99.26  and loss:  47.89633642882109
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.32116245885845274
test acc: top1 ->  91.78 ; top5 ->  99.2  and loss:  48.16889958828688
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  4 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -2125.979387283325 , diff:  2125.979387283325
adv train loss:  -2125.7014961242676 , diff:  0.2778911590576172
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  31
test acc: top1 ->  19.69 ; top5 ->  68.67  and loss:  33153.15408325195
forward train acc: top1 ->  95.70199997558593 ; top5 ->  99.872  and loss:  18.422176904510707
test acc: top1 ->  91.09 ; top5 ->  98.99  and loss:  63.95905178785324
forward train acc: top1 ->  99.86 ; top5 ->  100.0  and loss:  0.5900509140919894
test acc: top1 ->  91.47 ; top5 ->  99.1  and loss:  60.88751634955406
forward train acc: top1 ->  99.93799997558594 ; top5 ->  100.0  and loss:  0.28184349136427045
test acc: top1 ->  91.55 ; top5 ->  99.06  and loss:  60.42565521597862
forward train acc: top1 ->  99.9300000024414 ; top5 ->  100.0  and loss:  0.2666555040050298
test acc: top1 ->  91.77 ; top5 ->  99.05  and loss:  60.37037394940853
forward train acc: top1 ->  99.94 ; top5 ->  100.0  and loss:  0.24005250335903838
test acc: top1 ->  91.7 ; top5 ->  99.07  and loss:  59.7464155331254
forward train acc: top1 ->  99.94 ; top5 ->  100.0  and loss:  0.22127496264874935
test acc: top1 ->  91.69 ; top5 ->  99.09  and loss:  59.41486311331391
forward train acc: top1 ->  99.96199997558594 ; top5 ->  100.0  and loss:  0.17268840968608856
test acc: top1 ->  91.73 ; top5 ->  99.11  and loss:  59.77951059862971
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.15147752547636628
test acc: top1 ->  91.8 ; top5 ->  99.09  and loss:  60.88850670680404
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.17247793782735243
test acc: top1 ->  91.77 ; top5 ->  99.08  and loss:  60.58761578053236
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.17890131956664845
test acc: top1 ->  91.76 ; top5 ->  99.07  and loss:  60.76397171989083
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  47 / 512 , inc:  16
---------------- start layer  13  ---------------
adv train loss:  -17628.10856628418 , diff:  17628.10856628418
adv train loss:  -28559.16537475586 , diff:  10931.05680847168
adv train loss:  -39384.18005371094 , diff:  10825.014678955078
adv train loss:  -50174.186431884766 , diff:  10790.006378173828
adv train loss:  -60940.878845214844 , diff:  10766.692413330078
adv train loss:  -71734.57763671875 , diff:  10793.698791503906
adv train loss:  -82535.34130859375 , diff:  10800.763671875
adv train loss:  -93313.83874511719 , diff:  10778.497436523438
adv train loss:  -104112.6870727539 , diff:  10798.848327636719
adv train loss:  -114908.99304199219 , diff:  10796.305969238281
layer  13  adv train finish, try to retain  5
test acc: top1 ->  10.0 ; top5 ->  57.63  and loss:  2663.1885147094727
forward train acc: top1 ->  14.235999995117188 ; top5 ->  73.28399997314453  and loss:  1338.2983121871948
test acc: top1 ->  22.88 ; top5 ->  78.03  and loss:  618.337188243866
forward train acc: top1 ->  34.0899999987793 ; top5 ->  86.57600000732423  and loss:  306.85562229156494
test acc: top1 ->  59.06 ; top5 ->  87.8  and loss:  156.540860414505
forward train acc: top1 ->  76.26799999023437 ; top5 ->  96.898  and loss:  86.88250476121902
test acc: top1 ->  76.22 ; top5 ->  97.08  and loss:  101.13095837831497
forward train acc: top1 ->  85.91400000488281 ; top5 ->  99.978  and loss:  61.54848659038544
test acc: top1 ->  78.25 ; top5 ->  97.26  and loss:  92.24719083309174
forward train acc: top1 ->  90.3820000024414 ; top5 ->  99.96199997558594  and loss:  52.13136348128319
test acc: top1 ->  84.0 ; top5 ->  97.34  and loss:  85.82643592357635
forward train acc: top1 ->  95.64600001708985 ; top5 ->  99.982  and loss:  45.65503427386284
test acc: top1 ->  84.24 ; top5 ->  97.28  and loss:  82.12000840902328
forward train acc: top1 ->  97.09000000976563 ; top5 ->  99.986  and loss:  41.01995089650154
test acc: top1 ->  86.22 ; top5 ->  97.38  and loss:  78.453273832798
forward train acc: top1 ->  98.07399998046876 ; top5 ->  99.992  and loss:  36.54073986411095
test acc: top1 ->  86.88 ; top5 ->  97.41  and loss:  75.13055318593979
forward train acc: top1 ->  98.37799998291015 ; top5 ->  99.988  and loss:  32.44667890667915
test acc: top1 ->  87.3 ; top5 ->  97.44  and loss:  72.18692138791084
forward train acc: top1 ->  98.6300000024414 ; top5 ->  99.984  and loss:  28.535324186086655
test acc: top1 ->  87.53 ; top5 ->  97.59  and loss:  69.6840657889843
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  9 / 512 , inc:  1
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.75  ==>  48 / 64 , inc:  8
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.55078125  ==>  141 / 256 , inc:  2
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.283203125  ==>  145 / 512 , inc:  32
layer  9  :  0.017578125  ==>  9 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.0078125  ==>  4 / 512 , inc:  1
layer  12  :  0.091796875  ==>  47 / 512 , inc:  8
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [0.8757877807617187, 1.167717041015625, 0.8757877807617187, 0.8757877807617187, 0.8757877807617187, 0.8757877807617187, 0.8757877807617187, 0.041052552223205564, 1.5569560546875, 1.5569560546875, 1.313681671142578, 1.167717041015625, 0.8757877807617187, 9.341736328125]  wait [2, 0, 2, 2, 2, 2, 2, 4, 0, 3, 2, 4, 2, 4]  inc [1, 8, 1, 1, 1, 1, 2, 1, 32, 1, 1, 1, 8, 1]  tol: 3
$$$$$$$$$$$$$ epoch  54  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1535.0097913742065 , diff:  1535.0097913742065
adv train loss:  -1533.662621498108 , diff:  1.3471698760986328
layer  0  adv train finish, try to retain  63
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -1541.163387298584 , diff:  1541.163387298584
adv train loss:  -1542.2750797271729 , diff:  1.1116924285888672
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  40
test acc: top1 ->  10.63 ; top5 ->  58.02  and loss:  2951.597017288208
forward train acc: top1 ->  86.47199998291016 ; top5 ->  97.926  and loss:  157.19846951588988
test acc: top1 ->  89.18 ; top5 ->  98.05  and loss:  82.94380855560303
forward train acc: top1 ->  98.85399998046876 ; top5 ->  99.976  and loss:  4.298479288816452
test acc: top1 ->  89.88 ; top5 ->  98.27  and loss:  78.12235389649868
forward train acc: top1 ->  99.20399998046875 ; top5 ->  99.984  and loss:  2.9641621536575258
test acc: top1 ->  90.12 ; top5 ->  98.4  and loss:  76.79234555363655
forward train acc: top1 ->  99.33799997558594 ; top5 ->  99.996  and loss:  2.350935420487076
test acc: top1 ->  90.36 ; top5 ->  98.55  and loss:  75.85399228334427
forward train acc: top1 ->  99.41800000488281 ; top5 ->  99.996  and loss:  1.9759693620726466
test acc: top1 ->  90.5 ; top5 ->  98.62  and loss:  75.65485841780901
forward train acc: top1 ->  99.52799997558594 ; top5 ->  99.996  and loss:  1.5448297972325236
test acc: top1 ->  90.6 ; top5 ->  98.62  and loss:  74.59275411814451
forward train acc: top1 ->  99.53799997558593 ; top5 ->  99.994  and loss:  1.6846012249588966
test acc: top1 ->  90.65 ; top5 ->  98.6  and loss:  74.20571622252464
forward train acc: top1 ->  99.59200000244141 ; top5 ->  99.994  and loss:  1.5314152245409787
test acc: top1 ->  90.66 ; top5 ->  98.62  and loss:  74.31101474910975
forward train acc: top1 ->  99.60999997802735 ; top5 ->  99.996  and loss:  1.3061684118583798
test acc: top1 ->  90.69 ; top5 ->  98.64  and loss:  74.35262336581945
forward train acc: top1 ->  99.62199997802735 ; top5 ->  99.998  and loss:  1.3101425205823034
test acc: top1 ->  90.59 ; top5 ->  98.6  and loss:  74.96969835460186
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  48 / 64 , inc:  8
---------------- start layer  2  ---------------
adv train loss:  -0.48967990395613015 , diff:  0.48967990395613015
adv train loss:  -0.4176394953392446 , diff:  0.07204040861688554
adv train loss:  -0.4067739404272288 , diff:  0.010865554912015796
adv train loss:  -0.4696718908380717 , diff:  0.0628979504108429
adv train loss:  -0.4230971969664097 , diff:  0.04657469387166202
adv train loss:  -0.42416288214735687 , diff:  0.0010656851809471846
layer  2  adv train finish, try to retain  125
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -0.4113560183905065 , diff:  0.4113560183905065
adv train loss:  -0.43281525489874184 , diff:  0.021459236508235335
adv train loss:  -0.5152577315457165 , diff:  0.08244247664697468
adv train loss:  -0.4414110388606787 , diff:  0.07384669268503785
adv train loss:  -0.4089817004278302 , diff:  0.032429338432848454
adv train loss:  -0.42274000821635127 , diff:  0.013758307788521051
adv train loss:  -0.46466824802337214 , diff:  0.04192823980702087
adv train loss:  -0.468528397846967 , diff:  0.0038601498235948384
layer  3  adv train finish, try to retain  127
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -0.47677529882639647 , diff:  0.47677529882639647
adv train loss:  -0.5572482398711145 , diff:  0.08047294104471803
adv train loss:  -0.36063445662148297 , diff:  0.19661378324963152
adv train loss:  -0.48036046989727765 , diff:  0.11972601327579468
adv train loss:  -0.3837468409910798 , diff:  0.09661362890619785
adv train loss:  -0.5241141808219254 , diff:  0.1403673398308456
adv train loss:  -0.41004591376986355 , diff:  0.11406826705206186
adv train loss:  -0.41649619420059025 , diff:  0.006450280430726707
layer  4  adv train finish, try to retain  249
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -0.449713462498039 , diff:  0.449713462498039
adv train loss:  -0.4435156864929013 , diff:  0.006197776005137712
layer  5  adv train finish, try to retain  248
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -0.5856362041085958 , diff:  0.5856362041085958
adv train loss:  -0.6498582332860678 , diff:  0.064222029177472
adv train loss:  -0.5463934927247465 , diff:  0.10346474056132138
adv train loss:  -0.5801567288581282 , diff:  0.033763236133381724
adv train loss:  -0.5561079878825694 , diff:  0.024048740975558758
adv train loss:  -0.44841086119413376 , diff:  0.10769712668843567
adv train loss:  -0.5701023773290217 , diff:  0.12169151613488793
adv train loss:  -0.48951984476298094 , diff:  0.08058253256604075
adv train loss:  -0.6292536505497992 , diff:  0.13973380578681827
adv train loss:  -0.5057234838604927 , diff:  0.1235301666893065
layer  6  adv train finish, try to retain  235
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
adv train loss:  -212.23848861455917 , diff:  212.23848861455917
adv train loss:  -425.4962477684021 , diff:  213.25775915384293
adv train loss:  -536.2101259231567 , diff:  110.71387815475464
adv train loss:  -570.1035709381104 , diff:  33.89344501495361
adv train loss:  -940.7176718711853 , diff:  370.61410093307495
adv train loss:  -1144.3010005950928 , diff:  203.58332872390747
adv train loss:  -1285.367597579956 , diff:  141.06659698486328
adv train loss:  -1543.580213546753 , diff:  258.2126159667969
adv train loss:  -1737.0336437225342 , diff:  193.45343017578125
adv train loss:  -1908.062204360962 , diff:  171.02856063842773
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  113
test acc: top1 ->  10.0 ; top5 ->  50.18  and loss:  89596063.0
forward train acc: top1 ->  99.862 ; top5 ->  100.0  and loss:  0.4558022387791425
test acc: top1 ->  91.9 ; top5 ->  98.95  and loss:  76.49336898326874
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.2540270686367876
test acc: top1 ->  91.88 ; top5 ->  98.94  and loss:  78.30816171318293
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.10361906459002057
test acc: top1 ->  91.87 ; top5 ->  98.99  and loss:  79.780248593539
forward train acc: top1 ->  99.942 ; top5 ->  100.0  and loss:  0.14576049913011957
test acc: top1 ->  91.89 ; top5 ->  98.94  and loss:  79.0638881623745
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.17788326619483996
test acc: top1 ->  91.92 ; top5 ->  99.08  and loss:  76.85084150731564
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.11820732908381615
test acc: top1 ->  91.86 ; top5 ->  98.96  and loss:  79.20511331222951
forward train acc: top1 ->  99.968 ; top5 ->  99.998  and loss:  0.08790011063683778
test acc: top1 ->  91.94 ; top5 ->  98.99  and loss:  78.97698391601443
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.1371483773982618
test acc: top1 ->  91.89 ; top5 ->  99.05  and loss:  80.42540925741196
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.11253068584483117
test acc: top1 ->  92.05 ; top5 ->  99.06  and loss:  79.55342956632376
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.09136368979307008
test acc: top1 ->  92.01 ; top5 ->  98.94  and loss:  80.46841553226113
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  145 / 512 , inc:  32
---------------- start layer  9  ---------------
### skip layer  9 wait:  3  ###
---------------- start layer  10  ---------------
adv train loss:  -935.0719289779663 , diff:  935.0719289779663
adv train loss:  -935.1418180465698 , diff:  0.06988906860351562
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  10
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  150886.47045898438
forward train acc: top1 ->  81.45400001708984 ; top5 ->  98.388  and loss:  61.41869255900383
test acc: top1 ->  82.06 ; top5 ->  96.94  and loss:  77.12217864394188
forward train acc: top1 ->  98.736 ; top5 ->  99.996  and loss:  6.5038610845804214
test acc: top1 ->  90.11 ; top5 ->  98.27  and loss:  59.42123830318451
forward train acc: top1 ->  99.47999997558594 ; top5 ->  99.998  and loss:  2.269403537735343
test acc: top1 ->  90.53 ; top5 ->  98.31  and loss:  60.31252866983414
forward train acc: top1 ->  99.754 ; top5 ->  100.0  and loss:  1.182762694079429
test acc: top1 ->  90.8 ; top5 ->  98.42  and loss:  62.582975938916206
forward train acc: top1 ->  99.76599997558594 ; top5 ->  100.0  and loss:  0.9643534650094807
test acc: top1 ->  90.84 ; top5 ->  98.46  and loss:  63.108969271183014
forward train acc: top1 ->  99.842 ; top5 ->  99.998  and loss:  0.7077468619681895
test acc: top1 ->  91.01 ; top5 ->  98.52  and loss:  62.803511954844
forward train acc: top1 ->  99.836 ; top5 ->  100.0  and loss:  0.6776733323931694
test acc: top1 ->  91.13 ; top5 ->  98.48  and loss:  64.0720869153738
forward train acc: top1 ->  99.8920000024414 ; top5 ->  100.0  and loss:  0.5180361024104059
test acc: top1 ->  91.07 ; top5 ->  98.61  and loss:  64.0318356975913
forward train acc: top1 ->  99.8680000024414 ; top5 ->  99.998  and loss:  0.5507314102724195
test acc: top1 ->  91.14 ; top5 ->  98.66  and loss:  64.06773962080479
forward train acc: top1 ->  99.88799997558594 ; top5 ->  100.0  and loss:  0.4579413956962526
test acc: top1 ->  90.98 ; top5 ->  98.55  and loss:  66.16258189827204
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  11 / 512 , inc:  1
---------------- start layer  11  ---------------
### skip layer  11 wait:  4  ###
---------------- start layer  12  ---------------
adv train loss:  -768.9213981628418 , diff:  768.9213981628418
adv train loss:  -769.3031272888184 , diff:  0.3817291259765625
layer  12  adv train finish, try to retain  468
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
### skip layer  13 wait:  4  ###
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.75  ==>  48 / 64 , inc:  4
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.55078125  ==>  141 / 256 , inc:  2
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.283203125  ==>  145 / 512 , inc:  16
layer  9  :  0.017578125  ==>  9 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.0078125  ==>  4 / 512 , inc:  1
layer  12  :  0.091796875  ==>  47 / 512 , inc:  8
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [1.7515755615234374, 0.8757877807617187, 1.7515755615234374, 1.7515755615234374, 1.7515755615234374, 1.7515755615234374, 1.7515755615234374, 0.041052552223205564, 1.167717041015625, 1.5569560546875, 0.9852612533569336, 1.167717041015625, 1.7515755615234374, 9.341736328125]  wait [2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 4, 3, 2, 3]  inc [1, 4, 1, 1, 1, 1, 2, 1, 16, 1, 1, 1, 8, 1]  tol: 3
$$$$$$$$$$$$$ epoch  55  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1717.0783977508545 , diff:  1717.0783977508545
adv train loss:  -1748.2736234664917 , diff:  31.195225715637207
adv train loss:  -1748.300043106079 , diff:  0.026419639587402344
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  31
test acc: top1 ->  10.0 ; top5 ->  49.98  and loss:  30451.46417236328
forward train acc: top1 ->  97.35399997802735 ; top5 ->  99.964  and loss:  12.853149134549312
test acc: top1 ->  90.5 ; top5 ->  98.9  and loss:  85.52586855739355
forward train acc: top1 ->  99.4800000024414 ; top5 ->  99.996  and loss:  2.01320406422019
test acc: top1 ->  90.96 ; top5 ->  98.93  and loss:  81.53509572148323
forward train acc: top1 ->  99.58200000488281 ; top5 ->  99.998  and loss:  1.4826060735504143
test acc: top1 ->  91.06 ; top5 ->  98.86  and loss:  80.81363993138075
forward train acc: top1 ->  99.65999997802734 ; top5 ->  100.0  and loss:  1.0644282950088382
test acc: top1 ->  91.14 ; top5 ->  98.97  and loss:  77.44864085316658
forward train acc: top1 ->  99.67200000244141 ; top5 ->  99.994  and loss:  1.176708668412175
test acc: top1 ->  91.34 ; top5 ->  99.0  and loss:  75.54975393414497
forward train acc: top1 ->  99.74199997802734 ; top5 ->  100.0  and loss:  0.8288029328687117
test acc: top1 ->  91.2 ; top5 ->  99.04  and loss:  74.92301161587238
forward train acc: top1 ->  99.6920000024414 ; top5 ->  100.0  and loss:  0.9251800086349249
test acc: top1 ->  91.26 ; top5 ->  99.06  and loss:  75.41031354665756
forward train acc: top1 ->  99.74799997558594 ; top5 ->  99.998  and loss:  0.8431318550137803
test acc: top1 ->  91.23 ; top5 ->  99.1  and loss:  74.08862026035786
forward train acc: top1 ->  99.73999997558593 ; top5 ->  99.998  and loss:  0.8371336003183387
test acc: top1 ->  91.46 ; top5 ->  99.02  and loss:  72.59278348833323
forward train acc: top1 ->  99.7760000024414 ; top5 ->  100.0  and loss:  0.6914707278483547
test acc: top1 ->  91.33 ; top5 ->  99.02  and loss:  72.73109119385481
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -0.17845126838074066 , diff:  0.17845126838074066
adv train loss:  -0.17687354543886613 , diff:  0.0015777229418745264
layer  1  adv train finish, try to retain  62
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -1108.3447985014645 , diff:  1108.3447985014645
adv train loss:  -1780.126480102539 , diff:  671.7816816010745
adv train loss:  -1815.535120010376 , diff:  35.408639907836914
adv train loss:  -1825.417682647705 , diff:  9.882562637329102
adv train loss:  -1829.0533428192139 , diff:  3.635660171508789
adv train loss:  -1827.6222972869873 , diff:  1.4310455322265625
adv train loss:  -1836.7674522399902 , diff:  9.14515495300293
adv train loss:  -1838.2128715515137 , diff:  1.4454193115234375
adv train loss:  -1825.6151599884033 , diff:  12.597711563110352
adv train loss:  -1824.2166118621826 , diff:  1.3985481262207031
layer  2  adv train finish, try to retain  45
test acc: top1 ->  10.05 ; top5 ->  49.99  and loss:  1523.960518836975
forward train acc: top1 ->  91.4960000024414 ; top5 ->  99.494  and loss:  39.62495440244675
test acc: top1 ->  84.58 ; top5 ->  98.34  and loss:  70.50381527841091
forward train acc: top1 ->  93.1240000024414 ; top5 ->  99.68  and loss:  22.13689574599266
test acc: top1 ->  85.8 ; top5 ->  98.75  and loss:  55.250961512327194
forward train acc: top1 ->  93.9360000024414 ; top5 ->  99.74999997558594  and loss:  18.44329123198986
test acc: top1 ->  86.41 ; top5 ->  98.91  and loss:  51.1884568631649
forward train acc: top1 ->  94.68799997314453 ; top5 ->  99.836  and loss:  16.24290955811739
test acc: top1 ->  86.8 ; top5 ->  98.92  and loss:  50.425499469041824
forward train acc: top1 ->  95.08399999511718 ; top5 ->  99.8440000024414  and loss:  14.686088405549526
test acc: top1 ->  87.14 ; top5 ->  98.94  and loss:  49.824665278196335
forward train acc: top1 ->  95.54999997314454 ; top5 ->  99.86999997558594  and loss:  13.543017029762268
test acc: top1 ->  87.41 ; top5 ->  98.94  and loss:  49.18774385750294
forward train acc: top1 ->  95.61200001464844 ; top5 ->  99.868  and loss:  13.04886368662119
test acc: top1 ->  87.33 ; top5 ->  99.0  and loss:  48.370567947626114
forward train acc: top1 ->  95.84400001464844 ; top5 ->  99.88999997558594  and loss:  12.342128604650497
test acc: top1 ->  87.57 ; top5 ->  98.96  and loss:  48.515103846788406
forward train acc: top1 ->  96.18799999023437 ; top5 ->  99.89199997558593  and loss:  11.523477159440517
test acc: top1 ->  87.84 ; top5 ->  99.05  and loss:  48.26008985936642
forward train acc: top1 ->  96.14200000976562 ; top5 ->  99.896  and loss:  11.497267045080662
test acc: top1 ->  87.73 ; top5 ->  98.99  and loss:  47.766301080584526
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -506.2414956856519 , diff:  506.2414956856519
adv train loss:  -828.6033353805542 , diff:  322.3618396949023
adv train loss:  -842.6708545684814 , diff:  14.067519187927246
adv train loss:  -854.2101368904114 , diff:  11.539282321929932
adv train loss:  -853.1214838027954 , diff:  1.0886530876159668
adv train loss:  -850.9248561859131 , diff:  2.196627616882324
adv train loss:  -853.0900139808655 , diff:  2.1651577949523926
adv train loss:  -856.329794883728 , diff:  3.239780902862549
adv train loss:  -856.949182510376 , diff:  0.6193876266479492
adv train loss:  -855.8233709335327 , diff:  1.1258115768432617
layer  3  adv train finish, try to retain  28
test acc: top1 ->  11.7 ; top5 ->  49.31  and loss:  635.0892577171326
forward train acc: top1 ->  81.88200000976562 ; top5 ->  98.22199997802734  and loss:  57.195955872535706
test acc: top1 ->  79.2 ; top5 ->  97.98  and loss:  65.64918783307076
forward train acc: top1 ->  85.04800001708985 ; top5 ->  98.92600000488281  and loss:  45.77090114355087
test acc: top1 ->  81.17 ; top5 ->  98.4  and loss:  60.537844479084015
forward train acc: top1 ->  86.23800000732422 ; top5 ->  99.1140000024414  and loss:  41.16545623540878
test acc: top1 ->  82.08 ; top5 ->  98.64  and loss:  58.277734726667404
forward train acc: top1 ->  87.14999998535156 ; top5 ->  99.20199997802735  and loss:  38.14529898762703
test acc: top1 ->  82.77 ; top5 ->  98.49  and loss:  55.7542787194252
forward train acc: top1 ->  88.17999998535156 ; top5 ->  99.3500000024414  and loss:  34.85899794101715
test acc: top1 ->  83.07 ; top5 ->  98.55  and loss:  55.19426444172859
forward train acc: top1 ->  88.70799999511719 ; top5 ->  99.40800000244141  and loss:  33.52384126186371
test acc: top1 ->  83.63 ; top5 ->  98.69  and loss:  53.65547576546669
forward train acc: top1 ->  89.14000001953124 ; top5 ->  99.4300000024414  and loss:  32.55768731236458
test acc: top1 ->  83.75 ; top5 ->  98.72  and loss:  52.72689959406853
forward train acc: top1 ->  89.20800001953126 ; top5 ->  99.47199997802734  and loss:  32.32748244702816
test acc: top1 ->  83.97 ; top5 ->  98.74  and loss:  51.93116167187691
forward train acc: top1 ->  89.34000001708985 ; top5 ->  99.492  and loss:  31.08060435950756
test acc: top1 ->  84.22 ; top5 ->  98.73  and loss:  51.95682352781296
forward train acc: top1 ->  89.75199997070312 ; top5 ->  99.53  and loss:  30.537436962127686
test acc: top1 ->  84.4 ; top5 ->  98.81  and loss:  51.3259609490633
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -97.1193640679121 , diff:  97.1193640679121
adv train loss:  -510.4919993877411 , diff:  413.372635319829
adv train loss:  -599.276370048523 , diff:  88.78437066078186
adv train loss:  -655.0849838256836 , diff:  55.808613777160645
adv train loss:  -663.2341208457947 , diff:  8.149137020111084
adv train loss:  -671.1691036224365 , diff:  7.934982776641846
adv train loss:  -675.1670699119568 , diff:  3.9979662895202637
adv train loss:  -684.6170048713684 , diff:  9.449934959411621
adv train loss:  -697.3129091262817 , diff:  12.69590425491333
adv train loss:  -745.7734751701355 , diff:  48.46056604385376
layer  4  adv train finish, try to retain  29
test acc: top1 ->  10.24 ; top5 ->  51.5  and loss:  914.8427262306213
forward train acc: top1 ->  77.99999997802735 ; top5 ->  98.08400000732422  and loss:  65.1489867568016
test acc: top1 ->  77.26 ; top5 ->  97.69  and loss:  71.18186369538307
forward train acc: top1 ->  81.43200000488281 ; top5 ->  98.7040000024414  and loss:  54.65919926762581
test acc: top1 ->  79.04 ; top5 ->  97.94  and loss:  65.93790265917778
forward train acc: top1 ->  83.18600000976562 ; top5 ->  98.90599997802734  and loss:  49.28060871362686
test acc: top1 ->  80.13 ; top5 ->  98.18  and loss:  61.69969403743744
forward train acc: top1 ->  84.50799998535156 ; top5 ->  99.01799997802735  and loss:  45.50006511807442
test acc: top1 ->  80.86 ; top5 ->  98.32  and loss:  59.73089036345482
forward train acc: top1 ->  85.03399997558594 ; top5 ->  99.1260000024414  and loss:  43.7892210483551
test acc: top1 ->  81.3 ; top5 ->  98.48  and loss:  58.10508742928505
forward train acc: top1 ->  85.40199998779296 ; top5 ->  99.19000000244141  and loss:  42.00260013341904
test acc: top1 ->  81.49 ; top5 ->  98.47  and loss:  57.52892217040062
forward train acc: top1 ->  85.89200001464843 ; top5 ->  99.16600000244141  and loss:  41.28000330924988
test acc: top1 ->  82.06 ; top5 ->  98.54  and loss:  56.336580246686935
forward train acc: top1 ->  86.1360000024414 ; top5 ->  99.30999998046875  and loss:  40.07711124420166
test acc: top1 ->  81.98 ; top5 ->  98.55  and loss:  56.039731442928314
forward train acc: top1 ->  86.13999997558594 ; top5 ->  99.30000000244141  and loss:  39.88470408320427
test acc: top1 ->  82.39 ; top5 ->  98.62  and loss:  55.230769485235214
forward train acc: top1 ->  86.73399997802734 ; top5 ->  99.26200000244141  and loss:  38.69449886679649
test acc: top1 ->  82.29 ; top5 ->  98.64  and loss:  55.39037612080574
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -184.38924439251423 , diff:  184.38924439251423
adv train loss:  -764.0250215530396 , diff:  579.6357771605253
adv train loss:  -869.0032348632812 , diff:  104.9782133102417
adv train loss:  -875.3109445571899 , diff:  6.307709693908691
adv train loss:  -888.2282829284668 , diff:  12.917338371276855
adv train loss:  -889.9131422042847 , diff:  1.684859275817871
adv train loss:  -896.9175806045532 , diff:  7.004438400268555
adv train loss:  -900.4131689071655 , diff:  3.4955883026123047
adv train loss:  -900.6846237182617 , diff:  0.2714548110961914
adv train loss:  -901.4187002182007 , diff:  0.7340764999389648
layer  5  adv train finish, try to retain  24
test acc: top1 ->  10.33 ; top5 ->  52.63  and loss:  814.6229853630066
forward train acc: top1 ->  81.88399998046874 ; top5 ->  98.6380000024414  and loss:  54.558470875024796
test acc: top1 ->  80.25 ; top5 ->  98.44  and loss:  62.95343592762947
forward train acc: top1 ->  85.71600000488282 ; top5 ->  99.21399997558593  and loss:  41.866835445165634
test acc: top1 ->  82.42 ; top5 ->  98.54  and loss:  56.871920704841614
forward train acc: top1 ->  87.41199997802734 ; top5 ->  99.30799997802734  and loss:  37.59453096985817
test acc: top1 ->  83.3 ; top5 ->  98.79  and loss:  53.156310230493546
forward train acc: top1 ->  88.41399999023437 ; top5 ->  99.42600000244141  and loss:  33.91610199213028
test acc: top1 ->  83.77 ; top5 ->  98.75  and loss:  52.56841662526131
forward train acc: top1 ->  88.98599999267579 ; top5 ->  99.49400000244141  and loss:  32.11409069597721
test acc: top1 ->  84.45 ; top5 ->  98.86  and loss:  50.6741749048233
forward train acc: top1 ->  89.63600001953125 ; top5 ->  99.51999997558593  and loss:  30.328880041837692
test acc: top1 ->  84.64 ; top5 ->  98.93  and loss:  49.063283666968346
forward train acc: top1 ->  89.89199998046875 ; top5 ->  99.54999997802734  and loss:  29.494702219963074
test acc: top1 ->  85.13 ; top5 ->  98.96  and loss:  48.20630073547363
forward train acc: top1 ->  89.90399998046875 ; top5 ->  99.59  and loss:  29.027730882167816
test acc: top1 ->  85.41 ; top5 ->  98.91  and loss:  47.999767392873764
forward train acc: top1 ->  90.29800000976563 ; top5 ->  99.60599997558593  and loss:  27.960484623908997
test acc: top1 ->  85.17 ; top5 ->  99.01  and loss:  48.457087844610214
forward train acc: top1 ->  90.45400001220703 ; top5 ->  99.63  and loss:  27.650130853056908
test acc: top1 ->  85.47 ; top5 ->  98.94  and loss:  48.03645379841328
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -163.48758492432535 , diff:  163.48758492432535
adv train loss:  -878.1694025993347 , diff:  714.6818176750094
adv train loss:  -962.5342493057251 , diff:  84.36484670639038
adv train loss:  -1032.752142906189 , diff:  70.21789360046387
adv train loss:  -1102.3791770935059 , diff:  69.6270341873169
adv train loss:  -1106.256643295288 , diff:  3.8774662017822266
adv train loss:  -1104.788405418396 , diff:  1.4682378768920898
adv train loss:  -1104.1580610275269 , diff:  0.6303443908691406
adv train loss:  -1127.8999738693237 , diff:  23.741912841796875
adv train loss:  -1129.2131700515747 , diff:  1.3131961822509766
layer  6  adv train finish, try to retain  10
test acc: top1 ->  11.5 ; top5 ->  54.53  and loss:  538.2846169471741
forward train acc: top1 ->  82.2700000024414 ; top5 ->  99.20799998046876  and loss:  50.66053667664528
test acc: top1 ->  81.26 ; top5 ->  98.75  and loss:  60.63103359937668
forward train acc: top1 ->  87.75999999511718 ; top5 ->  99.646  and loss:  34.648606806993484
test acc: top1 ->  83.38 ; top5 ->  98.86  and loss:  56.48867624998093
forward train acc: top1 ->  89.65200001464844 ; top5 ->  99.742  and loss:  29.574625253677368
test acc: top1 ->  84.54 ; top5 ->  98.92  and loss:  53.11836266517639
forward train acc: top1 ->  90.74199997070312 ; top5 ->  99.80199997558594  and loss:  26.266665011644363
test acc: top1 ->  84.65 ; top5 ->  98.96  and loss:  53.33245003223419
forward train acc: top1 ->  91.52999998046874 ; top5 ->  99.8  and loss:  24.103570997714996
test acc: top1 ->  85.03 ; top5 ->  98.99  and loss:  52.140525221824646
forward train acc: top1 ->  92.13999997314453 ; top5 ->  99.822  and loss:  22.565697833895683
test acc: top1 ->  85.69 ; top5 ->  99.03  and loss:  50.856574922800064
forward train acc: top1 ->  92.10199998291016 ; top5 ->  99.816  and loss:  22.292151108384132
test acc: top1 ->  85.94 ; top5 ->  99.11  and loss:  49.91309002041817
forward train acc: top1 ->  92.63200000488281 ; top5 ->  99.8340000024414  and loss:  21.22112937271595
test acc: top1 ->  85.6 ; top5 ->  99.07  and loss:  51.35863760113716
forward train acc: top1 ->  92.83200001464844 ; top5 ->  99.8580000024414  and loss:  20.345889627933502
test acc: top1 ->  86.27 ; top5 ->  99.08  and loss:  49.60124772787094
forward train acc: top1 ->  93.27800000976562 ; top5 ->  99.85599997558593  and loss:  19.48314292728901
test acc: top1 ->  86.32 ; top5 ->  99.14  and loss:  49.34098578989506
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  141 / 256 , inc:  2
---------------- start layer  7  ---------------
adv train loss:  -5.031051663681865 , diff:  5.031051663681865
adv train loss:  -4.882685458287597 , diff:  0.14836620539426804
adv train loss:  -4.950731938704848 , diff:  0.06804648041725159
adv train loss:  -4.779549637809396 , diff:  0.1711823008954525
adv train loss:  -4.929038533940911 , diff:  0.1494888961315155
adv train loss:  -4.789659453555942 , diff:  0.1393790803849697
adv train loss:  -4.838585983961821 , diff:  0.04892653040587902
adv train loss:  -4.817367140203714 , diff:  0.02121884375810623
adv train loss:  -5.0220957063138485 , diff:  0.20472856611013412
adv train loss:  -4.734174290671945 , diff:  0.2879214156419039
layer  7  adv train finish, try to retain  260
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -175.0377373099327 , diff:  175.0377373099327
adv train loss:  -371.94860672950745 , diff:  196.91086941957474
adv train loss:  -494.5659942626953 , diff:  122.61738753318787
adv train loss:  -529.802282333374 , diff:  35.23628807067871
adv train loss:  -540.3405284881592 , diff:  10.538246154785156
adv train loss:  -540.1535878181458 , diff:  0.18694067001342773
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  129
test acc: top1 ->  13.52 ; top5 ->  69.6  and loss:  19578264.109375
forward train acc: top1 ->  99.5040000024414 ; top5 ->  99.998  and loss:  1.5985563688445836
test acc: top1 ->  91.87 ; top5 ->  99.3  and loss:  46.55132055282593
forward train acc: top1 ->  99.708 ; top5 ->  99.998  and loss:  0.9083564670290798
test acc: top1 ->  91.96 ; top5 ->  99.29  and loss:  48.577444069087505
forward train acc: top1 ->  99.828 ; top5 ->  100.0  and loss:  0.5341791165992618
test acc: top1 ->  92.07 ; top5 ->  99.29  and loss:  52.158141788095236
forward train acc: top1 ->  99.808 ; top5 ->  100.0  and loss:  0.5060156839899719
test acc: top1 ->  92.04 ; top5 ->  99.24  and loss:  54.911739476025105
forward train acc: top1 ->  99.88199997558594 ; top5 ->  99.998  and loss:  0.38498740014620125
test acc: top1 ->  92.12 ; top5 ->  99.32  and loss:  55.37119197845459
==> this epoch:  129 / 512
---------------- start layer  9  ---------------
adv train loss:  -730.4904346466064 , diff:  730.4904346466064
adv train loss:  -731.3502826690674 , diff:  0.8598480224609375
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  8
test acc: top1 ->  26.69 ; top5 ->  73.84  and loss:  1945.070200920105
forward train acc: top1 ->  97.92600000244141 ; top5 ->  99.978  and loss:  6.757672602310777
test acc: top1 ->  91.43 ; top5 ->  99.25  and loss:  46.11946810781956
forward train acc: top1 ->  99.73599997558594 ; top5 ->  100.0  and loss:  1.0031877346336842
test acc: top1 ->  91.92 ; top5 ->  99.25  and loss:  46.58031004667282
forward train acc: top1 ->  99.818 ; top5 ->  99.998  and loss:  0.6393143197055906
test acc: top1 ->  92.01 ; top5 ->  99.22  and loss:  48.94039821624756
forward train acc: top1 ->  99.822 ; top5 ->  100.0  and loss:  0.511840027582366
test acc: top1 ->  92.02 ; top5 ->  99.27  and loss:  50.011284947395325
forward train acc: top1 ->  99.906 ; top5 ->  100.0  and loss:  0.3320710447151214
test acc: top1 ->  92.1 ; top5 ->  99.2  and loss:  51.43501806259155
forward train acc: top1 ->  99.864 ; top5 ->  100.0  and loss:  0.4191838292463217
test acc: top1 ->  92.11 ; top5 ->  99.21  and loss:  52.00468681007624
==> this epoch:  8 / 512
---------------- start layer  10  ---------------
adv train loss:  -71.39765387773514 , diff:  71.39765387773514
adv train loss:  -71.42186397314072 , diff:  0.024210095405578613
layer  10  adv train finish, try to retain  491
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -1292.9076471328735 , diff:  1292.9076471328735
adv train loss:  -1284.8355388641357 , diff:  8.072108268737793
adv train loss:  -1285.4942445755005 , diff:  0.6587057113647461
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  3
test acc: top1 ->  10.0 ; top5 ->  44.47  and loss:  9075.915626525879
forward train acc: top1 ->  85.55200000488281 ; top5 ->  98.604  and loss:  61.35905333608389
test acc: top1 ->  87.66 ; top5 ->  98.75  and loss:  54.26626816391945
forward train acc: top1 ->  98.89399998291016 ; top5 ->  100.0  and loss:  5.9372220523655415
test acc: top1 ->  90.24 ; top5 ->  98.93  and loss:  45.987944945693016
forward train acc: top1 ->  99.31599997558594 ; top5 ->  99.996  and loss:  3.393955359235406
test acc: top1 ->  90.69 ; top5 ->  98.98  and loss:  47.162792190909386
forward train acc: top1 ->  99.596 ; top5 ->  99.996  and loss:  2.058849023655057
test acc: top1 ->  90.92 ; top5 ->  99.02  and loss:  48.970500990748405
forward train acc: top1 ->  99.646 ; top5 ->  99.998  and loss:  1.5239012697711587
test acc: top1 ->  91.18 ; top5 ->  98.95  and loss:  50.15382468700409
forward train acc: top1 ->  99.75999997558594 ; top5 ->  99.998  and loss:  1.1209204336628318
test acc: top1 ->  91.2 ; top5 ->  98.94  and loss:  50.78813840448856
forward train acc: top1 ->  99.79200000244141 ; top5 ->  100.0  and loss:  1.0187870729714632
test acc: top1 ->  91.21 ; top5 ->  98.9  and loss:  51.737786166369915
forward train acc: top1 ->  99.78399997558594 ; top5 ->  100.0  and loss:  0.9254428800195456
test acc: top1 ->  91.33 ; top5 ->  98.95  and loss:  51.82529427111149
forward train acc: top1 ->  99.79599997558594 ; top5 ->  100.0  and loss:  0.8374857129529119
test acc: top1 ->  91.39 ; top5 ->  98.91  and loss:  52.26300138235092
forward train acc: top1 ->  99.82399997558593 ; top5 ->  100.0  and loss:  0.7517661675810814
test acc: top1 ->  91.38 ; top5 ->  98.86  and loss:  53.04119274765253
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  4 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -1016.2663726806641 , diff:  1016.2663726806641
adv train loss:  -939.6041898727417 , diff:  76.66218280792236
adv train loss:  -940.4346151351929 , diff:  0.8304252624511719
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  39
test acc: top1 ->  13.45 ; top5 ->  50.53  and loss:  28841.380416870117
forward train acc: top1 ->  63.446000009765626 ; top5 ->  91.78399997558594  and loss:  420.4145426750183
test acc: top1 ->  74.68 ; top5 ->  97.47  and loss:  116.22117155790329
forward train acc: top1 ->  94.89199997802734 ; top5 ->  99.992  and loss:  17.161718904972076
test acc: top1 ->  90.35 ; top5 ->  98.18  and loss:  49.64862886071205
forward train acc: top1 ->  99.526 ; top5 ->  99.994  and loss:  3.7798729352653027
test acc: top1 ->  90.74 ; top5 ->  98.21  and loss:  48.10886900871992
forward train acc: top1 ->  99.66200000488281 ; top5 ->  99.99799997558594  and loss:  2.5043469881638885
test acc: top1 ->  90.92 ; top5 ->  98.31  and loss:  47.58215506374836
forward train acc: top1 ->  99.696 ; top5 ->  99.996  and loss:  2.0588300013914704
test acc: top1 ->  91.07 ; top5 ->  98.32  and loss:  47.663162633776665
forward train acc: top1 ->  99.756 ; top5 ->  100.0  and loss:  1.7744424231350422
test acc: top1 ->  91.14 ; top5 ->  98.35  and loss:  47.89472085982561
forward train acc: top1 ->  99.75999997558594 ; top5 ->  100.0  and loss:  1.5508672278374434
test acc: top1 ->  91.17 ; top5 ->  98.37  and loss:  48.25302346050739
forward train acc: top1 ->  99.82599997558594 ; top5 ->  100.0  and loss:  1.3959981333464384
test acc: top1 ->  91.26 ; top5 ->  98.38  and loss:  48.773404002189636
forward train acc: top1 ->  99.79599997558594 ; top5 ->  100.0  and loss:  1.2794152237474918
test acc: top1 ->  91.27 ; top5 ->  98.41  and loss:  48.98164700716734
forward train acc: top1 ->  99.7760000024414 ; top5 ->  99.998  and loss:  1.270597011782229
test acc: top1 ->  91.39 ; top5 ->  98.39  and loss:  49.34019932150841
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  47 / 512 , inc:  8
---------------- start layer  13  ---------------
adv train loss:  -8850.440753936768 , diff:  8850.440753936768
adv train loss:  -15933.499435424805 , diff:  7083.058681488037
adv train loss:  -22527.869842529297 , diff:  6594.370407104492
adv train loss:  -29211.92462158203 , diff:  6684.054779052734
adv train loss:  -35922.1764831543 , diff:  6710.251861572266
adv train loss:  -42738.50765991211 , diff:  6816.3311767578125
adv train loss:  -49440.885345458984 , diff:  6702.377685546875
adv train loss:  -54878.48370361328 , diff:  5437.598358154297
adv train loss:  -58738.39514160156 , diff:  3859.9114379882812
adv train loss:  -60548.22772216797 , diff:  1809.8325805664062
layer  13  adv train finish, try to retain  26
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.75  ==>  48 / 64 , inc:  4
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.55078125  ==>  141 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.251953125  ==>  129 / 512 , inc:  32
layer  9  :  0.015625  ==>  8 / 512 , inc:  2
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.0078125  ==>  4 / 512 , inc:  1
layer  12  :  0.091796875  ==>  47 / 512 , inc:  4
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [1.313681671142578, 1.7515755615234374, 1.313681671142578, 1.313681671142578, 1.313681671142578, 1.313681671142578, 1.313681671142578, 0.08210510444641113, 1.167717041015625, 1.5569560546875, 1.9705225067138672, 0.8757877807617187, 1.313681671142578, 18.68347265625]  wait [2, 0, 2, 2, 2, 2, 2, 1, 0, 0, 2, 3, 2, 1]  inc [1, 4, 1, 1, 1, 1, 1, 1, 32, 2, 1, 1, 4, 1]  tol: 4
$$$$$$$$$$$$$ epoch  56  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1653.6470975875854 , diff:  1653.6470975875854
adv train loss:  -1655.2209329605103 , diff:  1.5738353729248047
adv train loss:  -1654.1118469238281 , diff:  1.109086036682129
adv train loss:  -1682.5996751785278 , diff:  28.487828254699707
adv train loss:  -1674.0558185577393 , diff:  8.543856620788574
adv train loss:  -1674.935709953308 , diff:  0.8798913955688477
adv train loss:  -1680.243516921997 , diff:  5.307806968688965
adv train loss:  -1672.7223119735718 , diff:  7.521204948425293
adv train loss:  -1678.3750896453857 , diff:  5.652777671813965
adv train loss:  -1680.0171766281128 , diff:  1.6420869827270508
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  31
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  14769.42668914795
forward train acc: top1 ->  95.44199997558594 ; top5 ->  99.918  and loss:  27.822308369912207
test acc: top1 ->  90.85 ; top5 ->  98.89  and loss:  78.0477014631033
forward train acc: top1 ->  99.39999997802734 ; top5 ->  99.99399997558594  and loss:  2.4644687711261213
test acc: top1 ->  91.05 ; top5 ->  98.97  and loss:  74.9380419254303
forward train acc: top1 ->  99.48799997558594 ; top5 ->  99.998  and loss:  1.8275399887934327
test acc: top1 ->  91.47 ; top5 ->  98.97  and loss:  71.6459710597992
forward train acc: top1 ->  99.54799997802735 ; top5 ->  100.0  and loss:  1.6158222490921617
test acc: top1 ->  91.46 ; top5 ->  98.95  and loss:  69.46768805384636
forward train acc: top1 ->  99.62399997558593 ; top5 ->  100.0  and loss:  1.280371620086953
test acc: top1 ->  91.74 ; top5 ->  99.02  and loss:  67.95507986843586
forward train acc: top1 ->  99.65999997802734 ; top5 ->  99.996  and loss:  1.1796045070514083
test acc: top1 ->  91.67 ; top5 ->  99.11  and loss:  68.07488445937634
forward train acc: top1 ->  99.67799997558593 ; top5 ->  99.998  and loss:  0.975372897926718
test acc: top1 ->  91.69 ; top5 ->  99.04  and loss:  67.40572188794613
forward train acc: top1 ->  99.68599997558594 ; top5 ->  99.996  and loss:  1.046324991621077
test acc: top1 ->  91.75 ; top5 ->  99.05  and loss:  66.10907959192991
forward train acc: top1 ->  99.732 ; top5 ->  100.0  and loss:  0.8210217180894688
test acc: top1 ->  91.74 ; top5 ->  99.08  and loss:  66.44688761234283
forward train acc: top1 ->  99.70800000244141 ; top5 ->  100.0  and loss:  0.8122173063457012
test acc: top1 ->  91.78 ; top5 ->  99.13  and loss:  65.94708194583654
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -0.660169426817447 , diff:  0.660169426817447
adv train loss:  -1.7646579064894468 , diff:  1.1044884796719998
adv train loss:  -1.8186982532497495 , diff:  0.05404034676030278
adv train loss:  -1.6664804231841117 , diff:  0.15221783006563783
adv train loss:  -1.946755925891921 , diff:  0.2802755027078092
adv train loss:  -1.6469706848729402 , diff:  0.29978524101898074
adv train loss:  -1.6349688082118519 , diff:  0.012001876661088318
adv train loss:  -1.6039337562397122 , diff:  0.031035051972139627
adv train loss:  -1.590655256062746 , diff:  0.01327850017696619
adv train loss:  -1.7188327047042549 , diff:  0.12817744864150882
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  44
test acc: top1 ->  29.22 ; top5 ->  75.86  and loss:  539.3319525718689
forward train acc: top1 ->  99.524 ; top5 ->  100.0  and loss:  1.5085686706006527
test acc: top1 ->  91.74 ; top5 ->  99.11  and loss:  61.28124535828829
forward train acc: top1 ->  99.7 ; top5 ->  99.998  and loss:  0.940030834171921
test acc: top1 ->  91.78 ; top5 ->  99.21  and loss:  60.877915881574154
forward train acc: top1 ->  99.79199997802735 ; top5 ->  100.0  and loss:  0.6332785184495151
test acc: top1 ->  91.75 ; top5 ->  99.15  and loss:  62.62401910871267
forward train acc: top1 ->  99.76999997558593 ; top5 ->  100.0  and loss:  0.6510242992080748
test acc: top1 ->  91.87 ; top5 ->  99.21  and loss:  61.9765389803797
forward train acc: top1 ->  99.79599997558594 ; top5 ->  100.0  and loss:  0.6161191638093442
test acc: top1 ->  91.86 ; top5 ->  99.14  and loss:  62.9207517914474
forward train acc: top1 ->  99.83599997558593 ; top5 ->  100.0  and loss:  0.5030856819357723
test acc: top1 ->  91.99 ; top5 ->  99.19  and loss:  60.20096070505679
forward train acc: top1 ->  99.86599997558594 ; top5 ->  100.0  and loss:  0.3865842861123383
test acc: top1 ->  92.01 ; top5 ->  99.18  and loss:  61.07322954759002
forward train acc: top1 ->  99.862 ; top5 ->  100.0  and loss:  0.3759038818679983
test acc: top1 ->  91.89 ; top5 ->  99.17  and loss:  62.70262832939625
forward train acc: top1 ->  99.864 ; top5 ->  100.0  and loss:  0.3335718712769449
test acc: top1 ->  92.11 ; top5 ->  99.22  and loss:  62.56916495412588
==> this epoch:  44 / 64
---------------- start layer  2  ---------------
adv train loss:  -858.594725246774 , diff:  858.594725246774
adv train loss:  -1454.294870376587 , diff:  595.7001451298129
adv train loss:  -1466.899748802185 , diff:  12.604878425598145
adv train loss:  -1463.6322975158691 , diff:  3.267451286315918
adv train loss:  -1464.1744232177734 , diff:  0.5421257019042969
adv train loss:  -1469.4762115478516 , diff:  5.301788330078125
adv train loss:  -1471.3360557556152 , diff:  1.8598442077636719
adv train loss:  -1463.4324750900269 , diff:  7.903580665588379
adv train loss:  -1463.287181854248 , diff:  0.1452932357788086
adv train loss:  -1472.4230394363403 , diff:  9.135857582092285
************ all values are small in this layer **********
layer  2  adv train finish, try to retain  99
test acc: top1 ->  16.86 ; top5 ->  61.12  and loss:  2616.993673324585
forward train acc: top1 ->  99.39599997558594 ; top5 ->  99.996  and loss:  1.9234391991049051
test acc: top1 ->  91.49 ; top5 ->  99.24  and loss:  60.20854987204075
forward train acc: top1 ->  99.55 ; top5 ->  99.996  and loss:  1.4709322713315487
test acc: top1 ->  91.61 ; top5 ->  99.24  and loss:  55.92130330204964
forward train acc: top1 ->  99.614 ; top5 ->  99.998  and loss:  1.1335658859461546
test acc: top1 ->  91.57 ; top5 ->  99.31  and loss:  54.731479063630104
forward train acc: top1 ->  99.60199997802735 ; top5 ->  99.998  and loss:  1.173883305862546
test acc: top1 ->  91.81 ; top5 ->  99.27  and loss:  53.27230104804039
forward train acc: top1 ->  99.70799997558593 ; top5 ->  100.0  and loss:  0.8530270149931312
test acc: top1 ->  91.52 ; top5 ->  99.28  and loss:  56.1393324136734
forward train acc: top1 ->  99.69400000244141 ; top5 ->  99.998  and loss:  0.9018076464999467
test acc: top1 ->  91.89 ; top5 ->  99.29  and loss:  53.431650295853615
forward train acc: top1 ->  99.74599997558593 ; top5 ->  100.0  and loss:  0.7983245928771794
test acc: top1 ->  91.81 ; top5 ->  99.22  and loss:  53.59264460206032
forward train acc: top1 ->  99.758 ; top5 ->  100.0  and loss:  0.7321498671371955
test acc: top1 ->  91.95 ; top5 ->  99.29  and loss:  54.23975560069084
forward train acc: top1 ->  99.72799997558593 ; top5 ->  100.0  and loss:  0.8415389403235167
test acc: top1 ->  91.84 ; top5 ->  99.24  and loss:  53.904179967939854
forward train acc: top1 ->  99.75 ; top5 ->  100.0  and loss:  0.705207300838083
test acc: top1 ->  91.79 ; top5 ->  99.21  and loss:  55.80316582322121
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -751.7415349655785 , diff:  751.7415349655785
adv train loss:  -1402.1731443405151 , diff:  650.4316093749367
adv train loss:  -1437.6374559402466 , diff:  35.464311599731445
adv train loss:  -1434.9835081100464 , diff:  2.6539478302001953
adv train loss:  -1439.916296005249 , diff:  4.932787895202637
adv train loss:  -1452.0033626556396 , diff:  12.087066650390625
adv train loss:  -1449.4237298965454 , diff:  2.5796327590942383
adv train loss:  -1451.9741868972778 , diff:  2.550457000732422
adv train loss:  -1448.2963247299194 , diff:  3.6778621673583984
adv train loss:  -1447.764726638794 , diff:  0.5315980911254883
************ all values are small in this layer **********
layer  3  adv train finish, try to retain  86
test acc: top1 ->  10.14 ; top5 ->  48.96  and loss:  15600.23257446289
forward train acc: top1 ->  99.66600000244141 ; top5 ->  100.0  and loss:  1.0313238229136914
test acc: top1 ->  91.75 ; top5 ->  99.14  and loss:  55.518565341830254
forward train acc: top1 ->  99.736 ; top5 ->  100.0  and loss:  0.7404669714160264
test acc: top1 ->  91.69 ; top5 ->  99.19  and loss:  58.373763520270586
forward train acc: top1 ->  99.76800000244141 ; top5 ->  100.0  and loss:  0.6795338292140514
test acc: top1 ->  91.71 ; top5 ->  99.22  and loss:  57.86079193651676
forward train acc: top1 ->  99.78199997558593 ; top5 ->  100.0  and loss:  0.6767506501637399
test acc: top1 ->  91.84 ; top5 ->  99.17  and loss:  55.48862873762846
forward train acc: top1 ->  99.81199997558593 ; top5 ->  100.0  and loss:  0.5503719197586179
test acc: top1 ->  91.74 ; top5 ->  99.34  and loss:  57.4226458966732
forward train acc: top1 ->  99.84799997558594 ; top5 ->  100.0  and loss:  0.48695260332897305
test acc: top1 ->  91.89 ; top5 ->  99.25  and loss:  58.743284180760384
forward train acc: top1 ->  99.8 ; top5 ->  100.0  and loss:  0.5579108593519777
test acc: top1 ->  91.95 ; top5 ->  99.27  and loss:  57.256987139582634
forward train acc: top1 ->  99.846 ; top5 ->  99.998  and loss:  0.4801503124035662
test acc: top1 ->  91.97 ; top5 ->  99.27  and loss:  57.447173073887825
forward train acc: top1 ->  99.842 ; top5 ->  100.0  and loss:  0.4470291589386761
test acc: top1 ->  91.96 ; top5 ->  99.3  and loss:  56.889740109443665
forward train acc: top1 ->  99.86000000244141 ; top5 ->  100.0  and loss:  0.44044806039892137
test acc: top1 ->  91.9 ; top5 ->  99.31  and loss:  58.33301657438278
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -214.780111590866 , diff:  214.780111590866
adv train loss:  -1460.6392316818237 , diff:  1245.8591200909577
adv train loss:  -1655.1979351043701 , diff:  194.5587034225464
adv train loss:  -1691.5297632217407 , diff:  36.331828117370605
adv train loss:  -1713.7280616760254 , diff:  22.198298454284668
adv train loss:  -1723.8021240234375 , diff:  10.07406234741211
adv train loss:  -1807.8170566558838 , diff:  84.01493263244629
adv train loss:  -1835.7327823638916 , diff:  27.915725708007812
adv train loss:  -1840.7981395721436 , diff:  5.065357208251953
adv train loss:  -1839.6236305236816 , diff:  1.174509048461914
************ all values are small in this layer **********
layer  4  adv train finish, try to retain  154
test acc: top1 ->  15.35 ; top5 ->  59.16  and loss:  17169.068115234375
forward train acc: top1 ->  98.35000000732421 ; top5 ->  99.982  and loss:  5.625585125759244
test acc: top1 ->  90.08 ; top5 ->  99.05  and loss:  50.69519577920437
forward train acc: top1 ->  98.69600000732422 ; top5 ->  99.996  and loss:  3.9773776307702065
test acc: top1 ->  90.22 ; top5 ->  99.24  and loss:  49.577548272907734
forward train acc: top1 ->  98.86599998291015 ; top5 ->  99.99  and loss:  3.510507090948522
test acc: top1 ->  90.5 ; top5 ->  99.1  and loss:  48.667078390717506
forward train acc: top1 ->  98.91999997558594 ; top5 ->  99.99  and loss:  3.1157139567658305
test acc: top1 ->  90.42 ; top5 ->  99.31  and loss:  48.64690378308296
forward train acc: top1 ->  98.964 ; top5 ->  99.994  and loss:  3.000069041736424
test acc: top1 ->  90.37 ; top5 ->  99.3  and loss:  48.01098673045635
forward train acc: top1 ->  99.04599997802734 ; top5 ->  99.99  and loss:  2.7274815551936626
test acc: top1 ->  90.58 ; top5 ->  99.32  and loss:  48.395700231194496
forward train acc: top1 ->  99.07999997558593 ; top5 ->  99.996  and loss:  2.676292004995048
test acc: top1 ->  90.54 ; top5 ->  99.3  and loss:  48.178284887224436
forward train acc: top1 ->  99.20800000244141 ; top5 ->  99.998  and loss:  2.376604164019227
test acc: top1 ->  90.43 ; top5 ->  99.36  and loss:  48.948315646499395
forward train acc: top1 ->  99.16399997802735 ; top5 ->  99.994  and loss:  2.3846478229388595
test acc: top1 ->  90.68 ; top5 ->  99.35  and loss:  49.43590493127704
forward train acc: top1 ->  99.2280000024414 ; top5 ->  99.998  and loss:  2.2907196078449488
test acc: top1 ->  90.54 ; top5 ->  99.26  and loss:  50.265499509871006
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -454.9480164926499 , diff:  454.9480164926499
adv train loss:  -1631.003339767456 , diff:  1176.0553232748061
adv train loss:  -1712.8379535675049 , diff:  81.83461380004883
adv train loss:  -1719.2846641540527 , diff:  6.446710586547852
adv train loss:  -1718.2631568908691 , diff:  1.0215072631835938
adv train loss:  -1722.247543334961 , diff:  3.984386444091797
adv train loss:  -1724.881591796875 , diff:  2.6340484619140625
adv train loss:  -1727.3760318756104 , diff:  2.4944400787353516
adv train loss:  -1726.2106819152832 , diff:  1.1653499603271484
adv train loss:  -1723.6165962219238 , diff:  2.594085693359375
************ all values are small in this layer **********
layer  5  adv train finish, try to retain  148
test acc: top1 ->  22.84 ; top5 ->  72.0  and loss:  2443.0162029266357
forward train acc: top1 ->  99.486 ; top5 ->  99.998  and loss:  1.4745890451595187
test acc: top1 ->  91.19 ; top5 ->  99.46  and loss:  51.16088907048106
forward train acc: top1 ->  99.656 ; top5 ->  100.0  and loss:  0.9977779337204993
test acc: top1 ->  91.19 ; top5 ->  99.35  and loss:  51.995233457535505
forward train acc: top1 ->  99.66999997558594 ; top5 ->  99.998  and loss:  0.9694406234193593
test acc: top1 ->  91.5 ; top5 ->  99.39  and loss:  52.71171097457409
forward train acc: top1 ->  99.70599997558594 ; top5 ->  100.0  and loss:  0.8111119994428009
test acc: top1 ->  91.53 ; top5 ->  99.32  and loss:  54.27512498944998
forward train acc: top1 ->  99.778 ; top5 ->  100.0  and loss:  0.607877956237644
test acc: top1 ->  91.54 ; top5 ->  99.36  and loss:  56.62590976431966
forward train acc: top1 ->  99.792 ; top5 ->  100.0  and loss:  0.641777534969151
test acc: top1 ->  91.59 ; top5 ->  99.38  and loss:  56.52926306799054
forward train acc: top1 ->  99.796 ; top5 ->  100.0  and loss:  0.5727258942788467
test acc: top1 ->  91.52 ; top5 ->  99.42  and loss:  57.07373437285423
forward train acc: top1 ->  99.80000000244141 ; top5 ->  100.0  and loss:  0.606627234024927
test acc: top1 ->  91.63 ; top5 ->  99.42  and loss:  56.639116344973445
forward train acc: top1 ->  99.82599997558594 ; top5 ->  100.0  and loss:  0.497303475625813
test acc: top1 ->  91.52 ; top5 ->  99.42  and loss:  57.43543320894241
forward train acc: top1 ->  99.82399997558593 ; top5 ->  100.0  and loss:  0.5091613160911947
test acc: top1 ->  91.66 ; top5 ->  99.44  and loss:  57.49685535393655
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -576.859150815988 , diff:  576.859150815988
adv train loss:  -2093.982328414917 , diff:  1517.123177598929
adv train loss:  -2175.757881164551 , diff:  81.77555274963379
adv train loss:  -2211.566614151001 , diff:  35.808732986450195
adv train loss:  -2231.259153366089 , diff:  19.69253921508789
adv train loss:  -2238.0105361938477 , diff:  6.751382827758789
adv train loss:  -2236.5234031677246 , diff:  1.4871330261230469
adv train loss:  -2233.000162124634 , diff:  3.5232410430908203
adv train loss:  -2235.513229370117 , diff:  2.5130672454833984
adv train loss:  -2238.3014030456543 , diff:  2.7881736755371094
************ all values are small in this layer **********
layer  6  adv train finish, try to retain  140
test acc: top1 ->  8.57 ; top5 ->  55.76  and loss:  1066527.671875
forward train acc: top1 ->  99.896 ; top5 ->  100.0  and loss:  0.40206996008055285
test acc: top1 ->  91.89 ; top5 ->  99.36  and loss:  60.22666010260582
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.2475933250389062
test acc: top1 ->  92.0 ; top5 ->  99.28  and loss:  61.04911512136459
forward train acc: top1 ->  99.892 ; top5 ->  100.0  and loss:  0.35400538524845615
test acc: top1 ->  92.17 ; top5 ->  99.29  and loss:  60.15808718279004
==> this epoch:  140 / 256
---------------- start layer  7  ---------------
adv train loss:  -0.26093618874438107 , diff:  0.26093618874438107
adv train loss:  -0.2638146110693924 , diff:  0.0028784223250113428
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  252
test acc: top1 ->  43.81 ; top5 ->  80.87  and loss:  259.14548683166504
forward train acc: top1 ->  99.2100000024414 ; top5 ->  99.996  and loss:  2.431245024083182
test acc: top1 ->  90.98 ; top5 ->  99.2  and loss:  60.3124745041132
forward train acc: top1 ->  99.69599997558593 ; top5 ->  99.998  and loss:  0.9572855301667005
test acc: top1 ->  91.07 ; top5 ->  99.21  and loss:  59.38301093131304
forward train acc: top1 ->  99.71000000488282 ; top5 ->  99.998  and loss:  0.8523228110279888
test acc: top1 ->  91.25 ; top5 ->  99.2  and loss:  59.59270065277815
forward train acc: top1 ->  99.814 ; top5 ->  100.0  and loss:  0.6084485168103129
test acc: top1 ->  91.51 ; top5 ->  99.19  and loss:  60.911811254918575
forward train acc: top1 ->  99.754 ; top5 ->  100.0  and loss:  0.7130774846300483
test acc: top1 ->  91.41 ; top5 ->  99.22  and loss:  61.36632943153381
forward train acc: top1 ->  99.80399997558594 ; top5 ->  99.998  and loss:  0.5972222997806966
test acc: top1 ->  91.52 ; top5 ->  99.21  and loss:  61.14031830430031
forward train acc: top1 ->  99.80199997558594 ; top5 ->  100.0  and loss:  0.5892260556574911
test acc: top1 ->  91.43 ; top5 ->  99.23  and loss:  60.9188988506794
forward train acc: top1 ->  99.852 ; top5 ->  99.998  and loss:  0.4913959138793871
test acc: top1 ->  91.55 ; top5 ->  99.21  and loss:  60.980148300528526
forward train acc: top1 ->  99.844 ; top5 ->  100.0  and loss:  0.4404767827363685
test acc: top1 ->  91.51 ; top5 ->  99.24  and loss:  60.9456542506814
forward train acc: top1 ->  99.874 ; top5 ->  100.0  and loss:  0.39672040176810697
test acc: top1 ->  91.51 ; top5 ->  99.2  and loss:  62.22716551274061
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  253 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -0.559495062334463 , diff:  0.559495062334463
adv train loss:  -0.6528132176026702 , diff:  0.09331815526820719
adv train loss:  -0.7182552507147193 , diff:  0.0654420331120491
adv train loss:  -0.7972542512579821 , diff:  0.07899900054326281
adv train loss:  -0.7416118154651485 , diff:  0.05564243579283357
adv train loss:  -0.7130220441613346 , diff:  0.028589771303813905
adv train loss:  -0.56320383050479 , diff:  0.14981821365654469
adv train loss:  -0.7983617330901325 , diff:  0.23515790258534253
adv train loss:  -0.6025493636261672 , diff:  0.1958123694639653
adv train loss:  -0.680382881546393 , diff:  0.07783351792022586
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  97
test acc: top1 ->  13.51 ; top5 ->  66.1  and loss:  4958823.484375
forward train acc: top1 ->  97.98599997558594 ; top5 ->  99.982  and loss:  7.815341202542186
test acc: top1 ->  90.95 ; top5 ->  99.16  and loss:  53.21873486787081
forward train acc: top1 ->  99.638 ; top5 ->  99.998  and loss:  1.3577708126977086
test acc: top1 ->  91.24 ; top5 ->  99.15  and loss:  55.948642671108246
forward train acc: top1 ->  99.70200000244141 ; top5 ->  100.0  and loss:  1.0142467678524554
test acc: top1 ->  91.37 ; top5 ->  99.2  and loss:  56.38604919239879
forward train acc: top1 ->  99.79399997802734 ; top5 ->  100.0  and loss:  0.7515691742300987
test acc: top1 ->  91.49 ; top5 ->  99.17  and loss:  57.53221484273672
forward train acc: top1 ->  99.808 ; top5 ->  100.0  and loss:  0.7513990313746035
test acc: top1 ->  91.58 ; top5 ->  99.22  and loss:  56.84719821810722
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  129 / 512 , inc:  32
---------------- start layer  9  ---------------
adv train loss:  -664.7427096366882 , diff:  664.7427096366882
adv train loss:  -672.8152475357056 , diff:  8.072537899017334
adv train loss:  -688.0364360809326 , diff:  15.22118854522705
adv train loss:  -733.0322184562683 , diff:  44.99578237533569
adv train loss:  -731.8200006484985 , diff:  1.2122178077697754
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  6
test acc: top1 ->  33.8 ; top5 ->  78.94  and loss:  1936.7807817459106
forward train acc: top1 ->  99.5220000024414 ; top5 ->  100.0  and loss:  1.4654971887357533
test acc: top1 ->  91.03 ; top5 ->  99.33  and loss:  62.218295216560364
forward train acc: top1 ->  99.928 ; top5 ->  100.0  and loss:  0.24856883054599166
test acc: top1 ->  91.86 ; top5 ->  99.38  and loss:  61.06294860690832
forward train acc: top1 ->  99.92599997558594 ; top5 ->  100.0  and loss:  0.2637550614308566
test acc: top1 ->  91.93 ; top5 ->  99.34  and loss:  62.44543208926916
forward train acc: top1 ->  99.91599997558593 ; top5 ->  100.0  and loss:  0.23212244966998696
test acc: top1 ->  92.01 ; top5 ->  99.33  and loss:  63.56388705223799
forward train acc: top1 ->  99.92799997558593 ; top5 ->  100.0  and loss:  0.2306756682228297
test acc: top1 ->  91.86 ; top5 ->  99.34  and loss:  64.3161835372448
forward train acc: top1 ->  99.936 ; top5 ->  100.0  and loss:  0.22878422754001804
test acc: top1 ->  92.02 ; top5 ->  99.35  and loss:  64.51259611919522
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.16664951713755727
test acc: top1 ->  92.03 ; top5 ->  99.31  and loss:  63.487813003361225
forward train acc: top1 ->  99.95199997558593 ; top5 ->  100.0  and loss:  0.149101598886773
test acc: top1 ->  91.98 ; top5 ->  99.37  and loss:  64.21336214244366
forward train acc: top1 ->  99.94399997558594 ; top5 ->  100.0  and loss:  0.16714063077233732
test acc: top1 ->  91.99 ; top5 ->  99.35  and loss:  64.01381561160088
forward train acc: top1 ->  99.942 ; top5 ->  100.0  and loss:  0.1537889787578024
test acc: top1 ->  91.98 ; top5 ->  99.37  and loss:  64.82969941943884
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  8 / 512 , inc:  2
---------------- start layer  10  ---------------
adv train loss:  -192.6966518163681 , diff:  192.6966518163681
adv train loss:  -193.51038134098053 , diff:  0.8137295246124268
adv train loss:  -192.2343968153 , diff:  1.275984525680542
adv train loss:  -201.26010358333588 , diff:  9.025706768035889
adv train loss:  -427.66421937942505 , diff:  226.40411579608917
adv train loss:  -525.3422541618347 , diff:  97.67803478240967
adv train loss:  -525.8726472854614 , diff:  0.530393123626709
adv train loss:  -526.6452751159668 , diff:  0.7726278305053711
adv train loss:  -557.8408970832825 , diff:  31.195621967315674
adv train loss:  -716.1983962059021 , diff:  158.35749912261963
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  10
test acc: top1 ->  10.0 ; top5 ->  53.12  and loss:  175431.56872558594
forward train acc: top1 ->  90.7060000024414 ; top5 ->  98.638  and loss:  62.38972918316722
test acc: top1 ->  90.7 ; top5 ->  98.98  and loss:  55.05066888034344
forward train acc: top1 ->  99.73599997558594 ; top5 ->  100.0  and loss:  1.4574456699192524
test acc: top1 ->  91.34 ; top5 ->  99.06  and loss:  51.57028532773256
forward train acc: top1 ->  99.824 ; top5 ->  100.0  and loss:  0.9449432445690036
test acc: top1 ->  91.51 ; top5 ->  99.1  and loss:  51.705742597579956
forward train acc: top1 ->  99.844 ; top5 ->  100.0  and loss:  0.7949086416047066
test acc: top1 ->  91.46 ; top5 ->  99.13  and loss:  51.77728649973869
forward train acc: top1 ->  99.874 ; top5 ->  100.0  and loss:  0.6885368102230132
test acc: top1 ->  91.62 ; top5 ->  99.14  and loss:  52.38590368628502
forward train acc: top1 ->  99.89599997558594 ; top5 ->  100.0  and loss:  0.621748807374388
test acc: top1 ->  91.59 ; top5 ->  99.15  and loss:  52.70895162969828
forward train acc: top1 ->  99.88399997558594 ; top5 ->  100.0  and loss:  0.590953360311687
test acc: top1 ->  91.57 ; top5 ->  99.17  and loss:  52.67724607139826
forward train acc: top1 ->  99.91199997558594 ; top5 ->  100.0  and loss:  0.4705970175564289
test acc: top1 ->  91.6 ; top5 ->  99.13  and loss:  53.08198290318251
forward train acc: top1 ->  99.902 ; top5 ->  100.0  and loss:  0.43123247928451747
test acc: top1 ->  91.77 ; top5 ->  99.17  and loss:  53.14552470296621
forward train acc: top1 ->  99.928 ; top5 ->  100.0  and loss:  0.3972516395151615
test acc: top1 ->  91.7 ; top5 ->  99.2  and loss:  53.4269345253706
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  11 / 512 , inc:  1
---------------- start layer  11  ---------------
### skip layer  11 wait:  3  ###
---------------- start layer  12  ---------------
adv train loss:  -2176.287929534912 , diff:  2176.287929534912
adv train loss:  -2333.844701766968 , diff:  157.55677223205566
adv train loss:  -2474.354404449463 , diff:  140.50970268249512
adv train loss:  -2473.680248260498 , diff:  0.6741561889648438
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  43
test acc: top1 ->  19.95 ; top5 ->  82.1  and loss:  9624.21672821045
forward train acc: top1 ->  97.564 ; top5 ->  99.988  and loss:  13.961678204126656
test acc: top1 ->  91.5 ; top5 ->  99.17  and loss:  68.49621523171663
forward train acc: top1 ->  99.886 ; top5 ->  100.0  and loss:  0.3548208926804364
test acc: top1 ->  91.7 ; top5 ->  99.15  and loss:  68.81904746219516
forward train acc: top1 ->  99.9120000024414 ; top5 ->  100.0  and loss:  0.27053999807685614
test acc: top1 ->  91.87 ; top5 ->  99.21  and loss:  69.10694063082337
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.19286825961899012
test acc: top1 ->  91.83 ; top5 ->  99.16  and loss:  70.40209650248289
forward train acc: top1 ->  99.94999997558594 ; top5 ->  100.0  and loss:  0.1572257480584085
test acc: top1 ->  91.88 ; top5 ->  99.24  and loss:  70.24780815839767
forward train acc: top1 ->  99.95800000244141 ; top5 ->  100.0  and loss:  0.1226643375121057
test acc: top1 ->  91.89 ; top5 ->  99.27  and loss:  70.41423749923706
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.15406979934778064
test acc: top1 ->  91.87 ; top5 ->  99.22  and loss:  71.23672460019588
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.13808897209673887
test acc: top1 ->  91.89 ; top5 ->  99.25  and loss:  70.6319915279746
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.16014736623037606
test acc: top1 ->  91.88 ; top5 ->  99.23  and loss:  70.04210309684277
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.164875375747215
test acc: top1 ->  92.0 ; top5 ->  99.22  and loss:  70.40390732884407
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  47 / 512 , inc:  4
---------------- start layer  13  ---------------
adv train loss:  -16323.822975158691 , diff:  16323.822975158691
adv train loss:  -30199.340072631836 , diff:  13875.517097473145
adv train loss:  -42306.800720214844 , diff:  12107.460647583008
adv train loss:  -53873.60516357422 , diff:  11566.804443359375
adv train loss:  -65232.685485839844 , diff:  11359.080322265625
adv train loss:  -76468.45922851562 , diff:  11235.773742675781
adv train loss:  -87605.41076660156 , diff:  11136.951538085938
adv train loss:  -98708.11883544922 , diff:  11102.708068847656
adv train loss:  -109745.81457519531 , diff:  11037.695739746094
adv train loss:  -120736.53503417969 , diff:  10990.720458984375
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  8
test acc: top1 ->  25.66 ; top5 ->  78.15  and loss:  581.5390119552612
forward train acc: top1 ->  77.3020000024414 ; top5 ->  97.386  and loss:  106.75176033377647
test acc: top1 ->  89.4 ; top5 ->  97.81  and loss:  53.533963441848755
forward train acc: top1 ->  99.512 ; top5 ->  99.994  and loss:  8.105463843792677
test acc: top1 ->  90.04 ; top5 ->  97.78  and loss:  49.632848247885704
forward train acc: top1 ->  99.61599997558594 ; top5 ->  99.994  and loss:  4.807913821190596
test acc: top1 ->  90.2 ; top5 ->  97.77  and loss:  50.20437724888325
forward train acc: top1 ->  99.69799997558594 ; top5 ->  99.996  and loss:  3.349545309320092
test acc: top1 ->  90.28 ; top5 ->  97.78  and loss:  51.69318874180317
forward train acc: top1 ->  99.722 ; top5 ->  99.996  and loss:  2.5253103487193584
test acc: top1 ->  90.52 ; top5 ->  97.65  and loss:  53.29184739291668
forward train acc: top1 ->  99.74799997558594 ; top5 ->  100.0  and loss:  2.0584758520126343
test acc: top1 ->  90.54 ; top5 ->  97.67  and loss:  53.1441653072834
forward train acc: top1 ->  99.75199997558593 ; top5 ->  100.0  and loss:  1.9114927388727665
test acc: top1 ->  90.51 ; top5 ->  97.67  and loss:  54.04862368106842
forward train acc: top1 ->  99.80799997802734 ; top5 ->  99.998  and loss:  1.6438851170241833
test acc: top1 ->  90.63 ; top5 ->  97.68  and loss:  54.830188140273094
forward train acc: top1 ->  99.82 ; top5 ->  100.0  and loss:  1.4544667648151517
test acc: top1 ->  90.63 ; top5 ->  97.66  and loss:  55.134175926446915
forward train acc: top1 ->  99.83399997558594 ; top5 ->  100.0  and loss:  1.3150810077786446
test acc: top1 ->  90.76 ; top5 ->  97.62  and loss:  56.021223306655884
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  9 / 512 , inc:  1
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  8
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.546875  ==>  140 / 256 , inc:  2
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.251953125  ==>  129 / 512 , inc:  16
layer  9  :  0.015625  ==>  8 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.0078125  ==>  4 / 512 , inc:  1
layer  12  :  0.091796875  ==>  47 / 512 , inc:  2
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [0.9852612533569336, 1.7515755615234374, 0.9852612533569336, 0.9852612533569336, 0.9852612533569336, 0.9852612533569336, 1.313681671142578, 0.06157882833480835, 0.8757877807617187, 1.167717041015625, 1.4778918800354004, 0.8757877807617187, 0.9852612533569336, 14.0126044921875]  wait [4, 0, 4, 4, 4, 4, 0, 3, 2, 2, 4, 2, 4, 3]  inc [1, 8, 1, 1, 1, 1, 2, 1, 16, 1, 1, 1, 2, 1]  tol: 4
$$$$$$$$$$$$$ epoch  57  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
adv train loss:  -43.83117488026619 , diff:  43.83117488026619
adv train loss:  -44.30020225048065 , diff:  0.4690273702144623
adv train loss:  -44.361716985702515 , diff:  0.06151473522186279
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  36
test acc: top1 ->  15.55 ; top5 ->  63.52  and loss:  650.3954520225525
forward train acc: top1 ->  98.82199997558594 ; top5 ->  99.972  and loss:  5.444011403247714
test acc: top1 ->  90.74 ; top5 ->  98.99  and loss:  76.35533118993044
forward train acc: top1 ->  99.36799997802734 ; top5 ->  99.996  and loss:  2.3827940532937646
test acc: top1 ->  90.97 ; top5 ->  99.01  and loss:  69.59825184568763
forward train acc: top1 ->  99.51200000244141 ; top5 ->  99.998  and loss:  1.630395658314228
test acc: top1 ->  91.14 ; top5 ->  99.14  and loss:  65.84194658696651
forward train acc: top1 ->  99.52199997558594 ; top5 ->  100.0  and loss:  1.467390375211835
test acc: top1 ->  91.09 ; top5 ->  99.18  and loss:  62.10581151396036
forward train acc: top1 ->  99.582 ; top5 ->  99.994  and loss:  1.2971942245494574
test acc: top1 ->  91.15 ; top5 ->  99.13  and loss:  62.844580605626106
forward train acc: top1 ->  99.606 ; top5 ->  99.994  and loss:  1.2278083933051676
test acc: top1 ->  91.13 ; top5 ->  99.23  and loss:  59.65696918219328
forward train acc: top1 ->  99.708 ; top5 ->  100.0  and loss:  0.9464393467642367
test acc: top1 ->  91.23 ; top5 ->  99.15  and loss:  59.475273391231894
forward train acc: top1 ->  99.68400000244141 ; top5 ->  100.0  and loss:  0.9732074569910765
test acc: top1 ->  91.3 ; top5 ->  99.17  and loss:  58.140750512480736
forward train acc: top1 ->  99.71199997558594 ; top5 ->  99.998  and loss:  0.8878365429118276
test acc: top1 ->  91.27 ; top5 ->  99.22  and loss:  58.64066394418478
forward train acc: top1 ->  99.69599997558593 ; top5 ->  99.998  and loss:  0.8791805021464825
test acc: top1 ->  91.21 ; top5 ->  99.27  and loss:  57.56261507421732
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  44 / 64 , inc:  8
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
adv train loss:  -451.4582123244181 , diff:  451.4582123244181
adv train loss:  -1688.9218435287476 , diff:  1237.4636312043294
adv train loss:  -1757.3943252563477 , diff:  68.4724817276001
adv train loss:  -1752.8126792907715 , diff:  4.581645965576172
adv train loss:  -1793.5321292877197 , diff:  40.71944999694824
adv train loss:  -1809.9247188568115 , diff:  16.392589569091797
adv train loss:  -1841.8008728027344 , diff:  31.87615394592285
adv train loss:  -1863.7927227020264 , diff:  21.991849899291992
adv train loss:  -1859.7389850616455 , diff:  4.053737640380859
adv train loss:  -1864.6747360229492 , diff:  4.935750961303711
************ all values are small in this layer **********
layer  6  adv train finish, try to retain  138
test acc: top1 ->  5.3 ; top5 ->  48.28  and loss:  3239614.8671875
forward train acc: top1 ->  99.808 ; top5 ->  100.0  and loss:  0.5650454645510763
test acc: top1 ->  91.56 ; top5 ->  99.06  and loss:  59.947732135653496
forward train acc: top1 ->  99.878 ; top5 ->  100.0  and loss:  0.3439096936490387
test acc: top1 ->  91.7 ; top5 ->  99.26  and loss:  58.50349925458431
forward train acc: top1 ->  99.918 ; top5 ->  99.998  and loss:  0.2706823193002492
test acc: top1 ->  91.69 ; top5 ->  99.22  and loss:  60.65180980414152
forward train acc: top1 ->  99.908 ; top5 ->  100.0  and loss:  0.29347600892651826
test acc: top1 ->  91.87 ; top5 ->  99.23  and loss:  62.265578627586365
forward train acc: top1 ->  99.90199997558594 ; top5 ->  100.0  and loss:  0.268175158649683
test acc: top1 ->  91.75 ; top5 ->  99.33  and loss:  61.68976667523384
forward train acc: top1 ->  99.942 ; top5 ->  100.0  and loss:  0.15763432113453746
test acc: top1 ->  91.76 ; top5 ->  99.25  and loss:  63.848015516996384
forward train acc: top1 ->  99.94399997558594 ; top5 ->  100.0  and loss:  0.1642011827789247
test acc: top1 ->  91.86 ; top5 ->  99.23  and loss:  65.77081352472305
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.15263272568699904
test acc: top1 ->  91.76 ; top5 ->  99.25  and loss:  66.32589095830917
forward train acc: top1 ->  99.918 ; top5 ->  100.0  and loss:  0.20607895723514957
test acc: top1 ->  91.72 ; top5 ->  99.19  and loss:  65.81171511113644
forward train acc: top1 ->  99.94599997558593 ; top5 ->  100.0  and loss:  0.17142111994326115
test acc: top1 ->  91.7 ; top5 ->  99.31  and loss:  64.9614985883236
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  140 / 256 , inc:  2
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
adv train loss:  -0.5110907263297122 , diff:  0.5110907263297122
adv train loss:  -0.4921754927490838 , diff:  0.01891523358062841
adv train loss:  -0.6091786632314324 , diff:  0.11700317048234865
adv train loss:  -0.5130579085089266 , diff:  0.09612075472250581
adv train loss:  -0.4413922108942643 , diff:  0.07166569761466235
adv train loss:  -0.5466223687399179 , diff:  0.1052301578456536
adv train loss:  -0.49773584958165884 , diff:  0.048886519158259034
adv train loss:  -0.5633114201482385 , diff:  0.0655755705665797
adv train loss:  -0.46599633479490876 , diff:  0.09731508535332978
adv train loss:  -0.490321536490228 , diff:  0.024325201695319265
layer  8  adv train finish, try to retain  383
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -1974.3560905456543 , diff:  1974.3560905456543
adv train loss:  -1982.9305305480957 , diff:  8.574440002441406
adv train loss:  -1983.4139385223389 , diff:  0.48340797424316406
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  7
test acc: top1 ->  34.08 ; top5 ->  87.24  and loss:  641.8283314704895
forward train acc: top1 ->  98.67199997802734 ; top5 ->  99.886  and loss:  5.339994594454765
test acc: top1 ->  91.21 ; top5 ->  98.33  and loss:  58.46002212166786
forward train acc: top1 ->  99.8340000024414 ; top5 ->  100.0  and loss:  0.7368787052109838
test acc: top1 ->  91.45 ; top5 ->  98.43  and loss:  58.772352397441864
forward train acc: top1 ->  99.882 ; top5 ->  100.0  and loss:  0.4929440503474325
test acc: top1 ->  91.58 ; top5 ->  98.46  and loss:  60.556370958685875
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.3204197541344911
test acc: top1 ->  91.71 ; top5 ->  98.5  and loss:  62.24158124625683
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.3151214820682071
test acc: top1 ->  91.59 ; top5 ->  98.56  and loss:  62.43279004842043
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.28638483030954376
test acc: top1 ->  91.6 ; top5 ->  98.55  and loss:  63.884803988039494
forward train acc: top1 ->  99.908 ; top5 ->  100.0  and loss:  0.2989584523020312
test acc: top1 ->  91.65 ; top5 ->  98.59  and loss:  63.84369622915983
forward train acc: top1 ->  99.92999997558594 ; top5 ->  100.0  and loss:  0.24130944861099124
test acc: top1 ->  91.69 ; top5 ->  98.67  and loss:  64.48727129399776
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.16712888501933776
test acc: top1 ->  91.71 ; top5 ->  98.67  and loss:  64.82813998311758
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.1876702219597064
test acc: top1 ->  91.7 ; top5 ->  98.73  and loss:  65.08681904524565
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  8 / 512 , inc:  1
---------------- start layer  10  ---------------
### skip layer  10 wait:  4  ###
---------------- start layer  11  ---------------
adv train loss:  -517.0948061943054 , diff:  517.0948061943054
adv train loss:  -515.0771493911743 , diff:  2.0176568031311035
adv train loss:  -516.0346207618713 , diff:  0.9574713706970215
layer  11  adv train finish, try to retain  469
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
### skip layer  13 wait:  3  ###
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  4
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.546875  ==>  140 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.251953125  ==>  129 / 512 , inc:  16
layer  9  :  0.015625  ==>  8 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.0078125  ==>  4 / 512 , inc:  1
layer  12  :  0.091796875  ==>  47 / 512 , inc:  2
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [0.9852612533569336, 1.313681671142578, 0.9852612533569336, 0.9852612533569336, 0.9852612533569336, 0.9852612533569336, 0.9852612533569336, 0.06157882833480835, 1.7515755615234374, 0.8757877807617187, 1.4778918800354004, 1.7515755615234374, 0.9852612533569336, 14.0126044921875]  wait [3, 2, 3, 3, 3, 3, 2, 2, 2, 4, 3, 2, 3, 2]  inc [1, 4, 1, 1, 1, 1, 1, 1, 16, 1, 1, 1, 2, 1]  tol: 4
$$$$$$$$$$$$$ epoch  58  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -0.6537119932472706 , diff:  0.6537119932472706
adv train loss:  -0.7810420570895076 , diff:  0.127330063842237
adv train loss:  -0.7436870690435171 , diff:  0.03735498804599047
adv train loss:  -0.7887127334252 , diff:  0.04502566438168287
adv train loss:  -0.6319456603378057 , diff:  0.15676707308739424
adv train loss:  -0.7541855284944177 , diff:  0.12223986815661192
adv train loss:  -0.6992304757004604 , diff:  0.05495505279395729
adv train loss:  -0.7469598166644573 , diff:  0.04772934096399695
adv train loss:  -0.8156142812222242 , diff:  0.06865446455776691
adv train loss:  -0.6854404828045517 , diff:  0.13017379841767251
layer  0  adv train finish, try to retain  64
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -0.6299498314037919 , diff:  0.6299498314037919
adv train loss:  -0.7432656143791974 , diff:  0.11331578297540545
adv train loss:  -0.6982319675153121 , diff:  0.045033646863885224
adv train loss:  -0.7995754424482584 , diff:  0.10134347493294626
adv train loss:  -0.7627707922365516 , diff:  0.03680465021170676
adv train loss:  -0.7200368628837168 , diff:  0.04273392935283482
adv train loss:  -0.7818492229562253 , diff:  0.061812360072508454
adv train loss:  -0.6912436857819557 , diff:  0.09060553717426956
adv train loss:  -0.6342669703881256 , diff:  0.05697671539383009
adv train loss:  -0.6900199512019753 , diff:  0.05575298081384972
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  40
test acc: top1 ->  23.03 ; top5 ->  71.35  and loss:  829.114589214325
forward train acc: top1 ->  99.816 ; top5 ->  100.0  and loss:  0.6146698422671761
test acc: top1 ->  91.6 ; top5 ->  99.12  and loss:  75.65527105331421
forward train acc: top1 ->  99.878 ; top5 ->  100.0  and loss:  0.35185155432191095
test acc: top1 ->  91.81 ; top5 ->  99.13  and loss:  73.60104233026505
forward train acc: top1 ->  99.888 ; top5 ->  100.0  and loss:  0.368101132218726
test acc: top1 ->  91.62 ; top5 ->  99.12  and loss:  72.96709518134594
forward train acc: top1 ->  99.89 ; top5 ->  100.0  and loss:  0.33861576768686064
test acc: top1 ->  91.78 ; top5 ->  99.19  and loss:  71.12468718737364
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.28626535169314593
test acc: top1 ->  91.75 ; top5 ->  99.21  and loss:  70.57602141797543
forward train acc: top1 ->  99.94 ; top5 ->  100.0  and loss:  0.2130515407770872
test acc: top1 ->  91.7 ; top5 ->  99.23  and loss:  69.1512714959681
forward train acc: top1 ->  99.936 ; top5 ->  100.0  and loss:  0.22187378458329476
test acc: top1 ->  91.67 ; top5 ->  99.23  and loss:  69.453867174685
forward train acc: top1 ->  99.928 ; top5 ->  100.0  and loss:  0.21619371382985264
test acc: top1 ->  91.9 ; top5 ->  99.17  and loss:  68.87492268532515
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.17311508860439062
test acc: top1 ->  91.73 ; top5 ->  99.16  and loss:  69.62582805007696
forward train acc: top1 ->  99.9360000024414 ; top5 ->  100.0  and loss:  0.22753111692145467
test acc: top1 ->  91.7 ; top5 ->  99.17  and loss:  69.69119000062346
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  44 / 64 , inc:  4
---------------- start layer  2  ---------------
adv train loss:  -0.18972729099914432 , diff:  0.18972729099914432
adv train loss:  -0.2646422021789476 , diff:  0.07491491117980331
adv train loss:  -0.1671701818704605 , diff:  0.09747202030848712
adv train loss:  -0.21731969597749412 , diff:  0.05014951410703361
adv train loss:  -0.2858698428608477 , diff:  0.06855014688335359
adv train loss:  -0.19314060923352372 , diff:  0.09272923362732399
adv train loss:  -0.17182157209026627 , diff:  0.021319037143257447
adv train loss:  -0.16632880676479544 , diff:  0.0054927653254708275
layer  2  adv train finish, try to retain  118
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -0.21415697317570448 , diff:  0.21415697317570448
adv train loss:  -0.18851693614851683 , diff:  0.025640037027187645
adv train loss:  -0.23828521533869207 , diff:  0.049768279190175235
adv train loss:  -0.20534041279461235 , diff:  0.03294480254407972
adv train loss:  -0.21232234267517924 , diff:  0.006981929880566895
layer  3  adv train finish, try to retain  126
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -0.19427820458076894 , diff:  0.19427820458076894
adv train loss:  -0.21440709891612642 , diff:  0.020128894335357472
adv train loss:  -0.16921634769823868 , diff:  0.04519075121788774
adv train loss:  -0.16497454838827252 , diff:  0.004241799309966154
layer  4  adv train finish, try to retain  233
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -0.19545645220205188 , diff:  0.19545645220205188
adv train loss:  -0.1849131922936067 , diff:  0.01054325990844518
adv train loss:  -0.21647930657491088 , diff:  0.03156611428130418
adv train loss:  -0.17904853244544938 , diff:  0.0374307741294615
adv train loss:  -0.1564792347198818 , diff:  0.022569297725567594
adv train loss:  -0.18648875341750681 , diff:  0.030009518697625026
adv train loss:  -0.2152560664981138 , diff:  0.028767313080606982
adv train loss:  -0.14722405828069896 , diff:  0.06803200821741484
adv train loss:  -0.2214376019546762 , diff:  0.07421354367397726
adv train loss:  -0.2537990957935108 , diff:  0.032361493838834576
layer  5  adv train finish, try to retain  253
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -0.22522191224561539 , diff:  0.22522191224561539
adv train loss:  -0.22261113076820038 , diff:  0.002610781477415003
layer  6  adv train finish, try to retain  226
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -0.1533955570193939 , diff:  0.1533955570193939
adv train loss:  -0.20856363512575626 , diff:  0.05516807810636237
adv train loss:  -0.23181563802063465 , diff:  0.023252002894878387
adv train loss:  -0.1566442459297832 , diff:  0.07517139209085144
adv train loss:  -0.16010607220232487 , diff:  0.003461826272541657
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  252
test acc: top1 ->  37.77 ; top5 ->  74.1  and loss:  361.26326847076416
forward train acc: top1 ->  99.4700000048828 ; top5 ->  99.996  and loss:  1.650110516231507
test acc: top1 ->  90.89 ; top5 ->  99.02  and loss:  66.94132673740387
forward train acc: top1 ->  99.70799997558593 ; top5 ->  100.0  and loss:  0.8971751546487212
test acc: top1 ->  91.19 ; top5 ->  99.15  and loss:  63.71678638458252
forward train acc: top1 ->  99.804 ; top5 ->  100.0  and loss:  0.5817476599477232
test acc: top1 ->  91.27 ; top5 ->  99.21  and loss:  63.86038565635681
forward train acc: top1 ->  99.8 ; top5 ->  100.0  and loss:  0.5416942674200982
test acc: top1 ->  91.32 ; top5 ->  99.09  and loss:  66.74792397022247
forward train acc: top1 ->  99.834 ; top5 ->  100.0  and loss:  0.48536240262910724
test acc: top1 ->  91.45 ; top5 ->  99.11  and loss:  65.2329291626811
forward train acc: top1 ->  99.848 ; top5 ->  100.0  and loss:  0.4456802586792037
test acc: top1 ->  91.52 ; top5 ->  99.13  and loss:  66.53473441302776
forward train acc: top1 ->  99.85199997558594 ; top5 ->  100.0  and loss:  0.4098054524511099
test acc: top1 ->  91.36 ; top5 ->  99.13  and loss:  66.77738577872515
forward train acc: top1 ->  99.86 ; top5 ->  100.0  and loss:  0.4291804067324847
test acc: top1 ->  91.5 ; top5 ->  99.21  and loss:  67.54344538599253
forward train acc: top1 ->  99.888 ; top5 ->  100.0  and loss:  0.3563709940935951
test acc: top1 ->  91.54 ; top5 ->  99.18  and loss:  67.41423460841179
forward train acc: top1 ->  99.9 ; top5 ->  100.0  and loss:  0.32031617406755686
test acc: top1 ->  91.59 ; top5 ->  99.22  and loss:  69.19477578252554
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  253 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -3.3761171833612025 , diff:  3.3761171833612025
adv train loss:  -3.2598485187627375 , diff:  0.11626866459846497
adv train loss:  -3.4128253548406065 , diff:  0.15297683607786894
adv train loss:  -3.165814060717821 , diff:  0.24701129412278533
adv train loss:  -3.4321266100741923 , diff:  0.26631254935637116
adv train loss:  -3.2463485347107053 , diff:  0.185778075363487
adv train loss:  -3.307829700410366 , diff:  0.06148116569966078
adv train loss:  -3.1487112515605986 , diff:  0.15911844884976745
adv train loss:  -3.501481377519667 , diff:  0.35277012595906854
adv train loss:  -3.158144590444863 , diff:  0.3433367870748043
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  113
test acc: top1 ->  10.69 ; top5 ->  69.53  and loss:  6590410.546875
forward train acc: top1 ->  97.5640000024414 ; top5 ->  99.924  and loss:  11.568389544263482
test acc: top1 ->  91.23 ; top5 ->  99.04  and loss:  68.91492592543364
forward train acc: top1 ->  99.77399997558594 ; top5 ->  100.0  and loss:  0.7113500209525228
test acc: top1 ->  91.58 ; top5 ->  99.0  and loss:  68.07040300220251
forward train acc: top1 ->  99.812 ; top5 ->  100.0  and loss:  0.5517367572756484
test acc: top1 ->  91.67 ; top5 ->  99.12  and loss:  66.90219342708588
forward train acc: top1 ->  99.868 ; top5 ->  100.0  and loss:  0.434238001704216
test acc: top1 ->  91.78 ; top5 ->  99.12  and loss:  67.35725793242455
forward train acc: top1 ->  99.90199997558594 ; top5 ->  100.0  and loss:  0.34714793879538774
test acc: top1 ->  91.68 ; top5 ->  99.14  and loss:  67.72415570169687
forward train acc: top1 ->  99.9 ; top5 ->  100.0  and loss:  0.3109050540952012
test acc: top1 ->  91.74 ; top5 ->  99.2  and loss:  68.09885400533676
forward train acc: top1 ->  99.918 ; top5 ->  100.0  and loss:  0.2719114834908396
test acc: top1 ->  91.71 ; top5 ->  99.12  and loss:  68.11227351427078
forward train acc: top1 ->  99.90199997558594 ; top5 ->  100.0  and loss:  0.29301144927740097
test acc: top1 ->  91.88 ; top5 ->  99.12  and loss:  67.3852561712265
forward train acc: top1 ->  99.898 ; top5 ->  100.0  and loss:  0.306557250325568
test acc: top1 ->  91.93 ; top5 ->  99.13  and loss:  67.63886705040932
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.18166673579253256
test acc: top1 ->  91.82 ; top5 ->  99.15  and loss:  68.39578084647655
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  129 / 512 , inc:  16
---------------- start layer  9  ---------------
adv train loss:  -48.98423320055008 , diff:  48.98423320055008
adv train loss:  -48.82158726453781 , diff:  0.16264593601226807
adv train loss:  -48.60481587052345 , diff:  0.21677139401435852
adv train loss:  -49.02369636297226 , diff:  0.41888049244880676
adv train loss:  -48.50491064786911 , diff:  0.5187857151031494
adv train loss:  -49.073540419340134 , diff:  0.5686297714710236
adv train loss:  -48.79615667462349 , diff:  0.2773837447166443
adv train loss:  -48.88002464175224 , diff:  0.08386796712875366
layer  9  adv train finish, try to retain  493
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -347.2002215385437 , diff:  347.2002215385437
adv train loss:  -346.8772871494293 , diff:  0.3229343891143799
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  10
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  129081.07482910156
forward train acc: top1 ->  93.31000000244141 ; top5 ->  99.944  and loss:  24.74167374148965
test acc: top1 ->  80.23 ; top5 ->  98.32  and loss:  90.79292520880699
forward train acc: top1 ->  99.782 ; top5 ->  100.0  and loss:  1.0363177680410445
test acc: top1 ->  91.53 ; top5 ->  98.98  and loss:  45.78613770008087
forward train acc: top1 ->  99.88999997558594 ; top5 ->  100.0  and loss:  0.6006898693740368
test acc: top1 ->  91.63 ; top5 ->  99.08  and loss:  46.96755562722683
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.3462872204836458
test acc: top1 ->  91.55 ; top5 ->  99.09  and loss:  49.44525174051523
forward train acc: top1 ->  99.942 ; top5 ->  100.0  and loss:  0.30420636758208275
test acc: top1 ->  91.75 ; top5 ->  99.1  and loss:  49.52427763864398
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.2739434087416157
test acc: top1 ->  91.67 ; top5 ->  99.05  and loss:  51.01133258640766
forward train acc: top1 ->  99.94 ; top5 ->  100.0  and loss:  0.2415074041346088
test acc: top1 ->  91.85 ; top5 ->  99.13  and loss:  51.74251680448651
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.19135461217956617
test acc: top1 ->  91.69 ; top5 ->  99.13  and loss:  52.49635509029031
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.21601648605428636
test acc: top1 ->  91.84 ; top5 ->  99.08  and loss:  53.14424969628453
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.18419783073477447
test acc: top1 ->  91.78 ; top5 ->  99.15  and loss:  54.087547082453966
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  11 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -1423.68092918396 , diff:  1423.68092918396
adv train loss:  -1526.2501153945923 , diff:  102.56918621063232
adv train loss:  -1623.308832168579 , diff:  97.05871677398682
adv train loss:  -1624.0127029418945 , diff:  0.7038707733154297
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  3
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  50629.591857910156
forward train acc: top1 ->  97.404 ; top5 ->  99.986  and loss:  10.343187529360875
test acc: top1 ->  91.29 ; top5 ->  98.54  and loss:  73.91380084306002
forward train acc: top1 ->  99.89999997558594 ; top5 ->  100.0  and loss:  0.4025595858693123
test acc: top1 ->  91.69 ; top5 ->  98.36  and loss:  72.67237874120474
forward train acc: top1 ->  99.918 ; top5 ->  100.0  and loss:  0.2682577168161515
test acc: top1 ->  91.83 ; top5 ->  98.46  and loss:  71.64988843351603
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.21030055708251894
test acc: top1 ->  91.74 ; top5 ->  98.47  and loss:  72.1756915524602
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.1510789815802127
test acc: top1 ->  91.87 ; top5 ->  98.53  and loss:  71.65355411916971
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.16467203915817663
test acc: top1 ->  91.93 ; top5 ->  98.5  and loss:  71.70489586889744
forward train acc: top1 ->  99.94999997558594 ; top5 ->  100.0  and loss:  0.18062849482521415
test acc: top1 ->  91.97 ; top5 ->  98.53  and loss:  71.12598365545273
forward train acc: top1 ->  99.96799997558594 ; top5 ->  100.0  and loss:  0.14351298846304417
test acc: top1 ->  91.95 ; top5 ->  98.54  and loss:  71.49463607370853
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.12425328095559962
test acc: top1 ->  91.87 ; top5 ->  98.51  and loss:  71.24327788501978
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.10993383119057398
test acc: top1 ->  91.92 ; top5 ->  98.51  and loss:  71.36306305229664
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  4 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -249.2693670988083 , diff:  249.2693670988083
adv train loss:  -248.91958796977997 , diff:  0.3497791290283203
layer  12  adv train finish, try to retain  468
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -12471.036109924316 , diff:  12471.036109924316
adv train loss:  -20270.89402770996 , diff:  7799.8579177856445
adv train loss:  -27858.31605529785 , diff:  7587.422027587891
adv train loss:  -35449.7668762207 , diff:  7591.450820922852
adv train loss:  -43037.81057739258 , diff:  7588.043701171875
adv train loss:  -50608.07843017578 , diff:  7570.267852783203
adv train loss:  -58172.54650878906 , diff:  7564.468078613281
adv train loss:  -65708.34185791016 , diff:  7535.795349121094
adv train loss:  -73273.45977783203 , diff:  7565.117919921875
adv train loss:  -80828.14379882812 , diff:  7554.684020996094
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  8
test acc: top1 ->  10.0 ; top5 ->  59.02  and loss:  1856.1964197158813
forward train acc: top1 ->  40.93199998535156 ; top5 ->  74.43000001953125  and loss:  612.4039248228073
test acc: top1 ->  63.19 ; top5 ->  94.17  and loss:  184.17617213726044
forward train acc: top1 ->  87.52599997802734 ; top5 ->  99.662  and loss:  48.315938264131546
test acc: top1 ->  87.84 ; top5 ->  97.07  and loss:  67.83357873558998
forward train acc: top1 ->  98.93400000488282 ; top5 ->  99.992  and loss:  15.609524182975292
test acc: top1 ->  88.93 ; top5 ->  97.27  and loss:  61.81795424222946
forward train acc: top1 ->  99.47400000488281 ; top5 ->  100.0  and loss:  9.548598416149616
test acc: top1 ->  89.35 ; top5 ->  97.26  and loss:  58.03460595011711
forward train acc: top1 ->  99.624 ; top5 ->  100.0  and loss:  5.774422459304333
test acc: top1 ->  89.84 ; top5 ->  97.49  and loss:  54.52926328778267
forward train acc: top1 ->  99.64 ; top5 ->  100.0  and loss:  4.290441617369652
test acc: top1 ->  89.95 ; top5 ->  97.53  and loss:  53.729542940855026
forward train acc: top1 ->  99.702 ; top5 ->  100.0  and loss:  3.623668357729912
test acc: top1 ->  90.03 ; top5 ->  97.63  and loss:  54.178555846214294
forward train acc: top1 ->  99.67399997558594 ; top5 ->  99.996  and loss:  3.3357689045369625
test acc: top1 ->  90.05 ; top5 ->  97.65  and loss:  53.92753764986992
forward train acc: top1 ->  99.778 ; top5 ->  100.0  and loss:  2.859396865591407
test acc: top1 ->  90.21 ; top5 ->  97.71  and loss:  53.917356595396996
forward train acc: top1 ->  99.776 ; top5 ->  100.0  and loss:  2.5512561444193125
test acc: top1 ->  90.26 ; top5 ->  97.73  and loss:  54.05226056277752
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  9 / 512 , inc:  1
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  2
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.546875  ==>  140 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.251953125  ==>  129 / 512 , inc:  8
layer  9  :  0.015625  ==>  8 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.0078125  ==>  4 / 512 , inc:  1
layer  12  :  0.091796875  ==>  47 / 512 , inc:  2
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [1.9705225067138672, 0.9852612533569336, 1.9705225067138672, 1.9705225067138672, 1.9705225067138672, 1.9705225067138672, 1.9705225067138672, 0.04618412125110626, 1.313681671142578, 1.7515755615234374, 1.1084189100265502, 1.313681671142578, 1.9705225067138672, 10.509453369140624]  wait [1, 2, 1, 1, 1, 1, 0, 2, 2, 2, 3, 2, 1, 2]  inc [1, 2, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 2, 1]  tol: 5
$$$$$$$$$$$$$ epoch  59  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1818.443591117859 , diff:  1818.443591117859
adv train loss:  -1832.8694667816162 , diff:  14.425875663757324
adv train loss:  -1845.5418701171875 , diff:  12.672403335571289
adv train loss:  -1848.2653045654297 , diff:  2.7234344482421875
adv train loss:  -1839.6813640594482 , diff:  8.583940505981445
adv train loss:  -1824.6069927215576 , diff:  15.074371337890625
adv train loss:  -1828.632583618164 , diff:  4.025590896606445
adv train loss:  -1821.0830669403076 , diff:  7.549516677856445
adv train loss:  -1824.427438735962 , diff:  3.344371795654297
adv train loss:  -1818.336404800415 , diff:  6.091033935546875
layer  0  adv train finish, try to retain  3
test acc: top1 ->  10.0 ; top5 ->  49.91  and loss:  1480.756615638733
forward train acc: top1 ->  13.109999996948241 ; top5 ->  56.99800000854492  and loss:  582.1126692295074
test acc: top1 ->  9.51 ; top5 ->  51.05  and loss:  295.9507131576538
forward train acc: top1 ->  14.927999999084472 ; top5 ->  59.64599999267578  and loss:  233.8873062133789
test acc: top1 ->  14.91 ; top5 ->  61.64  and loss:  229.6825077533722
forward train acc: top1 ->  16.336000004882813 ; top5 ->  62.60600001708984  and loss:  221.5569772720337
test acc: top1 ->  16.68 ; top5 ->  64.81  and loss:  224.22441577911377
forward train acc: top1 ->  17.697999993286132 ; top5 ->  65.38599998535156  and loss:  217.2137348651886
test acc: top1 ->  18.15 ; top5 ->  67.4  and loss:  219.73129892349243
forward train acc: top1 ->  19.161999996948243 ; top5 ->  68.53399998291016  and loss:  213.7111246585846
test acc: top1 ->  19.95 ; top5 ->  70.98  and loss:  215.2325885295868
forward train acc: top1 ->  20.338000001831055 ; top5 ->  71.81400001220703  and loss:  209.90028500556946
test acc: top1 ->  21.49 ; top5 ->  74.11  and loss:  211.59776735305786
forward train acc: top1 ->  21.347999993896483 ; top5 ->  73.68199999511718  and loss:  207.2842960357666
test acc: top1 ->  21.54 ; top5 ->  75.19  and loss:  209.25266218185425
forward train acc: top1 ->  21.89600000366211 ; top5 ->  75.09999998779297  and loss:  204.6451451778412
test acc: top1 ->  22.61 ; top5 ->  76.53  and loss:  206.3231246471405
forward train acc: top1 ->  22.953999999389648 ; top5 ->  76.49199998535157  and loss:  201.42406034469604
test acc: top1 ->  23.6 ; top5 ->  78.27  and loss:  203.05417454242706
forward train acc: top1 ->  23.867999998168944 ; top5 ->  77.814  and loss:  198.9346820116043
test acc: top1 ->  24.41 ; top5 ->  78.95  and loss:  201.31481778621674
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -154.69043505191803 , diff:  154.69043505191803
adv train loss:  -154.45855247974396 , diff:  0.23188257217407227
layer  1  adv train finish, try to retain  61
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -176.59123075008392 , diff:  176.59123075008392
adv train loss:  -222.26557040214539 , diff:  45.67433965206146
adv train loss:  -237.34470963478088 , diff:  15.079139232635498
adv train loss:  -237.24047303199768 , diff:  0.10423660278320312
************ all values are small in this layer **********
layer  2  adv train finish, try to retain  99
test acc: top1 ->  15.36 ; top5 ->  63.58  and loss:  305.1012473106384
forward train acc: top1 ->  95.80399997802735 ; top5 ->  99.916  and loss:  26.88138935342431
test acc: top1 ->  90.34 ; top5 ->  99.12  and loss:  38.11045853048563
forward train acc: top1 ->  99.438 ; top5 ->  99.996  and loss:  1.9787214947864413
test acc: top1 ->  90.7 ; top5 ->  99.17  and loss:  45.25080571323633
forward train acc: top1 ->  99.5880000024414 ; top5 ->  100.0  and loss:  1.3335478622466326
test acc: top1 ->  91.05 ; top5 ->  99.15  and loss:  49.94109809398651
forward train acc: top1 ->  99.658 ; top5 ->  99.998  and loss:  1.0940807787701488
test acc: top1 ->  91.11 ; top5 ->  99.14  and loss:  52.04514066129923
forward train acc: top1 ->  99.698 ; top5 ->  99.994  and loss:  0.9376959966029972
test acc: top1 ->  91.24 ; top5 ->  99.07  and loss:  55.31610353291035
forward train acc: top1 ->  99.718 ; top5 ->  100.0  and loss:  0.8927422850392759
test acc: top1 ->  91.18 ; top5 ->  99.11  and loss:  55.59344068169594
forward train acc: top1 ->  99.7060000024414 ; top5 ->  99.998  and loss:  0.8752172235399485
test acc: top1 ->  91.29 ; top5 ->  99.13  and loss:  55.165073335170746
forward train acc: top1 ->  99.762 ; top5 ->  99.998  and loss:  0.6748263995687012
test acc: top1 ->  91.37 ; top5 ->  99.15  and loss:  56.57634823024273
forward train acc: top1 ->  99.75599997558594 ; top5 ->  99.998  and loss:  0.7093933541327715
test acc: top1 ->  91.43 ; top5 ->  99.13  and loss:  56.70692877471447
forward train acc: top1 ->  99.76799997558594 ; top5 ->  99.998  and loss:  0.7349956268444657
test acc: top1 ->  91.23 ; top5 ->  99.19  and loss:  56.84457902610302
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -687.1047457903624 , diff:  687.1047457903624
adv train loss:  -1245.0350303649902 , diff:  557.9302845746279
adv train loss:  -1305.6762523651123 , diff:  60.64122200012207
adv train loss:  -1336.5912399291992 , diff:  30.914987564086914
adv train loss:  -1335.9980392456055 , diff:  0.59320068359375
adv train loss:  -1340.803258895874 , diff:  4.805219650268555
adv train loss:  -1345.2694654464722 , diff:  4.4662065505981445
adv train loss:  -1367.5648765563965 , diff:  22.295411109924316
adv train loss:  -1380.1140327453613 , diff:  12.549156188964844
adv train loss:  -1374.4742345809937 , diff:  5.639798164367676
layer  3  adv train finish, try to retain  23
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  1407.5027570724487
forward train acc: top1 ->  73.45199997070313 ; top5 ->  96.22799998535156  and loss:  97.26045548915863
test acc: top1 ->  72.85 ; top5 ->  96.47  and loss:  88.72814601659775
forward train acc: top1 ->  77.07200001708985 ; top5 ->  97.58399999023437  and loss:  69.7519741654396
test acc: top1 ->  74.7 ; top5 ->  97.23  and loss:  79.57910096645355
forward train acc: top1 ->  79.13599998291015 ; top5 ->  98.06400000976562  and loss:  62.708711087703705
test acc: top1 ->  76.39 ; top5 ->  97.47  and loss:  73.97927188873291
forward train acc: top1 ->  80.76599999511718 ; top5 ->  98.34400000732421  and loss:  57.65104320645332
test acc: top1 ->  77.6 ; top5 ->  97.78  and loss:  70.755789488554
forward train acc: top1 ->  81.40200000732422 ; top5 ->  98.49999998779298  and loss:  55.33822503685951
test acc: top1 ->  78.33 ; top5 ->  97.99  and loss:  68.15692123770714
forward train acc: top1 ->  82.60399997070313 ; top5 ->  98.7660000024414  and loss:  51.763817101716995
test acc: top1 ->  78.69 ; top5 ->  97.95  and loss:  66.94702333211899
forward train acc: top1 ->  83.01400001953125 ; top5 ->  98.81599997802735  and loss:  50.50872340798378
test acc: top1 ->  78.91 ; top5 ->  98.02  and loss:  65.90076911449432
forward train acc: top1 ->  83.16399998779296 ; top5 ->  98.83799998046875  and loss:  49.80126890540123
test acc: top1 ->  79.45 ; top5 ->  97.99  and loss:  64.80839991569519
forward train acc: top1 ->  83.65399999511719 ; top5 ->  98.89000000732422  and loss:  48.42972722649574
test acc: top1 ->  79.69 ; top5 ->  98.14  and loss:  63.757814794778824
forward train acc: top1 ->  84.19200001708984 ; top5 ->  98.88800000732422  and loss:  47.14830046892166
test acc: top1 ->  79.6 ; top5 ->  98.15  and loss:  63.36130425333977
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -63.23918169736862 , diff:  63.23918169736862
adv train loss:  -416.12195014953613 , diff:  352.8827684521675
adv train loss:  -528.9075212478638 , diff:  112.78557109832764
adv train loss:  -548.5499997138977 , diff:  19.642478466033936
adv train loss:  -574.6726698875427 , diff:  26.12267017364502
adv train loss:  -586.3584203720093 , diff:  11.685750484466553
adv train loss:  -586.7044906616211 , diff:  0.3460702896118164
adv train loss:  -588.9607677459717 , diff:  2.256277084350586
adv train loss:  -587.9328022003174 , diff:  1.0279655456542969
adv train loss:  -588.5229988098145 , diff:  0.5901966094970703
layer  4  adv train finish, try to retain  19
test acc: top1 ->  13.85 ; top5 ->  51.76  and loss:  591.0525312423706
forward train acc: top1 ->  66.57399999511719 ; top5 ->  95.80600000976563  and loss:  96.5442413687706
test acc: top1 ->  68.36 ; top5 ->  96.5  and loss:  93.29495340585709
forward train acc: top1 ->  71.79800000488281 ; top5 ->  97.31600000732422  and loss:  80.07256084680557
test acc: top1 ->  71.6 ; top5 ->  97.19  and loss:  83.42494660615921
forward train acc: top1 ->  74.28200000976562 ; top5 ->  97.84200000976563  and loss:  72.804682970047
test acc: top1 ->  73.28 ; top5 ->  97.34  and loss:  78.53197813034058
forward train acc: top1 ->  75.88999997070313 ; top5 ->  98.14399998046875  and loss:  67.89125669002533
test acc: top1 ->  74.86 ; top5 ->  97.72  and loss:  74.87429523468018
forward train acc: top1 ->  76.89599999755859 ; top5 ->  98.35599998046875  and loss:  64.79575926065445
test acc: top1 ->  75.8 ; top5 ->  97.8  and loss:  71.46174156665802
forward train acc: top1 ->  77.96399998535156 ; top5 ->  98.42399998291016  and loss:  61.846037566661835
test acc: top1 ->  76.31 ; top5 ->  97.9  and loss:  70.0806814134121
forward train acc: top1 ->  78.35999998046876 ; top5 ->  98.49199998291016  and loss:  60.89452451467514
test acc: top1 ->  76.5 ; top5 ->  97.97  and loss:  68.8331635594368
forward train acc: top1 ->  78.76400001953125 ; top5 ->  98.51399997802734  and loss:  59.38416773080826
test acc: top1 ->  76.82 ; top5 ->  97.95  and loss:  67.9809139072895
forward train acc: top1 ->  79.26999997558593 ; top5 ->  98.62999998046875  and loss:  58.075546979904175
test acc: top1 ->  77.16 ; top5 ->  98.01  and loss:  66.97563973069191
forward train acc: top1 ->  79.54400000976563 ; top5 ->  98.6200000024414  and loss:  57.944234132766724
test acc: top1 ->  77.57 ; top5 ->  98.06  and loss:  66.16006091237068
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -139.30539382249117 , diff:  139.30539382249117
adv train loss:  -771.292317867279 , diff:  631.9869240447879
adv train loss:  -837.9808053970337 , diff:  66.68848752975464
adv train loss:  -846.6174793243408 , diff:  8.636673927307129
adv train loss:  -854.1957998275757 , diff:  7.578320503234863
adv train loss:  -853.5962467193604 , diff:  0.599553108215332
adv train loss:  -853.6952381134033 , diff:  0.09899139404296875
adv train loss:  -848.1033554077148 , diff:  5.591882705688477
adv train loss:  -844.8534688949585 , diff:  3.2498865127563477
adv train loss:  -844.5033769607544 , diff:  0.35009193420410156
layer  5  adv train finish, try to retain  22
test acc: top1 ->  10.0 ; top5 ->  48.96  and loss:  774.8716158866882
forward train acc: top1 ->  79.82999998535156 ; top5 ->  98.84199997802735  and loss:  57.15554362535477
test acc: top1 ->  79.53 ; top5 ->  98.45  and loss:  62.87137421965599
forward train acc: top1 ->  84.35199998046875 ; top5 ->  99.38599997558593  and loss:  44.32510349154472
test acc: top1 ->  81.3 ; top5 ->  98.73  and loss:  58.081080108881
forward train acc: top1 ->  86.05399998046875 ; top5 ->  99.41000000244141  and loss:  39.22537353634834
test acc: top1 ->  82.51 ; top5 ->  98.91  and loss:  55.232097297906876
forward train acc: top1 ->  87.09199997314452 ; top5 ->  99.514  and loss:  36.259688436985016
test acc: top1 ->  83.33 ; top5 ->  98.95  and loss:  52.052552461624146
forward train acc: top1 ->  88.17199997070313 ; top5 ->  99.58599997558593  and loss:  33.306844502687454
test acc: top1 ->  84.0 ; top5 ->  98.97  and loss:  51.444910526275635
forward train acc: top1 ->  88.73800001464843 ; top5 ->  99.6320000024414  and loss:  31.879420816898346
test acc: top1 ->  84.18 ; top5 ->  98.95  and loss:  50.3303679227829
forward train acc: top1 ->  89.10800001708985 ; top5 ->  99.61399997558594  and loss:  30.98007383942604
test acc: top1 ->  84.59 ; top5 ->  99.04  and loss:  49.7523198723793
forward train acc: top1 ->  89.38599999511719 ; top5 ->  99.714  and loss:  30.240086033940315
test acc: top1 ->  84.58 ; top5 ->  98.99  and loss:  49.298041582107544
forward train acc: top1 ->  89.40000001464844 ; top5 ->  99.66  and loss:  29.65383368730545
test acc: top1 ->  84.85 ; top5 ->  99.04  and loss:  48.53570559620857
forward train acc: top1 ->  89.68799997802735 ; top5 ->  99.72000000244141  and loss:  29.003755494952202
test acc: top1 ->  84.96 ; top5 ->  99.06  and loss:  48.31606435775757
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -126.13736328855157 , diff:  126.13736328855157
adv train loss:  -875.9746117591858 , diff:  749.8372484706342
adv train loss:  -943.7370166778564 , diff:  67.76240491867065
adv train loss:  -975.181396484375 , diff:  31.444379806518555
adv train loss:  -977.9297904968262 , diff:  2.748394012451172
adv train loss:  -977.9880361557007 , diff:  0.05824565887451172
adv train loss:  -977.7458944320679 , diff:  0.2421417236328125
adv train loss:  -978.5390710830688 , diff:  0.7931766510009766
adv train loss:  -978.4030256271362 , diff:  0.1360454559326172
adv train loss:  -977.8744125366211 , diff:  0.5286130905151367
layer  6  adv train finish, try to retain  11
test acc: top1 ->  10.8 ; top5 ->  50.9  and loss:  515.6704769134521
forward train acc: top1 ->  82.00999999511718 ; top5 ->  98.61799997802734  and loss:  53.8079374730587
test acc: top1 ->  80.61 ; top5 ->  98.14  and loss:  65.39548024535179
forward train acc: top1 ->  88.71400000244141 ; top5 ->  99.56800000244141  and loss:  33.13218918442726
test acc: top1 ->  83.33 ; top5 ->  98.39  and loss:  59.83335581421852
forward train acc: top1 ->  90.66200000732422 ; top5 ->  99.6200000024414  and loss:  28.007281810045242
test acc: top1 ->  84.33 ; top5 ->  98.73  and loss:  55.91072317957878
forward train acc: top1 ->  91.99200000732422 ; top5 ->  99.688  and loss:  23.766948744654655
test acc: top1 ->  85.36 ; top5 ->  98.8  and loss:  52.78861716389656
forward train acc: top1 ->  92.4599999975586 ; top5 ->  99.768  and loss:  22.196402922272682
test acc: top1 ->  85.82 ; top5 ->  98.91  and loss:  50.71932843327522
forward train acc: top1 ->  93.15199998046874 ; top5 ->  99.78000000244141  and loss:  20.097812563180923
test acc: top1 ->  86.14 ; top5 ->  98.9  and loss:  50.193651512265205
forward train acc: top1 ->  93.21999997314452 ; top5 ->  99.804  and loss:  19.721766397356987
test acc: top1 ->  86.28 ; top5 ->  98.93  and loss:  49.09664508700371
forward train acc: top1 ->  93.66399997314453 ; top5 ->  99.83  and loss:  18.419792488217354
test acc: top1 ->  86.45 ; top5 ->  99.01  and loss:  49.78158591687679
forward train acc: top1 ->  93.77199997558594 ; top5 ->  99.83799997558594  and loss:  18.106330543756485
test acc: top1 ->  86.45 ; top5 ->  98.96  and loss:  49.71171833574772
forward train acc: top1 ->  93.98799999511719 ; top5 ->  99.82199997558594  and loss:  17.953628733754158
test acc: top1 ->  86.78 ; top5 ->  98.99  and loss:  48.255359813570976
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  140 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -3.7541084680706263 , diff:  3.7541084680706263
adv train loss:  -3.8201281260699034 , diff:  0.06601965799927711
adv train loss:  -3.762971503660083 , diff:  0.05715662240982056
adv train loss:  -3.733617542311549 , diff:  0.02935396134853363
adv train loss:  -3.886379526928067 , diff:  0.15276198461651802
adv train loss:  -3.7697040177881718 , diff:  0.11667550913989544
adv train loss:  -3.7078377176076174 , diff:  0.06186630018055439
adv train loss:  -3.738461470231414 , diff:  0.030623752623796463
adv train loss:  -3.8417325746268034 , diff:  0.10327110439538956
adv train loss:  -3.907918307930231 , diff:  0.0661857333034277
layer  7  adv train finish, try to retain  261
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -11.99431897699833 , diff:  11.99431897699833
adv train loss:  -11.916731461882591 , diff:  0.07758751511573792
adv train loss:  -11.998164869844913 , diff:  0.08143340796232224
adv train loss:  -27.392981365323067 , diff:  15.394816495478153
adv train loss:  -52.81542667746544 , diff:  25.422445312142372
adv train loss:  -58.36930599808693 , diff:  5.5538793206214905
adv train loss:  -84.04252082109451 , diff:  25.673214823007584
adv train loss:  -97.51530170440674 , diff:  13.472780883312225
adv train loss:  -99.14088410139084 , diff:  1.6255823969841003
adv train loss:  -100.95396864414215 , diff:  1.8130845427513123
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  121
test acc: top1 ->  13.01 ; top5 ->  59.69  and loss:  4075075.658203125
forward train acc: top1 ->  98.70200000488282 ; top5 ->  99.988  and loss:  4.845615460537374
test acc: top1 ->  91.67 ; top5 ->  99.33  and loss:  37.95079483091831
forward train acc: top1 ->  99.59399997558593 ; top5 ->  99.998  and loss:  1.29280467890203
test acc: top1 ->  91.88 ; top5 ->  99.35  and loss:  42.44677735120058
forward train acc: top1 ->  99.718 ; top5 ->  100.0  and loss:  0.8415896331425756
test acc: top1 ->  91.91 ; top5 ->  99.32  and loss:  45.73088884353638
forward train acc: top1 ->  99.844 ; top5 ->  100.0  and loss:  0.4933489605318755
test acc: top1 ->  91.88 ; top5 ->  99.27  and loss:  48.57924407720566
forward train acc: top1 ->  99.804 ; top5 ->  100.0  and loss:  0.5478377993567847
test acc: top1 ->  91.8 ; top5 ->  99.24  and loss:  50.50837729871273
forward train acc: top1 ->  99.83999997558594 ; top5 ->  100.0  and loss:  0.4787314406130463
test acc: top1 ->  92.01 ; top5 ->  99.31  and loss:  51.16150129586458
forward train acc: top1 ->  99.894 ; top5 ->  100.0  and loss:  0.3811051904922351
test acc: top1 ->  91.98 ; top5 ->  99.26  and loss:  52.173345260322094
forward train acc: top1 ->  99.886 ; top5 ->  100.0  and loss:  0.3409285300876945
test acc: top1 ->  92.14 ; top5 ->  99.27  and loss:  53.129812091588974
==> this epoch:  121 / 512
---------------- start layer  9  ---------------
adv train loss:  -1749.4115371704102 , diff:  1749.4115371704102
adv train loss:  -1807.163974761963 , diff:  57.752437591552734
adv train loss:  -1805.7099914550781 , diff:  1.4539833068847656
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  7
test acc: top1 ->  10.6 ; top5 ->  63.89  and loss:  2945.4841632843018
forward train acc: top1 ->  98.7 ; top5 ->  100.0  and loss:  5.565206236671656
test acc: top1 ->  91.14 ; top5 ->  99.11  and loss:  47.94109147042036
forward train acc: top1 ->  99.75199997558593 ; top5 ->  100.0  and loss:  0.9109870390966535
test acc: top1 ->  91.76 ; top5 ->  99.21  and loss:  47.27588115632534
forward train acc: top1 ->  99.7780000024414 ; top5 ->  100.0  and loss:  0.6626469725742936
test acc: top1 ->  91.62 ; top5 ->  99.29  and loss:  49.09782038629055
forward train acc: top1 ->  99.86399997558594 ; top5 ->  100.0  and loss:  0.4874391760677099
test acc: top1 ->  91.88 ; top5 ->  99.25  and loss:  50.615905940532684
forward train acc: top1 ->  99.85999997558594 ; top5 ->  100.0  and loss:  0.41539887245744467
test acc: top1 ->  92.03 ; top5 ->  99.31  and loss:  51.78663466870785
forward train acc: top1 ->  99.88599997558593 ; top5 ->  100.0  and loss:  0.3909684936515987
test acc: top1 ->  92.03 ; top5 ->  99.24  and loss:  52.86110159754753
forward train acc: top1 ->  99.86599997558594 ; top5 ->  100.0  and loss:  0.37489955057390034
test acc: top1 ->  91.99 ; top5 ->  99.27  and loss:  53.35773667320609
forward train acc: top1 ->  99.89199997558593 ; top5 ->  100.0  and loss:  0.3418157089035958
test acc: top1 ->  92.06 ; top5 ->  99.28  and loss:  53.11501743644476
forward train acc: top1 ->  99.904 ; top5 ->  100.0  and loss:  0.28880949621088803
test acc: top1 ->  92.05 ; top5 ->  99.28  and loss:  54.375086933374405
forward train acc: top1 ->  99.91399997558594 ; top5 ->  100.0  and loss:  0.2747154859825969
test acc: top1 ->  92.01 ; top5 ->  99.26  and loss:  54.63488221913576
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  8 / 512 , inc:  1
---------------- start layer  10  ---------------
### skip layer  10 wait:  3  ###
---------------- start layer  11  ---------------
adv train loss:  -1325.3207960128784 , diff:  1325.3207960128784
adv train loss:  -1331.2471914291382 , diff:  5.926395416259766
adv train loss:  -1370.4062824249268 , diff:  39.159090995788574
adv train loss:  -1391.3380451202393 , diff:  20.9317626953125
adv train loss:  -1391.623971939087 , diff:  0.28592681884765625
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  3
test acc: top1 ->  10.0 ; top5 ->  59.29  and loss:  39166.48666381836
forward train acc: top1 ->  97.136 ; top5 ->  99.44  and loss:  16.61027953401208
test acc: top1 ->  91.31 ; top5 ->  99.08  and loss:  51.713597774505615
forward train acc: top1 ->  99.81599997802735 ; top5 ->  100.0  and loss:  0.7817881237715483
test acc: top1 ->  91.75 ; top5 ->  99.15  and loss:  51.9315479695797
forward train acc: top1 ->  99.86399997558594 ; top5 ->  100.0  and loss:  0.5619090124964714
test acc: top1 ->  92.0 ; top5 ->  99.13  and loss:  51.55017623305321
forward train acc: top1 ->  99.894 ; top5 ->  100.0  and loss:  0.43438724463339895
test acc: top1 ->  92.09 ; top5 ->  99.2  and loss:  52.18203038722277
forward train acc: top1 ->  99.88 ; top5 ->  100.0  and loss:  0.44616653164848685
test acc: top1 ->  91.99 ; top5 ->  99.25  and loss:  52.71153035759926
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.32886762369889766
test acc: top1 ->  91.96 ; top5 ->  99.27  and loss:  53.82508097589016
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.2826064645778388
test acc: top1 ->  91.94 ; top5 ->  99.25  and loss:  53.58068434149027
forward train acc: top1 ->  99.91999997558594 ; top5 ->  100.0  and loss:  0.2869790024124086
test acc: top1 ->  92.01 ; top5 ->  99.24  and loss:  54.273752614855766
forward train acc: top1 ->  99.91 ; top5 ->  100.0  and loss:  0.32928004232235253
test acc: top1 ->  92.05 ; top5 ->  99.24  and loss:  54.308352038264275
forward train acc: top1 ->  99.92399997558594 ; top5 ->  100.0  and loss:  0.24252302898094058
test acc: top1 ->  92.05 ; top5 ->  99.25  and loss:  54.871771693229675
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  4 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -1003.5202784538269 , diff:  1003.5202784538269
adv train loss:  -2083.652261734009 , diff:  1080.1319832801819
adv train loss:  -2081.389659881592 , diff:  2.262601852416992
adv train loss:  -2082.8314476013184 , diff:  1.4417877197265625
adv train loss:  -2084.029348373413 , diff:  1.1979007720947266
adv train loss:  -2082.5326232910156 , diff:  1.496725082397461
adv train loss:  -2082.737190246582 , diff:  0.20456695556640625
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  45
test acc: top1 ->  19.28 ; top5 ->  50.0  and loss:  44938.69241333008
forward train acc: top1 ->  91.188 ; top5 ->  98.706  and loss:  92.78508303593844
test acc: top1 ->  91.07 ; top5 ->  98.6  and loss:  72.54783969372511
forward train acc: top1 ->  99.71999997558594 ; top5 ->  100.0  and loss:  1.2212412962689996
test acc: top1 ->  91.34 ; top5 ->  98.71  and loss:  70.47694047540426
forward train acc: top1 ->  99.796 ; top5 ->  100.0  and loss:  0.8083350989036262
test acc: top1 ->  91.47 ; top5 ->  98.79  and loss:  69.50777299702168
forward train acc: top1 ->  99.85 ; top5 ->  100.0  and loss:  0.5897639922332019
test acc: top1 ->  91.68 ; top5 ->  98.75  and loss:  70.17160767316818
forward train acc: top1 ->  99.868 ; top5 ->  100.0  and loss:  0.5554785637650639
test acc: top1 ->  91.69 ; top5 ->  98.8  and loss:  70.4543492347002
forward train acc: top1 ->  99.87799997558594 ; top5 ->  100.0  and loss:  0.5044383630156517
test acc: top1 ->  91.71 ; top5 ->  98.8  and loss:  71.11801432073116
forward train acc: top1 ->  99.89799997558593 ; top5 ->  100.0  and loss:  0.4090455938130617
test acc: top1 ->  91.8 ; top5 ->  98.82  and loss:  71.19617807120085
forward train acc: top1 ->  99.87799997558594 ; top5 ->  100.0  and loss:  0.40831973566673696
test acc: top1 ->  91.74 ; top5 ->  98.82  and loss:  71.21156033873558
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  47 / 512 , inc:  2
---------------- start layer  13  ---------------
adv train loss:  -10063.027442932129 , diff:  10063.027442932129
adv train loss:  -17265.87043762207 , diff:  7202.842994689941
adv train loss:  -24152.17268371582 , diff:  6886.30224609375
adv train loss:  -30960.237762451172 , diff:  6808.065078735352
adv train loss:  -37725.150970458984 , diff:  6764.9132080078125
adv train loss:  -44458.884857177734 , diff:  6733.73388671875
adv train loss:  -51190.48254394531 , diff:  6731.597686767578
adv train loss:  -57419.71026611328 , diff:  6229.227722167969
adv train loss:  -63125.92077636719 , diff:  5706.210510253906
adv train loss:  -68624.5267944336 , diff:  5498.606018066406
layer  13  adv train finish, try to retain  24
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  2
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.546875  ==>  140 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.236328125  ==>  121 / 512 , inc:  16
layer  9  :  0.015625  ==>  8 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.0078125  ==>  4 / 512 , inc:  1
layer  12  :  0.091796875  ==>  47 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [1.4778918800354004, 1.9705225067138672, 1.4778918800354004, 1.4778918800354004, 1.4778918800354004, 1.4778918800354004, 1.4778918800354004, 0.09236824250221252, 1.313681671142578, 1.313681671142578, 1.1084189100265502, 0.9852612533569336, 1.4778918800354004, 21.01890673828125]  wait [3, 2, 3, 3, 3, 3, 2, 2, 0, 4, 2, 4, 3, 2]  inc [1, 2, 1, 1, 1, 1, 1, 1, 16, 1, 1, 1, 1, 1]  tol: 5
$$$$$$$$$$$$$ epoch  60  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
adv train loss:  -273.891539812088 , diff:  273.891539812088
adv train loss:  -274.9818389415741 , diff:  1.090299129486084
adv train loss:  -274.8614299297333 , diff:  0.12040901184082031
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  42
test acc: top1 ->  11.89 ; top5 ->  56.44  and loss:  1369.4203748703003
forward train acc: top1 ->  97.88599997802734 ; top5 ->  100.0  and loss:  10.711486575892195
test acc: top1 ->  91.86 ; top5 ->  99.1  and loss:  71.58189768344164
forward train acc: top1 ->  99.8220000024414 ; top5 ->  100.0  and loss:  0.5054280110634863
test acc: top1 ->  91.94 ; top5 ->  99.21  and loss:  71.21482592076063
forward train acc: top1 ->  99.85599997558593 ; top5 ->  100.0  and loss:  0.4147878363437485
test acc: top1 ->  91.93 ; top5 ->  99.15  and loss:  70.72685633599758
forward train acc: top1 ->  99.89599997558594 ; top5 ->  100.0  and loss:  0.372327568475157
test acc: top1 ->  91.97 ; top5 ->  99.23  and loss:  70.01098711788654
forward train acc: top1 ->  99.90799997558594 ; top5 ->  100.0  and loss:  0.2726135393604636
test acc: top1 ->  91.92 ; top5 ->  99.21  and loss:  69.60305394977331
forward train acc: top1 ->  99.90199997558594 ; top5 ->  100.0  and loss:  0.26941142696887255
test acc: top1 ->  91.91 ; top5 ->  99.18  and loss:  70.90569975972176
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.22518665331881493
test acc: top1 ->  91.91 ; top5 ->  99.21  and loss:  70.28252533078194
forward train acc: top1 ->  99.908 ; top5 ->  100.0  and loss:  0.27432675642194226
test acc: top1 ->  91.92 ; top5 ->  99.14  and loss:  70.3985773473978
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.21660239808261395
test acc: top1 ->  91.93 ; top5 ->  99.22  and loss:  71.30233941227198
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.21641443250700831
test acc: top1 ->  91.97 ; top5 ->  99.19  and loss:  71.35301224887371
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  44 / 64 , inc:  2
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
adv train loss:  -685.7698114543455 , diff:  685.7698114543455
adv train loss:  -2640.6767807006836 , diff:  1954.9069692463381
adv train loss:  -2745.5717449188232 , diff:  104.89496421813965
adv train loss:  -2769.269542694092 , diff:  23.697797775268555
adv train loss:  -2769.9521312713623 , diff:  0.6825885772705078
adv train loss:  -2774.494302749634 , diff:  4.542171478271484
adv train loss:  -2783.571668624878 , diff:  9.07736587524414
adv train loss:  -2799.315725326538 , diff:  15.744056701660156
adv train loss:  -2798.9935035705566 , diff:  0.3222217559814453
adv train loss:  -2803.5753650665283 , diff:  4.58186149597168
layer  6  adv train finish, try to retain  16
test acc: top1 ->  17.61 ; top5 ->  66.87  and loss:  846.0839133262634
forward train acc: top1 ->  79.60800000488281 ; top5 ->  98.27400000732422  and loss:  105.20373156666756
test acc: top1 ->  79.97 ; top5 ->  97.62  and loss:  91.36030635237694
forward train acc: top1 ->  86.73599997558594 ; top5 ->  99.34999997802734  and loss:  41.243673741817474
test acc: top1 ->  82.06 ; top5 ->  97.89  and loss:  68.64043179154396
forward train acc: top1 ->  88.73599999511718 ; top5 ->  99.52799997558594  and loss:  33.48291939496994
test acc: top1 ->  83.21 ; top5 ->  98.16  and loss:  62.978708520531654
forward train acc: top1 ->  90.10599998779297 ; top5 ->  99.62800000244141  and loss:  29.02866716682911
test acc: top1 ->  84.03 ; top5 ->  98.25  and loss:  59.61106917262077
forward train acc: top1 ->  90.94199998779297 ; top5 ->  99.678  and loss:  26.419327080249786
test acc: top1 ->  84.31 ; top5 ->  98.12  and loss:  59.401505172252655
forward train acc: top1 ->  91.746 ; top5 ->  99.65999997802734  and loss:  24.159342482686043
test acc: top1 ->  85.17 ; top5 ->  98.46  and loss:  56.29366119205952
forward train acc: top1 ->  92.05799997802734 ; top5 ->  99.74199997558594  and loss:  22.952286452054977
test acc: top1 ->  85.56 ; top5 ->  98.53  and loss:  55.31817676126957
forward train acc: top1 ->  92.78599997558594 ; top5 ->  99.784  and loss:  21.33699458837509
test acc: top1 ->  85.85 ; top5 ->  98.58  and loss:  54.563281401991844
forward train acc: top1 ->  92.77599998046875 ; top5 ->  99.75399997558594  and loss:  21.233754813671112
test acc: top1 ->  86.09 ; top5 ->  98.58  and loss:  53.081328600645065
forward train acc: top1 ->  93.17599999511718 ; top5 ->  99.79999997558593  and loss:  19.747942700982094
test acc: top1 ->  86.34 ; top5 ->  98.63  and loss:  52.29762877523899
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  140 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -2.889154113829136 , diff:  2.889154113829136
adv train loss:  -2.931025955826044 , diff:  0.04187184199690819
adv train loss:  -2.9276507142931223 , diff:  0.003375241532921791
layer  7  adv train finish, try to retain  256
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -85.12775111198425 , diff:  85.12775111198425
adv train loss:  -84.4775983095169 , diff:  0.6501528024673462
adv train loss:  -84.5930193066597 , diff:  0.11542099714279175
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  105
test acc: top1 ->  10.02 ; top5 ->  60.54  and loss:  45307660.96875
forward train acc: top1 ->  99.16799997802734 ; top5 ->  99.998  and loss:  2.529915742110461
test acc: top1 ->  91.85 ; top5 ->  99.22  and loss:  43.4681631475687
forward train acc: top1 ->  99.846 ; top5 ->  100.0  and loss:  0.5342572655063123
test acc: top1 ->  92.0 ; top5 ->  99.22  and loss:  46.374780505895615
forward train acc: top1 ->  99.86199997802734 ; top5 ->  99.998  and loss:  0.39662119722925127
test acc: top1 ->  91.97 ; top5 ->  99.19  and loss:  49.94996661506593
forward train acc: top1 ->  99.91799997558594 ; top5 ->  100.0  and loss:  0.27096026693470776
test acc: top1 ->  91.93 ; top5 ->  99.2  and loss:  52.956848565489054
forward train acc: top1 ->  99.928 ; top5 ->  100.0  and loss:  0.21375624043866992
test acc: top1 ->  91.97 ; top5 ->  99.2  and loss:  54.680605970323086
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.18654755162424408
test acc: top1 ->  92.0 ; top5 ->  99.19  and loss:  55.37006322294474
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.2098361621319782
test acc: top1 ->  91.99 ; top5 ->  99.17  and loss:  56.417873702943325
forward train acc: top1 ->  99.94399997558594 ; top5 ->  100.0  and loss:  0.1732408159878105
test acc: top1 ->  92.11 ; top5 ->  99.16  and loss:  57.82472603954375
==> this epoch:  105 / 512
---------------- start layer  9  ---------------
### skip layer  9 wait:  4  ###
---------------- start layer  10  ---------------
adv train loss:  -80.81089663505554 , diff:  80.81089663505554
adv train loss:  -81.06212240457535 , diff:  0.2512257695198059
adv train loss:  -80.8879576921463 , diff:  0.17416471242904663
adv train loss:  -81.03197461366653 , diff:  0.14401692152023315
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  10
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  48902.60028076172
forward train acc: top1 ->  92.69 ; top5 ->  99.688  and loss:  24.668218597769737
test acc: top1 ->  71.35 ; top5 ->  97.94  and loss:  111.3679940700531
forward train acc: top1 ->  99.53799997558593 ; top5 ->  99.998  and loss:  2.9291250202804804
test acc: top1 ->  90.7 ; top5 ->  98.37  and loss:  49.72892116010189
forward train acc: top1 ->  99.74800000488281 ; top5 ->  100.0  and loss:  1.4457159079611301
test acc: top1 ->  91.08 ; top5 ->  98.47  and loss:  50.80753184109926
forward train acc: top1 ->  99.794 ; top5 ->  100.0  and loss:  0.9787454828619957
test acc: top1 ->  91.2 ; top5 ->  98.47  and loss:  52.895138666033745
forward train acc: top1 ->  99.886 ; top5 ->  100.0  and loss:  0.619747415650636
test acc: top1 ->  91.14 ; top5 ->  98.47  and loss:  55.73332316428423
forward train acc: top1 ->  99.854 ; top5 ->  100.0  and loss:  0.5993944806978106
test acc: top1 ->  91.3 ; top5 ->  98.55  and loss:  54.83270128071308
forward train acc: top1 ->  99.906 ; top5 ->  100.0  and loss:  0.490107087418437
test acc: top1 ->  91.51 ; top5 ->  98.56  and loss:  55.81416752189398
forward train acc: top1 ->  99.868 ; top5 ->  100.0  and loss:  0.48058820026926696
test acc: top1 ->  91.5 ; top5 ->  98.53  and loss:  56.420349180698395
forward train acc: top1 ->  99.88199997558594 ; top5 ->  100.0  and loss:  0.419560577487573
test acc: top1 ->  91.43 ; top5 ->  98.7  and loss:  57.871219120919704
forward train acc: top1 ->  99.918 ; top5 ->  100.0  and loss:  0.3498205461073667
test acc: top1 ->  91.57 ; top5 ->  98.58  and loss:  57.386828668415546
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  11 / 512 , inc:  1
---------------- start layer  11  ---------------
### skip layer  11 wait:  4  ###
---------------- start layer  12  ---------------
### skip layer  12 wait:  3  ###
---------------- start layer  13  ---------------
adv train loss:  -10007.007415771484 , diff:  10007.007415771484
adv train loss:  -16844.75814819336 , diff:  6837.750732421875
adv train loss:  -23532.421310424805 , diff:  6687.663162231445
adv train loss:  -30131.823333740234 , diff:  6599.40202331543
adv train loss:  -36673.15759277344 , diff:  6541.334259033203
adv train loss:  -43207.57467651367 , diff:  6534.417083740234
adv train loss:  -49720.379455566406 , diff:  6512.804779052734
adv train loss:  -56214.574157714844 , diff:  6494.1947021484375
adv train loss:  -62728.02404785156 , diff:  6513.449890136719
adv train loss:  -69203.96795654297 , diff:  6475.943908691406
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  8
test acc: top1 ->  10.8 ; top5 ->  53.64  and loss:  2871.368211746216
forward train acc: top1 ->  37.5560000012207 ; top5 ->  72.70600001464844  and loss:  1168.2812538146973
test acc: top1 ->  56.24 ; top5 ->  79.08  and loss:  452.04036474227905
forward train acc: top1 ->  75.04999997802734 ; top5 ->  87.45599997558594  and loss:  177.81129032373428
test acc: top1 ->  78.88 ; top5 ->  98.69  and loss:  88.41636782884598
forward train acc: top1 ->  92.43399998291015 ; top5 ->  99.984  and loss:  32.85040673613548
test acc: top1 ->  88.12 ; top5 ->  98.56  and loss:  58.143163710832596
forward train acc: top1 ->  98.71800000732422 ; top5 ->  99.996  and loss:  17.10246504098177
test acc: top1 ->  88.91 ; top5 ->  98.35  and loss:  55.60833156108856
forward train acc: top1 ->  99.15999997558593 ; top5 ->  99.994  and loss:  10.866103418171406
test acc: top1 ->  89.3 ; top5 ->  98.25  and loss:  55.25820207595825
forward train acc: top1 ->  99.29000000488281 ; top5 ->  99.996  and loss:  7.9615125842392445
test acc: top1 ->  89.54 ; top5 ->  98.21  and loss:  55.475317522883415
forward train acc: top1 ->  99.33199997802734 ; top5 ->  99.996  and loss:  6.7531972378492355
test acc: top1 ->  89.72 ; top5 ->  98.12  and loss:  56.28475330770016
forward train acc: top1 ->  99.46799997558594 ; top5 ->  99.998  and loss:  5.586789272725582
test acc: top1 ->  89.77 ; top5 ->  98.09  and loss:  56.68988026678562
forward train acc: top1 ->  99.49799997558594 ; top5 ->  99.996  and loss:  4.866302464157343
test acc: top1 ->  89.91 ; top5 ->  98.01  and loss:  57.10777339339256
forward train acc: top1 ->  99.50999997558594 ; top5 ->  99.998  and loss:  4.223942179232836
test acc: top1 ->  90.06 ; top5 ->  97.99  and loss:  57.42154675722122
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  9 / 512 , inc:  1
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.546875  ==>  140 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.205078125  ==>  105 / 512 , inc:  32
layer  9  :  0.015625  ==>  8 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.0078125  ==>  4 / 512 , inc:  1
layer  12  :  0.091796875  ==>  47 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [1.4778918800354004, 1.4778918800354004, 1.4778918800354004, 1.4778918800354004, 1.4778918800354004, 1.4778918800354004, 1.1084189100265502, 0.18473648500442505, 1.313681671142578, 1.313681671142578, 0.8313141825199126, 0.9852612533569336, 1.4778918800354004, 15.764180053710938]  wait [2, 4, 2, 2, 2, 2, 4, 2, 0, 3, 4, 3, 2, 4]  inc [1, 1, 1, 1, 1, 1, 1, 1, 32, 1, 1, 1, 1, 1]  tol: 5
$$$$$$$$$$$$$ epoch  61  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -2426.762233734131 , diff:  2426.762233734131
adv train loss:  -2480.4445095062256 , diff:  53.68227577209473
adv train loss:  -2490.1173915863037 , diff:  9.672882080078125
adv train loss:  -2500.806354522705 , diff:  10.688962936401367
adv train loss:  -2509.5172634124756 , diff:  8.710908889770508
adv train loss:  -2498.075054168701 , diff:  11.442209243774414
adv train loss:  -2503.128318786621 , diff:  5.053264617919922
adv train loss:  -2495.4667072296143 , diff:  7.661611557006836
adv train loss:  -2504.121421813965 , diff:  8.654714584350586
adv train loss:  -2501.322443008423 , diff:  2.798978805541992
layer  0  adv train finish, try to retain  4
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  1769.6036672592163
forward train acc: top1 ->  58.93799998046875 ; top5 ->  91.86399999023438  and loss:  478.09724843502045
test acc: top1 ->  44.42 ; top5 ->  81.4  and loss:  396.46367502212524
forward train acc: top1 ->  74.82000001953125 ; top5 ->  95.80799998535156  and loss:  101.56936699151993
test acc: top1 ->  74.94 ; top5 ->  95.99  and loss:  95.77444076538086
forward train acc: top1 ->  77.2919999951172 ; top5 ->  96.69999999023437  and loss:  76.78272098302841
test acc: top1 ->  76.76 ; top5 ->  96.56  and loss:  82.81141370534897
forward train acc: top1 ->  79.65399997802734 ; top5 ->  97.26800000732422  and loss:  66.83917140960693
test acc: top1 ->  78.14 ; top5 ->  97.02  and loss:  75.81377163529396
forward train acc: top1 ->  81.36999999267579 ; top5 ->  97.70200000732422  and loss:  60.32743966579437
test acc: top1 ->  79.01 ; top5 ->  97.18  and loss:  71.76503446698189
forward train acc: top1 ->  82.49799997070312 ; top5 ->  97.93000000732422  and loss:  56.344125628471375
test acc: top1 ->  79.45 ; top5 ->  97.39  and loss:  70.47879192233086
forward train acc: top1 ->  82.65800001953124 ; top5 ->  97.90599998291016  and loss:  55.25892052054405
test acc: top1 ->  79.61 ; top5 ->  97.45  and loss:  68.32416492700577
forward train acc: top1 ->  83.36600001220702 ; top5 ->  98.11399998291016  and loss:  53.4160196185112
test acc: top1 ->  80.0 ; top5 ->  97.6  and loss:  66.58425465226173
forward train acc: top1 ->  83.89600001953124 ; top5 ->  98.17799998291015  and loss:  51.605844646692276
test acc: top1 ->  80.43 ; top5 ->  97.56  and loss:  65.06573352217674
forward train acc: top1 ->  84.23200001220704 ; top5 ->  98.38600000732421  and loss:  49.49180620908737
test acc: top1 ->  80.44 ; top5 ->  97.68  and loss:  64.17524334788322
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
### skip layer  1 wait:  4  ###
---------------- start layer  2  ---------------
adv train loss:  -176.30612812191248 , diff:  176.30612812191248
adv train loss:  -446.0989465713501 , diff:  269.7928184494376
adv train loss:  -496.95496916770935 , diff:  50.85602259635925
adv train loss:  -536.418776512146 , diff:  39.463807344436646
adv train loss:  -556.3013935089111 , diff:  19.882616996765137
adv train loss:  -560.8930411338806 , diff:  4.591647624969482
adv train loss:  -564.6995949745178 , diff:  3.806553840637207
adv train loss:  -569.555362701416 , diff:  4.855767726898193
adv train loss:  -567.4799747467041 , diff:  2.075387954711914
adv train loss:  -569.3117365837097 , diff:  1.8317618370056152
layer  2  adv train finish, try to retain  79
test acc: top1 ->  9.08 ; top5 ->  55.67  and loss:  3769.004421234131
forward train acc: top1 ->  98.85800000488281 ; top5 ->  99.986  and loss:  3.9576684422791004
test acc: top1 ->  90.7 ; top5 ->  99.2  and loss:  44.540682181715965
forward train acc: top1 ->  99.20999998046875 ; top5 ->  99.988  and loss:  2.427241563796997
test acc: top1 ->  90.94 ; top5 ->  99.19  and loss:  45.25347116589546
forward train acc: top1 ->  99.4060000024414 ; top5 ->  99.994  and loss:  1.9145822264254093
test acc: top1 ->  90.76 ; top5 ->  99.23  and loss:  47.49676799029112
forward train acc: top1 ->  99.51199997558594 ; top5 ->  100.0  and loss:  1.4828186221420765
test acc: top1 ->  90.88 ; top5 ->  99.21  and loss:  51.16672870516777
forward train acc: top1 ->  99.55199997558594 ; top5 ->  99.998  and loss:  1.3125731395557523
test acc: top1 ->  90.93 ; top5 ->  99.13  and loss:  52.9680183082819
forward train acc: top1 ->  99.53199997558593 ; top5 ->  99.996  and loss:  1.3509019762277603
test acc: top1 ->  91.08 ; top5 ->  99.15  and loss:  51.985111609101295
forward train acc: top1 ->  99.64999997558594 ; top5 ->  100.0  and loss:  1.060762015171349
test acc: top1 ->  91.08 ; top5 ->  99.15  and loss:  51.79908545315266
forward train acc: top1 ->  99.61199997558593 ; top5 ->  100.0  and loss:  1.0839838613756
test acc: top1 ->  90.93 ; top5 ->  99.2  and loss:  52.11283315718174
forward train acc: top1 ->  99.65799997558594 ; top5 ->  99.998  and loss:  1.0208717668429017
test acc: top1 ->  91.0 ; top5 ->  99.18  and loss:  52.08975999057293
forward train acc: top1 ->  99.692 ; top5 ->  99.998  and loss:  0.9079672115622088
test acc: top1 ->  91.04 ; top5 ->  99.17  and loss:  52.9231938123703
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -703.5850184280425 , diff:  703.5850184280425
adv train loss:  -1244.2380361557007 , diff:  540.6530177276582
adv train loss:  -1293.7035837173462 , diff:  49.46554756164551
adv train loss:  -1296.4203872680664 , diff:  2.716803550720215
adv train loss:  -1293.7814483642578 , diff:  2.6389389038085938
adv train loss:  -1318.2966690063477 , diff:  24.515220642089844
adv train loss:  -1326.6838464736938 , diff:  8.387177467346191
adv train loss:  -1321.433087348938 , diff:  5.250759124755859
adv train loss:  -1321.47633934021 , diff:  0.043251991271972656
adv train loss:  -1325.5089807510376 , diff:  4.032641410827637
layer  3  adv train finish, try to retain  50
test acc: top1 ->  25.15 ; top5 ->  70.51  and loss:  738.2099738121033
forward train acc: top1 ->  94.61399999267579 ; top5 ->  99.848  and loss:  17.790918685495853
test acc: top1 ->  87.64 ; top5 ->  98.85  and loss:  48.447122395038605
forward train acc: top1 ->  95.584 ; top5 ->  99.896  and loss:  13.088807679712772
test acc: top1 ->  88.41 ; top5 ->  98.92  and loss:  46.50051440298557
forward train acc: top1 ->  96.19999999755859 ; top5 ->  99.896  and loss:  11.502275124192238
test acc: top1 ->  88.62 ; top5 ->  98.91  and loss:  45.26743568480015
forward train acc: top1 ->  96.44200000976562 ; top5 ->  99.92599997558594  and loss:  10.433576747775078
test acc: top1 ->  88.85 ; top5 ->  99.12  and loss:  45.4892972111702
forward train acc: top1 ->  96.84000000976563 ; top5 ->  99.944  and loss:  9.482016734778881
test acc: top1 ->  89.12 ; top5 ->  99.1  and loss:  45.2755843475461
forward train acc: top1 ->  96.89799999023438 ; top5 ->  99.94199997558594  and loss:  9.151202958077192
test acc: top1 ->  89.23 ; top5 ->  99.11  and loss:  45.06646391004324
forward train acc: top1 ->  97.11799998779297 ; top5 ->  99.94399997558594  and loss:  8.488649763166904
test acc: top1 ->  89.39 ; top5 ->  99.09  and loss:  44.600368052721024
forward train acc: top1 ->  97.25600000976563 ; top5 ->  99.944  and loss:  8.125525202602148
test acc: top1 ->  89.07 ; top5 ->  99.14  and loss:  45.3205209672451
forward train acc: top1 ->  97.28800001220704 ; top5 ->  99.94999997558594  and loss:  8.196703307330608
test acc: top1 ->  89.31 ; top5 ->  99.15  and loss:  44.161929957568645
forward train acc: top1 ->  97.35400000976563 ; top5 ->  99.97399997558594  and loss:  7.728009309619665
test acc: top1 ->  89.47 ; top5 ->  99.14  and loss:  44.48433497548103
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -205.97507156990469 , diff:  205.97507156990469
adv train loss:  -1057.20077419281 , diff:  851.2257026229054
adv train loss:  -1138.2547998428345 , diff:  81.05402565002441
adv train loss:  -1141.6319723129272 , diff:  3.3771724700927734
adv train loss:  -1141.680209159851 , diff:  0.048236846923828125
adv train loss:  -1158.3068523406982 , diff:  16.626643180847168
adv train loss:  -1155.1381540298462 , diff:  3.168698310852051
adv train loss:  -1160.034598350525 , diff:  4.896444320678711
adv train loss:  -1179.4665517807007 , diff:  19.43195343017578
adv train loss:  -1241.4176397323608 , diff:  61.951087951660156
layer  4  adv train finish, try to retain  39
test acc: top1 ->  29.71 ; top5 ->  78.37  and loss:  836.7361726760864
forward train acc: top1 ->  82.11800000976562 ; top5 ->  98.50399998046875  and loss:  55.12530070543289
test acc: top1 ->  79.38 ; top5 ->  98.18  and loss:  64.90202811360359
forward train acc: top1 ->  85.24600000732421 ; top5 ->  99.11600000244141  and loss:  43.09193328022957
test acc: top1 ->  81.4 ; top5 ->  98.54  and loss:  59.685840755701065
forward train acc: top1 ->  86.78200001708984 ; top5 ->  99.29200000488281  and loss:  38.44658035039902
test acc: top1 ->  82.2 ; top5 ->  98.62  and loss:  57.07764396071434
forward train acc: top1 ->  87.47399997070312 ; top5 ->  99.3860000024414  and loss:  36.35225549340248
test acc: top1 ->  82.68 ; top5 ->  98.68  and loss:  55.067066967487335
forward train acc: top1 ->  88.24999998779298 ; top5 ->  99.41000000244141  and loss:  34.04604882001877
test acc: top1 ->  83.51 ; top5 ->  98.78  and loss:  53.24634671211243
forward train acc: top1 ->  88.76400001220703 ; top5 ->  99.4920000024414  and loss:  32.794176280498505
test acc: top1 ->  83.41 ; top5 ->  98.71  and loss:  53.07516458630562
forward train acc: top1 ->  88.91999997314453 ; top5 ->  99.4560000024414  and loss:  32.18028652667999
test acc: top1 ->  83.55 ; top5 ->  98.76  and loss:  52.25983199477196
forward train acc: top1 ->  89.202 ; top5 ->  99.52199997802734  and loss:  31.36987580358982
test acc: top1 ->  83.65 ; top5 ->  98.84  and loss:  51.89824801683426
forward train acc: top1 ->  89.30200001464844 ; top5 ->  99.54199997558594  and loss:  30.666931584477425
test acc: top1 ->  83.75 ; top5 ->  98.85  and loss:  51.5535788834095
forward train acc: top1 ->  89.42799997558593 ; top5 ->  99.51800000488281  and loss:  30.64424216747284
test acc: top1 ->  83.85 ; top5 ->  98.77  and loss:  51.55141770839691
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -318.71674343571067 , diff:  318.71674343571067
adv train loss:  -1104.5839681625366 , diff:  785.867224726826
adv train loss:  -1191.1783475875854 , diff:  86.59437942504883
adv train loss:  -1218.8669958114624 , diff:  27.688648223876953
adv train loss:  -1239.894160270691 , diff:  21.027164459228516
adv train loss:  -1241.4866371154785 , diff:  1.5924768447875977
adv train loss:  -1250.5125617980957 , diff:  9.025924682617188
adv train loss:  -1285.3080978393555 , diff:  34.795536041259766
adv train loss:  -1289.694501876831 , diff:  4.386404037475586
adv train loss:  -1288.6668825149536 , diff:  1.0276193618774414
layer  5  adv train finish, try to retain  44
test acc: top1 ->  22.01 ; top5 ->  69.6  and loss:  797.7962422370911
forward train acc: top1 ->  89.08600001220704 ; top5 ->  99.65799997558594  and loss:  31.879813253879547
test acc: top1 ->  83.83 ; top5 ->  98.93  and loss:  53.26216146349907
forward train acc: top1 ->  91.84999998046875 ; top5 ->  99.78599997558594  and loss:  23.254095807671547
test acc: top1 ->  84.96 ; top5 ->  98.97  and loss:  51.49739062786102
forward train acc: top1 ->  92.91999997802735 ; top5 ->  99.836  and loss:  20.25363165140152
test acc: top1 ->  85.8 ; top5 ->  99.09  and loss:  48.283510476350784
forward train acc: top1 ->  93.5720000024414 ; top5 ->  99.824  and loss:  17.953168749809265
test acc: top1 ->  86.5 ; top5 ->  99.0  and loss:  48.11470088362694
forward train acc: top1 ->  94.30400000244141 ; top5 ->  99.892  and loss:  16.559960402548313
test acc: top1 ->  86.87 ; top5 ->  99.13  and loss:  46.7813890427351
forward train acc: top1 ->  94.6739999975586 ; top5 ->  99.896  and loss:  15.354439683258533
test acc: top1 ->  87.03 ; top5 ->  99.11  and loss:  46.86850269138813
forward train acc: top1 ->  94.66200001708984 ; top5 ->  99.90999997558593  and loss:  15.04656820744276
test acc: top1 ->  87.06 ; top5 ->  99.16  and loss:  47.06695553660393
forward train acc: top1 ->  94.84800001464843 ; top5 ->  99.912  and loss:  14.6276283711195
test acc: top1 ->  87.21 ; top5 ->  99.18  and loss:  46.28456571698189
forward train acc: top1 ->  95.03999997314453 ; top5 ->  99.89799997558593  and loss:  13.976312898099422
test acc: top1 ->  87.17 ; top5 ->  99.22  and loss:  46.668038442730904
forward train acc: top1 ->  95.10599999511719 ; top5 ->  99.914  and loss:  14.060269318521023
test acc: top1 ->  87.31 ; top5 ->  99.17  and loss:  45.96698574721813
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
adv train loss:  -2.3719358891248703 , diff:  2.3719358891248703
adv train loss:  -2.205046257004142 , diff:  0.1668896321207285
adv train loss:  -2.2252745889127254 , diff:  0.02022833190858364
adv train loss:  -2.3106000255793333 , diff:  0.08532543666660786
adv train loss:  -2.3168727196753025 , diff:  0.0062726940959692
layer  7  adv train finish, try to retain  256
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -73.58116513490677 , diff:  73.58116513490677
adv train loss:  -73.51656210422516 , diff:  0.06460303068161011
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  73
test acc: top1 ->  15.07 ; top5 ->  51.76  and loss:  7322829.29296875
forward train acc: top1 ->  98.99999997558594 ; top5 ->  99.992  and loss:  3.806535879150033
test acc: top1 ->  91.96 ; top5 ->  99.42  and loss:  37.97039271891117
forward train acc: top1 ->  99.748 ; top5 ->  99.998  and loss:  0.911158214090392
test acc: top1 ->  92.08 ; top5 ->  99.43  and loss:  41.08061556518078
forward train acc: top1 ->  99.82200000488281 ; top5 ->  100.0  and loss:  0.5621986333280802
test acc: top1 ->  92.17 ; top5 ->  99.39  and loss:  44.025024719536304
==> this epoch:  73 / 512
---------------- start layer  9  ---------------
### skip layer  9 wait:  3  ###
---------------- start layer  10  ---------------
### skip layer  10 wait:  4  ###
---------------- start layer  11  ---------------
### skip layer  11 wait:  3  ###
---------------- start layer  12  ---------------
adv train loss:  -1576.5116176605225 , diff:  1576.5116176605225
adv train loss:  -1576.6342821121216 , diff:  0.1226644515991211
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  46
test acc: top1 ->  11.1 ; top5 ->  67.32  and loss:  17250.726104736328
forward train acc: top1 ->  92.684 ; top5 ->  99.766  and loss:  27.768567517399788
test acc: top1 ->  91.9 ; top5 ->  98.76  and loss:  39.797000735998154
forward train acc: top1 ->  99.76399997558593 ; top5 ->  100.0  and loss:  1.6586160231381655
test acc: top1 ->  91.98 ; top5 ->  98.72  and loss:  40.9909847304225
forward train acc: top1 ->  99.826 ; top5 ->  100.0  and loss:  1.126456543803215
test acc: top1 ->  92.04 ; top5 ->  98.82  and loss:  42.48602037131786
forward train acc: top1 ->  99.87399997558593 ; top5 ->  99.998  and loss:  0.7753582932054996
test acc: top1 ->  92.03 ; top5 ->  98.67  and loss:  44.793674282729626
forward train acc: top1 ->  99.88999997558594 ; top5 ->  100.0  and loss:  0.6244227718561888
test acc: top1 ->  92.01 ; top5 ->  98.79  and loss:  45.371896497905254
forward train acc: top1 ->  99.886 ; top5 ->  100.0  and loss:  0.562396892812103
test acc: top1 ->  92.06 ; top5 ->  98.79  and loss:  46.23407672345638
forward train acc: top1 ->  99.918 ; top5 ->  100.0  and loss:  0.48154630046337843
test acc: top1 ->  92.1 ; top5 ->  98.8  and loss:  46.65373560041189
forward train acc: top1 ->  99.904 ; top5 ->  100.0  and loss:  0.45777338184416294
test acc: top1 ->  92.16 ; top5 ->  98.83  and loss:  47.33969671279192
==> this epoch:  46 / 512
---------------- start layer  13  ---------------
### skip layer  13 wait:  4  ###
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.546875  ==>  140 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.142578125  ==>  73 / 512 , inc:  36
layer  9  :  0.015625  ==>  8 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.0078125  ==>  4 / 512 , inc:  1
layer  12  :  0.08984375  ==>  46 / 512 , inc:  2
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [1.1084189100265502, 1.4778918800354004, 1.1084189100265502, 1.1084189100265502, 1.1084189100265502, 1.1084189100265502, 1.1084189100265502, 0.3694729700088501, 1.313681671142578, 1.313681671142578, 0.8313141825199126, 0.9852612533569336, 1.4778918800354004, 15.764180053710938]  wait [4, 3, 4, 4, 4, 4, 3, 2, 0, 2, 3, 2, 0, 3]  inc [1, 1, 1, 1, 1, 1, 1, 1, 36, 1, 1, 1, 2, 1]  tol: 5
$$$$$$$$$$$$$ epoch  62  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
### skip layer  1 wait:  3  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  3  ###
---------------- start layer  7  ---------------
adv train loss:  -1.2604273594915867 , diff:  1.2604273594915867
adv train loss:  -1.2034739344380796 , diff:  0.05695342505350709
adv train loss:  -1.2512137647718191 , diff:  0.04773983033373952
adv train loss:  -1.1524960519745946 , diff:  0.09871771279722452
adv train loss:  -1.1616555307991803 , diff:  0.009159478824585676
layer  7  adv train finish, try to retain  256
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -73.06636667251587 , diff:  73.06636667251587
adv train loss:  -73.05506908893585 , diff:  0.01129758358001709
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  37
test acc: top1 ->  10.0 ; top5 ->  51.64  and loss:  106068364.9375
forward train acc: top1 ->  98.94799997558594 ; top5 ->  99.984  and loss:  3.9508327883668244
test acc: top1 ->  91.86 ; top5 ->  98.88  and loss:  43.85783963650465
forward train acc: top1 ->  99.84399997558593 ; top5 ->  99.998  and loss:  0.759807872120291
test acc: top1 ->  91.97 ; top5 ->  98.92  and loss:  46.63554563000798
forward train acc: top1 ->  99.868 ; top5 ->  100.0  and loss:  0.545941068790853
test acc: top1 ->  91.98 ; top5 ->  98.93  and loss:  46.76954873651266
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.36988703662063926
test acc: top1 ->  91.92 ; top5 ->  98.96  and loss:  49.63824209198356
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.29608143027871847
test acc: top1 ->  92.16 ; top5 ->  99.01  and loss:  49.985115215182304
==> this epoch:  37 / 512
---------------- start layer  9  ---------------
adv train loss:  -1894.7673740386963 , diff:  1894.7673740386963
adv train loss:  -1913.8522510528564 , diff:  19.084877014160156
adv train loss:  -1916.347900390625 , diff:  2.4956493377685547
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  7
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  30805.78187561035
forward train acc: top1 ->  95.67199997802734 ; top5 ->  99.882  and loss:  20.583913977257907
test acc: top1 ->  90.88 ; top5 ->  97.94  and loss:  59.78579004108906
forward train acc: top1 ->  99.72599997558594 ; top5 ->  99.998  and loss:  1.2946396926417947
test acc: top1 ->  91.35 ; top5 ->  98.33  and loss:  57.78649523854256
forward train acc: top1 ->  99.8 ; top5 ->  100.0  and loss:  0.8527291393838823
test acc: top1 ->  91.56 ; top5 ->  98.42  and loss:  57.413709580898285
forward train acc: top1 ->  99.848 ; top5 ->  100.0  and loss:  0.5795004023239017
test acc: top1 ->  91.7 ; top5 ->  98.53  and loss:  57.49979294836521
forward train acc: top1 ->  99.89199997558593 ; top5 ->  100.0  and loss:  0.4695888487622142
test acc: top1 ->  91.64 ; top5 ->  98.59  and loss:  57.90324369072914
forward train acc: top1 ->  99.89199997558593 ; top5 ->  100.0  and loss:  0.4275462019722909
test acc: top1 ->  91.75 ; top5 ->  98.59  and loss:  58.06997759640217
forward train acc: top1 ->  99.9000000024414 ; top5 ->  100.0  and loss:  0.42861751280725
test acc: top1 ->  91.79 ; top5 ->  98.61  and loss:  57.64476729929447
forward train acc: top1 ->  99.908 ; top5 ->  100.0  and loss:  0.3661265670089051
test acc: top1 ->  91.86 ; top5 ->  98.71  and loss:  57.473244711756706
forward train acc: top1 ->  99.9040000024414 ; top5 ->  100.0  and loss:  0.3825071610044688
test acc: top1 ->  91.91 ; top5 ->  98.72  and loss:  57.169961139559746
forward train acc: top1 ->  99.88399997558594 ; top5 ->  99.996  and loss:  0.417929318966344
test acc: top1 ->  91.77 ; top5 ->  98.65  and loss:  57.7236994355917
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  8 / 512 , inc:  1
---------------- start layer  10  ---------------
### skip layer  10 wait:  3  ###
---------------- start layer  11  ---------------
adv train loss:  -320.46389412879944 , diff:  320.46389412879944
adv train loss:  -321.6749680042267 , diff:  1.211073875427246
adv train loss:  -320.6428692340851 , diff:  1.0320987701416016
adv train loss:  -320.87149596214294 , diff:  0.22862672805786133
layer  11  adv train finish, try to retain  468
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -837.7783136367798 , diff:  837.7783136367798
adv train loss:  -1328.2621536254883 , diff:  490.4838399887085
adv train loss:  -1613.5363502502441 , diff:  285.27419662475586
adv train loss:  -1701.3557977676392 , diff:  87.81944751739502
adv train loss:  -1910.6966342926025 , diff:  209.34083652496338
adv train loss:  -1908.123104095459 , diff:  2.5735301971435547
adv train loss:  -1910.1691188812256 , diff:  2.0460147857666016
adv train loss:  -1908.4218654632568 , diff:  1.74725341796875
adv train loss:  -1909.9859580993652 , diff:  1.5640926361083984
adv train loss:  -1909.8693981170654 , diff:  0.11655998229980469
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  44
test acc: top1 ->  10.0 ; top5 ->  50.82  and loss:  52947.79800415039
forward train acc: top1 ->  84.06200000244141 ; top5 ->  97.456  and loss:  108.6741218790412
test acc: top1 ->  89.63 ; top5 ->  98.52  and loss:  65.26232846081257
forward train acc: top1 ->  99.19599998046876 ; top5 ->  99.998  and loss:  5.925794344395399
test acc: top1 ->  90.48 ; top5 ->  98.58  and loss:  59.507656417787075
forward train acc: top1 ->  99.59599997558594 ; top5 ->  99.996  and loss:  2.9151559937745333
test acc: top1 ->  90.99 ; top5 ->  98.65  and loss:  57.19462192058563
forward train acc: top1 ->  99.678 ; top5 ->  100.0  and loss:  1.9050060892477632
test acc: top1 ->  91.23 ; top5 ->  98.56  and loss:  56.2924637272954
forward train acc: top1 ->  99.84399997558593 ; top5 ->  99.998  and loss:  1.204340061172843
test acc: top1 ->  91.33 ; top5 ->  98.63  and loss:  55.810062780976295
forward train acc: top1 ->  99.87599997558594 ; top5 ->  100.0  and loss:  0.9299348993226886
test acc: top1 ->  91.47 ; top5 ->  98.63  and loss:  55.535316579043865
forward train acc: top1 ->  99.86 ; top5 ->  100.0  and loss:  0.8820033054798841
test acc: top1 ->  91.69 ; top5 ->  98.59  and loss:  55.41555159538984
forward train acc: top1 ->  99.89599997558594 ; top5 ->  99.998  and loss:  0.730898485518992
test acc: top1 ->  91.75 ; top5 ->  98.57  and loss:  55.316179782152176
forward train acc: top1 ->  99.904 ; top5 ->  100.0  and loss:  0.6860222816467285
test acc: top1 ->  91.73 ; top5 ->  98.56  and loss:  55.235141426324844
forward train acc: top1 ->  99.92599997558594 ; top5 ->  99.998  and loss:  0.5721683911979198
test acc: top1 ->  91.84 ; top5 ->  98.57  and loss:  55.3907365500927
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  46 / 512 , inc:  2
---------------- start layer  13  ---------------
### skip layer  13 wait:  3  ###
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.546875  ==>  140 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.072265625  ==>  37 / 512 , inc:  18
layer  9  :  0.015625  ==>  8 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.0078125  ==>  4 / 512 , inc:  1
layer  12  :  0.08984375  ==>  46 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [1.1084189100265502, 1.4778918800354004, 1.1084189100265502, 1.1084189100265502, 1.1084189100265502, 1.1084189100265502, 1.1084189100265502, 0.7389459400177002, 1.313681671142578, 0.9852612533569336, 0.8313141825199126, 1.9705225067138672, 1.1084189100265502, 15.764180053710938]  wait [3, 2, 3, 3, 3, 3, 2, 2, 0, 4, 2, 2, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 18, 1, 1, 1, 1, 1]  tol: 5
$$$$$$$$$$$$$ epoch  63  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
adv train loss:  -136.16989135742188 , diff:  136.16989135742188
adv train loss:  -136.30209511518478 , diff:  0.13220375776290894
layer  1  adv train finish, try to retain  44
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
adv train loss:  -145.82619601488113 , diff:  145.82619601488113
adv train loss:  -773.1819401979446 , diff:  627.3557441830635
adv train loss:  -1677.6220026016235 , diff:  904.4400624036789
adv train loss:  -1929.144359588623 , diff:  251.5223569869995
adv train loss:  -2541.66943359375 , diff:  612.525074005127
adv train loss:  -2782.233467102051 , diff:  240.56403350830078
adv train loss:  -2834.844701766968 , diff:  52.61123466491699
adv train loss:  -2850.821300506592 , diff:  15.976598739624023
adv train loss:  -2889.9540729522705 , diff:  39.13277244567871
adv train loss:  -2896.0449295043945 , diff:  6.090856552124023
layer  6  adv train finish, try to retain  219
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -211.1794149875641 , diff:  211.1794149875641
adv train loss:  -213.61715626716614 , diff:  2.437741279602051
adv train loss:  -211.40906953811646 , diff:  2.2080867290496826
adv train loss:  -211.59218549728394 , diff:  0.18311595916748047
layer  7  adv train finish, try to retain  250
test acc: top1 ->  15.03 ; top5 ->  61.14  and loss:  1563.8946466445923
forward train acc: top1 ->  94.86799998046875 ; top5 ->  99.894  and loss:  16.103939935564995
test acc: top1 ->  89.04 ; top5 ->  98.73  and loss:  59.486449897289276
forward train acc: top1 ->  98.85999997802735 ; top5 ->  99.992  and loss:  3.625292317941785
test acc: top1 ->  90.12 ; top5 ->  99.0  and loss:  55.20049737393856
forward train acc: top1 ->  99.298 ; top5 ->  99.996  and loss:  2.2824371149763465
test acc: top1 ->  90.62 ; top5 ->  98.98  and loss:  55.50521932542324
forward train acc: top1 ->  99.47999997558594 ; top5 ->  99.998  and loss:  1.7305540405213833
test acc: top1 ->  90.82 ; top5 ->  99.05  and loss:  55.56040704250336
forward train acc: top1 ->  99.60799997558594 ; top5 ->  99.996  and loss:  1.316068020183593
test acc: top1 ->  91.02 ; top5 ->  99.17  and loss:  54.50482973456383
forward train acc: top1 ->  99.62799997558594 ; top5 ->  100.0  and loss:  1.21729030366987
test acc: top1 ->  91.02 ; top5 ->  99.17  and loss:  55.00747188180685
forward train acc: top1 ->  99.6480000024414 ; top5 ->  99.996  and loss:  1.0800156723707914
test acc: top1 ->  91.14 ; top5 ->  99.15  and loss:  55.3555576428771
forward train acc: top1 ->  99.706 ; top5 ->  100.0  and loss:  0.9478596604894847
test acc: top1 ->  91.17 ; top5 ->  99.11  and loss:  56.7216467410326
forward train acc: top1 ->  99.718 ; top5 ->  99.998  and loss:  0.9344847414176911
test acc: top1 ->  91.24 ; top5 ->  99.13  and loss:  56.044413931667805
forward train acc: top1 ->  99.7180000024414 ; top5 ->  100.0  and loss:  0.8166752397082746
test acc: top1 ->  91.34 ; top5 ->  99.07  and loss:  57.45649157464504
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  253 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -428.6222085952759 , diff:  428.6222085952759
adv train loss:  -491.3946304321289 , diff:  62.77242183685303
adv train loss:  -588.7579026222229 , diff:  97.363272190094
adv train loss:  -645.3773446083069 , diff:  56.619441986083984
adv train loss:  -645.9370360374451 , diff:  0.5596914291381836
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  19
test acc: top1 ->  16.14 ; top5 ->  62.95  and loss:  7659.824993133545
forward train acc: top1 ->  96.63800000732422 ; top5 ->  99.96  and loss:  15.778941100463271
test acc: top1 ->  90.91 ; top5 ->  98.93  and loss:  52.036223500967026
forward train acc: top1 ->  99.51000000488281 ; top5 ->  99.996  and loss:  1.6196098225191236
test acc: top1 ->  91.25 ; top5 ->  99.08  and loss:  51.46700017154217
forward train acc: top1 ->  99.74400000244141 ; top5 ->  99.998  and loss:  0.9187631260138005
test acc: top1 ->  91.4 ; top5 ->  99.06  and loss:  50.95838712900877
forward train acc: top1 ->  99.77199997558594 ; top5 ->  99.998  and loss:  0.748613269533962
test acc: top1 ->  91.65 ; top5 ->  99.08  and loss:  51.36945420503616
forward train acc: top1 ->  99.79399997558593 ; top5 ->  100.0  and loss:  0.6724551799707115
test acc: top1 ->  91.69 ; top5 ->  99.09  and loss:  51.41866873949766
forward train acc: top1 ->  99.82999997558593 ; top5 ->  100.0  and loss:  0.5822915977332741
test acc: top1 ->  91.69 ; top5 ->  99.16  and loss:  52.091324493288994
forward train acc: top1 ->  99.81399997558594 ; top5 ->  100.0  and loss:  0.5593895942438394
test acc: top1 ->  91.95 ; top5 ->  99.1  and loss:  52.206549637019634
forward train acc: top1 ->  99.86999997558594 ; top5 ->  100.0  and loss:  0.49464368633925915
test acc: top1 ->  91.84 ; top5 ->  99.15  and loss:  52.53941135853529
forward train acc: top1 ->  99.866 ; top5 ->  100.0  and loss:  0.4467038279399276
test acc: top1 ->  91.95 ; top5 ->  99.15  and loss:  52.00775162130594
forward train acc: top1 ->  99.896 ; top5 ->  100.0  and loss:  0.3699154758360237
test acc: top1 ->  91.95 ; top5 ->  99.19  and loss:  52.67633482813835
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  37 / 512 , inc:  18
---------------- start layer  9  ---------------
### skip layer  9 wait:  4  ###
---------------- start layer  10  ---------------
adv train loss:  -67.15322321653366 , diff:  67.15322321653366
adv train loss:  -66.36058032512665 , diff:  0.7926428914070129
adv train loss:  -65.89571240544319 , diff:  0.4648679196834564
adv train loss:  -66.40821945667267 , diff:  0.5125070512294769
adv train loss:  -66.4151237308979 , diff:  0.006904274225234985
layer  10  adv train finish, try to retain  492
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -2651.0237159729004 , diff:  2651.0237159729004
adv train loss:  -2744.667657852173 , diff:  93.64394187927246
adv train loss:  -2744.5043334960938 , diff:  0.16332435607910156
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  3
test acc: top1 ->  10.0 ; top5 ->  59.76  and loss:  24419.015075683594
forward train acc: top1 ->  98.56 ; top5 ->  99.998  and loss:  4.851973590673879
test acc: top1 ->  91.36 ; top5 ->  99.07  and loss:  63.353557869791985
forward train acc: top1 ->  99.88 ; top5 ->  100.0  and loss:  0.49339754902757704
test acc: top1 ->  91.72 ; top5 ->  99.06  and loss:  61.7485174536705
forward train acc: top1 ->  99.924 ; top5 ->  100.0  and loss:  0.3437936077825725
test acc: top1 ->  91.78 ; top5 ->  99.15  and loss:  61.63119263201952
forward train acc: top1 ->  99.936 ; top5 ->  100.0  and loss:  0.2746147274156101
test acc: top1 ->  91.82 ; top5 ->  99.06  and loss:  62.17446479946375
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.2265343051403761
test acc: top1 ->  91.96 ; top5 ->  99.06  and loss:  61.68155497312546
forward train acc: top1 ->  99.94 ; top5 ->  100.0  and loss:  0.20486122643342242
test acc: top1 ->  92.09 ; top5 ->  99.05  and loss:  61.99513986706734
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.2060550429741852
test acc: top1 ->  92.04 ; top5 ->  99.14  and loss:  62.158825382590294
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.1341731632128358
test acc: top1 ->  92.16 ; top5 ->  99.12  and loss:  61.65102061629295
==> this epoch:  3 / 512
---------------- start layer  12  ---------------
adv train loss:  -0.1577075228560716 , diff:  0.1577075228560716
adv train loss:  -147.06458440050483 , diff:  146.90687687764876
adv train loss:  -1088.9959235191345 , diff:  941.9313391186297
adv train loss:  -1281.5900707244873 , diff:  192.59414720535278
adv train loss:  -1282.6992044448853 , diff:  1.1091337203979492
adv train loss:  -1280.9468336105347 , diff:  1.752370834350586
adv train loss:  -1283.6298503875732 , diff:  2.683016777038574
adv train loss:  -1281.496548652649 , diff:  2.1333017349243164
adv train loss:  -1282.5259523391724 , diff:  1.0294036865234375
adv train loss:  -1280.4898843765259 , diff:  2.0360679626464844
layer  12  adv train finish, try to retain  470
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -5713.219585418701 , diff:  5713.219585418701
adv train loss:  -8740.445930480957 , diff:  3027.226345062256
adv train loss:  -12720.449111938477 , diff:  3980.0031814575195
adv train loss:  -17097.674041748047 , diff:  4377.22492980957
adv train loss:  -21071.168182373047 , diff:  3973.494140625
adv train loss:  -24900.862884521484 , diff:  3829.6947021484375
adv train loss:  -28687.659881591797 , diff:  3786.7969970703125
adv train loss:  -32397.958892822266 , diff:  3710.2990112304688
adv train loss:  -36064.36880493164 , diff:  3666.409912109375
adv train loss:  -39823.18395996094 , diff:  3758.815155029297
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  8
test acc: top1 ->  33.16 ; top5 ->  78.42  and loss:  1254.7157201766968
forward train acc: top1 ->  55.87599997314453 ; top5 ->  84.09799999023437  and loss:  446.3297505378723
test acc: top1 ->  70.85 ; top5 ->  95.61  and loss:  167.34080028533936
forward train acc: top1 ->  87.34799998046876 ; top5 ->  99.764  and loss:  55.662528932094574
test acc: top1 ->  88.4 ; top5 ->  98.42  and loss:  62.74149993062019
forward train acc: top1 ->  98.838 ; top5 ->  99.998  and loss:  20.23397235572338
test acc: top1 ->  89.21 ; top5 ->  98.56  and loss:  55.97931316494942
forward train acc: top1 ->  99.15999997802734 ; top5 ->  99.994  and loss:  14.012741968035698
test acc: top1 ->  89.24 ; top5 ->  98.55  and loss:  54.65646646916866
forward train acc: top1 ->  99.31600000244141 ; top5 ->  100.0  and loss:  10.431920811533928
test acc: top1 ->  89.44 ; top5 ->  98.48  and loss:  55.06238354742527
forward train acc: top1 ->  99.4120000024414 ; top5 ->  100.0  and loss:  8.496123604476452
test acc: top1 ->  89.49 ; top5 ->  98.5  and loss:  55.23004129528999
forward train acc: top1 ->  99.43200000732422 ; top5 ->  99.994  and loss:  7.703796111047268
test acc: top1 ->  89.61 ; top5 ->  98.51  and loss:  55.50960421562195
forward train acc: top1 ->  99.4500000024414 ; top5 ->  100.0  and loss:  6.808311313390732
test acc: top1 ->  89.57 ; top5 ->  98.48  and loss:  56.49704971909523
forward train acc: top1 ->  99.53599997558594 ; top5 ->  100.0  and loss:  5.917229443788528
test acc: top1 ->  89.72 ; top5 ->  98.49  and loss:  57.17819030582905
forward train acc: top1 ->  99.55399997558594 ; top5 ->  99.998  and loss:  5.44627370685339
test acc: top1 ->  89.86 ; top5 ->  98.57  and loss:  57.861772418022156
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  9 / 512 , inc:  1
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.546875  ==>  140 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.072265625  ==>  37 / 512 , inc:  9
layer  9  :  0.015625  ==>  8 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.005859375  ==>  3 / 512 , inc:  1
layer  12  :  0.08984375  ==>  46 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [1.1084189100265502, 2.955783760070801, 1.1084189100265502, 1.1084189100265502, 1.1084189100265502, 1.1084189100265502, 2.2168378200531005, 0.5542094550132751, 0.9852612533569336, 0.9852612533569336, 1.6626283650398253, 1.9705225067138672, 2.2168378200531005, 11.823135040283203]  wait [2, 2, 2, 2, 2, 2, 2, 4, 2, 3, 2, 0, 2, 4]  inc [1, 1, 1, 1, 1, 1, 1, 1, 9, 1, 1, 1, 1, 1]  tol: 5
$$$$$$$$$$$$$ epoch  64  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1856.7190017700195 , diff:  1856.7190017700195
adv train loss:  -1828.2959671020508 , diff:  28.42303466796875
adv train loss:  -2069.2541484832764 , diff:  240.9581813812256
adv train loss:  -2074.701499938965 , diff:  5.447351455688477
adv train loss:  -2117.6016216278076 , diff:  42.90012168884277
adv train loss:  -2140.232255935669 , diff:  22.630634307861328
adv train loss:  -2174.675416946411 , diff:  34.44316101074219
adv train loss:  -2177.9098567962646 , diff:  3.2344398498535156
adv train loss:  -2173.6731567382812 , diff:  4.236700057983398
adv train loss:  -2174.0270023345947 , diff:  0.35384559631347656
layer  0  adv train finish, try to retain  56
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -598.8279318809509 , diff:  598.8279318809509
adv train loss:  -598.4203023910522 , diff:  0.40762948989868164
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  43
test acc: top1 ->  18.49 ; top5 ->  54.54  and loss:  1714.1354990005493
forward train acc: top1 ->  94.23999997802734 ; top5 ->  99.992  and loss:  51.477438278496265
test acc: top1 ->  91.38 ; top5 ->  98.72  and loss:  84.7424747645855
forward train acc: top1 ->  99.76200000244141 ; top5 ->  100.0  and loss:  0.7621018420904875
test acc: top1 ->  91.53 ; top5 ->  98.77  and loss:  79.77368420362473
forward train acc: top1 ->  99.85999997558594 ; top5 ->  99.998  and loss:  0.42875736951828003
test acc: top1 ->  91.65 ; top5 ->  98.74  and loss:  80.2045200318098
forward train acc: top1 ->  99.868 ; top5 ->  100.0  and loss:  0.3773597542895004
test acc: top1 ->  91.87 ; top5 ->  98.8  and loss:  78.8249446451664
forward train acc: top1 ->  99.91 ; top5 ->  100.0  and loss:  0.2831335347145796
test acc: top1 ->  91.99 ; top5 ->  98.79  and loss:  78.67472791671753
forward train acc: top1 ->  99.912 ; top5 ->  100.0  and loss:  0.2812885570456274
test acc: top1 ->  91.9 ; top5 ->  98.8  and loss:  78.098178550601
forward train acc: top1 ->  99.918 ; top5 ->  100.0  and loss:  0.21369836677331477
test acc: top1 ->  91.97 ; top5 ->  98.77  and loss:  78.50711597502232
forward train acc: top1 ->  99.928 ; top5 ->  100.0  and loss:  0.20601550689025316
test acc: top1 ->  91.94 ; top5 ->  98.79  and loss:  77.40574817359447
forward train acc: top1 ->  99.92600000244141 ; top5 ->  100.0  and loss:  0.2607933506369591
test acc: top1 ->  91.85 ; top5 ->  98.87  and loss:  76.76390367746353
forward train acc: top1 ->  99.942 ; top5 ->  100.0  and loss:  0.1836566551210126
test acc: top1 ->  91.92 ; top5 ->  98.9  and loss:  76.35591293871403
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  44 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -631.9953907895833 , diff:  631.9953907895833
adv train loss:  -1590.321509361267 , diff:  958.3261185716838
adv train loss:  -1594.3133087158203 , diff:  3.9917993545532227
adv train loss:  -1622.7152700424194 , diff:  28.40196132659912
adv train loss:  -1622.3527355194092 , diff:  0.3625345230102539
adv train loss:  -1621.3909635543823 , diff:  0.9617719650268555
adv train loss:  -1613.5592069625854 , diff:  7.831756591796875
adv train loss:  -1624.4344720840454 , diff:  10.875265121459961
adv train loss:  -1621.015293121338 , diff:  3.4191789627075195
adv train loss:  -1619.4766597747803 , diff:  1.5386333465576172
layer  2  adv train finish, try to retain  124
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -1295.2106340266764 , diff:  1295.2106340266764
adv train loss:  -2250.4026260375977 , diff:  955.1919920109212
adv train loss:  -2281.4068565368652 , diff:  31.004230499267578
adv train loss:  -2299.0408573150635 , diff:  17.634000778198242
adv train loss:  -2306.294183731079 , diff:  7.253326416015625
adv train loss:  -2305.954957962036 , diff:  0.33922576904296875
adv train loss:  -2347.644052505493 , diff:  41.68909454345703
adv train loss:  -2352.676507949829 , diff:  5.0324554443359375
adv train loss:  -2350.404094696045 , diff:  2.2724132537841797
adv train loss:  -2382.094846725464 , diff:  31.690752029418945
layer  3  adv train finish, try to retain  118
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -476.8281505210325 , diff:  476.8281505210325
adv train loss:  -2322.545976638794 , diff:  1845.7178261177614
adv train loss:  -2419.5956630706787 , diff:  97.04968643188477
adv train loss:  -2458.8801441192627 , diff:  39.284481048583984
adv train loss:  -2647.3434410095215 , diff:  188.4632968902588
adv train loss:  -2732.986873626709 , diff:  85.6434326171875
adv train loss:  -2740.8696575164795 , diff:  7.882783889770508
adv train loss:  -2746.181465148926 , diff:  5.311807632446289
adv train loss:  -2747.986951828003 , diff:  1.8054866790771484
adv train loss:  -2786.470338821411 , diff:  38.4833869934082
layer  4  adv train finish, try to retain  211
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -926.1195011269301 , diff:  926.1195011269301
adv train loss:  -3049.880163192749 , diff:  2123.760662065819
adv train loss:  -3195.0717334747314 , diff:  145.19157028198242
adv train loss:  -3256.90767288208 , diff:  61.83593940734863
adv train loss:  -3277.289352416992 , diff:  20.38167953491211
adv train loss:  -3272.05650138855 , diff:  5.232851028442383
adv train loss:  -3278.782751083374 , diff:  6.726249694824219
adv train loss:  -3315.1823387145996 , diff:  36.399587631225586
adv train loss:  -3333.54093170166 , diff:  18.358592987060547
adv train loss:  -3336.7831172943115 , diff:  3.242185592651367
layer  5  adv train finish, try to retain  228
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -846.3216127129272 , diff:  846.3216127129272
adv train loss:  -3345.375883102417 , diff:  2499.05427038949
adv train loss:  -3566.9482231140137 , diff:  221.57234001159668
adv train loss:  -3566.461154937744 , diff:  0.48706817626953125
adv train loss:  -3566.8245277404785 , diff:  0.363372802734375
adv train loss:  -3571.140769958496 , diff:  4.316242218017578
adv train loss:  -3571.4400024414062 , diff:  0.29923248291015625
adv train loss:  -3560.6557426452637 , diff:  10.784259796142578
adv train loss:  -3568.7445640563965 , diff:  8.088821411132812
adv train loss:  -3548.067356109619 , diff:  20.677207946777344
layer  6  adv train finish, try to retain  5
test acc: top1 ->  10.26 ; top5 ->  50.0  and loss:  1559.8602685928345
forward train acc: top1 ->  47.91799999633789 ; top5 ->  91.37400001953125  and loss:  262.8695877790451
test acc: top1 ->  54.41 ; top5 ->  92.89  and loss:  166.366313457489
forward train acc: top1 ->  61.75999998657227 ; top5 ->  95.87799999267578  and loss:  111.80666464567184
test acc: top1 ->  60.42 ; top5 ->  94.32  and loss:  127.79687011241913
forward train acc: top1 ->  66.48800001464843 ; top5 ->  97.11400001953125  and loss:  95.04394084215164
test acc: top1 ->  65.86 ; top5 ->  95.47  and loss:  112.13536125421524
forward train acc: top1 ->  71.38799997802734 ; top5 ->  97.93600000976562  and loss:  82.08268928527832
test acc: top1 ->  69.22 ; top5 ->  96.0  and loss:  104.21256428956985
forward train acc: top1 ->  74.06999998779297 ; top5 ->  98.47399998046875  and loss:  73.90617775917053
test acc: top1 ->  71.31 ; top5 ->  96.66  and loss:  99.10380148887634
forward train acc: top1 ->  76.81599999267578 ; top5 ->  98.77600000732421  and loss:  67.44301146268845
test acc: top1 ->  72.94 ; top5 ->  97.1  and loss:  93.42411476373672
forward train acc: top1 ->  77.84799999511719 ; top5 ->  98.89400000732422  and loss:  63.91342514753342
test acc: top1 ->  74.15 ; top5 ->  97.24  and loss:  90.1726702451706
forward train acc: top1 ->  78.7760000024414 ; top5 ->  98.98199997802735  and loss:  61.6972516477108
test acc: top1 ->  74.57 ; top5 ->  97.47  and loss:  89.12725043296814
forward train acc: top1 ->  79.882 ; top5 ->  99.01000000488281  and loss:  58.75389340519905
test acc: top1 ->  75.36 ; top5 ->  97.61  and loss:  85.04711878299713
forward train acc: top1 ->  80.24199999023438 ; top5 ->  99.17400000488281  and loss:  56.39943438768387
test acc: top1 ->  75.95 ; top5 ->  97.6  and loss:  83.7615157365799
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  140 / 256 , inc:  1
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
adv train loss:  -21.461061120033264 , diff:  21.461061120033264
adv train loss:  -21.6914903819561 , diff:  0.2304292619228363
adv train loss:  -21.571282729506493 , diff:  0.12020765244960785
adv train loss:  -21.367711901664734 , diff:  0.20357082784175873
adv train loss:  -21.764638483524323 , diff:  0.3969265818595886
adv train loss:  -21.64468601346016 , diff:  0.11995247006416321
adv train loss:  -21.37716481089592 , diff:  0.2675212025642395
adv train loss:  -21.550443291664124 , diff:  0.17327848076820374
adv train loss:  -21.566494673490524 , diff:  0.016051381826400757
layer  8  adv train finish, try to retain  426
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
### skip layer  9 wait:  3  ###
---------------- start layer  10  ---------------
adv train loss:  -108.73039472103119 , diff:  108.73039472103119
adv train loss:  -108.87042492628098 , diff:  0.14003020524978638
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  10
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  104113.73114013672
forward train acc: top1 ->  74.79399998779297 ; top5 ->  96.164  and loss:  90.57556718587875
test acc: top1 ->  66.61 ; top5 ->  98.04  and loss:  111.93691343069077
forward train acc: top1 ->  98.5320000024414 ; top5 ->  99.992  and loss:  9.841564275324345
test acc: top1 ->  90.37 ; top5 ->  98.47  and loss:  49.27304509282112
forward train acc: top1 ->  99.3880000024414 ; top5 ->  100.0  and loss:  3.2311503700912
test acc: top1 ->  90.54 ; top5 ->  98.41  and loss:  50.646937891840935
forward train acc: top1 ->  99.64599997558594 ; top5 ->  99.994  and loss:  1.7713756822049618
test acc: top1 ->  90.97 ; top5 ->  98.45  and loss:  51.66176977753639
forward train acc: top1 ->  99.67999997558594 ; top5 ->  99.998  and loss:  1.3284598533064127
test acc: top1 ->  91.19 ; top5 ->  98.55  and loss:  52.4597510099411
forward train acc: top1 ->  99.756 ; top5 ->  100.0  and loss:  1.0667672199197114
test acc: top1 ->  91.23 ; top5 ->  98.54  and loss:  52.90064637362957
forward train acc: top1 ->  99.79 ; top5 ->  100.0  and loss:  0.9356508078053594
test acc: top1 ->  91.26 ; top5 ->  98.53  and loss:  53.86961279809475
forward train acc: top1 ->  99.81200000244141 ; top5 ->  100.0  and loss:  0.8076937450096011
test acc: top1 ->  91.26 ; top5 ->  98.6  and loss:  53.823367431759834
forward train acc: top1 ->  99.86399997558594 ; top5 ->  100.0  and loss:  0.6486426154151559
test acc: top1 ->  91.33 ; top5 ->  98.67  and loss:  54.32655404508114
forward train acc: top1 ->  99.82999997558593 ; top5 ->  100.0  and loss:  0.7137130158953369
test acc: top1 ->  91.33 ; top5 ->  98.62  and loss:  54.50806646049023
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  11 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -2105.151653289795 , diff:  2105.151653289795
adv train loss:  -2453.2858352661133 , diff:  348.13418197631836
adv train loss:  -2452.1072635650635 , diff:  1.1785717010498047
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  2
test acc: top1 ->  10.0 ; top5 ->  52.43  and loss:  36465.98742675781
forward train acc: top1 ->  97.71999997558594 ; top5 ->  99.98  and loss:  8.354372459463775
test acc: top1 ->  91.08 ; top5 ->  98.57  and loss:  59.82642839848995
forward train acc: top1 ->  99.85 ; top5 ->  100.0  and loss:  0.7962710987776518
test acc: top1 ->  91.12 ; top5 ->  98.61  and loss:  62.340672463178635
forward train acc: top1 ->  99.848 ; top5 ->  100.0  and loss:  0.6500799648929387
test acc: top1 ->  91.38 ; top5 ->  98.66  and loss:  62.00810848176479
forward train acc: top1 ->  99.874 ; top5 ->  100.0  and loss:  0.49096794333308935
test acc: top1 ->  91.38 ; top5 ->  98.62  and loss:  63.62902604043484
forward train acc: top1 ->  99.90599997558594 ; top5 ->  100.0  and loss:  0.389662591740489
test acc: top1 ->  91.53 ; top5 ->  98.62  and loss:  64.62678012251854
forward train acc: top1 ->  99.896 ; top5 ->  100.0  and loss:  0.332555070403032
test acc: top1 ->  91.7 ; top5 ->  98.64  and loss:  65.34077966213226
forward train acc: top1 ->  99.91 ; top5 ->  100.0  and loss:  0.33424912829650566
test acc: top1 ->  91.74 ; top5 ->  98.65  and loss:  65.48716066777706
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.21568776085041463
test acc: top1 ->  91.63 ; top5 ->  98.61  and loss:  66.21927371621132
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.23306920332834125
test acc: top1 ->  91.78 ; top5 ->  98.67  and loss:  65.36572203040123
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.2453676344593987
test acc: top1 ->  91.76 ; top5 ->  98.61  and loss:  66.57974847406149
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  3 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -503.0259703397751 , diff:  503.0259703397751
adv train loss:  -1308.428921699524 , diff:  805.4029513597488
adv train loss:  -1310.1328659057617 , diff:  1.703944206237793
adv train loss:  -1309.2246751785278 , diff:  0.9081907272338867
adv train loss:  -1308.784306526184 , diff:  0.44036865234375
adv train loss:  -1308.6322937011719 , diff:  0.15201282501220703
adv train loss:  -1309.7306432724 , diff:  1.0983495712280273
adv train loss:  -1309.7583951950073 , diff:  0.027751922607421875
layer  12  adv train finish, try to retain  1
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  7161.332702636719
forward train acc: top1 ->  30.315999986572265 ; top5 ->  76.36200001464844  and loss:  495.6585228443146
test acc: top1 ->  35.53 ; top5 ->  87.25  and loss:  180.06113743782043
forward train acc: top1 ->  49.33599998779297 ; top5 ->  95.908  and loss:  129.61102616786957
test acc: top1 ->  51.79 ; top5 ->  96.37  and loss:  136.80759572982788
forward train acc: top1 ->  59.394 ; top5 ->  99.89199997558593  and loss:  108.40672433376312
test acc: top1 ->  52.14 ; top5 ->  96.64  and loss:  130.96371138095856
forward train acc: top1 ->  63.90199998779297 ; top5 ->  99.9  and loss:  100.17469030618668
test acc: top1 ->  58.0 ; top5 ->  96.79  and loss:  127.87781226634979
forward train acc: top1 ->  69.18800000732422 ; top5 ->  99.93199997558594  and loss:  93.43777233362198
test acc: top1 ->  61.72 ; top5 ->  96.81  and loss:  125.09396666288376
forward train acc: top1 ->  71.01599997070312 ; top5 ->  99.93199997558594  and loss:  89.61177486181259
test acc: top1 ->  65.51 ; top5 ->  96.89  and loss:  124.11093604564667
forward train acc: top1 ->  72.66800001464844 ; top5 ->  99.93599997558594  and loss:  87.53157186508179
test acc: top1 ->  62.87 ; top5 ->  96.85  and loss:  124.39663845300674
forward train acc: top1 ->  74.74600000976562 ; top5 ->  99.972  and loss:  85.12463527917862
test acc: top1 ->  67.08 ; top5 ->  96.89  and loss:  123.47906565666199
forward train acc: top1 ->  74.64000001708985 ; top5 ->  99.966  and loss:  83.05298644304276
test acc: top1 ->  68.02 ; top5 ->  96.91  and loss:  123.00137144327164
forward train acc: top1 ->  76.648 ; top5 ->  99.93399997558593  and loss:  81.22162079811096
test acc: top1 ->  66.32 ; top5 ->  96.84  and loss:  122.60807543992996
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  46 / 512 , inc:  1
---------------- start layer  13  ---------------
### skip layer  13 wait:  4  ###
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.546875  ==>  140 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.072265625  ==>  37 / 512 , inc:  9
layer  9  :  0.015625  ==>  8 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.005859375  ==>  3 / 512 , inc:  1
layer  12  :  0.08984375  ==>  46 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [2.2168378200531005, 2.2168378200531005, 2.2168378200531005, 2.2168378200531005, 2.2168378200531005, 2.2168378200531005, 1.6626283650398253, 0.5542094550132751, 1.9705225067138672, 0.9852612533569336, 1.2469712737798688, 1.4778918800354004, 1.6626283650398253, 11.823135040283203]  wait [2, 4, 2, 2, 2, 2, 4, 3, 2, 2, 4, 2, 4, 3]  inc [1, 1, 1, 1, 1, 1, 1, 1, 9, 1, 1, 1, 1, 1]  tol: 5
$$$$$$$$$$$$$ epoch  65  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -2303.1078748703003 , diff:  2303.1078748703003
adv train loss:  -2353.859432220459 , diff:  50.75155735015869
adv train loss:  -2362.0392112731934 , diff:  8.179779052734375
adv train loss:  -2372.1936416625977 , diff:  10.154430389404297
adv train loss:  -2367.072311401367 , diff:  5.121330261230469
adv train loss:  -2363.31742477417 , diff:  3.7548866271972656
adv train loss:  -2371.3534965515137 , diff:  8.03607177734375
adv train loss:  -2367.3532905578613 , diff:  4.000205993652344
adv train loss:  -2354.348400115967 , diff:  13.004890441894531
adv train loss:  -2219.8883056640625 , diff:  134.4600944519043
layer  0  adv train finish, try to retain  2
test acc: top1 ->  10.0 ; top5 ->  46.59  and loss:  2959.128215789795
forward train acc: top1 ->  14.243999996643067 ; top5 ->  59.12199998779297  and loss:  953.1415019035339
test acc: top1 ->  14.0 ; top5 ->  52.37  and loss:  971.6797938346863
forward train acc: top1 ->  17.17000000366211 ; top5 ->  68.47400000976563  and loss:  279.07813143730164
test acc: top1 ->  20.16 ; top5 ->  71.93  and loss:  231.3749713897705
forward train acc: top1 ->  21.240000001831056 ; top5 ->  73.03599999755859  and loss:  218.94881582260132
test acc: top1 ->  22.64 ; top5 ->  75.52  and loss:  211.00309789180756
forward train acc: top1 ->  23.00199999938965 ; top5 ->  75.52799997314453  and loss:  205.4160932302475
test acc: top1 ->  23.63 ; top5 ->  76.62  and loss:  205.7661736011505
forward train acc: top1 ->  23.907999994506834 ; top5 ->  76.89199999267578  and loss:  201.14905297756195
test acc: top1 ->  24.9 ; top5 ->  78.05  and loss:  201.76646447181702
forward train acc: top1 ->  24.499999999389647 ; top5 ->  77.49000000732421  and loss:  198.50476479530334
test acc: top1 ->  25.51 ; top5 ->  79.14  and loss:  198.92638158798218
forward train acc: top1 ->  25.13199999694824 ; top5 ->  78.24600000976562  and loss:  197.08138763904572
test acc: top1 ->  26.03 ; top5 ->  79.74  and loss:  197.41377711296082
forward train acc: top1 ->  25.570000002441407 ; top5 ->  78.47000001708984  and loss:  196.01973223686218
test acc: top1 ->  26.87 ; top5 ->  80.15  and loss:  195.99906182289124
forward train acc: top1 ->  26.214000004882813 ; top5 ->  79.15799997558594  and loss:  194.42860174179077
test acc: top1 ->  27.3 ; top5 ->  80.59  and loss:  194.88716900348663
forward train acc: top1 ->  26.891999996948243 ; top5 ->  79.54199997802735  and loss:  192.9386261701584
test acc: top1 ->  27.94 ; top5 ->  81.16  and loss:  193.22288274765015
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -139.31173646450043 , diff:  139.31173646450043
adv train loss:  -139.28310751914978 , diff:  0.028628945350646973
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  43
test acc: top1 ->  15.86 ; top5 ->  60.78  and loss:  229.2766056060791
forward train acc: top1 ->  94.97999997558594 ; top5 ->  99.836  and loss:  27.794306380674243
test acc: top1 ->  91.25 ; top5 ->  99.03  and loss:  40.79547590017319
forward train acc: top1 ->  99.82400000244141 ; top5 ->  100.0  and loss:  0.9820586610585451
test acc: top1 ->  91.55 ; top5 ->  99.09  and loss:  45.12188258767128
forward train acc: top1 ->  99.864 ; top5 ->  100.0  and loss:  0.5991808499675244
test acc: top1 ->  91.8 ; top5 ->  99.13  and loss:  47.28011825680733
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.38125174352899194
test acc: top1 ->  91.77 ; top5 ->  99.09  and loss:  49.50572130084038
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.3120573952328414
test acc: top1 ->  92.1 ; top5 ->  99.04  and loss:  51.822314120829105
forward train acc: top1 ->  99.93999997558593 ; top5 ->  100.0  and loss:  0.2788074007257819
test acc: top1 ->  91.84 ; top5 ->  99.1  and loss:  52.693988889455795
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.1945233433507383
test acc: top1 ->  92.0 ; top5 ->  99.07  and loss:  53.22636837512255
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.1974151578033343
test acc: top1 ->  91.93 ; top5 ->  99.01  and loss:  54.28637434542179
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.17754105385392904
test acc: top1 ->  91.95 ; top5 ->  99.06  and loss:  54.81342498958111
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.18068918865174055
test acc: top1 ->  92.04 ; top5 ->  99.08  and loss:  54.82550348341465
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  44 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -397.0094608189538 , diff:  397.0094608189538
adv train loss:  -1023.4719591140747 , diff:  626.4624982951209
adv train loss:  -1076.9903268814087 , diff:  53.518367767333984
adv train loss:  -1089.4746828079224 , diff:  12.484355926513672
adv train loss:  -1086.7183485031128 , diff:  2.7563343048095703
adv train loss:  -1088.5880250930786 , diff:  1.8696765899658203
adv train loss:  -1086.6264219284058 , diff:  1.9616031646728516
adv train loss:  -1089.7994604110718 , diff:  3.1730384826660156
adv train loss:  -1088.693920135498 , diff:  1.1055402755737305
adv train loss:  -1088.0279026031494 , diff:  0.6660175323486328
layer  2  adv train finish, try to retain  66
test acc: top1 ->  10.0 ; top5 ->  51.01  and loss:  1312.2090396881104
forward train acc: top1 ->  97.09200001220704 ; top5 ->  99.88399997558594  and loss:  10.312781408429146
test acc: top1 ->  88.88 ; top5 ->  98.65  and loss:  51.83819256722927
forward train acc: top1 ->  97.68199998046875 ; top5 ->  99.938  and loss:  7.358847700059414
test acc: top1 ->  89.48 ; top5 ->  98.95  and loss:  48.41405211389065
forward train acc: top1 ->  98.07199998779296 ; top5 ->  99.964  and loss:  5.951775150373578
test acc: top1 ->  89.63 ; top5 ->  98.98  and loss:  48.019860818982124
forward train acc: top1 ->  98.29399998535156 ; top5 ->  99.976  and loss:  5.084578841924667
test acc: top1 ->  89.74 ; top5 ->  99.07  and loss:  48.182545840740204
forward train acc: top1 ->  98.46200001220703 ; top5 ->  99.978  and loss:  4.717375574633479
test acc: top1 ->  89.93 ; top5 ->  99.06  and loss:  46.52530471980572
forward train acc: top1 ->  98.59999997802734 ; top5 ->  99.98  and loss:  4.179573304951191
test acc: top1 ->  89.93 ; top5 ->  99.05  and loss:  46.31157933920622
forward train acc: top1 ->  98.68800000488281 ; top5 ->  99.986  and loss:  4.0019980520009995
test acc: top1 ->  90.02 ; top5 ->  99.07  and loss:  47.03101383894682
forward train acc: top1 ->  98.78800000976563 ; top5 ->  99.998  and loss:  3.5844464041292667
test acc: top1 ->  89.98 ; top5 ->  99.05  and loss:  47.56954663991928
forward train acc: top1 ->  98.80000000732421 ; top5 ->  99.988  and loss:  3.6339960116893053
test acc: top1 ->  90.07 ; top5 ->  99.07  and loss:  47.81465941667557
forward train acc: top1 ->  98.8680000024414 ; top5 ->  99.984  and loss:  3.40142160654068
test acc: top1 ->  90.09 ; top5 ->  99.04  and loss:  48.25102800130844
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -619.2269676476717 , diff:  619.2269676476717
adv train loss:  -1058.7539043426514 , diff:  439.52693669497967
adv train loss:  -1051.5722570419312 , diff:  7.181647300720215
adv train loss:  -1061.2164993286133 , diff:  9.644242286682129
adv train loss:  -1060.2004508972168 , diff:  1.0160484313964844
adv train loss:  -1052.7352018356323 , diff:  7.465249061584473
adv train loss:  -1053.1087760925293 , diff:  0.37357425689697266
adv train loss:  -1057.1369304656982 , diff:  4.028154373168945
adv train loss:  -1059.189579963684 , diff:  2.05264949798584
adv train loss:  -1056.1839628219604 , diff:  3.005617141723633
layer  3  adv train finish, try to retain  21
test acc: top1 ->  9.92 ; top5 ->  50.0  and loss:  800.6332921981812
forward train acc: top1 ->  73.65400001953125 ; top5 ->  96.89000000732422  and loss:  86.73537361621857
test acc: top1 ->  73.43 ; top5 ->  96.62  and loss:  84.23494800925255
forward train acc: top1 ->  77.77399998779296 ; top5 ->  97.94399998046875  and loss:  67.26640546321869
test acc: top1 ->  75.97 ; top5 ->  97.16  and loss:  76.3384662270546
forward train acc: top1 ->  80.08 ; top5 ->  98.26800000976563  and loss:  59.839277148246765
test acc: top1 ->  76.94 ; top5 ->  97.63  and loss:  71.89328810572624
forward train acc: top1 ->  81.29199997314453 ; top5 ->  98.56200000732422  and loss:  55.45248907804489
test acc: top1 ->  78.11 ; top5 ->  97.75  and loss:  68.28751105070114
forward train acc: top1 ->  82.28399999023438 ; top5 ->  98.66599997802734  and loss:  52.2819846868515
test acc: top1 ->  78.25 ; top5 ->  97.78  and loss:  67.6739073395729
forward train acc: top1 ->  82.99199999023438 ; top5 ->  98.828  and loss:  50.422834664583206
test acc: top1 ->  79.18 ; top5 ->  97.96  and loss:  65.52344101667404
forward train acc: top1 ->  83.55200000976562 ; top5 ->  98.83799997802734  and loss:  48.69779226183891
test acc: top1 ->  79.55 ; top5 ->  98.01  and loss:  64.19670993089676
forward train acc: top1 ->  83.68600001953125 ; top5 ->  98.79199997802735  and loss:  48.323675751686096
test acc: top1 ->  79.54 ; top5 ->  97.97  and loss:  64.00371035933495
forward train acc: top1 ->  83.9899999975586 ; top5 ->  98.96599998535156  and loss:  46.90192365646362
test acc: top1 ->  79.88 ; top5 ->  98.08  and loss:  62.565833032131195
forward train acc: top1 ->  84.31399999267578 ; top5 ->  98.96800000488281  and loss:  46.1388121843338
test acc: top1 ->  80.24 ; top5 ->  98.13  and loss:  61.711331874132156
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -73.0341293439269 , diff:  73.0341293439269
adv train loss:  -409.0630633831024 , diff:  336.0289340391755
adv train loss:  -513.9558653831482 , diff:  104.89280200004578
adv train loss:  -592.0753455162048 , diff:  78.11948013305664
adv train loss:  -603.0546760559082 , diff:  10.97933053970337
adv train loss:  -607.305742263794 , diff:  4.251066207885742
adv train loss:  -610.1354055404663 , diff:  2.8296632766723633
adv train loss:  -613.3655009269714 , diff:  3.230095386505127
adv train loss:  -611.0340256690979 , diff:  2.331475257873535
adv train loss:  -612.3140544891357 , diff:  1.2800288200378418
layer  4  adv train finish, try to retain  17
test acc: top1 ->  10.09 ; top5 ->  50.12  and loss:  546.3533773422241
forward train acc: top1 ->  66.48999998291016 ; top5 ->  95.58800001708984  and loss:  96.61391252279282
test acc: top1 ->  68.02 ; top5 ->  96.76  and loss:  92.63505041599274
forward train acc: top1 ->  71.71000001464844 ; top5 ->  97.19600001220704  and loss:  79.39440381526947
test acc: top1 ->  70.89 ; top5 ->  97.47  and loss:  84.20986890792847
forward train acc: top1 ->  74.18799997314453 ; top5 ->  97.82599998291016  and loss:  72.32366198301315
test acc: top1 ->  72.83 ; top5 ->  97.51  and loss:  79.57523256540298
forward train acc: top1 ->  75.98200001464843 ; top5 ->  98.02200000976562  and loss:  67.52161532640457
test acc: top1 ->  74.2 ; top5 ->  97.74  and loss:  75.42389363050461
forward train acc: top1 ->  77.34600001464844 ; top5 ->  98.19999998046875  and loss:  64.40709084272385
test acc: top1 ->  75.57 ; top5 ->  97.96  and loss:  72.59282153844833
forward train acc: top1 ->  77.87000000488281 ; top5 ->  98.39800000488282  and loss:  62.04425781965256
test acc: top1 ->  76.06 ; top5 ->  98.02  and loss:  70.6286288201809
forward train acc: top1 ->  78.54600000732422 ; top5 ->  98.49  and loss:  60.37253111600876
test acc: top1 ->  76.52 ; top5 ->  98.14  and loss:  69.38865783810616
forward train acc: top1 ->  79.04400001708984 ; top5 ->  98.52999998291016  and loss:  59.229073733091354
test acc: top1 ->  76.67 ; top5 ->  98.22  and loss:  68.90914604067802
forward train acc: top1 ->  79.43599997802734 ; top5 ->  98.54199998046874  and loss:  58.68055605888367
test acc: top1 ->  76.96 ; top5 ->  98.13  and loss:  68.38784736394882
forward train acc: top1 ->  79.61199999511719 ; top5 ->  98.55800000732422  and loss:  57.589275509119034
test acc: top1 ->  77.34 ; top5 ->  98.13  and loss:  67.07322376966476
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -205.90062279999256 , diff:  205.90062279999256
adv train loss:  -745.9417061805725 , diff:  540.04108338058
adv train loss:  -803.7240061759949 , diff:  57.78229999542236
adv train loss:  -833.0112752914429 , diff:  29.287269115447998
adv train loss:  -840.219256401062 , diff:  7.207981109619141
adv train loss:  -848.7860221862793 , diff:  8.566765785217285
adv train loss:  -852.9154596328735 , diff:  4.129437446594238
adv train loss:  -856.1493062973022 , diff:  3.233846664428711
adv train loss:  -867.0889053344727 , diff:  10.93959903717041
adv train loss:  -869.3223791122437 , diff:  2.233473777770996
layer  5  adv train finish, try to retain  9
test acc: top1 ->  10.0 ; top5 ->  57.92  and loss:  754.6351256370544
forward train acc: top1 ->  59.129999990234374 ; top5 ->  96.03000001220703  and loss:  108.60382467508316
test acc: top1 ->  64.32 ; top5 ->  97.12  and loss:  97.31414240598679
forward train acc: top1 ->  68.97000000244141 ; top5 ->  97.96800000976563  and loss:  82.940813601017
test acc: top1 ->  69.94 ; top5 ->  97.74  and loss:  84.94372659921646
forward train acc: top1 ->  73.96600000488282 ; top5 ->  98.40200000488281  and loss:  71.70115530490875
test acc: top1 ->  72.93 ; top5 ->  97.86  and loss:  78.9308679997921
forward train acc: top1 ->  76.49599998779297 ; top5 ->  98.59599998046875  and loss:  65.52562475204468
test acc: top1 ->  75.47 ; top5 ->  98.29  and loss:  72.77573639154434
forward train acc: top1 ->  78.39599999755859 ; top5 ->  98.89000000976563  and loss:  60.41741758584976
test acc: top1 ->  76.64 ; top5 ->  98.39  and loss:  69.38106387853622
forward train acc: top1 ->  79.84000000732422 ; top5 ->  98.99599997802734  and loss:  57.024385303258896
test acc: top1 ->  77.57 ; top5 ->  98.49  and loss:  67.5570600926876
forward train acc: top1 ->  80.37799997070313 ; top5 ->  98.95600000732422  and loss:  55.895066142082214
test acc: top1 ->  77.76 ; top5 ->  98.49  and loss:  66.67802840471268
forward train acc: top1 ->  81.018 ; top5 ->  99.05599998046875  and loss:  53.76479983329773
test acc: top1 ->  78.46 ; top5 ->  98.58  and loss:  65.49217492341995
forward train acc: top1 ->  81.38600000732421 ; top5 ->  99.11199998046875  and loss:  52.76545983552933
test acc: top1 ->  78.49 ; top5 ->  98.5  and loss:  64.61472335457802
forward train acc: top1 ->  81.57199997802735 ; top5 ->  99.11399997802734  and loss:  52.166359931230545
test acc: top1 ->  79.18 ; top5 ->  98.55  and loss:  62.99014326930046
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -99.64936739951372 , diff:  99.64936739951372
adv train loss:  -434.8968243598938 , diff:  335.2474569603801
adv train loss:  -687.5438976287842 , diff:  252.64707326889038
adv train loss:  -742.8460726737976 , diff:  55.30217504501343
adv train loss:  -790.7364664077759 , diff:  47.89039373397827
adv train loss:  -807.6516098976135 , diff:  16.915143489837646
adv train loss:  -822.3598546981812 , diff:  14.708244800567627
adv train loss:  -824.0036525726318 , diff:  1.6437978744506836
adv train loss:  -834.210994720459 , diff:  10.207342147827148
adv train loss:  -831.087776184082 , diff:  3.123218536376953
layer  6  adv train finish, try to retain  25
test acc: top1 ->  19.52 ; top5 ->  75.22  and loss:  459.8832287788391
forward train acc: top1 ->  92.16599997314454 ; top5 ->  99.846  and loss:  22.551069512963295
test acc: top1 ->  86.53 ; top5 ->  99.22  and loss:  46.38565793633461
forward train acc: top1 ->  94.76200001953126 ; top5 ->  99.942  and loss:  14.858771815896034
test acc: top1 ->  87.2 ; top5 ->  99.26  and loss:  45.76032246649265
forward train acc: top1 ->  95.81399999023438 ; top5 ->  99.926  and loss:  12.010824546217918
test acc: top1 ->  88.23 ; top5 ->  99.3  and loss:  44.24128630757332
forward train acc: top1 ->  96.43399999023437 ; top5 ->  99.958  and loss:  9.990843430161476
test acc: top1 ->  88.16 ; top5 ->  99.35  and loss:  45.721575900912285
forward train acc: top1 ->  96.84599999267579 ; top5 ->  99.96  and loss:  8.941708765923977
test acc: top1 ->  88.49 ; top5 ->  99.32  and loss:  45.17655998468399
forward train acc: top1 ->  97.19799998535156 ; top5 ->  99.978  and loss:  7.991162303835154
test acc: top1 ->  88.6 ; top5 ->  99.28  and loss:  45.73152565956116
forward train acc: top1 ->  97.29399997070313 ; top5 ->  99.982  and loss:  7.624092396348715
test acc: top1 ->  88.84 ; top5 ->  99.32  and loss:  45.33396442234516
forward train acc: top1 ->  97.44800000976562 ; top5 ->  99.972  and loss:  7.296187270432711
test acc: top1 ->  89.02 ; top5 ->  99.39  and loss:  45.168130204081535
forward train acc: top1 ->  97.50400000732422 ; top5 ->  99.97  and loss:  7.0909145921468735
test acc: top1 ->  88.97 ; top5 ->  99.41  and loss:  44.47117614746094
forward train acc: top1 ->  97.67200000488282 ; top5 ->  99.984  and loss:  6.526878532022238
test acc: top1 ->  89.1 ; top5 ->  99.34  and loss:  45.51607742905617
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  140 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -1.4237005803734064 , diff:  1.4237005803734064
adv train loss:  -1.732113691046834 , diff:  0.3084131106734276
adv train loss:  -1.6187157267704606 , diff:  0.11339796427637339
adv train loss:  -1.6677998434752226 , diff:  0.04908411670476198
adv train loss:  -1.651287973858416 , diff:  0.016511869616806507
adv train loss:  -1.5632229195907712 , diff:  0.08806505426764488
adv train loss:  -1.5659358855336905 , diff:  0.0027129659429192543
layer  7  adv train finish, try to retain  259
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -158.17814218997955 , diff:  158.17814218997955
adv train loss:  -168.2126659154892 , diff:  10.034523725509644
adv train loss:  -185.36491084098816 , diff:  17.152244925498962
adv train loss:  -186.50619208812714 , diff:  1.141281247138977
adv train loss:  -185.92306280136108 , diff:  0.5831292867660522
adv train loss:  -222.08016192913055 , diff:  36.15709912776947
adv train loss:  -278.1829946041107 , diff:  56.102832674980164
adv train loss:  -286.7767171859741 , diff:  8.593722581863403
adv train loss:  -286.4707782268524 , diff:  0.3059389591217041
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  28
test acc: top1 ->  54.46 ; top5 ->  93.86  and loss:  577.4595069885254
forward train acc: top1 ->  96.65599998046875 ; top5 ->  99.904  and loss:  10.928983880206943
test acc: top1 ->  91.02 ; top5 ->  99.18  and loss:  38.13032726943493
forward train acc: top1 ->  99.41 ; top5 ->  99.998  and loss:  2.120599084533751
test acc: top1 ->  91.28 ; top5 ->  99.29  and loss:  40.343089155852795
forward train acc: top1 ->  99.624 ; top5 ->  99.996  and loss:  1.3027641489170492
test acc: top1 ->  91.44 ; top5 ->  99.27  and loss:  42.85606952756643
forward train acc: top1 ->  99.73399997558593 ; top5 ->  100.0  and loss:  0.9023977285251021
test acc: top1 ->  91.59 ; top5 ->  99.18  and loss:  44.88296515494585
forward train acc: top1 ->  99.816 ; top5 ->  99.998  and loss:  0.7078568403376266
test acc: top1 ->  91.72 ; top5 ->  99.23  and loss:  45.7521655485034
forward train acc: top1 ->  99.83199997558594 ; top5 ->  99.998  and loss:  0.6001815851777792
test acc: top1 ->  91.74 ; top5 ->  99.21  and loss:  46.82079365849495
forward train acc: top1 ->  99.852 ; top5 ->  100.0  and loss:  0.49752694368362427
test acc: top1 ->  91.72 ; top5 ->  99.16  and loss:  48.244437143206596
forward train acc: top1 ->  99.87 ; top5 ->  100.0  and loss:  0.44654620392248034
test acc: top1 ->  91.7 ; top5 ->  99.18  and loss:  48.15463077276945
forward train acc: top1 ->  99.85999997558594 ; top5 ->  100.0  and loss:  0.4745065635070205
test acc: top1 ->  91.75 ; top5 ->  99.17  and loss:  48.65079914778471
forward train acc: top1 ->  99.87 ; top5 ->  100.0  and loss:  0.4308066943194717
test acc: top1 ->  91.8 ; top5 ->  99.22  and loss:  49.41489440947771
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  37 / 512 , inc:  9
---------------- start layer  9  ---------------
adv train loss:  -26.476982444524765 , diff:  26.476982444524765
adv train loss:  -26.52047737687826 , diff:  0.04349493235349655
layer  9  adv train finish, try to retain  493
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -93.76683539152145 , diff:  93.76683539152145
adv train loss:  -93.89542716741562 , diff:  0.12859177589416504
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  10
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  271052.400390625
forward train acc: top1 ->  74.79400001708984 ; top5 ->  97.69999997558594  and loss:  95.82496482133865
test acc: top1 ->  54.47 ; top5 ->  95.96  and loss:  175.0167340040207
forward train acc: top1 ->  98.31599998046875 ; top5 ->  99.982  and loss:  11.698642402887344
test acc: top1 ->  89.94 ; top5 ->  98.56  and loss:  44.61068123579025
forward train acc: top1 ->  99.468 ; top5 ->  99.996  and loss:  2.665403966791928
test acc: top1 ->  91.05 ; top5 ->  98.91  and loss:  42.4868404045701
forward train acc: top1 ->  99.76999997558593 ; top5 ->  100.0  and loss:  1.3025070112198591
test acc: top1 ->  91.33 ; top5 ->  98.9  and loss:  42.7710827589035
forward train acc: top1 ->  99.81999997558594 ; top5 ->  100.0  and loss:  0.8801271803677082
test acc: top1 ->  91.6 ; top5 ->  98.98  and loss:  43.81465431302786
forward train acc: top1 ->  99.884 ; top5 ->  100.0  and loss:  0.6807694572489709
test acc: top1 ->  91.78 ; top5 ->  98.97  and loss:  44.383444134145975
forward train acc: top1 ->  99.868 ; top5 ->  100.0  and loss:  0.6399523874279112
test acc: top1 ->  91.79 ; top5 ->  98.98  and loss:  44.388110764324665
forward train acc: top1 ->  99.848 ; top5 ->  99.998  and loss:  0.640294723212719
test acc: top1 ->  92.0 ; top5 ->  98.99  and loss:  44.71069400012493
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  11 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -1054.7896766662598 , diff:  1054.7896766662598
adv train loss:  -1176.6461448669434 , diff:  121.8564682006836
adv train loss:  -1196.9430484771729 , diff:  20.296903610229492
adv train loss:  -1196.4540662765503 , diff:  0.4889822006225586
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  2
test acc: top1 ->  10.0 ; top5 ->  59.83  and loss:  16847.533004760742
forward train acc: top1 ->  96.2600000024414 ; top5 ->  99.65  and loss:  19.492170564830303
test acc: top1 ->  91.27 ; top5 ->  98.66  and loss:  48.574903041124344
forward train acc: top1 ->  99.85599997558593 ; top5 ->  100.0  and loss:  0.65446603205055
test acc: top1 ->  91.79 ; top5 ->  98.9  and loss:  47.15498714148998
forward train acc: top1 ->  99.9000000024414 ; top5 ->  100.0  and loss:  0.4861493781208992
test acc: top1 ->  91.83 ; top5 ->  98.88  and loss:  48.26932258158922
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.3595169164473191
test acc: top1 ->  91.89 ; top5 ->  98.88  and loss:  48.5850193798542
forward train acc: top1 ->  99.91599997558593 ; top5 ->  100.0  and loss:  0.3205860825255513
test acc: top1 ->  92.11 ; top5 ->  98.84  and loss:  49.75866951048374
==> this epoch:  2 / 512
---------------- start layer  12  ---------------
adv train loss:  -0.282681793323718 , diff:  0.282681793323718
adv train loss:  -498.5150902196765 , diff:  498.2324084263528
adv train loss:  -1166.3962268829346 , diff:  667.8811366632581
adv train loss:  -1167.8898935317993 , diff:  1.493666648864746
adv train loss:  -1167.9046096801758 , diff:  0.014716148376464844
adv train loss:  -1166.9785776138306 , diff:  0.9260320663452148
adv train loss:  -1166.9770545959473 , diff:  0.0015230178833007812
layer  12  adv train finish, try to retain  1
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  3235.117181777954
forward train acc: top1 ->  28.300000001831055 ; top5 ->  61.03199999511719  and loss:  673.0981848239899
test acc: top1 ->  27.55 ; top5 ->  68.92  and loss:  296.5835003852844
forward train acc: top1 ->  34.14599999511719 ; top5 ->  87.89999998046875  and loss:  169.58811557292938
test acc: top1 ->  46.16 ; top5 ->  95.86  and loss:  144.52696013450623
forward train acc: top1 ->  53.83399999511719 ; top5 ->  99.60599997558593  and loss:  118.6821825504303
test acc: top1 ->  53.22 ; top5 ->  96.43  and loss:  134.62984561920166
forward train acc: top1 ->  62.16199999267578 ; top5 ->  99.812  and loss:  108.93644678592682
test acc: top1 ->  60.08 ; top5 ->  96.66  and loss:  130.85723316669464
forward train acc: top1 ->  65.87199997558594 ; top5 ->  99.808  and loss:  103.07186597585678
test acc: top1 ->  62.67 ; top5 ->  96.76  and loss:  127.75395226478577
forward train acc: top1 ->  70.12800001464844 ; top5 ->  99.852  and loss:  98.53687942028046
test acc: top1 ->  64.46 ; top5 ->  96.78  and loss:  127.43259954452515
forward train acc: top1 ->  72.45799998779297 ; top5 ->  99.912  and loss:  95.51101779937744
test acc: top1 ->  63.96 ; top5 ->  96.67  and loss:  125.67386221885681
forward train acc: top1 ->  73.63999998046874 ; top5 ->  99.924  and loss:  92.90102916955948
test acc: top1 ->  65.83 ; top5 ->  96.69  and loss:  124.60113507509232
forward train acc: top1 ->  74.22000001953126 ; top5 ->  99.93399997558593  and loss:  90.69524335861206
test acc: top1 ->  65.93 ; top5 ->  96.71  and loss:  123.38244718313217
forward train acc: top1 ->  74.85800001708985 ; top5 ->  99.92  and loss:  88.26070791482925
test acc: top1 ->  67.82 ; top5 ->  96.8  and loss:  122.80662709474564
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  46 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -6153.835826873779 , diff:  6153.835826873779
adv train loss:  -12805.056655883789 , diff:  6651.22082901001
adv train loss:  -20172.555435180664 , diff:  7367.498779296875
adv train loss:  -26569.448959350586 , diff:  6396.893524169922
adv train loss:  -32406.158630371094 , diff:  5836.709671020508
adv train loss:  -37995.25161743164 , diff:  5589.092987060547
adv train loss:  -43447.70785522461 , diff:  5452.456237792969
adv train loss:  -48852.81915283203 , diff:  5405.111297607422
adv train loss:  -54181.46871948242 , diff:  5328.649566650391
adv train loss:  -58547.11346435547 , diff:  4365.644744873047
layer  13  adv train finish, try to retain  9
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.546875  ==>  140 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.072265625  ==>  37 / 512 , inc:  4
layer  9  :  0.015625  ==>  8 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.08984375  ==>  46 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [1.6626283650398253, 1.6626283650398253, 1.6626283650398253, 1.6626283650398253, 1.6626283650398253, 1.6626283650398253, 1.2469712737798688, 1.1084189100265502, 1.4778918800354004, 1.9705225067138672, 0.9352284553349016, 1.4778918800354004, 1.2469712737798688, 23.646270080566406]  wait [2, 4, 2, 2, 2, 2, 4, 1, 2, 0, 4, 0, 4, 1]  inc [1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1]  tol: 6
$$$$$$$$$$$$$ epoch  66  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1884.3593425750732 , diff:  1884.3593425750732
adv train loss:  -1892.8095817565918 , diff:  8.450239181518555
adv train loss:  -1898.8335819244385 , diff:  6.02400016784668
adv train loss:  -1890.941167831421 , diff:  7.892414093017578
adv train loss:  -1891.097900390625 , diff:  0.15673255920410156
layer  0  adv train finish, try to retain  3
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  5374.907493591309
forward train acc: top1 ->  15.920000001220703 ; top5 ->  63.73599997314453  and loss:  841.3632526397705
test acc: top1 ->  8.29 ; top5 ->  45.48  and loss:  560.7629561424255
forward train acc: top1 ->  20.156000004882813 ; top5 ->  70.44  and loss:  269.33835077285767
test acc: top1 ->  23.72 ; top5 ->  76.52  and loss:  215.14393746852875
forward train acc: top1 ->  23.161999995727538 ; top5 ->  76.14000000244141  and loss:  207.8328824043274
test acc: top1 ->  24.44 ; top5 ->  79.08  and loss:  202.17202472686768
forward train acc: top1 ->  24.82400000427246 ; top5 ->  78.31999997558594  and loss:  198.39720165729523
test acc: top1 ->  26.45 ; top5 ->  79.73  and loss:  197.6098029613495
forward train acc: top1 ->  26.711999996948244 ; top5 ->  80.44599999023437  and loss:  193.1524716615677
test acc: top1 ->  28.2 ; top5 ->  82.47  and loss:  192.18795037269592
forward train acc: top1 ->  27.953999995117186 ; top5 ->  81.52999998779296  and loss:  189.59146857261658
test acc: top1 ->  29.76 ; top5 ->  83.21  and loss:  189.41999447345734
forward train acc: top1 ->  29.014000000610352 ; top5 ->  82.52600001464843  and loss:  187.35393810272217
test acc: top1 ->  30.57 ; top5 ->  83.8  and loss:  187.3156394958496
forward train acc: top1 ->  29.905999998779297 ; top5 ->  83.31399998535156  and loss:  184.99445700645447
test acc: top1 ->  31.11 ; top5 ->  83.97  and loss:  186.16142892837524
forward train acc: top1 ->  30.49400000427246 ; top5 ->  83.42599997802735  and loss:  183.18568193912506
test acc: top1 ->  32.39 ; top5 ->  84.89  and loss:  183.20677268505096
forward train acc: top1 ->  31.83999999206543 ; top5 ->  84.37199998535156  and loss:  180.61596715450287
test acc: top1 ->  32.8 ; top5 ->  85.34  and loss:  181.8192788362503
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
### skip layer  1 wait:  4  ###
---------------- start layer  2  ---------------
adv train loss:  -116.00165629386902 , diff:  116.00165629386902
adv train loss:  -140.47208833694458 , diff:  24.47043204307556
adv train loss:  -166.05241107940674 , diff:  25.580322742462158
adv train loss:  -176.60013687610626 , diff:  10.547725796699524
adv train loss:  -182.32847487926483 , diff:  5.728338003158569
adv train loss:  -185.90852999687195 , diff:  3.5800551176071167
adv train loss:  -185.6664261817932 , diff:  0.24210381507873535
adv train loss:  -186.60505044460297 , diff:  0.9386242628097534
adv train loss:  -186.3335076570511 , diff:  0.2715427875518799
adv train loss:  -185.59169149398804 , diff:  0.7418161630630493
layer  2  adv train finish, try to retain  83
test acc: top1 ->  19.65 ; top5 ->  64.1  and loss:  232.67565321922302
forward train acc: top1 ->  96.19400000488281 ; top5 ->  99.934  and loss:  26.281533617526293
test acc: top1 ->  89.69 ; top5 ->  98.96  and loss:  41.55167190730572
forward train acc: top1 ->  98.96999997802735 ; top5 ->  99.986  and loss:  3.768898244947195
test acc: top1 ->  90.37 ; top5 ->  99.04  and loss:  44.24791894853115
forward train acc: top1 ->  99.34200000244141 ; top5 ->  99.992  and loss:  2.2802900578826666
test acc: top1 ->  90.63 ; top5 ->  99.14  and loss:  48.1738210991025
forward train acc: top1 ->  99.40599997558594 ; top5 ->  99.996  and loss:  1.8415045188739896
test acc: top1 ->  90.79 ; top5 ->  99.08  and loss:  49.533129654824734
forward train acc: top1 ->  99.534 ; top5 ->  99.998  and loss:  1.5043352516368032
test acc: top1 ->  90.93 ; top5 ->  99.05  and loss:  49.54273694381118
forward train acc: top1 ->  99.57 ; top5 ->  99.998  and loss:  1.3069362498354167
test acc: top1 ->  91.0 ; top5 ->  99.06  and loss:  48.88349762931466
forward train acc: top1 ->  99.606 ; top5 ->  100.0  and loss:  1.1916760257445276
test acc: top1 ->  90.91 ; top5 ->  99.08  and loss:  49.64715736359358
forward train acc: top1 ->  99.6080000024414 ; top5 ->  99.998  and loss:  1.1647840291261673
test acc: top1 ->  90.99 ; top5 ->  99.14  and loss:  50.01562685891986
forward train acc: top1 ->  99.61999997802734 ; top5 ->  100.0  and loss:  1.177100945264101
test acc: top1 ->  91.21 ; top5 ->  99.16  and loss:  50.197858706116676
forward train acc: top1 ->  99.65999997802734 ; top5 ->  99.992  and loss:  1.0654193498194218
test acc: top1 ->  91.08 ; top5 ->  99.06  and loss:  50.177859507501125
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -675.7798427436501 , diff:  675.7798427436501
adv train loss:  -1176.0676431655884 , diff:  500.2878004219383
adv train loss:  -1211.7709493637085 , diff:  35.70330619812012
adv train loss:  -1216.508282661438 , diff:  4.737333297729492
adv train loss:  -1224.5040531158447 , diff:  7.995770454406738
adv train loss:  -1229.5923805236816 , diff:  5.088327407836914
adv train loss:  -1229.6390943527222 , diff:  0.046713829040527344
adv train loss:  -1225.4683876037598 , diff:  4.170706748962402
adv train loss:  -1230.5391340255737 , diff:  5.070746421813965
adv train loss:  -1230.5562286376953 , diff:  0.01709461212158203
layer  3  adv train finish, try to retain  32
test acc: top1 ->  11.15 ; top5 ->  50.64  and loss:  892.4017057418823
forward train acc: top1 ->  86.79799997070313 ; top5 ->  99.09799997802735  and loss:  45.114541321992874
test acc: top1 ->  82.35 ; top5 ->  98.12  and loss:  60.28003433346748
forward train acc: top1 ->  88.92999997070312 ; top5 ->  99.42000000488281  and loss:  34.357776910066605
test acc: top1 ->  83.47 ; top5 ->  98.58  and loss:  55.8801007270813
forward train acc: top1 ->  89.7720000024414 ; top5 ->  99.47799997558593  and loss:  30.880931451916695
test acc: top1 ->  84.18 ; top5 ->  98.72  and loss:  53.52984392642975
forward train acc: top1 ->  90.77200001953125 ; top5 ->  99.5460000024414  and loss:  28.05086261034012
test acc: top1 ->  84.72 ; top5 ->  98.71  and loss:  52.375036388635635
forward train acc: top1 ->  91.25999998535156 ; top5 ->  99.6080000024414  and loss:  26.483368903398514
test acc: top1 ->  85.25 ; top5 ->  98.82  and loss:  51.23772384226322
forward train acc: top1 ->  91.94800001220703 ; top5 ->  99.63999997558594  and loss:  24.440624698996544
test acc: top1 ->  85.41 ; top5 ->  98.75  and loss:  50.3031355291605
forward train acc: top1 ->  91.63 ; top5 ->  99.68199997558594  and loss:  24.586335107684135
test acc: top1 ->  85.58 ; top5 ->  98.8  and loss:  49.614884451031685
forward train acc: top1 ->  92.18600000976562 ; top5 ->  99.7040000024414  and loss:  23.473523288965225
test acc: top1 ->  85.71 ; top5 ->  98.91  and loss:  49.248599484562874
forward train acc: top1 ->  92.51399997802734 ; top5 ->  99.72399997802735  and loss:  22.662744268774986
test acc: top1 ->  85.89 ; top5 ->  98.85  and loss:  48.81411252915859
forward train acc: top1 ->  92.54799998779296 ; top5 ->  99.7060000024414  and loss:  22.42015616595745
test acc: top1 ->  86.07 ; top5 ->  98.9  and loss:  48.42377592623234
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -89.29600196704268 , diff:  89.29600196704268
adv train loss:  -482.7851707935333 , diff:  393.48916882649064
adv train loss:  -613.8883728981018 , diff:  131.10320210456848
adv train loss:  -644.3808951377869 , diff:  30.49252223968506
adv train loss:  -669.8370428085327 , diff:  25.45614767074585
adv train loss:  -685.0023050308228 , diff:  15.165262222290039
adv train loss:  -726.0121111869812 , diff:  41.00980615615845
adv train loss:  -778.8614320755005 , diff:  52.84932088851929
adv train loss:  -826.5498433113098 , diff:  47.688411235809326
adv train loss:  -823.0686774253845 , diff:  3.481165885925293
layer  4  adv train finish, try to retain  32
test acc: top1 ->  10.98 ; top5 ->  54.33  and loss:  514.235209941864
forward train acc: top1 ->  78.14600000244141 ; top5 ->  97.86399997802734  and loss:  65.6882609128952
test acc: top1 ->  77.85 ; top5 ->  97.92  and loss:  68.88941451907158
forward train acc: top1 ->  82.04800001464844 ; top5 ->  98.68200000244141  and loss:  52.982067078351974
test acc: top1 ->  79.71 ; top5 ->  98.17  and loss:  62.706588953733444
forward train acc: top1 ->  83.59599997802735 ; top5 ->  98.91600000244141  and loss:  48.16911682486534
test acc: top1 ->  80.83 ; top5 ->  98.41  and loss:  59.58998250961304
forward train acc: top1 ->  84.90200001464844 ; top5 ->  99.02799997802734  and loss:  44.66545391082764
test acc: top1 ->  81.35 ; top5 ->  98.53  and loss:  57.75125776231289
forward train acc: top1 ->  85.71200000732422 ; top5 ->  99.13199997558594  and loss:  41.812033116817474
test acc: top1 ->  82.11 ; top5 ->  98.64  and loss:  55.995620891451836
forward train acc: top1 ->  86.27399997070313 ; top5 ->  99.2120000024414  and loss:  40.07988283038139
test acc: top1 ->  82.38 ; top5 ->  98.7  and loss:  55.279118835926056
forward train acc: top1 ->  86.66000000244141 ; top5 ->  99.1980000024414  and loss:  39.35987603664398
test acc: top1 ->  82.64 ; top5 ->  98.77  and loss:  54.78336650133133
forward train acc: top1 ->  86.65800000732422 ; top5 ->  99.28799997802734  and loss:  38.954298317432404
test acc: top1 ->  82.69 ; top5 ->  98.79  and loss:  54.006218284368515
forward train acc: top1 ->  86.9079999951172 ; top5 ->  99.3320000024414  and loss:  37.463045716285706
test acc: top1 ->  82.9 ; top5 ->  98.83  and loss:  53.76130864024162
forward train acc: top1 ->  87.28599999267578 ; top5 ->  99.31799997558593  and loss:  37.31525522470474
test acc: top1 ->  82.93 ; top5 ->  98.78  and loss:  53.37848496437073
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -249.71354312077165 , diff:  249.71354312077165
adv train loss:  -830.4407067298889 , diff:  580.7271636091173
adv train loss:  -938.4544200897217 , diff:  108.01371335983276
adv train loss:  -960.4167127609253 , diff:  21.962292671203613
adv train loss:  -978.5177268981934 , diff:  18.101014137268066
adv train loss:  -979.6155595779419 , diff:  1.0978326797485352
adv train loss:  -978.1259326934814 , diff:  1.4896268844604492
adv train loss:  -978.3378391265869 , diff:  0.21190643310546875
adv train loss:  -983.1684379577637 , diff:  4.830598831176758
adv train loss:  -981.3178825378418 , diff:  1.850555419921875
layer  5  adv train finish, try to retain  33
test acc: top1 ->  21.36 ; top5 ->  64.33  and loss:  374.4196763038635
forward train acc: top1 ->  83.98800000732422 ; top5 ->  99.38200000488281  and loss:  45.12976863980293
test acc: top1 ->  80.98 ; top5 ->  98.57  and loss:  59.57375746965408
forward train acc: top1 ->  87.85000001708984 ; top5 ->  99.564  and loss:  34.380886018276215
test acc: top1 ->  83.35 ; top5 ->  98.89  and loss:  54.07567837834358
forward train acc: top1 ->  89.46399998291015 ; top5 ->  99.66599997558593  and loss:  30.057630479335785
test acc: top1 ->  84.48 ; top5 ->  98.97  and loss:  51.10824239253998
forward train acc: top1 ->  90.49600000976562 ; top5 ->  99.68199997558594  and loss:  26.973548263311386
test acc: top1 ->  84.88 ; top5 ->  98.96  and loss:  50.16362003982067
forward train acc: top1 ->  91.34000000488281 ; top5 ->  99.744  and loss:  25.175130367279053
test acc: top1 ->  85.44 ; top5 ->  98.98  and loss:  48.83600993454456
forward train acc: top1 ->  91.54399997558593 ; top5 ->  99.75399997558594  and loss:  24.261458694934845
test acc: top1 ->  85.67 ; top5 ->  99.01  and loss:  47.99903193116188
forward train acc: top1 ->  91.74200001220703 ; top5 ->  99.76  and loss:  23.611382201313972
test acc: top1 ->  85.85 ; top5 ->  99.01  and loss:  47.5325782597065
forward train acc: top1 ->  92.14599998046874 ; top5 ->  99.774  and loss:  22.599084720015526
test acc: top1 ->  86.02 ; top5 ->  98.99  and loss:  47.02366599440575
forward train acc: top1 ->  92.11999997314453 ; top5 ->  99.778  and loss:  22.272385597229004
test acc: top1 ->  86.09 ; top5 ->  99.02  and loss:  46.92068912088871
forward train acc: top1 ->  92.52799998535156 ; top5 ->  99.8080000024414  and loss:  21.33424174785614
test acc: top1 ->  86.18 ; top5 ->  99.09  and loss:  46.923937276005745
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
adv train loss:  -4.3482370134443045 , diff:  4.3482370134443045
adv train loss:  -4.406599039211869 , diff:  0.058362025767564774
adv train loss:  -4.435668209567666 , diff:  0.029069170355796814
adv train loss:  -4.299525298178196 , diff:  0.1361429113894701
adv train loss:  -4.235819498077035 , diff:  0.063705800101161
adv train loss:  -4.276651253923774 , diff:  0.040831755846738815
adv train loss:  -4.4343622624874115 , diff:  0.15771100856363773
adv train loss:  -4.211058109998703 , diff:  0.2233041524887085
adv train loss:  -4.269915239885449 , diff:  0.05885712988674641
adv train loss:  -4.272737393155694 , diff:  0.0028221532702445984
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  252
test acc: top1 ->  10.3 ; top5 ->  66.33  and loss:  7879500.09375
forward train acc: top1 ->  99.04399997558593 ; top5 ->  99.99  and loss:  4.390390656888485
test acc: top1 ->  91.01 ; top5 ->  99.23  and loss:  38.603331826627254
forward train acc: top1 ->  99.638 ; top5 ->  100.0  and loss:  1.3410212118178606
test acc: top1 ->  91.26 ; top5 ->  99.26  and loss:  41.94911751151085
forward train acc: top1 ->  99.70599997558594 ; top5 ->  100.0  and loss:  0.9519611811265349
test acc: top1 ->  91.39 ; top5 ->  99.23  and loss:  44.41873870790005
forward train acc: top1 ->  99.7820000024414 ; top5 ->  99.998  and loss:  0.7802125988528132
test acc: top1 ->  91.4 ; top5 ->  99.29  and loss:  47.387717083096504
forward train acc: top1 ->  99.83599997558593 ; top5 ->  100.0  and loss:  0.5587492818012834
test acc: top1 ->  91.36 ; top5 ->  99.29  and loss:  49.40188132226467
forward train acc: top1 ->  99.874 ; top5 ->  100.0  and loss:  0.48177384643349797
test acc: top1 ->  91.39 ; top5 ->  99.28  and loss:  50.20033784210682
forward train acc: top1 ->  99.862 ; top5 ->  100.0  and loss:  0.47024799417704344
test acc: top1 ->  91.38 ; top5 ->  99.26  and loss:  50.92939945310354
forward train acc: top1 ->  99.87 ; top5 ->  100.0  and loss:  0.43011891725473106
test acc: top1 ->  91.37 ; top5 ->  99.35  and loss:  51.6276676133275
forward train acc: top1 ->  99.874 ; top5 ->  100.0  and loss:  0.4196217352291569
test acc: top1 ->  91.48 ; top5 ->  99.32  and loss:  52.292777962982655
forward train acc: top1 ->  99.87799997558594 ; top5 ->  100.0  and loss:  0.44387781899422407
test acc: top1 ->  91.53 ; top5 ->  99.32  and loss:  52.65782084316015
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  253 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -78.2613930106163 , diff:  78.2613930106163
adv train loss:  -77.65142869949341 , diff:  0.6099643111228943
adv train loss:  -175.16847431659698 , diff:  97.51704561710358
adv train loss:  -207.87877118587494 , diff:  32.710296869277954
adv train loss:  -207.89384388923645 , diff:  0.01507270336151123
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  33
test acc: top1 ->  38.03 ; top5 ->  88.89  and loss:  3632860.8828125
forward train acc: top1 ->  98.11199997802734 ; top5 ->  99.994  and loss:  6.2530339024960995
test acc: top1 ->  91.44 ; top5 ->  99.33  and loss:  44.161611936986446
forward train acc: top1 ->  99.662 ; top5 ->  99.996  and loss:  1.214910814538598
test acc: top1 ->  91.61 ; top5 ->  99.39  and loss:  45.093281373381615
forward train acc: top1 ->  99.778 ; top5 ->  99.998  and loss:  0.8413505046628416
test acc: top1 ->  91.73 ; top5 ->  99.4  and loss:  46.40517096966505
forward train acc: top1 ->  99.85199997558594 ; top5 ->  99.998  and loss:  0.5930638527497649
test acc: top1 ->  91.67 ; top5 ->  99.41  and loss:  48.16937295347452
forward train acc: top1 ->  99.82199997558594 ; top5 ->  100.0  and loss:  0.5258820997551084
test acc: top1 ->  91.77 ; top5 ->  99.39  and loss:  49.69542354345322
forward train acc: top1 ->  99.876 ; top5 ->  100.0  and loss:  0.4176356568932533
test acc: top1 ->  91.82 ; top5 ->  99.4  and loss:  49.94795919954777
forward train acc: top1 ->  99.898 ; top5 ->  100.0  and loss:  0.38802874967223033
test acc: top1 ->  91.87 ; top5 ->  99.39  and loss:  50.13326767086983
forward train acc: top1 ->  99.882 ; top5 ->  100.0  and loss:  0.37087384646292776
test acc: top1 ->  91.96 ; top5 ->  99.38  and loss:  50.72828893363476
forward train acc: top1 ->  99.918 ; top5 ->  100.0  and loss:  0.29907626239582896
test acc: top1 ->  91.93 ; top5 ->  99.42  and loss:  51.882551200687885
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.2685212797950953
test acc: top1 ->  91.95 ; top5 ->  99.43  and loss:  52.55705185979605
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  37 / 512 , inc:  4
---------------- start layer  9  ---------------
adv train loss:  -173.59021592140198 , diff:  173.59021592140198
adv train loss:  -173.10138511657715 , diff:  0.4888308048248291
adv train loss:  -172.76331305503845 , diff:  0.3380720615386963
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  7
test acc: top1 ->  25.68 ; top5 ->  74.04  and loss:  1316.0595321655273
forward train acc: top1 ->  93.56199997802734 ; top5 ->  99.994  and loss:  24.958811981603503
test acc: top1 ->  89.31 ; top5 ->  99.02  and loss:  59.77916271984577
forward train acc: top1 ->  99.44999997802735 ; top5 ->  100.0  and loss:  2.36385896243155
test acc: top1 ->  90.81 ; top5 ->  98.96  and loss:  53.075381338596344
forward train acc: top1 ->  99.65999997558593 ; top5 ->  100.0  and loss:  1.4058375330641866
test acc: top1 ->  91.11 ; top5 ->  99.01  and loss:  54.256529942154884
forward train acc: top1 ->  99.774 ; top5 ->  100.0  and loss:  1.0625665923580527
test acc: top1 ->  91.2 ; top5 ->  98.99  and loss:  55.26278170198202
forward train acc: top1 ->  99.766 ; top5 ->  100.0  and loss:  0.8974672611802816
test acc: top1 ->  91.34 ; top5 ->  99.1  and loss:  56.20426005870104
forward train acc: top1 ->  99.7600000024414 ; top5 ->  100.0  and loss:  0.7891676034778357
test acc: top1 ->  91.45 ; top5 ->  99.12  and loss:  56.96069683879614
forward train acc: top1 ->  99.83 ; top5 ->  100.0  and loss:  0.6623541819863021
test acc: top1 ->  91.5 ; top5 ->  99.08  and loss:  57.34736567735672
forward train acc: top1 ->  99.86799997558593 ; top5 ->  100.0  and loss:  0.5646142805926502
test acc: top1 ->  91.6 ; top5 ->  99.08  and loss:  57.79146395623684
forward train acc: top1 ->  99.822 ; top5 ->  99.998  and loss:  0.5737686008214951
test acc: top1 ->  91.63 ; top5 ->  99.08  and loss:  58.149415984749794
forward train acc: top1 ->  99.844 ; top5 ->  100.0  and loss:  0.521582888904959
test acc: top1 ->  91.64 ; top5 ->  99.08  and loss:  58.880066864192486
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  8 / 512 , inc:  1
---------------- start layer  10  ---------------
### skip layer  10 wait:  4  ###
---------------- start layer  11  ---------------
adv train loss:  -1300.6151008605957 , diff:  1300.6151008605957
adv train loss:  -1299.6848821640015 , diff:  0.9302186965942383
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  1
test acc: top1 ->  10.0 ; top5 ->  50.75  and loss:  39149.718170166016
forward train acc: top1 ->  87.97800000732421 ; top5 ->  97.472  and loss:  52.39493642002344
test acc: top1 ->  86.55 ; top5 ->  98.47  and loss:  58.66239085793495
forward train acc: top1 ->  99.08199997558594 ; top5 ->  99.994  and loss:  6.425141200423241
test acc: top1 ->  89.94 ; top5 ->  98.47  and loss:  48.54641871154308
forward train acc: top1 ->  99.3880000024414 ; top5 ->  99.994  and loss:  3.7287970054894686
test acc: top1 ->  90.4 ; top5 ->  98.5  and loss:  49.23219259083271
forward train acc: top1 ->  99.56400000488281 ; top5 ->  99.998  and loss:  2.5460725352168083
test acc: top1 ->  90.41 ; top5 ->  98.59  and loss:  51.08311519026756
forward train acc: top1 ->  99.632 ; top5 ->  99.992  and loss:  1.8864891016855836
test acc: top1 ->  90.57 ; top5 ->  98.62  and loss:  52.32092210650444
forward train acc: top1 ->  99.692 ; top5 ->  100.0  and loss:  1.554861924611032
test acc: top1 ->  90.69 ; top5 ->  98.61  and loss:  53.16179519891739
forward train acc: top1 ->  99.76199997802735 ; top5 ->  99.998  and loss:  1.3177697751671076
test acc: top1 ->  90.8 ; top5 ->  98.54  and loss:  53.742542907595634
forward train acc: top1 ->  99.734 ; top5 ->  100.0  and loss:  1.2774262493476272
test acc: top1 ->  90.86 ; top5 ->  98.57  and loss:  54.53303921222687
forward train acc: top1 ->  99.78799997558593 ; top5 ->  100.0  and loss:  1.1608177702873945
test acc: top1 ->  90.87 ; top5 ->  98.59  and loss:  55.060036808252335
forward train acc: top1 ->  99.74199997558594 ; top5 ->  100.0  and loss:  1.1612514723092318
test acc: top1 ->  90.79 ; top5 ->  98.56  and loss:  55.77201655507088
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  2 / 512 , inc:  1
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
adv train loss:  -5693.895805358887 , diff:  5693.895805358887
adv train loss:  -8680.733177185059 , diff:  2986.837371826172
adv train loss:  -11453.886070251465 , diff:  2773.1528930664062
adv train loss:  -14159.356643676758 , diff:  2705.470573425293
adv train loss:  -16856.63818359375 , diff:  2697.281539916992
adv train loss:  -19555.69660949707 , diff:  2699.0584259033203
adv train loss:  -22464.626235961914 , diff:  2908.9296264648438
adv train loss:  -26445.471588134766 , diff:  3980.8453521728516
adv train loss:  -31662.2763671875 , diff:  5216.804779052734
adv train loss:  -36983.816497802734 , diff:  5321.540130615234
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  8
test acc: top1 ->  29.55 ; top5 ->  74.12  and loss:  1649.1215467453003
forward train acc: top1 ->  57.394000017089844 ; top5 ->  92.886  and loss:  426.7173540741205
test acc: top1 ->  86.63 ; top5 ->  98.56  and loss:  72.38619628548622
forward train acc: top1 ->  98.37999997802734 ; top5 ->  99.996  and loss:  11.50409572571516
test acc: top1 ->  88.91 ; top5 ->  98.74  and loss:  56.93836152553558
forward train acc: top1 ->  99.06600000488281 ; top5 ->  100.0  and loss:  7.579921685159206
test acc: top1 ->  89.36 ; top5 ->  98.79  and loss:  56.07147416472435
forward train acc: top1 ->  99.23400000488282 ; top5 ->  100.0  and loss:  6.086751371622086
test acc: top1 ->  89.34 ; top5 ->  98.77  and loss:  56.56527651846409
forward train acc: top1 ->  99.34599997802735 ; top5 ->  100.0  and loss:  5.063987888395786
test acc: top1 ->  89.57 ; top5 ->  98.76  and loss:  57.37498985230923
forward train acc: top1 ->  99.46199997558594 ; top5 ->  100.0  and loss:  4.4439450316131115
test acc: top1 ->  89.69 ; top5 ->  98.76  and loss:  57.60075601935387
forward train acc: top1 ->  99.39599997558594 ; top5 ->  99.998  and loss:  4.286157421767712
test acc: top1 ->  89.74 ; top5 ->  98.79  and loss:  58.111462444067
forward train acc: top1 ->  99.50600000244141 ; top5 ->  100.0  and loss:  3.886116759851575
test acc: top1 ->  89.86 ; top5 ->  98.74  and loss:  58.33760766685009
forward train acc: top1 ->  99.52 ; top5 ->  100.0  and loss:  3.6246336922049522
test acc: top1 ->  89.8 ; top5 ->  98.75  and loss:  59.23181043565273
forward train acc: top1 ->  99.528 ; top5 ->  100.0  and loss:  3.466879528015852
test acc: top1 ->  89.91 ; top5 ->  98.78  and loss:  59.62277714908123
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  9 / 512 , inc:  1
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.546875  ==>  140 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.072265625  ==>  37 / 512 , inc:  2
layer  9  :  0.015625  ==>  8 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.08984375  ==>  46 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [1.2469712737798688, 1.6626283650398253, 1.2469712737798688, 1.2469712737798688, 1.2469712737798688, 1.2469712737798688, 1.2469712737798688, 0.8313141825199126, 1.1084189100265502, 1.4778918800354004, 0.9352284553349016, 1.1084189100265502, 1.2469712737798688, 17.734702560424804]  wait [4, 3, 4, 4, 4, 4, 3, 3, 4, 2, 3, 2, 3, 3]  inc [1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1]  tol: 6
$$$$$$$$$$$$$ epoch  67  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1265.771842956543 , diff:  1265.771842956543
adv train loss:  -1348.47669506073 , diff:  82.70485210418701
adv train loss:  -1351.717064857483 , diff:  3.2403697967529297
adv train loss:  -1362.7579774856567 , diff:  11.040912628173828
adv train loss:  -1363.897539138794 , diff:  1.139561653137207
adv train loss:  -1373.2741565704346 , diff:  9.376617431640625
adv train loss:  -1375.940613746643 , diff:  2.666457176208496
adv train loss:  -1368.5945253372192 , diff:  7.346088409423828
adv train loss:  -1372.1000413894653 , diff:  3.5055160522460938
adv train loss:  -1360.1186475753784 , diff:  11.981393814086914
layer  0  adv train finish, try to retain  16
test acc: top1 ->  10.14 ; top5 ->  54.18  and loss:  2320.6489753723145
forward train acc: top1 ->  92.16400000244141 ; top5 ->  99.2  and loss:  52.420635893940926
test acc: top1 ->  84.89 ; top5 ->  97.98  and loss:  99.05320093035698
forward train acc: top1 ->  94.108 ; top5 ->  99.5400000024414  and loss:  25.401185497641563
test acc: top1 ->  87.59 ; top5 ->  98.5  and loss:  69.8261039853096
forward train acc: top1 ->  95.22999999511718 ; top5 ->  99.66799997802734  and loss:  18.28215315192938
test acc: top1 ->  87.85 ; top5 ->  98.62  and loss:  61.15906843543053
forward train acc: top1 ->  95.58400001708985 ; top5 ->  99.78199997558593  and loss:  15.52245969325304
test acc: top1 ->  88.18 ; top5 ->  98.82  and loss:  57.011112466454506
forward train acc: top1 ->  96.21600001464844 ; top5 ->  99.824  and loss:  12.58854753896594
test acc: top1 ->  88.47 ; top5 ->  98.89  and loss:  53.119926795363426
forward train acc: top1 ->  96.58800001464844 ; top5 ->  99.824  and loss:  11.315473277121782
test acc: top1 ->  88.46 ; top5 ->  98.83  and loss:  51.76898029446602
forward train acc: top1 ->  96.84000000488281 ; top5 ->  99.884  and loss:  10.241782516241074
test acc: top1 ->  88.69 ; top5 ->  98.92  and loss:  51.15269659459591
forward train acc: top1 ->  96.73599999267579 ; top5 ->  99.88199997558594  and loss:  10.266256660223007
test acc: top1 ->  88.89 ; top5 ->  98.92  and loss:  49.71524094045162
forward train acc: top1 ->  97.02800001220703 ; top5 ->  99.904  and loss:  9.367445562034845
test acc: top1 ->  88.92 ; top5 ->  98.95  and loss:  49.39324001967907
forward train acc: top1 ->  97.21399998291015 ; top5 ->  99.904  and loss:  8.998401712626219
test acc: top1 ->  89.02 ; top5 ->  99.0  and loss:  48.76457181572914
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -1.017681062221527 , diff:  1.017681062221527
adv train loss:  -1.0153047917410731 , diff:  0.002376270480453968
layer  1  adv train finish, try to retain  44
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -29.366633074358106 , diff:  29.366633074358106
adv train loss:  -133.02962511777878 , diff:  103.66299204342067
adv train loss:  -222.97661328315735 , diff:  89.94698816537857
adv train loss:  -247.43647837638855 , diff:  24.4598650932312
adv train loss:  -262.3977954387665 , diff:  14.96131706237793
adv train loss:  -302.91169786453247 , diff:  40.51390242576599
adv train loss:  -301.02141857147217 , diff:  1.8902792930603027
adv train loss:  -300.7866632938385 , diff:  0.234755277633667
adv train loss:  -300.2371299266815 , diff:  0.5495333671569824
adv train loss:  -300.89776372909546 , diff:  0.6606338024139404
layer  2  adv train finish, try to retain  92
test acc: top1 ->  24.16 ; top5 ->  60.89  and loss:  2287.1217155456543
forward train acc: top1 ->  99.64599997558594 ; top5 ->  99.996  and loss:  1.1559715177863836
test acc: top1 ->  91.4 ; top5 ->  99.32  and loss:  47.826837070286274
forward train acc: top1 ->  99.802 ; top5 ->  100.0  and loss:  0.5887574502266943
test acc: top1 ->  91.61 ; top5 ->  99.36  and loss:  51.09905755519867
forward train acc: top1 ->  99.8280000024414 ; top5 ->  99.998  and loss:  0.48936339281499386
test acc: top1 ->  91.37 ; top5 ->  99.31  and loss:  55.503852516412735
forward train acc: top1 ->  99.864 ; top5 ->  100.0  and loss:  0.37656009546481073
test acc: top1 ->  91.61 ; top5 ->  99.33  and loss:  55.18391742557287
forward train acc: top1 ->  99.898 ; top5 ->  100.0  and loss:  0.3159330036723986
test acc: top1 ->  91.55 ; top5 ->  99.25  and loss:  58.166583590209484
forward train acc: top1 ->  99.882 ; top5 ->  100.0  and loss:  0.3078377760393778
test acc: top1 ->  91.69 ; top5 ->  99.38  and loss:  57.72426858916879
forward train acc: top1 ->  99.91599997558593 ; top5 ->  100.0  and loss:  0.2531277472153306
test acc: top1 ->  91.71 ; top5 ->  99.31  and loss:  58.25292036682367
forward train acc: top1 ->  99.916 ; top5 ->  100.0  and loss:  0.2596399833346368
test acc: top1 ->  91.54 ; top5 ->  99.29  and loss:  58.70381461456418
forward train acc: top1 ->  99.906 ; top5 ->  100.0  and loss:  0.25484588800463825
test acc: top1 ->  91.77 ; top5 ->  99.32  and loss:  59.24415597319603
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.20560078241396695
test acc: top1 ->  91.72 ; top5 ->  99.16  and loss:  62.311693869531155
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -900.768148615025 , diff:  900.768148615025
adv train loss:  -1520.1522636413574 , diff:  619.3841150263324
adv train loss:  -1556.0292110443115 , diff:  35.8769474029541
adv train loss:  -1553.6171913146973 , diff:  2.412019729614258
adv train loss:  -1577.3243989944458 , diff:  23.707207679748535
adv train loss:  -1579.0250253677368 , diff:  1.7006263732910156
adv train loss:  -1580.919997215271 , diff:  1.8949718475341797
adv train loss:  -1571.7511835098267 , diff:  9.168813705444336
adv train loss:  -1570.9041442871094 , diff:  0.8470392227172852
adv train loss:  -1583.5580568313599 , diff:  12.653912544250488
layer  3  adv train finish, try to retain  74
test acc: top1 ->  21.23 ; top5 ->  66.25  and loss:  2195.490562438965
forward train acc: top1 ->  99.53999997558594 ; top5 ->  99.998  and loss:  1.4786380026489496
test acc: top1 ->  91.55 ; top5 ->  99.26  and loss:  57.76433172542602
forward train acc: top1 ->  99.6620000024414 ; top5 ->  100.0  and loss:  0.9719185223802924
test acc: top1 ->  91.52 ; top5 ->  99.34  and loss:  57.457303907722235
forward train acc: top1 ->  99.7160000024414 ; top5 ->  100.0  and loss:  0.9391425959765911
test acc: top1 ->  91.45 ; top5 ->  99.27  and loss:  56.94162302603945
forward train acc: top1 ->  99.67000000244141 ; top5 ->  100.0  and loss:  0.9095644699409604
test acc: top1 ->  91.58 ; top5 ->  99.22  and loss:  55.81169910682365
forward train acc: top1 ->  99.70799997558593 ; top5 ->  100.0  and loss:  0.8939182702451944
test acc: top1 ->  91.73 ; top5 ->  99.4  and loss:  55.26535260118544
forward train acc: top1 ->  99.744 ; top5 ->  100.0  and loss:  0.7137271030806005
test acc: top1 ->  91.73 ; top5 ->  99.3  and loss:  54.72961316257715
forward train acc: top1 ->  99.75 ; top5 ->  100.0  and loss:  0.6843776196474209
test acc: top1 ->  91.72 ; top5 ->  99.4  and loss:  54.257986126467586
forward train acc: top1 ->  99.766 ; top5 ->  100.0  and loss:  0.695169017650187
test acc: top1 ->  91.77 ; top5 ->  99.3  and loss:  54.64546250086278
forward train acc: top1 ->  99.792 ; top5 ->  100.0  and loss:  0.6715664360672235
test acc: top1 ->  91.56 ; top5 ->  99.36  and loss:  55.072263810783625
forward train acc: top1 ->  99.77199997558594 ; top5 ->  100.0  and loss:  0.6590782953426242
test acc: top1 ->  91.78 ; top5 ->  99.39  and loss:  54.74151550792158
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -365.6072581810877 , diff:  365.6072581810877
adv train loss:  -1540.1820459365845 , diff:  1174.5747877554968
adv train loss:  -1683.6308727264404 , diff:  143.44882678985596
adv train loss:  -1773.1808643341064 , diff:  89.54999160766602
adv train loss:  -1806.22731590271 , diff:  33.046451568603516
adv train loss:  -1825.5101985931396 , diff:  19.282882690429688
adv train loss:  -1831.918701171875 , diff:  6.408502578735352
adv train loss:  -1853.2805404663086 , diff:  21.361839294433594
adv train loss:  -1856.759620666504 , diff:  3.4790802001953125
adv train loss:  -1857.6853313446045 , diff:  0.9257106781005859
layer  4  adv train finish, try to retain  114
test acc: top1 ->  30.27 ; top5 ->  81.81  and loss:  1970.169114112854
forward train acc: top1 ->  98.45399998046875 ; top5 ->  99.978  and loss:  5.232407975941896
test acc: top1 ->  89.88 ; top5 ->  99.26  and loss:  52.540954396128654
forward train acc: top1 ->  98.79000000488281 ; top5 ->  99.972  and loss:  3.7215591315180063
test acc: top1 ->  90.13 ; top5 ->  99.32  and loss:  50.7924295142293
forward train acc: top1 ->  98.96800000732422 ; top5 ->  99.99  and loss:  3.039694707840681
test acc: top1 ->  90.45 ; top5 ->  99.18  and loss:  50.58059151470661
forward train acc: top1 ->  99.03600000244141 ; top5 ->  99.988  and loss:  2.8678960166871548
test acc: top1 ->  90.6 ; top5 ->  99.29  and loss:  48.63375498354435
forward train acc: top1 ->  99.16400000488281 ; top5 ->  99.996  and loss:  2.412659466266632
test acc: top1 ->  90.83 ; top5 ->  99.18  and loss:  50.478946685791016
forward train acc: top1 ->  99.20600000488281 ; top5 ->  99.994  and loss:  2.3686397429555655
test acc: top1 ->  90.7 ; top5 ->  99.28  and loss:  49.76114812493324
forward train acc: top1 ->  99.31799997558593 ; top5 ->  100.0  and loss:  1.9772927034646273
test acc: top1 ->  90.85 ; top5 ->  99.23  and loss:  50.1818465963006
forward train acc: top1 ->  99.29599998046875 ; top5 ->  99.992  and loss:  2.0332716405391693
test acc: top1 ->  90.83 ; top5 ->  99.31  and loss:  50.096370071172714
forward train acc: top1 ->  99.25999997558594 ; top5 ->  99.994  and loss:  2.183489803224802
test acc: top1 ->  90.85 ; top5 ->  99.35  and loss:  49.94306352734566
forward train acc: top1 ->  99.32799997558594 ; top5 ->  100.0  and loss:  1.905892290174961
test acc: top1 ->  90.93 ; top5 ->  99.25  and loss:  49.76178678125143
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -546.9384533539414 , diff:  546.9384533539414
adv train loss:  -1875.1970882415771 , diff:  1328.2586348876357
adv train loss:  -1908.4993152618408 , diff:  33.30222702026367
adv train loss:  -1918.4162902832031 , diff:  9.916975021362305
adv train loss:  -1961.0271606445312 , diff:  42.610870361328125
adv train loss:  -1965.89381980896 , diff:  4.866659164428711
adv train loss:  -1968.053617477417 , diff:  2.1597976684570312
adv train loss:  -1976.5183219909668 , diff:  8.464704513549805
adv train loss:  -1975.5277404785156 , diff:  0.9905815124511719
adv train loss:  -1979.7633380889893 , diff:  4.235597610473633
layer  5  adv train finish, try to retain  118
test acc: top1 ->  38.3 ; top5 ->  91.66  and loss:  3211.465705871582
forward train acc: top1 ->  99.30999997558594 ; top5 ->  100.0  and loss:  2.2096107793040574
test acc: top1 ->  91.16 ; top5 ->  99.34  and loss:  47.35080010816455
forward train acc: top1 ->  99.5340000024414 ; top5 ->  99.996  and loss:  1.3890967154875398
test acc: top1 ->  91.47 ; top5 ->  99.36  and loss:  48.08686474338174
forward train acc: top1 ->  99.6540000024414 ; top5 ->  99.998  and loss:  0.9788346523419023
test acc: top1 ->  91.61 ; top5 ->  99.39  and loss:  49.654275715351105
forward train acc: top1 ->  99.706 ; top5 ->  100.0  and loss:  0.9048308704514056
test acc: top1 ->  91.7 ; top5 ->  99.38  and loss:  50.75178807601333
forward train acc: top1 ->  99.75800000488282 ; top5 ->  100.0  and loss:  0.6983640482649207
test acc: top1 ->  91.78 ; top5 ->  99.42  and loss:  53.5618184171617
forward train acc: top1 ->  99.724 ; top5 ->  99.996  and loss:  0.814170980360359
test acc: top1 ->  91.75 ; top5 ->  99.24  and loss:  53.55775337293744
forward train acc: top1 ->  99.78 ; top5 ->  100.0  and loss:  0.6315771020017564
test acc: top1 ->  91.75 ; top5 ->  99.36  and loss:  53.168283032253385
forward train acc: top1 ->  99.778 ; top5 ->  100.0  and loss:  0.6436523960437626
test acc: top1 ->  91.8 ; top5 ->  99.31  and loss:  53.72860593535006
forward train acc: top1 ->  99.76599997558594 ; top5 ->  100.0  and loss:  0.6192201506346464
test acc: top1 ->  91.85 ; top5 ->  99.41  and loss:  54.15160441864282
forward train acc: top1 ->  99.76999997802734 ; top5 ->  100.0  and loss:  0.5963499946519732
test acc: top1 ->  91.81 ; top5 ->  99.38  and loss:  54.89578156545758
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -749.3577559376135 , diff:  749.3577559376135
adv train loss:  -2275.155158996582 , diff:  1525.7974030589685
adv train loss:  -2279.4574756622314 , diff:  4.302316665649414
adv train loss:  -2289.6565837860107 , diff:  10.199108123779297
adv train loss:  -2295.039224624634 , diff:  5.382640838623047
adv train loss:  -2285.0432052612305 , diff:  9.99601936340332
adv train loss:  -2285.466600418091 , diff:  0.42339515686035156
adv train loss:  -2286.7850437164307 , diff:  1.3184432983398438
adv train loss:  -2284.306665420532 , diff:  2.4783782958984375
adv train loss:  -2284.751148223877 , diff:  0.44448280334472656
layer  6  adv train finish, try to retain  101
test acc: top1 ->  65.5 ; top5 ->  97.65  and loss:  1101.7426776885986
forward train acc: top1 ->  99.88599997558593 ; top5 ->  100.0  and loss:  0.32840048684738576
test acc: top1 ->  91.87 ; top5 ->  99.38  and loss:  54.345550894737244
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.18678823672235012
test acc: top1 ->  92.14 ; top5 ->  99.27  and loss:  54.959814727306366
==> this epoch:  101 / 256
---------------- start layer  7  ---------------
adv train loss:  -0.21094813104718924 , diff:  0.21094813104718924
adv train loss:  -0.2013501824294508 , diff:  0.009597948617738439
layer  7  adv train finish, try to retain  259
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -30.293354958295822 , diff:  30.293354958295822
adv train loss:  -30.217934295535088 , diff:  0.07542066276073456
adv train loss:  -30.686578005552292 , diff:  0.4686437100172043
adv train loss:  -30.77772317826748 , diff:  0.09114517271518707
adv train loss:  -46.17074891924858 , diff:  15.393025740981102
adv train loss:  -106.093286216259 , diff:  59.92253729701042
adv train loss:  -105.2630569934845 , diff:  0.8302292227745056
adv train loss:  -105.75807082653046 , diff:  0.4950138330459595
adv train loss:  -189.83283650875092 , diff:  84.07476568222046
adv train loss:  -395.7313222885132 , diff:  205.89848577976227
layer  8  adv train finish, try to retain  451
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -2138.8161430358887 , diff:  2138.8161430358887
adv train loss:  -2170.9501991271973 , diff:  32.134056091308594
adv train loss:  -2218.21950340271 , diff:  47.269304275512695
adv train loss:  -2216.407371520996 , diff:  1.8121318817138672
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  7
test acc: top1 ->  5.2 ; top5 ->  42.78  and loss:  89652.53662109375
forward train acc: top1 ->  98.048 ; top5 ->  99.998  and loss:  6.2983034159988165
test acc: top1 ->  91.12 ; top5 ->  98.94  and loss:  52.55571721494198
forward train acc: top1 ->  99.82 ; top5 ->  100.0  and loss:  0.6384741018991917
test acc: top1 ->  91.82 ; top5 ->  99.05  and loss:  49.4224528670311
forward train acc: top1 ->  99.87 ; top5 ->  100.0  and loss:  0.4622780226636678
test acc: top1 ->  91.85 ; top5 ->  99.09  and loss:  50.41931153833866
forward train acc: top1 ->  99.918 ; top5 ->  100.0  and loss:  0.3428023887099698
test acc: top1 ->  91.89 ; top5 ->  99.08  and loss:  51.450090765953064
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.2833951646462083
test acc: top1 ->  91.94 ; top5 ->  99.08  and loss:  52.61468341201544
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.2649441927205771
test acc: top1 ->  91.97 ; top5 ->  99.11  and loss:  52.947732873260975
forward train acc: top1 ->  99.93599997558594 ; top5 ->  100.0  and loss:  0.2491040201857686
test acc: top1 ->  91.94 ; top5 ->  99.08  and loss:  53.77042570710182
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.19714465423021466
test acc: top1 ->  92.01 ; top5 ->  99.14  and loss:  54.1370454877615
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.20820972800720483
test acc: top1 ->  92.02 ; top5 ->  99.12  and loss:  54.96245560795069
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.1768691553734243
test acc: top1 ->  92.0 ; top5 ->  99.19  and loss:  54.998339146375656
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  8 / 512 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -132.23629426956177 , diff:  132.23629426956177
adv train loss:  -132.55348443984985 , diff:  0.31719017028808594
adv train loss:  -132.29126954078674 , diff:  0.26221489906311035
adv train loss:  -131.53331983089447 , diff:  0.757949709892273
adv train loss:  -132.31620967388153 , diff:  0.7828898429870605
adv train loss:  -131.9837132692337 , diff:  0.33249640464782715
adv train loss:  -131.43969905376434 , diff:  0.5440142154693604
adv train loss:  -132.09603452682495 , diff:  0.6563354730606079
adv train loss:  -131.59404730796814 , diff:  0.5019872188568115
adv train loss:  -132.18562936782837 , diff:  0.5915820598602295
layer  10  adv train finish, try to retain  483
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -2088.4080142974854 , diff:  2088.4080142974854
adv train loss:  -2145.4330978393555 , diff:  57.02508354187012
adv train loss:  -2175.9697456359863 , diff:  30.53664779663086
adv train loss:  -2174.7134742736816 , diff:  1.2562713623046875
layer  11  adv train finish, try to retain  494
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -984.330867767334 , diff:  984.330867767334
adv train loss:  -1002.5949192047119 , diff:  18.26405143737793
adv train loss:  -1002.1562089920044 , diff:  0.43871021270751953
adv train loss:  -1000.5420379638672 , diff:  1.614171028137207
adv train loss:  -1001.242959022522 , diff:  0.7009210586547852
adv train loss:  -1002.4764423370361 , diff:  1.2334833145141602
adv train loss:  -1002.3742733001709 , diff:  0.10216903686523438
layer  12  adv train finish, try to retain  2
test acc: top1 ->  9.97 ; top5 ->  50.0  and loss:  7367.888282775879
forward train acc: top1 ->  52.682000012207034 ; top5 ->  82.17600001708985  and loss:  466.7238459587097
test acc: top1 ->  62.98 ; top5 ->  89.11  and loss:  140.02244448661804
forward train acc: top1 ->  87.33600000976563 ; top5 ->  96.378  and loss:  56.588282853364944
test acc: top1 ->  83.85 ; top5 ->  97.96  and loss:  67.40219563245773
forward train acc: top1 ->  95.54799998291016 ; top5 ->  99.984  and loss:  30.58123242855072
test acc: top1 ->  87.03 ; top5 ->  98.04  and loss:  63.57176786661148
forward train acc: top1 ->  97.88599998291015 ; top5 ->  99.992  and loss:  23.453186556696892
test acc: top1 ->  88.04 ; top5 ->  97.91  and loss:  62.572662591934204
forward train acc: top1 ->  98.67400000488281 ; top5 ->  99.996  and loss:  18.55688451230526
test acc: top1 ->  88.47 ; top5 ->  97.89  and loss:  62.20753014087677
forward train acc: top1 ->  98.91600000488282 ; top5 ->  99.994  and loss:  15.539519667625427
test acc: top1 ->  88.46 ; top5 ->  97.91  and loss:  62.76716935634613
forward train acc: top1 ->  98.95799997802735 ; top5 ->  99.996  and loss:  13.712637595832348
test acc: top1 ->  88.73 ; top5 ->  97.92  and loss:  62.7414530813694
forward train acc: top1 ->  99.1160000048828 ; top5 ->  99.998  and loss:  11.982946828007698
test acc: top1 ->  88.78 ; top5 ->  97.89  and loss:  63.008191376924515
forward train acc: top1 ->  99.19599997558593 ; top5 ->  99.994  and loss:  10.56723491102457
test acc: top1 ->  88.79 ; top5 ->  97.81  and loss:  64.42237058281898
forward train acc: top1 ->  99.27600000732421 ; top5 ->  99.996  and loss:  9.283338472247124
test acc: top1 ->  88.97 ; top5 ->  97.89  and loss:  64.5091278553009
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  46 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -9890.13321685791 , diff:  9890.13321685791
adv train loss:  -15251.220581054688 , diff:  5361.087364196777
adv train loss:  -20652.087646484375 , diff:  5400.8670654296875
adv train loss:  -26251.127700805664 , diff:  5599.040054321289
adv train loss:  -32051.9169921875 , diff:  5800.789291381836
adv train loss:  -38444.531158447266 , diff:  6392.614166259766
adv train loss:  -45199.596588134766 , diff:  6755.0654296875
adv train loss:  -51483.41455078125 , diff:  6283.817962646484
adv train loss:  -57549.47772216797 , diff:  6066.063171386719
adv train loss:  -63483.444274902344 , diff:  5933.966552734375
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  8
test acc: top1 ->  25.37 ; top5 ->  59.04  and loss:  2467.2014026641846
forward train acc: top1 ->  36.929999998779294 ; top5 ->  72.58000000732422  and loss:  1071.0687119960785
test acc: top1 ->  40.95 ; top5 ->  75.89  and loss:  441.05974769592285
forward train acc: top1 ->  73.49399998046874 ; top5 ->  90.732  and loss:  138.86129000782967
test acc: top1 ->  88.84 ; top5 ->  98.58  and loss:  60.14314991235733
forward train acc: top1 ->  99.12799997802735 ; top5 ->  99.998  and loss:  16.786289855837822
test acc: top1 ->  89.72 ; top5 ->  98.52  and loss:  50.09750556945801
forward train acc: top1 ->  99.61000000244141 ; top5 ->  100.0  and loss:  8.71970997005701
test acc: top1 ->  90.23 ; top5 ->  98.5  and loss:  47.83222949504852
forward train acc: top1 ->  99.666 ; top5 ->  100.0  and loss:  5.494073569774628
test acc: top1 ->  90.64 ; top5 ->  98.42  and loss:  47.41183499991894
forward train acc: top1 ->  99.704 ; top5 ->  100.0  and loss:  4.261014107614756
test acc: top1 ->  90.64 ; top5 ->  98.47  and loss:  47.36524870991707
forward train acc: top1 ->  99.7520000024414 ; top5 ->  100.0  and loss:  3.606893751770258
test acc: top1 ->  90.57 ; top5 ->  98.42  and loss:  47.69772543013096
forward train acc: top1 ->  99.798 ; top5 ->  100.0  and loss:  3.099771611392498
test acc: top1 ->  90.64 ; top5 ->  98.36  and loss:  48.09443287551403
forward train acc: top1 ->  99.83199997558594 ; top5 ->  100.0  and loss:  2.6906460020691156
test acc: top1 ->  90.82 ; top5 ->  98.38  and loss:  48.16872368007898
forward train acc: top1 ->  99.764 ; top5 ->  100.0  and loss:  2.487956862896681
test acc: top1 ->  90.81 ; top5 ->  98.37  and loss:  49.03882744163275
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  9 / 512 , inc:  1
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.39453125  ==>  101 / 256 , inc:  2
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.072265625  ==>  37 / 512 , inc:  2
layer  9  :  0.015625  ==>  8 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.08984375  ==>  46 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [0.9352284553349016, 3.3252567300796505, 0.9352284553349016, 0.9352284553349016, 0.9352284553349016, 0.9352284553349016, 1.2469712737798688, 1.6626283650398253, 2.2168378200531005, 1.1084189100265502, 1.8704569106698032, 2.2168378200531005, 0.9352284553349016, 13.301026920318602]  wait [4, 1, 4, 4, 4, 4, 0, 1, 2, 2, 1, 0, 3, 3]  inc [1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1]  tol: 7
$$$$$$$$$$$$$ epoch  68  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
adv train loss:  -961.9282999038696 , diff:  961.9282999038696
adv train loss:  -961.1142120361328 , diff:  0.8140878677368164
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  43
test acc: top1 ->  11.98 ; top5 ->  53.45  and loss:  2866.856943130493
forward train acc: top1 ->  90.222 ; top5 ->  99.992  and loss:  75.59174349019304
test acc: top1 ->  91.46 ; top5 ->  98.78  and loss:  78.3724071085453
forward train acc: top1 ->  99.79199997558594 ; top5 ->  100.0  and loss:  1.0114357420243323
test acc: top1 ->  91.72 ; top5 ->  98.89  and loss:  71.77538464963436
forward train acc: top1 ->  99.87 ; top5 ->  100.0  and loss:  0.6303049339912832
test acc: top1 ->  91.98 ; top5 ->  98.76  and loss:  71.88267004489899
forward train acc: top1 ->  99.904 ; top5 ->  100.0  and loss:  0.4947304886300117
test acc: top1 ->  91.88 ; top5 ->  98.79  and loss:  72.97977866232395
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.3201299876673147
test acc: top1 ->  92.04 ; top5 ->  98.82  and loss:  71.43517792224884
forward train acc: top1 ->  99.92199997558593 ; top5 ->  100.0  and loss:  0.2984193424344994
test acc: top1 ->  91.94 ; top5 ->  98.85  and loss:  70.19550895690918
forward train acc: top1 ->  99.91599997558593 ; top5 ->  100.0  and loss:  0.3170861150720157
test acc: top1 ->  91.94 ; top5 ->  98.84  and loss:  72.42727392911911
forward train acc: top1 ->  99.912 ; top5 ->  100.0  and loss:  0.36889518331736326
test acc: top1 ->  91.85 ; top5 ->  98.86  and loss:  72.41377870738506
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.22601459897123277
test acc: top1 ->  91.79 ; top5 ->  98.89  and loss:  72.09877122938633
forward train acc: top1 ->  99.92000000244141 ; top5 ->  100.0  and loss:  0.3169134119525552
test acc: top1 ->  91.95 ; top5 ->  98.75  and loss:  73.01135036349297
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  44 / 64 , inc:  1
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
adv train loss:  -864.0281668897369 , diff:  864.0281668897369
adv train loss:  -2943.1101779937744 , diff:  2079.0820111040375
adv train loss:  -3072.7196311950684 , diff:  129.60945320129395
adv train loss:  -3186.394760131836 , diff:  113.67512893676758
adv train loss:  -3187.8213081359863 , diff:  1.4265480041503906
adv train loss:  -3196.083354949951 , diff:  8.262046813964844
adv train loss:  -3191.5844764709473 , diff:  4.498878479003906
adv train loss:  -3190.2000408172607 , diff:  1.3844356536865234
adv train loss:  -3183.305841445923 , diff:  6.894199371337891
adv train loss:  -3187.7623462677 , diff:  4.456504821777344
layer  6  adv train finish, try to retain  85
test acc: top1 ->  44.66 ; top5 ->  92.7  and loss:  6542.144157409668
forward train acc: top1 ->  99.804 ; top5 ->  100.0  and loss:  0.6166446943534538
test acc: top1 ->  91.64 ; top5 ->  98.86  and loss:  74.51506336033344
forward train acc: top1 ->  99.874 ; top5 ->  100.0  and loss:  0.3831485864939168
test acc: top1 ->  91.88 ; top5 ->  99.12  and loss:  77.84593809396029
forward train acc: top1 ->  99.90599997558594 ; top5 ->  100.0  and loss:  0.29172387276776135
test acc: top1 ->  91.82 ; top5 ->  99.09  and loss:  76.85778884589672
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.28793657454662025
test acc: top1 ->  91.84 ; top5 ->  99.06  and loss:  76.48099308460951
forward train acc: top1 ->  99.918 ; top5 ->  100.0  and loss:  0.24223918383358978
test acc: top1 ->  91.64 ; top5 ->  99.09  and loss:  78.82231048494577
forward train acc: top1 ->  99.916 ; top5 ->  100.0  and loss:  0.2459441375976894
test acc: top1 ->  91.8 ; top5 ->  99.13  and loss:  75.0511766448617
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.20801374015718466
test acc: top1 ->  91.89 ; top5 ->  99.13  and loss:  74.40802874416113
forward train acc: top1 ->  99.93599997558594 ; top5 ->  100.0  and loss:  0.18987051560543478
test acc: top1 ->  91.85 ; top5 ->  99.12  and loss:  75.76379711180925
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.23689776382525451
test acc: top1 ->  91.73 ; top5 ->  99.0  and loss:  75.480408962816
forward train acc: top1 ->  99.93599997558594 ; top5 ->  100.0  and loss:  0.16053742147050798
test acc: top1 ->  91.74 ; top5 ->  99.17  and loss:  76.0512368157506
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  101 / 256 , inc:  2
---------------- start layer  7  ---------------
adv train loss:  -0.1468422377947718 , diff:  0.1468422377947718
adv train loss:  -0.23480772675247863 , diff:  0.08796548895770684
adv train loss:  -0.24974579385161633 , diff:  0.014938067099137697
adv train loss:  -0.18616742377344053 , diff:  0.0635783700781758
adv train loss:  -0.17329033714486286 , diff:  0.012877086628577672
adv train loss:  -0.210052206995897 , diff:  0.036761869851034135
adv train loss:  -0.2397729087206244 , diff:  0.029720701724727405
adv train loss:  -0.25497710332274437 , diff:  0.01520419460211997
adv train loss:  -0.24826676631346345 , diff:  0.00671033700928092
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  252
test acc: top1 ->  10.12 ; top5 ->  66.7  and loss:  21786233.9375
forward train acc: top1 ->  99.784 ; top5 ->  99.998  and loss:  0.6847131762187928
test acc: top1 ->  91.02 ; top5 ->  99.13  and loss:  74.55032368004322
forward train acc: top1 ->  99.85599997558593 ; top5 ->  100.0  and loss:  0.4256835291162133
test acc: top1 ->  91.07 ; top5 ->  99.0  and loss:  72.52394293248653
forward train acc: top1 ->  99.89 ; top5 ->  100.0  and loss:  0.36357879942806903
test acc: top1 ->  91.4 ; top5 ->  99.19  and loss:  68.82849361747503
forward train acc: top1 ->  99.91799997558594 ; top5 ->  100.0  and loss:  0.2566753141582012
test acc: top1 ->  91.38 ; top5 ->  99.18  and loss:  68.00085631012917
forward train acc: top1 ->  99.89600000244141 ; top5 ->  100.0  and loss:  0.31871316069737077
test acc: top1 ->  91.18 ; top5 ->  99.12  and loss:  71.63380779325962
forward train acc: top1 ->  99.908 ; top5 ->  100.0  and loss:  0.22385707730427384
test acc: top1 ->  91.34 ; top5 ->  99.13  and loss:  69.21962974220514
forward train acc: top1 ->  99.902 ; top5 ->  100.0  and loss:  0.3129570251767291
test acc: top1 ->  91.27 ; top5 ->  99.11  and loss:  68.64338155090809
forward train acc: top1 ->  99.92800000244141 ; top5 ->  100.0  and loss:  0.2343556433916092
test acc: top1 ->  91.33 ; top5 ->  99.14  and loss:  68.74677983671427
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.19243715493939817
test acc: top1 ->  91.31 ; top5 ->  99.13  and loss:  68.54688848555088
forward train acc: top1 ->  99.942 ; top5 ->  100.0  and loss:  0.18021904025226831
test acc: top1 ->  91.46 ; top5 ->  99.18  and loss:  68.65632827579975
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  253 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -70.38504320383072 , diff:  70.38504320383072
adv train loss:  -69.22405189275742 , diff:  1.1609913110733032
adv train loss:  -69.62293440103531 , diff:  0.39888250827789307
adv train loss:  -84.55425453186035 , diff:  14.931320130825043
adv train loss:  -99.24710381031036 , diff:  14.692849278450012
adv train loss:  -200.9148188829422 , diff:  101.66771507263184
adv train loss:  -234.1515827178955 , diff:  33.23676383495331
adv train loss:  -372.423052072525 , diff:  138.27146935462952
adv train loss:  -811.4006938934326 , diff:  438.9776418209076
adv train loss:  -957.3922653198242 , diff:  145.9915714263916
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  35
test acc: top1 ->  24.09 ; top5 ->  85.86  and loss:  4520.230806350708
forward train acc: top1 ->  99.764 ; top5 ->  100.0  and loss:  0.66167496827984
test acc: top1 ->  91.76 ; top5 ->  99.14  and loss:  63.570041820406914
forward train acc: top1 ->  99.90799997558594 ; top5 ->  100.0  and loss:  0.2852626754902303
test acc: top1 ->  91.72 ; top5 ->  99.2  and loss:  64.17120179533958
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.24320645936677465
test acc: top1 ->  91.8 ; top5 ->  99.24  and loss:  64.12947828322649
forward train acc: top1 ->  99.9360000024414 ; top5 ->  100.0  and loss:  0.1942916582338512
test acc: top1 ->  91.87 ; top5 ->  99.3  and loss:  63.98737123608589
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.13860257237683982
test acc: top1 ->  92.09 ; top5 ->  99.24  and loss:  65.18623269349337
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.13764177112898324
test acc: top1 ->  92.0 ; top5 ->  99.27  and loss:  63.98289640247822
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  37 / 512 , inc:  2
---------------- start layer  9  ---------------
adv train loss:  -599.093987941742 , diff:  599.093987941742
adv train loss:  -717.6608505249023 , diff:  118.5668625831604
adv train loss:  -715.6995191574097 , diff:  1.9613313674926758
adv train loss:  -718.0658302307129 , diff:  2.3663110733032227
adv train loss:  -717.775007724762 , diff:  0.29082250595092773
layer  9  adv train finish, try to retain  466
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -400.54242420196533 , diff:  400.54242420196533
adv train loss:  -400.20381903648376 , diff:  0.3386051654815674
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  10
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  239745.04663085938
forward train acc: top1 ->  72.88399998535156 ; top5 ->  94.312  and loss:  173.87525057792664
test acc: top1 ->  51.5 ; top5 ->  96.24  and loss:  210.32897460460663
forward train acc: top1 ->  98.99999997558594 ; top5 ->  99.996  and loss:  10.676602639257908
test acc: top1 ->  89.95 ; top5 ->  98.53  and loss:  44.191700875759125
forward train acc: top1 ->  99.53399997558594 ; top5 ->  99.998  and loss:  4.513058817014098
test acc: top1 ->  90.46 ; top5 ->  98.48  and loss:  45.37086484581232
forward train acc: top1 ->  99.69400000244141 ; top5 ->  99.998  and loss:  2.663458973169327
test acc: top1 ->  90.66 ; top5 ->  98.4  and loss:  47.52167220413685
forward train acc: top1 ->  99.774 ; top5 ->  100.0  and loss:  1.7915769666433334
test acc: top1 ->  91.0 ; top5 ->  98.43  and loss:  49.11182297766209
forward train acc: top1 ->  99.788 ; top5 ->  100.0  and loss:  1.3748190524056554
test acc: top1 ->  90.98 ; top5 ->  98.51  and loss:  50.27593370527029
forward train acc: top1 ->  99.808 ; top5 ->  100.0  and loss:  1.1705269683152437
test acc: top1 ->  90.98 ; top5 ->  98.5  and loss:  51.25655369460583
forward train acc: top1 ->  99.87 ; top5 ->  100.0  and loss:  1.0028944448567927
test acc: top1 ->  91.07 ; top5 ->  98.47  and loss:  52.01384899020195
forward train acc: top1 ->  99.87199997802735 ; top5 ->  100.0  and loss:  0.8394560851156712
test acc: top1 ->  91.22 ; top5 ->  98.38  and loss:  52.888536520302296
forward train acc: top1 ->  99.902 ; top5 ->  100.0  and loss:  0.7687549823895097
test acc: top1 ->  91.22 ; top5 ->  98.44  and loss:  53.585107795894146
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  11 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -2978.898729324341 , diff:  2978.898729324341
adv train loss:  -2980.301404953003 , diff:  1.4026756286621094
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  1
test acc: top1 ->  9.57 ; top5 ->  59.11  and loss:  7048.715446472168
forward train acc: top1 ->  94.13599997558593 ; top5 ->  99.86  and loss:  24.422031987458467
test acc: top1 ->  87.46 ; top5 ->  98.56  and loss:  76.03089465200901
forward train acc: top1 ->  99.20199997558593 ; top5 ->  99.994  and loss:  3.4502232149243355
test acc: top1 ->  89.54 ; top5 ->  98.53  and loss:  65.49364963173866
forward train acc: top1 ->  99.4840000024414 ; top5 ->  99.996  and loss:  2.2348204236477613
test acc: top1 ->  90.04 ; top5 ->  98.59  and loss:  65.45441350340843
forward train acc: top1 ->  99.638 ; top5 ->  99.996  and loss:  1.571439248509705
test acc: top1 ->  90.21 ; top5 ->  98.58  and loss:  65.31978341192007
forward train acc: top1 ->  99.68599997558594 ; top5 ->  99.996  and loss:  1.2874908745288849
test acc: top1 ->  90.35 ; top5 ->  98.57  and loss:  66.1828268095851
forward train acc: top1 ->  99.706 ; top5 ->  99.998  and loss:  1.1409354554489255
test acc: top1 ->  90.44 ; top5 ->  98.57  and loss:  66.89025544375181
forward train acc: top1 ->  99.73199997558594 ; top5 ->  99.994  and loss:  1.0761338286101818
test acc: top1 ->  90.67 ; top5 ->  98.56  and loss:  66.4309196472168
forward train acc: top1 ->  99.794 ; top5 ->  100.0  and loss:  0.8346700761467218
test acc: top1 ->  90.7 ; top5 ->  98.68  and loss:  67.15477884560823
forward train acc: top1 ->  99.79799997558594 ; top5 ->  100.0  and loss:  0.7762363273650408
test acc: top1 ->  90.68 ; top5 ->  98.61  and loss:  68.76694465428591
forward train acc: top1 ->  99.78 ; top5 ->  99.99  and loss:  0.9260375096928328
test acc: top1 ->  90.73 ; top5 ->  98.64  and loss:  68.33496798574924
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  2 / 512 , inc:  1
---------------- start layer  12  ---------------
### skip layer  12 wait:  3  ###
---------------- start layer  13  ---------------
### skip layer  13 wait:  3  ###
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.39453125  ==>  101 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.072265625  ==>  37 / 512 , inc:  1
layer  9  :  0.015625  ==>  8 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.08984375  ==>  46 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [0.9352284553349016, 2.4939425475597377, 0.9352284553349016, 0.9352284553349016, 0.9352284553349016, 0.9352284553349016, 0.9352284553349016, 1.2469712737798688, 1.6626283650398253, 2.2168378200531005, 1.4028426830023524, 1.6626283650398253, 0.9352284553349016, 13.301026920318602]  wait [3, 3, 3, 3, 3, 3, 2, 3, 4, 2, 3, 2, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 7
$$$$$$$$$$$$$ epoch  69  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -24.283968791365623 , diff:  24.283968791365623
adv train loss:  -24.177088379859924 , diff:  0.10688041150569916
adv train loss:  -24.30744643509388 , diff:  0.13035805523395538
adv train loss:  -24.880503863096237 , diff:  0.5730574280023575
adv train loss:  -24.710679590702057 , diff:  0.1698242723941803
adv train loss:  -24.72394757717848 , diff:  0.013267986476421356
layer  0  adv train finish, try to retain  64
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -24.41297572851181 , diff:  24.41297572851181
adv train loss:  -24.532304272055626 , diff:  0.11932854354381561
adv train loss:  -24.876455307006836 , diff:  0.34415103495121
adv train loss:  -24.676208436489105 , diff:  0.2002468705177307
adv train loss:  -24.58833934366703 , diff:  0.08786909282207489
adv train loss:  -24.766680099070072 , diff:  0.17834075540304184
adv train loss:  -24.609304070472717 , diff:  0.1573760285973549
adv train loss:  -24.30876760184765 , diff:  0.30053646862506866
adv train loss:  -24.94269299507141 , diff:  0.6339253932237625
adv train loss:  -24.49475683271885 , diff:  0.44793616235256195
layer  1  adv train finish, try to retain  44
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -24.401985555887222 , diff:  24.401985555887222
adv train loss:  -24.097010888159275 , diff:  0.30497466772794724
adv train loss:  -24.304688841104507 , diff:  0.2076779529452324
adv train loss:  -24.135125435888767 , diff:  0.1695634052157402
adv train loss:  -24.50495694577694 , diff:  0.36983150988817215
adv train loss:  -24.239830911159515 , diff:  0.265126034617424
adv train loss:  -24.491406843066216 , diff:  0.25157593190670013
adv train loss:  -24.572813808918 , diff:  0.08140696585178375
adv train loss:  -24.49403638392687 , diff:  0.07877742499113083
adv train loss:  -24.378525033593178 , diff:  0.11551135033369064
layer  2  adv train finish, try to retain  122
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -24.170563608407974 , diff:  24.170563608407974
adv train loss:  -24.283546045422554 , diff:  0.11298243701457977
adv train loss:  -24.080928303301334 , diff:  0.20261774212121964
adv train loss:  -24.100236967206 , diff:  0.0193086639046669
layer  3  adv train finish, try to retain  127
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -24.23943281173706 , diff:  24.23943281173706
adv train loss:  -24.548939123749733 , diff:  0.3095063120126724
adv train loss:  -24.370645105838776 , diff:  0.17829401791095734
adv train loss:  -24.79990567266941 , diff:  0.42926056683063507
adv train loss:  -24.239732459187508 , diff:  0.5601732134819031
adv train loss:  -24.4238241314888 , diff:  0.18409167230129242
adv train loss:  -24.592981807887554 , diff:  0.16915767639875412
adv train loss:  -24.480956375598907 , diff:  0.1120254322886467
adv train loss:  -24.733212158083916 , diff:  0.25225578248500824
adv train loss:  -24.66466487199068 , diff:  0.06854728609323502
layer  4  adv train finish, try to retain  242
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -24.59364053606987 , diff:  24.59364053606987
adv train loss:  -24.30332252383232 , diff:  0.29031801223754883
adv train loss:  -24.442250341176987 , diff:  0.13892781734466553
adv train loss:  -23.904318436980247 , diff:  0.5379319041967392
adv train loss:  -24.303163565695286 , diff:  0.3988451287150383
adv train loss:  -24.608088970184326 , diff:  0.3049254044890404
adv train loss:  -24.330589346587658 , diff:  0.27749962359666824
adv train loss:  -24.579236336052418 , diff:  0.24864698946475983
adv train loss:  -24.167424365878105 , diff:  0.4118119701743126
adv train loss:  -24.688872650265694 , diff:  0.5214482843875885
layer  5  adv train finish, try to retain  253
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -25.307871237397194 , diff:  25.307871237397194
adv train loss:  -25.981581449508667 , diff:  0.6737102121114731
adv train loss:  -25.161889486014843 , diff:  0.819691963493824
adv train loss:  -25.678186737000942 , diff:  0.5162972509860992
adv train loss:  -25.239520207047462 , diff:  0.43866652995347977
adv train loss:  -25.328578338027 , diff:  0.08905813097953796
adv train loss:  -25.361312814056873 , diff:  0.032734476029872894
layer  6  adv train finish, try to retain  252
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -31.801773622632027 , diff:  31.801773622632027
adv train loss:  -31.360440284013748 , diff:  0.4413333386182785
adv train loss:  -34.72327384352684 , diff:  3.362833559513092
adv train loss:  -37.799711272120476 , diff:  3.0764374285936356
adv train loss:  -38.752869978547096 , diff:  0.9531587064266205
adv train loss:  -52.45284792780876 , diff:  13.699977949261665
adv train loss:  -57.5850565135479 , diff:  5.132208585739136
adv train loss:  -59.519674867391586 , diff:  1.934618353843689
adv train loss:  -67.74405473470688 , diff:  8.224379867315292
adv train loss:  -76.54805308580399 , diff:  8.803998351097107
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  252
test acc: top1 ->  10.39 ; top5 ->  61.0  and loss:  16234850.171875
forward train acc: top1 ->  99.6180000024414 ; top5 ->  99.992  and loss:  1.362066209083423
test acc: top1 ->  91.35 ; top5 ->  98.81  and loss:  82.28782007098198
forward train acc: top1 ->  99.85999997558594 ; top5 ->  100.0  and loss:  0.44553239503875375
test acc: top1 ->  91.41 ; top5 ->  98.89  and loss:  79.99332596361637
forward train acc: top1 ->  99.912 ; top5 ->  100.0  and loss:  0.264436403682339
test acc: top1 ->  91.49 ; top5 ->  98.93  and loss:  79.33685125410557
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.22410579762799898
test acc: top1 ->  91.57 ; top5 ->  98.91  and loss:  77.67282694578171
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.1625473896274343
test acc: top1 ->  91.7 ; top5 ->  99.08  and loss:  77.29224835336208
forward train acc: top1 ->  99.95399997558594 ; top5 ->  100.0  and loss:  0.1970888280775398
test acc: top1 ->  91.65 ; top5 ->  99.05  and loss:  77.81178103387356
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.2333485275230487
test acc: top1 ->  91.64 ; top5 ->  99.05  and loss:  77.59639786183834
forward train acc: top1 ->  99.92999997558594 ; top5 ->  100.0  and loss:  0.20359633583575487
test acc: top1 ->  91.66 ; top5 ->  99.03  and loss:  76.71996711194515
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.20548260954092257
test acc: top1 ->  91.52 ; top5 ->  99.03  and loss:  77.33775836229324
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.1726645379094407
test acc: top1 ->  91.62 ; top5 ->  99.1  and loss:  76.87831021100283
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  253 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -70.84440314769745 , diff:  70.84440314769745
adv train loss:  -71.34465563297272 , diff:  0.5002524852752686
adv train loss:  -71.95174333453178 , diff:  0.6070877015590668
adv train loss:  -70.78329735994339 , diff:  1.1684459745883942
adv train loss:  -70.87608289718628 , diff:  0.0927855372428894
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  36
test acc: top1 ->  20.39 ; top5 ->  60.69  and loss:  11922977.703125
forward train acc: top1 ->  99.73799997558594 ; top5 ->  100.0  and loss:  0.7839259202592075
test acc: top1 ->  91.52 ; top5 ->  98.72  and loss:  74.66592636704445
forward train acc: top1 ->  99.916 ; top5 ->  100.0  and loss:  0.28640175044711214
test acc: top1 ->  91.86 ; top5 ->  98.86  and loss:  72.84177732467651
forward train acc: top1 ->  99.924 ; top5 ->  100.0  and loss:  0.21026635961607099
test acc: top1 ->  91.74 ; top5 ->  98.77  and loss:  74.31384387612343
forward train acc: top1 ->  99.942 ; top5 ->  100.0  and loss:  0.18979590584058315
test acc: top1 ->  91.81 ; top5 ->  98.82  and loss:  75.6908156722784
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.17978178133489564
test acc: top1 ->  91.74 ; top5 ->  98.83  and loss:  75.43041275441647
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.07519538930500858
test acc: top1 ->  91.91 ; top5 ->  98.85  and loss:  76.05437285453081
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.14837957127019763
test acc: top1 ->  91.96 ; top5 ->  98.89  and loss:  75.73967459797859
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.07384293054929003
test acc: top1 ->  91.85 ; top5 ->  98.97  and loss:  75.63127567246556
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.09028005830623442
test acc: top1 ->  91.84 ; top5 ->  98.95  and loss:  76.33396361395717
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.12397571360270376
test acc: top1 ->  91.93 ; top5 ->  98.98  and loss:  75.37767678126693
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  37 / 512 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -822.1566152572632 , diff:  822.1566152572632
adv train loss:  -880.2929925918579 , diff:  58.13637733459473
adv train loss:  -901.8117237091064 , diff:  21.518731117248535
adv train loss:  -900.9492750167847 , diff:  0.8624486923217773
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  7
test acc: top1 ->  1.41 ; top5 ->  64.06  and loss:  10042.224105834961
forward train acc: top1 ->  97.892 ; top5 ->  99.994  and loss:  7.184424703242257
test acc: top1 ->  89.83 ; top5 ->  98.87  and loss:  86.172626465559
forward train acc: top1 ->  99.846 ; top5 ->  100.0  and loss:  0.5247930286859628
test acc: top1 ->  91.48 ; top5 ->  98.83  and loss:  76.4946227259934
forward train acc: top1 ->  99.882 ; top5 ->  100.0  and loss:  0.3590396665968001
test acc: top1 ->  91.7 ; top5 ->  98.9  and loss:  75.04042800143361
forward train acc: top1 ->  99.892 ; top5 ->  100.0  and loss:  0.33455092972144485
test acc: top1 ->  91.68 ; top5 ->  98.92  and loss:  74.70177674293518
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.19833328039385378
test acc: top1 ->  91.71 ; top5 ->  98.94  and loss:  74.96771471761167
forward train acc: top1 ->  99.932 ; top5 ->  99.998  and loss:  0.22769019813858904
test acc: top1 ->  91.74 ; top5 ->  98.96  and loss:  75.63683073222637
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.16049747890792787
test acc: top1 ->  91.8 ; top5 ->  99.02  and loss:  74.88526236824691
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.13269864558242261
test acc: top1 ->  91.84 ; top5 ->  98.99  and loss:  75.25367917865515
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.1628723214380443
test acc: top1 ->  91.77 ; top5 ->  98.97  and loss:  75.70356691069901
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.1114767638646299
test acc: top1 ->  91.81 ; top5 ->  98.98  and loss:  76.42404545750469
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  8 / 512 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -452.6223907470703 , diff:  452.6223907470703
adv train loss:  -453.3200821876526 , diff:  0.6976914405822754
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  10
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  151706.18579101562
forward train acc: top1 ->  81.75199998046875 ; top5 ->  98.082  and loss:  90.26713132858276
test acc: top1 ->  39.97 ; top5 ->  87.95  and loss:  381.3526029586792
forward train acc: top1 ->  99.28800000244141 ; top5 ->  99.99  and loss:  5.146705683320761
test acc: top1 ->  89.98 ; top5 ->  98.01  and loss:  52.95615476369858
forward train acc: top1 ->  99.61399997558594 ; top5 ->  99.998  and loss:  2.533773588016629
test acc: top1 ->  90.4 ; top5 ->  98.17  and loss:  52.97192159295082
forward train acc: top1 ->  99.702 ; top5 ->  100.0  and loss:  1.6917262598872185
test acc: top1 ->  90.66 ; top5 ->  98.2  and loss:  53.196378752589226
forward train acc: top1 ->  99.75399997558594 ; top5 ->  99.998  and loss:  1.307762561365962
test acc: top1 ->  90.87 ; top5 ->  98.14  and loss:  54.316635087132454
forward train acc: top1 ->  99.812 ; top5 ->  100.0  and loss:  0.9979019099846482
test acc: top1 ->  90.93 ; top5 ->  98.2  and loss:  55.03443360328674
forward train acc: top1 ->  99.79799997558594 ; top5 ->  100.0  and loss:  0.9356461907736957
test acc: top1 ->  90.97 ; top5 ->  98.11  and loss:  55.654579147696495
forward train acc: top1 ->  99.87800000244141 ; top5 ->  99.998  and loss:  0.7533355420455337
test acc: top1 ->  90.98 ; top5 ->  98.27  and loss:  55.78893905878067
forward train acc: top1 ->  99.828 ; top5 ->  99.998  and loss:  0.759157860185951
test acc: top1 ->  91.21 ; top5 ->  98.2  and loss:  56.21444392204285
forward train acc: top1 ->  99.884 ; top5 ->  100.0  and loss:  0.5974178626202047
test acc: top1 ->  91.2 ; top5 ->  98.1  and loss:  56.87802095711231
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  11 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -2767.2748527526855 , diff:  2767.2748527526855
adv train loss:  -2768.1860179901123 , diff:  0.9111652374267578
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  1
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  28901.334259033203
forward train acc: top1 ->  92.19399998291016 ; top5 ->  99.38  and loss:  40.15649378672242
test acc: top1 ->  88.52 ; top5 ->  98.42  and loss:  80.52949452400208
forward train acc: top1 ->  99.40000000488281 ; top5 ->  99.998  and loss:  3.8208230063319206
test acc: top1 ->  89.62 ; top5 ->  98.38  and loss:  75.75308500230312
forward train acc: top1 ->  99.60799997558594 ; top5 ->  99.998  and loss:  2.4015471171587706
test acc: top1 ->  90.08 ; top5 ->  98.59  and loss:  71.84756883978844
forward train acc: top1 ->  99.72799997802734 ; top5 ->  100.0  and loss:  1.4957804549485445
test acc: top1 ->  90.37 ; top5 ->  98.42  and loss:  71.46384055912495
forward train acc: top1 ->  99.76 ; top5 ->  100.0  and loss:  1.2570964717306197
test acc: top1 ->  90.31 ; top5 ->  98.48  and loss:  72.08439938724041
forward train acc: top1 ->  99.844 ; top5 ->  100.0  and loss:  0.8802221184596419
test acc: top1 ->  90.46 ; top5 ->  98.52  and loss:  71.64773018658161
forward train acc: top1 ->  99.83199997558594 ; top5 ->  100.0  and loss:  0.8400888480246067
test acc: top1 ->  90.52 ; top5 ->  98.5  and loss:  72.38756726682186
forward train acc: top1 ->  99.84999997558593 ; top5 ->  100.0  and loss:  0.7676759120076895
test acc: top1 ->  90.61 ; top5 ->  98.52  and loss:  72.4534881412983
forward train acc: top1 ->  99.848 ; top5 ->  100.0  and loss:  0.7561897379346192
test acc: top1 ->  90.75 ; top5 ->  98.51  and loss:  71.87562994658947
forward train acc: top1 ->  99.844 ; top5 ->  100.0  and loss:  0.7440198988188058
test acc: top1 ->  90.77 ; top5 ->  98.58  and loss:  72.09764084219933
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  2 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -129.83361554145813 , diff:  129.83361554145813
adv train loss:  -130.14540112018585 , diff:  0.31178557872772217
adv train loss:  -130.8440533876419 , diff:  0.6986522674560547
adv train loss:  -130.40367472171783 , diff:  0.44037866592407227
adv train loss:  -130.70745968818665 , diff:  0.30378496646881104
adv train loss:  -130.42063307762146 , diff:  0.28682661056518555
adv train loss:  -129.50900626182556 , diff:  0.9116268157958984
adv train loss:  -130.18886309862137 , diff:  0.6798568367958069
adv train loss:  -131.285242497921 , diff:  1.0963793992996216
adv train loss:  -131.27075493335724 , diff:  0.01448756456375122
layer  12  adv train finish, try to retain  467
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -7984.224021911621 , diff:  7984.224021911621
adv train loss:  -13265.30249786377 , diff:  5281.078475952148
adv train loss:  -19684.99432373047 , diff:  6419.691825866699
adv train loss:  -27776.488830566406 , diff:  8091.4945068359375
adv train loss:  -36039.997619628906 , diff:  8263.5087890625
adv train loss:  -43985.52444458008 , diff:  7945.526824951172
adv train loss:  -51375.43142700195 , diff:  7389.906982421875
adv train loss:  -58502.89025878906 , diff:  7127.458831787109
adv train loss:  -64848.353515625 , diff:  6345.4632568359375
adv train loss:  -70021.3359375 , diff:  5172.982421875
layer  13  adv train finish, try to retain  10
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.39453125  ==>  101 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.072265625  ==>  37 / 512 , inc:  1
layer  9  :  0.015625  ==>  8 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.08984375  ==>  46 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [1.8704569106698032, 4.987885095119475, 1.8704569106698032, 1.8704569106698032, 1.8704569106698032, 1.8704569106698032, 1.8704569106698032, 0.9352284553349016, 1.2469712737798688, 1.6626283650398253, 1.0521320122517643, 1.2469712737798688, 1.8704569106698032, 26.602053840637204]  wait [1, 1, 1, 1, 1, 1, 0, 3, 4, 2, 3, 2, 0, 0]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 8
$$$$$$$$$$$$$ epoch  70  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1226.5612602233887 , diff:  1226.5612602233887
adv train loss:  -1237.2590379714966 , diff:  10.69777774810791
adv train loss:  -1243.7871952056885 , diff:  6.5281572341918945
adv train loss:  -1235.04909324646 , diff:  8.738101959228516
adv train loss:  -1233.4117727279663 , diff:  1.6373205184936523
adv train loss:  -1250.8803625106812 , diff:  17.468589782714844
adv train loss:  -1231.5903749465942 , diff:  19.289987564086914
adv train loss:  -1236.2230138778687 , diff:  4.632638931274414
adv train loss:  -1239.4896383285522 , diff:  3.2666244506835938
adv train loss:  -1241.6950254440308 , diff:  2.2053871154785156
layer  0  adv train finish, try to retain  3
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  2297.4717903137207
forward train acc: top1 ->  30.615999989013673 ; top5 ->  77.74799998291016  and loss:  498.50882291793823
test acc: top1 ->  14.06 ; top5 ->  58.22  and loss:  599.8942303657532
forward train acc: top1 ->  36.41399999389648 ; top5 ->  82.90999998535156  and loss:  187.56112909317017
test acc: top1 ->  39.65 ; top5 ->  85.97  and loss:  174.68068659305573
forward train acc: top1 ->  39.76799998657226 ; top5 ->  85.67999998535156  and loss:  168.62998056411743
test acc: top1 ->  42.62 ; top5 ->  86.97  and loss:  165.32570624351501
forward train acc: top1 ->  42.204000004882815 ; top5 ->  87.33199997314453  and loss:  160.95096957683563
test acc: top1 ->  45.12 ; top5 ->  88.6  and loss:  156.6481100320816
forward train acc: top1 ->  44.9579999987793 ; top5 ->  88.77399999755859  and loss:  153.02891302108765
test acc: top1 ->  47.61 ; top5 ->  90.3  and loss:  149.2695300579071
forward train acc: top1 ->  47.002 ; top5 ->  89.68799997314453  and loss:  147.2976859807968
test acc: top1 ->  49.18 ; top5 ->  90.6  and loss:  145.6529802083969
forward train acc: top1 ->  48.34399999511719 ; top5 ->  90.166  and loss:  144.08229732513428
test acc: top1 ->  49.81 ; top5 ->  91.11  and loss:  142.81648898124695
forward train acc: top1 ->  49.711999993896484 ; top5 ->  90.93600000488281  and loss:  140.58139860630035
test acc: top1 ->  51.25 ; top5 ->  91.8  and loss:  139.54360914230347
forward train acc: top1 ->  51.00399999023438 ; top5 ->  91.37800000244141  and loss:  137.24065029621124
test acc: top1 ->  53.1 ; top5 ->  92.12  and loss:  136.08085680007935
forward train acc: top1 ->  52.558 ; top5 ->  91.76600000732422  and loss:  134.02713251113892
test acc: top1 ->  54.34 ; top5 ->  92.42  and loss:  132.63547909259796
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -40.17764759063721 , diff:  40.17764759063721
adv train loss:  -40.1940136551857 , diff:  0.01636606454849243
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  43
test acc: top1 ->  10.3 ; top5 ->  50.41  and loss:  392.1428265571594
forward train acc: top1 ->  99.678 ; top5 ->  100.0  and loss:  5.259517253492959
test acc: top1 ->  91.63 ; top5 ->  99.18  and loss:  45.40259948372841
forward train acc: top1 ->  99.93199997558594 ; top5 ->  100.0  and loss:  0.27850652346387506
test acc: top1 ->  91.81 ; top5 ->  99.16  and loss:  49.5865408051759
forward train acc: top1 ->  99.94799997558594 ; top5 ->  100.0  and loss:  0.22290586587041616
test acc: top1 ->  91.89 ; top5 ->  99.16  and loss:  51.99185770750046
forward train acc: top1 ->  99.94400000244141 ; top5 ->  100.0  and loss:  0.19867607485502958
test acc: top1 ->  91.89 ; top5 ->  99.18  and loss:  53.912540677934885
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.16840766751556657
test acc: top1 ->  91.85 ; top5 ->  99.13  and loss:  55.42207329720259
forward train acc: top1 ->  99.97000000244141 ; top5 ->  100.0  and loss:  0.11141546629369259
test acc: top1 ->  91.95 ; top5 ->  99.17  and loss:  56.38934150338173
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.13450650707818568
test acc: top1 ->  91.95 ; top5 ->  99.22  and loss:  56.80242026038468
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.09172534447861835
test acc: top1 ->  92.0 ; top5 ->  99.24  and loss:  57.44254513271153
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.08787535144074354
test acc: top1 ->  92.0 ; top5 ->  99.19  and loss:  58.13246283121407
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.06199980387464166
test acc: top1 ->  91.94 ; top5 ->  99.17  and loss:  59.155904956161976
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  44 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -28.446478717960417 , diff:  28.446478717960417
adv train loss:  -111.33725041151047 , diff:  82.89077169355005
adv train loss:  -132.43820464611053 , diff:  21.100954234600067
adv train loss:  -133.6154284477234 , diff:  1.177223801612854
adv train loss:  -146.09503781795502 , diff:  12.479609370231628
adv train loss:  -215.12471103668213 , diff:  69.02967321872711
adv train loss:  -267.19984126091003 , diff:  52.075130224227905
adv train loss:  -300.40472650527954 , diff:  33.20488524436951
adv train loss:  -309.0710458755493 , diff:  8.666319370269775
adv train loss:  -308.75166511535645 , diff:  0.3193807601928711
layer  2  adv train finish, try to retain  79
test acc: top1 ->  10.0 ; top5 ->  49.9  and loss:  2527.7850704193115
forward train acc: top1 ->  99.3 ; top5 ->  99.992  and loss:  2.3852091229055077
test acc: top1 ->  90.5 ; top5 ->  99.15  and loss:  53.70779176056385
forward train acc: top1 ->  99.472 ; top5 ->  99.992  and loss:  1.665275045670569
test acc: top1 ->  90.75 ; top5 ->  99.27  and loss:  50.929268434643745
forward train acc: top1 ->  99.57 ; top5 ->  99.996  and loss:  1.2239899770356715
test acc: top1 ->  90.75 ; top5 ->  99.05  and loss:  53.33114168047905
forward train acc: top1 ->  99.59199997558594 ; top5 ->  100.0  and loss:  1.1643735021352768
test acc: top1 ->  90.8 ; top5 ->  99.14  and loss:  54.36528468132019
forward train acc: top1 ->  99.67399997558594 ; top5 ->  100.0  and loss:  1.0024807658046484
test acc: top1 ->  90.75 ; top5 ->  99.22  and loss:  53.98075321316719
forward train acc: top1 ->  99.7240000024414 ; top5 ->  100.0  and loss:  0.8528240974992514
test acc: top1 ->  90.92 ; top5 ->  99.23  and loss:  54.32416784018278
forward train acc: top1 ->  99.68399997558593 ; top5 ->  100.0  and loss:  0.952267840038985
test acc: top1 ->  90.96 ; top5 ->  99.24  and loss:  53.68853823840618
forward train acc: top1 ->  99.7060000024414 ; top5 ->  99.992  and loss:  0.901020004414022
test acc: top1 ->  90.97 ; top5 ->  99.28  and loss:  53.54255159199238
forward train acc: top1 ->  99.684 ; top5 ->  99.998  and loss:  0.8079176973551512
test acc: top1 ->  91.01 ; top5 ->  99.31  and loss:  53.47831488400698
forward train acc: top1 ->  99.71999997558594 ; top5 ->  99.998  and loss:  0.7851243577897549
test acc: top1 ->  90.96 ; top5 ->  99.31  and loss:  54.38238851726055
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -646.3728400301188 , diff:  646.3728400301188
adv train loss:  -1142.3556470870972 , diff:  495.98280705697834
adv train loss:  -1154.9209632873535 , diff:  12.565316200256348
adv train loss:  -1178.5037479400635 , diff:  23.58278465270996
adv train loss:  -1209.2364339828491 , diff:  30.732686042785645
adv train loss:  -1208.3753643035889 , diff:  0.8610696792602539
adv train loss:  -1209.2946472167969 , diff:  0.9192829132080078
adv train loss:  -1209.384648323059 , diff:  0.09000110626220703
adv train loss:  -1211.5615282058716 , diff:  2.1768798828125
adv train loss:  -1216.4932699203491 , diff:  4.931741714477539
layer  3  adv train finish, try to retain  20
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  1172.8201122283936
forward train acc: top1 ->  71.09599997314453 ; top5 ->  97.00999998291016  and loss:  102.37164860963821
test acc: top1 ->  71.32 ; top5 ->  97.09  and loss:  87.13105380535126
forward train acc: top1 ->  75.04 ; top5 ->  97.87199998779298  and loss:  72.73529952764511
test acc: top1 ->  73.4 ; top5 ->  97.37  and loss:  80.83759784698486
forward train acc: top1 ->  77.42199998535156 ; top5 ->  98.07199998535157  and loss:  65.86927562952042
test acc: top1 ->  75.26 ; top5 ->  97.76  and loss:  76.27779847383499
forward train acc: top1 ->  79.328 ; top5 ->  98.34999997802734  and loss:  60.767001897096634
test acc: top1 ->  76.04 ; top5 ->  97.85  and loss:  72.78577402234077
forward train acc: top1 ->  80.40000001464844 ; top5 ->  98.58200000732423  and loss:  57.635630398988724
test acc: top1 ->  77.05 ; top5 ->  97.96  and loss:  69.95494943857193
forward train acc: top1 ->  81.13000001708984 ; top5 ->  98.57999998046876  and loss:  55.24262547492981
test acc: top1 ->  77.57 ; top5 ->  98.17  and loss:  67.85026776790619
forward train acc: top1 ->  81.78199997070313 ; top5 ->  98.79200000488281  and loss:  53.313944876194
test acc: top1 ->  78.08 ; top5 ->  98.11  and loss:  67.34553334116936
forward train acc: top1 ->  82.11799999267578 ; top5 ->  98.7580000024414  and loss:  52.76437857747078
test acc: top1 ->  78.24 ; top5 ->  98.18  and loss:  65.94234412908554
forward train acc: top1 ->  82.44400000732422 ; top5 ->  98.89399997802734  and loss:  51.397262305021286
test acc: top1 ->  78.64 ; top5 ->  98.19  and loss:  65.1636817753315
forward train acc: top1 ->  82.72200001464844 ; top5 ->  98.89799997802734  and loss:  50.58208844065666
test acc: top1 ->  78.65 ; top5 ->  98.12  and loss:  64.1766873896122
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -74.0144276842475 , diff:  74.0144276842475
adv train loss:  -362.9196627140045 , diff:  288.905235029757
adv train loss:  -418.2408962249756 , diff:  55.32123351097107
adv train loss:  -454.3590769767761 , diff:  36.11818075180054
adv train loss:  -469.9862856864929 , diff:  15.627208709716797
adv train loss:  -472.86898612976074 , diff:  2.8827004432678223
adv train loss:  -477.2355155944824 , diff:  4.36652946472168
adv train loss:  -494.00003480911255 , diff:  16.764519214630127
adv train loss:  -496.956889629364 , diff:  2.956854820251465
adv train loss:  -502.79171419143677 , diff:  5.834824562072754
layer  4  adv train finish, try to retain  22
test acc: top1 ->  10.0 ; top5 ->  51.59  and loss:  688.058575630188
forward train acc: top1 ->  66.78999998291016 ; top5 ->  95.48799997070313  and loss:  97.92669469118118
test acc: top1 ->  68.37 ; top5 ->  95.87  and loss:  95.4621906876564
forward train acc: top1 ->  72.626 ; top5 ->  97.38599998291015  and loss:  78.50430428981781
test acc: top1 ->  71.91 ; top5 ->  97.1  and loss:  84.10411816835403
forward train acc: top1 ->  75.49399999023437 ; top5 ->  97.99399997558594  and loss:  70.47084200382233
test acc: top1 ->  73.52 ; top5 ->  97.57  and loss:  78.74819135665894
forward train acc: top1 ->  77.56600000244141 ; top5 ->  98.16400000732422  and loss:  64.73856890201569
test acc: top1 ->  74.76 ; top5 ->  97.64  and loss:  75.40613341331482
forward train acc: top1 ->  78.42599998046875 ; top5 ->  98.42999998291016  and loss:  61.89092403650284
test acc: top1 ->  76.08 ; top5 ->  97.86  and loss:  71.51808851957321
forward train acc: top1 ->  79.46199997802735 ; top5 ->  98.49200000976562  and loss:  59.22200506925583
test acc: top1 ->  76.36 ; top5 ->  97.87  and loss:  70.66978839039803
forward train acc: top1 ->  79.72200001464844 ; top5 ->  98.53200000732421  and loss:  58.099913001060486
test acc: top1 ->  77.08 ; top5 ->  97.92  and loss:  69.49915391206741
forward train acc: top1 ->  80.14600000488281 ; top5 ->  98.66200000488281  and loss:  57.06650269031525
test acc: top1 ->  77.27 ; top5 ->  98.06  and loss:  67.85943603515625
forward train acc: top1 ->  80.51999997314454 ; top5 ->  98.61800000732421  and loss:  55.877094745635986
test acc: top1 ->  77.62 ; top5 ->  98.08  and loss:  66.95352676510811
forward train acc: top1 ->  80.92800000732421 ; top5 ->  98.70799997802735  and loss:  54.903243243694305
test acc: top1 ->  77.94 ; top5 ->  98.09  and loss:  66.20508959889412
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -195.48086465150118 , diff:  195.48086465150118
adv train loss:  -703.2292580604553 , diff:  507.74839340895414
adv train loss:  -791.3486318588257 , diff:  88.11937379837036
adv train loss:  -802.3211374282837 , diff:  10.972505569458008
adv train loss:  -805.3959269523621 , diff:  3.074789524078369
adv train loss:  -803.1348967552185 , diff:  2.2610301971435547
adv train loss:  -802.8098616600037 , diff:  0.32503509521484375
adv train loss:  -806.640296459198 , diff:  3.830434799194336
adv train loss:  -814.015950679779 , diff:  7.375654220581055
adv train loss:  -814.499171257019 , diff:  0.48322057723999023
layer  5  adv train finish, try to retain  28
test acc: top1 ->  10.01 ; top5 ->  52.57  and loss:  1467.3025093078613
forward train acc: top1 ->  84.40799997070313 ; top5 ->  99.33599997558593  and loss:  44.79303619265556
test acc: top1 ->  81.45 ; top5 ->  98.83  and loss:  57.86359199881554
forward train acc: top1 ->  87.49199997558594 ; top5 ->  99.59199997558594  and loss:  35.706960797309875
test acc: top1 ->  83.29 ; top5 ->  98.88  and loss:  52.33427394926548
forward train acc: top1 ->  88.93200001708985 ; top5 ->  99.67199997558593  and loss:  31.33328104019165
test acc: top1 ->  84.24 ; top5 ->  99.01  and loss:  50.57985758781433
forward train acc: top1 ->  89.89199999023438 ; top5 ->  99.688  and loss:  28.902982160449028
test acc: top1 ->  84.75 ; top5 ->  99.04  and loss:  49.58118882775307
forward train acc: top1 ->  90.41600000976563 ; top5 ->  99.74399997558594  and loss:  27.247488707304
test acc: top1 ->  84.97 ; top5 ->  99.1  and loss:  48.82014927268028
forward train acc: top1 ->  90.81199998779297 ; top5 ->  99.71799997558594  and loss:  25.923590674996376
test acc: top1 ->  85.46 ; top5 ->  99.11  and loss:  47.12381149828434
forward train acc: top1 ->  91.25200001220703 ; top5 ->  99.758  and loss:  24.896306842565536
test acc: top1 ->  85.23 ; top5 ->  99.14  and loss:  47.72042874991894
forward train acc: top1 ->  91.50200000488282 ; top5 ->  99.754  and loss:  24.04625877737999
test acc: top1 ->  85.86 ; top5 ->  99.11  and loss:  47.0718674659729
forward train acc: top1 ->  91.62 ; top5 ->  99.7780000024414  and loss:  23.926591768860817
test acc: top1 ->  85.88 ; top5 ->  99.17  and loss:  46.40686742961407
forward train acc: top1 ->  91.79199998046874 ; top5 ->  99.782  and loss:  23.36910378932953
test acc: top1 ->  86.0 ; top5 ->  99.2  and loss:  46.202544286847115
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -220.097587518394 , diff:  220.097587518394
adv train loss:  -1028.6943006515503 , diff:  808.5967131331563
adv train loss:  -1127.5630149841309 , diff:  98.86871433258057
adv train loss:  -1148.673002243042 , diff:  21.109987258911133
adv train loss:  -1153.6063499450684 , diff:  4.933347702026367
adv train loss:  -1156.4101676940918 , diff:  2.8038177490234375
adv train loss:  -1159.2006616592407 , diff:  2.790493965148926
adv train loss:  -1157.7727355957031 , diff:  1.4279260635375977
adv train loss:  -1156.9726943969727 , diff:  0.8000411987304688
adv train loss:  -1157.9929151535034 , diff:  1.0202207565307617
layer  6  adv train finish, try to retain  15
test acc: top1 ->  19.12 ; top5 ->  58.98  and loss:  515.5959420204163
forward train acc: top1 ->  82.72799999511719 ; top5 ->  99.14  and loss:  52.16796350479126
test acc: top1 ->  82.41 ; top5 ->  98.72  and loss:  61.48676806688309
forward train acc: top1 ->  90.38400000732422 ; top5 ->  99.684  and loss:  29.01230038702488
test acc: top1 ->  84.85 ; top5 ->  98.94  and loss:  54.619875997304916
forward train acc: top1 ->  92.26799997558594 ; top5 ->  99.79599997558594  and loss:  22.633971005678177
test acc: top1 ->  86.08 ; top5 ->  99.09  and loss:  50.04946230351925
forward train acc: top1 ->  93.47199998046875 ; top5 ->  99.828  and loss:  19.375755473971367
test acc: top1 ->  86.52 ; top5 ->  99.1  and loss:  50.20486523211002
forward train acc: top1 ->  93.97799997314453 ; top5 ->  99.87199997558594  and loss:  17.158245101571083
test acc: top1 ->  87.02 ; top5 ->  99.05  and loss:  48.65311463177204
forward train acc: top1 ->  94.74199997070312 ; top5 ->  99.9  and loss:  15.248050592839718
test acc: top1 ->  87.49 ; top5 ->  99.11  and loss:  46.971178486943245
forward train acc: top1 ->  94.80399997070313 ; top5 ->  99.904  and loss:  14.696028113365173
test acc: top1 ->  87.59 ; top5 ->  99.19  and loss:  47.56984253227711
forward train acc: top1 ->  95.09399997558593 ; top5 ->  99.91199997558594  and loss:  14.130806349217892
test acc: top1 ->  87.91 ; top5 ->  99.16  and loss:  46.64783622324467
forward train acc: top1 ->  95.49199999267579 ; top5 ->  99.936  and loss:  13.298629499971867
test acc: top1 ->  87.98 ; top5 ->  99.14  and loss:  46.6959763020277
forward train acc: top1 ->  95.50000001953126 ; top5 ->  99.924  and loss:  13.040950313210487
test acc: top1 ->  88.0 ; top5 ->  99.1  and loss:  46.67598870396614
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  101 / 256 , inc:  1
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
### skip layer  8 wait:  4  ###
---------------- start layer  9  ---------------
adv train loss:  -120.69361609220505 , diff:  120.69361609220505
adv train loss:  -147.29216372966766 , diff:  26.598547637462616
adv train loss:  -146.36434185504913 , diff:  0.9278218746185303
adv train loss:  -147.2499452829361 , diff:  0.8856034278869629
adv train loss:  -146.97387635707855 , diff:  0.27606892585754395
adv train loss:  -147.365968644619 , diff:  0.3920922875404358
adv train loss:  -155.7578990459442 , diff:  8.391930401325226
adv train loss:  -155.52188277244568 , diff:  0.23601627349853516
adv train loss:  -155.6792607307434 , diff:  0.1573779582977295
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  7
test acc: top1 ->  10.0 ; top5 ->  53.56  and loss:  19817.381591796875
forward train acc: top1 ->  98.67000000244141 ; top5 ->  99.996  and loss:  4.358778623864055
test acc: top1 ->  90.82 ; top5 ->  99.16  and loss:  47.29543635994196
forward train acc: top1 ->  99.63199997558594 ; top5 ->  99.996  and loss:  1.2325574764981866
test acc: top1 ->  91.89 ; top5 ->  99.25  and loss:  43.63488933071494
forward train acc: top1 ->  99.76999997558593 ; top5 ->  100.0  and loss:  0.7567844989243895
test acc: top1 ->  91.94 ; top5 ->  99.27  and loss:  46.48127128556371
forward train acc: top1 ->  99.82 ; top5 ->  100.0  and loss:  0.6221013111062348
test acc: top1 ->  92.17 ; top5 ->  99.21  and loss:  48.34990634769201
==> this epoch:  7 / 512
---------------- start layer  10  ---------------
### skip layer  10 wait:  3  ###
---------------- start layer  11  ---------------
adv train loss:  -1539.8918828964233 , diff:  1539.8918828964233
adv train loss:  -1969.8935222625732 , diff:  430.0016393661499
adv train loss:  -1971.6526699066162 , diff:  1.7591476440429688
adv train loss:  -2114.234552383423 , diff:  142.58188247680664
adv train loss:  -2151.9468898773193 , diff:  37.712337493896484
adv train loss:  -2154.6007976531982 , diff:  2.6539077758789062
adv train loss:  -2156.355577468872 , diff:  1.7547798156738281
adv train loss:  -2154.6636180877686 , diff:  1.6919593811035156
adv train loss:  -2154.5782432556152 , diff:  0.08537483215332031
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  1
test acc: top1 ->  10.0 ; top5 ->  41.04  and loss:  11633.397567749023
forward train acc: top1 ->  36.951999978027345 ; top5 ->  79.45199998046876  and loss:  256.67489659786224
test acc: top1 ->  27.22 ; top5 ->  83.85  and loss:  288.8466889858246
forward train acc: top1 ->  84.98200001464843 ; top5 ->  99.784  and loss:  59.53116151690483
test acc: top1 ->  85.61 ; top5 ->  98.41  and loss:  60.21614721417427
forward train acc: top1 ->  97.63199998291016 ; top5 ->  99.992  and loss:  16.149877779185772
test acc: top1 ->  89.16 ; top5 ->  98.63  and loss:  48.506727024912834
forward train acc: top1 ->  98.766 ; top5 ->  99.996  and loss:  7.393339071422815
test acc: top1 ->  89.77 ; top5 ->  98.63  and loss:  49.24553580582142
forward train acc: top1 ->  99.1440000024414 ; top5 ->  99.996  and loss:  4.7774816527962685
test acc: top1 ->  90.3 ; top5 ->  98.7  and loss:  49.76061475276947
forward train acc: top1 ->  99.32799997558594 ; top5 ->  99.996  and loss:  3.7649949938058853
test acc: top1 ->  90.49 ; top5 ->  98.68  and loss:  50.50874535739422
forward train acc: top1 ->  99.3460000024414 ; top5 ->  99.996  and loss:  3.2510791029781103
test acc: top1 ->  90.54 ; top5 ->  98.7  and loss:  50.92997835576534
forward train acc: top1 ->  99.454 ; top5 ->  100.0  and loss:  2.858653023838997
test acc: top1 ->  90.65 ; top5 ->  98.73  and loss:  51.47866556048393
forward train acc: top1 ->  99.4840000024414 ; top5 ->  100.0  and loss:  2.5449068900197744
test acc: top1 ->  90.8 ; top5 ->  98.72  and loss:  52.219370134174824
forward train acc: top1 ->  99.54800000244141 ; top5 ->  100.0  and loss:  2.2471065875142813
test acc: top1 ->  90.91 ; top5 ->  98.77  and loss:  52.80745121836662
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  2 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -1978.2661685943604 , diff:  1978.2661685943604
adv train loss:  -1976.960973739624 , diff:  1.3051948547363281
adv train loss:  -1977.9826068878174 , diff:  1.0216331481933594
layer  12  adv train finish, try to retain  1
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  7354.508720397949
forward train acc: top1 ->  19.721999997558594 ; top5 ->  68.75799998291015  and loss:  1080.3923916816711
test acc: top1 ->  31.05 ; top5 ->  69.91  and loss:  470.71659231185913
forward train acc: top1 ->  50.76399999511719 ; top5 ->  76.03199998046875  and loss:  252.65867149829865
test acc: top1 ->  47.35 ; top5 ->  86.72  and loss:  178.46190679073334
forward train acc: top1 ->  59.68999999511719 ; top5 ->  94.2700000024414  and loss:  115.13356178998947
test acc: top1 ->  59.81 ; top5 ->  95.24  and loss:  128.92869520187378
forward train acc: top1 ->  67.71400001220704 ; top5 ->  99.74400000244141  and loss:  95.92573434114456
test acc: top1 ->  64.92 ; top5 ->  95.62  and loss:  125.69393938779831
forward train acc: top1 ->  72.00399998535157 ; top5 ->  99.84000000488281  and loss:  90.96742874383926
test acc: top1 ->  65.55 ; top5 ->  95.98  and loss:  124.0253381729126
forward train acc: top1 ->  74.64200001464843 ; top5 ->  99.89  and loss:  87.74905490875244
test acc: top1 ->  67.16 ; top5 ->  96.13  and loss:  123.11856979131699
forward train acc: top1 ->  76.11399997314453 ; top5 ->  99.90599997558594  and loss:  85.91414481401443
test acc: top1 ->  70.82 ; top5 ->  96.14  and loss:  122.68251276016235
forward train acc: top1 ->  76.76600000976562 ; top5 ->  99.914  and loss:  84.0982375741005
test acc: top1 ->  69.14 ; top5 ->  96.22  and loss:  122.11781150102615
forward train acc: top1 ->  77.95999998535156 ; top5 ->  99.924  and loss:  82.06104356050491
test acc: top1 ->  68.77 ; top5 ->  96.23  and loss:  121.50526839494705
forward train acc: top1 ->  79.00599999023437 ; top5 ->  99.92  and loss:  80.04331511259079
test acc: top1 ->  68.65 ; top5 ->  96.24  and loss:  120.75697642564774
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  46 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -8994.0414352417 , diff:  8994.0414352417
adv train loss:  -13229.864311218262 , diff:  4235.8228759765625
adv train loss:  -18003.174713134766 , diff:  4773.310401916504
adv train loss:  -23925.358428955078 , diff:  5922.1837158203125
adv train loss:  -29850.435516357422 , diff:  5925.077087402344
adv train loss:  -35765.85739135742 , diff:  5915.421875
adv train loss:  -41500.266662597656 , diff:  5734.409271240234
adv train loss:  -47123.40020751953 , diff:  5623.133544921875
adv train loss:  -52831.17687988281 , diff:  5707.776672363281
adv train loss:  -58367.132263183594 , diff:  5535.955383300781
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  8
test acc: top1 ->  19.47 ; top5 ->  71.82  and loss:  1649.9682216644287
forward train acc: top1 ->  66.85200001220703 ; top5 ->  95.674  and loss:  275.1866488158703
test acc: top1 ->  85.95 ; top5 ->  98.41  and loss:  70.12142005562782
forward train acc: top1 ->  97.94799998779297 ; top5 ->  99.996  and loss:  14.495586812496185
test acc: top1 ->  87.9 ; top5 ->  98.57  and loss:  62.68486988544464
forward train acc: top1 ->  98.72200000732421 ; top5 ->  100.0  and loss:  9.624805592000484
test acc: top1 ->  88.33 ; top5 ->  98.55  and loss:  60.952696159482
forward train acc: top1 ->  98.87999997558593 ; top5 ->  99.996  and loss:  7.619080435484648
test acc: top1 ->  88.72 ; top5 ->  98.54  and loss:  60.055912747979164
forward train acc: top1 ->  99.13999997802735 ; top5 ->  99.998  and loss:  5.8402267172932625
test acc: top1 ->  89.04 ; top5 ->  98.63  and loss:  59.230290189385414
forward train acc: top1 ->  99.24399997558594 ; top5 ->  99.998  and loss:  4.947557117789984
test acc: top1 ->  89.24 ; top5 ->  98.63  and loss:  59.13725186884403
forward train acc: top1 ->  99.2540000024414 ; top5 ->  100.0  and loss:  4.694319473579526
test acc: top1 ->  89.41 ; top5 ->  98.75  and loss:  59.09793169796467
forward train acc: top1 ->  99.36799997558593 ; top5 ->  100.0  and loss:  4.0324643068015575
test acc: top1 ->  89.6 ; top5 ->  98.74  and loss:  58.805858731269836
forward train acc: top1 ->  99.45399997802734 ; top5 ->  99.996  and loss:  3.6452004946768284
test acc: top1 ->  89.7 ; top5 ->  98.68  and loss:  58.9365669041872
forward train acc: top1 ->  99.47999997802734 ; top5 ->  100.0  and loss:  3.358031013980508
test acc: top1 ->  89.92 ; top5 ->  98.8  and loss:  58.748872354626656
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  9 / 512 , inc:  1
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.39453125  ==>  101 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.072265625  ==>  37 / 512 , inc:  1
layer  9  :  0.013671875  ==>  7 / 512 , inc:  2
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.08984375  ==>  46 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [1.4028426830023524, 3.7409138213396065, 1.4028426830023524, 1.4028426830023524, 1.4028426830023524, 1.4028426830023524, 1.4028426830023524, 0.9352284553349016, 1.2469712737798688, 1.6626283650398253, 1.0521320122517643, 0.9352284553349016, 1.4028426830023524, 19.9515403804779]  wait [3, 3, 3, 3, 3, 3, 2, 2, 3, 0, 2, 4, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]  tol: 8
$$$$$$$$$$$$$ epoch  71  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
### skip layer  1 wait:  3  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
adv train loss:  -452.8702368736267 , diff:  452.8702368736267
adv train loss:  -867.14883852005 , diff:  414.27860164642334
adv train loss:  -2233.8582334518433 , diff:  1366.7093949317932
adv train loss:  -3092.969701766968 , diff:  859.1114683151245
adv train loss:  -3248.5479583740234 , diff:  155.57825660705566
adv train loss:  -3352.7240657806396 , diff:  104.17610740661621
adv train loss:  -3382.854766845703 , diff:  30.130701065063477
adv train loss:  -3387.819049835205 , diff:  4.964282989501953
adv train loss:  -3441.6766204833984 , diff:  53.85757064819336
adv train loss:  -3442.173027038574 , diff:  0.49640655517578125
layer  6  adv train finish, try to retain  28
test acc: top1 ->  30.47 ; top5 ->  93.55  and loss:  726.9813203811646
forward train acc: top1 ->  87.62799998779298 ; top5 ->  99.40399997558593  and loss:  68.00337042659521
test acc: top1 ->  86.49 ; top5 ->  98.51  and loss:  98.64346328377724
forward train acc: top1 ->  96.22400001464844 ; top5 ->  99.896  and loss:  14.9573556445539
test acc: top1 ->  87.58 ; top5 ->  98.73  and loss:  85.74125647544861
forward train acc: top1 ->  97.05800001708984 ; top5 ->  99.92  and loss:  11.135616358369589
test acc: top1 ->  88.08 ; top5 ->  98.77  and loss:  82.37206998467445
forward train acc: top1 ->  97.63999998291015 ; top5 ->  99.922  and loss:  9.04960087314248
test acc: top1 ->  88.57 ; top5 ->  98.94  and loss:  77.78067368268967
forward train acc: top1 ->  97.89199998046875 ; top5 ->  99.954  and loss:  7.57190415635705
test acc: top1 ->  88.66 ; top5 ->  98.87  and loss:  75.30376036465168
forward train acc: top1 ->  98.03600001464844 ; top5 ->  99.964  and loss:  6.929562037810683
test acc: top1 ->  88.96 ; top5 ->  98.9  and loss:  73.47024105489254
forward train acc: top1 ->  98.10999998535156 ; top5 ->  99.966  and loss:  6.507771786302328
test acc: top1 ->  89.0 ; top5 ->  98.86  and loss:  72.6126510053873
forward train acc: top1 ->  98.25200000976562 ; top5 ->  99.964  and loss:  6.263447437435389
test acc: top1 ->  89.08 ; top5 ->  99.01  and loss:  71.0203898102045
forward train acc: top1 ->  98.31199998046876 ; top5 ->  99.952  and loss:  5.987090028822422
test acc: top1 ->  89.23 ; top5 ->  99.0  and loss:  68.67463101446629
forward train acc: top1 ->  98.35399998046876 ; top5 ->  99.966  and loss:  5.642039410769939
test acc: top1 ->  89.45 ; top5 ->  99.06  and loss:  67.14717230200768
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  101 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -1.9354915376752615 , diff:  1.9354915376752615
adv train loss:  -2.257932302542031 , diff:  0.3224407648667693
adv train loss:  -2.3441591328009963 , diff:  0.08622683025896549
adv train loss:  -2.2570845475420356 , diff:  0.08707458525896072
adv train loss:  -2.360760017298162 , diff:  0.1036754697561264
adv train loss:  -2.15795514639467 , diff:  0.20280487090349197
adv train loss:  -2.202014386188239 , diff:  0.04405923979356885
adv train loss:  -2.1112173283472657 , diff:  0.09079705784097314
adv train loss:  -2.124242434743792 , diff:  0.013025106396526098
adv train loss:  -2.3014155589044094 , diff:  0.1771731241606176
layer  7  adv train finish, try to retain  246
test acc: top1 ->  59.15 ; top5 ->  95.47  and loss:  412.9524714946747
forward train acc: top1 ->  99.06799997558593 ; top5 ->  99.98  and loss:  3.2563905008137226
test acc: top1 ->  91.05 ; top5 ->  99.05  and loss:  59.457677125930786
forward train acc: top1 ->  99.64399997558594 ; top5 ->  100.0  and loss:  1.0906889699399471
test acc: top1 ->  91.26 ; top5 ->  99.17  and loss:  58.72323615849018
forward train acc: top1 ->  99.754 ; top5 ->  100.0  and loss:  0.7197736571542919
test acc: top1 ->  91.19 ; top5 ->  99.2  and loss:  61.68423506617546
forward train acc: top1 ->  99.75399997558594 ; top5 ->  100.0  and loss:  0.6811585719697177
test acc: top1 ->  91.23 ; top5 ->  99.17  and loss:  63.88377292454243
forward train acc: top1 ->  99.802 ; top5 ->  100.0  and loss:  0.5550986581947654
test acc: top1 ->  91.49 ; top5 ->  99.27  and loss:  62.81892576813698
forward train acc: top1 ->  99.846 ; top5 ->  99.998  and loss:  0.5013709168415517
test acc: top1 ->  91.43 ; top5 ->  99.25  and loss:  62.31429176777601
forward train acc: top1 ->  99.856 ; top5 ->  99.998  and loss:  0.4122724437620491
test acc: top1 ->  91.5 ; top5 ->  99.29  and loss:  62.501277916133404
forward train acc: top1 ->  99.868 ; top5 ->  100.0  and loss:  0.4270461031701416
test acc: top1 ->  91.46 ; top5 ->  99.28  and loss:  62.2089923620224
forward train acc: top1 ->  99.876 ; top5 ->  100.0  and loss:  0.34944848192390054
test acc: top1 ->  91.36 ; top5 ->  99.34  and loss:  63.7884728461504
forward train acc: top1 ->  99.8680000024414 ; top5 ->  100.0  and loss:  0.41993434983305633
test acc: top1 ->  91.57 ; top5 ->  99.24  and loss:  63.20829599350691
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  253 / 512 , inc:  1
---------------- start layer  8  ---------------
### skip layer  8 wait:  3  ###
---------------- start layer  9  ---------------
adv train loss:  -81.42093527317047 , diff:  81.42093527317047
adv train loss:  -81.09208023548126 , diff:  0.328855037689209
adv train loss:  -82.17405027151108 , diff:  1.0819700360298157
adv train loss:  -81.72413882613182 , diff:  0.4499114453792572
adv train loss:  -80.50796812772751 , diff:  1.2161706984043121
adv train loss:  -80.46313774585724 , diff:  0.044830381870269775
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  5
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  50970.11544799805
forward train acc: top1 ->  80.75599998779298 ; top5 ->  94.88799997558594  and loss:  116.78216119110584
test acc: top1 ->  78.68 ; top5 ->  97.73  and loss:  86.8375526368618
forward train acc: top1 ->  98.60799998046875 ; top5 ->  99.98  and loss:  6.306713696569204
test acc: top1 ->  89.6 ; top5 ->  98.39  and loss:  55.04591931402683
forward train acc: top1 ->  99.08400000488281 ; top5 ->  99.988  and loss:  3.808736078441143
test acc: top1 ->  90.27 ; top5 ->  98.55  and loss:  54.80231653153896
forward train acc: top1 ->  99.36000000244141 ; top5 ->  99.994  and loss:  2.652461128309369
test acc: top1 ->  90.42 ; top5 ->  98.56  and loss:  55.44492273032665
forward train acc: top1 ->  99.51000000488281 ; top5 ->  99.996  and loss:  1.888609680812806
test acc: top1 ->  90.62 ; top5 ->  98.59  and loss:  56.5212939158082
forward train acc: top1 ->  99.56799997802734 ; top5 ->  100.0  and loss:  1.6106487065553665
test acc: top1 ->  90.77 ; top5 ->  98.59  and loss:  56.73927181214094
forward train acc: top1 ->  99.58399997802735 ; top5 ->  100.0  and loss:  1.4724066876806319
test acc: top1 ->  90.91 ; top5 ->  98.64  and loss:  56.636744529008865
forward train acc: top1 ->  99.64199997558593 ; top5 ->  100.0  and loss:  1.329942600801587
test acc: top1 ->  91.1 ; top5 ->  98.61  and loss:  56.78093405067921
forward train acc: top1 ->  99.722 ; top5 ->  100.0  and loss:  1.0697020695079118
test acc: top1 ->  91.06 ; top5 ->  98.65  and loss:  56.917446970939636
forward train acc: top1 ->  99.726 ; top5 ->  99.996  and loss:  1.059331321157515
test acc: top1 ->  91.05 ; top5 ->  98.66  and loss:  57.26256676763296
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  7 / 512 , inc:  2
---------------- start layer  10  ---------------
adv train loss:  -893.0806970596313 , diff:  893.0806970596313
adv train loss:  -892.5493516921997 , diff:  0.5313453674316406
layer  10  adv train finish, try to retain  494
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
### skip layer  11 wait:  4  ###
---------------- start layer  12  ---------------
adv train loss:  -2226.3103466033936 , diff:  2226.3103466033936
adv train loss:  -2223.696044921875 , diff:  2.6143016815185547
adv train loss:  -2225.5861320495605 , diff:  1.8900871276855469
adv train loss:  -2227.185987472534 , diff:  1.5998554229736328
adv train loss:  -2226.2094497680664 , diff:  0.9765377044677734
adv train loss:  -2223.8211002349854 , diff:  2.3883495330810547
adv train loss:  -2225.3255882263184 , diff:  1.5044879913330078
adv train loss:  -2226.8668174743652 , diff:  1.541229248046875
adv train loss:  -2227.364637374878 , diff:  0.4978199005126953
layer  12  adv train finish, try to retain  2
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  2197.8144884109497
forward train acc: top1 ->  43.68399999511719 ; top5 ->  85.9640000048828  and loss:  525.5910971164703
test acc: top1 ->  72.3 ; top5 ->  96.97  and loss:  119.60379725694656
forward train acc: top1 ->  91.17600001708985 ; top5 ->  99.90799997558594  and loss:  52.15170639753342
test acc: top1 ->  86.68 ; top5 ->  97.97  and loss:  76.74046170711517
forward train acc: top1 ->  97.89200000488282 ; top5 ->  100.0  and loss:  34.296190083026886
test acc: top1 ->  87.55 ; top5 ->  98.08  and loss:  69.0658133327961
forward train acc: top1 ->  98.50799997802734 ; top5 ->  99.992  and loss:  24.822417810559273
test acc: top1 ->  87.94 ; top5 ->  98.1  and loss:  64.35145327448845
forward train acc: top1 ->  98.65999997558593 ; top5 ->  99.99  and loss:  18.5208543241024
test acc: top1 ->  88.26 ; top5 ->  98.07  and loss:  62.60906258225441
forward train acc: top1 ->  98.82400000488282 ; top5 ->  99.996  and loss:  15.165887475013733
test acc: top1 ->  88.37 ; top5 ->  98.08  and loss:  62.47323575615883
forward train acc: top1 ->  98.88999997802735 ; top5 ->  99.99  and loss:  13.340559728443623
test acc: top1 ->  88.49 ; top5 ->  98.11  and loss:  62.3625553548336
forward train acc: top1 ->  99.00999998046875 ; top5 ->  99.996  and loss:  11.72419174015522
test acc: top1 ->  88.56 ; top5 ->  98.09  and loss:  62.62686014175415
forward train acc: top1 ->  98.99600000488282 ; top5 ->  99.998  and loss:  10.773639656603336
test acc: top1 ->  88.57 ; top5 ->  98.06  and loss:  63.10033339262009
forward train acc: top1 ->  99.1380000024414 ; top5 ->  100.0  and loss:  9.344850607216358
test acc: top1 ->  88.67 ; top5 ->  98.03  and loss:  63.71702241897583
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  46 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -6111.148662567139 , diff:  6111.148662567139
adv train loss:  -9412.949485778809 , diff:  3301.80082321167
adv train loss:  -12389.843872070312 , diff:  2976.894386291504
adv train loss:  -15360.143829345703 , diff:  2970.2999572753906
adv train loss:  -18358.042739868164 , diff:  2997.898910522461
adv train loss:  -21489.703353881836 , diff:  3131.660614013672
adv train loss:  -24845.1505279541 , diff:  3355.4471740722656
adv train loss:  -28371.905487060547 , diff:  3526.7549591064453
adv train loss:  -32110.930389404297 , diff:  3739.02490234375
adv train loss:  -36352.4013671875 , diff:  4241.470977783203
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  8
test acc: top1 ->  19.91 ; top5 ->  59.79  and loss:  2679.136754989624
forward train acc: top1 ->  27.668000003662108 ; top5 ->  69.19000001708984  and loss:  1193.18847322464
test acc: top1 ->  37.45 ; top5 ->  73.22  and loss:  566.3429870605469
forward train acc: top1 ->  49.185999975585936 ; top5 ->  82.56800001953125  and loss:  275.92331743240356
test acc: top1 ->  63.5 ; top5 ->  89.13  and loss:  154.02674913406372
forward train acc: top1 ->  78.09799998046876 ; top5 ->  95.376  and loss:  86.51522678136826
test acc: top1 ->  77.02 ; top5 ->  98.64  and loss:  83.6028464436531
forward train acc: top1 ->  89.7339999975586 ; top5 ->  99.986  and loss:  54.230233281850815
test acc: top1 ->  80.02 ; top5 ->  98.64  and loss:  73.94266206026077
forward train acc: top1 ->  93.72999998779297 ; top5 ->  99.992  and loss:  42.923969864845276
test acc: top1 ->  86.02 ; top5 ->  98.62  and loss:  66.36598774790764
forward train acc: top1 ->  96.61000000732422 ; top5 ->  99.996  and loss:  35.195202112197876
test acc: top1 ->  87.26 ; top5 ->  98.53  and loss:  62.55567362904549
forward train acc: top1 ->  97.97999998046875 ; top5 ->  99.994  and loss:  30.100072264671326
test acc: top1 ->  87.93 ; top5 ->  98.57  and loss:  59.804604947566986
forward train acc: top1 ->  98.50200000488282 ; top5 ->  100.0  and loss:  24.938737332820892
test acc: top1 ->  88.5 ; top5 ->  98.51  and loss:  56.0766199529171
forward train acc: top1 ->  98.88599997802734 ; top5 ->  100.0  and loss:  20.79796315729618
test acc: top1 ->  88.95 ; top5 ->  98.51  and loss:  54.317715018987656
forward train acc: top1 ->  98.996 ; top5 ->  100.0  and loss:  17.701197862625122
test acc: top1 ->  89.2 ; top5 ->  98.49  and loss:  52.49735328555107
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  9 / 512 , inc:  1
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.39453125  ==>  101 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.072265625  ==>  37 / 512 , inc:  1
layer  9  :  0.013671875  ==>  7 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.08984375  ==>  46 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [1.4028426830023524, 3.7409138213396065, 1.4028426830023524, 1.4028426830023524, 1.4028426830023524, 1.4028426830023524, 1.0521320122517643, 0.7014213415011762, 1.2469712737798688, 1.2469712737798688, 2.1042640245035287, 0.9352284553349016, 1.0521320122517643, 14.963655285358426]  wait [2, 2, 2, 2, 2, 2, 4, 4, 2, 2, 2, 3, 4, 4]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 8
$$$$$$$$$$$$$ epoch  72  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -2476.535972595215 , diff:  2476.535972595215
adv train loss:  -2551.8549671173096 , diff:  75.31899452209473
adv train loss:  -2579.6674938201904 , diff:  27.81252670288086
adv train loss:  -2589.996171951294 , diff:  10.328678131103516
adv train loss:  -2585.96022605896 , diff:  4.035945892333984
adv train loss:  -2586.6978130340576 , diff:  0.7375869750976562
layer  0  adv train finish, try to retain  5
test acc: top1 ->  10.4 ; top5 ->  50.0  and loss:  2770.5068035125732
forward train acc: top1 ->  39.46599999511719 ; top5 ->  85.1059999975586  and loss:  728.8664383888245
test acc: top1 ->  35.53 ; top5 ->  74.71  and loss:  465.56548261642456
forward train acc: top1 ->  54.245999990234374 ; top5 ->  89.84200000976563  and loss:  215.16651022434235
test acc: top1 ->  58.77 ; top5 ->  91.85  and loss:  164.8165946006775
forward train acc: top1 ->  57.366000001220705 ; top5 ->  91.26799998291015  and loss:  154.66302490234375
test acc: top1 ->  60.91 ; top5 ->  92.99  and loss:  136.29447650909424
forward train acc: top1 ->  60.29000000732422 ; top5 ->  92.50800000488282  and loss:  131.57849645614624
test acc: top1 ->  63.66 ; top5 ->  93.99  and loss:  120.5776070356369
forward train acc: top1 ->  62.990000002441406 ; top5 ->  93.35199997070312  and loss:  119.39734089374542
test acc: top1 ->  65.59 ; top5 ->  94.33  and loss:  112.27658253908157
forward train acc: top1 ->  64.96599999267578 ; top5 ->  93.87599997070312  and loss:  112.28660571575165
test acc: top1 ->  66.68 ; top5 ->  94.65  and loss:  107.25946867465973
forward train acc: top1 ->  65.77600001708984 ; top5 ->  94.3140000024414  and loss:  108.06467372179031
test acc: top1 ->  68.04 ; top5 ->  95.04  and loss:  102.83466297388077
forward train acc: top1 ->  66.8879999963379 ; top5 ->  94.6859999975586  and loss:  105.01248800754547
test acc: top1 ->  68.77 ; top5 ->  95.2  and loss:  99.80285584926605
forward train acc: top1 ->  67.79199999755859 ; top5 ->  94.99400001220702  and loss:  100.37158447504044
test acc: top1 ->  69.56 ; top5 ->  95.48  and loss:  97.09684163331985
forward train acc: top1 ->  68.80399998535157 ; top5 ->  95.23199999511719  and loss:  96.97994083166122
test acc: top1 ->  70.33 ; top5 ->  95.71  and loss:  94.50923550128937
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -18.935486316680908 , diff:  18.935486316680908
adv train loss:  -19.013343170285225 , diff:  0.07785685360431671
adv train loss:  -19.001042932271957 , diff:  0.012300238013267517
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  43
test acc: top1 ->  10.01 ; top5 ->  51.67  and loss:  420.32838582992554
forward train acc: top1 ->  99.54799997558594 ; top5 ->  99.994  and loss:  3.4470820110291243
test acc: top1 ->  91.67 ; top5 ->  99.0  and loss:  40.05338864400983
forward train acc: top1 ->  99.818 ; top5 ->  100.0  and loss:  0.7758676360826939
test acc: top1 ->  91.82 ; top5 ->  98.94  and loss:  44.15218026936054
forward train acc: top1 ->  99.858 ; top5 ->  100.0  and loss:  0.5142134819179773
test acc: top1 ->  91.83 ; top5 ->  98.98  and loss:  47.21792896836996
forward train acc: top1 ->  99.896 ; top5 ->  100.0  and loss:  0.4002863799687475
test acc: top1 ->  91.91 ; top5 ->  98.96  and loss:  48.80800776928663
forward train acc: top1 ->  99.86199997558593 ; top5 ->  99.998  and loss:  0.44489946216344833
test acc: top1 ->  91.94 ; top5 ->  98.99  and loss:  48.08936145529151
forward train acc: top1 ->  99.9 ; top5 ->  100.0  and loss:  0.29479452676605433
test acc: top1 ->  92.04 ; top5 ->  98.93  and loss:  49.51269472017884
forward train acc: top1 ->  99.9240000024414 ; top5 ->  100.0  and loss:  0.2807066598907113
test acc: top1 ->  91.96 ; top5 ->  98.94  and loss:  50.744291603565216
forward train acc: top1 ->  99.942 ; top5 ->  100.0  and loss:  0.2539900146657601
test acc: top1 ->  92.23 ; top5 ->  98.95  and loss:  50.792433589696884
==> this epoch:  43 / 64
---------------- start layer  2  ---------------
adv train loss:  -4.814230189658701 , diff:  4.814230189658701
adv train loss:  -11.549431581050158 , diff:  6.735201391391456
adv train loss:  -11.42784414254129 , diff:  0.12158743850886822
adv train loss:  -12.056070875376463 , diff:  0.6282267328351736
adv train loss:  -11.281718980520964 , diff:  0.7743518948554993
adv train loss:  -11.83364599570632 , diff:  0.5519270151853561
adv train loss:  -15.241235472261906 , diff:  3.407589476555586
adv train loss:  -24.9551465138793 , diff:  9.713911041617393
adv train loss:  -28.776851251721382 , diff:  3.821704737842083
adv train loss:  -37.5034119784832 , diff:  8.726560726761818
layer  2  adv train finish, try to retain  92
test acc: top1 ->  23.65 ; top5 ->  63.02  and loss:  861.4143586158752
forward train acc: top1 ->  99.7460000024414 ; top5 ->  99.998  and loss:  0.7568077817559242
test acc: top1 ->  91.57 ; top5 ->  98.84  and loss:  56.07603173702955
forward train acc: top1 ->  99.78999997558594 ; top5 ->  99.998  and loss:  0.7007674188353121
test acc: top1 ->  91.66 ; top5 ->  99.08  and loss:  53.24017708003521
forward train acc: top1 ->  99.77199997558594 ; top5 ->  100.0  and loss:  0.6502890107221901
test acc: top1 ->  91.63 ; top5 ->  98.96  and loss:  52.1187536790967
forward train acc: top1 ->  99.83199997558594 ; top5 ->  99.998  and loss:  0.5247047515586019
test acc: top1 ->  91.63 ; top5 ->  99.05  and loss:  57.52480421215296
forward train acc: top1 ->  99.83999997558594 ; top5 ->  99.998  and loss:  0.4926237082108855
test acc: top1 ->  91.82 ; top5 ->  98.97  and loss:  55.01290659606457
forward train acc: top1 ->  99.86 ; top5 ->  100.0  and loss:  0.41264393940218724
test acc: top1 ->  91.9 ; top5 ->  99.05  and loss:  54.34433427453041
forward train acc: top1 ->  99.87599997558594 ; top5 ->  100.0  and loss:  0.3616076670587063
test acc: top1 ->  91.66 ; top5 ->  99.08  and loss:  53.318484500050545
forward train acc: top1 ->  99.852 ; top5 ->  100.0  and loss:  0.45927306107478216
test acc: top1 ->  91.85 ; top5 ->  99.11  and loss:  53.35053027421236
forward train acc: top1 ->  99.846 ; top5 ->  100.0  and loss:  0.4351905930088833
test acc: top1 ->  91.73 ; top5 ->  99.0  and loss:  54.49869794398546
forward train acc: top1 ->  99.86999997558594 ; top5 ->  100.0  and loss:  0.39015684416517615
test acc: top1 ->  91.77 ; top5 ->  99.1  and loss:  55.17049816250801
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -766.8690096200444 , diff:  766.8690096200444
adv train loss:  -1329.4634447097778 , diff:  562.5944350897335
adv train loss:  -1314.9321327209473 , diff:  14.531311988830566
adv train loss:  -1297.2868633270264 , diff:  17.6452693939209
adv train loss:  -1289.6305618286133 , diff:  7.656301498413086
adv train loss:  -1289.3377704620361 , diff:  0.29279136657714844
adv train loss:  -1304.2827854156494 , diff:  14.945014953613281
adv train loss:  -1308.956389427185 , diff:  4.6736040115356445
adv train loss:  -1317.3603801727295 , diff:  8.403990745544434
adv train loss:  -1301.7808017730713 , diff:  15.579578399658203
layer  3  adv train finish, try to retain  52
test acc: top1 ->  23.96 ; top5 ->  67.53  and loss:  874.0224890708923
forward train acc: top1 ->  94.64599999267578 ; top5 ->  99.734  and loss:  19.394669823348522
test acc: top1 ->  87.58 ; top5 ->  98.85  and loss:  50.97470508515835
forward train acc: top1 ->  95.66600001708984 ; top5 ->  99.874  and loss:  13.272009007632732
test acc: top1 ->  87.77 ; top5 ->  99.0  and loss:  48.803571701049805
forward train acc: top1 ->  96.22799998535156 ; top5 ->  99.87  and loss:  11.916448794305325
test acc: top1 ->  88.41 ; top5 ->  99.15  and loss:  46.338245287537575
forward train acc: top1 ->  96.43400001953125 ; top5 ->  99.902  and loss:  10.884252239018679
test acc: top1 ->  88.53 ; top5 ->  99.08  and loss:  45.80416630208492
forward train acc: top1 ->  96.60799998779297 ; top5 ->  99.91599997558593  and loss:  10.234192363917828
test acc: top1 ->  88.74 ; top5 ->  98.88  and loss:  46.02083267271519
forward train acc: top1 ->  96.94599999267578 ; top5 ->  99.924  and loss:  9.269489172846079
test acc: top1 ->  88.73 ; top5 ->  99.13  and loss:  44.66315017640591
forward train acc: top1 ->  96.98800000732422 ; top5 ->  99.93999997558593  and loss:  8.854789230972528
test acc: top1 ->  88.98 ; top5 ->  99.08  and loss:  44.7929250895977
forward train acc: top1 ->  97.20799998535156 ; top5 ->  99.938  and loss:  8.479032333940268
test acc: top1 ->  88.91 ; top5 ->  99.1  and loss:  45.645886927843094
forward train acc: top1 ->  97.13599998535156 ; top5 ->  99.952  and loss:  8.525268822908401
test acc: top1 ->  89.08 ; top5 ->  99.11  and loss:  44.946629256010056
forward train acc: top1 ->  97.42599998535157 ; top5 ->  99.95199997558593  and loss:  7.861274477094412
test acc: top1 ->  89.21 ; top5 ->  99.16  and loss:  44.606858775019646
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -181.19226682931185 , diff:  181.19226682931185
adv train loss:  -971.1404485702515 , diff:  789.9481817409396
adv train loss:  -1094.650918006897 , diff:  123.51046943664551
adv train loss:  -1101.1004524230957 , diff:  6.4495344161987305
adv train loss:  -1103.9120321273804 , diff:  2.811579704284668
adv train loss:  -1127.5702533721924 , diff:  23.65822124481201
adv train loss:  -1203.1553220748901 , diff:  75.58506870269775
adv train loss:  -1208.2313175201416 , diff:  5.075995445251465
adv train loss:  -1205.573950767517 , diff:  2.6573667526245117
adv train loss:  -1211.0714197158813 , diff:  5.497468948364258
layer  4  adv train finish, try to retain  48
test acc: top1 ->  33.57 ; top5 ->  86.92  and loss:  640.7545471191406
forward train acc: top1 ->  86.82599997314453 ; top5 ->  99.17  and loss:  41.19886291027069
test acc: top1 ->  83.11 ; top5 ->  98.65  and loss:  54.955042719841
forward train acc: top1 ->  88.78600001464844 ; top5 ->  99.41599997802734  and loss:  33.44188693165779
test acc: top1 ->  84.16 ; top5 ->  98.85  and loss:  51.735104978084564
forward train acc: top1 ->  89.81000001953124 ; top5 ->  99.54999997802734  and loss:  30.33875174820423
test acc: top1 ->  85.13 ; top5 ->  98.82  and loss:  49.789011776447296
forward train acc: top1 ->  90.59800000244141 ; top5 ->  99.604  and loss:  27.756611168384552
test acc: top1 ->  85.4 ; top5 ->  98.94  and loss:  48.175698444247246
forward train acc: top1 ->  91.15399998291015 ; top5 ->  99.60999997558594  and loss:  26.26978436112404
test acc: top1 ->  85.76 ; top5 ->  98.9  and loss:  47.58201685547829
forward train acc: top1 ->  91.56799999023437 ; top5 ->  99.622  and loss:  25.004087284207344
test acc: top1 ->  85.77 ; top5 ->  99.04  and loss:  47.29919423162937
forward train acc: top1 ->  91.52399998535157 ; top5 ->  99.686  and loss:  24.987596452236176
test acc: top1 ->  85.74 ; top5 ->  99.11  and loss:  46.4561251103878
forward train acc: top1 ->  91.79200000732422 ; top5 ->  99.68  and loss:  24.14927040040493
test acc: top1 ->  85.91 ; top5 ->  99.08  and loss:  46.273853093385696
forward train acc: top1 ->  91.8360000048828 ; top5 ->  99.67999997802734  and loss:  23.69228169322014
test acc: top1 ->  86.04 ; top5 ->  99.04  and loss:  46.84448625147343
forward train acc: top1 ->  92.08000000488282 ; top5 ->  99.69599997558593  and loss:  23.07809703052044
test acc: top1 ->  86.07 ; top5 ->  99.04  and loss:  45.842492043972015
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -238.78255758807063 , diff:  238.78255758807063
adv train loss:  -1002.5574111938477 , diff:  763.774853605777
adv train loss:  -1050.1358728408813 , diff:  47.57846164703369
adv train loss:  -1057.0628681182861 , diff:  6.926995277404785
adv train loss:  -1075.3204097747803 , diff:  18.25754165649414
adv train loss:  -1089.8167963027954 , diff:  14.496386528015137
adv train loss:  -1092.959379196167 , diff:  3.142582893371582
adv train loss:  -1096.2969398498535 , diff:  3.3375606536865234
adv train loss:  -1095.7394914627075 , diff:  0.5574483871459961
adv train loss:  -1104.2368116378784 , diff:  8.497320175170898
layer  5  adv train finish, try to retain  74
test acc: top1 ->  47.67 ; top5 ->  82.52  and loss:  365.81908535957336
forward train acc: top1 ->  95.22200000976562 ; top5 ->  99.862  and loss:  14.328934788703918
test acc: top1 ->  88.48 ; top5 ->  99.18  and loss:  41.416210412979126
forward train acc: top1 ->  96.79400001464843 ; top5 ->  99.928  and loss:  9.430545166134834
test acc: top1 ->  89.27 ; top5 ->  99.21  and loss:  40.6541381329298
forward train acc: top1 ->  97.30200000732422 ; top5 ->  99.95999997558594  and loss:  7.8752865474671125
test acc: top1 ->  89.66 ; top5 ->  99.29  and loss:  40.78120146691799
forward train acc: top1 ->  97.68800000976563 ; top5 ->  99.97  and loss:  6.684611221775413
test acc: top1 ->  89.65 ; top5 ->  99.32  and loss:  41.67434945702553
forward train acc: top1 ->  97.85799998046875 ; top5 ->  99.972  and loss:  6.141324941068888
test acc: top1 ->  90.07 ; top5 ->  99.34  and loss:  42.346580758690834
forward train acc: top1 ->  97.99399998291015 ; top5 ->  99.98  and loss:  5.780660891905427
test acc: top1 ->  90.11 ; top5 ->  99.37  and loss:  41.94032387435436
forward train acc: top1 ->  98.09400000488282 ; top5 ->  99.978  and loss:  5.526774629950523
test acc: top1 ->  89.94 ; top5 ->  99.29  and loss:  42.68017493188381
forward train acc: top1 ->  98.20399997802734 ; top5 ->  99.98  and loss:  5.153700089082122
test acc: top1 ->  90.19 ; top5 ->  99.34  and loss:  41.707655385136604
forward train acc: top1 ->  98.18999998779297 ; top5 ->  99.984  and loss:  5.192782558500767
test acc: top1 ->  90.21 ; top5 ->  99.39  and loss:  41.98626546561718
forward train acc: top1 ->  98.29800001220703 ; top5 ->  99.99  and loss:  4.890225380659103
test acc: top1 ->  90.35 ; top5 ->  99.29  and loss:  42.09243892878294
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -316.60631509963423 , diff:  316.60631509963423
adv train loss:  -1393.9242153167725 , diff:  1077.3179002171382
adv train loss:  -1497.5456008911133 , diff:  103.62138557434082
adv train loss:  -1510.0381164550781 , diff:  12.492515563964844
adv train loss:  -1510.159483909607 , diff:  0.1213674545288086
adv train loss:  -1554.877926826477 , diff:  44.71844291687012
adv train loss:  -1559.2800378799438 , diff:  4.402111053466797
adv train loss:  -1552.9475708007812 , diff:  6.332467079162598
adv train loss:  -1550.9861316680908 , diff:  1.9614391326904297
adv train loss:  -1554.553861618042 , diff:  3.567729949951172
layer  6  adv train finish, try to retain  244
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -1.392181122675538 , diff:  1.392181122675538
adv train loss:  -1.356321819126606 , diff:  0.035859303548932076
adv train loss:  -1.3633548496291041 , diff:  0.00703303050249815
layer  7  adv train finish, try to retain  259
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -24.318508446216583 , diff:  24.318508446216583
adv train loss:  -24.41406136751175 , diff:  0.09555292129516602
adv train loss:  -24.45011107623577 , diff:  0.03604970872402191
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  36
test acc: top1 ->  10.77 ; top5 ->  50.05  and loss:  6121533.45703125
forward train acc: top1 ->  94.37999997802734 ; top5 ->  99.82  and loss:  23.839511077851057
test acc: top1 ->  90.92 ; top5 ->  98.97  and loss:  39.56706517934799
forward train acc: top1 ->  99.4760000048828 ; top5 ->  100.0  and loss:  2.791909011080861
test acc: top1 ->  91.39 ; top5 ->  99.04  and loss:  39.37197281420231
forward train acc: top1 ->  99.63399997802735 ; top5 ->  99.994  and loss:  1.6257154559716582
test acc: top1 ->  91.58 ; top5 ->  99.13  and loss:  40.71382901445031
forward train acc: top1 ->  99.80400000244141 ; top5 ->  100.0  and loss:  1.0103306081146002
test acc: top1 ->  91.76 ; top5 ->  99.09  and loss:  41.47279601171613
forward train acc: top1 ->  99.816 ; top5 ->  100.0  and loss:  0.7699502403847873
test acc: top1 ->  91.76 ; top5 ->  99.13  and loss:  43.09723472222686
forward train acc: top1 ->  99.84599997558594 ; top5 ->  100.0  and loss:  0.6866500591859221
test acc: top1 ->  91.73 ; top5 ->  99.12  and loss:  43.589937545359135
forward train acc: top1 ->  99.90999997558593 ; top5 ->  100.0  and loss:  0.5127858868800104
test acc: top1 ->  91.78 ; top5 ->  99.18  and loss:  44.135877922177315
forward train acc: top1 ->  99.862 ; top5 ->  100.0  and loss:  0.552073354832828
test acc: top1 ->  91.83 ; top5 ->  99.16  and loss:  44.33157990127802
forward train acc: top1 ->  99.906 ; top5 ->  100.0  and loss:  0.4521840256638825
test acc: top1 ->  91.76 ; top5 ->  99.18  and loss:  45.08261377364397
forward train acc: top1 ->  99.89799997558593 ; top5 ->  100.0  and loss:  0.42655217181891203
test acc: top1 ->  91.75 ; top5 ->  99.18  and loss:  45.57990965247154
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  37 / 512 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -85.39633804559708 , diff:  85.39633804559708
adv train loss:  -120.31040662527084 , diff:  34.91406857967377
adv train loss:  -120.01484566926956 , diff:  0.29556095600128174
adv train loss:  -120.88185328245163 , diff:  0.8670076131820679
adv train loss:  -120.5199356675148 , diff:  0.3619176149368286
adv train loss:  -120.96875178813934 , diff:  0.44881612062454224
adv train loss:  -120.65847909450531 , diff:  0.3102726936340332
adv train loss:  -120.72908198833466 , diff:  0.0706028938293457
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  6
test acc: top1 ->  10.0 ; top5 ->  45.86  and loss:  24099.962326049805
forward train acc: top1 ->  95.504 ; top5 ->  99.89  and loss:  17.73553726822138
test acc: top1 ->  84.76 ; top5 ->  98.43  and loss:  81.45173847675323
forward train acc: top1 ->  99.71 ; top5 ->  100.0  and loss:  1.4329066313803196
test acc: top1 ->  91.5 ; top5 ->  98.98  and loss:  46.89379035681486
forward train acc: top1 ->  99.83399997558594 ; top5 ->  100.0  and loss:  0.8285224423743784
test acc: top1 ->  91.7 ; top5 ->  99.0  and loss:  47.004115022718906
forward train acc: top1 ->  99.896 ; top5 ->  99.998  and loss:  0.6208714758977294
test acc: top1 ->  91.81 ; top5 ->  99.11  and loss:  46.82586448639631
forward train acc: top1 ->  99.88799997558594 ; top5 ->  100.0  and loss:  0.5257706884294748
test acc: top1 ->  91.96 ; top5 ->  99.15  and loss:  46.93924067914486
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.45075753214769065
test acc: top1 ->  91.93 ; top5 ->  99.13  and loss:  47.33315435796976
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.38346898928284645
test acc: top1 ->  91.94 ; top5 ->  99.12  and loss:  47.39326590299606
forward train acc: top1 ->  99.93399997558593 ; top5 ->  100.0  and loss:  0.37139718467369676
test acc: top1 ->  91.94 ; top5 ->  99.07  and loss:  48.03878841549158
forward train acc: top1 ->  99.916 ; top5 ->  100.0  and loss:  0.3747321435948834
test acc: top1 ->  91.98 ; top5 ->  99.1  and loss:  47.663985170423985
forward train acc: top1 ->  99.93599997558594 ; top5 ->  100.0  and loss:  0.3110431428067386
test acc: top1 ->  91.96 ; top5 ->  99.13  and loss:  48.51505712419748
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  7 / 512 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -185.68245267868042 , diff:  185.68245267868042
adv train loss:  -185.05725991725922 , diff:  0.6251927614212036
adv train loss:  -185.27024936676025 , diff:  0.2129894495010376
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  10
test acc: top1 ->  10.0 ; top5 ->  54.06  and loss:  122877.271484375
forward train acc: top1 ->  78.57999997802735 ; top5 ->  98.658  and loss:  81.68367549031973
test acc: top1 ->  32.66 ; top5 ->  96.33  and loss:  200.77696990966797
forward train acc: top1 ->  99.29400000244141 ; top5 ->  99.996  and loss:  6.333128854632378
test acc: top1 ->  90.55 ; top5 ->  98.9  and loss:  42.47480824589729
forward train acc: top1 ->  99.61 ; top5 ->  100.0  and loss:  2.9782474637031555
test acc: top1 ->  91.13 ; top5 ->  98.98  and loss:  43.62933946400881
forward train acc: top1 ->  99.72 ; top5 ->  99.998  and loss:  1.9420885080471635
test acc: top1 ->  91.38 ; top5 ->  99.06  and loss:  44.44995451718569
forward train acc: top1 ->  99.816 ; top5 ->  100.0  and loss:  1.3450176371261477
test acc: top1 ->  91.45 ; top5 ->  98.97  and loss:  46.266360603272915
forward train acc: top1 ->  99.79599997558594 ; top5 ->  100.0  and loss:  1.1189931435510516
test acc: top1 ->  91.58 ; top5 ->  98.95  and loss:  46.88919869065285
forward train acc: top1 ->  99.84 ; top5 ->  99.998  and loss:  1.0006776321679354
test acc: top1 ->  91.48 ; top5 ->  99.04  and loss:  47.561361357569695
forward train acc: top1 ->  99.83399997558594 ; top5 ->  99.998  and loss:  0.9592619929462671
test acc: top1 ->  91.49 ; top5 ->  98.97  and loss:  47.809670589864254
forward train acc: top1 ->  99.87 ; top5 ->  100.0  and loss:  0.8275921093299985
test acc: top1 ->  91.48 ; top5 ->  98.98  and loss:  49.128641456365585
forward train acc: top1 ->  99.86 ; top5 ->  100.0  and loss:  0.7640488860197365
test acc: top1 ->  91.59 ; top5 ->  98.89  and loss:  49.55898480117321
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  11 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -359.79028058052063 , diff:  359.79028058052063
adv train loss:  -361.3692135810852 , diff:  1.5789330005645752
adv train loss:  -360.80771374702454 , diff:  0.561499834060669
layer  11  adv train finish, try to retain  461
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -2557.865942001343 , diff:  2557.865942001343
adv train loss:  -2559.092330932617 , diff:  1.226388931274414
layer  12  adv train finish, try to retain  470
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -6834.906677246094 , diff:  6834.906677246094
adv train loss:  -10589.256156921387 , diff:  3754.349479675293
adv train loss:  -14239.832405090332 , diff:  3650.5762481689453
adv train loss:  -17902.93226623535 , diff:  3663.0998611450195
adv train loss:  -21589.541442871094 , diff:  3686.609176635742
adv train loss:  -25301.606536865234 , diff:  3712.0650939941406
adv train loss:  -29006.45294189453 , diff:  3704.846405029297
adv train loss:  -32735.254608154297 , diff:  3728.8016662597656
adv train loss:  -36462.86688232422 , diff:  3727.612274169922
adv train loss:  -40194.95651245117 , diff:  3732.089630126953
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  8
test acc: top1 ->  19.94 ; top5 ->  60.91  and loss:  1148.9426488876343
forward train acc: top1 ->  71.72199998046875 ; top5 ->  92.038  and loss:  207.09896618127823
test acc: top1 ->  89.16 ; top5 ->  98.39  and loss:  51.74490249156952
forward train acc: top1 ->  99.16199997802734 ; top5 ->  99.996  and loss:  8.851749289780855
test acc: top1 ->  90.16 ; top5 ->  98.32  and loss:  45.83466702699661
forward train acc: top1 ->  99.54800000244141 ; top5 ->  99.998  and loss:  4.974265865981579
test acc: top1 ->  90.64 ; top5 ->  98.43  and loss:  45.82114601135254
forward train acc: top1 ->  99.692 ; top5 ->  99.998  and loss:  3.471947267651558
test acc: top1 ->  90.82 ; top5 ->  98.48  and loss:  45.66314011067152
forward train acc: top1 ->  99.7220000024414 ; top5 ->  99.996  and loss:  2.78009682148695
test acc: top1 ->  90.77 ; top5 ->  98.46  and loss:  46.3853624612093
forward train acc: top1 ->  99.80399997558594 ; top5 ->  100.0  and loss:  2.2365590687841177
test acc: top1 ->  90.91 ; top5 ->  98.5  and loss:  46.09854929149151
forward train acc: top1 ->  99.796 ; top5 ->  100.0  and loss:  2.0831664092838764
test acc: top1 ->  90.94 ; top5 ->  98.47  and loss:  46.83605982363224
forward train acc: top1 ->  99.80799997558594 ; top5 ->  99.998  and loss:  1.9146763989701867
test acc: top1 ->  90.96 ; top5 ->  98.55  and loss:  47.08591914921999
forward train acc: top1 ->  99.794 ; top5 ->  99.998  and loss:  1.7748096277937293
test acc: top1 ->  90.97 ; top5 ->  98.59  and loss:  47.52911738306284
forward train acc: top1 ->  99.83599997558593 ; top5 ->  100.0  and loss:  1.6054370775818825
test acc: top1 ->  91.0 ; top5 ->  98.56  and loss:  48.25980280339718
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  9 / 512 , inc:  1
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.671875  ==>  43 / 64 , inc:  2
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.39453125  ==>  101 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.072265625  ==>  37 / 512 , inc:  1
layer  9  :  0.013671875  ==>  7 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.08984375  ==>  46 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [1.0521320122517643, 3.7409138213396065, 1.0521320122517643, 1.0521320122517643, 1.0521320122517643, 1.0521320122517643, 2.1042640245035287, 1.4028426830023524, 0.9352284553349016, 0.9352284553349016, 1.5781980183776465, 1.8704569106698032, 2.1042640245035287, 11.222741464018819]  wait [2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 4]  inc [1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 9
$$$$$$$$$$$$$ epoch  73  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -980.3928409814835 , diff:  980.3928409814835
adv train loss:  -1010.3102941513062 , diff:  29.917453169822693
adv train loss:  -1084.5806407928467 , diff:  74.27034664154053
adv train loss:  -1094.8899030685425 , diff:  10.3092622756958
adv train loss:  -1097.1272687911987 , diff:  2.23736572265625
adv train loss:  -1099.0779008865356 , diff:  1.950632095336914
adv train loss:  -1095.9250764846802 , diff:  3.1528244018554688
adv train loss:  -1098.4976873397827 , diff:  2.572610855102539
adv train loss:  -1095.0763549804688 , diff:  3.421332359313965
adv train loss:  -1098.4997749328613 , diff:  3.423419952392578
layer  0  adv train finish, try to retain  62
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -56.196171790361404 , diff:  56.196171790361404
adv train loss:  -56.31471684575081 , diff:  0.1185450553894043
adv train loss:  -56.81780529022217 , diff:  0.5030884444713593
adv train loss:  -56.184697568416595 , diff:  0.6331077218055725
adv train loss:  -57.0046226978302 , diff:  0.8199251294136047
adv train loss:  -55.957061529159546 , diff:  1.0475611686706543
adv train loss:  -56.828562557697296 , diff:  0.8715010285377502
adv train loss:  -55.08254587650299 , diff:  1.7460166811943054
adv train loss:  -56.948368310928345 , diff:  1.865822434425354
adv train loss:  -56.42617094516754 , diff:  0.5221973657608032
layer  1  adv train finish, try to retain  43
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -64.48219037055969 , diff:  64.48219037055969
adv train loss:  -85.36976820230484 , diff:  20.887577831745148
adv train loss:  -110.7122557759285 , diff:  25.342487573623657
adv train loss:  -116.39488142728806 , diff:  5.682625651359558
adv train loss:  -141.15527629852295 , diff:  24.760394871234894
adv train loss:  -151.10372757911682 , diff:  9.948451280593872
adv train loss:  -152.93644678592682 , diff:  1.8327192068099976
adv train loss:  -151.446697473526 , diff:  1.4897493124008179
adv train loss:  -150.2279942035675 , diff:  1.218703269958496
adv train loss:  -151.93630278110504 , diff:  1.7083085775375366
layer  2  adv train finish, try to retain  123
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -912.5772603154182 , diff:  912.5772603154182
adv train loss:  -1519.5777044296265 , diff:  607.0004441142082
adv train loss:  -1521.4990854263306 , diff:  1.9213809967041016
adv train loss:  -1521.733811378479 , diff:  0.2347259521484375
adv train loss:  -1530.8090105056763 , diff:  9.075199127197266
adv train loss:  -1523.9667568206787 , diff:  6.842253684997559
adv train loss:  -1533.5364513397217 , diff:  9.569694519042969
adv train loss:  -1537.6792974472046 , diff:  4.14284610748291
adv train loss:  -1526.3028593063354 , diff:  11.37643814086914
adv train loss:  -1533.821231842041 , diff:  7.518372535705566
layer  3  adv train finish, try to retain  121
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -255.28685089945793 , diff:  255.28685089945793
adv train loss:  -1408.4580144882202 , diff:  1153.1711635887623
adv train loss:  -1783.4210834503174 , diff:  374.96306896209717
adv train loss:  -1833.783483505249 , diff:  50.36240005493164
adv train loss:  -1866.4603958129883 , diff:  32.67691230773926
adv train loss:  -1872.0020141601562 , diff:  5.541618347167969
adv train loss:  -1878.195749282837 , diff:  6.193735122680664
adv train loss:  -1916.0575904846191 , diff:  37.86184120178223
adv train loss:  -1927.606517791748 , diff:  11.548927307128906
adv train loss:  -1928.9776439666748 , diff:  1.3711261749267578
layer  4  adv train finish, try to retain  210
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -190.99413201212883 , diff:  190.99413201212883
adv train loss:  -1515.9361824989319 , diff:  1324.942050486803
adv train loss:  -2277.962001800537 , diff:  762.0258193016052
adv train loss:  -2313.520975112915 , diff:  35.55897331237793
adv train loss:  -2327.419382095337 , diff:  13.898406982421875
adv train loss:  -2337.4716186523438 , diff:  10.052236557006836
adv train loss:  -2357.1985397338867 , diff:  19.72692108154297
adv train loss:  -2355.347116470337 , diff:  1.8514232635498047
adv train loss:  -2374.3183917999268 , diff:  18.971275329589844
adv train loss:  -2388.8436431884766 , diff:  14.525251388549805
layer  5  adv train finish, try to retain  224
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -107.98334050178528 , diff:  107.98334050178528
adv train loss:  -1431.6421308517456 , diff:  1323.6587903499603
adv train loss:  -1968.2777423858643 , diff:  536.6356115341187
adv train loss:  -2214.4239826202393 , diff:  246.146240234375
adv train loss:  -2344.50919342041 , diff:  130.0852108001709
adv train loss:  -2367.672914505005 , diff:  23.163721084594727
adv train loss:  -2395.4795722961426 , diff:  27.806657791137695
adv train loss:  -2414.4208221435547 , diff:  18.94124984741211
adv train loss:  -2411.251226425171 , diff:  3.169595718383789
adv train loss:  -2409.5143127441406 , diff:  1.7369136810302734
layer  6  adv train finish, try to retain  3
test acc: top1 ->  9.9 ; top5 ->  50.11  and loss:  710.5515098571777
forward train acc: top1 ->  33.81400000732422 ; top5 ->  82.86199998046875  and loss:  332.6629339456558
test acc: top1 ->  42.13 ; top5 ->  90.94  and loss:  173.5737625360489
forward train acc: top1 ->  47.67199999633789 ; top5 ->  93.278  and loss:  139.57039046287537
test acc: top1 ->  52.46 ; top5 ->  94.63  and loss:  133.70972502231598
forward train acc: top1 ->  54.6100000024414 ; top5 ->  95.74800001464844  and loss:  118.46590971946716
test acc: top1 ->  56.62 ; top5 ->  95.62  and loss:  118.19949358701706
forward train acc: top1 ->  59.04199999145508 ; top5 ->  96.75400001464844  and loss:  104.40561479330063
test acc: top1 ->  60.64 ; top5 ->  96.3  and loss:  107.77937996387482
forward train acc: top1 ->  62.91400000366211 ; top5 ->  97.43599998535156  and loss:  94.48832827806473
test acc: top1 ->  62.44 ; top5 ->  96.81  and loss:  100.93734699487686
forward train acc: top1 ->  65.55399999511718 ; top5 ->  97.84599998535157  and loss:  87.95162934064865
test acc: top1 ->  64.81 ; top5 ->  97.06  and loss:  96.86497831344604
forward train acc: top1 ->  66.4459999987793 ; top5 ->  98.13200001220703  and loss:  84.21601390838623
test acc: top1 ->  65.31 ; top5 ->  97.38  and loss:  94.69570034742355
forward train acc: top1 ->  68.12199997802735 ; top5 ->  98.21800000976563  and loss:  81.03081274032593
test acc: top1 ->  66.59 ; top5 ->  97.39  and loss:  91.27153772115707
forward train acc: top1 ->  69.12599999023438 ; top5 ->  98.33399998291016  and loss:  78.14253425598145
test acc: top1 ->  67.83 ; top5 ->  97.53  and loss:  89.38025850057602
forward train acc: top1 ->  70.28399998535156 ; top5 ->  98.45199997558593  and loss:  75.5664826631546
test acc: top1 ->  68.24 ; top5 ->  97.59  and loss:  86.71940958499908
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  101 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -39.13914731144905 , diff:  39.13914731144905
adv train loss:  -39.04199734330177 , diff:  0.09714996814727783
adv train loss:  -41.156044483184814 , diff:  2.1140471398830414
adv train loss:  -52.46656334400177 , diff:  11.310518860816956
adv train loss:  -56.7679249048233 , diff:  4.301361560821533
adv train loss:  -56.547621071338654 , diff:  0.22030383348464966
adv train loss:  -57.46478119492531 , diff:  0.9171601235866547
adv train loss:  -64.60359007120132 , diff:  7.138808876276016
adv train loss:  -66.93857038021088 , diff:  2.334980309009552
adv train loss:  -66.5028504729271 , diff:  0.43571990728378296
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  252
test acc: top1 ->  18.63 ; top5 ->  56.57  and loss:  7933257.4140625
forward train acc: top1 ->  98.76599997802734 ; top5 ->  99.986  and loss:  4.441374904476106
test acc: top1 ->  90.71 ; top5 ->  98.86  and loss:  46.71175582706928
forward train acc: top1 ->  99.67799997558593 ; top5 ->  100.0  and loss:  1.1460882718674839
test acc: top1 ->  91.25 ; top5 ->  99.04  and loss:  48.36620780080557
forward train acc: top1 ->  99.78199997558593 ; top5 ->  100.0  and loss:  0.7840936854481697
test acc: top1 ->  91.4 ; top5 ->  99.0  and loss:  49.358177699148655
forward train acc: top1 ->  99.78600000244141 ; top5 ->  100.0  and loss:  0.6455869432538748
test acc: top1 ->  91.41 ; top5 ->  98.97  and loss:  51.44106785207987
forward train acc: top1 ->  99.808 ; top5 ->  99.998  and loss:  0.5766705279238522
test acc: top1 ->  91.33 ; top5 ->  98.98  and loss:  53.494095377624035
forward train acc: top1 ->  99.874 ; top5 ->  100.0  and loss:  0.4083670759573579
test acc: top1 ->  91.55 ; top5 ->  99.03  and loss:  51.718495815992355
forward train acc: top1 ->  99.866 ; top5 ->  100.0  and loss:  0.3973140639718622
test acc: top1 ->  91.52 ; top5 ->  98.92  and loss:  52.93187315762043
forward train acc: top1 ->  99.9 ; top5 ->  100.0  and loss:  0.34181843866826966
test acc: top1 ->  91.66 ; top5 ->  99.0  and loss:  52.67661890015006
forward train acc: top1 ->  99.90399997558593 ; top5 ->  100.0  and loss:  0.3083794917911291
test acc: top1 ->  91.55 ; top5 ->  99.05  and loss:  53.9981459453702
forward train acc: top1 ->  99.886 ; top5 ->  100.0  and loss:  0.3477944200858474
test acc: top1 ->  91.65 ; top5 ->  98.9  and loss:  53.96484376490116
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  253 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -174.88052606582642 , diff:  174.88052606582642
adv train loss:  -173.55334210395813 , diff:  1.3271839618682861
adv train loss:  -173.73908388614655 , diff:  0.18574178218841553
layer  8  adv train finish, try to retain  437
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -38.35569083690643 , diff:  38.35569083690643
adv train loss:  -39.21543422341347 , diff:  0.8597433865070343
adv train loss:  -39.40775437653065 , diff:  0.19232015311717987
adv train loss:  -38.4501364082098 , diff:  0.9576179683208466
adv train loss:  -38.3700220733881 , diff:  0.08011433482170105
adv train loss:  -38.311877861618996 , diff:  0.058144211769104004
layer  9  adv train finish, try to retain  494
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -415.6201901435852 , diff:  415.6201901435852
adv train loss:  -415.6105282306671 , diff:  0.00966191291809082
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  10
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  232004.5948486328
forward train acc: top1 ->  85.59400001220703 ; top5 ->  97.696  and loss:  71.55580249428749
test acc: top1 ->  61.19 ; top5 ->  95.17  and loss:  155.5053078532219
forward train acc: top1 ->  99.0900000024414 ; top5 ->  99.996  and loss:  5.443104485049844
test acc: top1 ->  90.01 ; top5 ->  97.61  and loss:  56.81095327436924
forward train acc: top1 ->  99.46599997802734 ; top5 ->  99.998  and loss:  2.9727435540407896
test acc: top1 ->  90.36 ; top5 ->  97.75  and loss:  57.83396378904581
forward train acc: top1 ->  99.68 ; top5 ->  99.998  and loss:  1.7504391809925437
test acc: top1 ->  90.87 ; top5 ->  98.17  and loss:  60.08113922178745
forward train acc: top1 ->  99.73399997802734 ; top5 ->  100.0  and loss:  1.2766140345484018
test acc: top1 ->  90.99 ; top5 ->  98.28  and loss:  59.07449995726347
forward train acc: top1 ->  99.76200000244141 ; top5 ->  100.0  and loss:  1.1163428528234363
test acc: top1 ->  91.09 ; top5 ->  98.36  and loss:  58.28140813857317
forward train acc: top1 ->  99.81 ; top5 ->  100.0  and loss:  0.9461434963159263
test acc: top1 ->  91.1 ; top5 ->  98.44  and loss:  58.80709823966026
forward train acc: top1 ->  99.82799997558594 ; top5 ->  100.0  and loss:  0.8250032011419535
test acc: top1 ->  91.06 ; top5 ->  98.43  and loss:  60.3483060747385
forward train acc: top1 ->  99.838 ; top5 ->  100.0  and loss:  0.7844388531520963
test acc: top1 ->  91.04 ; top5 ->  98.44  and loss:  61.227598056197166
forward train acc: top1 ->  99.838 ; top5 ->  99.998  and loss:  0.6968625793233514
test acc: top1 ->  91.29 ; top5 ->  98.54  and loss:  60.00984928756952
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  11 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -1692.1720390319824 , diff:  1692.1720390319824
adv train loss:  -1689.4013118743896 , diff:  2.7707271575927734
adv train loss:  -1689.9767036437988 , diff:  0.5753917694091797
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  1
test acc: top1 ->  10.0 ; top5 ->  58.77  and loss:  12688.983833312988
forward train acc: top1 ->  84.61799997802734 ; top5 ->  98.468  and loss:  68.83271366357803
test acc: top1 ->  69.29 ; top5 ->  98.48  and loss:  91.37372589111328
forward train acc: top1 ->  99.13199997802734 ; top5 ->  100.0  and loss:  6.73042306676507
test acc: top1 ->  90.19 ; top5 ->  98.76  and loss:  46.45014961063862
forward train acc: top1 ->  99.49199997802734 ; top5 ->  100.0  and loss:  3.493041805922985
test acc: top1 ->  90.29 ; top5 ->  98.75  and loss:  48.35293582081795
forward train acc: top1 ->  99.61 ; top5 ->  99.996  and loss:  2.3791225207969546
test acc: top1 ->  90.44 ; top5 ->  98.7  and loss:  48.98229740560055
forward train acc: top1 ->  99.6740000024414 ; top5 ->  99.996  and loss:  1.835730125196278
test acc: top1 ->  90.46 ; top5 ->  98.75  and loss:  50.75807964801788
forward train acc: top1 ->  99.724 ; top5 ->  100.0  and loss:  1.5422822451218963
test acc: top1 ->  90.57 ; top5 ->  98.76  and loss:  50.79057948291302
forward train acc: top1 ->  99.76 ; top5 ->  100.0  and loss:  1.300606613047421
test acc: top1 ->  90.66 ; top5 ->  98.67  and loss:  51.96664255857468
forward train acc: top1 ->  99.77 ; top5 ->  100.0  and loss:  1.1823487053625286
test acc: top1 ->  90.66 ; top5 ->  98.75  and loss:  52.19551229476929
forward train acc: top1 ->  99.756 ; top5 ->  99.998  and loss:  1.1841258429922163
test acc: top1 ->  90.66 ; top5 ->  98.73  and loss:  53.181229919195175
forward train acc: top1 ->  99.808 ; top5 ->  99.998  and loss:  1.004689295310527
test acc: top1 ->  90.64 ; top5 ->  98.74  and loss:  54.4237135797739
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  2 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -4155.031742095947 , diff:  4155.031742095947
adv train loss:  -4154.852981567383 , diff:  0.17876052856445312
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  45
test acc: top1 ->  10.02 ; top5 ->  68.26  and loss:  41749.85467529297
forward train acc: top1 ->  97.108 ; top5 ->  100.0  and loss:  10.88962824526243
test acc: top1 ->  91.41 ; top5 ->  98.89  and loss:  63.013293996453285
forward train acc: top1 ->  99.904 ; top5 ->  100.0  and loss:  0.35691357066389173
test acc: top1 ->  91.58 ; top5 ->  98.95  and loss:  63.3011711537838
forward train acc: top1 ->  99.91799997558594 ; top5 ->  100.0  and loss:  0.26686080475337803
test acc: top1 ->  91.61 ; top5 ->  99.0  and loss:  62.44424842298031
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.20653230595053174
test acc: top1 ->  91.61 ; top5 ->  98.99  and loss:  63.537105441093445
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.15091366996057332
test acc: top1 ->  91.79 ; top5 ->  99.07  and loss:  62.90072735399008
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.13235905568581074
test acc: top1 ->  91.82 ; top5 ->  99.04  and loss:  64.19043882191181
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.13593146891798824
test acc: top1 ->  91.7 ; top5 ->  99.09  and loss:  63.96486888080835
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.16059675413998775
test acc: top1 ->  91.79 ; top5 ->  99.04  and loss:  63.39680712670088
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.1069770596222952
test acc: top1 ->  91.79 ; top5 ->  99.06  and loss:  63.85630142316222
forward train acc: top1 ->  99.94799997558594 ; top5 ->  100.0  and loss:  0.14956175908446312
test acc: top1 ->  91.74 ; top5 ->  99.09  and loss:  64.21971395611763
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  46 / 512 , inc:  1
---------------- start layer  13  ---------------
### skip layer  13 wait:  4  ###
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.671875  ==>  43 / 64 , inc:  2
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.39453125  ==>  101 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.072265625  ==>  37 / 512 , inc:  1
layer  9  :  0.013671875  ==>  7 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.08984375  ==>  46 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [2.1042640245035287, 7.481827642679213, 2.1042640245035287, 2.1042640245035287, 2.1042640245035287, 2.1042640245035287, 1.5781980183776465, 1.0521320122517643, 1.8704569106698032, 1.8704569106698032, 1.1836485137832349, 1.4028426830023524, 1.5781980183776465, 11.222741464018819]  wait [2, 0, 2, 2, 2, 2, 4, 4, 2, 2, 4, 3, 4, 3]  inc [1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 9
$$$$$$$$$$$$$ epoch  74  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1031.883732217364 , diff:  1031.883732217364
adv train loss:  -1193.2938394546509 , diff:  161.41010723728687
adv train loss:  -997.1804285049438 , diff:  196.11341094970703
adv train loss:  -995.2861661911011 , diff:  1.8942623138427734
adv train loss:  -1040.81240940094 , diff:  45.52624320983887
adv train loss:  -1054.8933010101318 , diff:  14.080891609191895
adv train loss:  -1048.0482501983643 , diff:  6.845050811767578
adv train loss:  -1058.1028089523315 , diff:  10.054558753967285
adv train loss:  -1053.9729452133179 , diff:  4.129863739013672
adv train loss:  -1060.0448112487793 , diff:  6.071866035461426
layer  0  adv train finish, try to retain  5
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  640.306875705719
forward train acc: top1 ->  39.310000008544925 ; top5 ->  84.05200000732422  and loss:  323.0597199201584
test acc: top1 ->  16.83 ; top5 ->  61.54  and loss:  340.1217679977417
forward train acc: top1 ->  45.734000004882816 ; top5 ->  87.73399998779297  and loss:  166.87089836597443
test acc: top1 ->  51.07 ; top5 ->  90.55  and loss:  149.0028339624405
forward train acc: top1 ->  50.02599999145508 ; top5 ->  89.90199999267578  and loss:  147.66262304782867
test acc: top1 ->  53.45 ; top5 ->  91.54  and loss:  138.81847774982452
forward train acc: top1 ->  53.31399998413086 ; top5 ->  91.15799997558594  and loss:  137.1804895401001
test acc: top1 ->  56.49 ; top5 ->  92.28  and loss:  130.9367196559906
forward train acc: top1 ->  56.04400000244141 ; top5 ->  92.25399999023438  and loss:  128.60506105422974
test acc: top1 ->  59.22 ; top5 ->  93.24  and loss:  122.55448585748672
forward train acc: top1 ->  58.345999984130856 ; top5 ->  92.90400000976562  and loss:  122.67840766906738
test acc: top1 ->  60.48 ; top5 ->  93.68  and loss:  119.16175079345703
forward train acc: top1 ->  59.68800000366211 ; top5 ->  93.41000001953125  and loss:  118.85634410381317
test acc: top1 ->  61.59 ; top5 ->  94.02  and loss:  115.74721384048462
forward train acc: top1 ->  61.14599999023437 ; top5 ->  93.70399997314453  and loss:  114.71663665771484
test acc: top1 ->  62.95 ; top5 ->  94.17  and loss:  112.52041029930115
forward train acc: top1 ->  62.25000000366211 ; top5 ->  94.01800001708985  and loss:  111.5958821773529
test acc: top1 ->  63.65 ; top5 ->  94.44  and loss:  109.41788601875305
forward train acc: top1 ->  63.40599998779297 ; top5 ->  94.40799999267578  and loss:  108.28772211074829
test acc: top1 ->  64.82 ; top5 ->  94.51  and loss:  107.20913201570511
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -23.56267662346363 , diff:  23.56267662346363
adv train loss:  -23.58100886642933 , diff:  0.018332242965698242
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  41
test acc: top1 ->  10.91 ; top5 ->  50.04  and loss:  389.695760011673
forward train acc: top1 ->  99.55999997558594 ; top5 ->  99.996  and loss:  3.589593656361103
test acc: top1 ->  91.52 ; top5 ->  99.13  and loss:  38.47727458178997
forward train acc: top1 ->  99.774 ; top5 ->  99.998  and loss:  0.7825518981553614
test acc: top1 ->  91.67 ; top5 ->  99.14  and loss:  43.77673997357488
forward train acc: top1 ->  99.818 ; top5 ->  100.0  and loss:  0.5697822815272957
test acc: top1 ->  91.74 ; top5 ->  99.06  and loss:  47.43949504196644
forward train acc: top1 ->  99.87 ; top5 ->  100.0  and loss:  0.41583419078961015
test acc: top1 ->  92.0 ; top5 ->  99.08  and loss:  49.11199086531997
forward train acc: top1 ->  99.888 ; top5 ->  100.0  and loss:  0.33103420166298747
test acc: top1 ->  91.91 ; top5 ->  99.06  and loss:  50.89949980378151
forward train acc: top1 ->  99.91399997558594 ; top5 ->  100.0  and loss:  0.275295726954937
test acc: top1 ->  91.96 ; top5 ->  99.13  and loss:  51.58825745433569
forward train acc: top1 ->  99.91000000244141 ; top5 ->  100.0  and loss:  0.3073225882835686
test acc: top1 ->  91.95 ; top5 ->  98.94  and loss:  52.2940418869257
forward train acc: top1 ->  99.904 ; top5 ->  99.998  and loss:  0.2821618015659624
test acc: top1 ->  92.02 ; top5 ->  99.07  and loss:  52.208769930526614
forward train acc: top1 ->  99.936 ; top5 ->  100.0  and loss:  0.2400529549922794
test acc: top1 ->  92.0 ; top5 ->  99.16  and loss:  54.10727855749428
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.19557913322933018
test acc: top1 ->  91.86 ; top5 ->  99.15  and loss:  54.55878794379532
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  43 / 64 , inc:  2
---------------- start layer  2  ---------------
adv train loss:  -2.0325976591557264 , diff:  2.0325976591557264
adv train loss:  -5.053239533677697 , diff:  3.0206418745219707
adv train loss:  -4.944495534524322 , diff:  0.10874399915337563
adv train loss:  -10.964678881689906 , diff:  6.020183347165585
adv train loss:  -11.453576639294624 , diff:  0.4888977576047182
adv train loss:  -11.362849466502666 , diff:  0.09072717279195786
adv train loss:  -10.990251112729311 , diff:  0.3725983537733555
adv train loss:  -15.769617915153503 , diff:  4.779366802424192
adv train loss:  -15.891148280352354 , diff:  0.12153036519885063
adv train loss:  -16.682905212044716 , diff:  0.7917569316923618
layer  2  adv train finish, try to retain  86
test acc: top1 ->  10.0 ; top5 ->  50.39  and loss:  898.266294002533
forward train acc: top1 ->  99.4980000024414 ; top5 ->  99.994  and loss:  1.581766875460744
test acc: top1 ->  91.39 ; top5 ->  99.04  and loss:  53.9685175716877
forward train acc: top1 ->  99.62399997558593 ; top5 ->  99.994  and loss:  1.1221064473502338
test acc: top1 ->  91.61 ; top5 ->  99.14  and loss:  49.608757603913546
forward train acc: top1 ->  99.67199997558593 ; top5 ->  99.998  and loss:  0.9091851999983191
test acc: top1 ->  91.35 ; top5 ->  99.21  and loss:  50.645962223410606
forward train acc: top1 ->  99.752 ; top5 ->  100.0  and loss:  0.7551192806567997
test acc: top1 ->  91.74 ; top5 ->  99.12  and loss:  50.045772060751915
forward train acc: top1 ->  99.796 ; top5 ->  100.0  and loss:  0.6066739381058142
test acc: top1 ->  91.48 ; top5 ->  99.24  and loss:  50.70099228620529
forward train acc: top1 ->  99.806 ; top5 ->  100.0  and loss:  0.5802033335785381
test acc: top1 ->  91.73 ; top5 ->  99.11  and loss:  51.81574013829231
forward train acc: top1 ->  99.802 ; top5 ->  99.996  and loss:  0.6346570157911628
test acc: top1 ->  91.79 ; top5 ->  99.18  and loss:  50.9114311337471
forward train acc: top1 ->  99.832 ; top5 ->  100.0  and loss:  0.4670391196850687
test acc: top1 ->  91.67 ; top5 ->  99.21  and loss:  52.57134302705526
forward train acc: top1 ->  99.81000000244141 ; top5 ->  100.0  and loss:  0.5185889177955687
test acc: top1 ->  91.68 ; top5 ->  99.18  and loss:  52.64977529644966
forward train acc: top1 ->  99.85199997558594 ; top5 ->  100.0  and loss:  0.4397107129916549
test acc: top1 ->  91.79 ; top5 ->  99.09  and loss:  52.65791595727205
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -643.622683792375 , diff:  643.622683792375
adv train loss:  -1087.139946937561 , diff:  443.517263145186
adv train loss:  -1100.226556777954 , diff:  13.086609840393066
adv train loss:  -1113.9161367416382 , diff:  13.689579963684082
adv train loss:  -1112.4607858657837 , diff:  1.4553508758544922
adv train loss:  -1117.2721910476685 , diff:  4.811405181884766
adv train loss:  -1114.6065015792847 , diff:  2.665689468383789
adv train loss:  -1107.3006372451782 , diff:  7.305864334106445
adv train loss:  -1120.6102457046509 , diff:  13.309608459472656
adv train loss:  -1109.3903312683105 , diff:  11.219914436340332
layer  3  adv train finish, try to retain  23
test acc: top1 ->  9.47 ; top5 ->  50.06  and loss:  827.0097246170044
forward train acc: top1 ->  76.08799998291016 ; top5 ->  97.37800000488281  and loss:  89.67217916250229
test acc: top1 ->  74.28 ; top5 ->  97.38  and loss:  80.07476916909218
forward train acc: top1 ->  79.3880000024414 ; top5 ->  98.10000000732421  and loss:  61.9975920021534
test acc: top1 ->  76.08 ; top5 ->  97.54  and loss:  74.06672576069832
forward train acc: top1 ->  80.81599999023437 ; top5 ->  98.32800000732422  and loss:  56.89284560084343
test acc: top1 ->  77.62 ; top5 ->  97.91  and loss:  68.9718097448349
forward train acc: top1 ->  82.22599997070313 ; top5 ->  98.57599997802734  and loss:  53.13435235619545
test acc: top1 ->  78.64 ; top5 ->  98.06  and loss:  65.95372930169106
forward train acc: top1 ->  83.22599999511719 ; top5 ->  98.78399998291016  and loss:  49.853918105363846
test acc: top1 ->  79.08 ; top5 ->  98.11  and loss:  64.43885430693626
forward train acc: top1 ->  83.80600000732422 ; top5 ->  98.88799997802734  and loss:  47.935806185007095
test acc: top1 ->  79.73 ; top5 ->  98.16  and loss:  62.780846893787384
forward train acc: top1 ->  84.18199998779296 ; top5 ->  98.92600000488281  and loss:  46.755614042282104
test acc: top1 ->  79.91 ; top5 ->  98.31  and loss:  62.194261968135834
forward train acc: top1 ->  84.41999999023437 ; top5 ->  98.99800000488281  and loss:  45.56461322307587
test acc: top1 ->  80.21 ; top5 ->  98.33  and loss:  60.744074791669846
forward train acc: top1 ->  85.00799998779297 ; top5 ->  99.02599997802734  and loss:  44.29006603360176
test acc: top1 ->  80.38 ; top5 ->  98.38  and loss:  60.697085082530975
forward train acc: top1 ->  85.09000000732422 ; top5 ->  99.0900000024414  and loss:  43.95434060692787
test acc: top1 ->  80.73 ; top5 ->  98.44  and loss:  59.5560282766819
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -50.92938544228673 , diff:  50.92938544228673
adv train loss:  -448.9129559993744 , diff:  397.98357055708766
adv train loss:  -567.6396112442017 , diff:  118.72665524482727
adv train loss:  -614.1057391166687 , diff:  46.46612787246704
adv train loss:  -631.1767911911011 , diff:  17.071052074432373
adv train loss:  -650.2114315032959 , diff:  19.034640312194824
adv train loss:  -687.7171611785889 , diff:  37.50572967529297
adv train loss:  -692.7881226539612 , diff:  5.0709614753723145
adv train loss:  -695.6231656074524 , diff:  2.835042953491211
adv train loss:  -696.0845680236816 , diff:  0.46140241622924805
layer  4  adv train finish, try to retain  21
test acc: top1 ->  10.14 ; top5 ->  50.8  and loss:  632.7370014190674
forward train acc: top1 ->  70.90400001708984 ; top5 ->  96.81999998046875  and loss:  86.72226613759995
test acc: top1 ->  71.5 ; top5 ->  97.08  and loss:  85.02915716171265
forward train acc: top1 ->  75.75800000732421 ; top5 ->  97.80400000488281  and loss:  70.9121984243393
test acc: top1 ->  74.16 ; top5 ->  97.65  and loss:  77.21088492870331
forward train acc: top1 ->  77.99599999511719 ; top5 ->  98.20999998291016  and loss:  63.90199327468872
test acc: top1 ->  76.02 ; top5 ->  97.81  and loss:  72.27877500653267
forward train acc: top1 ->  79.58599997802735 ; top5 ->  98.4540000024414  and loss:  59.55310261249542
test acc: top1 ->  76.95 ; top5 ->  98.04  and loss:  70.01545983552933
forward train acc: top1 ->  80.38399997802735 ; top5 ->  98.57400000244141  and loss:  56.96559879183769
test acc: top1 ->  77.83 ; top5 ->  98.07  and loss:  66.8794395327568
forward train acc: top1 ->  81.43000001220703 ; top5 ->  98.66799998291016  and loss:  54.314406365156174
test acc: top1 ->  78.59 ; top5 ->  98.18  and loss:  65.61000776290894
forward train acc: top1 ->  81.37200001953126 ; top5 ->  98.69400000488281  and loss:  53.78119596838951
test acc: top1 ->  78.81 ; top5 ->  98.06  and loss:  64.85177484154701
forward train acc: top1 ->  82.06600001220703 ; top5 ->  98.7580000024414  and loss:  52.21760219335556
test acc: top1 ->  79.1 ; top5 ->  98.17  and loss:  63.656678289175034
forward train acc: top1 ->  82.19599997802734 ; top5 ->  98.79400000244141  and loss:  51.561845898628235
test acc: top1 ->  79.23 ; top5 ->  98.3  and loss:  62.604225784540176
forward train acc: top1 ->  82.406 ; top5 ->  98.87999998046875  and loss:  50.733674108982086
test acc: top1 ->  79.57 ; top5 ->  98.39  and loss:  61.988786578178406
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -99.83880627900362 , diff:  99.83880627900362
adv train loss:  -474.6509499549866 , diff:  374.81214367598295
adv train loss:  -583.9175972938538 , diff:  109.26664733886719
adv train loss:  -738.0155429840088 , diff:  154.09794569015503
adv train loss:  -773.8180990219116 , diff:  35.80255603790283
adv train loss:  -782.5380811691284 , diff:  8.719982147216797
adv train loss:  -798.8824110031128 , diff:  16.344329833984375
adv train loss:  -802.3827967643738 , diff:  3.5003857612609863
adv train loss:  -803.0280537605286 , diff:  0.6452569961547852
adv train loss:  -820.206335067749 , diff:  17.17828130722046
layer  5  adv train finish, try to retain  15
test acc: top1 ->  10.04 ; top5 ->  50.79  and loss:  2133.0687866210938
forward train acc: top1 ->  73.27000001464843 ; top5 ->  97.43200000732422  and loss:  77.16458010673523
test acc: top1 ->  73.63 ; top5 ->  97.47  and loss:  79.36733967065811
forward train acc: top1 ->  78.87199997802735 ; top5 ->  98.68400000488282  and loss:  60.45232850313187
test acc: top1 ->  76.28 ; top5 ->  98.08  and loss:  72.32589107751846
forward train acc: top1 ->  81.37599999267579 ; top5 ->  98.9980000024414  and loss:  53.13634154200554
test acc: top1 ->  77.98 ; top5 ->  98.3  and loss:  67.7379706799984
forward train acc: top1 ->  83.10999997314453 ; top5 ->  99.0700000024414  and loss:  48.890710920095444
test acc: top1 ->  79.4 ; top5 ->  98.55  and loss:  63.29234054684639
forward train acc: top1 ->  84.26599999023438 ; top5 ->  99.18400000488282  and loss:  45.652146726846695
test acc: top1 ->  80.94 ; top5 ->  98.61  and loss:  59.89234936237335
forward train acc: top1 ->  84.89199997070313 ; top5 ->  99.29599998046875  and loss:  43.36579692363739
test acc: top1 ->  80.9 ; top5 ->  98.76  and loss:  59.81775361299515
forward train acc: top1 ->  85.22399997802735 ; top5 ->  99.32999997802735  and loss:  42.555304795503616
test acc: top1 ->  81.3 ; top5 ->  98.7  and loss:  59.092702865600586
forward train acc: top1 ->  85.82399997802735 ; top5 ->  99.3940000024414  and loss:  40.89567995071411
test acc: top1 ->  81.6 ; top5 ->  98.77  and loss:  57.537361055612564
forward train acc: top1 ->  86.02199999023438 ; top5 ->  99.332  and loss:  40.11375892162323
test acc: top1 ->  81.86 ; top5 ->  98.72  and loss:  57.82944744825363
forward train acc: top1 ->  86.376 ; top5 ->  99.40400000488282  and loss:  39.36990574002266
test acc: top1 ->  82.0 ; top5 ->  98.81  and loss:  56.58899721503258
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
adv train loss:  -84.34120148420334 , diff:  84.34120148420334
adv train loss:  -84.55457162857056 , diff:  0.21337014436721802
adv train loss:  -84.61367303133011 , diff:  0.059101402759552
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  36
test acc: top1 ->  22.03 ; top5 ->  59.55  and loss:  2375762.154296875
forward train acc: top1 ->  97.9300000024414 ; top5 ->  99.974  and loss:  9.4619593732059
test acc: top1 ->  91.36 ; top5 ->  99.19  and loss:  36.66671755164862
forward train acc: top1 ->  99.46 ; top5 ->  100.0  and loss:  1.8969538719393313
test acc: top1 ->  91.72 ; top5 ->  99.14  and loss:  40.03030329942703
forward train acc: top1 ->  99.70599997558594 ; top5 ->  99.998  and loss:  0.9839201187714934
test acc: top1 ->  91.62 ; top5 ->  99.19  and loss:  43.10381703451276
forward train acc: top1 ->  99.8 ; top5 ->  100.0  and loss:  0.7281685722991824
test acc: top1 ->  91.69 ; top5 ->  99.18  and loss:  45.281347151845694
forward train acc: top1 ->  99.834 ; top5 ->  100.0  and loss:  0.5992637156741694
test acc: top1 ->  91.78 ; top5 ->  99.19  and loss:  45.732494838535786
forward train acc: top1 ->  99.848 ; top5 ->  100.0  and loss:  0.5299419356742874
test acc: top1 ->  91.74 ; top5 ->  99.26  and loss:  46.752495739609
forward train acc: top1 ->  99.87 ; top5 ->  100.0  and loss:  0.476340968394652
test acc: top1 ->  91.67 ; top5 ->  99.2  and loss:  47.175000846385956
forward train acc: top1 ->  99.882 ; top5 ->  100.0  and loss:  0.3990895007736981
test acc: top1 ->  91.82 ; top5 ->  99.21  and loss:  46.97621737793088
forward train acc: top1 ->  99.88599997558593 ; top5 ->  100.0  and loss:  0.39562535379081964
test acc: top1 ->  91.91 ; top5 ->  99.26  and loss:  47.496579606086016
forward train acc: top1 ->  99.874 ; top5 ->  100.0  and loss:  0.3893789874855429
test acc: top1 ->  91.82 ; top5 ->  99.23  and loss:  47.95427031069994
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  37 / 512 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -1670.0670413970947 , diff:  1670.0670413970947
adv train loss:  -1674.0444765090942 , diff:  3.9774351119995117
adv train loss:  -1680.1786136627197 , diff:  6.134137153625488
adv train loss:  -1683.9287786483765 , diff:  3.7501649856567383
adv train loss:  -1681.67311668396 , diff:  2.255661964416504
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  6
test acc: top1 ->  11.78 ; top5 ->  56.32  and loss:  2831.5277214050293
forward train acc: top1 ->  99.514 ; top5 ->  100.0  and loss:  1.66323236329481
test acc: top1 ->  91.11 ; top5 ->  99.15  and loss:  49.61543143540621
forward train acc: top1 ->  99.9 ; top5 ->  100.0  and loss:  0.4151520193554461
test acc: top1 ->  91.87 ; top5 ->  99.21  and loss:  48.06955723464489
forward train acc: top1 ->  99.87599997558594 ; top5 ->  100.0  and loss:  0.37089938996359706
test acc: top1 ->  91.89 ; top5 ->  99.22  and loss:  49.59347828477621
forward train acc: top1 ->  99.902 ; top5 ->  100.0  and loss:  0.3120073595782742
test acc: top1 ->  91.84 ; top5 ->  99.23  and loss:  52.31315345317125
forward train acc: top1 ->  99.90399997558593 ; top5 ->  100.0  and loss:  0.2838619016110897
test acc: top1 ->  91.93 ; top5 ->  99.26  and loss:  53.99187084287405
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.2540447629871778
test acc: top1 ->  91.95 ; top5 ->  99.23  and loss:  53.57641864567995
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.17862363066524267
test acc: top1 ->  91.93 ; top5 ->  99.24  and loss:  54.815815791487694
forward train acc: top1 ->  99.93999997558593 ; top5 ->  100.0  and loss:  0.17405013646930456
test acc: top1 ->  92.01 ; top5 ->  99.2  and loss:  55.33623021095991
forward train acc: top1 ->  99.95199997558593 ; top5 ->  100.0  and loss:  0.18253104155883193
test acc: top1 ->  92.06 ; top5 ->  99.14  and loss:  56.76869843900204
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.19750346441287547
test acc: top1 ->  91.99 ; top5 ->  99.22  and loss:  57.06039371713996
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  7 / 512 , inc:  1
---------------- start layer  10  ---------------
### skip layer  10 wait:  4  ###
---------------- start layer  11  ---------------
### skip layer  11 wait:  3  ###
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
### skip layer  13 wait:  3  ###
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.671875  ==>  43 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.39453125  ==>  101 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.072265625  ==>  37 / 512 , inc:  1
layer  9  :  0.013671875  ==>  7 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.08984375  ==>  46 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [1.5781980183776465, 5.6113707320094095, 1.5781980183776465, 1.5781980183776465, 1.5781980183776465, 1.5781980183776465, 1.5781980183776465, 1.0521320122517643, 1.4028426830023524, 1.4028426830023524, 1.1836485137832349, 1.4028426830023524, 1.5781980183776465, 11.222741464018819]  wait [4, 2, 4, 4, 4, 4, 3, 3, 4, 4, 3, 2, 3, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 9
$$$$$$$$$$$$$ epoch  75  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -949.553086578846 , diff:  949.553086578846
adv train loss:  -1034.4764318466187 , diff:  84.92334526777267
adv train loss:  -1034.7741794586182 , diff:  0.2977476119995117
adv train loss:  -1036.0324535369873 , diff:  1.2582740783691406
adv train loss:  -1029.1063899993896 , diff:  6.926063537597656
adv train loss:  -1032.4095602035522 , diff:  3.3031702041625977
adv train loss:  -1037.1301984786987 , diff:  4.720638275146484
adv train loss:  -1035.8501110076904 , diff:  1.2800874710083008
adv train loss:  -1031.0222425460815 , diff:  4.827868461608887
adv train loss:  -1036.4774055480957 , diff:  5.45516300201416
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  31
test acc: top1 ->  10.0 ; top5 ->  50.64  and loss:  32994.27404785156
forward train acc: top1 ->  97.72599998535156 ; top5 ->  99.93799997558594  and loss:  8.94309857673943
test acc: top1 ->  89.02 ; top5 ->  98.77  and loss:  58.806038454174995
forward train acc: top1 ->  98.27199998046875 ; top5 ->  99.964  and loss:  5.556555513292551
test acc: top1 ->  89.81 ; top5 ->  99.01  and loss:  46.557686135172844
forward train acc: top1 ->  98.57600000732423 ; top5 ->  99.964  and loss:  4.449008125811815
test acc: top1 ->  90.01 ; top5 ->  99.22  and loss:  45.23849564045668
forward train acc: top1 ->  98.65999997802734 ; top5 ->  99.966  and loss:  4.124712789431214
test acc: top1 ->  90.25 ; top5 ->  99.17  and loss:  45.50311183184385
forward train acc: top1 ->  98.8940000024414 ; top5 ->  99.992  and loss:  3.278508223593235
test acc: top1 ->  90.49 ; top5 ->  99.31  and loss:  45.32814169675112
forward train acc: top1 ->  99.05400000732422 ; top5 ->  99.97999997558594  and loss:  2.951046290807426
test acc: top1 ->  90.38 ; top5 ->  99.13  and loss:  46.05152990669012
forward train acc: top1 ->  98.94599997558593 ; top5 ->  99.988  and loss:  3.012130015529692
test acc: top1 ->  90.66 ; top5 ->  99.22  and loss:  45.48615212738514
forward train acc: top1 ->  99.10800000488281 ; top5 ->  99.992  and loss:  2.65149859059602
test acc: top1 ->  90.71 ; top5 ->  99.29  and loss:  45.6957328915596
forward train acc: top1 ->  99.0520000024414 ; top5 ->  99.992  and loss:  2.65010947547853
test acc: top1 ->  90.51 ; top5 ->  99.2  and loss:  47.27318371832371
forward train acc: top1 ->  99.14599998046874 ; top5 ->  99.99  and loss:  2.4771283976733685
test acc: top1 ->  90.51 ; top5 ->  99.28  and loss:  46.6810701712966
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -0.5066505819559097 , diff:  0.5066505819559097
adv train loss:  -0.5407659669872373 , diff:  0.034115385031327605
adv train loss:  -0.5832583552692086 , diff:  0.042492388281971216
adv train loss:  -0.5292421677149832 , diff:  0.054016187554225326
adv train loss:  -0.5249560717493296 , diff:  0.004286095965653658
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  42
test acc: top1 ->  10.96 ; top5 ->  51.65  and loss:  524.1741485595703
forward train acc: top1 ->  99.84799997558594 ; top5 ->  99.998  and loss:  0.5015287399291992
test acc: top1 ->  91.73 ; top5 ->  99.47  and loss:  47.725081749260426
forward train acc: top1 ->  99.914 ; top5 ->  100.0  and loss:  0.28897450293879956
test acc: top1 ->  91.79 ; top5 ->  99.45  and loss:  51.41327815502882
forward train acc: top1 ->  99.912 ; top5 ->  99.998  and loss:  0.2615506827714853
test acc: top1 ->  91.61 ; top5 ->  99.37  and loss:  57.133422054350376
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.17415736051043496
test acc: top1 ->  92.0 ; top5 ->  99.44  and loss:  56.33558875322342
forward train acc: top1 ->  99.936 ; top5 ->  100.0  and loss:  0.18640887225046754
test acc: top1 ->  91.91 ; top5 ->  99.44  and loss:  58.08627641946077
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.18757667648606002
test acc: top1 ->  92.02 ; top5 ->  99.25  and loss:  57.301641292870045
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.1708663127683394
test acc: top1 ->  92.05 ; top5 ->  99.4  and loss:  56.24569375067949
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.13304579874966294
test acc: top1 ->  92.07 ; top5 ->  99.4  and loss:  57.18031142652035
forward train acc: top1 ->  99.94199997558594 ; top5 ->  100.0  and loss:  0.15119605837389827
test acc: top1 ->  92.06 ; top5 ->  99.33  and loss:  57.88961152732372
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.11819582246243954
test acc: top1 ->  91.99 ; top5 ->  99.41  and loss:  56.682844802737236
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  43 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -2.6077105794101954 , diff:  2.6077105794101954
adv train loss:  -4.810324335470796 , diff:  2.2026137560606003
adv train loss:  -5.073976566083729 , diff:  0.26365223061293364
adv train loss:  -4.865649103187025 , diff:  0.20832746289670467
adv train loss:  -5.087420137599111 , diff:  0.221771034412086
adv train loss:  -5.101288456469774 , diff:  0.013868318870663643
adv train loss:  -4.820558509789407 , diff:  0.280729946680367
adv train loss:  -4.846674147993326 , diff:  0.026115638203918934
adv train loss:  -4.99378957785666 , diff:  0.1471154298633337
adv train loss:  -4.938105830922723 , diff:  0.05568374693393707
************ all values are small in this layer **********
layer  2  adv train finish, try to retain  99
test acc: top1 ->  9.74 ; top5 ->  47.33  and loss:  1039.0638370513916
forward train acc: top1 ->  99.796 ; top5 ->  100.0  and loss:  0.5750034586526453
test acc: top1 ->  92.1 ; top5 ->  99.34  and loss:  56.070855528116226
forward train acc: top1 ->  99.846 ; top5 ->  99.998  and loss:  0.5249927093391307
test acc: top1 ->  92.18 ; top5 ->  99.39  and loss:  54.40983296558261
==> this epoch:  99 / 128
---------------- start layer  3  ---------------
adv train loss:  -599.8664371520281 , diff:  599.8664371520281
adv train loss:  -1229.7500219345093 , diff:  629.8835847824812
adv train loss:  -1241.6406145095825 , diff:  11.890592575073242
adv train loss:  -1264.4404907226562 , diff:  22.79987621307373
adv train loss:  -1251.6470079421997 , diff:  12.793482780456543
adv train loss:  -1256.2920064926147 , diff:  4.644998550415039
adv train loss:  -1251.2644729614258 , diff:  5.027533531188965
adv train loss:  -1259.4150218963623 , diff:  8.150548934936523
adv train loss:  -1251.5170078277588 , diff:  7.898014068603516
adv train loss:  -1267.479941368103 , diff:  15.962933540344238
************ all values are small in this layer **********
layer  3  adv train finish, try to retain  86
test acc: top1 ->  9.69 ; top5 ->  53.16  and loss:  18003.434310913086
forward train acc: top1 ->  99.70800000244141 ; top5 ->  100.0  and loss:  0.9590137219056487
test acc: top1 ->  91.32 ; top5 ->  99.34  and loss:  57.57305532693863
forward train acc: top1 ->  99.74799997558594 ; top5 ->  100.0  and loss:  0.8248749068006873
test acc: top1 ->  91.75 ; top5 ->  99.35  and loss:  53.35477416589856
forward train acc: top1 ->  99.7660000024414 ; top5 ->  100.0  and loss:  0.6705862190574408
test acc: top1 ->  91.67 ; top5 ->  99.38  and loss:  55.62059583514929
forward train acc: top1 ->  99.80399997558594 ; top5 ->  100.0  and loss:  0.6007122402079403
test acc: top1 ->  91.68 ; top5 ->  99.36  and loss:  55.30546882748604
forward train acc: top1 ->  99.784 ; top5 ->  99.998  and loss:  0.5936763533391058
test acc: top1 ->  91.67 ; top5 ->  99.28  and loss:  56.11790791898966
forward train acc: top1 ->  99.858 ; top5 ->  100.0  and loss:  0.4637895177002065
test acc: top1 ->  91.8 ; top5 ->  99.37  and loss:  54.47922436520457
forward train acc: top1 ->  99.808 ; top5 ->  100.0  and loss:  0.4704046626575291
test acc: top1 ->  91.82 ; top5 ->  99.39  and loss:  55.09663835912943
forward train acc: top1 ->  99.85199997558594 ; top5 ->  100.0  and loss:  0.4626111416146159
test acc: top1 ->  91.83 ; top5 ->  99.39  and loss:  54.58181423321366
forward train acc: top1 ->  99.864 ; top5 ->  100.0  and loss:  0.38815281906863675
test acc: top1 ->  91.82 ; top5 ->  99.31  and loss:  55.97801282629371
forward train acc: top1 ->  99.848 ; top5 ->  100.0  and loss:  0.400657341000624
test acc: top1 ->  91.96 ; top5 ->  99.38  and loss:  55.704690761864185
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -294.997679868713 , diff:  294.997679868713
adv train loss:  -1436.5538663864136 , diff:  1141.5561865177006
adv train loss:  -1682.2795000076294 , diff:  245.72563362121582
adv train loss:  -1733.030860900879 , diff:  50.75136089324951
adv train loss:  -1730.999189376831 , diff:  2.0316715240478516
adv train loss:  -1744.9709796905518 , diff:  13.971790313720703
adv train loss:  -1741.4156799316406 , diff:  3.555299758911133
adv train loss:  -1746.8099040985107 , diff:  5.394224166870117
adv train loss:  -1753.8490905761719 , diff:  7.039186477661133
adv train loss:  -1779.32737159729 , diff:  25.478281021118164
************ all values are small in this layer **********
layer  4  adv train finish, try to retain  154
test acc: top1 ->  10.35 ; top5 ->  47.42  and loss:  51976.863677978516
forward train acc: top1 ->  98.26199997802735 ; top5 ->  99.96  and loss:  5.975863978266716
test acc: top1 ->  90.05 ; top5 ->  99.31  and loss:  49.56767516210675
forward train acc: top1 ->  98.55199997802734 ; top5 ->  99.982  and loss:  4.425676556304097
test acc: top1 ->  90.1 ; top5 ->  99.36  and loss:  46.131818763911724
forward train acc: top1 ->  98.60399997802735 ; top5 ->  99.984  and loss:  4.0614298563450575
test acc: top1 ->  90.19 ; top5 ->  99.33  and loss:  44.71677837520838
forward train acc: top1 ->  98.8360000048828 ; top5 ->  99.992  and loss:  3.3846716983243823
test acc: top1 ->  90.49 ; top5 ->  99.37  and loss:  45.430755112320185
forward train acc: top1 ->  98.87599998291016 ; top5 ->  99.994  and loss:  3.2177058225497603
test acc: top1 ->  90.4 ; top5 ->  99.35  and loss:  46.47566242516041
forward train acc: top1 ->  98.95199997802735 ; top5 ->  99.986  and loss:  3.008886924944818
test acc: top1 ->  90.58 ; top5 ->  99.43  and loss:  46.04291743412614
forward train acc: top1 ->  99.0200000024414 ; top5 ->  99.992  and loss:  2.843798181042075
test acc: top1 ->  90.66 ; top5 ->  99.35  and loss:  46.02796794846654
forward train acc: top1 ->  98.97200001464844 ; top5 ->  99.998  and loss:  3.102106614038348
test acc: top1 ->  90.69 ; top5 ->  99.44  and loss:  45.61663443595171
forward train acc: top1 ->  99.00000000244141 ; top5 ->  99.996  and loss:  2.6879858849570155
test acc: top1 ->  90.75 ; top5 ->  99.37  and loss:  45.17409083247185
forward train acc: top1 ->  99.08399998291016 ; top5 ->  99.994  and loss:  2.6276531787589192
test acc: top1 ->  90.59 ; top5 ->  99.26  and loss:  48.01793644577265
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -399.76168048754334 , diff:  399.76168048754334
adv train loss:  -1598.4200716018677 , diff:  1198.6583911143243
adv train loss:  -1672.533055305481 , diff:  74.11298370361328
adv train loss:  -1697.9859352111816 , diff:  25.452879905700684
adv train loss:  -1704.7227840423584 , diff:  6.736848831176758
adv train loss:  -1696.0840873718262 , diff:  8.638696670532227
adv train loss:  -1700.6317710876465 , diff:  4.5476837158203125
adv train loss:  -1709.2026748657227 , diff:  8.570903778076172
adv train loss:  -1724.9641094207764 , diff:  15.761434555053711
adv train loss:  -1735.355052947998 , diff:  10.39094352722168
************ all values are small in this layer **********
layer  5  adv train finish, try to retain  148
test acc: top1 ->  9.78 ; top5 ->  58.46  and loss:  11774.688011169434
forward train acc: top1 ->  99.29000000488281 ; top5 ->  99.998  and loss:  2.09861087333411
test acc: top1 ->  91.42 ; top5 ->  99.31  and loss:  45.49843296408653
forward train acc: top1 ->  99.4780000024414 ; top5 ->  100.0  and loss:  1.5584537368267775
test acc: top1 ->  90.92 ; top5 ->  99.33  and loss:  48.22716584801674
forward train acc: top1 ->  99.574 ; top5 ->  100.0  and loss:  1.2724728370085359
test acc: top1 ->  91.53 ; top5 ->  99.3  and loss:  47.8523737937212
forward train acc: top1 ->  99.59999997558593 ; top5 ->  99.998  and loss:  1.201102658174932
test acc: top1 ->  91.36 ; top5 ->  99.32  and loss:  49.01844984292984
forward train acc: top1 ->  99.6360000024414 ; top5 ->  100.0  and loss:  1.1375774713233113
test acc: top1 ->  91.6 ; top5 ->  99.32  and loss:  50.37579208612442
forward train acc: top1 ->  99.64799997558593 ; top5 ->  100.0  and loss:  1.0164577052928507
test acc: top1 ->  91.36 ; top5 ->  99.36  and loss:  50.57942409068346
forward train acc: top1 ->  99.722 ; top5 ->  100.0  and loss:  0.8303446500794962
test acc: top1 ->  91.2 ; top5 ->  99.35  and loss:  51.786569997668266
forward train acc: top1 ->  99.672 ; top5 ->  100.0  and loss:  0.957036181120202
test acc: top1 ->  91.45 ; top5 ->  99.31  and loss:  51.18452758342028
forward train acc: top1 ->  99.668 ; top5 ->  100.0  and loss:  0.8464729634579271
test acc: top1 ->  91.45 ; top5 ->  99.35  and loss:  51.428604647517204
forward train acc: top1 ->  99.68799997558594 ; top5 ->  100.0  and loss:  0.8665789142251015
test acc: top1 ->  91.23 ; top5 ->  99.35  and loss:  53.19949947297573
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -382.4633006863296 , diff:  382.4633006863296
adv train loss:  -1484.4459590911865 , diff:  1101.982658404857
adv train loss:  -1540.7332592010498 , diff:  56.28730010986328
adv train loss:  -1630.3311834335327 , diff:  89.59792423248291
adv train loss:  -1676.5029668807983 , diff:  46.171783447265625
adv train loss:  -1683.062497138977 , diff:  6.559530258178711
adv train loss:  -1678.1938724517822 , diff:  4.868624687194824
adv train loss:  -1680.3470735549927 , diff:  2.153201103210449
adv train loss:  -1682.37802028656 , diff:  2.030946731567383
adv train loss:  -1682.3210906982422 , diff:  0.056929588317871094
************ all values are small in this layer **********
layer  6  adv train finish, try to retain  100
test acc: top1 ->  10.02 ; top5 ->  47.45  and loss:  4022971.494140625
forward train acc: top1 ->  99.88 ; top5 ->  100.0  and loss:  0.3343278709362494
test acc: top1 ->  92.01 ; top5 ->  99.37  and loss:  52.534665651619434
forward train acc: top1 ->  99.896 ; top5 ->  100.0  and loss:  0.2638298964011483
test acc: top1 ->  92.04 ; top5 ->  99.38  and loss:  54.137104012072086
forward train acc: top1 ->  99.924 ; top5 ->  100.0  and loss:  0.21133868530159816
test acc: top1 ->  91.9 ; top5 ->  99.31  and loss:  58.09800409525633
forward train acc: top1 ->  99.918 ; top5 ->  100.0  and loss:  0.19302032413543202
test acc: top1 ->  91.93 ; top5 ->  99.25  and loss:  60.81202480196953
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.21348733338527381
test acc: top1 ->  91.89 ; top5 ->  99.23  and loss:  60.57280993834138
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.1789693820901448
test acc: top1 ->  92.07 ; top5 ->  99.33  and loss:  58.06043151579797
forward train acc: top1 ->  99.946 ; top5 ->  99.998  and loss:  0.14802009024424478
test acc: top1 ->  92.02 ; top5 ->  99.4  and loss:  59.825732881203294
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.1456846707915247
test acc: top1 ->  92.18 ; top5 ->  99.4  and loss:  59.15589404478669
==> this epoch:  100 / 256
---------------- start layer  7  ---------------
adv train loss:  -0.16523537824105006 , diff:  0.16523537824105006
adv train loss:  -0.15850753127597272 , diff:  0.006727846965077333
layer  7  adv train finish, try to retain  257
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -1013.3870868682861 , diff:  1013.3870868682861
adv train loss:  -1013.3008937835693 , diff:  0.08619308471679688
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  36
test acc: top1 ->  12.55 ; top5 ->  62.86  and loss:  18790.95376586914
forward train acc: top1 ->  98.43399997558593 ; top5 ->  99.988  and loss:  6.113103839568794
test acc: top1 ->  91.37 ; top5 ->  99.25  and loss:  43.436562813818455
forward train acc: top1 ->  99.822 ; top5 ->  100.0  and loss:  0.6662295497953892
test acc: top1 ->  91.74 ; top5 ->  99.29  and loss:  44.772938553243876
forward train acc: top1 ->  99.88199997558594 ; top5 ->  100.0  and loss:  0.5016661016270518
test acc: top1 ->  91.6 ; top5 ->  99.35  and loss:  47.22063182294369
forward train acc: top1 ->  99.904 ; top5 ->  99.998  and loss:  0.3576171032618731
test acc: top1 ->  91.69 ; top5 ->  99.2  and loss:  49.166036546230316
forward train acc: top1 ->  99.89799997558593 ; top5 ->  100.0  and loss:  0.3412949265912175
test acc: top1 ->  91.75 ; top5 ->  99.28  and loss:  50.229309897869825
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.27291761711239815
test acc: top1 ->  91.77 ; top5 ->  99.32  and loss:  50.964252933859825
forward train acc: top1 ->  99.91999997558594 ; top5 ->  100.0  and loss:  0.26999099319800735
test acc: top1 ->  91.7 ; top5 ->  99.29  and loss:  51.68266882002354
forward train acc: top1 ->  99.89599997558594 ; top5 ->  100.0  and loss:  0.30609178729355335
test acc: top1 ->  91.77 ; top5 ->  99.22  and loss:  52.94035068154335
forward train acc: top1 ->  99.91999997558594 ; top5 ->  100.0  and loss:  0.25599411875009537
test acc: top1 ->  91.69 ; top5 ->  99.3  and loss:  53.051461935043335
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.2624086537398398
test acc: top1 ->  91.88 ; top5 ->  99.23  and loss:  53.27706325426698
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  37 / 512 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -539.9439506530762 , diff:  539.9439506530762
adv train loss:  -617.4106884002686 , diff:  77.46673774719238
adv train loss:  -639.9894428253174 , diff:  22.578754425048828
adv train loss:  -662.3500423431396 , diff:  22.360599517822266
adv train loss:  -661.5858364105225 , diff:  0.7642059326171875
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  6
test acc: top1 ->  20.19 ; top5 ->  50.45  and loss:  7331.031410217285
forward train acc: top1 ->  99.416 ; top5 ->  99.996  and loss:  1.866184376529418
test acc: top1 ->  86.7 ; top5 ->  98.84  and loss:  90.3006586432457
forward train acc: top1 ->  99.882 ; top5 ->  100.0  and loss:  0.3575128804659471
test acc: top1 ->  92.06 ; top5 ->  99.25  and loss:  54.98147402331233
forward train acc: top1 ->  99.92999997558594 ; top5 ->  100.0  and loss:  0.2635227581486106
test acc: top1 ->  92.07 ; top5 ->  99.28  and loss:  55.16563306003809
forward train acc: top1 ->  99.94599997558593 ; top5 ->  100.0  and loss:  0.20877057639881968
test acc: top1 ->  92.22 ; top5 ->  99.21  and loss:  56.04022679571062
==> this epoch:  6 / 512
---------------- start layer  10  ---------------
adv train loss:  -325.48750019073486 , diff:  325.48750019073486
adv train loss:  -323.5303337574005 , diff:  1.9571664333343506
adv train loss:  -323.7910168170929 , diff:  0.2606830596923828
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  10
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  333403.9997558594
forward train acc: top1 ->  82.68199998046875 ; top5 ->  97.79599997558594  and loss:  75.86628319323063
test acc: top1 ->  54.04 ; top5 ->  94.6  and loss:  163.14855551719666
forward train acc: top1 ->  98.7340000024414 ; top5 ->  99.926  and loss:  10.345861408859491
test acc: top1 ->  89.37 ; top5 ->  96.94  and loss:  54.79150865972042
forward train acc: top1 ->  99.37800000732422 ; top5 ->  99.954  and loss:  4.55246402695775
test acc: top1 ->  90.43 ; top5 ->  97.34  and loss:  54.04733921587467
forward train acc: top1 ->  99.57999997558593 ; top5 ->  99.994  and loss:  2.753941046074033
test acc: top1 ->  90.61 ; top5 ->  97.61  and loss:  54.83811168372631
forward train acc: top1 ->  99.67999997558594 ; top5 ->  99.98  and loss:  1.8407728783786297
test acc: top1 ->  90.8 ; top5 ->  97.62  and loss:  55.097549110651016
forward train acc: top1 ->  99.698 ; top5 ->  99.992  and loss:  1.538762324489653
test acc: top1 ->  90.9 ; top5 ->  97.81  and loss:  55.3919041454792
forward train acc: top1 ->  99.758 ; top5 ->  99.998  and loss:  1.350900369696319
test acc: top1 ->  90.92 ; top5 ->  97.75  and loss:  55.81943944096565
forward train acc: top1 ->  99.756 ; top5 ->  99.99  and loss:  1.2258504731580615
test acc: top1 ->  90.94 ; top5 ->  97.78  and loss:  56.60650813579559
forward train acc: top1 ->  99.804 ; top5 ->  99.998  and loss:  0.9803382563404739
test acc: top1 ->  90.95 ; top5 ->  97.87  and loss:  56.5860505849123
forward train acc: top1 ->  99.78399997558594 ; top5 ->  99.996  and loss:  0.9875768050551414
test acc: top1 ->  90.92 ; top5 ->  97.93  and loss:  58.22476689517498
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  11 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -995.1592473983765 , diff:  995.1592473983765
adv train loss:  -996.5287933349609 , diff:  1.3695459365844727
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  1
test acc: top1 ->  10.01 ; top5 ->  52.24  and loss:  6595.677551269531
forward train acc: top1 ->  59.762000002441404 ; top5 ->  95.608  and loss:  189.10988855361938
test acc: top1 ->  58.78 ; top5 ->  98.28  and loss:  123.88241213560104
forward train acc: top1 ->  96.10600000488282 ; top5 ->  99.982  and loss:  21.577807769179344
test acc: top1 ->  88.49 ; top5 ->  98.47  and loss:  54.17358747124672
forward train acc: top1 ->  98.3760000024414 ; top5 ->  99.992  and loss:  9.962300084531307
test acc: top1 ->  89.48 ; top5 ->  98.52  and loss:  51.40444214642048
forward train acc: top1 ->  99.116 ; top5 ->  99.996  and loss:  5.121997654438019
test acc: top1 ->  90.12 ; top5 ->  98.64  and loss:  50.67598403990269
forward train acc: top1 ->  99.3800000024414 ; top5 ->  99.998  and loss:  3.216066725552082
test acc: top1 ->  90.42 ; top5 ->  98.68  and loss:  50.60484578460455
forward train acc: top1 ->  99.53 ; top5 ->  99.998  and loss:  2.423304745927453
test acc: top1 ->  90.45 ; top5 ->  98.68  and loss:  51.60066247731447
forward train acc: top1 ->  99.61599997802735 ; top5 ->  100.0  and loss:  2.0656014643609524
test acc: top1 ->  90.7 ; top5 ->  98.65  and loss:  51.84906631708145
forward train acc: top1 ->  99.61999997558594 ; top5 ->  99.998  and loss:  1.8825877513736486
test acc: top1 ->  90.72 ; top5 ->  98.71  and loss:  52.53286261856556
forward train acc: top1 ->  99.6260000024414 ; top5 ->  99.996  and loss:  1.7116250358521938
test acc: top1 ->  90.81 ; top5 ->  98.71  and loss:  53.62727065384388
forward train acc: top1 ->  99.69599997558593 ; top5 ->  99.998  and loss:  1.4360986156389117
test acc: top1 ->  90.89 ; top5 ->  98.77  and loss:  53.86405035853386
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  2 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -2901.9029083251953 , diff:  2901.9029083251953
adv train loss:  -2902.701759338379 , diff:  0.7988510131835938
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  45
test acc: top1 ->  10.0 ; top5 ->  59.5  and loss:  57825.307525634766
forward train acc: top1 ->  93.204 ; top5 ->  99.326  and loss:  37.830087141133845
test acc: top1 ->  91.61 ; top5 ->  98.66  and loss:  61.22042464837432
forward train acc: top1 ->  99.894 ; top5 ->  99.996  and loss:  0.44287592207547277
test acc: top1 ->  91.81 ; top5 ->  98.66  and loss:  60.79766244068742
forward train acc: top1 ->  99.914 ; top5 ->  100.0  and loss:  0.2993809304898605
test acc: top1 ->  91.89 ; top5 ->  98.72  and loss:  60.697001207619905
forward train acc: top1 ->  99.95999997558594 ; top5 ->  100.0  and loss:  0.21007633255794644
test acc: top1 ->  91.93 ; top5 ->  98.75  and loss:  60.66639391332865
forward train acc: top1 ->  99.92799997558593 ; top5 ->  100.0  and loss:  0.2760629616677761
test acc: top1 ->  92.01 ; top5 ->  98.74  and loss:  61.25824334844947
forward train acc: top1 ->  99.944 ; top5 ->  99.998  and loss:  0.21579914609901607
test acc: top1 ->  92.04 ; top5 ->  98.76  and loss:  60.905575685203075
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.22124219860415906
test acc: top1 ->  91.99 ; top5 ->  98.75  and loss:  60.68621187098324
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.1556224314845167
test acc: top1 ->  92.03 ; top5 ->  98.77  and loss:  61.413931895047426
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.16380355972796679
test acc: top1 ->  92.05 ; top5 ->  98.8  and loss:  61.44430657103658
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.18341926974244416
test acc: top1 ->  92.08 ; top5 ->  98.81  and loss:  61.65163156017661
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  46 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -8209.75789642334 , diff:  8209.75789642334
adv train loss:  -13887.546653747559 , diff:  5677.788757324219
adv train loss:  -19051.657516479492 , diff:  5164.110862731934
adv train loss:  -24084.469161987305 , diff:  5032.8116455078125
adv train loss:  -29070.32406616211 , diff:  4985.854904174805
adv train loss:  -34052.397216796875 , diff:  4982.073150634766
adv train loss:  -38994.407470703125 , diff:  4942.01025390625
adv train loss:  -43948.2014465332 , diff:  4953.793975830078
adv train loss:  -48763.25955200195 , diff:  4815.05810546875
adv train loss:  -52635.2507019043 , diff:  3871.9911499023438
layer  13  adv train finish, try to retain  18
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.671875  ==>  43 / 64 , inc:  1
layer  2  :  0.7734375  ==>  99 / 128 , inc:  2
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.390625  ==>  100 / 256 , inc:  2
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.072265625  ==>  37 / 512 , inc:  1
layer  9  :  0.01171875  ==>  6 / 512 , inc:  2
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.08984375  ==>  46 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [1.1836485137832349, 4.208528049007057, 1.5781980183776465, 1.1836485137832349, 1.1836485137832349, 1.1836485137832349, 1.5781980183776465, 2.1042640245035287, 1.0521320122517643, 1.4028426830023524, 0.8877363853374262, 1.0521320122517643, 1.1836485137832349, 22.445482928037638]  wait [4, 2, 0, 4, 4, 4, 0, 1, 4, 0, 3, 2, 3, 0]  inc [1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1]  tol: 10
$$$$$$$$$$$$$ epoch  76  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
adv train loss:  -0.17419012798927724 , diff:  0.17419012798927724
adv train loss:  -0.2452819548198022 , diff:  0.07109182683052495
adv train loss:  -0.16982109201489948 , diff:  0.07546086280490272
adv train loss:  -0.1788834537146613 , diff:  0.009062361699761823
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  42
test acc: top1 ->  10.05 ; top5 ->  49.45  and loss:  1586.6345949172974
forward train acc: top1 ->  99.862 ; top5 ->  99.998  and loss:  0.4679317076515872
test acc: top1 ->  91.7 ; top5 ->  98.82  and loss:  68.60529112070799
forward train acc: top1 ->  99.884 ; top5 ->  100.0  and loss:  0.345587017131038
test acc: top1 ->  91.98 ; top5 ->  98.97  and loss:  66.84662075713277
forward train acc: top1 ->  99.886 ; top5 ->  100.0  and loss:  0.3640332164359279
test acc: top1 ->  91.83 ; top5 ->  98.96  and loss:  65.9896672964096
forward train acc: top1 ->  99.896 ; top5 ->  100.0  and loss:  0.2776944867800921
test acc: top1 ->  91.93 ; top5 ->  99.0  and loss:  67.31987325847149
forward train acc: top1 ->  99.89599997558594 ; top5 ->  100.0  and loss:  0.2948872479610145
test acc: top1 ->  91.91 ; top5 ->  99.06  and loss:  68.20274098962545
forward train acc: top1 ->  99.94199997558594 ; top5 ->  100.0  and loss:  0.19424209464341402
test acc: top1 ->  91.95 ; top5 ->  99.11  and loss:  68.19169243425131
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.21016491428599693
test acc: top1 ->  92.03 ; top5 ->  99.06  and loss:  69.86873470246792
forward train acc: top1 ->  99.94 ; top5 ->  99.998  and loss:  0.24555095072719269
test acc: top1 ->  92.02 ; top5 ->  99.04  and loss:  68.78173185139894
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.18433569061016897
test acc: top1 ->  91.96 ; top5 ->  99.14  and loss:  68.75540804862976
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.19404532079352066
test acc: top1 ->  92.09 ; top5 ->  99.16  and loss:  68.1524708867073
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  43 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -4.981913341209292 , diff:  4.981913341209292
adv train loss:  -7.413905474357307 , diff:  2.4319921331480145
adv train loss:  -7.598257463425398 , diff:  0.18435198906809092
adv train loss:  -7.3050800720229745 , diff:  0.2931773914024234
adv train loss:  -7.628996575251222 , diff:  0.32391650322824717
adv train loss:  -7.592075367458165 , diff:  0.036921207793056965
adv train loss:  -7.465234717354178 , diff:  0.12684065010398626
adv train loss:  -6.890764322131872 , diff:  0.5744703952223063
adv train loss:  -7.387836151290685 , diff:  0.49707182915881276
adv train loss:  -7.235962092876434 , diff:  0.1518740584142506
************ all values are small in this layer **********
layer  2  adv train finish, try to retain  97
test acc: top1 ->  10.0 ; top5 ->  49.41  and loss:  5954736.96875
forward train acc: top1 ->  99.894 ; top5 ->  99.998  and loss:  0.34574962957412936
test acc: top1 ->  91.98 ; top5 ->  99.05  and loss:  67.299585852772
forward train acc: top1 ->  99.91399997558594 ; top5 ->  100.0  and loss:  0.2329448179807514
test acc: top1 ->  91.96 ; top5 ->  99.1  and loss:  66.63520758971572
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.17678065417567268
test acc: top1 ->  91.9 ; top5 ->  99.08  and loss:  69.73973813652992
forward train acc: top1 ->  99.914 ; top5 ->  100.0  and loss:  0.31121138476009946
test acc: top1 ->  91.79 ; top5 ->  99.06  and loss:  70.11819618940353
forward train acc: top1 ->  99.95399997558594 ; top5 ->  99.99799997558594  and loss:  0.19592888234183192
test acc: top1 ->  92.06 ; top5 ->  99.15  and loss:  67.0471216198057
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.12769362691324204
test acc: top1 ->  92.15 ; top5 ->  99.22  and loss:  66.7076795771718
==> this epoch:  97 / 128
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
adv train loss:  -593.6982039376162 , diff:  593.6982039376162
adv train loss:  -2366.7650356292725 , diff:  1773.0668316916563
adv train loss:  -2478.4130325317383 , diff:  111.64799690246582
adv train loss:  -2475.217752456665 , diff:  3.195280075073242
adv train loss:  -2472.9978103637695 , diff:  2.219942092895508
adv train loss:  -2488.3658714294434 , diff:  15.368061065673828
adv train loss:  -2478.981643676758 , diff:  9.384227752685547
adv train loss:  -2477.6610355377197 , diff:  1.320608139038086
adv train loss:  -2480.353956222534 , diff:  2.692920684814453
adv train loss:  -2476.761743545532 , diff:  3.592212677001953
************ all values are small in this layer **********
layer  6  adv train finish, try to retain  98
test acc: top1 ->  10.74 ; top5 ->  54.25  and loss:  10888672.7578125
forward train acc: top1 ->  99.88599997558593 ; top5 ->  100.0  and loss:  0.3499062405899167
test acc: top1 ->  91.59 ; top5 ->  99.03  and loss:  72.11905445158482
forward train acc: top1 ->  99.9 ; top5 ->  100.0  and loss:  0.30973263015039265
test acc: top1 ->  91.88 ; top5 ->  99.09  and loss:  68.96208891272545
forward train acc: top1 ->  99.90799997558594 ; top5 ->  99.998  and loss:  0.2839006604626775
test acc: top1 ->  91.75 ; top5 ->  99.13  and loss:  69.38453429564834
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.20651347899547545
test acc: top1 ->  91.81 ; top5 ->  99.17  and loss:  67.69476319849491
forward train acc: top1 ->  99.92799997558593 ; top5 ->  100.0  and loss:  0.21940155420452356
test acc: top1 ->  91.94 ; top5 ->  99.21  and loss:  68.18778866529465
forward train acc: top1 ->  99.94 ; top5 ->  100.0  and loss:  0.17092883436271222
test acc: top1 ->  91.93 ; top5 ->  99.1  and loss:  66.78273111209273
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.09945171338040382
test acc: top1 ->  91.83 ; top5 ->  99.11  and loss:  68.08987772092223
forward train acc: top1 ->  99.94 ; top5 ->  100.0  and loss:  0.17307134554721415
test acc: top1 ->  91.83 ; top5 ->  98.86  and loss:  70.03062372654676
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.12558792636264116
test acc: top1 ->  91.99 ; top5 ->  99.05  and loss:  67.09082216024399
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.14490220721927471
test acc: top1 ->  91.83 ; top5 ->  99.13  and loss:  68.81392975151539
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  100 / 256 , inc:  2
---------------- start layer  7  ---------------
adv train loss:  -0.19868327188305557 , diff:  0.19868327188305557
adv train loss:  -0.2037102222093381 , diff:  0.005026950326282531
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  252
test acc: top1 ->  18.35 ; top5 ->  68.26  and loss:  9517962.5234375
forward train acc: top1 ->  99.72199997558593 ; top5 ->  100.0  and loss:  0.7533607273362577
test acc: top1 ->  91.27 ; top5 ->  99.16  and loss:  63.730297438800335
forward train acc: top1 ->  99.8520000024414 ; top5 ->  100.0  and loss:  0.422031391877681
test acc: top1 ->  91.36 ; top5 ->  98.98  and loss:  67.1422893628478
forward train acc: top1 ->  99.882 ; top5 ->  100.0  and loss:  0.3572148658859078
test acc: top1 ->  91.66 ; top5 ->  99.1  and loss:  63.25677856616676
forward train acc: top1 ->  99.874 ; top5 ->  100.0  and loss:  0.37485417458810844
test acc: top1 ->  91.63 ; top5 ->  99.16  and loss:  63.25757700949907
forward train acc: top1 ->  99.872 ; top5 ->  100.0  and loss:  0.3626326040830463
test acc: top1 ->  91.3 ; top5 ->  99.09  and loss:  67.40463946759701
forward train acc: top1 ->  99.914 ; top5 ->  100.0  and loss:  0.2699802563583944
test acc: top1 ->  91.5 ; top5 ->  99.13  and loss:  65.63276137411594
forward train acc: top1 ->  99.896 ; top5 ->  99.998  and loss:  0.284961010562256
test acc: top1 ->  91.55 ; top5 ->  99.12  and loss:  64.97881249710917
forward train acc: top1 ->  99.90399997558593 ; top5 ->  100.0  and loss:  0.3384262244217098
test acc: top1 ->  91.56 ; top5 ->  99.13  and loss:  65.17696675658226
forward train acc: top1 ->  99.912 ; top5 ->  100.0  and loss:  0.2418470997363329
test acc: top1 ->  91.71 ; top5 ->  99.08  and loss:  65.9337541013956
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.21200640592724085
test acc: top1 ->  91.58 ; top5 ->  99.06  and loss:  67.94319383054972
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  253 / 512 , inc:  1
---------------- start layer  8  ---------------
### skip layer  8 wait:  4  ###
---------------- start layer  9  ---------------
adv train loss:  -2772.171142578125 , diff:  2772.171142578125
adv train loss:  -2826.325979232788 , diff:  54.154836654663086
adv train loss:  -2832.15242767334 , diff:  5.826448440551758
adv train loss:  -2832.819610595703 , diff:  0.6671829223632812
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  4
test acc: top1 ->  10.01 ; top5 ->  47.62  and loss:  2601.164390563965
forward train acc: top1 ->  96.36200000488282 ; top5 ->  99.99  and loss:  10.944309760816395
test acc: top1 ->  67.88 ; top5 ->  95.97  and loss:  199.76124346256256
forward train acc: top1 ->  99.61799997558593 ; top5 ->  100.0  and loss:  1.756303288973868
test acc: top1 ->  90.38 ; top5 ->  98.03  and loss:  66.0343302488327
forward train acc: top1 ->  99.67399997558594 ; top5 ->  100.0  and loss:  1.1684417193755507
test acc: top1 ->  90.82 ; top5 ->  98.05  and loss:  67.02879719436169
forward train acc: top1 ->  99.8 ; top5 ->  99.998  and loss:  0.8670176207087934
test acc: top1 ->  90.98 ; top5 ->  98.15  and loss:  67.791489854455
forward train acc: top1 ->  99.836 ; top5 ->  100.0  and loss:  0.737040729727596
test acc: top1 ->  90.94 ; top5 ->  98.21  and loss:  68.03323972970247
forward train acc: top1 ->  99.82799997558594 ; top5 ->  100.0  and loss:  0.6747271413914859
test acc: top1 ->  91.12 ; top5 ->  98.13  and loss:  67.38210133835673
forward train acc: top1 ->  99.86799997558593 ; top5 ->  100.0  and loss:  0.5228267549537122
test acc: top1 ->  91.16 ; top5 ->  98.26  and loss:  67.8226043060422
forward train acc: top1 ->  99.8640000024414 ; top5 ->  100.0  and loss:  0.5795191978104413
test acc: top1 ->  91.25 ; top5 ->  98.29  and loss:  68.7701257430017
forward train acc: top1 ->  99.884 ; top5 ->  100.0  and loss:  0.48888358287513256
test acc: top1 ->  91.3 ; top5 ->  98.31  and loss:  68.54504072666168
forward train acc: top1 ->  99.884 ; top5 ->  99.998  and loss:  0.47149839042685926
test acc: top1 ->  91.37 ; top5 ->  98.25  and loss:  68.32768362760544
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  6 / 512 , inc:  2
---------------- start layer  10  ---------------
### skip layer  10 wait:  3  ###
---------------- start layer  11  ---------------
adv train loss:  -2445.5609703063965 , diff:  2445.5609703063965
adv train loss:  -2521.7532596588135 , diff:  76.19228935241699
adv train loss:  -2805.5904865264893 , diff:  283.8372268676758
adv train loss:  -3152.8785438537598 , diff:  347.2880573272705
adv train loss:  -3140.6284790039062 , diff:  12.250064849853516
adv train loss:  -3220.315299987793 , diff:  79.68682098388672
adv train loss:  -3272.694221496582 , diff:  52.37892150878906
adv train loss:  -3244.2976989746094 , diff:  28.396522521972656
adv train loss:  -3243.5819206237793 , diff:  0.7157783508300781
layer  11  adv train finish, try to retain  503
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
### skip layer  12 wait:  3  ###
---------------- start layer  13  ---------------
adv train loss:  -5787.548263549805 , diff:  5787.548263549805
adv train loss:  -9933.09001159668 , diff:  4145.541748046875
adv train loss:  -14160.529418945312 , diff:  4227.439407348633
adv train loss:  -20275.02604675293 , diff:  6114.496627807617
adv train loss:  -27313.77067565918 , diff:  7038.74462890625
adv train loss:  -33756.390563964844 , diff:  6442.619888305664
adv train loss:  -40374.58444213867 , diff:  6618.193878173828
adv train loss:  -47059.349060058594 , diff:  6684.764617919922
adv train loss:  -53369.19186401367 , diff:  6309.842803955078
adv train loss:  -59407.01110839844 , diff:  6037.819244384766
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  8
test acc: top1 ->  38.79 ; top5 ->  84.93  and loss:  735.6777362823486
forward train acc: top1 ->  82.41000000488282 ; top5 ->  97.144  and loss:  105.12672631442547
test acc: top1 ->  89.46 ; top5 ->  98.79  and loss:  60.932495564222336
forward train acc: top1 ->  99.40199998046874 ; top5 ->  100.0  and loss:  2.874452027492225
test acc: top1 ->  89.96 ; top5 ->  98.88  and loss:  57.81227374821901
forward train acc: top1 ->  99.6320000024414 ; top5 ->  100.0  and loss:  1.9763711914420128
test acc: top1 ->  90.31 ; top5 ->  98.89  and loss:  57.583527125418186
forward train acc: top1 ->  99.64199998046875 ; top5 ->  100.0  and loss:  1.6409492241218686
test acc: top1 ->  90.52 ; top5 ->  98.91  and loss:  57.99932457879186
forward train acc: top1 ->  99.71399997558593 ; top5 ->  100.0  and loss:  1.3689037142321467
test acc: top1 ->  90.74 ; top5 ->  98.94  and loss:  58.474097058176994
forward train acc: top1 ->  99.77 ; top5 ->  99.994  and loss:  1.238105554599315
test acc: top1 ->  90.73 ; top5 ->  98.94  and loss:  58.9505254290998
forward train acc: top1 ->  99.7340000024414 ; top5 ->  99.998  and loss:  1.1708153295330703
test acc: top1 ->  90.71 ; top5 ->  98.9  and loss:  58.69814081862569
forward train acc: top1 ->  99.7780000024414 ; top5 ->  100.0  and loss:  1.125811864156276
test acc: top1 ->  90.76 ; top5 ->  98.89  and loss:  59.13318036496639
forward train acc: top1 ->  99.78000000244141 ; top5 ->  99.998  and loss:  1.0429815186653286
test acc: top1 ->  90.8 ; top5 ->  98.87  and loss:  59.60015461593866
forward train acc: top1 ->  99.798 ; top5 ->  100.0  and loss:  0.9600039324723184
test acc: top1 ->  90.87 ; top5 ->  98.89  and loss:  59.79669161140919
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  9 / 512 , inc:  1
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.671875  ==>  43 / 64 , inc:  1
layer  2  :  0.7578125  ==>  97 / 128 , inc:  4
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.390625  ==>  100 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.072265625  ==>  37 / 512 , inc:  1
layer  9  :  0.01171875  ==>  6 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.08984375  ==>  46 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [1.1836485137832349, 3.156396036755293, 1.5781980183776465, 1.1836485137832349, 1.1836485137832349, 1.1836485137832349, 1.1836485137832349, 1.5781980183776465, 1.0521320122517643, 1.0521320122517643, 0.8877363853374262, 2.1042640245035287, 1.1836485137832349, 16.83411219602823]  wait [3, 4, 0, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2]  inc [1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 10
$$$$$$$$$$$$$ epoch  77  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
### skip layer  1 wait:  4  ###
---------------- start layer  2  ---------------
adv train loss:  -46.13045285642147 , diff:  46.13045285642147
adv train loss:  -50.976121962070465 , diff:  4.8456691056489944
adv train loss:  -51.21008834242821 , diff:  0.2339663803577423
adv train loss:  -50.98462790250778 , diff:  0.22546043992042542
adv train loss:  -51.159684121608734 , diff:  0.17505621910095215
adv train loss:  -57.16907474398613 , diff:  6.009390622377396
adv train loss:  -62.53043740987778 , diff:  5.361362665891647
adv train loss:  -63.305574625730515 , diff:  0.7751372158527374
adv train loss:  -63.83303049206734 , diff:  0.5274558663368225
adv train loss:  -64.15334913134575 , diff:  0.32031863927841187
************ all values are small in this layer **********
layer  2  adv train finish, try to retain  93
test acc: top1 ->  10.11 ; top5 ->  45.45  and loss:  2622.7291889190674
forward train acc: top1 ->  99.666 ; top5 ->  100.0  and loss:  1.193041167454794
test acc: top1 ->  91.78 ; top5 ->  99.17  and loss:  82.91302658617496
forward train acc: top1 ->  99.95999997558594 ; top5 ->  100.0  and loss:  0.13111835508607328
test acc: top1 ->  92.12 ; top5 ->  99.21  and loss:  79.32631008699536
==> this epoch:  93 / 128
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
adv train loss:  -730.9853345181327 , diff:  730.9853345181327
adv train loss:  -3002.6266765594482 , diff:  2271.6413420413155
adv train loss:  -3145.5289154052734 , diff:  142.9022388458252
adv train loss:  -3209.4935035705566 , diff:  63.9645881652832
adv train loss:  -3231.5841751098633 , diff:  22.09067153930664
adv train loss:  -3229.99928855896 , diff:  1.5848865509033203
adv train loss:  -3231.2480754852295 , diff:  1.2487869262695312
adv train loss:  -3222.870481491089 , diff:  8.377593994140625
adv train loss:  -3227.8515625 , diff:  4.981081008911133
adv train loss:  -3246.2322311401367 , diff:  18.38066864013672
layer  6  adv train finish, try to retain  82
test acc: top1 ->  28.67 ; top5 ->  84.79  and loss:  10039.741584777832
forward train acc: top1 ->  99.894 ; top5 ->  100.0  and loss:  0.3035752182477154
test acc: top1 ->  91.86 ; top5 ->  99.17  and loss:  80.74337546527386
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.21959823596989736
test acc: top1 ->  91.69 ; top5 ->  99.23  and loss:  78.23555931448936
forward train acc: top1 ->  99.93199997558594 ; top5 ->  100.0  and loss:  0.17109426576644182
test acc: top1 ->  92.01 ; top5 ->  99.28  and loss:  77.92962270230055
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.2211188870769547
test acc: top1 ->  91.96 ; top5 ->  99.27  and loss:  79.2535158470273
forward train acc: top1 ->  99.928 ; top5 ->  100.0  and loss:  0.19129395484924316
test acc: top1 ->  91.72 ; top5 ->  99.24  and loss:  82.34502721205354
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.1410643239505589
test acc: top1 ->  92.01 ; top5 ->  99.3  and loss:  76.45922180311754
forward train acc: top1 ->  99.936 ; top5 ->  100.0  and loss:  0.18692384310998023
test acc: top1 ->  91.89 ; top5 ->  99.3  and loss:  76.99594226107001
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.14090901329473127
test acc: top1 ->  92.05 ; top5 ->  99.35  and loss:  75.75582004897296
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.14846447207764868
test acc: top1 ->  91.94 ; top5 ->  99.29  and loss:  76.62561834882945
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.11657230754462944
test acc: top1 ->  92.05 ; top5 ->  99.37  and loss:  76.24279236001894
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  100 / 256 , inc:  1
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
### skip layer  8 wait:  3  ###
---------------- start layer  9  ---------------
adv train loss:  -181.42586612701416 , diff:  181.42586612701416
adv train loss:  -244.80769228935242 , diff:  63.38182616233826
adv train loss:  -244.14341616630554 , diff:  0.664276123046875
adv train loss:  -245.28294801712036 , diff:  1.1395318508148193
adv train loss:  -294.9950361251831 , diff:  49.712088108062744
adv train loss:  -317.9984977245331 , diff:  23.003461599349976
adv train loss:  -317.91877722740173 , diff:  0.07972049713134766
layer  9  adv train finish, try to retain  463
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -4.2540041375905275 , diff:  4.2540041375905275
adv train loss:  -4.123254303820431 , diff:  0.1307498337700963
adv train loss:  -4.219019747339189 , diff:  0.09576544351875782
adv train loss:  -4.274371670559049 , diff:  0.0553519232198596
adv train loss:  -4.010709411464632 , diff:  0.2636622590944171
adv train loss:  -4.270034762099385 , diff:  0.2593253506347537
adv train loss:  -4.224575441330671 , diff:  0.04545932076871395
adv train loss:  -4.121074291877449 , diff:  0.10350114945322275
adv train loss:  -4.0728935692459345 , diff:  0.04818072263151407
adv train loss:  -4.190720838494599 , diff:  0.11782726924866438
layer  10  adv train finish, try to retain  485
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -2745.3595581054688 , diff:  2745.3595581054688
adv train loss:  -2820.47407913208 , diff:  75.11452102661133
adv train loss:  -2963.019292831421 , diff:  142.54521369934082
adv train loss:  -3004.162187576294 , diff:  41.14289474487305
adv train loss:  -3004.306167602539 , diff:  0.1439800262451172
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  1
test acc: top1 ->  11.25 ; top5 ->  59.8  and loss:  5056.0358810424805
forward train acc: top1 ->  89.57 ; top5 ->  97.778  and loss:  51.18458234705031
test acc: top1 ->  90.19 ; top5 ->  98.52  and loss:  45.91341809928417
forward train acc: top1 ->  99.72799997558593 ; top5 ->  100.0  and loss:  2.7160942992195487
test acc: top1 ->  91.02 ; top5 ->  98.68  and loss:  46.66856177896261
forward train acc: top1 ->  99.82199997558594 ; top5 ->  100.0  and loss:  1.501990206539631
test acc: top1 ->  91.14 ; top5 ->  98.78  and loss:  50.183307968080044
forward train acc: top1 ->  99.81 ; top5 ->  100.0  and loss:  1.0752257499843836
test acc: top1 ->  91.24 ; top5 ->  98.76  and loss:  52.28943081945181
forward train acc: top1 ->  99.87399997802734 ; top5 ->  100.0  and loss:  0.7746269181370735
test acc: top1 ->  91.3 ; top5 ->  98.74  and loss:  55.09914916008711
forward train acc: top1 ->  99.86199997558593 ; top5 ->  100.0  and loss:  0.7358708097599447
test acc: top1 ->  91.19 ; top5 ->  98.77  and loss:  56.9844054505229
forward train acc: top1 ->  99.8620000024414 ; top5 ->  99.998  and loss:  0.6964051052927971
test acc: top1 ->  91.36 ; top5 ->  98.81  and loss:  57.32995028793812
forward train acc: top1 ->  99.85799997558594 ; top5 ->  100.0  and loss:  0.6080235447734594
test acc: top1 ->  91.32 ; top5 ->  98.78  and loss:  58.22691874951124
forward train acc: top1 ->  99.9 ; top5 ->  100.0  and loss:  0.4946019402705133
test acc: top1 ->  91.33 ; top5 ->  98.79  and loss:  59.26805831864476
forward train acc: top1 ->  99.87399997558593 ; top5 ->  100.0  and loss:  0.5334591986611485
test acc: top1 ->  91.36 ; top5 ->  98.83  and loss:  59.94587932527065
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  2 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -2664.8474102020264 , diff:  2664.8474102020264
adv train loss:  -2662.7684116363525 , diff:  2.078998565673828
adv train loss:  -2665.238666534424 , diff:  2.470254898071289
adv train loss:  -2663.9805221557617 , diff:  1.2581443786621094
layer  12  adv train finish, try to retain  5
test acc: top1 ->  10.01 ; top5 ->  50.08  and loss:  50010.05712890625
forward train acc: top1 ->  99.194 ; top5 ->  100.0  and loss:  4.9698636687826365
test acc: top1 ->  92.02 ; top5 ->  98.99  and loss:  79.02669095993042
forward train acc: top1 ->  99.96199997558594 ; top5 ->  100.0  and loss:  0.1205391779076308
test acc: top1 ->  92.02 ; top5 ->  99.07  and loss:  79.4804553464055
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.08916486642556265
test acc: top1 ->  91.96 ; top5 ->  99.06  and loss:  79.01450178772211
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.07225446547090542
test acc: top1 ->  92.06 ; top5 ->  99.05  and loss:  77.70991459488869
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.09823566020349972
test acc: top1 ->  92.19 ; top5 ->  99.02  and loss:  78.63576828688383
==> this epoch:  5 / 512
---------------- start layer  13  ---------------
adv train loss:  -4759.106950759888 , diff:  4759.106950759888
adv train loss:  -8207.648067474365 , diff:  3448.5411167144775
adv train loss:  -11816.536018371582 , diff:  3608.887950897217
adv train loss:  -15240.650131225586 , diff:  3424.114112854004
adv train loss:  -18633.84959411621 , diff:  3393.199462890625
adv train loss:  -22169.21226501465 , diff:  3535.3626708984375
adv train loss:  -25715.586883544922 , diff:  3546.3746185302734
adv train loss:  -29191.695678710938 , diff:  3476.1087951660156
adv train loss:  -32724.61996459961 , diff:  3532.924285888672
adv train loss:  -37321.3307800293 , diff:  4596.7108154296875
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  8
test acc: top1 ->  18.49 ; top5 ->  74.79  and loss:  1794.8966388702393
forward train acc: top1 ->  51.861999992675784 ; top5 ->  84.38399999755859  and loss:  560.2793887853622
test acc: top1 ->  69.32 ; top5 ->  89.22  and loss:  205.8933379650116
forward train acc: top1 ->  84.61800000976562 ; top5 ->  95.344  and loss:  79.88203769922256
test acc: top1 ->  83.73 ; top5 ->  98.39  and loss:  69.8126138150692
forward train acc: top1 ->  97.59199998291015 ; top5 ->  99.994  and loss:  26.98508295416832
test acc: top1 ->  88.15 ; top5 ->  98.14  and loss:  63.480511635541916
forward train acc: top1 ->  98.81799997558593 ; top5 ->  99.998  and loss:  20.01505196094513
test acc: top1 ->  88.5 ; top5 ->  98.06  and loss:  60.53748872876167
forward train acc: top1 ->  98.98799998046874 ; top5 ->  99.998  and loss:  15.157678209245205
test acc: top1 ->  88.83 ; top5 ->  98.08  and loss:  58.52964334189892
forward train acc: top1 ->  99.2140000048828 ; top5 ->  100.0  and loss:  12.0552449375391
test acc: top1 ->  88.83 ; top5 ->  98.13  and loss:  58.179933935403824
forward train acc: top1 ->  99.30199997802734 ; top5 ->  99.992  and loss:  10.443079628050327
test acc: top1 ->  88.91 ; top5 ->  98.15  and loss:  58.40289758145809
forward train acc: top1 ->  99.33400000488281 ; top5 ->  99.996  and loss:  9.037317864596844
test acc: top1 ->  89.1 ; top5 ->  98.15  and loss:  58.255455300211906
forward train acc: top1 ->  99.4 ; top5 ->  99.998  and loss:  7.945563606917858
test acc: top1 ->  89.03 ; top5 ->  98.12  and loss:  58.53787776827812
forward train acc: top1 ->  99.4540000024414 ; top5 ->  100.0  and loss:  6.9540480971336365
test acc: top1 ->  89.11 ; top5 ->  98.14  and loss:  59.406997099518776
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  9 / 512 , inc:  1
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.671875  ==>  43 / 64 , inc:  1
layer  2  :  0.7265625  ==>  93 / 128 , inc:  8
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.390625  ==>  100 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.072265625  ==>  37 / 512 , inc:  1
layer  9  :  0.01171875  ==>  6 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.009765625  ==>  5 / 512 , inc:  2
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [1.1836485137832349, 3.156396036755293, 1.5781980183776465, 1.1836485137832349, 1.1836485137832349, 1.1836485137832349, 0.8877363853374262, 1.5781980183776465, 1.0521320122517643, 2.1042640245035287, 1.7754727706748523, 1.5781980183776465, 1.1836485137832349, 12.625584147021172]  wait [2, 3, 0, 2, 2, 2, 4, 2, 2, 2, 2, 4, 0, 4]  inc [1, 1, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1]  tol: 10
$$$$$$$$$$$$$ epoch  78  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -2107.9392528533936 , diff:  2107.9392528533936
adv train loss:  -2124.0790634155273 , diff:  16.13981056213379
adv train loss:  -2119.3389377593994 , diff:  4.74012565612793
adv train loss:  -2139.9727458953857 , diff:  20.633808135986328
adv train loss:  -2165.790988922119 , diff:  25.8182430267334
adv train loss:  -2174.2828254699707 , diff:  8.491836547851562
adv train loss:  -2179.506790161133 , diff:  5.223964691162109
adv train loss:  -2174.7193603515625 , diff:  4.7874298095703125
adv train loss:  -2179.2448139190674 , diff:  4.525453567504883
adv train loss:  -2099.233123779297 , diff:  80.01169013977051
layer  0  adv train finish, try to retain  26
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  42398.03414916992
forward train acc: top1 ->  95.89399997558594 ; top5 ->  99.9  and loss:  25.099264374002814
test acc: top1 ->  89.42 ; top5 ->  98.51  and loss:  108.48920968174934
forward train acc: top1 ->  98.74199997802734 ; top5 ->  99.952  and loss:  6.395757164340466
test acc: top1 ->  90.2 ; top5 ->  98.9  and loss:  95.31787395477295
forward train acc: top1 ->  98.96799997802735 ; top5 ->  99.966  and loss:  4.750048192683607
test acc: top1 ->  90.33 ; top5 ->  98.94  and loss:  86.64217501878738
forward train acc: top1 ->  99.11999997558594 ; top5 ->  99.98  and loss:  3.7259979164227843
test acc: top1 ->  90.58 ; top5 ->  99.07  and loss:  80.71763247996569
forward train acc: top1 ->  99.23599997558594 ; top5 ->  99.99  and loss:  2.958349007880315
test acc: top1 ->  90.65 ; top5 ->  99.12  and loss:  77.6697175949812
forward train acc: top1 ->  99.29200000244141 ; top5 ->  99.982  and loss:  2.7807195954956114
test acc: top1 ->  90.52 ; top5 ->  99.09  and loss:  74.87605226784945
forward train acc: top1 ->  99.28599997558594 ; top5 ->  99.984  and loss:  2.7489263638854027
test acc: top1 ->  90.64 ; top5 ->  99.09  and loss:  73.77148881554604
forward train acc: top1 ->  99.3680000024414 ; top5 ->  99.986  and loss:  2.259092489257455
test acc: top1 ->  90.64 ; top5 ->  99.1  and loss:  71.65481463074684
forward train acc: top1 ->  99.31600000244141 ; top5 ->  99.998  and loss:  2.3673381959088147
test acc: top1 ->  90.75 ; top5 ->  99.18  and loss:  69.7134892642498
forward train acc: top1 ->  99.44199997802734 ; top5 ->  99.998  and loss:  2.042774566449225
test acc: top1 ->  90.82 ; top5 ->  99.18  and loss:  68.35321742296219
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
### skip layer  1 wait:  3  ###
---------------- start layer  2  ---------------
adv train loss:  -1.7028852617368102 , diff:  1.7028852617368102
adv train loss:  -2.328068525996059 , diff:  0.625183264259249
adv train loss:  -2.3557349080219865 , diff:  0.027666382025927305
adv train loss:  -5.0665875328704715 , diff:  2.710852624848485
adv train loss:  -6.677893909625709 , diff:  1.6113063767552376
adv train loss:  -7.277619860135019 , diff:  0.5997259505093098
adv train loss:  -7.129466259852052 , diff:  0.1481536002829671
adv train loss:  -6.539120512083173 , diff:  0.5903457477688789
adv train loss:  -6.759286176413298 , diff:  0.22016566433012486
adv train loss:  -6.669727206230164 , diff:  0.08955897018313408
************ all values are small in this layer **********
layer  2  adv train finish, try to retain  85
test acc: top1 ->  10.02 ; top5 ->  51.66  and loss:  1033.1401853561401
forward train acc: top1 ->  99.622 ; top5 ->  99.998  and loss:  1.285214542527683
test acc: top1 ->  91.32 ; top5 ->  99.15  and loss:  68.3163240402937
forward train acc: top1 ->  99.72600000244141 ; top5 ->  99.998  and loss:  0.8681672415696084
test acc: top1 ->  91.05 ; top5 ->  99.19  and loss:  68.66671794652939
forward train acc: top1 ->  99.812 ; top5 ->  100.0  and loss:  0.6215367268159753
test acc: top1 ->  91.43 ; top5 ->  99.29  and loss:  67.19694259762764
forward train acc: top1 ->  99.77 ; top5 ->  100.0  and loss:  0.7519687008752953
test acc: top1 ->  91.49 ; top5 ->  99.26  and loss:  63.96254505217075
forward train acc: top1 ->  99.80399997558594 ; top5 ->  100.0  and loss:  0.6333696045912802
test acc: top1 ->  91.5 ; top5 ->  99.33  and loss:  61.81948080658913
forward train acc: top1 ->  99.84199997558594 ; top5 ->  100.0  and loss:  0.5110243284143507
test acc: top1 ->  91.59 ; top5 ->  99.27  and loss:  61.93369720876217
forward train acc: top1 ->  99.874 ; top5 ->  100.0  and loss:  0.4083738340996206
test acc: top1 ->  91.69 ; top5 ->  99.32  and loss:  61.03586621582508
forward train acc: top1 ->  99.866 ; top5 ->  100.0  and loss:  0.4387639828491956
test acc: top1 ->  91.66 ; top5 ->  99.36  and loss:  61.42099602520466
forward train acc: top1 ->  99.87 ; top5 ->  99.998  and loss:  0.37844180621323176
test acc: top1 ->  91.73 ; top5 ->  99.31  and loss:  61.85237021744251
forward train acc: top1 ->  99.88 ; top5 ->  100.0  and loss:  0.35737276973668486
test acc: top1 ->  91.67 ; top5 ->  99.33  and loss:  62.73476038873196
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  93 / 128 , inc:  8
---------------- start layer  3  ---------------
adv train loss:  -705.8675133991055 , diff:  705.8675133991055
adv train loss:  -1267.124366760254 , diff:  561.2568533611484
adv train loss:  -1312.032917022705 , diff:  44.90855026245117
adv train loss:  -1327.5846900939941 , diff:  15.551773071289062
adv train loss:  -1323.0534868240356 , diff:  4.531203269958496
adv train loss:  -1334.5313930511475 , diff:  11.477906227111816
adv train loss:  -1342.1537504196167 , diff:  7.622357368469238
adv train loss:  -1351.7820978164673 , diff:  9.628347396850586
adv train loss:  -1337.2029809951782 , diff:  14.579116821289062
adv train loss:  -1326.8896045684814 , diff:  10.313376426696777
layer  3  adv train finish, try to retain  76
test acc: top1 ->  19.24 ; top5 ->  64.2  and loss:  2105.333438873291
forward train acc: top1 ->  99.54799997558594 ; top5 ->  99.996  and loss:  1.4275366021320224
test acc: top1 ->  91.36 ; top5 ->  99.11  and loss:  59.60971466451883
forward train acc: top1 ->  99.62399997558593 ; top5 ->  99.996  and loss:  1.1813897555693984
test acc: top1 ->  91.55 ; top5 ->  99.21  and loss:  58.88523315638304
forward train acc: top1 ->  99.64999997558594 ; top5 ->  100.0  and loss:  1.0069733071140945
test acc: top1 ->  91.59 ; top5 ->  99.17  and loss:  57.005910474807024
forward train acc: top1 ->  99.7360000024414 ; top5 ->  100.0  and loss:  0.7743241265416145
test acc: top1 ->  91.61 ; top5 ->  99.22  and loss:  58.41038686782122
forward train acc: top1 ->  99.72799997558593 ; top5 ->  100.0  and loss:  0.8681135829538107
test acc: top1 ->  91.57 ; top5 ->  99.28  and loss:  55.08858584612608
forward train acc: top1 ->  99.7480000024414 ; top5 ->  100.0  and loss:  0.6775249717757106
test acc: top1 ->  91.64 ; top5 ->  99.33  and loss:  55.677045214921236
forward train acc: top1 ->  99.7580000024414 ; top5 ->  100.0  and loss:  0.6867382926866412
test acc: top1 ->  91.63 ; top5 ->  99.28  and loss:  56.93503649532795
forward train acc: top1 ->  99.79 ; top5 ->  99.998  and loss:  0.6697296989150345
test acc: top1 ->  91.65 ; top5 ->  99.28  and loss:  56.09573467075825
forward train acc: top1 ->  99.81 ; top5 ->  99.998  and loss:  0.5321576951537281
test acc: top1 ->  91.72 ; top5 ->  99.25  and loss:  57.100579999387264
forward train acc: top1 ->  99.764 ; top5 ->  99.996  and loss:  0.6164370834594592
test acc: top1 ->  91.54 ; top5 ->  99.31  and loss:  56.56802800297737
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -254.62961705215275 , diff:  254.62961705215275
adv train loss:  -1510.6460905075073 , diff:  1256.0164734553546
adv train loss:  -1639.3899278640747 , diff:  128.74383735656738
adv train loss:  -1673.2131271362305 , diff:  33.82319927215576
adv train loss:  -1689.5494270324707 , diff:  16.336299896240234
adv train loss:  -1698.8923816680908 , diff:  9.342954635620117
adv train loss:  -1717.9945516586304 , diff:  19.10216999053955
adv train loss:  -1721.2872161865234 , diff:  3.2926645278930664
adv train loss:  -1731.7243843078613 , diff:  10.43716812133789
adv train loss:  -1739.5264110565186 , diff:  7.802026748657227
layer  4  adv train finish, try to retain  105
test acc: top1 ->  22.18 ; top5 ->  77.27  and loss:  2191.0336380004883
forward train acc: top1 ->  97.10799998535157 ; top5 ->  99.94  and loss:  9.697438172996044
test acc: top1 ->  88.73 ; top5 ->  99.05  and loss:  52.18762020021677
forward train acc: top1 ->  97.75999998535156 ; top5 ->  99.952  and loss:  7.061412138864398
test acc: top1 ->  89.22 ; top5 ->  99.1  and loss:  48.677731938660145
forward train acc: top1 ->  98.07600001220703 ; top5 ->  99.972  and loss:  5.867999231442809
test acc: top1 ->  89.36 ; top5 ->  99.16  and loss:  48.68232834339142
forward train acc: top1 ->  98.26999998046875 ; top5 ->  99.982  and loss:  5.232898788526654
test acc: top1 ->  89.61 ; top5 ->  99.15  and loss:  48.471575528383255
forward train acc: top1 ->  98.36799997802734 ; top5 ->  99.98  and loss:  4.899734476581216
test acc: top1 ->  89.75 ; top5 ->  99.23  and loss:  47.476585514843464
forward train acc: top1 ->  98.51799998046874 ; top5 ->  99.974  and loss:  4.3650664780288935
test acc: top1 ->  89.86 ; top5 ->  99.2  and loss:  47.36225502192974
forward train acc: top1 ->  98.60799998046875 ; top5 ->  99.992  and loss:  4.072404189035296
test acc: top1 ->  89.96 ; top5 ->  99.2  and loss:  47.024175837635994
forward train acc: top1 ->  98.67199998046875 ; top5 ->  99.982  and loss:  4.079750183969736
test acc: top1 ->  89.91 ; top5 ->  99.21  and loss:  47.31392610818148
forward train acc: top1 ->  98.63599997802734 ; top5 ->  99.99  and loss:  3.8987688291817904
test acc: top1 ->  89.95 ; top5 ->  99.22  and loss:  47.18990150094032
forward train acc: top1 ->  98.71000000488282 ; top5 ->  99.99  and loss:  3.68863957375288
test acc: top1 ->  89.96 ; top5 ->  99.2  and loss:  47.4070498496294
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -435.58352416940033 , diff:  435.58352416940033
adv train loss:  -1799.434434890747 , diff:  1363.8509107213467
adv train loss:  -1879.4241542816162 , diff:  79.98971939086914
adv train loss:  -1883.1744918823242 , diff:  3.750337600708008
adv train loss:  -1891.0548973083496 , diff:  7.880405426025391
adv train loss:  -1871.1856956481934 , diff:  19.86920166015625
adv train loss:  -1882.0774574279785 , diff:  10.891761779785156
adv train loss:  -1885.4987545013428 , diff:  3.421297073364258
adv train loss:  -1887.4953155517578 , diff:  1.996561050415039
adv train loss:  -1903.6558990478516 , diff:  16.16058349609375
layer  5  adv train finish, try to retain  114
test acc: top1 ->  29.34 ; top5 ->  79.17  and loss:  3957.1623611450195
forward train acc: top1 ->  99.09999998291016 ; top5 ->  99.99399997558594  and loss:  2.698417730629444
test acc: top1 ->  91.08 ; top5 ->  99.36  and loss:  47.170575991272926
forward train acc: top1 ->  99.39400000488281 ; top5 ->  100.0  and loss:  1.8231271523982286
test acc: top1 ->  90.98 ; top5 ->  99.43  and loss:  48.02987329661846
forward train acc: top1 ->  99.5160000024414 ; top5 ->  100.0  and loss:  1.4237703243270516
test acc: top1 ->  91.51 ; top5 ->  99.3  and loss:  49.12824626266956
forward train acc: top1 ->  99.62599997558594 ; top5 ->  100.0  and loss:  1.1279850965365767
test acc: top1 ->  91.41 ; top5 ->  99.39  and loss:  50.20127421617508
forward train acc: top1 ->  99.60799997558594 ; top5 ->  99.998  and loss:  1.1545057548210025
test acc: top1 ->  91.4 ; top5 ->  99.4  and loss:  50.38335698097944
forward train acc: top1 ->  99.67600000244141 ; top5 ->  99.998  and loss:  0.9239536980167031
test acc: top1 ->  91.58 ; top5 ->  99.39  and loss:  52.22479638457298
forward train acc: top1 ->  99.73199997558594 ; top5 ->  100.0  and loss:  0.8241166765801609
test acc: top1 ->  91.43 ; top5 ->  99.4  and loss:  52.01462818682194
forward train acc: top1 ->  99.70800000244141 ; top5 ->  100.0  and loss:  0.8751930156722665
test acc: top1 ->  91.49 ; top5 ->  99.39  and loss:  52.292677223682404
forward train acc: top1 ->  99.71599997558594 ; top5 ->  100.0  and loss:  0.8037962466478348
test acc: top1 ->  91.53 ; top5 ->  99.36  and loss:  52.83297326415777
forward train acc: top1 ->  99.694 ; top5 ->  100.0  and loss:  0.8427240860182792
test acc: top1 ->  91.48 ; top5 ->  99.34  and loss:  53.348954662680626
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
adv train loss:  -0.544643301749602 , diff:  0.544643301749602
adv train loss:  -0.5034905248321593 , diff:  0.04115277691744268
adv train loss:  -0.5489091915078461 , diff:  0.045418666675686836
adv train loss:  -0.49319146666675806 , diff:  0.05571772484108806
adv train loss:  -0.5356143550015986 , diff:  0.042422888334840536
adv train loss:  -0.4418061124160886 , diff:  0.09380824258551002
adv train loss:  -0.49440715042874217 , diff:  0.05260103801265359
adv train loss:  -0.502258174441522 , diff:  0.007851024012779817
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  252
test acc: top1 ->  18.71 ; top5 ->  56.57  and loss:  19878743.609375
forward train acc: top1 ->  99.70799997558593 ; top5 ->  100.0  and loss:  0.9533721273764968
test acc: top1 ->  91.23 ; top5 ->  99.19  and loss:  54.29860592633486
forward train acc: top1 ->  99.878 ; top5 ->  100.0  and loss:  0.45298886339878663
test acc: top1 ->  91.48 ; top5 ->  99.15  and loss:  54.9517989680171
forward train acc: top1 ->  99.888 ; top5 ->  100.0  and loss:  0.3185061311814934
test acc: top1 ->  91.5 ; top5 ->  99.19  and loss:  57.29782687872648
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.2643030076287687
test acc: top1 ->  91.66 ; top5 ->  99.21  and loss:  59.33809968456626
forward train acc: top1 ->  99.906 ; top5 ->  100.0  and loss:  0.2704815991455689
test acc: top1 ->  91.83 ; top5 ->  99.19  and loss:  61.20425969362259
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.17793677817098796
test acc: top1 ->  91.75 ; top5 ->  99.21  and loss:  62.348945789039135
forward train acc: top1 ->  99.928 ; top5 ->  100.0  and loss:  0.22981000525760464
test acc: top1 ->  91.73 ; top5 ->  99.16  and loss:  62.886939495801926
forward train acc: top1 ->  99.93199997558594 ; top5 ->  100.0  and loss:  0.23627541214227676
test acc: top1 ->  91.82 ; top5 ->  99.24  and loss:  62.0536455437541
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.23774322355166078
test acc: top1 ->  91.78 ; top5 ->  99.15  and loss:  62.57386723905802
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.13395820511505008
test acc: top1 ->  91.62 ; top5 ->  99.19  and loss:  63.69540084898472
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  253 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -70.52427506446838 , diff:  70.52427506446838
adv train loss:  -70.42345279455185 , diff:  0.10082226991653442
layer  8  adv train finish, try to retain  450
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -2595.7246379852295 , diff:  2595.7246379852295
adv train loss:  -2636.9677238464355 , diff:  41.243085861206055
adv train loss:  -2633.5978298187256 , diff:  3.369894027709961
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  5
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  54125.607177734375
forward train acc: top1 ->  99.52199997558594 ; top5 ->  100.0  and loss:  1.365389145212248
test acc: top1 ->  89.74 ; top5 ->  98.71  and loss:  76.64777690172195
forward train acc: top1 ->  99.912 ; top5 ->  100.0  and loss:  0.2844477026956156
test acc: top1 ->  91.98 ; top5 ->  99.06  and loss:  63.09535413980484
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.16767382419493515
test acc: top1 ->  91.88 ; top5 ->  99.02  and loss:  63.93960551172495
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.12812237186881248
test acc: top1 ->  91.95 ; top5 ->  99.02  and loss:  64.97407749295235
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.15751429683587048
test acc: top1 ->  92.21 ; top5 ->  99.08  and loss:  66.25781092047691
==> this epoch:  5 / 512
---------------- start layer  10  ---------------
adv train loss:  -20.461015477776527 , diff:  20.461015477776527
adv train loss:  -20.399130038917065 , diff:  0.06188543885946274
adv train loss:  -19.972051851451397 , diff:  0.4270781874656677
adv train loss:  -20.09419682621956 , diff:  0.12214497476816177
adv train loss:  -20.443783961236477 , diff:  0.3495871350169182
adv train loss:  -20.485597759485245 , diff:  0.04181379824876785
adv train loss:  -20.317592553794384 , diff:  0.16800520569086075
adv train loss:  -20.37817157059908 , diff:  0.06057901680469513
adv train loss:  -20.65214343369007 , diff:  0.273971863090992
adv train loss:  -20.147376887500286 , diff:  0.504766546189785
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  10
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  221268.78454589844
forward train acc: top1 ->  64.72400001708985 ; top5 ->  93.742  and loss:  199.84386357665062
test acc: top1 ->  23.68 ; top5 ->  66.6  and loss:  683.3692111968994
forward train acc: top1 ->  96.64799998291015 ; top5 ->  99.986  and loss:  20.5129339620471
test acc: top1 ->  88.77 ; top5 ->  98.0  and loss:  49.466467544436455
forward train acc: top1 ->  98.98800000244141 ; top5 ->  99.996  and loss:  8.408452056348324
test acc: top1 ->  89.82 ; top5 ->  98.16  and loss:  46.87526088953018
forward train acc: top1 ->  99.412 ; top5 ->  99.998  and loss:  4.142528833821416
test acc: top1 ->  90.57 ; top5 ->  98.15  and loss:  46.89636054635048
forward train acc: top1 ->  99.68 ; top5 ->  100.0  and loss:  2.3092373805120587
test acc: top1 ->  90.71 ; top5 ->  98.29  and loss:  47.50707945227623
forward train acc: top1 ->  99.76599997802734 ; top5 ->  99.996  and loss:  1.6444058632478118
test acc: top1 ->  90.77 ; top5 ->  98.24  and loss:  48.90005786716938
forward train acc: top1 ->  99.78399997558594 ; top5 ->  100.0  and loss:  1.4464377835392952
test acc: top1 ->  91.17 ; top5 ->  98.31  and loss:  48.8644517660141
forward train acc: top1 ->  99.82399997558593 ; top5 ->  100.0  and loss:  1.1753377318382263
test acc: top1 ->  91.08 ; top5 ->  98.3  and loss:  49.80515918880701
forward train acc: top1 ->  99.8400000024414 ; top5 ->  100.0  and loss:  1.055986494757235
test acc: top1 ->  91.32 ; top5 ->  98.41  and loss:  49.577352955937386
forward train acc: top1 ->  99.838 ; top5 ->  100.0  and loss:  0.9997527841478586
test acc: top1 ->  91.29 ; top5 ->  98.41  and loss:  50.41187580674887
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  11 / 512 , inc:  1
---------------- start layer  11  ---------------
### skip layer  11 wait:  4  ###
---------------- start layer  12  ---------------
adv train loss:  -1432.1930952072144 , diff:  1432.1930952072144
adv train loss:  -1431.9054908752441 , diff:  0.28760433197021484
layer  12  adv train finish, try to retain  4
test acc: top1 ->  10.0 ; top5 ->  58.3  and loss:  42627.84490966797
forward train acc: top1 ->  91.84400000488282 ; top5 ->  99.412  and loss:  51.18798986170441
test acc: top1 ->  90.45 ; top5 ->  98.58  and loss:  77.35955072939396
forward train acc: top1 ->  99.68599997558594 ; top5 ->  100.0  and loss:  1.400040808133781
test acc: top1 ->  91.0 ; top5 ->  98.62  and loss:  73.56914443522692
forward train acc: top1 ->  99.842 ; top5 ->  99.998  and loss:  0.8562663400080055
test acc: top1 ->  91.21 ; top5 ->  98.65  and loss:  70.98694529384375
forward train acc: top1 ->  99.854 ; top5 ->  100.0  and loss:  0.6860008640214801
test acc: top1 ->  91.26 ; top5 ->  98.69  and loss:  70.05805471912026
forward train acc: top1 ->  99.906 ; top5 ->  100.0  and loss:  0.49575227010063827
test acc: top1 ->  91.27 ; top5 ->  98.7  and loss:  68.93370947986841
forward train acc: top1 ->  99.898 ; top5 ->  100.0  and loss:  0.46138039336074144
test acc: top1 ->  91.37 ; top5 ->  98.59  and loss:  70.26432439684868
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.35092602018266916
test acc: top1 ->  91.5 ; top5 ->  98.68  and loss:  69.55540181696415
forward train acc: top1 ->  99.914 ; top5 ->  100.0  and loss:  0.4092343698721379
test acc: top1 ->  91.49 ; top5 ->  98.62  and loss:  71.38699413836002
forward train acc: top1 ->  99.9 ; top5 ->  100.0  and loss:  0.44571001757867634
test acc: top1 ->  91.4 ; top5 ->  98.64  and loss:  69.04935409873724
forward train acc: top1 ->  99.918 ; top5 ->  100.0  and loss:  0.3478877318557352
test acc: top1 ->  91.49 ; top5 ->  98.62  and loss:  68.4669601842761
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  5 / 512 , inc:  2
---------------- start layer  13  ---------------
### skip layer  13 wait:  4  ###
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.671875  ==>  43 / 64 , inc:  1
layer  2  :  0.7265625  ==>  93 / 128 , inc:  4
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.390625  ==>  100 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.072265625  ==>  37 / 512 , inc:  1
layer  9  :  0.009765625  ==>  5 / 512 , inc:  2
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.009765625  ==>  5 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [0.8877363853374262, 3.156396036755293, 1.1836485137832349, 0.8877363853374262, 0.8877363853374262, 0.8877363853374262, 0.8877363853374262, 1.1836485137832349, 2.1042640245035287, 2.1042640245035287, 1.3316045780061392, 1.5781980183776465, 0.8877363853374262, 12.625584147021172]  wait [4, 2, 2, 4, 4, 4, 3, 4, 2, 0, 4, 3, 2, 3]  inc [1, 1, 4, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]  tol: 10
$$$$$$$$$$$$$ epoch  79  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
adv train loss:  -381.27412700653076 , diff:  381.27412700653076
adv train loss:  -381.0700435638428 , diff:  0.20408344268798828
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  42
test acc: top1 ->  10.72 ; top5 ->  54.83  and loss:  1821.0206356048584
forward train acc: top1 ->  97.93199997558594 ; top5 ->  100.0  and loss:  20.523372248979285
test acc: top1 ->  91.57 ; top5 ->  99.05  and loss:  74.64454346895218
forward train acc: top1 ->  99.88399997558594 ; top5 ->  99.998  and loss:  0.39454989484511316
test acc: top1 ->  91.82 ; top5 ->  99.11  and loss:  72.97532499954104
forward train acc: top1 ->  99.906 ; top5 ->  100.0  and loss:  0.25761243802844547
test acc: top1 ->  91.93 ; top5 ->  99.08  and loss:  72.54560973495245
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.22123219435161445
test acc: top1 ->  92.0 ; top5 ->  99.07  and loss:  72.65893477201462
forward train acc: top1 ->  99.94 ; top5 ->  100.0  and loss:  0.1878092765036854
test acc: top1 ->  91.97 ; top5 ->  99.09  and loss:  72.68343169242144
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.13953006926749367
test acc: top1 ->  92.01 ; top5 ->  99.1  and loss:  72.97256650030613
forward train acc: top1 ->  99.94 ; top5 ->  100.0  and loss:  0.18762121102190576
test acc: top1 ->  91.94 ; top5 ->  99.12  and loss:  73.35408991575241
forward train acc: top1 ->  99.93999997558593 ; top5 ->  100.0  and loss:  0.1977032396607683
test acc: top1 ->  91.85 ; top5 ->  99.11  and loss:  72.95523651689291
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.1719122998038074
test acc: top1 ->  91.95 ; top5 ->  99.09  and loss:  73.2716852016747
forward train acc: top1 ->  99.94399997558594 ; top5 ->  100.0  and loss:  0.18555811373516917
test acc: top1 ->  91.82 ; top5 ->  99.12  and loss:  73.18180437386036
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  43 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -3.5879978273733286 , diff:  3.5879978273733286
adv train loss:  -5.814337629824877 , diff:  2.226339802451548
adv train loss:  -5.4527790658175945 , diff:  0.36155856400728226
adv train loss:  -5.475173796061426 , diff:  0.022394730243831873
adv train loss:  -6.038858854910359 , diff:  0.5636850588489324
adv train loss:  -5.427375816740096 , diff:  0.6114830381702632
adv train loss:  -5.678599006962031 , diff:  0.2512231902219355
adv train loss:  -5.547425297554582 , diff:  0.13117370940744877
adv train loss:  -5.746126842102967 , diff:  0.19870154454838485
adv train loss:  -5.485152331413701 , diff:  0.260974510689266
layer  2  adv train finish, try to retain  93
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  3  ###
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
adv train loss:  -111.47525155544281 , diff:  111.47525155544281
adv train loss:  -112.05684298276901 , diff:  0.5815914273262024
adv train loss:  -114.36880427598953 , diff:  2.31196129322052
adv train loss:  -114.86674183607101 , diff:  0.49793756008148193
adv train loss:  -115.9344453215599 , diff:  1.0677034854888916
adv train loss:  -100.719746530056 , diff:  15.214698791503906
adv train loss:  -99.11992698907852 , diff:  1.599819540977478
adv train loss:  -98.34194099903107 , diff:  0.7779859900474548
adv train loss:  -98.50577473640442 , diff:  0.16383373737335205
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  36
test acc: top1 ->  10.0 ; top5 ->  60.18  and loss:  10674568.5390625
forward train acc: top1 ->  97.37799997558594 ; top5 ->  99.908  and loss:  13.810574560426176
test acc: top1 ->  90.77 ; top5 ->  98.86  and loss:  60.01483342051506
forward train acc: top1 ->  99.68199997558594 ; top5 ->  100.0  and loss:  1.086689276387915
test acc: top1 ->  90.98 ; top5 ->  98.94  and loss:  60.40255269408226
forward train acc: top1 ->  99.8260000024414 ; top5 ->  100.0  and loss:  0.615359905641526
test acc: top1 ->  91.29 ; top5 ->  99.03  and loss:  60.60810010135174
forward train acc: top1 ->  99.8380000024414 ; top5 ->  100.0  and loss:  0.5389102390035987
test acc: top1 ->  91.3 ; top5 ->  99.04  and loss:  61.173252165317535
forward train acc: top1 ->  99.862 ; top5 ->  100.0  and loss:  0.4712616839678958
test acc: top1 ->  91.3 ; top5 ->  99.05  and loss:  61.124800108373165
forward train acc: top1 ->  99.89599997558594 ; top5 ->  100.0  and loss:  0.4341086192871444
test acc: top1 ->  91.35 ; top5 ->  99.12  and loss:  61.84550800174475
forward train acc: top1 ->  99.89199997558593 ; top5 ->  100.0  and loss:  0.36966442700941116
test acc: top1 ->  91.46 ; top5 ->  99.09  and loss:  61.51551531255245
forward train acc: top1 ->  99.90599997558594 ; top5 ->  99.998  and loss:  0.3420220073312521
test acc: top1 ->  91.67 ; top5 ->  99.16  and loss:  61.89964483678341
forward train acc: top1 ->  99.896 ; top5 ->  99.998  and loss:  0.3326524021103978
test acc: top1 ->  91.53 ; top5 ->  99.19  and loss:  62.10612133145332
forward train acc: top1 ->  99.91599997558593 ; top5 ->  100.0  and loss:  0.2973272576928139
test acc: top1 ->  91.67 ; top5 ->  99.19  and loss:  62.372400626540184
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  37 / 512 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -797.0045046806335 , diff:  797.0045046806335
adv train loss:  -855.53151512146 , diff:  58.527010440826416
adv train loss:  -854.9535465240479 , diff:  0.5779685974121094
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  3
test acc: top1 ->  10.03 ; top5 ->  50.06  and loss:  3644.5741481781006
forward train acc: top1 ->  97.74199997558594 ; top5 ->  100.0  and loss:  9.758692178875208
test acc: top1 ->  89.12 ; top5 ->  98.38  and loss:  64.18722802400589
forward train acc: top1 ->  99.88799997558594 ; top5 ->  100.0  and loss:  0.5869993017986417
test acc: top1 ->  91.81 ; top5 ->  98.52  and loss:  54.90529778599739
forward train acc: top1 ->  99.918 ; top5 ->  100.0  and loss:  0.3946415892569348
test acc: top1 ->  91.89 ; top5 ->  98.57  and loss:  56.40022477507591
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.2964668869972229
test acc: top1 ->  91.97 ; top5 ->  98.58  and loss:  58.08636752516031
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.25921793770976365
test acc: top1 ->  91.93 ; top5 ->  98.59  and loss:  58.92284605652094
forward train acc: top1 ->  99.95399997558594 ; top5 ->  99.998  and loss:  0.24593502515926957
test acc: top1 ->  91.97 ; top5 ->  98.58  and loss:  59.40899720042944
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.209344191767741
test acc: top1 ->  91.98 ; top5 ->  98.55  and loss:  59.422406263649464
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.18662212206982076
test acc: top1 ->  92.05 ; top5 ->  98.58  and loss:  59.81933982670307
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.1575158016057685
test acc: top1 ->  92.09 ; top5 ->  98.55  and loss:  60.046086460351944
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.1547902527381666
test acc: top1 ->  92.06 ; top5 ->  98.59  and loss:  61.17931415885687
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  5 / 512 , inc:  2
---------------- start layer  10  ---------------
### skip layer  10 wait:  4  ###
---------------- start layer  11  ---------------
### skip layer  11 wait:  3  ###
---------------- start layer  12  ---------------
adv train loss:  -52.83686253428459 , diff:  52.83686253428459
adv train loss:  -52.49315917491913 , diff:  0.34370335936546326
adv train loss:  -53.153008818626404 , diff:  0.6598496437072754
adv train loss:  -52.080651462078094 , diff:  1.0723573565483093
adv train loss:  -53.08878031373024 , diff:  1.0081288516521454
adv train loss:  -53.434105187654495 , diff:  0.34532487392425537
adv train loss:  -51.979279428720474 , diff:  1.454825758934021
adv train loss:  -52.44206804037094 , diff:  0.4627886116504669
adv train loss:  -52.59843119978905 , diff:  0.15636315941810608
adv train loss:  -53.79904252290726 , diff:  1.2006113231182098
layer  12  adv train finish, try to retain  508
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
### skip layer  13 wait:  3  ###
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.671875  ==>  43 / 64 , inc:  1
layer  2  :  0.7265625  ==>  93 / 128 , inc:  4
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.390625  ==>  100 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.072265625  ==>  37 / 512 , inc:  1
layer  9  :  0.009765625  ==>  5 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.009765625  ==>  5 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [0.8877363853374262, 2.3672970275664698, 2.3672970275664698, 0.8877363853374262, 0.8877363853374262, 0.8877363853374262, 0.8877363853374262, 1.1836485137832349, 1.5781980183776465, 1.5781980183776465, 1.3316045780061392, 1.5781980183776465, 1.7754727706748523, 12.625584147021172]  wait [3, 4, 2, 3, 3, 3, 2, 3, 4, 2, 3, 2, 2, 2]  inc [1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 10
$$$$$$$$$$$$$ epoch  80  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -4.382561455480754 , diff:  4.382561455480754
adv train loss:  -4.380245793610811 , diff:  0.002315661869943142
layer  0  adv train finish, try to retain  64
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -4.635669564828277 , diff:  4.635669564828277
adv train loss:  -4.490255570039153 , diff:  0.14541399478912354
adv train loss:  -4.442948704585433 , diff:  0.04730686545372009
adv train loss:  -4.351949332281947 , diff:  0.09099937230348587
adv train loss:  -4.582197533920407 , diff:  0.23024820163846016
adv train loss:  -4.4300653003156185 , diff:  0.15213223360478878
adv train loss:  -4.187468226999044 , diff:  0.2425970733165741
adv train loss:  -4.451769582927227 , diff:  0.2643013559281826
adv train loss:  -4.438927091658115 , diff:  0.012842491269111633
adv train loss:  -4.528925081714988 , diff:  0.08999799005687237
layer  1  adv train finish, try to retain  43
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -12.784580568782985 , diff:  12.784580568782985
adv train loss:  -14.913932032883167 , diff:  2.129351464100182
adv train loss:  -15.555835783481598 , diff:  0.6419037505984306
adv train loss:  -14.76463383436203 , diff:  0.7912019491195679
adv train loss:  -14.698134046047926 , diff:  0.06649978831410408
adv train loss:  -14.720422696322203 , diff:  0.022288650274276733
adv train loss:  -15.235714044421911 , diff:  0.5152913480997086
adv train loss:  -15.534433983266354 , diff:  0.29871993884444237
adv train loss:  -15.582753784954548 , diff:  0.048319801688194275
adv train loss:  -15.30197249725461 , diff:  0.2807812876999378
layer  2  adv train finish, try to retain  92
test acc: top1 ->  10.0 ; top5 ->  51.67  and loss:  2066.275230407715
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.24956768300035037
test acc: top1 ->  92.16 ; top5 ->  99.18  and loss:  74.82644402235746
==> this epoch:  92 / 128
---------------- start layer  3  ---------------
adv train loss:  -0.1164647229743423 , diff:  0.1164647229743423
adv train loss:  -0.1367329553468153 , diff:  0.020268232372472994
adv train loss:  -0.17863780885818414 , diff:  0.041904853511368856
adv train loss:  -0.11771738575771451 , diff:  0.060920423100469634
adv train loss:  -0.16438082151580602 , diff:  0.04666343575809151
adv train loss:  -0.1650691304821521 , diff:  0.0006883089663460851
layer  3  adv train finish, try to retain  126
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -0.1316318818135187 , diff:  0.1316318818135187
adv train loss:  -0.19582224288024008 , diff:  0.06419036106672138
adv train loss:  -0.15718219154223334 , diff:  0.03864005133800674
adv train loss:  -0.13719750389282126 , diff:  0.01998468764941208
adv train loss:  -0.20552056259475648 , diff:  0.06832305870193522
adv train loss:  -0.13262938719708472 , diff:  0.07289117539767176
adv train loss:  -0.13134650361462263 , diff:  0.001282883582462091
layer  4  adv train finish, try to retain  245
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -0.1722761820856249 , diff:  0.1722761820856249
adv train loss:  -0.1428982490906492 , diff:  0.02937793299497571
adv train loss:  -0.14335213333833963 , diff:  0.0004538842476904392
layer  5  adv train finish, try to retain  250
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -0.41250014302204363 , diff:  0.41250014302204363
adv train loss:  -0.4280476112035103 , diff:  0.015547468181466684
adv train loss:  -0.5423101299384143 , diff:  0.114262518734904
adv train loss:  -0.4378018907736987 , diff:  0.10450823916471563
adv train loss:  -0.40866232095868327 , diff:  0.02913956981501542
adv train loss:  -0.42442673741606995 , diff:  0.015764416457386687
adv train loss:  -0.44413641473511234 , diff:  0.019709677319042385
adv train loss:  -0.36448169394861907 , diff:  0.07965472078649327
adv train loss:  -0.4140127297723666 , diff:  0.049531035823747516
adv train loss:  -0.4844447569921613 , diff:  0.07043202721979469
layer  6  adv train finish, try to retain  248
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -0.193626739703177 , diff:  0.193626739703177
adv train loss:  -0.1843045482528396 , diff:  0.009322191450337414
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  252
test acc: top1 ->  10.13 ; top5 ->  66.47  and loss:  20219366.1171875
forward train acc: top1 ->  99.62399997558593 ; top5 ->  99.998  and loss:  1.3565790900029242
test acc: top1 ->  91.22 ; top5 ->  99.07  and loss:  73.89270260930061
forward train acc: top1 ->  99.81599997558594 ; top5 ->  100.0  and loss:  0.5723814538214356
test acc: top1 ->  91.37 ; top5 ->  99.18  and loss:  72.29065418988466
forward train acc: top1 ->  99.8580000024414 ; top5 ->  99.996  and loss:  0.4789495531003922
test acc: top1 ->  91.32 ; top5 ->  99.14  and loss:  71.78820961713791
forward train acc: top1 ->  99.8880000024414 ; top5 ->  100.0  and loss:  0.4059378221863881
test acc: top1 ->  91.52 ; top5 ->  99.14  and loss:  71.16826463490725
forward train acc: top1 ->  99.892 ; top5 ->  100.0  and loss:  0.32372260343981907
test acc: top1 ->  91.42 ; top5 ->  99.17  and loss:  71.22071928530931
forward train acc: top1 ->  99.912 ; top5 ->  100.0  and loss:  0.2555526842479594
test acc: top1 ->  91.49 ; top5 ->  99.16  and loss:  71.30432172864676
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.24630729320051614
test acc: top1 ->  91.51 ; top5 ->  99.17  and loss:  70.81489319354296
forward train acc: top1 ->  99.908 ; top5 ->  99.998  and loss:  0.2518143593915738
test acc: top1 ->  91.63 ; top5 ->  99.2  and loss:  70.78481476008892
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.2549653108580969
test acc: top1 ->  91.56 ; top5 ->  99.17  and loss:  72.22343124449253
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.23646450191154145
test acc: top1 ->  91.68 ; top5 ->  99.16  and loss:  71.87136711925268
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  253 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -108.35229963064194 , diff:  108.35229963064194
adv train loss:  -108.18451422452927 , diff:  0.1677854061126709
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  36
test acc: top1 ->  18.14 ; top5 ->  61.88  and loss:  13029413.4140625
forward train acc: top1 ->  98.66 ; top5 ->  99.998  and loss:  4.252618066035211
test acc: top1 ->  91.39 ; top5 ->  99.15  and loss:  62.233966298401356
forward train acc: top1 ->  99.84799997558594 ; top5 ->  100.0  and loss:  0.46820793813094497
test acc: top1 ->  91.5 ; top5 ->  99.24  and loss:  62.53157740086317
forward train acc: top1 ->  99.8760000024414 ; top5 ->  100.0  and loss:  0.36094482871703804
test acc: top1 ->  91.64 ; top5 ->  99.21  and loss:  63.407725270837545
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.2781964801833965
test acc: top1 ->  91.72 ; top5 ->  99.31  and loss:  62.68431993573904
forward train acc: top1 ->  99.93599997558594 ; top5 ->  100.0  and loss:  0.2623575378092937
test acc: top1 ->  91.68 ; top5 ->  99.33  and loss:  63.5231550373137
forward train acc: top1 ->  99.916 ; top5 ->  100.0  and loss:  0.2722024128306657
test acc: top1 ->  91.76 ; top5 ->  99.26  and loss:  64.03819708153605
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.20892034043208696
test acc: top1 ->  91.86 ; top5 ->  99.28  and loss:  64.04520424548537
forward train acc: top1 ->  99.94 ; top5 ->  99.998  and loss:  0.2307114575523883
test acc: top1 ->  91.77 ; top5 ->  99.26  and loss:  64.03851592261344
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.14661415334558114
test acc: top1 ->  91.75 ; top5 ->  99.27  and loss:  64.66371593624353
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.17405576512828702
test acc: top1 ->  91.76 ; top5 ->  99.27  and loss:  65.95277880877256
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  37 / 512 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -382.28250098228455 , diff:  382.28250098228455
adv train loss:  -388.0165767669678 , diff:  5.7340757846832275
adv train loss:  -388.9134404659271 , diff:  0.8968636989593506
adv train loss:  -387.1296880245209 , diff:  1.78375244140625
adv train loss:  -456.43481040000916 , diff:  69.30512237548828
adv train loss:  -515.0921320915222 , diff:  58.65732169151306
adv train loss:  -513.7680268287659 , diff:  1.3241052627563477
adv train loss:  -513.3189268112183 , diff:  0.4491000175476074
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  4
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  10646.924346923828
forward train acc: top1 ->  98.54999997558593 ; top5 ->  100.0  and loss:  6.037454333156347
test acc: top1 ->  71.48 ; top5 ->  98.3  and loss:  193.563330411911
forward train acc: top1 ->  99.914 ; top5 ->  100.0  and loss:  0.3681564787402749
test acc: top1 ->  91.89 ; top5 ->  98.5  and loss:  64.33473584242165
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.22210725484183058
test acc: top1 ->  91.84 ; top5 ->  98.62  and loss:  65.72079937532544
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.19281098851934075
test acc: top1 ->  91.93 ; top5 ->  98.58  and loss:  66.57727954536676
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.16745752026326954
test acc: top1 ->  91.89 ; top5 ->  98.6  and loss:  66.79369579814374
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.1687744716182351
test acc: top1 ->  91.9 ; top5 ->  98.61  and loss:  67.81522458977997
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.15871577376674395
test acc: top1 ->  91.92 ; top5 ->  98.61  and loss:  68.75674100592732
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.15086373926897068
test acc: top1 ->  91.95 ; top5 ->  98.61  and loss:  68.47097957134247
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.14867979660630226
test acc: top1 ->  91.92 ; top5 ->  98.55  and loss:  68.70459966734052
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.12520472539472394
test acc: top1 ->  92.01 ; top5 ->  98.54  and loss:  69.18003197014332
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  5 / 512 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -306.11905336380005 , diff:  306.11905336380005
adv train loss:  -305.5580253601074 , diff:  0.561028003692627
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  10
test acc: top1 ->  10.01 ; top5 ->  50.0  and loss:  278844.3439941406
forward train acc: top1 ->  73.83000000488282 ; top5 ->  97.328  and loss:  178.37224589288235
test acc: top1 ->  59.36 ; top5 ->  98.71  and loss:  169.9882150888443
forward train acc: top1 ->  98.71599998046875 ; top5 ->  100.0  and loss:  6.615343738347292
test acc: top1 ->  89.87 ; top5 ->  98.75  and loss:  63.900284834206104
forward train acc: top1 ->  99.38599997558593 ; top5 ->  99.998  and loss:  3.0629449179396033
test acc: top1 ->  90.77 ; top5 ->  98.92  and loss:  60.66956240683794
forward train acc: top1 ->  99.63999997558594 ; top5 ->  99.996  and loss:  1.672682230360806
test acc: top1 ->  91.02 ; top5 ->  98.94  and loss:  59.475000739097595
forward train acc: top1 ->  99.744 ; top5 ->  99.996  and loss:  1.257169543299824
test acc: top1 ->  91.31 ; top5 ->  98.95  and loss:  60.35434455424547
forward train acc: top1 ->  99.84 ; top5 ->  99.998  and loss:  0.8107935404404998
test acc: top1 ->  91.32 ; top5 ->  99.04  and loss:  59.90194587409496
forward train acc: top1 ->  99.84799997558594 ; top5 ->  100.0  and loss:  0.7492819239851087
test acc: top1 ->  91.38 ; top5 ->  99.02  and loss:  60.32675787061453
forward train acc: top1 ->  99.878 ; top5 ->  99.998  and loss:  0.6005935349967331
test acc: top1 ->  91.43 ; top5 ->  98.93  and loss:  61.194367587566376
forward train acc: top1 ->  99.866 ; top5 ->  100.0  and loss:  0.5975676339585334
test acc: top1 ->  91.45 ; top5 ->  98.95  and loss:  61.715603940188885
forward train acc: top1 ->  99.888 ; top5 ->  100.0  and loss:  0.5175214847549796
test acc: top1 ->  91.41 ; top5 ->  98.97  and loss:  60.73424664884806
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  11 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -2459.5981283187866 , diff:  2459.5981283187866
adv train loss:  -2546.133144378662 , diff:  86.53501605987549
adv train loss:  -2546.614091873169 , diff:  0.48094749450683594
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  1
test acc: top1 ->  10.0 ; top5 ->  57.37  and loss:  48779.4345703125
forward train acc: top1 ->  86.79800000732422 ; top5 ->  99.44  and loss:  88.93296567350626
test acc: top1 ->  89.19 ; top5 ->  97.71  and loss:  69.31648091971874
forward train acc: top1 ->  99.4620000024414 ; top5 ->  99.998  and loss:  2.958008885383606
test acc: top1 ->  90.18 ; top5 ->  98.04  and loss:  62.22489295899868
forward train acc: top1 ->  99.692 ; top5 ->  100.0  and loss:  1.7802848191931844
test acc: top1 ->  90.43 ; top5 ->  98.2  and loss:  60.46389289945364
forward train acc: top1 ->  99.804 ; top5 ->  100.0  and loss:  1.2517059687525034
test acc: top1 ->  90.81 ; top5 ->  98.2  and loss:  60.382179483771324
forward train acc: top1 ->  99.77 ; top5 ->  100.0  and loss:  1.1741788093931973
test acc: top1 ->  90.89 ; top5 ->  98.31  and loss:  60.220601208508015
forward train acc: top1 ->  99.846 ; top5 ->  99.998  and loss:  0.9423270132392645
test acc: top1 ->  90.94 ; top5 ->  98.3  and loss:  60.34633695334196
forward train acc: top1 ->  99.84199997558594 ; top5 ->  99.998  and loss:  0.9306008159182966
test acc: top1 ->  90.94 ; top5 ->  98.33  and loss:  60.437646351754665
forward train acc: top1 ->  99.842 ; top5 ->  100.0  and loss:  0.8575137774460018
test acc: top1 ->  90.97 ; top5 ->  98.3  and loss:  60.51805755496025
forward train acc: top1 ->  99.886 ; top5 ->  100.0  and loss:  0.7357393940910697
test acc: top1 ->  90.87 ; top5 ->  98.28  and loss:  60.4315639436245
forward train acc: top1 ->  99.87999997558593 ; top5 ->  99.998  and loss:  0.7212636014446616
test acc: top1 ->  90.94 ; top5 ->  98.31  and loss:  60.94618670642376
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  2 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -571.0377645541448 , diff:  571.0377645541448
adv train loss:  -1235.8017492294312 , diff:  664.7639846752863
adv train loss:  -1228.31946849823 , diff:  7.482280731201172
adv train loss:  -1228.291781425476 , diff:  0.02768707275390625
adv train loss:  -1230.5238637924194 , diff:  2.2320823669433594
adv train loss:  -1231.8427000045776 , diff:  1.3188362121582031
adv train loss:  -1227.1021003723145 , diff:  4.740599632263184
adv train loss:  -1228.5599975585938 , diff:  1.4578971862792969
adv train loss:  -1229.1525526046753 , diff:  0.592555046081543
adv train loss:  -1228.1454343795776 , diff:  1.0071182250976562
layer  12  adv train finish, try to retain  2
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  9561.131927490234
forward train acc: top1 ->  64.68199997070313 ; top5 ->  86.214  and loss:  296.7898653447628
test acc: top1 ->  77.79 ; top5 ->  98.47  and loss:  79.04753428697586
forward train acc: top1 ->  97.41799997558594 ; top5 ->  100.0  and loss:  21.28270500153303
test acc: top1 ->  88.9 ; top5 ->  98.29  and loss:  53.87035435438156
forward train acc: top1 ->  99.20999997802734 ; top5 ->  100.0  and loss:  9.823094762861729
test acc: top1 ->  88.82 ; top5 ->  98.33  and loss:  56.00285530090332
forward train acc: top1 ->  99.3540000048828 ; top5 ->  100.0  and loss:  6.68697077780962
test acc: top1 ->  89.02 ; top5 ->  98.32  and loss:  58.10504001379013
forward train acc: top1 ->  99.4660000024414 ; top5 ->  99.998  and loss:  5.050467297434807
test acc: top1 ->  89.25 ; top5 ->  98.3  and loss:  59.49416525661945
forward train acc: top1 ->  99.566 ; top5 ->  99.998  and loss:  4.2309043519198895
test acc: top1 ->  89.54 ; top5 ->  98.29  and loss:  60.09008485078812
forward train acc: top1 ->  99.56800000488282 ; top5 ->  99.996  and loss:  3.998503305017948
test acc: top1 ->  89.56 ; top5 ->  98.31  and loss:  60.615714371204376
forward train acc: top1 ->  99.61600000244141 ; top5 ->  99.998  and loss:  3.582527546212077
test acc: top1 ->  89.69 ; top5 ->  98.31  and loss:  60.56478798389435
forward train acc: top1 ->  99.59599997802735 ; top5 ->  100.0  and loss:  3.313034974038601
test acc: top1 ->  89.65 ; top5 ->  98.3  and loss:  61.457391396164894
forward train acc: top1 ->  99.6240000024414 ; top5 ->  100.0  and loss:  3.027216352522373
test acc: top1 ->  89.89 ; top5 ->  98.28  and loss:  61.303877741098404
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  5 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -14206.693786621094 , diff:  14206.693786621094
adv train loss:  -21895.856887817383 , diff:  7689.163101196289
adv train loss:  -29658.70492553711 , diff:  7762.848037719727
adv train loss:  -37417.1780090332 , diff:  7758.473083496094
adv train loss:  -45179.33297729492 , diff:  7762.154968261719
adv train loss:  -52934.892028808594 , diff:  7755.559051513672
adv train loss:  -60716.65643310547 , diff:  7781.764404296875
adv train loss:  -68518.0112915039 , diff:  7801.3548583984375
adv train loss:  -76353.45324707031 , diff:  7835.441955566406
adv train loss:  -84256.78662109375 , diff:  7903.3333740234375
layer  13  adv train finish, try to retain  4
test acc: top1 ->  8.04 ; top5 ->  50.0  and loss:  3658.8847274780273
forward train acc: top1 ->  16.073999998779296 ; top5 ->  51.634  and loss:  1316.2857859134674
test acc: top1 ->  21.45 ; top5 ->  59.66  and loss:  466.3893368244171
forward train acc: top1 ->  35.25599999145508 ; top5 ->  78.37799997802735  and loss:  247.57638227939606
test acc: top1 ->  38.52 ; top5 ->  84.57  and loss:  191.75299286842346
forward train acc: top1 ->  45.185999995117186 ; top5 ->  97.0200000024414  and loss:  137.20364594459534
test acc: top1 ->  46.05 ; top5 ->  95.9  and loss:  154.90675234794617
forward train acc: top1 ->  52.08799999389648 ; top5 ->  99.752  and loss:  122.05124044418335
test acc: top1 ->  47.61 ; top5 ->  96.2  and loss:  144.89712536334991
forward train acc: top1 ->  56.44999998779297 ; top5 ->  99.816  and loss:  114.24144470691681
test acc: top1 ->  52.02 ; top5 ->  96.55  and loss:  137.92580127716064
forward train acc: top1 ->  61.3139999975586 ; top5 ->  99.894  and loss:  108.5183265209198
test acc: top1 ->  53.13 ; top5 ->  96.64  and loss:  135.10364842414856
forward train acc: top1 ->  63.26200000976562 ; top5 ->  99.898  and loss:  105.38697552680969
test acc: top1 ->  55.36 ; top5 ->  96.86  and loss:  132.59276473522186
forward train acc: top1 ->  65.35999999267578 ; top5 ->  99.94199997558594  and loss:  102.18224263191223
test acc: top1 ->  57.3 ; top5 ->  96.87  and loss:  129.86475503444672
forward train acc: top1 ->  66.75600001464844 ; top5 ->  99.934  and loss:  99.58103150129318
test acc: top1 ->  57.58 ; top5 ->  97.01  and loss:  127.80361938476562
forward train acc: top1 ->  68.70400001464844 ; top5 ->  99.934  and loss:  96.55826449394226
test acc: top1 ->  59.89 ; top5 ->  97.14  and loss:  126.00275588035583
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  9 / 512 , inc:  1
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.671875  ==>  43 / 64 , inc:  1
layer  2  :  0.71875  ==>  92 / 128 , inc:  8
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.390625  ==>  100 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.072265625  ==>  37 / 512 , inc:  1
layer  9  :  0.009765625  ==>  5 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.009765625  ==>  5 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [1.7754727706748523, 4.7345940551329395, 2.3672970275664698, 1.7754727706748523, 1.7754727706748523, 1.7754727706748523, 1.7754727706748523, 0.8877363853374262, 1.1836485137832349, 1.1836485137832349, 0.9987034335046043, 1.1836485137832349, 1.3316045780061392, 9.469188110265879]  wait [1, 2, 0, 1, 1, 1, 0, 3, 4, 2, 3, 2, 2, 2]  inc [1, 1, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 11
$$$$$$$$$$$$$ epoch  81  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -2000.729655265808 , diff:  2000.729655265808
adv train loss:  -2083.1098461151123 , diff:  82.3801908493042
adv train loss:  -2091.261754989624 , diff:  8.151908874511719
adv train loss:  -2083.2594680786133 , diff:  8.002286911010742
adv train loss:  -2090.742036819458 , diff:  7.482568740844727
adv train loss:  -2097.0790634155273 , diff:  6.337026596069336
adv train loss:  -2093.809652328491 , diff:  3.269411087036133
adv train loss:  -2094.7244834899902 , diff:  0.9148311614990234
layer  0  adv train finish, try to retain  2
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  3179.0294094085693
forward train acc: top1 ->  47.76399998413086 ; top5 ->  83.52599997070313  and loss:  508.02598321437836
test acc: top1 ->  33.41 ; top5 ->  75.04  and loss:  372.12903809547424
forward train acc: top1 ->  57.87800000854492 ; top5 ->  89.88600001220703  and loss:  175.46722674369812
test acc: top1 ->  62.01 ; top5 ->  92.32  and loss:  145.9450216293335
forward train acc: top1 ->  62.56200000366211 ; top5 ->  92.15600000488281  and loss:  130.22421622276306
test acc: top1 ->  65.37 ; top5 ->  93.43  and loss:  118.69305831193924
forward train acc: top1 ->  65.47399998046875 ; top5 ->  93.24799999511718  and loss:  114.40067952871323
test acc: top1 ->  67.54 ; top5 ->  94.42  and loss:  108.77296513319016
forward train acc: top1 ->  67.10599999511719 ; top5 ->  94.03399999511718  and loss:  106.49560511112213
test acc: top1 ->  69.13 ; top5 ->  95.14  and loss:  101.00481247901917
forward train acc: top1 ->  68.77800000488281 ; top5 ->  94.604  and loss:  100.67088317871094
test acc: top1 ->  69.94 ; top5 ->  95.26  and loss:  98.34569334983826
forward train acc: top1 ->  69.44800000976562 ; top5 ->  94.70999997802734  and loss:  98.28362387418747
test acc: top1 ->  70.67 ; top5 ->  95.64  and loss:  96.68649965524673
forward train acc: top1 ->  70.46000001464844 ; top5 ->  95.14999999267579  and loss:  94.65378481149673
test acc: top1 ->  71.02 ; top5 ->  95.86  and loss:  93.88044899702072
forward train acc: top1 ->  71.23000000488281 ; top5 ->  95.49400001708985  and loss:  91.43662518262863
test acc: top1 ->  71.72 ; top5 ->  96.02  and loss:  91.81099563837051
forward train acc: top1 ->  71.85999997802735 ; top5 ->  95.71399998779297  and loss:  89.0918276309967
test acc: top1 ->  72.3 ; top5 ->  96.1  and loss:  90.00552827119827
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -15.776351988315582 , diff:  15.776351988315582
adv train loss:  -15.888004794716835 , diff:  0.11165280640125275
adv train loss:  -15.959602952003479 , diff:  0.07159815728664398
adv train loss:  -15.822594955563545 , diff:  0.13700799643993378
adv train loss:  -15.836841396987438 , diff:  0.014246441423892975
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  42
test acc: top1 ->  10.85 ; top5 ->  52.86  and loss:  406.47274589538574
forward train acc: top1 ->  99.61599997558594 ; top5 ->  99.998  and loss:  2.7061238097958267
test acc: top1 ->  91.55 ; top5 ->  99.08  and loss:  42.2580092959106
forward train acc: top1 ->  99.868 ; top5 ->  99.996  and loss:  0.5976406638510525
test acc: top1 ->  91.78 ; top5 ->  99.09  and loss:  46.35097745619714
forward train acc: top1 ->  99.9 ; top5 ->  100.0  and loss:  0.41011412581428885
test acc: top1 ->  91.72 ; top5 ->  99.0  and loss:  50.48711336031556
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.31253848946653306
test acc: top1 ->  91.96 ; top5 ->  99.03  and loss:  51.897559225559235
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.24678492231760174
test acc: top1 ->  92.02 ; top5 ->  99.03  and loss:  53.70616307668388
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.2162498343968764
test acc: top1 ->  91.91 ; top5 ->  98.96  and loss:  54.658555302768946
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.19746248581213877
test acc: top1 ->  91.88 ; top5 ->  98.97  and loss:  55.98818998783827
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.1502754925750196
test acc: top1 ->  91.83 ; top5 ->  98.97  and loss:  55.13560225442052
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.1696544197620824
test acc: top1 ->  91.97 ; top5 ->  99.0  and loss:  56.62197979912162
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.15014338235778268
test acc: top1 ->  91.93 ; top5 ->  99.02  and loss:  57.7565381526947
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  43 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -0.29334581457078457 , diff:  0.29334581457078457
adv train loss:  -0.2699221454677172 , diff:  0.02342366910306737
adv train loss:  -0.3997026098659262 , diff:  0.129780464398209
adv train loss:  -0.4046948228497058 , diff:  0.004992212983779609
layer  2  adv train finish, try to retain  74
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  950.2918381690979
forward train acc: top1 ->  99.00799997802734 ; top5 ->  99.972  and loss:  3.6420843526721
test acc: top1 ->  90.58 ; top5 ->  99.11  and loss:  56.3736967779696
forward train acc: top1 ->  99.29200000244141 ; top5 ->  99.99  and loss:  2.3491869904100895
test acc: top1 ->  90.9 ; top5 ->  98.95  and loss:  52.49793615937233
forward train acc: top1 ->  99.462 ; top5 ->  99.996  and loss:  1.6453352952376008
test acc: top1 ->  91.04 ; top5 ->  99.04  and loss:  52.45479741692543
forward train acc: top1 ->  99.528 ; top5 ->  99.996  and loss:  1.4563397862948477
test acc: top1 ->  90.93 ; top5 ->  98.99  and loss:  53.20505713671446
forward train acc: top1 ->  99.5520000024414 ; top5 ->  99.994  and loss:  1.4109066589735448
test acc: top1 ->  91.02 ; top5 ->  98.95  and loss:  54.23253537714481
forward train acc: top1 ->  99.63400000488281 ; top5 ->  99.994  and loss:  1.150410782545805
test acc: top1 ->  91.09 ; top5 ->  99.05  and loss:  53.61738062649965
forward train acc: top1 ->  99.66599997558593 ; top5 ->  99.994  and loss:  1.0085566146299243
test acc: top1 ->  90.9 ; top5 ->  99.05  and loss:  54.20425357669592
forward train acc: top1 ->  99.61799997558593 ; top5 ->  99.998  and loss:  1.1648271935991943
test acc: top1 ->  91.07 ; top5 ->  99.0  and loss:  52.41954879462719
forward train acc: top1 ->  99.6260000024414 ; top5 ->  99.998  and loss:  1.109324919525534
test acc: top1 ->  91.07 ; top5 ->  99.05  and loss:  52.12756533175707
forward train acc: top1 ->  99.7060000024414 ; top5 ->  99.998  and loss:  0.9337800890207291
test acc: top1 ->  91.11 ; top5 ->  99.04  and loss:  53.080641724169254
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  92 / 128 , inc:  8
---------------- start layer  3  ---------------
adv train loss:  -532.5249881711788 , diff:  532.5249881711788
adv train loss:  -1088.4527883529663 , diff:  555.9278001817875
adv train loss:  -1144.6053562164307 , diff:  56.152567863464355
adv train loss:  -1135.900818824768 , diff:  8.704537391662598
adv train loss:  -1128.7344903945923 , diff:  7.166328430175781
adv train loss:  -1139.271180152893 , diff:  10.536689758300781
adv train loss:  -1142.996132850647 , diff:  3.7249526977539062
adv train loss:  -1131.89098072052 , diff:  11.105152130126953
adv train loss:  -1151.8867483139038 , diff:  19.99576759338379
adv train loss:  -1161.8270654678345 , diff:  9.940317153930664
layer  3  adv train finish, try to retain  30
test acc: top1 ->  10.01 ; top5 ->  50.01  and loss:  1565.8630771636963
forward train acc: top1 ->  82.47599999755859 ; top5 ->  98.22199997802734  and loss:  65.4291881620884
test acc: top1 ->  79.14 ; top5 ->  97.52  and loss:  70.47679990530014
forward train acc: top1 ->  85.15199998535157 ; top5 ->  98.82400000488282  and loss:  46.09184792637825
test acc: top1 ->  81.15 ; top5 ->  97.94  and loss:  63.41123670339584
forward train acc: top1 ->  86.74799997558594 ; top5 ->  99.02400000244141  and loss:  40.773481011390686
test acc: top1 ->  82.19 ; top5 ->  98.15  and loss:  59.457900285720825
forward train acc: top1 ->  87.68799998291016 ; top5 ->  99.19000000244141  and loss:  37.75050097703934
test acc: top1 ->  83.05 ; top5 ->  98.21  and loss:  57.253991574048996
forward train acc: top1 ->  88.54800001464844 ; top5 ->  99.29600000488281  and loss:  34.776427656412125
test acc: top1 ->  83.49 ; top5 ->  98.28  and loss:  55.87950137257576
forward train acc: top1 ->  89.05999999755859 ; top5 ->  99.3640000024414  and loss:  33.02671101689339
test acc: top1 ->  83.85 ; top5 ->  98.33  and loss:  54.97463473677635
forward train acc: top1 ->  89.238 ; top5 ->  99.36999997802734  and loss:  32.48521128296852
test acc: top1 ->  84.02 ; top5 ->  98.31  and loss:  54.33246564865112
forward train acc: top1 ->  89.50800001464843 ; top5 ->  99.42399997558594  and loss:  31.755347162485123
test acc: top1 ->  84.17 ; top5 ->  98.35  and loss:  54.281466990709305
forward train acc: top1 ->  90.05399999511718 ; top5 ->  99.46199997802735  and loss:  30.50893299281597
test acc: top1 ->  84.48 ; top5 ->  98.49  and loss:  53.17668668925762
forward train acc: top1 ->  89.94999999023437 ; top5 ->  99.48199997802735  and loss:  30.191129565238953
test acc: top1 ->  84.51 ; top5 ->  98.48  and loss:  52.90515333414078
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -96.69984352961183 , diff:  96.69984352961183
adv train loss:  -487.5807857513428 , diff:  390.88094222173095
adv train loss:  -641.6276521682739 , diff:  154.04686641693115
adv train loss:  -751.8923916816711 , diff:  110.26473951339722
adv train loss:  -759.7116260528564 , diff:  7.819234371185303
adv train loss:  -764.9051742553711 , diff:  5.193548202514648
adv train loss:  -770.4256973266602 , diff:  5.5205230712890625
adv train loss:  -775.1977691650391 , diff:  4.772071838378906
adv train loss:  -794.4885125160217 , diff:  19.290743350982666
adv train loss:  -818.5751237869263 , diff:  24.08661127090454
layer  4  adv train finish, try to retain  23
test acc: top1 ->  10.0 ; top5 ->  47.64  and loss:  1592.641513824463
forward train acc: top1 ->  72.50199998046875 ; top5 ->  96.94200001220703  and loss:  81.95290452241898
test acc: top1 ->  73.59 ; top5 ->  97.2  and loss:  81.325479388237
forward train acc: top1 ->  77.52399999023437 ; top5 ->  98.06400000976562  and loss:  65.95182567834854
test acc: top1 ->  75.89 ; top5 ->  97.53  and loss:  74.07352325320244
forward train acc: top1 ->  79.49399999267578 ; top5 ->  98.32799997802735  and loss:  60.22203153371811
test acc: top1 ->  77.39 ; top5 ->  97.79  and loss:  69.39541521668434
forward train acc: top1 ->  80.71999999755859 ; top5 ->  98.48399998046875  and loss:  56.37285956740379
test acc: top1 ->  77.98 ; top5 ->  98.0  and loss:  67.19248884916306
forward train acc: top1 ->  81.48399998291016 ; top5 ->  98.67600000244141  and loss:  54.0702138543129
test acc: top1 ->  78.77 ; top5 ->  98.11  and loss:  64.74640101194382
forward train acc: top1 ->  82.14799997802734 ; top5 ->  98.75400000488281  and loss:  52.11635446548462
test acc: top1 ->  79.21 ; top5 ->  98.1  and loss:  64.02347984910011
forward train acc: top1 ->  82.74400000976563 ; top5 ->  98.83000000244141  and loss:  50.69164228439331
test acc: top1 ->  79.23 ; top5 ->  98.24  and loss:  62.8014677464962
forward train acc: top1 ->  82.9979999975586 ; top5 ->  98.83200000488281  and loss:  49.630142509937286
test acc: top1 ->  79.79 ; top5 ->  98.29  and loss:  61.92391201853752
forward train acc: top1 ->  83.03599998046874 ; top5 ->  98.90200000244141  and loss:  49.14933314919472
test acc: top1 ->  79.82 ; top5 ->  98.29  and loss:  61.270478934049606
forward train acc: top1 ->  83.31400001708984 ; top5 ->  98.86199997802734  and loss:  48.618571400642395
test acc: top1 ->  80.02 ; top5 ->  98.32  and loss:  60.556925028562546
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -116.95291146636009 , diff:  116.95291146636009
adv train loss:  -623.7855381965637 , diff:  506.83262673020363
adv train loss:  -829.8879542350769 , diff:  206.10241603851318
adv train loss:  -848.4297075271606 , diff:  18.54175329208374
adv train loss:  -883.507435798645 , diff:  35.077728271484375
adv train loss:  -890.1908369064331 , diff:  6.683401107788086
adv train loss:  -890.1036081314087 , diff:  0.08722877502441406
adv train loss:  -888.0097513198853 , diff:  2.0938568115234375
adv train loss:  -885.9847278594971 , diff:  2.0250234603881836
adv train loss:  -889.7829780578613 , diff:  3.798250198364258
layer  5  adv train finish, try to retain  24
test acc: top1 ->  10.24 ; top5 ->  50.6  and loss:  792.3202576637268
forward train acc: top1 ->  78.89399997070312 ; top5 ->  97.85599997558593  and loss:  64.12122058868408
test acc: top1 ->  78.76 ; top5 ->  97.81  and loss:  67.22463405132294
forward train acc: top1 ->  84.43799999023437 ; top5 ->  99.02599997802734  and loss:  45.95048174262047
test acc: top1 ->  81.56 ; top5 ->  98.32  and loss:  59.81005302071571
forward train acc: top1 ->  86.66199999511718 ; top5 ->  99.264  and loss:  39.67930844426155
test acc: top1 ->  82.71 ; top5 ->  98.49  and loss:  56.515741378068924
forward train acc: top1 ->  87.77799999511718 ; top5 ->  99.43400000244141  and loss:  36.11122453212738
test acc: top1 ->  83.84 ; top5 ->  98.68  and loss:  53.78505724668503
forward train acc: top1 ->  88.66200001708984 ; top5 ->  99.50199997558593  and loss:  33.55163446068764
test acc: top1 ->  84.13 ; top5 ->  98.67  and loss:  52.42270091176033
forward train acc: top1 ->  89.16199997314453 ; top5 ->  99.49199997802734  and loss:  31.811037093400955
test acc: top1 ->  84.48 ; top5 ->  98.7  and loss:  51.339703768491745
forward train acc: top1 ->  89.584 ; top5 ->  99.55199997558594  and loss:  30.66833008825779
test acc: top1 ->  84.7 ; top5 ->  98.85  and loss:  51.10010150074959
forward train acc: top1 ->  89.96000001708984 ; top5 ->  99.55399997558594  and loss:  29.825364008545876
test acc: top1 ->  84.76 ; top5 ->  98.76  and loss:  50.705571949481964
forward train acc: top1 ->  89.96800001220703 ; top5 ->  99.57400000244141  and loss:  29.504543006420135
test acc: top1 ->  85.01 ; top5 ->  98.81  and loss:  49.96934017539024
forward train acc: top1 ->  90.16799998535156 ; top5 ->  99.54199997558594  and loss:  28.84523396193981
test acc: top1 ->  84.97 ; top5 ->  98.83  and loss:  49.47800010442734
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -164.86650605499744 , diff:  164.86650605499744
adv train loss:  -764.7263579368591 , diff:  599.8598518818617
adv train loss:  -957.9727191925049 , diff:  193.24636125564575
adv train loss:  -986.8490886688232 , diff:  28.87636947631836
adv train loss:  -986.0178260803223 , diff:  0.8312625885009766
adv train loss:  -985.8206481933594 , diff:  0.19717788696289062
adv train loss:  -1005.2267656326294 , diff:  19.40611743927002
adv train loss:  -1027.890055656433 , diff:  22.66329002380371
adv train loss:  -1029.3446140289307 , diff:  1.4545583724975586
adv train loss:  -1028.7278060913086 , diff:  0.6168079376220703
layer  6  adv train finish, try to retain  12
test acc: top1 ->  12.79 ; top5 ->  54.25  and loss:  606.0558466911316
forward train acc: top1 ->  85.0699999975586 ; top5 ->  99.31199997558593  and loss:  43.98738345503807
test acc: top1 ->  82.65 ; top5 ->  98.7  and loss:  57.97941169142723
forward train acc: top1 ->  90.72999997070312 ; top5 ->  99.72199997558593  and loss:  27.034369125962257
test acc: top1 ->  84.7 ; top5 ->  98.86  and loss:  52.43216893076897
forward train acc: top1 ->  92.4159999975586 ; top5 ->  99.796  and loss:  22.102470248937607
test acc: top1 ->  85.73 ; top5 ->  98.96  and loss:  50.977737814188
forward train acc: top1 ->  93.27199997070312 ; top5 ->  99.85  and loss:  19.1043319106102
test acc: top1 ->  86.44 ; top5 ->  99.1  and loss:  49.55776619911194
forward train acc: top1 ->  94.20999999023438 ; top5 ->  99.846  and loss:  17.158384814858437
test acc: top1 ->  86.73 ; top5 ->  99.07  and loss:  48.97918464243412
forward train acc: top1 ->  94.61599999267578 ; top5 ->  99.872  and loss:  15.702626913785934
test acc: top1 ->  86.72 ; top5 ->  99.11  and loss:  49.22227028012276
forward train acc: top1 ->  94.94400001708985 ; top5 ->  99.904  and loss:  14.596047155559063
test acc: top1 ->  87.26 ; top5 ->  99.15  and loss:  48.06319469213486
forward train acc: top1 ->  95.19600001953125 ; top5 ->  99.892  and loss:  14.046970292925835
test acc: top1 ->  87.37 ; top5 ->  99.11  and loss:  47.747227773070335
forward train acc: top1 ->  95.23200001708985 ; top5 ->  99.904  and loss:  13.872243121266365
test acc: top1 ->  87.43 ; top5 ->  99.21  and loss:  47.80333785712719
forward train acc: top1 ->  95.38999999267578 ; top5 ->  99.922  and loss:  13.275707125663757
test acc: top1 ->  87.61 ; top5 ->  99.15  and loss:  47.94411809742451
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  100 / 256 , inc:  1
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
### skip layer  8 wait:  4  ###
---------------- start layer  9  ---------------
adv train loss:  -1502.2424974441528 , diff:  1502.2424974441528
adv train loss:  -1507.6575241088867 , diff:  5.415026664733887
adv train loss:  -1508.73752784729 , diff:  1.0800037384033203
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  4
test acc: top1 ->  10.02 ; top5 ->  50.06  and loss:  1985.4943752288818
forward train acc: top1 ->  97.662 ; top5 ->  99.986  and loss:  10.896811770275235
test acc: top1 ->  84.13 ; top5 ->  98.2  and loss:  59.07979243993759
forward train acc: top1 ->  99.616 ; top5 ->  99.992  and loss:  1.9141066372394562
test acc: top1 ->  91.68 ; top5 ->  98.76  and loss:  41.1879795268178
forward train acc: top1 ->  99.72599997802735 ; top5 ->  100.0  and loss:  1.1607412807643414
test acc: top1 ->  91.63 ; top5 ->  98.8  and loss:  43.916929833590984
forward train acc: top1 ->  99.748 ; top5 ->  99.998  and loss:  0.9119137560483068
test acc: top1 ->  91.74 ; top5 ->  98.77  and loss:  45.075808208435774
forward train acc: top1 ->  99.844 ; top5 ->  100.0  and loss:  0.6046844469383359
test acc: top1 ->  91.63 ; top5 ->  98.83  and loss:  47.45560522004962
forward train acc: top1 ->  99.818 ; top5 ->  99.998  and loss:  0.5883072407450527
test acc: top1 ->  91.85 ; top5 ->  98.81  and loss:  47.86281327903271
forward train acc: top1 ->  99.874 ; top5 ->  99.998  and loss:  0.490354425041005
test acc: top1 ->  91.81 ; top5 ->  98.79  and loss:  48.276241950690746
forward train acc: top1 ->  99.88799997558594 ; top5 ->  100.0  and loss:  0.4319195169955492
test acc: top1 ->  91.89 ; top5 ->  98.84  and loss:  48.810408379882574
forward train acc: top1 ->  99.90799997558594 ; top5 ->  100.0  and loss:  0.3990909936837852
test acc: top1 ->  91.81 ; top5 ->  98.77  and loss:  49.69503369927406
forward train acc: top1 ->  99.88799997558594 ; top5 ->  100.0  and loss:  0.40356504172086716
test acc: top1 ->  91.84 ; top5 ->  98.76  and loss:  50.15260145813227
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  5 / 512 , inc:  1
---------------- start layer  10  ---------------
### skip layer  10 wait:  3  ###
---------------- start layer  11  ---------------
adv train loss:  -1687.7021207809448 , diff:  1687.7021207809448
adv train loss:  -1688.032148361206 , diff:  0.33002758026123047
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  1
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  40873.64099121094
forward train acc: top1 ->  93.66399997558594 ; top5 ->  99.914  and loss:  23.78256541863084
test acc: top1 ->  88.15 ; top5 ->  98.36  and loss:  50.74296127259731
forward train acc: top1 ->  99.486 ; top5 ->  100.0  and loss:  3.7417582627385855
test acc: top1 ->  90.75 ; top5 ->  98.67  and loss:  45.81093081086874
forward train acc: top1 ->  99.64199997558593 ; top5 ->  100.0  and loss:  2.2350437426939607
test acc: top1 ->  91.02 ; top5 ->  98.68  and loss:  46.7808288782835
forward train acc: top1 ->  99.738 ; top5 ->  100.0  and loss:  1.535424955189228
test acc: top1 ->  91.14 ; top5 ->  98.74  and loss:  48.721381299197674
forward train acc: top1 ->  99.81599997558594 ; top5 ->  100.0  and loss:  1.0312907923944294
test acc: top1 ->  91.4 ; top5 ->  98.85  and loss:  50.97495758533478
forward train acc: top1 ->  99.84799997558594 ; top5 ->  100.0  and loss:  0.8067644755356014
test acc: top1 ->  91.34 ; top5 ->  98.82  and loss:  53.30950245633721
forward train acc: top1 ->  99.858 ; top5 ->  100.0  and loss:  0.6392170372419059
test acc: top1 ->  91.43 ; top5 ->  98.91  and loss:  55.11109987646341
forward train acc: top1 ->  99.892 ; top5 ->  100.0  and loss:  0.5495968696195632
test acc: top1 ->  91.42 ; top5 ->  98.9  and loss:  55.54876370728016
forward train acc: top1 ->  99.91 ; top5 ->  99.998  and loss:  0.49010072043165565
test acc: top1 ->  91.54 ; top5 ->  98.91  and loss:  57.2023356333375
forward train acc: top1 ->  99.88399997558594 ; top5 ->  100.0  and loss:  0.48860461078584194
test acc: top1 ->  91.69 ; top5 ->  98.85  and loss:  58.54358629323542
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  2 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -1368.5151495933533 , diff:  1368.5151495933533
adv train loss:  -1601.8074216842651 , diff:  233.29227209091187
adv train loss:  -2013.5202188491821 , diff:  411.712797164917
adv train loss:  -2068.0109252929688 , diff:  54.49070644378662
adv train loss:  -2067.844024658203 , diff:  0.166900634765625
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  4
test acc: top1 ->  18.6 ; top5 ->  56.99  and loss:  77293.0287475586
forward train acc: top1 ->  92.86599997558594 ; top5 ->  99.996  and loss:  37.83457896113396
test acc: top1 ->  90.77 ; top5 ->  99.19  and loss:  58.347445987164974
forward train acc: top1 ->  99.66199998046875 ; top5 ->  100.0  and loss:  1.517503671348095
test acc: top1 ->  91.01 ; top5 ->  99.3  and loss:  56.91223219782114
forward train acc: top1 ->  99.81599997558594 ; top5 ->  100.0  and loss:  1.0230709733441472
test acc: top1 ->  91.13 ; top5 ->  99.34  and loss:  56.35414058342576
forward train acc: top1 ->  99.80999997558594 ; top5 ->  100.0  and loss:  0.830259425798431
test acc: top1 ->  91.28 ; top5 ->  99.33  and loss:  56.85941097140312
forward train acc: top1 ->  99.866 ; top5 ->  100.0  and loss:  0.649546692147851
test acc: top1 ->  91.43 ; top5 ->  99.35  and loss:  56.48591362684965
forward train acc: top1 ->  99.85599997558593 ; top5 ->  100.0  and loss:  0.5952660979237407
test acc: top1 ->  91.37 ; top5 ->  99.35  and loss:  56.785010401159525
forward train acc: top1 ->  99.906 ; top5 ->  100.0  and loss:  0.4902098069433123
test acc: top1 ->  91.46 ; top5 ->  99.33  and loss:  56.769003765657544
forward train acc: top1 ->  99.90599997558594 ; top5 ->  100.0  and loss:  0.4259475985309109
test acc: top1 ->  91.51 ; top5 ->  99.33  and loss:  57.31048148870468
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.4347182053606957
test acc: top1 ->  91.5 ; top5 ->  99.33  and loss:  56.736404383555055
forward train acc: top1 ->  99.9040000024414 ; top5 ->  100.0  and loss:  0.4165746009675786
test acc: top1 ->  91.46 ; top5 ->  99.33  and loss:  57.217869840562344
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  5 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -8057.179302215576 , diff:  8057.179302215576
adv train loss:  -12682.260711669922 , diff:  4625.081409454346
adv train loss:  -16874.636032104492 , diff:  4192.37532043457
adv train loss:  -20996.010955810547 , diff:  4121.374923706055
adv train loss:  -25092.036010742188 , diff:  4096.025054931641
adv train loss:  -29182.508178710938 , diff:  4090.47216796875
adv train loss:  -33272.88232421875 , diff:  4090.3741455078125
adv train loss:  -37373.95556640625 , diff:  4101.0732421875
adv train loss:  -40311.5334777832 , diff:  2937.577911376953
adv train loss:  -41140.1259765625 , diff:  828.5924987792969
layer  13  adv train finish, try to retain  22
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.671875  ==>  43 / 64 , inc:  1
layer  2  :  0.71875  ==>  92 / 128 , inc:  4
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.390625  ==>  100 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.072265625  ==>  37 / 512 , inc:  1
layer  9  :  0.009765625  ==>  5 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.009765625  ==>  5 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [1.3316045780061392, 3.5509455413497046, 1.7754727706748523, 1.3316045780061392, 1.3316045780061392, 1.3316045780061392, 1.3316045780061392, 0.8877363853374262, 1.1836485137832349, 0.8877363853374262, 0.9987034335046043, 0.8877363853374262, 0.9987034335046043, 18.938376220531758]  wait [3, 4, 2, 3, 3, 3, 2, 2, 3, 4, 2, 4, 4, 2]  inc [1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 11
$$$$$$$$$$$$$ epoch  82  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1162.092558503151 , diff:  1162.092558503151
adv train loss:  -1255.1331644058228 , diff:  93.04060590267181
adv train loss:  -1245.3355655670166 , diff:  9.797598838806152
adv train loss:  -1249.61563205719 , diff:  4.28006649017334
adv train loss:  -1240.7212600708008 , diff:  8.89437198638916
adv train loss:  -1242.8160982131958 , diff:  2.0948381423950195
adv train loss:  -1246.9055528640747 , diff:  4.089454650878906
adv train loss:  -1249.053415298462 , diff:  2.147862434387207
adv train loss:  -1243.349669456482 , diff:  5.7037458419799805
adv train loss:  -1253.1956901550293 , diff:  9.846020698547363
layer  0  adv train finish, try to retain  13
test acc: top1 ->  11.25 ; top5 ->  53.17  and loss:  1503.9103507995605
forward train acc: top1 ->  76.176 ; top5 ->  96.27799998779297  and loss:  144.00354999303818
test acc: top1 ->  70.27 ; top5 ->  95.13  and loss:  131.36852741241455
forward train acc: top1 ->  80.2780000024414 ; top5 ->  97.38000000976562  and loss:  67.99186995625496
test acc: top1 ->  79.03 ; top5 ->  97.2  and loss:  77.56741577386856
forward train acc: top1 ->  83.15199998291016 ; top5 ->  98.09799997802735  and loss:  55.37955364584923
test acc: top1 ->  80.21 ; top5 ->  97.56  and loss:  70.69325390458107
forward train acc: top1 ->  84.99399998535156 ; top5 ->  98.51400001220703  and loss:  48.440078377723694
test acc: top1 ->  81.74 ; top5 ->  97.71  and loss:  65.22246459126472
forward train acc: top1 ->  86.51200001953126 ; top5 ->  98.71999997802735  and loss:  43.89640247821808
test acc: top1 ->  82.32 ; top5 ->  97.99  and loss:  61.58367842435837
forward train acc: top1 ->  87.19800001220703 ; top5 ->  98.87799998046874  and loss:  40.37144497036934
test acc: top1 ->  82.93 ; top5 ->  97.95  and loss:  59.7384235560894
forward train acc: top1 ->  87.95400000976562 ; top5 ->  99.06199997802734  and loss:  37.91579034924507
test acc: top1 ->  83.05 ; top5 ->  98.21  and loss:  59.17638090252876
forward train acc: top1 ->  88.51000001708984 ; top5 ->  99.09199997802735  and loss:  36.45895755290985
test acc: top1 ->  83.76 ; top5 ->  98.23  and loss:  57.57461550831795
forward train acc: top1 ->  89.17599997314453 ; top5 ->  99.15200000488281  and loss:  34.08351573348045
test acc: top1 ->  83.91 ; top5 ->  98.29  and loss:  56.72769618034363
forward train acc: top1 ->  89.47400001953125 ; top5 ->  99.19799997558594  and loss:  33.07419994473457
test acc: top1 ->  84.42 ; top5 ->  98.32  and loss:  56.18699213862419
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -4.166963893920183 , diff:  4.166963893920183
adv train loss:  -4.244411263614893 , diff:  0.07744736969470978
adv train loss:  -4.165037345141172 , diff:  0.07937391847372055
adv train loss:  -4.146646596491337 , diff:  0.018390748649835587
adv train loss:  -4.086832256987691 , diff:  0.0598143395036459
adv train loss:  -4.1015334986150265 , diff:  0.014701241627335548
adv train loss:  -4.054865960031748 , diff:  0.046667538583278656
adv train loss:  -4.141628574579954 , diff:  0.08676261454820633
adv train loss:  -4.198112025856972 , diff:  0.05648345127701759
adv train loss:  -4.115558099001646 , diff:  0.0825539268553257
layer  1  adv train finish, try to retain  43
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -5.418911922723055 , diff:  5.418911922723055
adv train loss:  -5.66570420563221 , diff:  0.2467922829091549
adv train loss:  -5.759377151727676 , diff:  0.09367294609546661
adv train loss:  -5.731556564569473 , diff:  0.027820587158203125
adv train loss:  -5.743747230619192 , diff:  0.012190666049718857
adv train loss:  -5.70898700132966 , diff:  0.03476022928953171
adv train loss:  -5.576289422810078 , diff:  0.13269757851958275
adv train loss:  -5.528356432914734 , diff:  0.04793298989534378
adv train loss:  -5.579025901854038 , diff:  0.05066946893930435
adv train loss:  -5.647875472903252 , diff:  0.06884957104921341
layer  2  adv train finish, try to retain  92
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -354.7459454871714 , diff:  354.7459454871714
adv train loss:  -601.6631269454956 , diff:  246.9171814583242
adv train loss:  -608.3401598930359 , diff:  6.677032947540283
adv train loss:  -620.3306879997253 , diff:  11.990528106689453
adv train loss:  -627.7555046081543 , diff:  7.424816608428955
adv train loss:  -630.031843662262 , diff:  2.276339054107666
adv train loss:  -631.6255550384521 , diff:  1.5937113761901855
adv train loss:  -632.395875453949 , diff:  0.7703204154968262
adv train loss:  -636.5287327766418 , diff:  4.132857322692871
adv train loss:  -631.8403630256653 , diff:  4.6883697509765625
layer  3  adv train finish, try to retain  63
test acc: top1 ->  27.83 ; top5 ->  74.62  and loss:  392.94797801971436
forward train acc: top1 ->  97.53799998046875 ; top5 ->  99.944  and loss:  7.691890539601445
test acc: top1 ->  89.96 ; top5 ->  99.11  and loss:  43.1242221146822
forward train acc: top1 ->  98.37599998535157 ; top5 ->  99.98  and loss:  4.9622353333979845
test acc: top1 ->  90.31 ; top5 ->  99.1  and loss:  45.08132591098547
forward train acc: top1 ->  98.64799998535156 ; top5 ->  99.984  and loss:  4.003401041030884
test acc: top1 ->  90.42 ; top5 ->  99.08  and loss:  47.38905371725559
forward train acc: top1 ->  98.74999998046874 ; top5 ->  99.988  and loss:  3.6945838071405888
test acc: top1 ->  90.5 ; top5 ->  99.06  and loss:  46.15993557870388
forward train acc: top1 ->  98.8400000024414 ; top5 ->  99.988  and loss:  3.4614017056301236
test acc: top1 ->  90.63 ; top5 ->  99.21  and loss:  45.39244365692139
forward train acc: top1 ->  98.94000000488282 ; top5 ->  99.992  and loss:  3.0586077077314258
test acc: top1 ->  90.72 ; top5 ->  99.17  and loss:  46.037887051701546
forward train acc: top1 ->  99.0320000024414 ; top5 ->  99.996  and loss:  2.767413968220353
test acc: top1 ->  90.73 ; top5 ->  99.15  and loss:  46.50748363137245
forward train acc: top1 ->  99.1140000024414 ; top5 ->  99.996  and loss:  2.6179279210045934
test acc: top1 ->  90.8 ; top5 ->  99.22  and loss:  46.717125698924065
forward train acc: top1 ->  99.11200000488282 ; top5 ->  99.992  and loss:  2.669443255290389
test acc: top1 ->  90.66 ; top5 ->  99.15  and loss:  46.63425878435373
forward train acc: top1 ->  99.14400000488281 ; top5 ->  99.996  and loss:  2.6540879644453526
test acc: top1 ->  90.81 ; top5 ->  99.29  and loss:  45.81313472986221
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -174.64035788085312 , diff:  174.64035788085312
adv train loss:  -1050.978723526001 , diff:  876.3383656451479
adv train loss:  -1154.9662351608276 , diff:  103.98751163482666
adv train loss:  -1165.198450088501 , diff:  10.23221492767334
adv train loss:  -1185.539436340332 , diff:  20.340986251831055
adv train loss:  -1172.9023122787476 , diff:  12.637124061584473
adv train loss:  -1197.292646408081 , diff:  24.390334129333496
adv train loss:  -1249.5810451507568 , diff:  52.28839874267578
adv train loss:  -1335.4864110946655 , diff:  85.90536594390869
adv train loss:  -1372.8045597076416 , diff:  37.318148612976074
layer  4  adv train finish, try to retain  71
test acc: top1 ->  31.55 ; top5 ->  83.45  and loss:  1204.1515398025513
forward train acc: top1 ->  93.08799999511719 ; top5 ->  99.702  and loss:  22.714069917798042
test acc: top1 ->  87.32 ; top5 ->  98.74  and loss:  47.28607106208801
forward train acc: top1 ->  94.10999999267578 ; top5 ->  99.78799997558593  and loss:  17.697666324675083
test acc: top1 ->  87.83 ; top5 ->  98.99  and loss:  44.84627556800842
forward train acc: top1 ->  94.70199997314454 ; top5 ->  99.856  and loss:  15.776074908673763
test acc: top1 ->  88.16 ; top5 ->  98.94  and loss:  44.28552830219269
forward train acc: top1 ->  95.04399997558593 ; top5 ->  99.866  and loss:  14.428100191056728
test acc: top1 ->  88.48 ; top5 ->  99.03  and loss:  43.86133731901646
forward train acc: top1 ->  95.13599999023438 ; top5 ->  99.86  and loss:  14.408920787274837
test acc: top1 ->  88.62 ; top5 ->  99.04  and loss:  42.88915924727917
forward train acc: top1 ->  95.64999999267579 ; top5 ->  99.896  and loss:  13.102258510887623
test acc: top1 ->  88.72 ; top5 ->  98.94  and loss:  43.226265363395214
forward train acc: top1 ->  95.63400001708985 ; top5 ->  99.878  and loss:  13.064009375870228
test acc: top1 ->  88.79 ; top5 ->  99.11  and loss:  42.99030591547489
forward train acc: top1 ->  95.70999999267578 ; top5 ->  99.87999997558593  and loss:  12.280685536563396
test acc: top1 ->  88.6 ; top5 ->  99.08  and loss:  43.58272045850754
forward train acc: top1 ->  95.94799998779297 ; top5 ->  99.9  and loss:  11.884767711162567
test acc: top1 ->  88.83 ; top5 ->  99.11  and loss:  42.6319904923439
forward train acc: top1 ->  95.79400001220704 ; top5 ->  99.918  and loss:  12.104719541966915
test acc: top1 ->  88.93 ; top5 ->  99.1  and loss:  42.940238028764725
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -192.73489438463002 , diff:  192.73489438463002
adv train loss:  -1100.974624633789 , diff:  908.239730249159
adv train loss:  -1433.801181793213 , diff:  332.8265571594238
adv train loss:  -1450.0080871582031 , diff:  16.206905364990234
adv train loss:  -1458.0999917984009 , diff:  8.091904640197754
adv train loss:  -1454.641466140747 , diff:  3.4585256576538086
adv train loss:  -1455.5296201705933 , diff:  0.8881540298461914
adv train loss:  -1455.0699853897095 , diff:  0.45963478088378906
adv train loss:  -1456.6531190872192 , diff:  1.5831336975097656
adv train loss:  -1459.4082679748535 , diff:  2.7551488876342773
layer  5  adv train finish, try to retain  71
test acc: top1 ->  28.57 ; top5 ->  75.96  and loss:  920.8279023170471
forward train acc: top1 ->  94.92400001464844 ; top5 ->  99.896  and loss:  14.922209829092026
test acc: top1 ->  87.98 ; top5 ->  99.27  and loss:  44.545238107442856
forward train acc: top1 ->  96.31200001464843 ; top5 ->  99.932  and loss:  10.772563479840755
test acc: top1 ->  88.33 ; top5 ->  99.3  and loss:  43.89724387228489
forward train acc: top1 ->  96.92199998535156 ; top5 ->  99.974  and loss:  8.79756399616599
test acc: top1 ->  88.81 ; top5 ->  99.34  and loss:  43.58020339906216
forward train acc: top1 ->  97.25199998046875 ; top5 ->  99.956  and loss:  7.9927621483802795
test acc: top1 ->  89.03 ; top5 ->  99.32  and loss:  43.55843801796436
forward train acc: top1 ->  97.40799998779296 ; top5 ->  99.96199997558594  and loss:  7.496398903429508
test acc: top1 ->  89.16 ; top5 ->  99.26  and loss:  43.95058062672615
forward train acc: top1 ->  97.56400000976562 ; top5 ->  99.98  and loss:  6.956018693745136
test acc: top1 ->  89.5 ; top5 ->  99.38  and loss:  42.67046648263931
forward train acc: top1 ->  97.65799998535157 ; top5 ->  99.978  and loss:  6.629147883504629
test acc: top1 ->  89.51 ; top5 ->  99.35  and loss:  43.24360474944115
forward train acc: top1 ->  97.73000000732422 ; top5 ->  99.98  and loss:  6.443773217499256
test acc: top1 ->  89.38 ; top5 ->  99.29  and loss:  43.17384630441666
forward train acc: top1 ->  97.77000001220703 ; top5 ->  99.97  and loss:  6.489886157214642
test acc: top1 ->  89.69 ; top5 ->  99.37  and loss:  42.5678605735302
forward train acc: top1 ->  97.85400000732422 ; top5 ->  99.978  and loss:  6.160275427624583
test acc: top1 ->  89.79 ; top5 ->  99.34  and loss:  42.921143010258675
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -370.4471403984353 , diff:  370.4471403984353
adv train loss:  -1411.9670581817627 , diff:  1041.5199177833274
adv train loss:  -1455.8500471115112 , diff:  43.882988929748535
adv train loss:  -1477.2922592163086 , diff:  21.442212104797363
adv train loss:  -1476.1652164459229 , diff:  1.1270427703857422
adv train loss:  -1482.6371774673462 , diff:  6.47196102142334
adv train loss:  -1496.6108722686768 , diff:  13.973694801330566
adv train loss:  -1509.1898050308228 , diff:  12.578932762145996
adv train loss:  -1496.6497030258179 , diff:  12.540102005004883
adv train loss:  -1493.6632280349731 , diff:  2.9864749908447266
layer  6  adv train finish, try to retain  40
test acc: top1 ->  68.33 ; top5 ->  95.23  and loss:  448.20917296409607
forward train acc: top1 ->  98.01599998291016 ; top5 ->  99.988  and loss:  5.975083991885185
test acc: top1 ->  90.03 ; top5 ->  99.37  and loss:  44.66355734318495
forward train acc: top1 ->  98.66599997802734 ; top5 ->  99.994  and loss:  3.677303485572338
test acc: top1 ->  90.41 ; top5 ->  99.35  and loss:  44.63446372002363
forward train acc: top1 ->  99.09799997558594 ; top5 ->  99.996  and loss:  2.770924241282046
test acc: top1 ->  90.59 ; top5 ->  99.25  and loss:  46.1307919472456
forward train acc: top1 ->  99.18800000732422 ; top5 ->  99.998  and loss:  2.294585784897208
test acc: top1 ->  90.76 ; top5 ->  99.26  and loss:  47.063536293804646
forward train acc: top1 ->  99.3340000024414 ; top5 ->  99.998  and loss:  2.0130185806192458
test acc: top1 ->  90.98 ; top5 ->  99.22  and loss:  48.74615287780762
forward train acc: top1 ->  99.34199997802735 ; top5 ->  100.0  and loss:  1.889837152324617
test acc: top1 ->  90.86 ; top5 ->  99.3  and loss:  48.21842474490404
forward train acc: top1 ->  99.41599998046875 ; top5 ->  100.0  and loss:  1.6854281891137362
test acc: top1 ->  91.02 ; top5 ->  99.3  and loss:  48.19500809907913
forward train acc: top1 ->  99.42199997558593 ; top5 ->  100.0  and loss:  1.5386227006092668
test acc: top1 ->  91.15 ; top5 ->  99.32  and loss:  48.44007362052798
forward train acc: top1 ->  99.44399997558594 ; top5 ->  99.994  and loss:  1.6237201923504472
test acc: top1 ->  90.87 ; top5 ->  99.3  and loss:  49.31773615628481
forward train acc: top1 ->  99.49999997802735 ; top5 ->  99.998  and loss:  1.4515131362713873
test acc: top1 ->  91.05 ; top5 ->  99.26  and loss:  49.192770443856716
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  100 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -0.7945012759882957 , diff:  0.7945012759882957
adv train loss:  -0.7423951486125588 , diff:  0.05210612737573683
adv train loss:  -0.7866058899089694 , diff:  0.04421074129641056
adv train loss:  -0.7576038469560444 , diff:  0.029002042952924967
adv train loss:  -0.7766307431738824 , diff:  0.01902689621783793
adv train loss:  -0.7910286276601255 , diff:  0.014397884486243129
adv train loss:  -0.8007564293220639 , diff:  0.009727801661938429
layer  7  adv train finish, try to retain  259
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -29.385885447263718 , diff:  29.385885447263718
adv train loss:  -29.33488494157791 , diff:  0.051000505685806274
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  36
test acc: top1 ->  10.98 ; top5 ->  49.55  and loss:  5965362.33203125
forward train acc: top1 ->  95.942 ; top5 ->  99.742  and loss:  14.538245059549809
test acc: top1 ->  90.94 ; top5 ->  99.38  and loss:  39.8849236369133
forward train acc: top1 ->  99.694 ; top5 ->  100.0  and loss:  1.3855364648625255
test acc: top1 ->  91.2 ; top5 ->  99.4  and loss:  41.99445030838251
forward train acc: top1 ->  99.7780000024414 ; top5 ->  100.0  and loss:  0.9293036093004048
test acc: top1 ->  91.28 ; top5 ->  99.46  and loss:  43.50046860426664
forward train acc: top1 ->  99.81600000244141 ; top5 ->  100.0  and loss:  0.7170327056664973
test acc: top1 ->  91.4 ; top5 ->  99.42  and loss:  45.3165528178215
forward train acc: top1 ->  99.876 ; top5 ->  100.0  and loss:  0.5313251317711547
test acc: top1 ->  91.64 ; top5 ->  99.41  and loss:  45.94438133388758
forward train acc: top1 ->  99.894 ; top5 ->  100.0  and loss:  0.4332183329970576
test acc: top1 ->  91.58 ; top5 ->  99.37  and loss:  46.904242768883705
forward train acc: top1 ->  99.906 ; top5 ->  100.0  and loss:  0.3447235382045619
test acc: top1 ->  91.61 ; top5 ->  99.41  and loss:  47.716155149042606
forward train acc: top1 ->  99.912 ; top5 ->  100.0  and loss:  0.3605797707568854
test acc: top1 ->  91.69 ; top5 ->  99.39  and loss:  47.97728955000639
forward train acc: top1 ->  99.942 ; top5 ->  100.0  and loss:  0.3063122202293016
test acc: top1 ->  91.73 ; top5 ->  99.36  and loss:  48.69726834446192
forward train acc: top1 ->  99.92199997558593 ; top5 ->  100.0  and loss:  0.3158229884575121
test acc: top1 ->  91.82 ; top5 ->  99.34  and loss:  49.37186689674854
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  37 / 512 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -28.379991695284843 , diff:  28.379991695284843
adv train loss:  -29.1619992852211 , diff:  0.7820075899362564
adv train loss:  -28.376369029283524 , diff:  0.7856302559375763
adv train loss:  -28.649408280849457 , diff:  0.2730392515659332
adv train loss:  -28.520659297704697 , diff:  0.12874898314476013
adv train loss:  -28.194931514561176 , diff:  0.32572778314352036
adv train loss:  -29.068651914596558 , diff:  0.8737204000353813
adv train loss:  -28.944477200508118 , diff:  0.12417471408843994
adv train loss:  -28.48084780573845 , diff:  0.4636293947696686
adv train loss:  -28.13630023598671 , diff:  0.3445475697517395
layer  9  adv train finish, try to retain  502
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -20.08233618736267 , diff:  20.08233618736267
adv train loss:  -19.944534435868263 , diff:  0.13780175149440765
adv train loss:  -20.490783348679543 , diff:  0.5462489128112793
adv train loss:  -20.593317285180092 , diff:  0.10253393650054932
adv train loss:  -19.821656733751297 , diff:  0.7716605514287949
adv train loss:  -19.895704105496407 , diff:  0.07404737174510956
adv train loss:  -20.365817591547966 , diff:  0.47011348605155945
adv train loss:  -20.20462755858898 , diff:  0.16119003295898438
adv train loss:  -20.274611465632915 , diff:  0.06998390704393387
adv train loss:  -20.22552178800106 , diff:  0.04908967763185501
layer  10  adv train finish, try to retain  501
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -346.8632674217224 , diff:  346.8632674217224
adv train loss:  -347.90643882751465 , diff:  1.0431714057922363
adv train loss:  -347.3552827835083 , diff:  0.5511560440063477
layer  11  adv train finish, try to retain  455
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -1269.8465147018433 , diff:  1269.8465147018433
adv train loss:  -1270.1316995620728 , diff:  0.2851848602294922
layer  12  adv train finish, try to retain  510
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -8837.370307922363 , diff:  8837.370307922363
adv train loss:  -17153.69281768799 , diff:  8316.322509765625
adv train loss:  -24982.72621154785 , diff:  7829.033393859863
adv train loss:  -32102.143829345703 , diff:  7119.417617797852
adv train loss:  -38874.71008300781 , diff:  6772.566253662109
adv train loss:  -45483.57080078125 , diff:  6608.8607177734375
adv train loss:  -52011.180603027344 , diff:  6527.609802246094
adv train loss:  -58483.69189453125 , diff:  6472.511291503906
adv train loss:  -64935.588317871094 , diff:  6451.896423339844
adv train loss:  -71310.04461669922 , diff:  6374.456298828125
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  8
test acc: top1 ->  18.4 ; top5 ->  53.32  and loss:  1124.0644836425781
forward train acc: top1 ->  32.2320000024414 ; top5 ->  80.17400000488281  and loss:  307.0763529539108
test acc: top1 ->  43.85 ; top5 ->  97.06  and loss:  142.05941021442413
forward train acc: top1 ->  56.34399999267578 ; top5 ->  99.778  and loss:  113.55381333827972
test acc: top1 ->  54.59 ; top5 ->  98.25  and loss:  128.7637858390808
forward train acc: top1 ->  63.76799997802734 ; top5 ->  99.908  and loss:  104.46720278263092
test acc: top1 ->  61.4 ; top5 ->  98.31  and loss:  122.81995677947998
forward train acc: top1 ->  67.47200001464844 ; top5 ->  99.95599997558594  and loss:  97.61187762022018
test acc: top1 ->  64.19 ; top5 ->  98.31  and loss:  118.82259500026703
forward train acc: top1 ->  71.63599998535156 ; top5 ->  99.966  and loss:  91.97236412763596
test acc: top1 ->  64.73 ; top5 ->  98.27  and loss:  116.30124431848526
forward train acc: top1 ->  74.25199998535156 ; top5 ->  99.964  and loss:  88.0890639424324
test acc: top1 ->  69.46 ; top5 ->  98.29  and loss:  113.75777441263199
forward train acc: top1 ->  75.8779999951172 ; top5 ->  99.984  and loss:  85.27819120883942
test acc: top1 ->  68.46 ; top5 ->  98.27  and loss:  112.26660466194153
forward train acc: top1 ->  78.02999999755859 ; top5 ->  99.95199997558593  and loss:  82.81702721118927
test acc: top1 ->  68.48 ; top5 ->  98.25  and loss:  111.13620221614838
forward train acc: top1 ->  79.0439999975586 ; top5 ->  99.974  and loss:  80.2651938199997
test acc: top1 ->  71.42 ; top5 ->  98.27  and loss:  109.24748384952545
forward train acc: top1 ->  80.71399999267578 ; top5 ->  99.982  and loss:  77.4712764620781
test acc: top1 ->  72.88 ; top5 ->  98.28  and loss:  108.52957147359848
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  9 / 512 , inc:  1
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.671875  ==>  43 / 64 , inc:  1
layer  2  :  0.71875  ==>  92 / 128 , inc:  4
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.390625  ==>  100 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.072265625  ==>  37 / 512 , inc:  1
layer  9  :  0.009765625  ==>  5 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.009765625  ==>  5 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [0.9987034335046043, 7.101891082699409, 3.5509455413497046, 0.9987034335046043, 0.9987034335046043, 0.9987034335046043, 0.9987034335046043, 1.7754727706748523, 0.8877363853374262, 1.7754727706748523, 1.9974068670092087, 1.7754727706748523, 1.9974068670092087, 14.203782165398819]  wait [3, 2, 0, 3, 3, 3, 2, 0, 3, 2, 0, 2, 2, 2]  inc [1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 12
$$$$$$$$$$$$$ epoch  83  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
adv train loss:  -319.47999119758606 , diff:  319.47999119758606
adv train loss:  -320.56724524497986 , diff:  1.0872540473937988
adv train loss:  -321.0423593521118 , diff:  0.475114107131958
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  42
test acc: top1 ->  10.02 ; top5 ->  53.03  and loss:  983.158637046814
forward train acc: top1 ->  97.552 ; top5 ->  99.996  and loss:  23.252018549537752
test acc: top1 ->  91.86 ; top5 ->  98.81  and loss:  60.237642876803875
forward train acc: top1 ->  99.86199997558593 ; top5 ->  99.998  and loss:  0.4961700919084251
test acc: top1 ->  91.96 ; top5 ->  98.83  and loss:  59.531131502240896
forward train acc: top1 ->  99.9 ; top5 ->  100.0  and loss:  0.3460178470122628
test acc: top1 ->  92.0 ; top5 ->  98.94  and loss:  58.8167221583426
forward train acc: top1 ->  99.894 ; top5 ->  99.996  and loss:  0.34114730774308555
test acc: top1 ->  92.12 ; top5 ->  98.95  and loss:  58.6939273327589
==> this epoch:  42 / 64
---------------- start layer  2  ---------------
adv train loss:  -1.388954914160422 , diff:  1.388954914160422
adv train loss:  -2.0678927960107103 , diff:  0.6789378818502882
adv train loss:  -1.720512647763826 , diff:  0.34738014824688435
adv train loss:  -1.8528905519051477 , diff:  0.13237790414132178
adv train loss:  -1.8697988933417946 , diff:  0.01690834143664688
adv train loss:  -1.7713894735788926 , diff:  0.09840941976290196
adv train loss:  -1.8419385670567863 , diff:  0.07054909347789362
adv train loss:  -1.8591853195684962 , diff:  0.01724675251170993
adv train loss:  -2.133436700212769 , diff:  0.274251380644273
adv train loss:  -1.828289274359122 , diff:  0.3051474258536473
layer  2  adv train finish, try to retain  90
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  1759.8893947601318
forward train acc: top1 ->  99.846 ; top5 ->  99.998  and loss:  0.4821065188443754
test acc: top1 ->  92.17 ; top5 ->  99.07  and loss:  59.60162625834346
==> this epoch:  90 / 128
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
adv train loss:  -0.6392479117785115 , diff:  0.6392479117785115
adv train loss:  -0.6580840397218708 , diff:  0.018836127943359315
adv train loss:  -0.7278776484890841 , diff:  0.0697936087672133
adv train loss:  -0.6889447625144385 , diff:  0.038932885974645615
adv train loss:  -0.8134889924549498 , diff:  0.12454422994051129
adv train loss:  -0.7016308483143803 , diff:  0.1118581441405695
adv train loss:  -0.7058270788111258 , diff:  0.004196230496745557
layer  6  adv train finish, try to retain  217
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -0.34221882984275 , diff:  0.34221882984275
adv train loss:  -0.4056401782654575 , diff:  0.06342134842270752
adv train loss:  -0.3197504638810642 , diff:  0.0858897143843933
adv train loss:  -0.33905081429111306 , diff:  0.01930035041004885
adv train loss:  -0.3462659349606838 , diff:  0.0072151206695707515
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  252
test acc: top1 ->  19.04 ; top5 ->  52.41  and loss:  10682304.7578125
forward train acc: top1 ->  99.7180000024414 ; top5 ->  100.0  and loss:  0.9366743030259386
test acc: top1 ->  91.61 ; top5 ->  99.26  and loss:  60.38862216472626
forward train acc: top1 ->  99.7700000024414 ; top5 ->  100.0  and loss:  0.6890137785230763
test acc: top1 ->  91.45 ; top5 ->  99.24  and loss:  59.1567787155509
forward train acc: top1 ->  99.828 ; top5 ->  100.0  and loss:  0.5141070015961304
test acc: top1 ->  91.58 ; top5 ->  99.21  and loss:  61.520459704101086
forward train acc: top1 ->  99.812 ; top5 ->  99.998  and loss:  0.5593597401457373
test acc: top1 ->  91.69 ; top5 ->  99.27  and loss:  61.117432460188866
forward train acc: top1 ->  99.85399997558594 ; top5 ->  100.0  and loss:  0.4724641289212741
test acc: top1 ->  91.41 ; top5 ->  99.27  and loss:  62.6772951092571
forward train acc: top1 ->  99.87 ; top5 ->  100.0  and loss:  0.37790997573756613
test acc: top1 ->  91.64 ; top5 ->  99.27  and loss:  61.95722557231784
forward train acc: top1 ->  99.9 ; top5 ->  100.0  and loss:  0.31848717891261913
test acc: top1 ->  91.69 ; top5 ->  99.21  and loss:  61.391239538788795
forward train acc: top1 ->  99.896 ; top5 ->  100.0  and loss:  0.33533014592831023
test acc: top1 ->  91.73 ; top5 ->  99.25  and loss:  63.639402490109205
forward train acc: top1 ->  99.868 ; top5 ->  100.0  and loss:  0.374730899813585
test acc: top1 ->  91.7 ; top5 ->  99.22  and loss:  62.67160240188241
forward train acc: top1 ->  99.896 ; top5 ->  99.998  and loss:  0.34153205349866766
test acc: top1 ->  91.71 ; top5 ->  99.23  and loss:  61.38337034732103
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  253 / 512 , inc:  1
---------------- start layer  8  ---------------
### skip layer  8 wait:  3  ###
---------------- start layer  9  ---------------
adv train loss:  -2079.5953636169434 , diff:  2079.5953636169434
adv train loss:  -2094.3677463531494 , diff:  14.772382736206055
adv train loss:  -2117.553518295288 , diff:  23.185771942138672
adv train loss:  -2129.071075439453 , diff:  11.517557144165039
adv train loss:  -2128.988639831543 , diff:  0.08243560791015625
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  4
test acc: top1 ->  10.0 ; top5 ->  50.07  and loss:  4670.377006530762
forward train acc: top1 ->  97.302 ; top5 ->  99.994  and loss:  11.157106704544276
test acc: top1 ->  90.46 ; top5 ->  98.9  and loss:  61.04353678226471
forward train acc: top1 ->  99.85199997558594 ; top5 ->  100.0  and loss:  0.5745059011969715
test acc: top1 ->  91.71 ; top5 ->  99.1  and loss:  55.76999104395509
forward train acc: top1 ->  99.874 ; top5 ->  100.0  and loss:  0.4434961301740259
test acc: top1 ->  91.76 ; top5 ->  99.14  and loss:  55.951891243457794
forward train acc: top1 ->  99.91 ; top5 ->  100.0  and loss:  0.3230070967110805
test acc: top1 ->  91.85 ; top5 ->  99.19  and loss:  57.00540504232049
forward train acc: top1 ->  99.916 ; top5 ->  100.0  and loss:  0.3518411513068713
test acc: top1 ->  91.96 ; top5 ->  99.15  and loss:  56.226364221423864
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.26950544222199824
test acc: top1 ->  92.05 ; top5 ->  99.17  and loss:  56.225998263806105
forward train acc: top1 ->  99.94 ; top5 ->  100.0  and loss:  0.21202648563485127
test acc: top1 ->  91.99 ; top5 ->  99.17  and loss:  56.66683831438422
forward train acc: top1 ->  99.93400000244141 ; top5 ->  100.0  and loss:  0.22548173886025324
test acc: top1 ->  92.02 ; top5 ->  99.18  and loss:  56.39756304398179
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.25403657933929935
test acc: top1 ->  91.91 ; top5 ->  99.17  and loss:  57.466413516551256
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.18027896361309104
test acc: top1 ->  92.12 ; top5 ->  99.15  and loss:  57.24440979026258
==> this epoch:  4 / 512
---------------- start layer  10  ---------------
adv train loss:  -191.82345390319824 , diff:  191.82345390319824
adv train loss:  -191.11632001399994 , diff:  0.7071338891983032
adv train loss:  -190.26071393489838 , diff:  0.8556060791015625
adv train loss:  -190.80522418022156 , diff:  0.5445102453231812
adv train loss:  -191.75795996189117 , diff:  0.9527357816696167
adv train loss:  -190.51148545742035 , diff:  1.2464745044708252
adv train loss:  -190.86457192897797 , diff:  0.3530864715576172
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  10
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  144459.39184570312
forward train acc: top1 ->  77.07599999267578 ; top5 ->  98.59799997558594  and loss:  105.8316579759121
test acc: top1 ->  54.5 ; top5 ->  97.12  and loss:  140.3152803182602
forward train acc: top1 ->  97.69999998046875 ; top5 ->  99.986  and loss:  12.422551184892654
test acc: top1 ->  89.62 ; top5 ->  98.79  and loss:  41.50619888305664
forward train acc: top1 ->  98.81199998046876 ; top5 ->  99.996  and loss:  5.907807774841785
test acc: top1 ->  90.19 ; top5 ->  98.9  and loss:  41.560087114572525
forward train acc: top1 ->  99.28199997802734 ; top5 ->  100.0  and loss:  3.4723912347108126
test acc: top1 ->  90.44 ; top5 ->  99.03  and loss:  41.528484769165516
forward train acc: top1 ->  99.46399997802735 ; top5 ->  100.0  and loss:  2.4042720710858703
test acc: top1 ->  90.84 ; top5 ->  99.08  and loss:  41.43101078271866
forward train acc: top1 ->  99.606 ; top5 ->  100.0  and loss:  1.8739877380430698
test acc: top1 ->  91.07 ; top5 ->  99.07  and loss:  42.10296010226011
forward train acc: top1 ->  99.55199997558594 ; top5 ->  100.0  and loss:  1.8619184992276132
test acc: top1 ->  91.0 ; top5 ->  99.04  and loss:  42.349506855010986
forward train acc: top1 ->  99.686 ; top5 ->  100.0  and loss:  1.4974591126665473
test acc: top1 ->  91.04 ; top5 ->  99.05  and loss:  42.58371716737747
forward train acc: top1 ->  99.6980000024414 ; top5 ->  100.0  and loss:  1.2928113508969545
test acc: top1 ->  91.21 ; top5 ->  99.13  and loss:  42.83315037190914
forward train acc: top1 ->  99.736 ; top5 ->  100.0  and loss:  1.2218531291000545
test acc: top1 ->  91.22 ; top5 ->  99.05  and loss:  43.83911654353142
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  11 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -2004.7687091827393 , diff:  2004.7687091827393
adv train loss:  -2006.0737953186035 , diff:  1.3050861358642578
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  1
test acc: top1 ->  10.0 ; top5 ->  59.58  and loss:  15749.997421264648
forward train acc: top1 ->  79.96600000976562 ; top5 ->  99.572  and loss:  84.55572038888931
test acc: top1 ->  87.41 ; top5 ->  98.76  and loss:  55.66875675320625
forward train acc: top1 ->  98.89399997558594 ; top5 ->  99.998  and loss:  9.67599394544959
test acc: top1 ->  89.83 ; top5 ->  98.85  and loss:  49.24829286336899
forward train acc: top1 ->  99.34400000488282 ; top5 ->  100.0  and loss:  4.070000158622861
test acc: top1 ->  90.35 ; top5 ->  98.95  and loss:  50.310474425554276
forward train acc: top1 ->  99.51000000488281 ; top5 ->  99.998  and loss:  2.640290370211005
test acc: top1 ->  90.81 ; top5 ->  98.97  and loss:  51.22976140677929
forward train acc: top1 ->  99.67999997558594 ; top5 ->  100.0  and loss:  1.8164157513529062
test acc: top1 ->  90.9 ; top5 ->  98.98  and loss:  52.239290066063404
forward train acc: top1 ->  99.69000000244141 ; top5 ->  100.0  and loss:  1.5739484336227179
test acc: top1 ->  90.98 ; top5 ->  98.98  and loss:  52.68016109615564
forward train acc: top1 ->  99.7040000024414 ; top5 ->  100.0  and loss:  1.3800248308107257
test acc: top1 ->  90.98 ; top5 ->  98.95  and loss:  53.287541046738625
forward train acc: top1 ->  99.7360000024414 ; top5 ->  100.0  and loss:  1.2781038582324982
test acc: top1 ->  90.99 ; top5 ->  98.95  and loss:  53.606667689979076
forward train acc: top1 ->  99.73800000244141 ; top5 ->  99.998  and loss:  1.2077646795660257
test acc: top1 ->  91.2 ; top5 ->  98.93  and loss:  54.3649332895875
forward train acc: top1 ->  99.782 ; top5 ->  100.0  and loss:  1.0271274587139487
test acc: top1 ->  91.25 ; top5 ->  98.96  and loss:  55.00518064200878
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  2 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -2677.2427864074707 , diff:  2677.2427864074707
adv train loss:  -3216.5727672576904 , diff:  539.3299808502197
adv train loss:  -3176.2595539093018 , diff:  40.31321334838867
adv train loss:  -3119.7421550750732 , diff:  56.517398834228516
adv train loss:  -3121.0033626556396 , diff:  1.2612075805664062
layer  12  adv train finish, try to retain  1
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  8369.586624145508
forward train acc: top1 ->  32.01199999023437 ; top5 ->  79.99000000976562  and loss:  699.9741613864899
test acc: top1 ->  37.75 ; top5 ->  89.12  and loss:  213.3272224664688
forward train acc: top1 ->  48.80199999633789 ; top5 ->  95.838  and loss:  133.8316588997841
test acc: top1 ->  52.69 ; top5 ->  97.77  and loss:  117.91772544384003
forward train acc: top1 ->  56.46199998901367 ; top5 ->  99.954  and loss:  96.97082936763763
test acc: top1 ->  55.07 ; top5 ->  98.34  and loss:  114.02227437496185
forward train acc: top1 ->  62.22000001220703 ; top5 ->  99.968  and loss:  91.62583392858505
test acc: top1 ->  60.82 ; top5 ->  98.43  and loss:  109.89954233169556
forward train acc: top1 ->  65.9279999987793 ; top5 ->  99.98199997558594  and loss:  87.34276229143143
test acc: top1 ->  61.8 ; top5 ->  98.46  and loss:  107.36096972227097
forward train acc: top1 ->  67.57999997070313 ; top5 ->  99.97  and loss:  84.95801222324371
test acc: top1 ->  63.7 ; top5 ->  98.51  and loss:  105.94201046228409
forward train acc: top1 ->  69.91999999511718 ; top5 ->  99.986  and loss:  82.68745481967926
test acc: top1 ->  65.83 ; top5 ->  98.5  and loss:  104.73528718948364
forward train acc: top1 ->  71.26399997558593 ; top5 ->  99.978  and loss:  80.79893010854721
test acc: top1 ->  66.19 ; top5 ->  98.5  and loss:  103.28375107049942
forward train acc: top1 ->  72.924 ; top5 ->  99.982  and loss:  78.92756497859955
test acc: top1 ->  68.18 ; top5 ->  98.55  and loss:  101.65718859434128
forward train acc: top1 ->  76.09799998291015 ; top5 ->  99.984  and loss:  76.28844380378723
test acc: top1 ->  70.63 ; top5 ->  98.48  and loss:  99.91770285367966
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  5 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -16434.95310974121 , diff:  16434.95310974121
adv train loss:  -25031.368286132812 , diff:  8596.415176391602
adv train loss:  -33601.061614990234 , diff:  8569.693328857422
adv train loss:  -42199.96481323242 , diff:  8598.903198242188
adv train loss:  -50843.05078125 , diff:  8643.085968017578
adv train loss:  -59499.964294433594 , diff:  8656.913513183594
adv train loss:  -68180.6215209961 , diff:  8680.6572265625
adv train loss:  -76867.49035644531 , diff:  8686.868835449219
adv train loss:  -85625.86639404297 , diff:  8758.376037597656
adv train loss:  -94347.73236083984 , diff:  8721.865966796875
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  8
test acc: top1 ->  10.14 ; top5 ->  58.35  and loss:  4584.018726348877
forward train acc: top1 ->  40.06599997802734 ; top5 ->  79.70000001220703  and loss:  1089.100816488266
test acc: top1 ->  65.62 ; top5 ->  88.52  and loss:  255.96365344524384
forward train acc: top1 ->  83.24799997070312 ; top5 ->  94.516  and loss:  94.93270844221115
test acc: top1 ->  78.02 ; top5 ->  98.66  and loss:  102.65166342258453
forward train acc: top1 ->  96.61200000488282 ; top5 ->  99.992  and loss:  20.342527374625206
test acc: top1 ->  88.3 ; top5 ->  98.69  and loss:  74.10927376151085
forward train acc: top1 ->  98.46999998046876 ; top5 ->  99.998  and loss:  12.635758742690086
test acc: top1 ->  88.48 ; top5 ->  98.68  and loss:  69.13630205392838
forward train acc: top1 ->  98.74199998046875 ; top5 ->  99.996  and loss:  9.623333245515823
test acc: top1 ->  88.86 ; top5 ->  98.64  and loss:  67.48352512717247
forward train acc: top1 ->  98.92999998291016 ; top5 ->  99.994  and loss:  8.143368411809206
test acc: top1 ->  88.9 ; top5 ->  98.63  and loss:  66.83663855493069
forward train acc: top1 ->  99.052 ; top5 ->  99.998  and loss:  7.260460324585438
test acc: top1 ->  89.1 ; top5 ->  98.64  and loss:  65.74335615336895
forward train acc: top1 ->  99.14199997558593 ; top5 ->  100.0  and loss:  6.505214735865593
test acc: top1 ->  89.28 ; top5 ->  98.66  and loss:  65.46568401157856
forward train acc: top1 ->  99.23399997802734 ; top5 ->  99.994  and loss:  5.8849732130765915
test acc: top1 ->  89.38 ; top5 ->  98.66  and loss:  64.98915457725525
forward train acc: top1 ->  99.24399997802735 ; top5 ->  99.998  and loss:  5.319003775715828
test acc: top1 ->  89.3 ; top5 ->  98.7  and loss:  65.18496382236481
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  9 / 512 , inc:  1
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.65625  ==>  42 / 64 , inc:  2
layer  2  :  0.703125  ==>  90 / 128 , inc:  8
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.390625  ==>  100 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.072265625  ==>  37 / 512 , inc:  1
layer  9  :  0.0078125  ==>  4 / 512 , inc:  2
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.009765625  ==>  5 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [0.9987034335046043, 7.101891082699409, 3.5509455413497046, 0.9987034335046043, 0.9987034335046043, 0.9987034335046043, 1.9974068670092087, 1.3316045780061392, 0.8877363853374262, 1.7754727706748523, 1.4980551502569064, 1.3316045780061392, 1.4980551502569064, 10.652836624049113]  wait [2, 0, 0, 2, 2, 2, 2, 2, 2, 0, 2, 4, 4, 4]  inc [1, 2, 8, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]  tol: 12
$$$$$$$$$$$$$ epoch  84  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -155.19482934474945 , diff:  155.19482934474945
adv train loss:  -154.94931781291962 , diff:  0.24551153182983398
layer  0  adv train finish, try to retain  64
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -155.03855752944946 , diff:  155.03855752944946
adv train loss:  -155.51905226707458 , diff:  0.48049473762512207
adv train loss:  -154.84997141361237 , diff:  0.6690808534622192
adv train loss:  -155.09828412532806 , diff:  0.24831271171569824
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  40
test acc: top1 ->  10.0 ; top5 ->  50.91  and loss:  2414.459297180176
forward train acc: top1 ->  97.26400000488282 ; top5 ->  99.996  and loss:  8.960939270444214
test acc: top1 ->  90.45 ; top5 ->  98.85  and loss:  67.42867892980576
forward train acc: top1 ->  99.53799997558593 ; top5 ->  99.998  and loss:  1.6114472886547446
test acc: top1 ->  91.01 ; top5 ->  98.94  and loss:  67.56702964752913
forward train acc: top1 ->  99.644 ; top5 ->  99.998  and loss:  1.2724018935114145
test acc: top1 ->  91.13 ; top5 ->  98.94  and loss:  69.0500188395381
forward train acc: top1 ->  99.73399997558593 ; top5 ->  100.0  and loss:  0.858328316709958
test acc: top1 ->  91.4 ; top5 ->  98.92  and loss:  70.62049141526222
forward train acc: top1 ->  99.76199997558594 ; top5 ->  99.998  and loss:  0.8908417311613448
test acc: top1 ->  91.49 ; top5 ->  98.91  and loss:  71.27204715833068
forward train acc: top1 ->  99.80999997558594 ; top5 ->  100.0  and loss:  0.665434135182295
test acc: top1 ->  91.5 ; top5 ->  98.86  and loss:  70.9918357655406
forward train acc: top1 ->  99.83399997558594 ; top5 ->  100.0  and loss:  0.581099365837872
test acc: top1 ->  91.66 ; top5 ->  98.89  and loss:  71.13508689776063
forward train acc: top1 ->  99.858 ; top5 ->  100.0  and loss:  0.4909557062201202
test acc: top1 ->  91.65 ; top5 ->  98.96  and loss:  71.15896367281675
forward train acc: top1 ->  99.84399997558593 ; top5 ->  99.998  and loss:  0.565025232557673
test acc: top1 ->  91.74 ; top5 ->  98.95  and loss:  69.78805089741945
forward train acc: top1 ->  99.878 ; top5 ->  100.0  and loss:  0.3927842683624476
test acc: top1 ->  91.88 ; top5 ->  98.98  and loss:  69.19615636765957
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  42 / 64 , inc:  2
---------------- start layer  2  ---------------
adv train loss:  -0.5462375389179215 , diff:  0.5462375389179215
adv train loss:  -0.4014899015601259 , diff:  0.1447476373577956
adv train loss:  -0.36627969742403366 , diff:  0.035210204136092216
adv train loss:  -0.367324921884574 , diff:  0.0010452244605403394
************ all values are small in this layer **********
layer  2  adv train finish, try to retain  82
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  1655.604808807373
forward train acc: top1 ->  99.56200000244141 ; top5 ->  99.996  and loss:  1.645343288546428
test acc: top1 ->  91.52 ; top5 ->  99.01  and loss:  70.74961605668068
forward train acc: top1 ->  99.72600000244141 ; top5 ->  99.998  and loss:  0.9359125973423943
test acc: top1 ->  91.82 ; top5 ->  98.86  and loss:  67.40226612985134
forward train acc: top1 ->  99.73399997558593 ; top5 ->  99.994  and loss:  0.7781942888977937
test acc: top1 ->  91.84 ; top5 ->  99.02  and loss:  68.18619680404663
forward train acc: top1 ->  99.782 ; top5 ->  99.998  and loss:  0.6731406865583267
test acc: top1 ->  91.76 ; top5 ->  98.91  and loss:  66.75512264668941
forward train acc: top1 ->  99.82999997558593 ; top5 ->  100.0  and loss:  0.5840075945889112
test acc: top1 ->  91.85 ; top5 ->  98.98  and loss:  68.37938243150711
forward train acc: top1 ->  99.832 ; top5 ->  100.0  and loss:  0.5749471347371582
test acc: top1 ->  91.99 ; top5 ->  99.04  and loss:  65.3565828204155
forward train acc: top1 ->  99.87399997558593 ; top5 ->  99.998  and loss:  0.4254182693839539
test acc: top1 ->  91.87 ; top5 ->  99.1  and loss:  66.05351787805557
forward train acc: top1 ->  99.824 ; top5 ->  100.0  and loss:  0.49440614544437267
test acc: top1 ->  91.96 ; top5 ->  99.07  and loss:  65.3281029611826
forward train acc: top1 ->  99.85 ; top5 ->  100.0  and loss:  0.5109912590123713
test acc: top1 ->  91.75 ; top5 ->  98.98  and loss:  66.81094896793365
forward train acc: top1 ->  99.85799997558594 ; top5 ->  100.0  and loss:  0.40282410773215815
test acc: top1 ->  91.75 ; top5 ->  99.04  and loss:  66.51695929467678
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  90 / 128 , inc:  8
---------------- start layer  3  ---------------
adv train loss:  -0.18950068589765579 , diff:  0.18950068589765579
adv train loss:  -0.160482352875988 , diff:  0.029018333021667786
adv train loss:  -0.1757907763676485 , diff:  0.015308423491660506
adv train loss:  -0.13251473382115364 , diff:  0.043276042546494864
adv train loss:  -0.20530849508941174 , diff:  0.0727937612682581
adv train loss:  -0.14170728319732007 , diff:  0.06360121189209167
adv train loss:  -0.17194830533117056 , diff:  0.030241022133850493
adv train loss:  -0.17368454915413167 , diff:  0.0017362438229611143
layer  3  adv train finish, try to retain  108
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -0.2025453633032157 , diff:  0.2025453633032157
adv train loss:  -0.13015675642236602 , diff:  0.07238860688084969
adv train loss:  -0.19096009181521367 , diff:  0.06080333539284766
adv train loss:  -0.15254391881535412 , diff:  0.038416172999859555
adv train loss:  -0.15179046825505793 , diff:  0.0007534505602961872
layer  4  adv train finish, try to retain  205
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -0.16365413198946044 , diff:  0.16365413198946044
adv train loss:  -0.17417096978169866 , diff:  0.01051683779223822
adv train loss:  -0.15733808303775731 , diff:  0.016832886743941344
adv train loss:  -0.21517842988396296 , diff:  0.05784034684620565
adv train loss:  -0.1680582884582691 , diff:  0.04712014142569387
adv train loss:  -0.13782402727520093 , diff:  0.030234261183068156
adv train loss:  -0.19479481922462583 , diff:  0.05697079194942489
adv train loss:  -0.14136870179208927 , diff:  0.05342611743253656
adv train loss:  -0.1557155720947776 , diff:  0.01434687030268833
adv train loss:  -0.12936439061741112 , diff:  0.02635118147736648
layer  5  adv train finish, try to retain  196
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -687.7622019878472 , diff:  687.7622019878472
adv train loss:  -2828.4806537628174 , diff:  2140.71845177497
adv train loss:  -2857.2092304229736 , diff:  28.72857666015625
adv train loss:  -2871.10520362854 , diff:  13.895973205566406
adv train loss:  -2883.807722091675 , diff:  12.702518463134766
adv train loss:  -2874.4626426696777 , diff:  9.34507942199707
adv train loss:  -2894.8972816467285 , diff:  20.43463897705078
adv train loss:  -2904.2773036956787 , diff:  9.380022048950195
adv train loss:  -2927.595453262329 , diff:  23.31814956665039
adv train loss:  -2925.779022216797 , diff:  1.8164310455322266
layer  6  adv train finish, try to retain  8
test acc: top1 ->  10.38 ; top5 ->  50.01  and loss:  1911.9235877990723
forward train acc: top1 ->  61.752 ; top5 ->  96.07200000976563  and loss:  229.31034636497498
test acc: top1 ->  67.03 ; top5 ->  95.9  and loss:  128.52990299463272
forward train acc: top1 ->  77.71600000488282 ; top5 ->  98.75399997802734  and loss:  62.59406065940857
test acc: top1 ->  73.44 ; top5 ->  97.5  and loss:  90.49808830022812
forward train acc: top1 ->  82.76400001220703 ; top5 ->  99.17000000244141  and loss:  48.82431074976921
test acc: top1 ->  77.24 ; top5 ->  97.91  and loss:  78.4039326608181
forward train acc: top1 ->  85.45800000244141 ; top5 ->  99.38399997558594  and loss:  41.46224567294121
test acc: top1 ->  79.92 ; top5 ->  98.04  and loss:  72.12299606204033
forward train acc: top1 ->  87.53199999511719 ; top5 ->  99.54199997558594  and loss:  35.96930578351021
test acc: top1 ->  81.13 ; top5 ->  98.35  and loss:  66.55054983496666
forward train acc: top1 ->  89.04199997558594 ; top5 ->  99.62399997802734  and loss:  32.04551586508751
test acc: top1 ->  81.81 ; top5 ->  98.43  and loss:  65.6801768541336
forward train acc: top1 ->  89.65599998291016 ; top5 ->  99.648  and loss:  29.946655616164207
test acc: top1 ->  82.29 ; top5 ->  98.42  and loss:  64.17720547318459
forward train acc: top1 ->  90.17799999023437 ; top5 ->  99.686  and loss:  28.43583934009075
test acc: top1 ->  83.1 ; top5 ->  98.48  and loss:  62.60884988307953
forward train acc: top1 ->  90.59200001464843 ; top5 ->  99.698  and loss:  27.360958725214005
test acc: top1 ->  83.34 ; top5 ->  98.58  and loss:  61.231472343206406
forward train acc: top1 ->  91.05 ; top5 ->  99.7  and loss:  26.111721232533455
test acc: top1 ->  84.25 ; top5 ->  98.65  and loss:  59.3985849916935
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  100 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -11.894323453307152 , diff:  11.894323453307152
adv train loss:  -11.590751312673092 , diff:  0.3035721406340599
adv train loss:  -11.853775307536125 , diff:  0.2630239948630333
adv train loss:  -45.29297426342964 , diff:  33.43919895589352
adv train loss:  -77.239830493927 , diff:  31.94685623049736
adv train loss:  -93.65257447957993 , diff:  16.412743985652924
adv train loss:  -102.05673611164093 , diff:  8.404161632061005
adv train loss:  -106.17891603708267 , diff:  4.122179925441742
adv train loss:  -110.77275854349136 , diff:  4.593842506408691
adv train loss:  -118.3739242553711 , diff:  7.60116571187973
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  252
test acc: top1 ->  10.0 ; top5 ->  49.98  and loss:  6117515.84765625
forward train acc: top1 ->  98.89799997558593 ; top5 ->  99.994  and loss:  3.680101308040321
test acc: top1 ->  91.3 ; top5 ->  99.18  and loss:  48.34280648827553
forward train acc: top1 ->  99.70600000488281 ; top5 ->  99.998  and loss:  1.0090642319992185
test acc: top1 ->  91.39 ; top5 ->  99.21  and loss:  53.44908928871155
forward train acc: top1 ->  99.81199997558593 ; top5 ->  99.996  and loss:  0.6164657587651163
test acc: top1 ->  91.57 ; top5 ->  99.24  and loss:  57.05471979826689
forward train acc: top1 ->  99.854 ; top5 ->  100.0  and loss:  0.47640756820328534
test acc: top1 ->  91.68 ; top5 ->  99.16  and loss:  60.1877723261714
forward train acc: top1 ->  99.88999997558594 ; top5 ->  100.0  and loss:  0.3767500575631857
test acc: top1 ->  91.59 ; top5 ->  99.22  and loss:  61.99581327289343
forward train acc: top1 ->  99.86799997558593 ; top5 ->  100.0  and loss:  0.4068676682654768
test acc: top1 ->  91.93 ; top5 ->  99.3  and loss:  59.91888941824436
forward train acc: top1 ->  99.896 ; top5 ->  99.998  and loss:  0.34086192719405517
test acc: top1 ->  91.8 ; top5 ->  99.19  and loss:  62.278514083474874
forward train acc: top1 ->  99.878 ; top5 ->  100.0  and loss:  0.35479932534508407
test acc: top1 ->  92.03 ; top5 ->  99.24  and loss:  61.2894218750298
forward train acc: top1 ->  99.89 ; top5 ->  100.0  and loss:  0.3392616610508412
test acc: top1 ->  91.9 ; top5 ->  99.25  and loss:  62.239711521193385
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.25042660429608077
test acc: top1 ->  91.93 ; top5 ->  99.23  and loss:  63.645552936941385
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  253 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -20.624226205050945 , diff:  20.624226205050945
adv train loss:  -21.199346773326397 , diff:  0.5751205682754517
adv train loss:  -21.037265546619892 , diff:  0.16208122670650482
adv train loss:  -21.398415260016918 , diff:  0.36114971339702606
adv train loss:  -20.94906859844923 , diff:  0.449346661567688
adv train loss:  -21.119379863142967 , diff:  0.17031126469373703
adv train loss:  -21.34715512394905 , diff:  0.22777526080608368
adv train loss:  -20.83523663878441 , diff:  0.5119184851646423
adv train loss:  -21.187565989792347 , diff:  0.3523293510079384
adv train loss:  -21.207230120897293 , diff:  0.019664131104946136
layer  8  adv train finish, try to retain  415
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -714.5765781402588 , diff:  714.5765781402588
adv train loss:  -768.1364688873291 , diff:  53.55989074707031
adv train loss:  -769.9187293052673 , diff:  1.7822604179382324
adv train loss:  -766.7918419837952 , diff:  3.126887321472168
adv train loss:  -767.4661860466003 , diff:  0.6743440628051758
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  2
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  14425.565185546875
forward train acc: top1 ->  97.94799997802734 ; top5 ->  99.978  and loss:  6.781645415350795
test acc: top1 ->  56.51 ; top5 ->  98.2  and loss:  218.05390524864197
forward train acc: top1 ->  99.84199997558594 ; top5 ->  99.998  and loss:  0.7824247488752007
test acc: top1 ->  91.71 ; top5 ->  98.61  and loss:  54.81595075875521
forward train acc: top1 ->  99.88199997558594 ; top5 ->  100.0  and loss:  0.5455836807377636
test acc: top1 ->  91.64 ; top5 ->  98.61  and loss:  56.68039075285196
forward train acc: top1 ->  99.93 ; top5 ->  99.998  and loss:  0.4053994055138901
test acc: top1 ->  91.8 ; top5 ->  98.67  and loss:  57.83257162198424
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.31209234055131674
test acc: top1 ->  91.92 ; top5 ->  98.67  and loss:  57.866079069674015
forward train acc: top1 ->  99.92399997558594 ; top5 ->  100.0  and loss:  0.2794826263561845
test acc: top1 ->  91.93 ; top5 ->  98.59  and loss:  58.555472219362855
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.26780356059316546
test acc: top1 ->  91.9 ; top5 ->  98.73  and loss:  59.149377496913075
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.2236876575043425
test acc: top1 ->  92.04 ; top5 ->  98.66  and loss:  60.5770174190402
forward train acc: top1 ->  99.93999997558593 ; top5 ->  100.0  and loss:  0.2383595467545092
test acc: top1 ->  92.05 ; top5 ->  98.68  and loss:  60.670112423598766
forward train acc: top1 ->  99.958 ; top5 ->  99.998  and loss:  0.22084745991742238
test acc: top1 ->  91.91 ; top5 ->  98.81  and loss:  60.79355524480343
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  4 / 512 , inc:  2
---------------- start layer  10  ---------------
adv train loss:  -234.84680032730103 , diff:  234.84680032730103
adv train loss:  -235.80517315864563 , diff:  0.9583728313446045
adv train loss:  -262.5137996673584 , diff:  26.70862650871277
adv train loss:  -313.53228640556335 , diff:  51.018486738204956
adv train loss:  -313.2401478290558 , diff:  0.29213857650756836
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  10
test acc: top1 ->  10.0 ; top5 ->  59.52  and loss:  233559.9090576172
forward train acc: top1 ->  73.82999997558593 ; top5 ->  92.916  and loss:  135.93746852874756
test acc: top1 ->  50.99 ; top5 ->  96.82  and loss:  143.47874546051025
forward train acc: top1 ->  99.52 ; top5 ->  99.992  and loss:  6.628157019615173
test acc: top1 ->  91.11 ; top5 ->  98.92  and loss:  35.216904543340206
forward train acc: top1 ->  99.75599997558594 ; top5 ->  99.998  and loss:  2.365070575848222
test acc: top1 ->  91.41 ; top5 ->  98.92  and loss:  37.093950003385544
forward train acc: top1 ->  99.82599997802734 ; top5 ->  100.0  and loss:  1.3227764833718538
test acc: top1 ->  91.75 ; top5 ->  99.03  and loss:  39.00813356786966
forward train acc: top1 ->  99.882 ; top5 ->  100.0  and loss:  0.842763626947999
test acc: top1 ->  91.74 ; top5 ->  99.07  and loss:  40.42846377938986
forward train acc: top1 ->  99.924 ; top5 ->  100.0  and loss:  0.6546555147506297
test acc: top1 ->  91.79 ; top5 ->  99.11  and loss:  41.81783202663064
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.47266425658017397
test acc: top1 ->  91.8 ; top5 ->  99.09  and loss:  42.45522587373853
forward train acc: top1 ->  99.93199997558594 ; top5 ->  100.0  and loss:  0.44061884144321084
test acc: top1 ->  91.88 ; top5 ->  99.02  and loss:  43.27729456871748
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.37530683167278767
test acc: top1 ->  91.91 ; top5 ->  99.02  and loss:  44.04265246540308
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.3837702020537108
test acc: top1 ->  91.87 ; top5 ->  99.04  and loss:  44.62865022197366
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  11 / 512 , inc:  1
---------------- start layer  11  ---------------
### skip layer  11 wait:  4  ###
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
### skip layer  13 wait:  4  ###
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.65625  ==>  42 / 64 , inc:  1
layer  2  :  0.703125  ==>  90 / 128 , inc:  4
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.390625  ==>  100 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.072265625  ==>  37 / 512 , inc:  1
layer  9  :  0.0078125  ==>  4 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.009765625  ==>  5 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [1.9974068670092087, 5.326418312024557, 2.6632091560122784, 1.9974068670092087, 1.9974068670092087, 1.9974068670092087, 1.4980551502569064, 0.9987034335046043, 1.7754727706748523, 1.3316045780061392, 1.1235413626926798, 1.3316045780061392, 1.4980551502569064, 10.652836624049113]  wait [2, 2, 2, 2, 2, 2, 4, 4, 2, 2, 4, 3, 3, 3]  inc [1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 12
$$$$$$$$$$$$$ epoch  85  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1378.6543130874634 , diff:  1378.6543130874634
adv train loss:  -1399.1316175460815 , diff:  20.477304458618164
adv train loss:  -1393.716588973999 , diff:  5.4150285720825195
adv train loss:  -1394.3417320251465 , diff:  0.6251430511474609
adv train loss:  -1390.2292766571045 , diff:  4.112455368041992
adv train loss:  -1394.9984731674194 , diff:  4.769196510314941
adv train loss:  -1387.764539718628 , diff:  7.233933448791504
adv train loss:  -1397.9701118469238 , diff:  10.205572128295898
adv train loss:  -1395.5302276611328 , diff:  2.4398841857910156
adv train loss:  -1394.1434087753296 , diff:  1.3868188858032227
layer  0  adv train finish, try to retain  3
test acc: top1 ->  10.0 ; top5 ->  50.07  and loss:  1933.6329135894775
forward train acc: top1 ->  32.50000000854492 ; top5 ->  78.66600000976563  and loss:  389.15811717510223
test acc: top1 ->  17.54 ; top5 ->  61.79  and loss:  299.3099048137665
forward train acc: top1 ->  39.07199998779297 ; top5 ->  85.26600000732422  and loss:  174.83286082744598
test acc: top1 ->  43.17 ; top5 ->  88.24  and loss:  166.0723317861557
forward train acc: top1 ->  42.35799998535156 ; top5 ->  87.59999999023438  and loss:  163.75575995445251
test acc: top1 ->  46.29 ; top5 ->  89.74  and loss:  156.68322837352753
forward train acc: top1 ->  45.259999997558594 ; top5 ->  89.24999999023437  and loss:  155.7701072692871
test acc: top1 ->  49.19 ; top5 ->  90.68  and loss:  149.87104427814484
forward train acc: top1 ->  48.496000006103515 ; top5 ->  90.13199997802734  and loss:  147.62607836723328
test acc: top1 ->  51.89 ; top5 ->  91.4  and loss:  141.78777432441711
forward train acc: top1 ->  50.28800000732422 ; top5 ->  90.99799998046875  and loss:  142.6501420736313
test acc: top1 ->  53.33 ; top5 ->  91.96  and loss:  138.72555196285248
forward train acc: top1 ->  52.15399998779297 ; top5 ->  91.36399997314453  and loss:  138.21800088882446
test acc: top1 ->  54.86 ; top5 ->  92.31  and loss:  134.30668652057648
forward train acc: top1 ->  53.43199999389648 ; top5 ->  91.85399997802735  and loss:  135.30721616744995
test acc: top1 ->  56.09 ; top5 ->  92.8  and loss:  130.58945560455322
forward train acc: top1 ->  55.075999998779295 ; top5 ->  92.11599997558594  and loss:  131.650573015213
test acc: top1 ->  57.52 ; top5 ->  92.89  and loss:  127.84456241130829
forward train acc: top1 ->  56.1179999987793 ; top5 ->  92.47999998046875  and loss:  128.55573225021362
test acc: top1 ->  58.86 ; top5 ->  93.22  and loss:  124.57631820440292
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -35.22380769252777 , diff:  35.22380769252777
adv train loss:  -35.17119440436363 , diff:  0.052613288164138794
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  41
test acc: top1 ->  15.21 ; top5 ->  56.22  and loss:  261.7470066547394
forward train acc: top1 ->  99.75199997558593 ; top5 ->  100.0  and loss:  4.860237084329128
test acc: top1 ->  91.85 ; top5 ->  99.2  and loss:  40.28814934194088
forward train acc: top1 ->  99.89600000244141 ; top5 ->  100.0  and loss:  0.44323195423930883
test acc: top1 ->  91.7 ; top5 ->  99.16  and loss:  45.82481490820646
forward train acc: top1 ->  99.94000000244141 ; top5 ->  100.0  and loss:  0.28053052071481943
test acc: top1 ->  91.85 ; top5 ->  99.13  and loss:  49.04817063361406
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.30005875299684703
test acc: top1 ->  91.79 ; top5 ->  99.05  and loss:  51.029778599739075
forward train acc: top1 ->  99.928 ; top5 ->  100.0  and loss:  0.2389169791713357
test acc: top1 ->  92.0 ; top5 ->  99.04  and loss:  51.85759952664375
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.2031014352105558
test acc: top1 ->  92.1 ; top5 ->  99.09  and loss:  53.08696895092726
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.24527494414360262
test acc: top1 ->  92.13 ; top5 ->  99.1  and loss:  52.7794419080019
==> this epoch:  41 / 64
---------------- start layer  2  ---------------
adv train loss:  -0.20668011251837015 , diff:  0.20668011251837015
adv train loss:  -0.16692588478326797 , diff:  0.03975422773510218
adv train loss:  -0.2062097079178784 , diff:  0.03928382313461043
adv train loss:  -0.25376066379249096 , diff:  0.047550955874612555
adv train loss:  -0.20756713405717164 , diff:  0.046193529735319316
adv train loss:  -0.2558536276919767 , diff:  0.04828649363480508
adv train loss:  -0.20482333278050646 , diff:  0.051030294911470264
adv train loss:  -0.23402089439332485 , diff:  0.02919756161281839
adv train loss:  -0.20735794608481228 , diff:  0.02666294830851257
adv train loss:  -0.2207418940961361 , diff:  0.01338394801132381
layer  2  adv train finish, try to retain  90
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -558.3871932849288 , diff:  558.3871932849288
adv train loss:  -1309.245656967163 , diff:  750.8584636822343
adv train loss:  -1355.3448791503906 , diff:  46.09922218322754
adv train loss:  -1384.1022129058838 , diff:  28.757333755493164
adv train loss:  -1392.1234331130981 , diff:  8.021220207214355
adv train loss:  -1393.1568965911865 , diff:  1.033463478088379
adv train loss:  -1391.509892463684 , diff:  1.6470041275024414
adv train loss:  -1391.4908351898193 , diff:  0.019057273864746094
adv train loss:  -1394.5654678344727 , diff:  3.0746326446533203
adv train loss:  -1385.7531881332397 , diff:  8.81227970123291
layer  3  adv train finish, try to retain  24
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  2028.9941024780273
forward train acc: top1 ->  77.20600000732422 ; top5 ->  97.53800000732421  and loss:  82.21252655982971
test acc: top1 ->  75.86 ; top5 ->  97.01  and loss:  77.80198001861572
forward train acc: top1 ->  80.4180000024414 ; top5 ->  98.36399998046875  and loss:  58.95910784602165
test acc: top1 ->  77.91 ; top5 ->  97.64  and loss:  70.28221887350082
forward train acc: top1 ->  81.98999997314453 ; top5 ->  98.56799998046876  and loss:  53.58852270245552
test acc: top1 ->  79.06 ; top5 ->  97.87  and loss:  67.1939015686512
forward train acc: top1 ->  83.24599997314454 ; top5 ->  98.872  and loss:  49.785372614860535
test acc: top1 ->  79.5 ; top5 ->  97.9  and loss:  65.43653684854507
forward train acc: top1 ->  84.17199997314454 ; top5 ->  98.93999998291015  and loss:  47.024756610393524
test acc: top1 ->  80.67 ; top5 ->  98.24  and loss:  61.2925643324852
forward train acc: top1 ->  84.83200000976562 ; top5 ->  99.08600000244141  and loss:  44.74349546432495
test acc: top1 ->  81.02 ; top5 ->  98.18  and loss:  60.52111944556236
forward train acc: top1 ->  85.24200000488281 ; top5 ->  99.14799997802734  and loss:  43.64594125747681
test acc: top1 ->  81.35 ; top5 ->  98.28  and loss:  59.636024951934814
forward train acc: top1 ->  85.44200001708984 ; top5 ->  99.12800000244141  and loss:  43.05923140048981
test acc: top1 ->  81.33 ; top5 ->  98.24  and loss:  59.887930661439896
forward train acc: top1 ->  85.98399997802734 ; top5 ->  99.1360000024414  and loss:  41.7521316409111
test acc: top1 ->  81.35 ; top5 ->  98.29  and loss:  59.65586030483246
forward train acc: top1 ->  86.08599999267578 ; top5 ->  99.18800000488281  and loss:  41.21513441205025
test acc: top1 ->  81.85 ; top5 ->  98.33  and loss:  57.787566781044006
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -86.99235588684678 , diff:  86.99235588684678
adv train loss:  -437.16255235671997 , diff:  350.1701964698732
adv train loss:  -564.8453407287598 , diff:  127.6827883720398
adv train loss:  -572.4223794937134 , diff:  7.577038764953613
adv train loss:  -581.3016486167908 , diff:  8.879269123077393
adv train loss:  -587.4276356697083 , diff:  6.1259870529174805
adv train loss:  -590.2699398994446 , diff:  2.842304229736328
adv train loss:  -590.0753908157349 , diff:  0.1945490837097168
adv train loss:  -598.6763844490051 , diff:  8.600993633270264
adv train loss:  -601.3472437858582 , diff:  2.6708593368530273
layer  4  adv train finish, try to retain  18
test acc: top1 ->  10.0 ; top5 ->  50.02  and loss:  1169.850811958313
forward train acc: top1 ->  63.26799999023437 ; top5 ->  95.45799998291015  and loss:  103.34825956821442
test acc: top1 ->  66.33 ; top5 ->  95.92  and loss:  98.5692407488823
forward train acc: top1 ->  69.86599997314453 ; top5 ->  96.87600001220703  and loss:  85.41449511051178
test acc: top1 ->  69.13 ; top5 ->  96.74  and loss:  89.69692713022232
forward train acc: top1 ->  72.79000000488281 ; top5 ->  97.43600000732422  and loss:  77.16267198324203
test acc: top1 ->  72.04 ; top5 ->  97.21  and loss:  80.84519785642624
forward train acc: top1 ->  74.7519999975586 ; top5 ->  97.74199998291016  and loss:  71.81081956624985
test acc: top1 ->  73.66 ; top5 ->  97.6  and loss:  77.4932508468628
forward train acc: top1 ->  76.17400000488281 ; top5 ->  97.97999999023438  and loss:  68.1058531999588
test acc: top1 ->  74.84 ; top5 ->  97.68  and loss:  73.95234543085098
forward train acc: top1 ->  77.03399997070312 ; top5 ->  98.08200000732423  and loss:  65.73554188013077
test acc: top1 ->  75.39 ; top5 ->  97.84  and loss:  72.61648505926132
forward train acc: top1 ->  77.44399999023437 ; top5 ->  98.16799998291016  and loss:  64.29668939113617
test acc: top1 ->  75.86 ; top5 ->  97.93  and loss:  71.14841163158417
forward train acc: top1 ->  77.66800000976562 ; top5 ->  98.18200000732422  and loss:  64.03103256225586
test acc: top1 ->  75.88 ; top5 ->  97.97  and loss:  70.60267198085785
forward train acc: top1 ->  78.13599999023438 ; top5 ->  98.29599998779297  and loss:  62.55853992700577
test acc: top1 ->  76.29 ; top5 ->  98.03  and loss:  69.20888623595238
forward train acc: top1 ->  78.72200000976562 ; top5 ->  98.35199998291016  and loss:  61.007913410663605
test acc: top1 ->  76.45 ; top5 ->  98.12  and loss:  68.75293070077896
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -168.08685659617186 , diff:  168.08685659617186
adv train loss:  -775.1128425598145 , diff:  607.0259859636426
adv train loss:  -830.7230005264282 , diff:  55.61015796661377
adv train loss:  -839.9136457443237 , diff:  9.190645217895508
adv train loss:  -841.1894416809082 , diff:  1.2757959365844727
adv train loss:  -856.6406965255737 , diff:  15.451254844665527
adv train loss:  -867.2537975311279 , diff:  10.6131010055542
adv train loss:  -866.6643514633179 , diff:  0.5894460678100586
adv train loss:  -871.5776081085205 , diff:  4.913256645202637
adv train loss:  -871.0963468551636 , diff:  0.4812612533569336
layer  5  adv train finish, try to retain  24
test acc: top1 ->  14.79 ; top5 ->  51.1  and loss:  790.9774127006531
forward train acc: top1 ->  82.3619999975586 ; top5 ->  99.0520000024414  and loss:  50.402038395404816
test acc: top1 ->  80.04 ; top5 ->  98.73  and loss:  60.55297574400902
forward train acc: top1 ->  85.45600000488281 ; top5 ->  99.38799998046875  and loss:  41.180593609809875
test acc: top1 ->  81.91 ; top5 ->  98.98  and loss:  56.09116044640541
forward train acc: top1 ->  86.9659999975586 ; top5 ->  99.53599997802735  and loss:  37.11354821920395
test acc: top1 ->  83.22 ; top5 ->  98.99  and loss:  53.08498680591583
forward train acc: top1 ->  88.05799998779297 ; top5 ->  99.5940000024414  and loss:  33.807504534721375
test acc: top1 ->  83.64 ; top5 ->  98.99  and loss:  52.06533160805702
forward train acc: top1 ->  88.92999999267577 ; top5 ->  99.676  and loss:  31.557487040758133
test acc: top1 ->  84.13 ; top5 ->  98.92  and loss:  51.84445255994797
forward train acc: top1 ->  89.47000000732422 ; top5 ->  99.62799997558594  and loss:  30.033331722021103
test acc: top1 ->  84.66 ; top5 ->  99.18  and loss:  49.511200964450836
forward train acc: top1 ->  89.63199999023438 ; top5 ->  99.692  and loss:  29.678940013051033
test acc: top1 ->  84.53 ; top5 ->  99.09  and loss:  50.15406268835068
forward train acc: top1 ->  89.88600001464843 ; top5 ->  99.66199997802734  and loss:  28.77297855913639
test acc: top1 ->  84.98 ; top5 ->  99.15  and loss:  48.885459288954735
forward train acc: top1 ->  90.16000001708984 ; top5 ->  99.72399997558594  and loss:  27.89303134381771
test acc: top1 ->  84.89 ; top5 ->  99.11  and loss:  49.14594113826752
forward train acc: top1 ->  90.30399997802735 ; top5 ->  99.72999997558594  and loss:  27.61957396566868
test acc: top1 ->  85.08 ; top5 ->  99.15  and loss:  48.67929865419865
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -198.6691518574953 , diff:  198.6691518574953
adv train loss:  -868.4305448532104 , diff:  669.7613929957151
adv train loss:  -936.9418964385986 , diff:  68.51135158538818
adv train loss:  -959.6172971725464 , diff:  22.675400733947754
adv train loss:  -961.3938322067261 , diff:  1.7765350341796875
adv train loss:  -977.2583341598511 , diff:  15.864501953125
adv train loss:  -982.7925033569336 , diff:  5.5341691970825195
adv train loss:  -995.5578155517578 , diff:  12.765312194824219
adv train loss:  -1027.0360651016235 , diff:  31.478249549865723
adv train loss:  -1052.219069480896 , diff:  25.18300437927246
layer  6  adv train finish, try to retain  27
test acc: top1 ->  62.14 ; top5 ->  94.34  and loss:  136.24320590496063
forward train acc: top1 ->  95.78600001953124 ; top5 ->  99.91  and loss:  12.719659350812435
test acc: top1 ->  89.28 ; top5 ->  99.21  and loss:  42.98356720805168
forward train acc: top1 ->  97.34199998535156 ; top5 ->  99.976  and loss:  7.652019664645195
test acc: top1 ->  89.35 ; top5 ->  99.25  and loss:  43.2773068472743
forward train acc: top1 ->  97.94000000732422 ; top5 ->  99.99  and loss:  5.974053531885147
test acc: top1 ->  89.6 ; top5 ->  99.14  and loss:  45.7726060077548
forward train acc: top1 ->  98.29599998535156 ; top5 ->  99.992  and loss:  4.929774096235633
test acc: top1 ->  89.83 ; top5 ->  99.25  and loss:  46.41126398742199
forward train acc: top1 ->  98.48799998535156 ; top5 ->  99.98999997558593  and loss:  4.4180003590881824
test acc: top1 ->  90.03 ; top5 ->  99.19  and loss:  47.58620437979698
forward train acc: top1 ->  98.69599998046876 ; top5 ->  99.992  and loss:  3.8259111903607845
test acc: top1 ->  90.03 ; top5 ->  99.2  and loss:  47.145344987511635
forward train acc: top1 ->  98.63199998291016 ; top5 ->  99.986  and loss:  3.8656966537237167
test acc: top1 ->  90.24 ; top5 ->  99.21  and loss:  47.45863176137209
forward train acc: top1 ->  98.75399998046875 ; top5 ->  99.988  and loss:  3.6637116987258196
test acc: top1 ->  90.03 ; top5 ->  99.25  and loss:  47.92386756837368
forward train acc: top1 ->  98.74400000976563 ; top5 ->  99.992  and loss:  3.659908652305603
test acc: top1 ->  90.12 ; top5 ->  99.19  and loss:  48.291188418865204
forward train acc: top1 ->  98.84399997802734 ; top5 ->  99.99  and loss:  3.375979881733656
test acc: top1 ->  90.23 ; top5 ->  99.24  and loss:  48.428012393414974
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  100 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -2.079889504238963 , diff:  2.079889504238963
adv train loss:  -2.1309893829748034 , diff:  0.05109987873584032
adv train loss:  -2.011537190526724 , diff:  0.11945219244807959
adv train loss:  -2.0469985846430063 , diff:  0.03546139411628246
adv train loss:  -2.0716039519757032 , diff:  0.024605367332696915
adv train loss:  -1.939671704545617 , diff:  0.13193224743008614
adv train loss:  -2.0230308528989553 , diff:  0.08335914835333824
adv train loss:  -2.0223421044647694 , diff:  0.0006887484341859818
layer  7  adv train finish, try to retain  259
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -223.94981980323792 , diff:  223.94981980323792
adv train loss:  -225.06640720367432 , diff:  1.1165874004364014
adv train loss:  -259.2635998725891 , diff:  34.197192668914795
adv train loss:  -328.9892268180847 , diff:  69.7256269454956
adv train loss:  -328.9706587791443 , diff:  0.018568038940429688
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  36
test acc: top1 ->  14.63 ; top5 ->  55.81  and loss:  4458441.8125
forward train acc: top1 ->  98.27999997802735 ; top5 ->  99.996  and loss:  6.016093149781227
test acc: top1 ->  91.55 ; top5 ->  99.26  and loss:  40.968753822147846
forward train acc: top1 ->  99.66600000244141 ; top5 ->  100.0  and loss:  1.2289968021214008
test acc: top1 ->  91.82 ; top5 ->  99.29  and loss:  45.30580858141184
forward train acc: top1 ->  99.804 ; top5 ->  100.0  and loss:  0.742183739785105
test acc: top1 ->  91.94 ; top5 ->  99.2  and loss:  48.06766491010785
forward train acc: top1 ->  99.80799997558594 ; top5 ->  100.0  and loss:  0.6220671571791172
test acc: top1 ->  91.89 ; top5 ->  99.3  and loss:  49.67659301310778
forward train acc: top1 ->  99.816 ; top5 ->  99.998  and loss:  0.5769993528956547
test acc: top1 ->  92.02 ; top5 ->  99.35  and loss:  49.771351993083954
forward train acc: top1 ->  99.85599997558593 ; top5 ->  100.0  and loss:  0.4514571516774595
test acc: top1 ->  92.13 ; top5 ->  99.34  and loss:  50.68314952403307
==> this epoch:  36 / 512
---------------- start layer  9  ---------------
adv train loss:  -613.0653982162476 , diff:  613.0653982162476
adv train loss:  -727.5449810028076 , diff:  114.47958278656006
adv train loss:  -757.1236848831177 , diff:  29.57870388031006
adv train loss:  -758.6153025627136 , diff:  1.4916176795959473
adv train loss:  -759.6859793663025 , diff:  1.0706768035888672
adv train loss:  -759.186375617981 , diff:  0.4996037483215332
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  3
test acc: top1 ->  10.0 ; top5 ->  51.6  and loss:  12509.983139038086
forward train acc: top1 ->  98.37199997802735 ; top5 ->  99.944  and loss:  5.59666633233428
test acc: top1 ->  73.42 ; top5 ->  98.23  and loss:  130.93380188941956
forward train acc: top1 ->  99.694 ; top5 ->  100.0  and loss:  1.1990211252123117
test acc: top1 ->  91.35 ; top5 ->  98.82  and loss:  52.31112319231033
forward train acc: top1 ->  99.78799997558593 ; top5 ->  100.0  and loss:  0.7808825392276049
test acc: top1 ->  91.67 ; top5 ->  98.89  and loss:  53.44430869817734
forward train acc: top1 ->  99.83999997558594 ; top5 ->  100.0  and loss:  0.6335183586925268
test acc: top1 ->  91.68 ; top5 ->  98.91  and loss:  53.85030208528042
forward train acc: top1 ->  99.876 ; top5 ->  100.0  and loss:  0.5050364232156426
test acc: top1 ->  91.69 ; top5 ->  98.97  and loss:  55.52516242861748
forward train acc: top1 ->  99.896 ; top5 ->  100.0  and loss:  0.4126615554559976
test acc: top1 ->  91.84 ; top5 ->  99.04  and loss:  55.8251546472311
forward train acc: top1 ->  99.904 ; top5 ->  100.0  and loss:  0.37631944520398974
test acc: top1 ->  91.94 ; top5 ->  99.0  and loss:  55.80993615835905
forward train acc: top1 ->  99.896 ; top5 ->  100.0  and loss:  0.36355850496329367
test acc: top1 ->  91.9 ; top5 ->  99.01  and loss:  56.24609521776438
forward train acc: top1 ->  99.91199997558594 ; top5 ->  99.998  and loss:  0.3320364165119827
test acc: top1 ->  92.0 ; top5 ->  98.98  and loss:  57.17100287973881
forward train acc: top1 ->  99.914 ; top5 ->  100.0  and loss:  0.3477262371452525
test acc: top1 ->  91.97 ; top5 ->  99.03  and loss:  57.63106415234506
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  4 / 512 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -444.1050684452057 , diff:  444.1050684452057
adv train loss:  -445.03962659835815 , diff:  0.9345581531524658
adv train loss:  -445.8913416862488 , diff:  0.851715087890625
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  10
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  443461.8269042969
forward train acc: top1 ->  90.61799998535156 ; top5 ->  99.336  and loss:  40.74825172871351
test acc: top1 ->  21.93 ; top5 ->  79.87  and loss:  660.4346675872803
forward train acc: top1 ->  99.12799997802735 ; top5 ->  99.996  and loss:  4.663121400400996
test acc: top1 ->  90.01 ; top5 ->  98.96  and loss:  44.61239047348499
forward train acc: top1 ->  99.3920000024414 ; top5 ->  99.994  and loss:  2.7229650020599365
test acc: top1 ->  90.99 ; top5 ->  98.92  and loss:  42.96342894434929
forward train acc: top1 ->  99.67000000244141 ; top5 ->  100.0  and loss:  1.553710684645921
test acc: top1 ->  91.04 ; top5 ->  99.08  and loss:  44.52011874318123
forward train acc: top1 ->  99.68999997802734 ; top5 ->  100.0  and loss:  1.210254083853215
test acc: top1 ->  91.32 ; top5 ->  99.18  and loss:  45.85511239618063
forward train acc: top1 ->  99.786 ; top5 ->  100.0  and loss:  0.9359784212429076
test acc: top1 ->  91.49 ; top5 ->  99.1  and loss:  46.27098175138235
forward train acc: top1 ->  99.81199997558593 ; top5 ->  100.0  and loss:  0.8628173768520355
test acc: top1 ->  91.49 ; top5 ->  99.16  and loss:  46.553965620696545
forward train acc: top1 ->  99.81599997558594 ; top5 ->  100.0  and loss:  0.7760884480085224
test acc: top1 ->  91.58 ; top5 ->  99.15  and loss:  47.03765222802758
forward train acc: top1 ->  99.84 ; top5 ->  100.0  and loss:  0.6431442860048264
test acc: top1 ->  91.53 ; top5 ->  99.15  and loss:  47.51497393101454
forward train acc: top1 ->  99.82999997558593 ; top5 ->  100.0  and loss:  0.6902685681125149
test acc: top1 ->  91.51 ; top5 ->  99.23  and loss:  48.25959421694279
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  11 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -3373.7370643615723 , diff:  3373.7370643615723
adv train loss:  -3374.7701988220215 , diff:  1.0331344604492188
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  1
test acc: top1 ->  9.99 ; top5 ->  63.83  and loss:  10002.365989685059
forward train acc: top1 ->  80.53800000976563 ; top5 ->  97.036  and loss:  105.94364556670189
test acc: top1 ->  82.1 ; top5 ->  98.6  and loss:  68.2674406170845
forward train acc: top1 ->  98.19400000732422 ; top5 ->  99.996  and loss:  9.361668393015862
test acc: top1 ->  89.08 ; top5 ->  98.65  and loss:  54.16218948364258
forward train acc: top1 ->  98.75000000244141 ; top5 ->  99.994  and loss:  6.02263456210494
test acc: top1 ->  89.48 ; top5 ->  98.74  and loss:  53.59900372847915
forward train acc: top1 ->  99.0920000048828 ; top5 ->  99.998  and loss:  4.326499285176396
test acc: top1 ->  89.99 ; top5 ->  98.76  and loss:  54.338120982050896
forward train acc: top1 ->  99.28800000244141 ; top5 ->  100.0  and loss:  3.2994579151272774
test acc: top1 ->  90.03 ; top5 ->  98.8  and loss:  55.592199601233006
forward train acc: top1 ->  99.446 ; top5 ->  100.0  and loss:  2.612778623588383
test acc: top1 ->  90.31 ; top5 ->  98.83  and loss:  55.63238013163209
forward train acc: top1 ->  99.55 ; top5 ->  99.996  and loss:  2.2929077250882983
test acc: top1 ->  90.31 ; top5 ->  98.86  and loss:  56.23148339986801
forward train acc: top1 ->  99.55599997802734 ; top5 ->  100.0  and loss:  2.011241391301155
test acc: top1 ->  90.45 ; top5 ->  98.89  and loss:  56.46753310598433
forward train acc: top1 ->  99.6020000024414 ; top5 ->  100.0  and loss:  1.8284364622086287
test acc: top1 ->  90.46 ; top5 ->  98.88  and loss:  56.948533326387405
forward train acc: top1 ->  99.5940000024414 ; top5 ->  99.998  and loss:  1.7572025060653687
test acc: top1 ->  90.59 ; top5 ->  98.92  and loss:  57.515552340075374
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  2 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -1685.7365064620972 , diff:  1685.7365064620972
adv train loss:  -1889.2452545166016 , diff:  203.5087480545044
adv train loss:  -1891.057970046997 , diff:  1.8127155303955078
adv train loss:  -1889.479471206665 , diff:  1.5784988403320312
adv train loss:  -1890.247179031372 , diff:  0.7677078247070312
layer  12  adv train finish, try to retain  2
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  5091.211891174316
forward train acc: top1 ->  52.65199999267578 ; top5 ->  89.91599997802734  and loss:  542.8964240550995
test acc: top1 ->  61.66 ; top5 ->  98.26  and loss:  185.08382272720337
forward train acc: top1 ->  87.54399998291015 ; top5 ->  99.94  and loss:  46.78616374731064
test acc: top1 ->  87.58 ; top5 ->  98.49  and loss:  63.13591882586479
forward train acc: top1 ->  98.00199997802734 ; top5 ->  99.99  and loss:  18.223496943712234
test acc: top1 ->  88.25 ; top5 ->  98.58  and loss:  58.35416367650032
forward train acc: top1 ->  98.51999998046875 ; top5 ->  99.988  and loss:  13.578736670315266
test acc: top1 ->  88.68 ; top5 ->  98.67  and loss:  56.837019592523575
forward train acc: top1 ->  98.79200000488281 ; top5 ->  99.994  and loss:  10.906112298369408
test acc: top1 ->  89.08 ; top5 ->  98.63  and loss:  56.881338849663734
forward train acc: top1 ->  98.956 ; top5 ->  99.998  and loss:  9.404303573071957
test acc: top1 ->  89.21 ; top5 ->  98.66  and loss:  55.98841652274132
forward train acc: top1 ->  99.08599997802735 ; top5 ->  99.996  and loss:  8.633421838283539
test acc: top1 ->  89.1 ; top5 ->  98.69  and loss:  56.270176097750664
forward train acc: top1 ->  99.05000000732421 ; top5 ->  99.996  and loss:  7.942392736673355
test acc: top1 ->  89.41 ; top5 ->  98.66  and loss:  57.00044992566109
forward train acc: top1 ->  99.16799997802734 ; top5 ->  99.992  and loss:  7.149864640086889
test acc: top1 ->  89.32 ; top5 ->  98.66  and loss:  56.9845055937767
forward train acc: top1 ->  99.1860000024414 ; top5 ->  99.996  and loss:  6.579162422567606
test acc: top1 ->  89.41 ; top5 ->  98.61  and loss:  58.00635872781277
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  5 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -8721.844993591309 , diff:  8721.844993591309
adv train loss:  -13536.348983764648 , diff:  4814.50399017334
adv train loss:  -18159.984176635742 , diff:  4623.635192871094
adv train loss:  -22771.234588623047 , diff:  4611.250411987305
adv train loss:  -27445.504028320312 , diff:  4674.269439697266
adv train loss:  -32223.328063964844 , diff:  4777.824035644531
adv train loss:  -37023.331634521484 , diff:  4800.003570556641
adv train loss:  -41879.851989746094 , diff:  4856.520355224609
adv train loss:  -46724.096618652344 , diff:  4844.24462890625
adv train loss:  -51286.238830566406 , diff:  4562.1422119140625
layer  13  adv train finish, try to retain  24
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.640625  ==>  41 / 64 , inc:  2
layer  2  :  0.703125  ==>  90 / 128 , inc:  4
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.390625  ==>  100 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.0703125  ==>  36 / 512 , inc:  2
layer  9  :  0.0078125  ==>  4 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.009765625  ==>  5 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [1.4980551502569064, 5.326418312024557, 5.326418312024557, 1.4980551502569064, 1.4980551502569064, 1.4980551502569064, 1.1235413626926798, 1.9974068670092087, 1.7754727706748523, 0.9987034335046043, 0.8426560220195098, 0.9987034335046043, 1.1235413626926798, 21.305673248098227]  wait [2, 0, 0, 2, 2, 2, 4, 2, 0, 2, 4, 3, 3, 1]  inc [1, 2, 4, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1]  tol: 13
$$$$$$$$$$$$$ epoch  86  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -2331.1512603759766 , diff:  2331.1512603759766
adv train loss:  -2389.7722969055176 , diff:  58.621036529541016
adv train loss:  -2371.573305130005 , diff:  18.198991775512695
adv train loss:  -2376.596803665161 , diff:  5.02349853515625
adv train loss:  -2378.7516708374023 , diff:  2.154867172241211
adv train loss:  -2366.924259185791 , diff:  11.827411651611328
adv train loss:  -2372.983678817749 , diff:  6.059419631958008
adv train loss:  -2377.9140338897705 , diff:  4.930355072021484
adv train loss:  -2371.5748977661133 , diff:  6.339136123657227
adv train loss:  -2327.2712745666504 , diff:  44.30362319946289
layer  0  adv train finish, try to retain  6
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  5961.473358154297
forward train acc: top1 ->  34.83999999267578 ; top5 ->  79.8859999975586  and loss:  641.0103471279144
test acc: top1 ->  15.95 ; top5 ->  61.08  and loss:  736.0976605415344
forward train acc: top1 ->  42.999999985351565 ; top5 ->  86.16799997314453  and loss:  215.16498160362244
test acc: top1 ->  48.07 ; top5 ->  88.31  and loss:  168.740860581398
forward train acc: top1 ->  47.56599999389648 ; top5 ->  88.80400001708985  and loss:  158.4864432811737
test acc: top1 ->  50.95 ; top5 ->  90.17  and loss:  149.17356705665588
forward train acc: top1 ->  50.03200000366211 ; top5 ->  90.24200001708985  and loss:  145.68422484397888
test acc: top1 ->  53.23 ; top5 ->  91.18  and loss:  139.7561845779419
forward train acc: top1 ->  52.302000004882814 ; top5 ->  91.06  and loss:  138.8336638212204
test acc: top1 ->  55.13 ; top5 ->  92.08  and loss:  134.8536182641983
forward train acc: top1 ->  53.68999999633789 ; top5 ->  91.71400000976563  and loss:  134.70524418354034
test acc: top1 ->  56.59 ; top5 ->  92.26  and loss:  132.36938166618347
forward train acc: top1 ->  55.02199998901367 ; top5 ->  92.04000000488281  and loss:  131.5502610206604
test acc: top1 ->  57.16 ; top5 ->  92.63  and loss:  129.28088188171387
forward train acc: top1 ->  56.115999997558596 ; top5 ->  92.45199997802735  and loss:  128.5416419506073
test acc: top1 ->  58.22 ; top5 ->  92.83  and loss:  126.89063715934753
forward train acc: top1 ->  57.284 ; top5 ->  92.81799997314454  and loss:  124.96372091770172
test acc: top1 ->  59.53 ; top5 ->  93.21  and loss:  123.2570778131485
forward train acc: top1 ->  58.76200000854492 ; top5 ->  93.26000000732422  and loss:  121.4709484577179
test acc: top1 ->  60.6 ; top5 ->  93.6  and loss:  119.88173592090607
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -31.93558958172798 , diff:  31.93558958172798
adv train loss:  -31.929749220609665 , diff:  0.00584036111831665
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  39
test acc: top1 ->  13.39 ; top5 ->  50.2  and loss:  308.0487473011017
forward train acc: top1 ->  99.32399997558593 ; top5 ->  99.994  and loss:  7.104384591802955
test acc: top1 ->  91.37 ; top5 ->  99.28  and loss:  36.90025907754898
forward train acc: top1 ->  99.7640000024414 ; top5 ->  99.998  and loss:  1.044145722873509
test acc: top1 ->  91.69 ; top5 ->  99.26  and loss:  42.30406380444765
forward train acc: top1 ->  99.84200000244141 ; top5 ->  100.0  and loss:  0.614705890417099
test acc: top1 ->  91.68 ; top5 ->  99.24  and loss:  46.739942878484726
forward train acc: top1 ->  99.834 ; top5 ->  100.0  and loss:  0.5940593516570516
test acc: top1 ->  91.72 ; top5 ->  99.2  and loss:  47.6281034424901
forward train acc: top1 ->  99.8520000024414 ; top5 ->  99.998  and loss:  0.5143971852958202
test acc: top1 ->  91.77 ; top5 ->  99.24  and loss:  49.451862663030624
forward train acc: top1 ->  99.886 ; top5 ->  100.0  and loss:  0.4255893512163311
test acc: top1 ->  91.71 ; top5 ->  99.18  and loss:  48.838668547570705
forward train acc: top1 ->  99.90800000244141 ; top5 ->  100.0  and loss:  0.343983362428844
test acc: top1 ->  91.88 ; top5 ->  99.28  and loss:  50.3819380402565
forward train acc: top1 ->  99.87 ; top5 ->  100.0  and loss:  0.3834943362744525
test acc: top1 ->  91.67 ; top5 ->  99.26  and loss:  50.80214709043503
forward train acc: top1 ->  99.896 ; top5 ->  99.998  and loss:  0.36599218047922477
test acc: top1 ->  91.78 ; top5 ->  99.23  and loss:  50.26162810251117
forward train acc: top1 ->  99.908 ; top5 ->  100.0  and loss:  0.3030359495896846
test acc: top1 ->  91.85 ; top5 ->  99.22  and loss:  51.765895806252956
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  41 / 64 , inc:  2
---------------- start layer  2  ---------------
adv train loss:  -0.5372208654880524 , diff:  0.5372208654880524
adv train loss:  -0.39090409164782614 , diff:  0.14631677384022623
adv train loss:  -0.49939094949513674 , diff:  0.1084868578473106
adv train loss:  -0.47342152218334377 , diff:  0.02596942731179297
adv train loss:  -0.5170485051348805 , diff:  0.043626982951536775
adv train loss:  -0.5059275343082845 , diff:  0.011120970826596022
adv train loss:  -0.41715632379055023 , diff:  0.08877121051773429
adv train loss:  -0.5749418456107378 , diff:  0.15778552182018757
adv train loss:  -0.4373145259451121 , diff:  0.1376273196656257
adv train loss:  -0.5685983302537352 , diff:  0.13128380430862308
************ all values are small in this layer **********
layer  2  adv train finish, try to retain  86
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  1198.2149515151978
forward train acc: top1 ->  99.73399997802734 ; top5 ->  99.996  and loss:  0.8442049268633127
test acc: top1 ->  91.76 ; top5 ->  99.2  and loss:  55.5624882504344
forward train acc: top1 ->  99.83 ; top5 ->  100.0  and loss:  0.5513957044458948
test acc: top1 ->  91.76 ; top5 ->  99.36  and loss:  53.85133668035269
forward train acc: top1 ->  99.84199997558594 ; top5 ->  100.0  and loss:  0.5103709390386939
test acc: top1 ->  92.0 ; top5 ->  99.26  and loss:  54.26728058606386
forward train acc: top1 ->  99.864 ; top5 ->  100.0  and loss:  0.3998400531709194
test acc: top1 ->  91.77 ; top5 ->  99.25  and loss:  58.130211021751165
forward train acc: top1 ->  99.86999997558594 ; top5 ->  100.0  and loss:  0.3788982592523098
test acc: top1 ->  91.79 ; top5 ->  99.24  and loss:  56.20746700838208
forward train acc: top1 ->  99.878 ; top5 ->  100.0  and loss:  0.3835123139433563
test acc: top1 ->  91.97 ; top5 ->  99.23  and loss:  57.957668516784906
forward train acc: top1 ->  99.902 ; top5 ->  100.0  and loss:  0.3206193918013014
test acc: top1 ->  91.92 ; top5 ->  99.3  and loss:  58.320346251130104
forward train acc: top1 ->  99.89999997558594 ; top5 ->  100.0  and loss:  0.2901468179188669
test acc: top1 ->  92.03 ; top5 ->  99.32  and loss:  57.83535145968199
forward train acc: top1 ->  99.8980000024414 ; top5 ->  100.0  and loss:  0.28962851967662573
test acc: top1 ->  92.07 ; top5 ->  99.18  and loss:  57.555022560060024
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  90 / 128 , inc:  4
---------------- start layer  3  ---------------
adv train loss:  -678.8037914475426 , diff:  678.8037914475426
adv train loss:  -1365.3483629226685 , diff:  686.5445714751258
adv train loss:  -1497.5951251983643 , diff:  132.2467622756958
adv train loss:  -1552.6275177001953 , diff:  55.032392501831055
adv train loss:  -1559.732385635376 , diff:  7.104867935180664
adv train loss:  -1557.4385566711426 , diff:  2.2938289642333984
adv train loss:  -1559.3694515228271 , diff:  1.9308948516845703
adv train loss:  -1556.2470254898071 , diff:  3.1224260330200195
adv train loss:  -1556.5508308410645 , diff:  0.3038053512573242
adv train loss:  -1560.930263519287 , diff:  4.379432678222656
layer  3  adv train finish, try to retain  33
test acc: top1 ->  16.92 ; top5 ->  57.03  and loss:  966.19002866745
forward train acc: top1 ->  87.30999998046875 ; top5 ->  99.24199997558594  and loss:  47.07279954850674
test acc: top1 ->  82.88 ; top5 ->  98.53  and loss:  59.82205554842949
forward train acc: top1 ->  89.07800001953125 ; top5 ->  99.48199997558594  and loss:  32.82473303377628
test acc: top1 ->  83.74 ; top5 ->  98.7  and loss:  53.69599464535713
forward train acc: top1 ->  90.13399998535156 ; top5 ->  99.62200000244141  and loss:  28.8279497474432
test acc: top1 ->  84.31 ; top5 ->  98.87  and loss:  52.01967966556549
forward train acc: top1 ->  90.87200001220702 ; top5 ->  99.70200000244141  and loss:  26.62581941485405
test acc: top1 ->  84.96 ; top5 ->  98.96  and loss:  50.38208791613579
forward train acc: top1 ->  91.31400001220703 ; top5 ->  99.684  and loss:  25.439754873514175
test acc: top1 ->  85.29 ; top5 ->  98.99  and loss:  49.310846388339996
forward train acc: top1 ->  91.87999997558593 ; top5 ->  99.75199997558593  and loss:  23.55342073738575
test acc: top1 ->  85.54 ; top5 ->  98.9  and loss:  48.4547553062439
forward train acc: top1 ->  91.86400000488281 ; top5 ->  99.71199997558594  and loss:  23.990229785442352
test acc: top1 ->  85.56 ; top5 ->  98.93  and loss:  48.279780596494675
forward train acc: top1 ->  91.96599998046875 ; top5 ->  99.734  and loss:  23.275611862540245
test acc: top1 ->  85.92 ; top5 ->  99.05  and loss:  47.637625962495804
forward train acc: top1 ->  92.21799998046875 ; top5 ->  99.7420000024414  and loss:  22.61622492969036
test acc: top1 ->  86.02 ; top5 ->  99.0  and loss:  46.9982378333807
forward train acc: top1 ->  92.56000000488281 ; top5 ->  99.74599997558593  and loss:  21.69660323858261
test acc: top1 ->  86.32 ; top5 ->  99.03  and loss:  46.979305639863014
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -101.09022291749716 , diff:  101.09022291749716
adv train loss:  -649.4709701538086 , diff:  548.3807472363114
adv train loss:  -771.6464400291443 , diff:  122.1754698753357
adv train loss:  -841.550567150116 , diff:  69.90412712097168
adv train loss:  -877.0819158554077 , diff:  35.53134870529175
adv train loss:  -883.2971420288086 , diff:  6.215226173400879
adv train loss:  -886.3408184051514 , diff:  3.0436763763427734
adv train loss:  -883.9126663208008 , diff:  2.428152084350586
adv train loss:  -890.991756439209 , diff:  7.079090118408203
adv train loss:  -894.6900730133057 , diff:  3.6983165740966797
layer  4  adv train finish, try to retain  36
test acc: top1 ->  15.01 ; top5 ->  67.9  and loss:  858.269947052002
forward train acc: top1 ->  77.51 ; top5 ->  97.73400000732421  and loss:  68.51620376110077
test acc: top1 ->  76.68 ; top5 ->  97.6  and loss:  72.52746641635895
forward train acc: top1 ->  82.27800001708984 ; top5 ->  98.74599997802734  and loss:  52.27317026257515
test acc: top1 ->  78.86 ; top5 ->  98.15  and loss:  65.4323699772358
forward train acc: top1 ->  83.93800000976563 ; top5 ->  98.96599997558594  and loss:  46.846723049879074
test acc: top1 ->  80.22 ; top5 ->  98.43  and loss:  61.70791685581207
forward train acc: top1 ->  85.27999999267578 ; top5 ->  99.1740000024414  and loss:  43.257993429899216
test acc: top1 ->  81.09 ; top5 ->  98.59  and loss:  59.17030030488968
forward train acc: top1 ->  86.07999999267578 ; top5 ->  99.1920000024414  and loss:  41.16237872838974
test acc: top1 ->  82.05 ; top5 ->  98.73  and loss:  56.06978562474251
forward train acc: top1 ->  86.74400000732422 ; top5 ->  99.33399997558594  and loss:  38.871119529008865
test acc: top1 ->  81.99 ; top5 ->  98.72  and loss:  55.64704221487045
forward train acc: top1 ->  87.06400000976562 ; top5 ->  99.36799997558593  and loss:  37.58489108085632
test acc: top1 ->  82.62 ; top5 ->  98.78  and loss:  55.022229850292206
forward train acc: top1 ->  87.30999999023437 ; top5 ->  99.37599997558594  and loss:  36.999129086732864
test acc: top1 ->  82.85 ; top5 ->  98.78  and loss:  53.909101128578186
forward train acc: top1 ->  87.20599999023437 ; top5 ->  99.31399997558594  and loss:  37.29148709774017
test acc: top1 ->  82.77 ; top5 ->  98.75  and loss:  53.798993080854416
forward train acc: top1 ->  87.50200001953125 ; top5 ->  99.40399997558593  and loss:  35.78806710243225
test acc: top1 ->  83.12 ; top5 ->  98.83  and loss:  52.85637125372887
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -229.52222725376487 , diff:  229.52222725376487
adv train loss:  -822.8738269805908 , diff:  593.351599726826
adv train loss:  -918.5030117034912 , diff:  95.62918472290039
adv train loss:  -918.7904424667358 , diff:  0.2874307632446289
adv train loss:  -935.2904243469238 , diff:  16.49998188018799
adv train loss:  -937.9741640090942 , diff:  2.68373966217041
adv train loss:  -936.513011932373 , diff:  1.4611520767211914
adv train loss:  -942.8079528808594 , diff:  6.294940948486328
adv train loss:  -944.1549434661865 , diff:  1.3469905853271484
adv train loss:  -946.29017162323 , diff:  2.135228157043457
layer  5  adv train finish, try to retain  43
test acc: top1 ->  14.56 ; top5 ->  62.42  and loss:  1205.9078702926636
forward train acc: top1 ->  90.06999997070312 ; top5 ->  99.51799997558594  and loss:  28.753544583916664
test acc: top1 ->  85.58 ; top5 ->  99.08  and loss:  46.27117457985878
forward train acc: top1 ->  92.60800000976562 ; top5 ->  99.728  and loss:  21.599241986870766
test acc: top1 ->  86.28 ; top5 ->  99.14  and loss:  46.24412454664707
forward train acc: top1 ->  93.32799998046875 ; top5 ->  99.79799997558594  and loss:  19.5640669465065
test acc: top1 ->  86.69 ; top5 ->  99.19  and loss:  44.58125750720501
forward train acc: top1 ->  93.95400001708984 ; top5 ->  99.832  and loss:  17.336246207356453
test acc: top1 ->  87.53 ; top5 ->  99.15  and loss:  43.56244853138924
forward train acc: top1 ->  94.42200000244141 ; top5 ->  99.878  and loss:  16.06729981303215
test acc: top1 ->  87.83 ; top5 ->  99.2  and loss:  42.75185567140579
forward train acc: top1 ->  94.742 ; top5 ->  99.874  and loss:  15.124076247215271
test acc: top1 ->  88.15 ; top5 ->  99.16  and loss:  42.40164543688297
forward train acc: top1 ->  94.80399999511718 ; top5 ->  99.878  and loss:  14.76943002641201
test acc: top1 ->  88.12 ; top5 ->  99.17  and loss:  42.52575954794884
forward train acc: top1 ->  95.03999999267577 ; top5 ->  99.91  and loss:  14.193955533206463
test acc: top1 ->  88.2 ; top5 ->  99.19  and loss:  42.5740442276001
forward train acc: top1 ->  95.06599999755859 ; top5 ->  99.912  and loss:  14.016540803015232
test acc: top1 ->  88.27 ; top5 ->  99.21  and loss:  42.110540464520454
forward train acc: top1 ->  95.29599998779297 ; top5 ->  99.886  and loss:  13.452360235154629
test acc: top1 ->  88.48 ; top5 ->  99.17  and loss:  42.36229847371578
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
adv train loss:  -4.016915399581194 , diff:  4.016915399581194
adv train loss:  -4.0184995494782925 , diff:  0.0015841498970985413
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  252
test acc: top1 ->  10.02 ; top5 ->  50.01  and loss:  8754079.28125
forward train acc: top1 ->  96.68000000488281 ; top5 ->  99.978  and loss:  11.880345802754164
test acc: top1 ->  90.84 ; top5 ->  99.07  and loss:  37.488017454743385
forward train acc: top1 ->  99.41600000244141 ; top5 ->  100.0  and loss:  2.4362051570788026
test acc: top1 ->  91.09 ; top5 ->  99.08  and loss:  40.339194785803556
forward train acc: top1 ->  99.5960000024414 ; top5 ->  99.998  and loss:  1.5252806656062603
test acc: top1 ->  91.12 ; top5 ->  99.07  and loss:  43.69301957450807
forward train acc: top1 ->  99.75199997558593 ; top5 ->  100.0  and loss:  1.0615968592464924
test acc: top1 ->  91.35 ; top5 ->  99.09  and loss:  44.777089677751064
forward train acc: top1 ->  99.762 ; top5 ->  99.998  and loss:  0.9054997323546559
test acc: top1 ->  91.54 ; top5 ->  99.15  and loss:  45.87651662528515
forward train acc: top1 ->  99.80000000244141 ; top5 ->  100.0  and loss:  0.7388047792483121
test acc: top1 ->  91.51 ; top5 ->  99.19  and loss:  46.096309481188655
forward train acc: top1 ->  99.82399997558593 ; top5 ->  100.0  and loss:  0.6960241836495697
test acc: top1 ->  91.54 ; top5 ->  99.19  and loss:  47.02930564433336
forward train acc: top1 ->  99.82999997558593 ; top5 ->  100.0  and loss:  0.6277322671376169
test acc: top1 ->  91.62 ; top5 ->  99.17  and loss:  47.343319525010884
forward train acc: top1 ->  99.84200000244141 ; top5 ->  100.0  and loss:  0.5579828207846731
test acc: top1 ->  91.63 ; top5 ->  99.18  and loss:  47.988137474283576
forward train acc: top1 ->  99.8460000024414 ; top5 ->  99.998  and loss:  0.5556498537771404
test acc: top1 ->  91.73 ; top5 ->  99.21  and loss:  48.567843897268176
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  253 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -234.35680484771729 , diff:  234.35680484771729
adv train loss:  -234.89417266845703 , diff:  0.5373678207397461
adv train loss:  -233.68708670139313 , diff:  1.2070859670639038
adv train loss:  -234.12505781650543 , diff:  0.4379711151123047
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  34
test acc: top1 ->  2.0 ; top5 ->  51.07  and loss:  32632811.8125
forward train acc: top1 ->  97.54800000244141 ; top5 ->  99.962  and loss:  9.658385675400496
test acc: top1 ->  91.48 ; top5 ->  99.1  and loss:  44.16008708626032
forward train acc: top1 ->  99.7420000024414 ; top5 ->  100.0  and loss:  0.993969569914043
test acc: top1 ->  91.67 ; top5 ->  99.14  and loss:  43.74526232481003
forward train acc: top1 ->  99.846 ; top5 ->  100.0  and loss:  0.6289499788545072
test acc: top1 ->  91.83 ; top5 ->  99.16  and loss:  44.27611266076565
forward train acc: top1 ->  99.908 ; top5 ->  100.0  and loss:  0.44927501515485346
test acc: top1 ->  91.97 ; top5 ->  99.22  and loss:  45.72207310050726
forward train acc: top1 ->  99.928 ; top5 ->  100.0  and loss:  0.35517888236790895
test acc: top1 ->  92.09 ; top5 ->  99.26  and loss:  46.198763597756624
forward train acc: top1 ->  99.906 ; top5 ->  100.0  and loss:  0.352079187752679
test acc: top1 ->  92.09 ; top5 ->  99.19  and loss:  47.097274608910084
forward train acc: top1 ->  99.924 ; top5 ->  100.0  and loss:  0.29925477132201195
test acc: top1 ->  92.06 ; top5 ->  99.23  and loss:  47.03901902958751
forward train acc: top1 ->  99.9 ; top5 ->  100.0  and loss:  0.33361108420649543
test acc: top1 ->  92.06 ; top5 ->  99.25  and loss:  47.80399828962982
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.23693419457413256
test acc: top1 ->  92.04 ; top5 ->  99.23  and loss:  48.48911238461733
forward train acc: top1 ->  99.936 ; top5 ->  100.0  and loss:  0.24882324412465096
test acc: top1 ->  92.09 ; top5 ->  99.23  and loss:  48.55057880282402
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  36 / 512 , inc:  2
---------------- start layer  9  ---------------
adv train loss:  -39.16937558352947 , diff:  39.16937558352947
adv train loss:  -39.43934750556946 , diff:  0.26997192203998566
adv train loss:  -39.09322538971901 , diff:  0.3461221158504486
adv train loss:  -39.50419642031193 , diff:  0.4109710305929184
adv train loss:  -39.13027735054493 , diff:  0.3739190697669983
adv train loss:  -39.445330530405045 , diff:  0.31505317986011505
adv train loss:  -38.524941831827164 , diff:  0.9203886985778809
adv train loss:  -39.58343632519245 , diff:  1.0584944933652878
adv train loss:  -39.36162528395653 , diff:  0.22181104123592377
adv train loss:  -39.07181166112423 , diff:  0.2898136228322983
layer  9  adv train finish, try to retain  510
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
### skip layer  10 wait:  4  ###
---------------- start layer  11  ---------------
### skip layer  11 wait:  3  ###
---------------- start layer  12  ---------------
### skip layer  12 wait:  3  ###
---------------- start layer  13  ---------------
adv train loss:  -11964.434288024902 , diff:  11964.434288024902
adv train loss:  -18745.803909301758 , diff:  6781.3696212768555
adv train loss:  -26116.254364013672 , diff:  7370.450454711914
adv train loss:  -34567.36587524414 , diff:  8451.111511230469
adv train loss:  -42706.8766784668 , diff:  8139.510803222656
adv train loss:  -50466.61019897461 , diff:  7759.7335205078125
adv train loss:  -58050.74597167969 , diff:  7584.135772705078
adv train loss:  -65536.40881347656 , diff:  7485.662841796875
adv train loss:  -72979.09020996094 , diff:  7442.681396484375
adv train loss:  -80356.25305175781 , diff:  7377.162841796875
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  8
test acc: top1 ->  14.48 ; top5 ->  73.61  and loss:  720.2114500999451
forward train acc: top1 ->  45.77799999877929 ; top5 ->  97.46799997558594  and loss:  163.82242572307587
test acc: top1 ->  57.51 ; top5 ->  98.03  and loss:  115.78602612018585
forward train acc: top1 ->  71.8720000048828 ; top5 ->  99.94  and loss:  87.13145166635513
test acc: top1 ->  66.1 ; top5 ->  97.99  and loss:  108.46072840690613
forward train acc: top1 ->  79.6360000024414 ; top5 ->  99.976  and loss:  78.82459896802902
test acc: top1 ->  69.39 ; top5 ->  98.01  and loss:  107.01269567012787
forward train acc: top1 ->  83.31199999755859 ; top5 ->  99.968  and loss:  73.8822010755539
test acc: top1 ->  77.2 ; top5 ->  97.97  and loss:  105.0772071480751
forward train acc: top1 ->  86.31999997314453 ; top5 ->  99.986  and loss:  70.04891568422318
test acc: top1 ->  79.01 ; top5 ->  97.9  and loss:  104.04879301786423
forward train acc: top1 ->  88.12600000732422 ; top5 ->  99.974  and loss:  66.96335834264755
test acc: top1 ->  80.74 ; top5 ->  97.84  and loss:  103.28680390119553
forward train acc: top1 ->  89.50200000488282 ; top5 ->  99.982  and loss:  64.71230125427246
test acc: top1 ->  78.87 ; top5 ->  97.85  and loss:  104.00818711519241
forward train acc: top1 ->  90.32800000488281 ; top5 ->  99.986  and loss:  62.686745405197144
test acc: top1 ->  81.83 ; top5 ->  97.79  and loss:  103.05524039268494
forward train acc: top1 ->  91.55200000488281 ; top5 ->  99.992  and loss:  60.20326977968216
test acc: top1 ->  81.57 ; top5 ->  97.76  and loss:  103.32787758111954
forward train acc: top1 ->  91.61199997070312 ; top5 ->  99.984  and loss:  58.82722043991089
test acc: top1 ->  81.99 ; top5 ->  97.73  and loss:  102.54950630664825
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  9 / 512 , inc:  1
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.640625  ==>  41 / 64 , inc:  1
layer  2  :  0.703125  ==>  90 / 128 , inc:  2
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.390625  ==>  100 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.0703125  ==>  36 / 512 , inc:  1
layer  9  :  0.0078125  ==>  4 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.009765625  ==>  5 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [1.1235413626926798, 3.9948137340184173, 3.9948137340184173, 1.1235413626926798, 1.1235413626926798, 1.1235413626926798, 1.1235413626926798, 1.4980551502569064, 1.3316045780061392, 1.9974068670092087, 0.8426560220195098, 0.9987034335046043, 1.1235413626926798, 15.97925493607367]  wait [4, 2, 2, 4, 4, 4, 3, 4, 2, 2, 3, 2, 2, 3]  inc [1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 13
$$$$$$$$$$$$$ epoch  87  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1811.4430050849915 , diff:  1811.4430050849915
adv train loss:  -1902.0195579528809 , diff:  90.5765528678894
adv train loss:  -1904.9323425292969 , diff:  2.9127845764160156
adv train loss:  -1896.396198272705 , diff:  8.536144256591797
adv train loss:  -1906.3772106170654 , diff:  9.981012344360352
adv train loss:  -1901.97287940979 , diff:  4.404331207275391
adv train loss:  -1902.512243270874 , diff:  0.5393638610839844
adv train loss:  -1891.858491897583 , diff:  10.653751373291016
adv train loss:  -1899.3769569396973 , diff:  7.518465042114258
adv train loss:  -1900.7193336486816 , diff:  1.342376708984375
layer  0  adv train finish, try to retain  26
test acc: top1 ->  10.11 ; top5 ->  51.44  and loss:  11137.219551086426
forward train acc: top1 ->  97.56200000488282 ; top5 ->  99.916  and loss:  10.088806356303394
test acc: top1 ->  90.31 ; top5 ->  98.78  and loss:  77.70818355679512
forward train acc: top1 ->  98.69000000488282 ; top5 ->  99.962  and loss:  5.002697279676795
test acc: top1 ->  90.76 ; top5 ->  98.89  and loss:  67.54898431897163
forward train acc: top1 ->  98.93000000976562 ; top5 ->  99.98  and loss:  3.5842776438221335
test acc: top1 ->  91.06 ; top5 ->  99.09  and loss:  61.469105914235115
forward train acc: top1 ->  99.09399997802734 ; top5 ->  99.98  and loss:  2.8148678825236857
test acc: top1 ->  91.27 ; top5 ->  99.23  and loss:  58.48960521817207
forward train acc: top1 ->  99.27000000488282 ; top5 ->  99.982  and loss:  2.412398209795356
test acc: top1 ->  91.39 ; top5 ->  99.19  and loss:  56.678131103515625
forward train acc: top1 ->  99.31399998046875 ; top5 ->  99.986  and loss:  2.269317875849083
test acc: top1 ->  91.41 ; top5 ->  99.21  and loss:  55.2708470672369
forward train acc: top1 ->  99.34399997558593 ; top5 ->  99.99  and loss:  2.1731485261116177
test acc: top1 ->  91.42 ; top5 ->  99.25  and loss:  53.786435052752495
forward train acc: top1 ->  99.40999997558593 ; top5 ->  99.994  and loss:  1.807320065330714
test acc: top1 ->  91.41 ; top5 ->  99.31  and loss:  53.518590182065964
forward train acc: top1 ->  99.50199997802734 ; top5 ->  99.998  and loss:  1.5381554246414453
test acc: top1 ->  91.4 ; top5 ->  99.32  and loss:  54.223596170544624
forward train acc: top1 ->  99.44399997558594 ; top5 ->  99.998  and loss:  1.582542659714818
test acc: top1 ->  91.33 ; top5 ->  99.3  and loss:  52.74105650186539
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -0.7675651253666729 , diff:  0.7675651253666729
adv train loss:  -0.723859037971124 , diff:  0.04370608739554882
adv train loss:  -0.8621642575599253 , diff:  0.13830521958880126
adv train loss:  -0.7580493659479544 , diff:  0.10411489161197096
adv train loss:  -0.7352716568857431 , diff:  0.022777709062211215
adv train loss:  -0.7236930360086262 , diff:  0.011578620877116919
adv train loss:  -0.7348542746622115 , diff:  0.011161238653585315
adv train loss:  -0.7008880820358172 , diff:  0.03396619262639433
adv train loss:  -0.6468831945676357 , diff:  0.05400488746818155
adv train loss:  -0.713834922760725 , diff:  0.06695172819308937
layer  1  adv train finish, try to retain  41
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -0.5831366463098675 , diff:  0.5831366463098675
adv train loss:  -0.5986429096665233 , diff:  0.015506263356655836
adv train loss:  -0.6687517121899873 , diff:  0.07010880252346396
adv train loss:  -0.6304698423482478 , diff:  0.038281869841739535
adv train loss:  -0.652803095523268 , diff:  0.022333253175020218
adv train loss:  -0.7186500830575824 , diff:  0.0658469875343144
adv train loss:  -0.6606388774234802 , diff:  0.058011205634102225
adv train loss:  -0.6605271236039698 , diff:  0.00011175381951034069
layer  2  adv train finish, try to retain  72
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  1269.0838842391968
forward train acc: top1 ->  98.53399997802734 ; top5 ->  99.974  and loss:  4.597845861688256
test acc: top1 ->  90.79 ; top5 ->  99.16  and loss:  52.11924812942743
forward train acc: top1 ->  98.91599998535156 ; top5 ->  99.984  and loss:  3.2226693062111735
test acc: top1 ->  90.83 ; top5 ->  99.18  and loss:  49.10331529378891
forward train acc: top1 ->  99.23399997802734 ; top5 ->  99.994  and loss:  2.389602738432586
test acc: top1 ->  90.93 ; top5 ->  99.19  and loss:  50.488669756799936
forward train acc: top1 ->  99.29599997558594 ; top5 ->  99.994  and loss:  2.0383106572553515
test acc: top1 ->  91.14 ; top5 ->  99.2  and loss:  51.33680473268032
forward train acc: top1 ->  99.3340000024414 ; top5 ->  99.998  and loss:  1.9397813621908426
test acc: top1 ->  90.86 ; top5 ->  99.25  and loss:  51.96003407984972
forward train acc: top1 ->  99.46799997558594 ; top5 ->  99.996  and loss:  1.564504544250667
test acc: top1 ->  91.08 ; top5 ->  99.22  and loss:  51.187559846788645
forward train acc: top1 ->  99.54599997558594 ; top5 ->  100.0  and loss:  1.3843768243677914
test acc: top1 ->  91.05 ; top5 ->  99.24  and loss:  50.898379135876894
forward train acc: top1 ->  99.51399997802734 ; top5 ->  99.998  and loss:  1.459558704867959
test acc: top1 ->  91.24 ; top5 ->  99.27  and loss:  51.68155599758029
forward train acc: top1 ->  99.55000000244141 ; top5 ->  100.0  and loss:  1.2958759171888232
test acc: top1 ->  91.07 ; top5 ->  99.25  and loss:  51.384696416556835
forward train acc: top1 ->  99.56600000244141 ; top5 ->  100.0  and loss:  1.3044669986702502
test acc: top1 ->  91.24 ; top5 ->  99.21  and loss:  51.93639391288161
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  90 / 128 , inc:  2
---------------- start layer  3  ---------------
adv train loss:  -577.4488473450765 , diff:  577.4488473450765
adv train loss:  -1229.0590839385986 , diff:  651.6102365935221
adv train loss:  -1285.7539901733398 , diff:  56.69490623474121
adv train loss:  -1275.4526834487915 , diff:  10.30130672454834
adv train loss:  -1306.226858139038 , diff:  30.774174690246582
adv train loss:  -1310.9658575057983 , diff:  4.738999366760254
adv train loss:  -1352.8315238952637 , diff:  41.86566638946533
adv train loss:  -1369.3328504562378 , diff:  16.50132656097412
adv train loss:  -1371.7077236175537 , diff:  2.374873161315918
adv train loss:  -1371.8603420257568 , diff:  0.152618408203125
layer  3  adv train finish, try to retain  81
test acc: top1 ->  15.84 ; top5 ->  58.24  and loss:  4468.736022949219
forward train acc: top1 ->  99.68199997558594 ; top5 ->  100.0  and loss:  0.9088162467814982
test acc: top1 ->  91.69 ; top5 ->  99.39  and loss:  51.931309305131435
forward train acc: top1 ->  99.79199997558594 ; top5 ->  100.0  and loss:  0.6420490490272641
test acc: top1 ->  91.84 ; top5 ->  99.38  and loss:  54.41708509251475
forward train acc: top1 ->  99.83399997558594 ; top5 ->  99.998  and loss:  0.5664406821597368
test acc: top1 ->  91.72 ; top5 ->  99.31  and loss:  56.218850165605545
forward train acc: top1 ->  99.85599997558593 ; top5 ->  99.998  and loss:  0.4681560159660876
test acc: top1 ->  91.87 ; top5 ->  99.41  and loss:  55.98848698288202
forward train acc: top1 ->  99.832 ; top5 ->  100.0  and loss:  0.4720957105746493
test acc: top1 ->  91.76 ; top5 ->  99.38  and loss:  57.629174157977104
forward train acc: top1 ->  99.86 ; top5 ->  100.0  and loss:  0.3964052232913673
test acc: top1 ->  91.89 ; top5 ->  99.35  and loss:  58.07937328517437
forward train acc: top1 ->  99.888 ; top5 ->  100.0  and loss:  0.3452663510106504
test acc: top1 ->  92.07 ; top5 ->  99.46  and loss:  58.09812627732754
forward train acc: top1 ->  99.862 ; top5 ->  100.0  and loss:  0.370989860733971
test acc: top1 ->  91.94 ; top5 ->  99.43  and loss:  58.64586376398802
forward train acc: top1 ->  99.89599997558594 ; top5 ->  100.0  and loss:  0.3094753304030746
test acc: top1 ->  91.93 ; top5 ->  99.38  and loss:  57.84814568608999
forward train acc: top1 ->  99.87 ; top5 ->  99.998  and loss:  0.3467803755775094
test acc: top1 ->  92.01 ; top5 ->  99.44  and loss:  57.89149187877774
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -269.6120262821205 , diff:  269.6120262821205
adv train loss:  -1469.5163087844849 , diff:  1199.9042825023644
adv train loss:  -1687.5901489257812 , diff:  218.0738401412964
adv train loss:  -1710.8497524261475 , diff:  23.25960350036621
adv train loss:  -1707.572850227356 , diff:  3.276902198791504
adv train loss:  -1724.1462516784668 , diff:  16.57340145111084
adv train loss:  -1756.0168991088867 , diff:  31.870647430419922
adv train loss:  -1767.0146007537842 , diff:  10.997701644897461
adv train loss:  -1792.7700958251953 , diff:  25.755495071411133
adv train loss:  -1803.9342460632324 , diff:  11.16415023803711
layer  4  adv train finish, try to retain  99
test acc: top1 ->  17.92 ; top5 ->  75.76  and loss:  4700.812906265259
forward train acc: top1 ->  97.27600001708984 ; top5 ->  99.94999997558594  and loss:  9.35599547997117
test acc: top1 ->  89.85 ; top5 ->  99.2  and loss:  51.892355747520924
forward train acc: top1 ->  97.56199998779297 ; top5 ->  99.98  and loss:  7.047999434173107
test acc: top1 ->  89.88 ; top5 ->  99.32  and loss:  46.53140538930893
forward train acc: top1 ->  97.93200000732422 ; top5 ->  99.984  and loss:  6.06655777990818
test acc: top1 ->  90.12 ; top5 ->  99.32  and loss:  45.383624494075775
forward train acc: top1 ->  98.08400001220703 ; top5 ->  99.978  and loss:  5.498953564092517
test acc: top1 ->  90.1 ; top5 ->  99.33  and loss:  45.14708613604307
forward train acc: top1 ->  98.22199998291016 ; top5 ->  99.982  and loss:  5.168403735384345
test acc: top1 ->  90.3 ; top5 ->  99.26  and loss:  45.87779377400875
forward train acc: top1 ->  98.27000000976562 ; top5 ->  99.986  and loss:  5.042282942682505
test acc: top1 ->  90.44 ; top5 ->  99.26  and loss:  44.612174831330776
forward train acc: top1 ->  98.38599998291015 ; top5 ->  99.986  and loss:  4.690128495916724
test acc: top1 ->  90.29 ; top5 ->  99.36  and loss:  44.56887376308441
forward train acc: top1 ->  98.38200000976562 ; top5 ->  99.984  and loss:  4.7005782425403595
test acc: top1 ->  90.49 ; top5 ->  99.3  and loss:  44.54475233703852
forward train acc: top1 ->  98.43399998291015 ; top5 ->  99.992  and loss:  4.459994815289974
test acc: top1 ->  90.54 ; top5 ->  99.3  and loss:  44.60824992507696
forward train acc: top1 ->  98.43399998291015 ; top5 ->  99.98799997558594  and loss:  4.34262765198946
test acc: top1 ->  90.5 ; top5 ->  99.31  and loss:  45.117675982415676
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  155 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -450.6912644095719 , diff:  450.6912644095719
adv train loss:  -1574.0487442016602 , diff:  1123.3574797920883
adv train loss:  -1660.416431427002 , diff:  86.3676872253418
adv train loss:  -1668.3422622680664 , diff:  7.925830841064453
adv train loss:  -1675.9174575805664 , diff:  7.5751953125
adv train loss:  -1672.2161540985107 , diff:  3.701303482055664
adv train loss:  -1669.9517917633057 , diff:  2.264362335205078
adv train loss:  -1670.108835220337 , diff:  0.15704345703125
adv train loss:  -1680.0391159057617 , diff:  9.930280685424805
adv train loss:  -1692.6789169311523 , diff:  12.639801025390625
layer  5  adv train finish, try to retain  111
test acc: top1 ->  20.97 ; top5 ->  79.07  and loss:  2615.8313179016113
forward train acc: top1 ->  99.03199997802734 ; top5 ->  99.996  and loss:  2.802730412222445
test acc: top1 ->  90.58 ; top5 ->  99.36  and loss:  46.459402836859226
forward train acc: top1 ->  99.34599997802735 ; top5 ->  100.0  and loss:  1.8808529684320092
test acc: top1 ->  91.1 ; top5 ->  99.25  and loss:  46.61388203129172
forward train acc: top1 ->  99.52200000488281 ; top5 ->  100.0  and loss:  1.437546624802053
test acc: top1 ->  91.05 ; top5 ->  99.39  and loss:  47.38680136576295
forward train acc: top1 ->  99.50599997558594 ; top5 ->  99.998  and loss:  1.3878238340839744
test acc: top1 ->  91.21 ; top5 ->  99.44  and loss:  49.287306535989046
forward train acc: top1 ->  99.57400000244141 ; top5 ->  100.0  and loss:  1.1846079137176275
test acc: top1 ->  91.28 ; top5 ->  99.35  and loss:  49.89942392706871
forward train acc: top1 ->  99.638 ; top5 ->  100.0  and loss:  1.046844456344843
test acc: top1 ->  91.42 ; top5 ->  99.39  and loss:  50.37185191735625
forward train acc: top1 ->  99.6200000024414 ; top5 ->  100.0  and loss:  1.0057665286585689
test acc: top1 ->  91.29 ; top5 ->  99.36  and loss:  50.83315970748663
forward train acc: top1 ->  99.678 ; top5 ->  100.0  and loss:  0.9672522285254672
test acc: top1 ->  91.37 ; top5 ->  99.38  and loss:  51.29694593697786
forward train acc: top1 ->  99.67199997558593 ; top5 ->  100.0  and loss:  0.9215478808619082
test acc: top1 ->  91.32 ; top5 ->  99.39  and loss:  51.12843710556626
forward train acc: top1 ->  99.672 ; top5 ->  100.0  and loss:  1.0133600663393736
test acc: top1 ->  91.43 ; top5 ->  99.45  and loss:  51.57737009972334
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -539.1387082841247 , diff:  539.1387082841247
adv train loss:  -1883.0679817199707 , diff:  1343.929273435846
adv train loss:  -2049.2366428375244 , diff:  166.1686611175537
adv train loss:  -2135.3722133636475 , diff:  86.13557052612305
adv train loss:  -2137.1754398345947 , diff:  1.8032264709472656
adv train loss:  -2144.200637817383 , diff:  7.025197982788086
adv train loss:  -2143.77388381958 , diff:  0.4267539978027344
adv train loss:  -2142.233331680298 , diff:  1.5405521392822266
adv train loss:  -2141.913549423218 , diff:  0.3197822570800781
adv train loss:  -2142.4189224243164 , diff:  0.5053730010986328
layer  6  adv train finish, try to retain  79
test acc: top1 ->  30.03 ; top5 ->  88.92  and loss:  2132.6726846694946
forward train acc: top1 ->  99.88999997558594 ; top5 ->  100.0  and loss:  0.373396577546373
test acc: top1 ->  91.72 ; top5 ->  99.27  and loss:  53.134588934481144
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.28056788648245856
test acc: top1 ->  91.86 ; top5 ->  99.38  and loss:  53.687601290643215
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.24468111037276685
test acc: top1 ->  91.93 ; top5 ->  99.31  and loss:  55.7618861682713
forward train acc: top1 ->  99.924 ; top5 ->  100.0  and loss:  0.22960087726823986
test acc: top1 ->  91.87 ; top5 ->  99.32  and loss:  58.14605602622032
forward train acc: top1 ->  99.92799997558593 ; top5 ->  100.0  and loss:  0.19559369515627623
test acc: top1 ->  91.94 ; top5 ->  99.26  and loss:  58.31838272511959
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.16770876781083643
test acc: top1 ->  92.12 ; top5 ->  99.37  and loss:  58.49194747582078
==> this epoch:  79 / 256
---------------- start layer  7  ---------------
adv train loss:  -0.16124755643249955 , diff:  0.16124755643249955
adv train loss:  -0.1936352252960205 , diff:  0.03238766886352096
adv train loss:  -0.1821930396836251 , diff:  0.011442185612395406
adv train loss:  -0.16769251332152635 , diff:  0.014500526362098753
adv train loss:  -0.2343574302503839 , diff:  0.06666491692885756
adv train loss:  -0.15887875366024673 , diff:  0.07547867659013718
adv train loss:  -0.1497840358497342 , diff:  0.00909471781051252
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  252
test acc: top1 ->  10.0 ; top5 ->  60.31  and loss:  13382842.7265625
forward train acc: top1 ->  99.702 ; top5 ->  100.0  and loss:  0.9831848456524312
test acc: top1 ->  91.36 ; top5 ->  99.1  and loss:  58.64205922186375
forward train acc: top1 ->  99.802 ; top5 ->  100.0  and loss:  0.6180990993743762
test acc: top1 ->  91.27 ; top5 ->  99.08  and loss:  58.85104422271252
forward train acc: top1 ->  99.8220000024414 ; top5 ->  100.0  and loss:  0.5176321242470294
test acc: top1 ->  91.38 ; top5 ->  99.09  and loss:  60.887031737715006
forward train acc: top1 ->  99.87199997558594 ; top5 ->  100.0  and loss:  0.40190048795193434
test acc: top1 ->  91.44 ; top5 ->  99.02  and loss:  62.04065641760826
forward train acc: top1 ->  99.86799997558593 ; top5 ->  100.0  and loss:  0.35321439430117607
test acc: top1 ->  91.37 ; top5 ->  99.22  and loss:  64.53649519011378
forward train acc: top1 ->  99.85599997558593 ; top5 ->  100.0  and loss:  0.4235808381345123
test acc: top1 ->  91.48 ; top5 ->  99.25  and loss:  62.90430353209376
forward train acc: top1 ->  99.872 ; top5 ->  100.0  and loss:  0.35280189849436283
test acc: top1 ->  91.52 ; top5 ->  99.26  and loss:  62.870137155056
forward train acc: top1 ->  99.894 ; top5 ->  100.0  and loss:  0.3118111928924918
test acc: top1 ->  91.34 ; top5 ->  99.19  and loss:  65.48712944984436
forward train acc: top1 ->  99.914 ; top5 ->  100.0  and loss:  0.2813284009025665
test acc: top1 ->  91.47 ; top5 ->  99.23  and loss:  64.64178466424346
forward train acc: top1 ->  99.886 ; top5 ->  100.0  and loss:  0.32431036478374153
test acc: top1 ->  91.56 ; top5 ->  99.32  and loss:  63.69828858226538
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  253 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -53.26334470510483 , diff:  53.26334470510483
adv train loss:  -52.836481511592865 , diff:  0.4268631935119629
adv train loss:  -53.178645342588425 , diff:  0.3421638309955597
adv train loss:  -52.483853071928024 , diff:  0.6947922706604004
adv train loss:  -274.61566177010536 , diff:  222.13180869817734
adv train loss:  -342.71614837646484 , diff:  68.10048660635948
adv train loss:  -457.22835063934326 , diff:  114.51220226287842
adv train loss:  -627.2321438789368 , diff:  170.0037932395935
adv train loss:  -776.6975402832031 , diff:  149.46539640426636
adv train loss:  -811.7275295257568 , diff:  35.02998924255371
layer  8  adv train finish, try to retain  7
test acc: top1 ->  54.42 ; top5 ->  95.74  and loss:  332.1244020462036
forward train acc: top1 ->  99.114 ; top5 ->  99.998  and loss:  2.8614409156143665
test acc: top1 ->  91.34 ; top5 ->  99.15  and loss:  53.54137537255883
forward train acc: top1 ->  99.82799997558594 ; top5 ->  99.998  and loss:  0.6094913450069726
test acc: top1 ->  91.63 ; top5 ->  99.25  and loss:  54.23935681208968
forward train acc: top1 ->  99.90800000244141 ; top5 ->  100.0  and loss:  0.35556814493611455
test acc: top1 ->  91.66 ; top5 ->  99.16  and loss:  56.020934872329235
forward train acc: top1 ->  99.90800000244141 ; top5 ->  100.0  and loss:  0.32341568171977997
test acc: top1 ->  91.66 ; top5 ->  99.17  and loss:  57.44404159113765
forward train acc: top1 ->  99.918 ; top5 ->  100.0  and loss:  0.26895821827929467
test acc: top1 ->  91.69 ; top5 ->  99.26  and loss:  57.371955916285515
forward train acc: top1 ->  99.90999997558593 ; top5 ->  100.0  and loss:  0.2649324687663466
test acc: top1 ->  91.68 ; top5 ->  99.2  and loss:  57.630035012960434
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.1929210929083638
test acc: top1 ->  91.8 ; top5 ->  99.23  and loss:  57.40460250340402
forward train acc: top1 ->  99.93199997558594 ; top5 ->  100.0  and loss:  0.21990503417328
test acc: top1 ->  91.89 ; top5 ->  99.26  and loss:  57.339592499658465
forward train acc: top1 ->  99.928 ; top5 ->  100.0  and loss:  0.2344939112663269
test acc: top1 ->  91.85 ; top5 ->  99.15  and loss:  59.05090841278434
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.18740519904531538
test acc: top1 ->  91.85 ; top5 ->  99.28  and loss:  58.345274940133095
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  36 / 512 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -2094.8613681793213 , diff:  2094.8613681793213
adv train loss:  -2136.159158706665 , diff:  41.29779052734375
adv train loss:  -2155.5549087524414 , diff:  19.395750045776367
adv train loss:  -2156.4245986938477 , diff:  0.86968994140625
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  3
test acc: top1 ->  10.02 ; top5 ->  50.58  and loss:  3443.668821334839
forward train acc: top1 ->  99.44999997558594 ; top5 ->  99.998  and loss:  1.59172939741984
test acc: top1 ->  70.81 ; top5 ->  95.63  and loss:  247.73903000354767
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.266357637650799
test acc: top1 ->  91.57 ; top5 ->  98.59  and loss:  65.37545522302389
forward train acc: top1 ->  99.89999997558594 ; top5 ->  100.0  and loss:  0.3222853899933398
test acc: top1 ->  91.7 ; top5 ->  98.68  and loss:  64.86970596015453
forward train acc: top1 ->  99.936 ; top5 ->  100.0  and loss:  0.25275375798810273
test acc: top1 ->  91.78 ; top5 ->  98.71  and loss:  65.99731750041246
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.21852738643065095
test acc: top1 ->  91.96 ; top5 ->  98.69  and loss:  64.13112799450755
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.1680408213287592
test acc: top1 ->  91.9 ; top5 ->  98.8  and loss:  65.00645577907562
forward train acc: top1 ->  99.94399997558594 ; top5 ->  100.0  and loss:  0.18222314910963178
test acc: top1 ->  91.85 ; top5 ->  98.73  and loss:  64.99869458004832
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.13855350280937273
test acc: top1 ->  92.0 ; top5 ->  98.73  and loss:  64.59092851728201
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.15261876923614182
test acc: top1 ->  91.92 ; top5 ->  98.75  and loss:  64.52166664600372
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.15583350136876106
test acc: top1 ->  92.01 ; top5 ->  98.82  and loss:  64.61160302907228
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  4 / 512 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -9.88368535041809 , diff:  9.88368535041809
adv train loss:  -9.863246634602547 , diff:  0.02043871581554413
adv train loss:  -9.849342994391918 , diff:  0.01390364021062851
layer  10  adv train finish, try to retain  486
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -702.8076791763306 , diff:  702.8076791763306
adv train loss:  -712.0766849517822 , diff:  9.26900577545166
adv train loss:  -722.7558889389038 , diff:  10.679203987121582
adv train loss:  -721.9144582748413 , diff:  0.8414306640625
layer  11  adv train finish, try to retain  512
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -1269.6086274655536 , diff:  1269.6086274655536
adv train loss:  -3162.8610458374023 , diff:  1893.2524183718488
adv train loss:  -3164.5058002471924 , diff:  1.644754409790039
adv train loss:  -3164.4436893463135 , diff:  0.06211090087890625
adv train loss:  -3165.512336730957 , diff:  1.0686473846435547
adv train loss:  -3163.802442550659 , diff:  1.7098941802978516
adv train loss:  -3165.7238788604736 , diff:  1.9214363098144531
adv train loss:  -3162.8200759887695 , diff:  2.9038028717041016
adv train loss:  -3164.0682640075684 , diff:  1.2481880187988281
adv train loss:  -3163.1589069366455 , diff:  0.9093570709228516
layer  12  adv train finish, try to retain  5
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -7722.638236999512 , diff:  7722.638236999512
adv train loss:  -13089.91187286377 , diff:  5367.273635864258
adv train loss:  -18087.849838256836 , diff:  4997.937965393066
adv train loss:  -26080.375030517578 , diff:  7992.525192260742
adv train loss:  -40423.51239013672 , diff:  14343.13735961914
adv train loss:  -52262.93377685547 , diff:  11839.42138671875
adv train loss:  -62205.3720703125 , diff:  9942.438293457031
adv train loss:  -71353.63275146484 , diff:  9148.260681152344
adv train loss:  -80070.69018554688 , diff:  8717.057434082031
adv train loss:  -88536.91912841797 , diff:  8466.228942871094
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  8
test acc: top1 ->  23.47 ; top5 ->  73.5  and loss:  588.9039850234985
forward train acc: top1 ->  64.72599999755859 ; top5 ->  96.878  and loss:  127.88721877336502
test acc: top1 ->  73.12 ; top5 ->  97.85  and loss:  106.45897024869919
forward train acc: top1 ->  85.30799999023438 ; top5 ->  99.966  and loss:  69.74706566333771
test acc: top1 ->  78.66 ; top5 ->  97.72  and loss:  101.68647688627243
forward train acc: top1 ->  89.19800000976562 ; top5 ->  99.988  and loss:  61.439881920814514
test acc: top1 ->  79.99 ; top5 ->  97.68  and loss:  101.30408525466919
forward train acc: top1 ->  90.33000000732422 ; top5 ->  99.982  and loss:  57.9536509513855
test acc: top1 ->  81.86 ; top5 ->  97.59  and loss:  101.51001292467117
forward train acc: top1 ->  91.40600000732422 ; top5 ->  99.988  and loss:  54.65692609548569
test acc: top1 ->  82.29 ; top5 ->  97.65  and loss:  102.20874637365341
forward train acc: top1 ->  93.12199999755859 ; top5 ->  99.992  and loss:  51.59987533092499
test acc: top1 ->  83.08 ; top5 ->  97.6  and loss:  101.97079485654831
forward train acc: top1 ->  93.46599997314453 ; top5 ->  99.988  and loss:  50.19622355699539
test acc: top1 ->  83.3 ; top5 ->  97.58  and loss:  102.1435609459877
forward train acc: top1 ->  94.11400000488281 ; top5 ->  99.992  and loss:  48.385359317064285
test acc: top1 ->  83.53 ; top5 ->  97.6  and loss:  102.55180257558823
forward train acc: top1 ->  94.234 ; top5 ->  99.99  and loss:  47.42230811715126
test acc: top1 ->  83.69 ; top5 ->  97.53  and loss:  102.84559416770935
forward train acc: top1 ->  94.45600001953125 ; top5 ->  99.988  and loss:  45.81570664048195
test acc: top1 ->  84.21 ; top5 ->  97.55  and loss:  103.11205404996872
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  9 / 512 , inc:  1
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.640625  ==>  41 / 64 , inc:  1
layer  2  :  0.703125  ==>  90 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.30859375  ==>  79 / 256 , inc:  2
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.0703125  ==>  36 / 512 , inc:  1
layer  9  :  0.0078125  ==>  4 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.009765625  ==>  5 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [0.8426560220195098, 7.989627468036835, 2.9961103005138128, 0.8426560220195098, 0.8426560220195098, 0.8426560220195098, 1.1235413626926798, 1.1235413626926798, 0.9987034335046043, 1.4980551502569064, 1.6853120440390197, 1.9974068670092087, 2.2470827253853596, 11.984441202055251]  wait [4, 0, 2, 4, 4, 4, 0, 4, 2, 2, 1, 0, 0, 3]  inc [1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1]  tol: 14
$$$$$$$$$$$$$ epoch  88  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
adv train loss:  -35.79935932159424 , diff:  35.79935932159424
adv train loss:  -35.20027966797352 , diff:  0.5990796536207199
adv train loss:  -34.710783407092094 , diff:  0.48949626088142395
adv train loss:  -35.26625542342663 , diff:  0.5554720163345337
adv train loss:  -35.56548611819744 , diff:  0.299230694770813
adv train loss:  -35.01151603460312 , diff:  0.5539700835943222
adv train loss:  -35.37366680055857 , diff:  0.36215076595544815
adv train loss:  -36.56701931357384 , diff:  1.1933525130152702
adv train loss:  -35.72968901693821 , diff:  0.8373302966356277
adv train loss:  -36.38071998953819 , diff:  0.6510309725999832
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  40
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  3231.7055168151855
forward train acc: top1 ->  99.534 ; top5 ->  99.996  and loss:  1.7065808907500468
test acc: top1 ->  91.74 ; top5 ->  98.7  and loss:  90.09485249221325
forward train acc: top1 ->  99.878 ; top5 ->  100.0  and loss:  0.5016697128594387
test acc: top1 ->  91.93 ; top5 ->  98.93  and loss:  88.32838165014982
forward train acc: top1 ->  99.93399997558593 ; top5 ->  100.0  and loss:  0.2077801458799513
test acc: top1 ->  91.91 ; top5 ->  98.96  and loss:  86.42808613926172
forward train acc: top1 ->  99.912 ; top5 ->  100.0  and loss:  0.2471341815253254
test acc: top1 ->  91.72 ; top5 ->  98.92  and loss:  89.01415536552668
forward train acc: top1 ->  99.91 ; top5 ->  100.0  and loss:  0.2919568837096449
test acc: top1 ->  91.77 ; top5 ->  98.87  and loss:  84.88837096095085
forward train acc: top1 ->  99.94599997558593 ; top5 ->  100.0  and loss:  0.16023357669473626
test acc: top1 ->  91.92 ; top5 ->  98.94  and loss:  84.78207616508007
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.21721904617152177
test acc: top1 ->  91.98 ; top5 ->  98.95  and loss:  84.63984889537096
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.15963652334176004
test acc: top1 ->  91.99 ; top5 ->  98.97  and loss:  84.82757571339607
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.22569503214981523
test acc: top1 ->  91.95 ; top5 ->  99.0  and loss:  84.83950166404247
forward train acc: top1 ->  99.96599997558594 ; top5 ->  100.0  and loss:  0.09737168718129396
test acc: top1 ->  91.86 ; top5 ->  99.01  and loss:  84.65754623711109
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  41 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -0.2386476928659249 , diff:  0.2386476928659249
adv train loss:  -0.18207200395409018 , diff:  0.05657568891183473
adv train loss:  -0.13141651113983244 , diff:  0.05065549281425774
adv train loss:  -0.1784128508234062 , diff:  0.04699633968357375
adv train loss:  -0.19066438724985346 , diff:  0.012251536426447274
adv train loss:  -0.21904045229166513 , diff:  0.02837606504181167
adv train loss:  -0.18143431057978887 , diff:  0.03760614171187626
adv train loss:  -0.18097810674225911 , diff:  0.00045620383752975613
layer  2  adv train finish, try to retain  90
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
adv train loss:  -1151.650956394631 , diff:  1151.650956394631
adv train loss:  -3890.280632019043 , diff:  2738.629675624412
adv train loss:  -3972.3417205810547 , diff:  82.06108856201172
adv train loss:  -3974.409324645996 , diff:  2.0676040649414062
adv train loss:  -3979.065662384033 , diff:  4.656337738037109
adv train loss:  -3970.2562408447266 , diff:  8.80942153930664
adv train loss:  -3994.159023284912 , diff:  23.902782440185547
adv train loss:  -4018.4586791992188 , diff:  24.29965591430664
adv train loss:  -4161.639549255371 , diff:  143.18087005615234
adv train loss:  -4123.84973526001 , diff:  37.78981399536133
layer  6  adv train finish, try to retain  64
test acc: top1 ->  22.73 ; top5 ->  88.04  and loss:  4977.648178100586
forward train acc: top1 ->  99.816 ; top5 ->  100.0  and loss:  0.5897247442044318
test acc: top1 ->  91.62 ; top5 ->  98.91  and loss:  85.36502961814404
forward train acc: top1 ->  99.85799997558594 ; top5 ->  100.0  and loss:  0.44784170399361756
test acc: top1 ->  91.71 ; top5 ->  99.19  and loss:  83.36467684805393
forward train acc: top1 ->  99.87 ; top5 ->  100.0  and loss:  0.4234500756720081
test acc: top1 ->  91.74 ; top5 ->  99.05  and loss:  83.92394490540028
forward train acc: top1 ->  99.878 ; top5 ->  100.0  and loss:  0.38329416194028454
test acc: top1 ->  91.75 ; top5 ->  99.04  and loss:  81.255170956254
forward train acc: top1 ->  99.8560000024414 ; top5 ->  100.0  and loss:  0.3872171900875401
test acc: top1 ->  91.83 ; top5 ->  99.26  and loss:  79.77525260299444
forward train acc: top1 ->  99.89 ; top5 ->  100.0  and loss:  0.35286783306946745
test acc: top1 ->  91.91 ; top5 ->  99.34  and loss:  79.87769639492035
forward train acc: top1 ->  99.928 ; top5 ->  100.0  and loss:  0.23241517157293856
test acc: top1 ->  91.81 ; top5 ->  99.33  and loss:  79.1366315856576
forward train acc: top1 ->  99.928 ; top5 ->  100.0  and loss:  0.23975838167825714
test acc: top1 ->  92.0 ; top5 ->  99.17  and loss:  80.40279702842236
forward train acc: top1 ->  99.92799997558593 ; top5 ->  100.0  and loss:  0.2332880661051604
test acc: top1 ->  91.89 ; top5 ->  99.25  and loss:  78.36445106565952
forward train acc: top1 ->  99.902 ; top5 ->  100.0  and loss:  0.26982694686739706
test acc: top1 ->  91.96 ; top5 ->  99.24  and loss:  78.42137410491705
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  79 / 256 , inc:  2
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
adv train loss:  -67.20389208197594 , diff:  67.20389208197594
adv train loss:  -66.01933479309082 , diff:  1.1845572888851166
adv train loss:  -66.44548654556274 , diff:  0.42615175247192383
adv train loss:  -67.59681767225266 , diff:  1.1513311266899109
adv train loss:  -67.2611099332571 , diff:  0.33570773899555206
adv train loss:  -66.32538890838623 , diff:  0.9357210248708725
adv train loss:  -67.25346091389656 , diff:  0.9280720055103302
adv train loss:  -65.93522542715073 , diff:  1.3182354867458344
adv train loss:  -66.3861684948206 , diff:  0.45094306766986847
adv train loss:  -66.83420197665691 , diff:  0.44803348183631897
layer  8  adv train finish, try to retain  477
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -1423.005781173706 , diff:  1423.005781173706
adv train loss:  -1494.4442539215088 , diff:  71.43847274780273
adv train loss:  -1522.3842325210571 , diff:  27.93997859954834
adv train loss:  -1519.33047580719 , diff:  3.0537567138671875
adv train loss:  -1519.3886232376099 , diff:  0.058147430419921875
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  3
test acc: top1 ->  10.0 ; top5 ->  50.11  and loss:  6761.8168296813965
forward train acc: top1 ->  99.742 ; top5 ->  100.0  and loss:  0.8243131245253608
test acc: top1 ->  90.75 ; top5 ->  98.49  and loss:  86.95525382459164
forward train acc: top1 ->  99.93999997558593 ; top5 ->  99.998  and loss:  0.20845387154258788
test acc: top1 ->  91.9 ; top5 ->  98.77  and loss:  71.51929251477122
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.16607655056577642
test acc: top1 ->  91.89 ; top5 ->  98.69  and loss:  72.47333233430982
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.1755881349381525
test acc: top1 ->  91.81 ; top5 ->  98.86  and loss:  72.87374339997768
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.15144502124894643
test acc: top1 ->  91.74 ; top5 ->  98.94  and loss:  73.01796265691519
forward train acc: top1 ->  99.95599997558594 ; top5 ->  100.0  and loss:  0.1665769295359496
test acc: top1 ->  91.86 ; top5 ->  98.88  and loss:  72.7085300758481
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.10250844964321004
test acc: top1 ->  91.76 ; top5 ->  98.96  and loss:  72.39443397894502
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.1671917905623559
test acc: top1 ->  91.89 ; top5 ->  98.9  and loss:  71.62760755047202
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.11262389180774335
test acc: top1 ->  91.9 ; top5 ->  98.86  and loss:  71.81446129456162
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.12836393653560663
test acc: top1 ->  91.89 ; top5 ->  98.92  and loss:  72.37349607050419
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  4 / 512 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -117.61057859659195 , diff:  117.61057859659195
adv train loss:  -117.42050671577454 , diff:  0.19007188081741333
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  10
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  331844.1281738281
forward train acc: top1 ->  84.8920000024414 ; top5 ->  96.688  and loss:  97.45020735263824
test acc: top1 ->  60.0 ; top5 ->  97.86  and loss:  147.5971817970276
forward train acc: top1 ->  99.62599997558594 ; top5 ->  100.0  and loss:  3.8007527757436037
test acc: top1 ->  90.9 ; top5 ->  99.14  and loss:  40.19532184302807
forward train acc: top1 ->  99.798 ; top5 ->  100.0  and loss:  1.9144331980496645
test acc: top1 ->  91.06 ; top5 ->  99.17  and loss:  41.938233345746994
forward train acc: top1 ->  99.83599997802735 ; top5 ->  100.0  and loss:  1.2638198249042034
test acc: top1 ->  91.19 ; top5 ->  99.25  and loss:  43.179578721523285
forward train acc: top1 ->  99.864 ; top5 ->  100.0  and loss:  0.8650359935127199
test acc: top1 ->  91.48 ; top5 ->  99.22  and loss:  44.16875954717398
forward train acc: top1 ->  99.876 ; top5 ->  100.0  and loss:  0.6975401346571743
test acc: top1 ->  91.48 ; top5 ->  99.24  and loss:  44.78886995092034
forward train acc: top1 ->  99.918 ; top5 ->  100.0  and loss:  0.5456139333546162
test acc: top1 ->  91.48 ; top5 ->  99.2  and loss:  45.61946853995323
forward train acc: top1 ->  99.916 ; top5 ->  100.0  and loss:  0.50880335457623
test acc: top1 ->  91.72 ; top5 ->  99.22  and loss:  46.08148570358753
forward train acc: top1 ->  99.936 ; top5 ->  100.0  and loss:  0.4017991062719375
test acc: top1 ->  91.64 ; top5 ->  99.26  and loss:  46.867759600281715
forward train acc: top1 ->  99.94599997558593 ; top5 ->  100.0  and loss:  0.3464692453853786
test acc: top1 ->  91.68 ; top5 ->  99.21  and loss:  47.821955397725105
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  11 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -3326.582166671753 , diff:  3326.582166671753
adv train loss:  -3326.7927074432373 , diff:  0.210540771484375
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  1
test acc: top1 ->  10.0 ; top5 ->  57.99  and loss:  9947.409614562988
forward train acc: top1 ->  80.40599997314453 ; top5 ->  97.2840000024414  and loss:  127.4312984496355
test acc: top1 ->  87.59 ; top5 ->  98.28  and loss:  63.662770852446556
forward train acc: top1 ->  98.61399997802734 ; top5 ->  99.994  and loss:  6.221784148365259
test acc: top1 ->  89.9 ; top5 ->  98.38  and loss:  52.56753705814481
forward train acc: top1 ->  99.26399997558593 ; top5 ->  99.998  and loss:  3.4592776633799076
test acc: top1 ->  90.38 ; top5 ->  98.4  and loss:  52.683793887495995
forward train acc: top1 ->  99.4740000024414 ; top5 ->  99.998  and loss:  2.470081523992121
test acc: top1 ->  90.44 ; top5 ->  98.53  and loss:  51.883862532675266
forward train acc: top1 ->  99.55199997802734 ; top5 ->  100.0  and loss:  2.015821611508727
test acc: top1 ->  90.5 ; top5 ->  98.6  and loss:  52.36428328230977
forward train acc: top1 ->  99.57 ; top5 ->  99.998  and loss:  1.7731638066470623
test acc: top1 ->  90.6 ; top5 ->  98.62  and loss:  52.7643895894289
forward train acc: top1 ->  99.65199997558594 ; top5 ->  100.0  and loss:  1.5450415862724185
test acc: top1 ->  90.79 ; top5 ->  98.66  and loss:  52.5947424992919
forward train acc: top1 ->  99.642 ; top5 ->  100.0  and loss:  1.4280777890235186
test acc: top1 ->  90.78 ; top5 ->  98.61  and loss:  53.889925092458725
forward train acc: top1 ->  99.69799997558594 ; top5 ->  100.0  and loss:  1.259943375363946
test acc: top1 ->  90.82 ; top5 ->  98.64  and loss:  53.94373455271125
forward train acc: top1 ->  99.69600000244141 ; top5 ->  100.0  and loss:  1.2453869013115764
test acc: top1 ->  90.83 ; top5 ->  98.68  and loss:  53.792803417891264
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  2 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -2278.603317260742 , diff:  2278.603317260742
adv train loss:  -2496.169454574585 , diff:  217.56613731384277
adv train loss:  -2497.0758018493652 , diff:  0.9063472747802734
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  4
test acc: top1 ->  10.0 ; top5 ->  59.54  and loss:  48115.06707763672
forward train acc: top1 ->  92.59799997558594 ; top5 ->  99.744  and loss:  46.27150064241141
test acc: top1 ->  91.0 ; top5 ->  98.96  and loss:  73.4876808077097
forward train acc: top1 ->  99.842 ; top5 ->  100.0  and loss:  0.6596816377714276
test acc: top1 ->  91.42 ; top5 ->  99.01  and loss:  71.43038583174348
forward train acc: top1 ->  99.88999997558594 ; top5 ->  99.998  and loss:  0.438828393118456
test acc: top1 ->  91.59 ; top5 ->  98.99  and loss:  71.0525706782937
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.33901000348851085
test acc: top1 ->  91.72 ; top5 ->  99.02  and loss:  71.11810642853379
forward train acc: top1 ->  99.914 ; top5 ->  100.0  and loss:  0.3424973754445091
test acc: top1 ->  91.62 ; top5 ->  99.08  and loss:  70.81169040873647
forward train acc: top1 ->  99.942 ; top5 ->  100.0  and loss:  0.24973305012099445
test acc: top1 ->  91.58 ; top5 ->  99.02  and loss:  70.70951657369733
forward train acc: top1 ->  99.936 ; top5 ->  100.0  and loss:  0.2517737365560606
test acc: top1 ->  91.63 ; top5 ->  98.97  and loss:  70.96461196988821
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.26984351221472025
test acc: top1 ->  91.6 ; top5 ->  99.04  and loss:  70.79382814466953
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.24544040055479854
test acc: top1 ->  91.75 ; top5 ->  99.0  and loss:  70.39392769522965
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.19543630629777908
test acc: top1 ->  91.66 ; top5 ->  99.13  and loss:  71.0656604655087
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  5 / 512 , inc:  1
---------------- start layer  13  ---------------
### skip layer  13 wait:  3  ###
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.640625  ==>  41 / 64 , inc:  1
layer  2  :  0.703125  ==>  90 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.60546875  ==>  155 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.30859375  ==>  79 / 256 , inc:  1
layer  7  :  0.494140625  ==>  253 / 512 , inc:  1
layer  8  :  0.0703125  ==>  36 / 512 , inc:  1
layer  9  :  0.0078125  ==>  4 / 512 , inc:  1
layer  10  :  0.021484375  ==>  11 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.009765625  ==>  5 / 512 , inc:  1
layer  13  :  0.017578125  ==>  9 / 512 , inc:  1
eps [0.8426560220195098, 5.9922206010276255, 5.9922206010276255, 0.8426560220195098, 0.8426560220195098, 0.8426560220195098, 0.8426560220195098, 1.1235413626926798, 1.9974068670092087, 1.1235413626926798, 1.2639840330292649, 1.4980551502569064, 1.6853120440390197, 11.984441202055251]  wait [3, 2, 2, 3, 3, 3, 2, 3, 2, 4, 3, 2, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 14
$$$$$$$$$$$$$ epoch  89  $$$$$$$$$$$$
-------------- post train ------------
forward train acc: top1 ->  99.95199997558593 ; top5 ->  100.0  and loss:  0.13929458375787362
test acc: top1 ->  91.87 ; top5 ->  99.26  and loss:  76.60183615237474
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.1168439472821774
test acc: top1 ->  91.86 ; top5 ->  99.33  and loss:  76.60806749016047
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.08967284664686304
test acc: top1 ->  91.85 ; top5 ->  99.18  and loss:  77.76297345012426
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.07751980234388611
test acc: top1 ->  91.96 ; top5 ->  99.3  and loss:  77.66095681488514
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.082638032574323
test acc: top1 ->  91.93 ; top5 ->  99.29  and loss:  78.58921281248331
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.10059445182560012
test acc: top1 ->  92.01 ; top5 ->  99.31  and loss:  77.99117863923311
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.08824697067029774
test acc: top1 ->  91.91 ; top5 ->  99.26  and loss:  79.62622685730457
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.10356733519802219
test acc: top1 ->  92.01 ; top5 ->  99.27  and loss:  80.47064100205898
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.055118826414400246
test acc: top1 ->  92.08 ; top5 ->  99.25  and loss:  80.53586648404598
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.10698835967923515
test acc: top1 ->  92.01 ; top5 ->  99.22  and loss:  81.2143112719059
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.08264986444555689
test acc: top1 ->  91.9 ; top5 ->  99.3  and loss:  80.86822502687573
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.05892281388514675
test acc: top1 ->  92.07 ; top5 ->  99.25  and loss:  82.01048878207803
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.04597178543917835
test acc: top1 ->  92.0 ; top5 ->  99.28  and loss:  82.02479251846671
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.06545051538159896
test acc: top1 ->  91.97 ; top5 ->  99.35  and loss:  81.73596215248108
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.10510273789986968
test acc: top1 ->  91.89 ; top5 ->  99.33  and loss:  81.79626085609198
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.09025009941251483
test acc: top1 ->  91.93 ; top5 ->  99.3  and loss:  81.76592915132642
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.0566567903952091
test acc: top1 ->  91.97 ; top5 ->  99.3  and loss:  82.71714328229427
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.06944261886019376
test acc: top1 ->  91.85 ; top5 ->  99.25  and loss:  82.5780801475048
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.09854749197165802
test acc: top1 ->  91.84 ; top5 ->  99.31  and loss:  81.75622221454978
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.025756380389793776
test acc: top1 ->  91.96 ; top5 ->  99.33  and loss:  81.8362870849669
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.09204376828620298
test acc: top1 ->  91.96 ; top5 ->  99.31  and loss:  82.84595960378647
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.05768863412322389
test acc: top1 ->  91.94 ; top5 ->  99.34  and loss:  82.48321192897856
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.06315655645084917
test acc: top1 ->  91.95 ; top5 ->  99.27  and loss:  82.89875517040491
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.07629530983103905
test acc: top1 ->  91.96 ; top5 ->  99.33  and loss:  82.8749397546053
forward train acc: top1 ->  99.98199997558594 ; top5 ->  100.0  and loss:  0.06382139258676034
test acc: top1 ->  92.01 ; top5 ->  99.28  and loss:  82.65601190365851
forward train acc: top1 ->  99.97599997558594 ; top5 ->  100.0  and loss:  0.06456490138953086
test acc: top1 ->  92.04 ; top5 ->  99.36  and loss:  82.39542703516781
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.07787112887308467
test acc: top1 ->  92.04 ; top5 ->  99.3  and loss:  82.41386340372264
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.06291419581430091
test acc: top1 ->  92.08 ; top5 ->  99.25  and loss:  83.27449307404459
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.07766150167208252
test acc: top1 ->  91.93 ; top5 ->  99.26  and loss:  82.9050488229841
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.0713934911746037
test acc: top1 ->  91.97 ; top5 ->  99.27  and loss:  82.82552261836827
forward train acc: top1 ->  99.98599997558594 ; top5 ->  100.0  and loss:  0.04835058119306268
test acc: top1 ->  91.95 ; top5 ->  99.3  and loss:  82.39451114460826
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.04519409266686125
test acc: top1 ->  91.96 ; top5 ->  99.25  and loss:  83.31505736336112
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.07400379934551893
test acc: top1 ->  91.98 ; top5 ->  99.24  and loss:  83.2625603377819
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.05047751014717505
test acc: top1 ->  91.98 ; top5 ->  99.25  and loss:  83.0668017975986
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.060175115811944124
test acc: top1 ->  91.91 ; top5 ->  99.31  and loss:  82.77122279256582
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.09725591680034995
test acc: top1 ->  91.98 ; top5 ->  99.33  and loss:  82.72647729888558
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.06482773284369614
test acc: top1 ->  91.89 ; top5 ->  99.34  and loss:  82.63752998411655
forward train acc: top1 ->  99.98199997558594 ; top5 ->  100.0  and loss:  0.06628865840320941
test acc: top1 ->  91.97 ; top5 ->  99.31  and loss:  82.5397310834378
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.09325056725174363
test acc: top1 ->  92.06 ; top5 ->  99.32  and loss:  82.84962099790573
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.03146046663459856
test acc: top1 ->  91.95 ; top5 ->  99.32  and loss:  82.68538950011134
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.05822917020850582
test acc: top1 ->  92.0 ; top5 ->  99.26  and loss:  82.73226459138095
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.05898912459088024
test acc: top1 ->  91.99 ; top5 ->  99.34  and loss:  83.13374629989266
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.05913252972004557
test acc: top1 ->  91.95 ; top5 ->  99.31  and loss:  82.55076778680086
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.04989133599883644
test acc: top1 ->  92.01 ; top5 ->  99.27  and loss:  82.78159288689494
forward train acc: top1 ->  99.9800000024414 ; top5 ->  100.0  and loss:  0.07326208656013478
test acc: top1 ->  92.02 ; top5 ->  99.34  and loss:  82.49310877174139
forward train acc: top1 ->  99.96999997558594 ; top5 ->  100.0  and loss:  0.06126702693245534
test acc: top1 ->  92.02 ; top5 ->  99.32  and loss:  82.68402007035911
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.0983190141241721
test acc: top1 ->  91.94 ; top5 ->  99.32  and loss:  82.6431485824287
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.037089025747263804
test acc: top1 ->  92.0 ; top5 ->  99.29  and loss:  82.64665856212378
forward train acc: top1 ->  99.9780000024414 ; top5 ->  100.0  and loss:  0.047844634971625055
test acc: top1 ->  91.97 ; top5 ->  99.26  and loss:  82.82599109970033
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.09962680167791405
test acc: top1 ->  92.05 ; top5 ->  99.27  and loss:  83.05127428472042
------------- sparsity -----------
layer  0  :  0.5  ==>  32 / 64
layer  1  :  0.640625  ==>  41 / 64
layer  2  :  0.703125  ==>  90 / 128
layer  3  :  0.6796875  ==>  87 / 128
layer  4  :  0.60546875  ==>  155 / 256
layer  5  :  0.58203125  ==>  149 / 256
layer  6  :  0.30859375  ==>  79 / 256
layer  7  :  0.494140625  ==>  253 / 512
layer  8  :  0.0703125  ==>  36 / 512
layer  9  :  0.0078125  ==>  4 / 512
layer  10  :  0.021484375  ==>  11 / 512
layer  11  :  0.00390625  ==>  2 / 512
layer  12  :  0.009765625  ==>  5 / 512
layer  13  :  0.017578125  ==>  9 / 512
