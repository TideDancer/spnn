DataParallel(
  (module): CONV_Mask(
    (net): AlexNet(
      (features): Sequential(
        (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))
        (1): ReLU(inplace)
        (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
        (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        (4): ReLU(inplace)
        (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
        (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (7): ReLU(inplace)
        (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (9): ReLU(inplace)
        (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (11): ReLU(inplace)
        (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
      )
      (classifier): Sequential(
        (0): Dropout(p=0.5)
        (1): Linear(in_features=9216, out_features=4096, bias=True)
        (2): ReLU(inplace)
        (3): Dropout(p=0.5)
        (4): Linear(in_features=4096, out_features=4096, bias=True)
        (5): ReLU(inplace)
        (6): Linear(in_features=4096, out_features=1000, bias=True)
      )
    )
    (mask): ParameterList(
        (0): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (1): Parameter containing: [torch.cuda.FloatTensor of size 192 (GPU 0)]
        (2): Parameter containing: [torch.cuda.FloatTensor of size 384 (GPU 0)]
        (3): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
        (4): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
        (5): Parameter containing: [torch.cuda.FloatTensor of size 9216 (GPU 0)]
        (6): Parameter containing: [torch.cuda.FloatTensor of size 4096 (GPU 0)]
        (7): Parameter containing: [torch.cuda.FloatTensor of size 4096 (GPU 0)]
    )
  )
)
eps:  [0.0015625, 0.0005208333333333333, 0.00026041666666666666, 0.000390625, 0.000390625, 1.0850694444444445e-05, 2.44140625e-05, 2.44140625e-05]
$$$$$$$$$$$$$ epoch  0  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -102.14578330516815 , diff:  102.14578330516815
adv train loss:  -101.34045720100403 , diff:  0.8053261041641235
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  63
test acc: top1 ->  53.148 ; top5 ->  76.6  and loss:  808.5620802640915
forward train acc: top1 ->  50.1484375 ; top5 ->  73.95703125  and loss:  440.49863588809967
test acc: top1 ->  53.922 ; top5 ->  77.54  and loss:  788.6890214681625
forward train acc: top1 ->  49.4453125 ; top5 ->  73.85546875  and loss:  444.4072287082672
test acc: top1 ->  53.406 ; top5 ->  77.26  and loss:  803.22158613801
forward train acc: top1 ->  49.42578125 ; top5 ->  73.46484375  and loss:  449.9516932964325
test acc: top1 ->  53.35 ; top5 ->  77.158  and loss:  801.1633299589157
forward train acc: top1 ->  49.72265625 ; top5 ->  73.79296875  and loss:  446.41196489334106
test acc: top1 ->  54.426 ; top5 ->  77.966  and loss:  779.5009201467037
forward train acc: top1 ->  50.0234375 ; top5 ->  73.90234375  and loss:  444.80917060375214
test acc: top1 ->  54.678 ; top5 ->  78.156  and loss:  777.4699030816555
forward train acc: top1 ->  50.3671875 ; top5 ->  74.1640625  and loss:  442.7981308698654
test acc: top1 ->  54.546 ; top5 ->  77.886  and loss:  780.8731726109982
forward train acc: top1 ->  50.64453125 ; top5 ->  74.4609375  and loss:  436.87837159633636
test acc: top1 ->  54.92 ; top5 ->  78.244  and loss:  772.2942247390747
forward train acc: top1 ->  51.1875 ; top5 ->  74.8984375  and loss:  430.97771418094635
test acc: top1 ->  55.032 ; top5 ->  78.3  and loss:  771.0943748950958
==> this epoch:  63 / 64
---------------- start layer  1  ---------------
adv train loss:  -107.80687749385834 , diff:  107.80687749385834
adv train loss:  -110.03772366046906 , diff:  2.2308461666107178
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  191
test acc: top1 ->  54.774 ; top5 ->  78.102  and loss:  775.8321589827538
forward train acc: top1 ->  49.4921875 ; top5 ->  73.421875  and loss:  450.1148394346237
test acc: top1 ->  52.972 ; top5 ->  76.914  and loss:  805.563315153122
forward train acc: top1 ->  49.34765625 ; top5 ->  73.55078125  and loss:  448.8871433734894
test acc: top1 ->  53.106 ; top5 ->  77.044  and loss:  801.7807485461235
forward train acc: top1 ->  48.75390625 ; top5 ->  73.33984375  and loss:  458.10557222366333
test acc: top1 ->  53.284 ; top5 ->  77.116  and loss:  804.5905923247337
forward train acc: top1 ->  48.9296875 ; top5 ->  73.203125  and loss:  453.9881558418274
test acc: top1 ->  53.942 ; top5 ->  77.592  and loss:  790.8169977068901
forward train acc: top1 ->  49.86328125 ; top5 ->  73.7265625  and loss:  448.45693826675415
test acc: top1 ->  54.158 ; top5 ->  77.676  and loss:  789.6757496595383
forward train acc: top1 ->  50.1953125 ; top5 ->  74.02734375  and loss:  445.96510434150696
test acc: top1 ->  54.048 ; top5 ->  77.588  and loss:  789.546363890171
forward train acc: top1 ->  50.3828125 ; top5 ->  74.10546875  and loss:  439.53162586688995
test acc: top1 ->  54.608 ; top5 ->  77.874  and loss:  781.3535552918911
forward train acc: top1 ->  50.36328125 ; top5 ->  73.91796875  and loss:  440.78387701511383
test acc: top1 ->  54.664 ; top5 ->  77.922  and loss:  779.171639919281
forward train acc: top1 ->  50.1015625 ; top5 ->  73.71484375  and loss:  445.544309258461
test acc: top1 ->  54.82 ; top5 ->  78.186  and loss:  775.1933852434158
forward train acc: top1 ->  50.60546875 ; top5 ->  74.48828125  and loss:  435.14622128009796
test acc: top1 ->  55.102 ; top5 ->  78.168  and loss:  772.6527187228203
==> this epoch:  191 / 192
---------------- start layer  2  ---------------
adv train loss:  -109.365314245224 , diff:  109.365314245224
adv train loss:  -107.90932488441467 , diff:  1.4559893608093262
layer  2  adv train finish, try to retain  362
test acc: top1 ->  53.596 ; top5 ->  77.238  and loss:  799.1190769076347
forward train acc: top1 ->  48.20703125 ; top5 ->  72.55078125  and loss:  464.4309192895889
test acc: top1 ->  53.048 ; top5 ->  76.808  and loss:  810.099759221077
forward train acc: top1 ->  47.76953125 ; top5 ->  72.05859375  and loss:  469.5648877620697
test acc: top1 ->  52.584 ; top5 ->  76.728  and loss:  814.5194759964943
forward train acc: top1 ->  47.3046875 ; top5 ->  71.61328125  and loss:  474.3373441696167
test acc: top1 ->  52.6 ; top5 ->  76.596  and loss:  813.7116401791573
forward train acc: top1 ->  48.44140625 ; top5 ->  72.93359375  and loss:  460.37063467502594
test acc: top1 ->  53.436 ; top5 ->  77.408  and loss:  799.2566050887108
forward train acc: top1 ->  49.328125 ; top5 ->  73.33203125  and loss:  454.05340802669525
test acc: top1 ->  53.514 ; top5 ->  77.212  and loss:  798.6159042716026
forward train acc: top1 ->  48.98828125 ; top5 ->  72.80078125  and loss:  457.91136026382446
test acc: top1 ->  53.52 ; top5 ->  77.06  and loss:  799.8348643183708
forward train acc: top1 ->  48.953125 ; top5 ->  72.953125  and loss:  455.20956563949585
test acc: top1 ->  54.08 ; top5 ->  77.534  and loss:  787.9612649679184
forward train acc: top1 ->  49.73046875 ; top5 ->  73.31640625  and loss:  450.134490609169
test acc: top1 ->  54.146 ; top5 ->  77.67  and loss:  786.7284301519394
forward train acc: top1 ->  49.7734375 ; top5 ->  73.5390625  and loss:  448.4548041820526
test acc: top1 ->  54.294 ; top5 ->  77.648  and loss:  786.8113055229187
forward train acc: top1 ->  49.58984375 ; top5 ->  73.87109375  and loss:  447.2658506631851
test acc: top1 ->  54.524 ; top5 ->  77.774  and loss:  782.6097174882889
forward train acc: top1 ->  50.01171875 ; top5 ->  74.06640625  and loss:  444.57425940036774
test acc: top1 ->  54.508 ; top5 ->  78.002  and loss:  780.9791603088379
forward train acc: top1 ->  50.1328125 ; top5 ->  74.13671875  and loss:  446.2492733001709
test acc: top1 ->  54.44 ; top5 ->  77.834  and loss:  782.1484225392342
forward train acc: top1 ->  50.8359375 ; top5 ->  74.77734375  and loss:  439.5181151628494
test acc: top1 ->  54.614 ; top5 ->  77.944  and loss:  780.1864022612572
forward train acc: top1 ->  50.20703125 ; top5 ->  73.86328125  and loss:  443.68744027614594
test acc: top1 ->  54.634 ; top5 ->  77.924  and loss:  778.8245570659637
forward train acc: top1 ->  50.21484375 ; top5 ->  74.08984375  and loss:  443.83590066432953
test acc: top1 ->  54.604 ; top5 ->  77.892  and loss:  778.8699385523796
forward train acc: top1 ->  50.44921875 ; top5 ->  73.83203125  and loss:  443.5901885032654
test acc: top1 ->  54.602 ; top5 ->  77.988  and loss:  777.7132813334465
forward train acc: top1 ->  50.828125 ; top5 ->  74.27734375  and loss:  441.38786256313324
test acc: top1 ->  54.67 ; top5 ->  78.038  and loss:  777.0614134669304
forward train acc: top1 ->  50.3359375 ; top5 ->  74.20703125  and loss:  439.3841849565506
test acc: top1 ->  54.734 ; top5 ->  78.022  and loss:  777.0667469501495
forward train acc: top1 ->  50.71875 ; top5 ->  74.07421875  and loss:  439.1987951993942
test acc: top1 ->  54.7 ; top5 ->  78.05  and loss:  776.9292960166931
forward train acc: top1 ->  50.34375 ; top5 ->  73.9921875  and loss:  442.11831736564636
test acc: top1 ->  54.706 ; top5 ->  78.028  and loss:  777.110848903656
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  384 / 384 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -114.22070443630219 , diff:  114.22070443630219
adv train loss:  -114.12381207942963 , diff:  0.0968923568725586
layer  3  adv train finish, try to retain  240
test acc: top1 ->  51.982 ; top5 ->  76.27  and loss:  830.8513389229774
forward train acc: top1 ->  47.71875 ; top5 ->  72.09375  and loss:  468.24944710731506
test acc: top1 ->  52.48 ; top5 ->  76.542  and loss:  818.6257450580597
forward train acc: top1 ->  48.16015625 ; top5 ->  72.26171875  and loss:  466.72688019275665
test acc: top1 ->  52.3 ; top5 ->  76.41  and loss:  820.51774045825
forward train acc: top1 ->  47.90625 ; top5 ->  72.01953125  and loss:  469.0699418783188
test acc: top1 ->  52.368 ; top5 ->  76.484  and loss:  817.4862262904644
forward train acc: top1 ->  48.60546875 ; top5 ->  72.52734375  and loss:  464.03129863739014
test acc: top1 ->  53.47 ; top5 ->  77.086  and loss:  802.5895593166351
forward train acc: top1 ->  48.52734375 ; top5 ->  72.4609375  and loss:  462.8840676546097
test acc: top1 ->  53.406 ; top5 ->  77.366  and loss:  799.1767943501472
forward train acc: top1 ->  49.0625 ; top5 ->  73.1796875  and loss:  455.07362937927246
test acc: top1 ->  53.422 ; top5 ->  77.278  and loss:  798.6763555407524
forward train acc: top1 ->  49.32421875 ; top5 ->  73.19140625  and loss:  454.7919169664383
test acc: top1 ->  53.638 ; top5 ->  77.398  and loss:  793.1731071472168
forward train acc: top1 ->  49.17578125 ; top5 ->  73.0546875  and loss:  456.6746236085892
test acc: top1 ->  53.97 ; top5 ->  77.634  and loss:  790.3273436129093
forward train acc: top1 ->  49.59375 ; top5 ->  72.81640625  and loss:  457.80875194072723
test acc: top1 ->  53.996 ; top5 ->  77.566  and loss:  789.4238673746586
forward train acc: top1 ->  49.5859375 ; top5 ->  73.06640625  and loss:  452.15125012397766
test acc: top1 ->  54.122 ; top5 ->  77.582  and loss:  786.3561094403267
forward train acc: top1 ->  50.46875 ; top5 ->  73.60546875  and loss:  448.38454735279083
test acc: top1 ->  54.276 ; top5 ->  77.694  and loss:  786.1156498193741
forward train acc: top1 ->  50.09765625 ; top5 ->  74.05859375  and loss:  444.6131030321121
test acc: top1 ->  54.222 ; top5 ->  77.76  and loss:  785.0534818768501
forward train acc: top1 ->  49.62109375 ; top5 ->  73.515625  and loss:  452.1519695520401
test acc: top1 ->  54.35 ; top5 ->  77.858  and loss:  783.9526863098145
forward train acc: top1 ->  50.3046875 ; top5 ->  73.8828125  and loss:  445.04918909072876
test acc: top1 ->  54.322 ; top5 ->  77.772  and loss:  784.2651577591896
forward train acc: top1 ->  49.79296875 ; top5 ->  73.84765625  and loss:  446.7724896669388
test acc: top1 ->  54.356 ; top5 ->  77.816  and loss:  783.3120514750481
forward train acc: top1 ->  50.015625 ; top5 ->  73.7421875  and loss:  446.1869513988495
test acc: top1 ->  54.322 ; top5 ->  77.812  and loss:  782.9526894688606
forward train acc: top1 ->  50.4609375 ; top5 ->  73.66015625  and loss:  445.4919329881668
test acc: top1 ->  54.438 ; top5 ->  77.83  and loss:  782.0370708703995
forward train acc: top1 ->  49.921875 ; top5 ->  73.6171875  and loss:  447.87209010124207
test acc: top1 ->  54.428 ; top5 ->  77.85  and loss:  782.1251170635223
forward train acc: top1 ->  50.125 ; top5 ->  73.5390625  and loss:  449.5119893550873
test acc: top1 ->  54.432 ; top5 ->  77.852  and loss:  781.8188571333885
forward train acc: top1 ->  50.16796875 ; top5 ->  74.00390625  and loss:  446.1057254076004
test acc: top1 ->  54.448 ; top5 ->  77.838  and loss:  781.3553552627563
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -117.98935747146606 , diff:  117.98935747146606
adv train loss:  -116.51192808151245 , diff:  1.4774293899536133
layer  4  adv train finish, try to retain  241
test acc: top1 ->  50.854 ; top5 ->  75.122  and loss:  851.9538094997406
forward train acc: top1 ->  47.83203125 ; top5 ->  71.90234375  and loss:  471.82844173908234
test acc: top1 ->  52.862 ; top5 ->  76.808  and loss:  814.6099353432655
forward train acc: top1 ->  47.09375 ; top5 ->  71.640625  and loss:  475.0354633331299
test acc: top1 ->  52.63 ; top5 ->  76.526  and loss:  817.4490420222282
forward train acc: top1 ->  47.265625 ; top5 ->  71.85546875  and loss:  475.9574068784714
test acc: top1 ->  51.988 ; top5 ->  76.13  and loss:  829.1350055932999
forward train acc: top1 ->  47.1875 ; top5 ->  71.578125  and loss:  474.7918747663498
test acc: top1 ->  53.214 ; top5 ->  77.034  and loss:  804.8187825679779
forward train acc: top1 ->  48.00390625 ; top5 ->  71.92578125  and loss:  470.46795177459717
test acc: top1 ->  53.394 ; top5 ->  77.14  and loss:  802.4658831357956
forward train acc: top1 ->  48.04296875 ; top5 ->  71.9375  and loss:  466.97926354408264
test acc: top1 ->  53.716 ; top5 ->  77.316  and loss:  798.8302958607674
forward train acc: top1 ->  48.9609375 ; top5 ->  73.13671875  and loss:  457.40722918510437
test acc: top1 ->  54.006 ; top5 ->  77.498  and loss:  793.1976524293423
forward train acc: top1 ->  49.46875 ; top5 ->  72.91796875  and loss:  456.285462975502
test acc: top1 ->  54.182 ; top5 ->  77.41  and loss:  792.3780661821365
forward train acc: top1 ->  49.30859375 ; top5 ->  73.1875  and loss:  454.1594125032425
test acc: top1 ->  54.228 ; top5 ->  77.58  and loss:  790.438254326582
forward train acc: top1 ->  49.19921875 ; top5 ->  72.8046875  and loss:  460.0988669395447
test acc: top1 ->  54.364 ; top5 ->  77.616  and loss:  787.5703356266022
forward train acc: top1 ->  49.29296875 ; top5 ->  73.49609375  and loss:  453.74178886413574
test acc: top1 ->  54.482 ; top5 ->  77.712  and loss:  785.615700751543
forward train acc: top1 ->  49.53125 ; top5 ->  73.2734375  and loss:  454.7641067504883
test acc: top1 ->  54.338 ; top5 ->  77.766  and loss:  785.0677080154419
forward train acc: top1 ->  49.6484375 ; top5 ->  73.5078125  and loss:  452.91109228134155
test acc: top1 ->  54.396 ; top5 ->  77.75  and loss:  784.9282696545124
forward train acc: top1 ->  49.5546875 ; top5 ->  73.23828125  and loss:  453.45327174663544
test acc: top1 ->  54.414 ; top5 ->  77.764  and loss:  784.4785991311073
forward train acc: top1 ->  49.72265625 ; top5 ->  73.1953125  and loss:  454.11517095565796
test acc: top1 ->  54.518 ; top5 ->  77.822  and loss:  783.2988640367985
forward train acc: top1 ->  49.75390625 ; top5 ->  73.58203125  and loss:  453.9510340690613
test acc: top1 ->  54.546 ; top5 ->  77.848  and loss:  783.4011147916317
forward train acc: top1 ->  50.06640625 ; top5 ->  73.66796875  and loss:  449.1617604494095
test acc: top1 ->  54.6 ; top5 ->  77.88  and loss:  782.2806190252304
forward train acc: top1 ->  49.6875 ; top5 ->  73.30859375  and loss:  451.33094799518585
test acc: top1 ->  54.544 ; top5 ->  77.938  and loss:  781.6904016435146
forward train acc: top1 ->  49.75 ; top5 ->  73.61328125  and loss:  450.720517039299
test acc: top1 ->  54.592 ; top5 ->  77.938  and loss:  781.7436733841896
forward train acc: top1 ->  49.69921875 ; top5 ->  73.109375  and loss:  452.1373565196991
test acc: top1 ->  54.606 ; top5 ->  77.95  and loss:  781.7113564014435
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -116.30100166797638 , diff:  116.30100166797638
adv train loss:  -113.26354539394379 , diff:  3.0374562740325928
adv train loss:  -115.06199276447296 , diff:  1.7984473705291748
************ all values are small in this layer **********
layer  5  adv train finish, try to retain  9215
test acc: top1 ->  53.722 ; top5 ->  77.538  and loss:  803.62786719203
forward train acc: top1 ->  48.640625 ; top5 ->  72.03125  and loss:  464.2714568376541
test acc: top1 ->  52.722 ; top5 ->  76.846  and loss:  813.6599462032318
forward train acc: top1 ->  48.36328125 ; top5 ->  72.30859375  and loss:  467.14504981040955
test acc: top1 ->  52.786 ; top5 ->  76.682  and loss:  815.5919456481934
forward train acc: top1 ->  47.80078125 ; top5 ->  72.0703125  and loss:  470.3286534547806
test acc: top1 ->  52.764 ; top5 ->  76.848  and loss:  813.1411039233208
forward train acc: top1 ->  48.51171875 ; top5 ->  72.76171875  and loss:  465.0437099933624
test acc: top1 ->  53.514 ; top5 ->  77.272  and loss:  800.7944256663322
forward train acc: top1 ->  48.765625 ; top5 ->  72.640625  and loss:  463.71950340270996
test acc: top1 ->  53.554 ; top5 ->  77.23  and loss:  800.5440428853035
forward train acc: top1 ->  49.04296875 ; top5 ->  72.9609375  and loss:  458.1008166074753
test acc: top1 ->  53.786 ; top5 ->  77.244  and loss:  799.4981747865677
forward train acc: top1 ->  49.16015625 ; top5 ->  72.8671875  and loss:  459.56546103954315
test acc: top1 ->  54.008 ; top5 ->  77.564  and loss:  792.5433323979378
forward train acc: top1 ->  49.484375 ; top5 ->  73.28515625  and loss:  455.15389132499695
test acc: top1 ->  54.274 ; top5 ->  77.664  and loss:  788.1699242591858
forward train acc: top1 ->  49.59375 ; top5 ->  72.984375  and loss:  454.32880330085754
test acc: top1 ->  54.26 ; top5 ->  77.696  and loss:  787.9928022623062
forward train acc: top1 ->  49.19921875 ; top5 ->  73.0  and loss:  456.73584723472595
test acc: top1 ->  54.446 ; top5 ->  77.75  and loss:  786.1288683414459
forward train acc: top1 ->  49.63671875 ; top5 ->  73.2890625  and loss:  455.8839750289917
test acc: top1 ->  54.492 ; top5 ->  77.86  and loss:  783.9155992269516
forward train acc: top1 ->  49.62109375 ; top5 ->  73.51953125  and loss:  451.06926250457764
test acc: top1 ->  54.384 ; top5 ->  77.752  and loss:  785.0532971024513
forward train acc: top1 ->  50.50390625 ; top5 ->  73.984375  and loss:  444.84559535980225
test acc: top1 ->  54.536 ; top5 ->  77.886  and loss:  782.9110071063042
forward train acc: top1 ->  49.92578125 ; top5 ->  73.59375  and loss:  450.7045028209686
test acc: top1 ->  54.572 ; top5 ->  77.95  and loss:  782.8962926268578
forward train acc: top1 ->  49.79296875 ; top5 ->  73.42578125  and loss:  452.38161039352417
test acc: top1 ->  54.62 ; top5 ->  77.984  and loss:  782.6247369647026
forward train acc: top1 ->  49.546875 ; top5 ->  73.48046875  and loss:  451.0759754180908
test acc: top1 ->  54.588 ; top5 ->  77.99  and loss:  781.8794647455215
forward train acc: top1 ->  50.015625 ; top5 ->  73.52734375  and loss:  452.3409276008606
test acc: top1 ->  54.65 ; top5 ->  77.936  and loss:  782.0965362787247
forward train acc: top1 ->  50.13671875 ; top5 ->  74.01171875  and loss:  446.70990669727325
test acc: top1 ->  54.686 ; top5 ->  77.984  and loss:  781.5583484768867
forward train acc: top1 ->  49.828125 ; top5 ->  73.5  and loss:  452.6194131374359
test acc: top1 ->  54.68 ; top5 ->  77.988  and loss:  780.9675760865211
forward train acc: top1 ->  49.82421875 ; top5 ->  73.68359375  and loss:  451.48865139484406
test acc: top1 ->  54.716 ; top5 ->  77.962  and loss:  780.8540262579918
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  9216 / 9216 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -113.63891124725342 , diff:  113.63891124725342
adv train loss:  -113.58601701259613 , diff:  0.0528942346572876
layer  6  adv train finish, try to retain  4053
test acc: top1 ->  54.654 ; top5 ->  77.966  and loss:  780.9122947454453
forward train acc: top1 ->  48.87890625 ; top5 ->  72.5390625  and loss:  464.4380464553833
test acc: top1 ->  52.74 ; top5 ->  76.904  and loss:  813.992066860199
forward train acc: top1 ->  47.859375 ; top5 ->  71.58203125  and loss:  472.3198643922806
test acc: top1 ->  52.642 ; top5 ->  76.548  and loss:  818.7230349183083
forward train acc: top1 ->  47.48828125 ; top5 ->  71.5859375  and loss:  477.5602173805237
test acc: top1 ->  52.83 ; top5 ->  76.698  and loss:  817.9607746005058
forward train acc: top1 ->  47.82421875 ; top5 ->  72.3046875  and loss:  470.5423392057419
test acc: top1 ->  53.206 ; top5 ->  76.88  and loss:  810.4940600395203
forward train acc: top1 ->  47.921875 ; top5 ->  71.92578125  and loss:  470.28254437446594
test acc: top1 ->  53.592 ; top5 ->  77.22  and loss:  802.085955798626
forward train acc: top1 ->  47.9765625 ; top5 ->  72.2421875  and loss:  466.49252700805664
test acc: top1 ->  53.496 ; top5 ->  77.272  and loss:  803.5307021141052
forward train acc: top1 ->  48.93359375 ; top5 ->  72.87890625  and loss:  461.28911876678467
test acc: top1 ->  53.814 ; top5 ->  77.392  and loss:  797.4766113758087
forward train acc: top1 ->  48.71875 ; top5 ->  72.828125  and loss:  462.20832681655884
test acc: top1 ->  53.836 ; top5 ->  77.49  and loss:  794.829082429409
forward train acc: top1 ->  48.7421875 ; top5 ->  73.0  and loss:  462.347727894783
test acc: top1 ->  53.91 ; top5 ->  77.614  and loss:  793.8481070995331
forward train acc: top1 ->  49.33984375 ; top5 ->  72.83984375  and loss:  458.1667979955673
test acc: top1 ->  54.1 ; top5 ->  77.558  and loss:  791.7130419611931
forward train acc: top1 ->  50.31640625 ; top5 ->  73.6640625  and loss:  450.3693151473999
test acc: top1 ->  54.24 ; top5 ->  77.618  and loss:  790.5510728359222
forward train acc: top1 ->  49.6796875 ; top5 ->  73.2265625  and loss:  455.2266044616699
test acc: top1 ->  54.282 ; top5 ->  77.646  and loss:  788.7984996438026
forward train acc: top1 ->  50.171875 ; top5 ->  73.58203125  and loss:  451.8947579860687
test acc: top1 ->  54.342 ; top5 ->  77.764  and loss:  787.8249185681343
forward train acc: top1 ->  49.52734375 ; top5 ->  73.80859375  and loss:  456.364866733551
test acc: top1 ->  54.46 ; top5 ->  77.73  and loss:  787.3755828738213
forward train acc: top1 ->  49.46875 ; top5 ->  73.05078125  and loss:  458.15377032756805
test acc: top1 ->  54.446 ; top5 ->  77.738  and loss:  787.1869663000107
forward train acc: top1 ->  49.61328125 ; top5 ->  73.55859375  and loss:  452.8342876434326
test acc: top1 ->  54.526 ; top5 ->  77.762  and loss:  786.3445304632187
forward train acc: top1 ->  49.2421875 ; top5 ->  73.1640625  and loss:  458.05799436569214
test acc: top1 ->  54.57 ; top5 ->  77.76  and loss:  785.5632719993591
forward train acc: top1 ->  49.69140625 ; top5 ->  73.4765625  and loss:  454.18624913692474
test acc: top1 ->  54.548 ; top5 ->  77.794  and loss:  785.4127053618431
forward train acc: top1 ->  49.41796875 ; top5 ->  73.05859375  and loss:  455.49672544002533
test acc: top1 ->  54.57 ; top5 ->  77.818  and loss:  785.406074821949
forward train acc: top1 ->  49.66015625 ; top5 ->  73.4921875  and loss:  451.5893220901489
test acc: top1 ->  54.584 ; top5 ->  77.842  and loss:  785.0785749554634
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  4096 / 4096 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -112.12035751342773 , diff:  112.12035751342773
adv train loss:  -114.39607965946198 , diff:  2.2757221460342407
layer  7  adv train finish, try to retain  3980
test acc: top1 ->  54.472 ; top5 ->  77.75  and loss:  786.2624596953392
forward train acc: top1 ->  48.24609375 ; top5 ->  71.890625  and loss:  470.1840674877167
test acc: top1 ->  52.902 ; top5 ->  76.722  and loss:  814.0972868800163
forward train acc: top1 ->  47.62890625 ; top5 ->  71.58203125  and loss:  476.65271985530853
test acc: top1 ->  52.656 ; top5 ->  76.556  and loss:  818.8040013313293
forward train acc: top1 ->  47.546875 ; top5 ->  71.80859375  and loss:  474.5316731929779
test acc: top1 ->  52.5 ; top5 ->  76.398  and loss:  822.9576318264008
forward train acc: top1 ->  47.8671875 ; top5 ->  71.609375  and loss:  471.7919225692749
test acc: top1 ->  53.114 ; top5 ->  76.886  and loss:  810.154194355011
forward train acc: top1 ->  48.68359375 ; top5 ->  72.6640625  and loss:  465.4400864839554
test acc: top1 ->  53.212 ; top5 ->  76.918  and loss:  810.289693236351
forward train acc: top1 ->  48.21875 ; top5 ->  72.33984375  and loss:  469.7540639638901
test acc: top1 ->  53.796 ; top5 ->  77.312  and loss:  800.4632035493851
forward train acc: top1 ->  48.90234375 ; top5 ->  72.8359375  and loss:  463.1470139026642
test acc: top1 ->  53.81 ; top5 ->  77.34  and loss:  798.4543839097023
forward train acc: top1 ->  48.73046875 ; top5 ->  72.58203125  and loss:  465.12077021598816
test acc: top1 ->  53.828 ; top5 ->  77.466  and loss:  796.94278216362
forward train acc: top1 ->  49.66015625 ; top5 ->  73.16015625  and loss:  457.358016371727
test acc: top1 ->  53.938 ; top5 ->  77.424  and loss:  796.2282794117928
forward train acc: top1 ->  49.13671875 ; top5 ->  72.83203125  and loss:  458.5097099542618
test acc: top1 ->  53.994 ; top5 ->  77.552  and loss:  792.4980595707893
forward train acc: top1 ->  48.89453125 ; top5 ->  72.515625  and loss:  461.86567068099976
test acc: top1 ->  54.032 ; top5 ->  77.558  and loss:  791.5484073162079
forward train acc: top1 ->  49.09765625 ; top5 ->  73.12109375  and loss:  458.5531758069992
test acc: top1 ->  54.196 ; top5 ->  77.544  and loss:  791.016486287117
forward train acc: top1 ->  50.0546875 ; top5 ->  73.39453125  and loss:  453.5830372571945
test acc: top1 ->  54.202 ; top5 ->  77.594  and loss:  789.7400501966476
forward train acc: top1 ->  49.2265625 ; top5 ->  73.0  and loss:  460.4572205543518
test acc: top1 ->  54.212 ; top5 ->  77.6  and loss:  789.3400350213051
forward train acc: top1 ->  49.4375 ; top5 ->  73.34375  and loss:  456.4158397912979
test acc: top1 ->  54.276 ; top5 ->  77.7  and loss:  788.0004057884216
forward train acc: top1 ->  50.234375 ; top5 ->  73.42578125  and loss:  453.96204662323
test acc: top1 ->  54.272 ; top5 ->  77.736  and loss:  788.1998553276062
forward train acc: top1 ->  49.80859375 ; top5 ->  73.34375  and loss:  457.45256304740906
test acc: top1 ->  54.228 ; top5 ->  77.736  and loss:  787.9652832150459
forward train acc: top1 ->  49.71875 ; top5 ->  73.36328125  and loss:  453.6586071252823
test acc: top1 ->  54.346 ; top5 ->  77.738  and loss:  787.3160613179207
forward train acc: top1 ->  49.53125 ; top5 ->  73.54296875  and loss:  453.66295289993286
test acc: top1 ->  54.394 ; top5 ->  77.762  and loss:  787.305106818676
forward train acc: top1 ->  49.4609375 ; top5 ->  73.33984375  and loss:  455.1902508735657
test acc: top1 ->  54.338 ; top5 ->  77.734  and loss:  787.3042674660683
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  4096 / 4096 , inc:  1
layer  0  :  0.984375  ==>  63 / 64 , inc:  2
layer  1  :  0.9947916666666666  ==>  191 / 192 , inc:  2
layer  2  :  1.0  ==>  384 / 384 , inc:  1
layer  3  :  1.0  ==>  256 / 256 , inc:  1
layer  4  :  1.0  ==>  256 / 256 , inc:  1
layer  5  :  1.0  ==>  9216 / 9216 , inc:  1
layer  6  :  1.0  ==>  4096 / 4096 , inc:  1
layer  7  :  1.0  ==>  4096 / 4096 , inc:  1
eps [0.0015625, 0.0005208333333333333, 0.0001953125, 0.00029296875000000004, 0.00029296875000000004, 8.138020833333333e-06, 1.8310546875000003e-05, 1.8310546875000003e-05]  wait [0, 0, 2, 2, 2, 2, 2, 2]  inc [2, 2, 1, 1, 1, 1, 1, 1]  tol: 0
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  1  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -123.63658702373505 , diff:  123.63658702373505
adv train loss:  -123.37126398086548 , diff:  0.26532304286956787
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  61
test acc: top1 ->  47.942 ; top5 ->  72.23  and loss:  918.9072598218918
forward train acc: top1 ->  47.2890625 ; top5 ->  71.33984375  and loss:  474.99551141262054
test acc: top1 ->  52.558 ; top5 ->  76.542  and loss:  821.6335130929947
forward train acc: top1 ->  46.58203125 ; top5 ->  70.9765625  and loss:  488.16697549819946
test acc: top1 ->  52.342 ; top5 ->  76.344  and loss:  824.7026646733284
forward train acc: top1 ->  46.99609375 ; top5 ->  71.49609375  and loss:  480.11520648002625
test acc: top1 ->  52.262 ; top5 ->  76.402  and loss:  825.8715999722481
forward train acc: top1 ->  47.53515625 ; top5 ->  71.37890625  and loss:  475.8534880876541
test acc: top1 ->  52.894 ; top5 ->  76.706  and loss:  815.4795362353325
forward train acc: top1 ->  48.09375 ; top5 ->  72.0625  and loss:  472.2473188638687
test acc: top1 ->  52.966 ; top5 ->  76.89  and loss:  813.0848861932755
forward train acc: top1 ->  48.67578125 ; top5 ->  72.34375  and loss:  467.8786462545395
test acc: top1 ->  53.17 ; top5 ->  76.97  and loss:  810.7375156283379
forward train acc: top1 ->  48.5390625 ; top5 ->  72.36328125  and loss:  467.2030324935913
test acc: top1 ->  53.454 ; top5 ->  77.1  and loss:  806.5992930531502
forward train acc: top1 ->  48.72265625 ; top5 ->  72.83984375  and loss:  462.43115866184235
test acc: top1 ->  53.508 ; top5 ->  77.11  and loss:  806.8938997983932
forward train acc: top1 ->  48.69140625 ; top5 ->  72.3984375  and loss:  465.56251311302185
test acc: top1 ->  53.488 ; top5 ->  77.234  and loss:  803.3063853979111
forward train acc: top1 ->  49.04296875 ; top5 ->  72.91796875  and loss:  462.31670916080475
test acc: top1 ->  53.782 ; top5 ->  77.346  and loss:  799.7595759034157
forward train acc: top1 ->  48.76953125 ; top5 ->  72.5234375  and loss:  466.9529527425766
test acc: top1 ->  54.0 ; top5 ->  77.428  and loss:  797.5537658333778
forward train acc: top1 ->  49.08203125 ; top5 ->  72.72265625  and loss:  464.4948390722275
test acc: top1 ->  53.838 ; top5 ->  77.522  and loss:  797.9214407205582
forward train acc: top1 ->  48.84375 ; top5 ->  72.96875  and loss:  462.20177364349365
test acc: top1 ->  54.026 ; top5 ->  77.542  and loss:  795.8357121348381
forward train acc: top1 ->  49.33984375 ; top5 ->  73.37109375  and loss:  455.7698460817337
test acc: top1 ->  54.068 ; top5 ->  77.516  and loss:  794.7868757843971
forward train acc: top1 ->  48.8984375 ; top5 ->  72.8046875  and loss:  462.7164319753647
test acc: top1 ->  54.006 ; top5 ->  77.598  and loss:  795.4047167897224
forward train acc: top1 ->  49.76171875 ; top5 ->  73.09375  and loss:  457.40906822681427
test acc: top1 ->  54.1 ; top5 ->  77.68  and loss:  793.7100535035133
forward train acc: top1 ->  49.0234375 ; top5 ->  73.03125  and loss:  458.91889226436615
test acc: top1 ->  54.088 ; top5 ->  77.64  and loss:  794.048032939434
forward train acc: top1 ->  49.34375 ; top5 ->  73.16015625  and loss:  458.0706989765167
test acc: top1 ->  54.176 ; top5 ->  77.666  and loss:  793.3464197516441
forward train acc: top1 ->  49.6796875 ; top5 ->  73.5625  and loss:  453.52633929252625
test acc: top1 ->  54.162 ; top5 ->  77.646  and loss:  793.4936538338661
forward train acc: top1 ->  49.37890625 ; top5 ->  73.30859375  and loss:  457.088703751564
test acc: top1 ->  54.17 ; top5 ->  77.664  and loss:  793.4193006753922
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  63 / 64 , inc:  2
---------------- start layer  1  ---------------
adv train loss:  -129.98452734947205 , diff:  129.98452734947205
adv train loss:  -131.23280549049377 , diff:  1.2482781410217285
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  189
test acc: top1 ->  49.418 ; top5 ->  73.728  and loss:  890.3469213247299
forward train acc: top1 ->  46.85546875 ; top5 ->  71.3359375  and loss:  481.9762295484543
test acc: top1 ->  52.568 ; top5 ->  76.36  and loss:  825.5519474744797
forward train acc: top1 ->  46.80078125 ; top5 ->  71.234375  and loss:  482.3244537115097
test acc: top1 ->  52.03 ; top5 ->  76.138  and loss:  832.2916660308838
forward train acc: top1 ->  47.1875 ; top5 ->  71.59765625  and loss:  479.1119238138199
test acc: top1 ->  52.128 ; top5 ->  76.232  and loss:  829.6067078709602
forward train acc: top1 ->  47.19140625 ; top5 ->  71.69921875  and loss:  474.92206501960754
test acc: top1 ->  52.974 ; top5 ->  76.888  and loss:  817.0166954994202
forward train acc: top1 ->  48.0625 ; top5 ->  71.90625  and loss:  473.2921265363693
test acc: top1 ->  53.254 ; top5 ->  76.976  and loss:  810.9747956395149
forward train acc: top1 ->  48.29296875 ; top5 ->  72.046875  and loss:  470.57601606845856
test acc: top1 ->  53.268 ; top5 ->  77.014  and loss:  809.7868700623512
forward train acc: top1 ->  48.1171875 ; top5 ->  72.30859375  and loss:  468.6998471021652
test acc: top1 ->  53.648 ; top5 ->  77.298  and loss:  803.1479934453964
forward train acc: top1 ->  48.88671875 ; top5 ->  72.78125  and loss:  464.62057387828827
test acc: top1 ->  53.638 ; top5 ->  77.372  and loss:  802.5688932538033
forward train acc: top1 ->  48.77734375 ; top5 ->  73.0625  and loss:  461.9963835477829
test acc: top1 ->  53.85 ; top5 ->  77.29  and loss:  801.5331182479858
forward train acc: top1 ->  49.05859375 ; top5 ->  72.49609375  and loss:  465.86092925071716
test acc: top1 ->  53.994 ; top5 ->  77.458  and loss:  797.7130661010742
forward train acc: top1 ->  48.92578125 ; top5 ->  72.8984375  and loss:  462.65626990795135
test acc: top1 ->  53.976 ; top5 ->  77.484  and loss:  797.9641320705414
forward train acc: top1 ->  49.015625 ; top5 ->  72.35546875  and loss:  462.9186261892319
test acc: top1 ->  53.98 ; top5 ->  77.502  and loss:  796.630097091198
forward train acc: top1 ->  48.8984375 ; top5 ->  72.79296875  and loss:  461.89076578617096
test acc: top1 ->  54.138 ; top5 ->  77.634  and loss:  795.3330051898956
forward train acc: top1 ->  48.90234375 ; top5 ->  72.578125  and loss:  465.5986348390579
test acc: top1 ->  54.1 ; top5 ->  77.576  and loss:  795.6314659118652
forward train acc: top1 ->  49.484375 ; top5 ->  72.88671875  and loss:  459.6897040605545
test acc: top1 ->  54.154 ; top5 ->  77.572  and loss:  794.4569951891899
forward train acc: top1 ->  49.41796875 ; top5 ->  72.87109375  and loss:  459.8540645837784
test acc: top1 ->  54.258 ; top5 ->  77.652  and loss:  793.6904984712601
forward train acc: top1 ->  49.0078125 ; top5 ->  72.67578125  and loss:  462.08138048648834
test acc: top1 ->  54.206 ; top5 ->  77.612  and loss:  793.6500666737556
forward train acc: top1 ->  48.7578125 ; top5 ->  72.33984375  and loss:  466.272900223732
test acc: top1 ->  54.21 ; top5 ->  77.706  and loss:  793.2960208654404
forward train acc: top1 ->  49.66015625 ; top5 ->  73.0234375  and loss:  459.9022927284241
test acc: top1 ->  54.198 ; top5 ->  77.692  and loss:  792.9476295113564
forward train acc: top1 ->  49.4296875 ; top5 ->  72.5390625  and loss:  463.8036822080612
test acc: top1 ->  54.218 ; top5 ->  77.68  and loss:  792.6253138780594
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  191 / 192 , inc:  2
---------------- start layer  2  ---------------
adv train loss:  -114.79261445999146 , diff:  114.79261445999146
adv train loss:  -119.12816107273102 , diff:  4.335546612739563
adv train loss:  -117.70379304885864 , diff:  1.4243680238723755
layer  2  adv train finish, try to retain  367
test acc: top1 ->  52.54 ; top5 ->  76.166  and loss:  828.5082847476006
forward train acc: top1 ->  47.15234375 ; top5 ->  71.1875  and loss:  485.1849458217621
test acc: top1 ->  52.762 ; top5 ->  76.48  and loss:  823.3469819426537
forward train acc: top1 ->  47.2109375 ; top5 ->  71.00390625  and loss:  486.5662108659744
test acc: top1 ->  52.106 ; top5 ->  76.144  and loss:  833.1497569680214
forward train acc: top1 ->  46.41015625 ; top5 ->  70.609375  and loss:  491.8510650396347
test acc: top1 ->  52.16 ; top5 ->  76.26  and loss:  830.2477022409439
forward train acc: top1 ->  47.34765625 ; top5 ->  71.28125  and loss:  479.4567778110504
test acc: top1 ->  52.768 ; top5 ->  76.584  and loss:  819.8958349227905
forward train acc: top1 ->  47.69921875 ; top5 ->  71.60546875  and loss:  479.94596123695374
test acc: top1 ->  52.882 ; top5 ->  76.534  and loss:  818.7709369063377
forward train acc: top1 ->  47.6328125 ; top5 ->  72.23828125  and loss:  473.9076634645462
test acc: top1 ->  53.042 ; top5 ->  76.782  and loss:  816.2775282263756
forward train acc: top1 ->  47.921875 ; top5 ->  71.5859375  and loss:  474.7551407814026
test acc: top1 ->  53.212 ; top5 ->  77.02  and loss:  810.0430880188942
forward train acc: top1 ->  48.19921875 ; top5 ->  72.05859375  and loss:  474.32416689395905
test acc: top1 ->  53.326 ; top5 ->  77.12  and loss:  807.8552207350731
forward train acc: top1 ->  47.76171875 ; top5 ->  71.7109375  and loss:  474.5379321575165
test acc: top1 ->  53.484 ; top5 ->  77.098  and loss:  807.9353134632111
forward train acc: top1 ->  48.66015625 ; top5 ->  72.1484375  and loss:  470.42031931877136
test acc: top1 ->  53.592 ; top5 ->  77.302  and loss:  802.830558180809
forward train acc: top1 ->  48.7734375 ; top5 ->  72.30859375  and loss:  467.9232783317566
test acc: top1 ->  53.626 ; top5 ->  77.272  and loss:  803.6123991012573
forward train acc: top1 ->  48.39453125 ; top5 ->  72.5234375  and loss:  467.66757225990295
test acc: top1 ->  53.748 ; top5 ->  77.308  and loss:  801.7580431103706
forward train acc: top1 ->  48.32421875 ; top5 ->  72.40625  and loss:  467.5931704044342
test acc: top1 ->  53.736 ; top5 ->  77.384  and loss:  800.5370980501175
forward train acc: top1 ->  49.265625 ; top5 ->  72.9140625  and loss:  460.61488008499146
test acc: top1 ->  53.748 ; top5 ->  77.292  and loss:  801.1887197494507
forward train acc: top1 ->  48.53125 ; top5 ->  72.625  and loss:  464.6258523464203
test acc: top1 ->  53.874 ; top5 ->  77.344  and loss:  799.887663424015
forward train acc: top1 ->  48.8125 ; top5 ->  72.5546875  and loss:  464.4522306919098
test acc: top1 ->  53.936 ; top5 ->  77.426  and loss:  799.0605771541595
forward train acc: top1 ->  48.671875 ; top5 ->  72.984375  and loss:  461.6570863723755
test acc: top1 ->  53.924 ; top5 ->  77.412  and loss:  798.6899716258049
forward train acc: top1 ->  48.9921875 ; top5 ->  72.6171875  and loss:  462.2788362503052
test acc: top1 ->  53.87 ; top5 ->  77.432  and loss:  798.6561632752419
forward train acc: top1 ->  49.1171875 ; top5 ->  72.90625  and loss:  463.99783527851105
test acc: top1 ->  53.904 ; top5 ->  77.468  and loss:  798.471376478672
forward train acc: top1 ->  48.7578125 ; top5 ->  72.34375  and loss:  468.1971560716629
test acc: top1 ->  53.912 ; top5 ->  77.452  and loss:  798.3112970590591
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  384 / 384 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -118.94655275344849 , diff:  118.94655275344849
adv train loss:  -119.39386487007141 , diff:  0.4473121166229248
layer  3  adv train finish, try to retain  247
test acc: top1 ->  51.712 ; top5 ->  75.596  and loss:  842.6625745296478
forward train acc: top1 ->  46.90625 ; top5 ->  70.80859375  and loss:  488.5971210002899
test acc: top1 ->  52.364 ; top5 ->  76.466  and loss:  825.5409762859344
forward train acc: top1 ->  46.7890625 ; top5 ->  70.75390625  and loss:  487.00920844078064
test acc: top1 ->  52.184 ; top5 ->  76.272  and loss:  831.7359391450882
forward train acc: top1 ->  46.08984375 ; top5 ->  70.6171875  and loss:  490.7553267478943
test acc: top1 ->  52.058 ; top5 ->  76.406  and loss:  832.3362236618996
forward train acc: top1 ->  47.5234375 ; top5 ->  71.27734375  and loss:  480.6193417310715
test acc: top1 ->  53.008 ; top5 ->  76.812  and loss:  817.2477151155472
forward train acc: top1 ->  46.890625 ; top5 ->  71.15625  and loss:  483.5265234708786
test acc: top1 ->  52.938 ; top5 ->  76.786  and loss:  816.8591378331184
forward train acc: top1 ->  47.6171875 ; top5 ->  71.671875  and loss:  478.4745316505432
test acc: top1 ->  52.892 ; top5 ->  76.936  and loss:  816.6264081597328
forward train acc: top1 ->  48.10546875 ; top5 ->  72.2734375  and loss:  471.9928947687149
test acc: top1 ->  53.234 ; top5 ->  77.158  and loss:  809.3439168930054
forward train acc: top1 ->  47.9609375 ; top5 ->  71.78125  and loss:  475.0907769203186
test acc: top1 ->  53.428 ; top5 ->  77.126  and loss:  806.6840517520905
forward train acc: top1 ->  48.171875 ; top5 ->  72.15234375  and loss:  471.16506254673004
test acc: top1 ->  53.62 ; top5 ->  77.304  and loss:  805.1513124108315
forward train acc: top1 ->  47.58984375 ; top5 ->  72.05078125  and loss:  474.7978093624115
test acc: top1 ->  53.712 ; top5 ->  77.414  and loss:  802.3166327476501
forward train acc: top1 ->  48.53125 ; top5 ->  72.34375  and loss:  468.24455976486206
test acc: top1 ->  53.67 ; top5 ->  77.438  and loss:  801.4104669094086
forward train acc: top1 ->  48.98828125 ; top5 ->  72.7734375  and loss:  464.4565531015396
test acc: top1 ->  53.716 ; top5 ->  77.48  and loss:  799.9413427114487
forward train acc: top1 ->  48.2109375 ; top5 ->  71.70703125  and loss:  473.2132234573364
test acc: top1 ->  53.862 ; top5 ->  77.496  and loss:  798.3811725378036
forward train acc: top1 ->  48.9765625 ; top5 ->  72.89453125  and loss:  464.56129944324493
test acc: top1 ->  53.906 ; top5 ->  77.506  and loss:  798.6075496077538
forward train acc: top1 ->  48.921875 ; top5 ->  72.5078125  and loss:  467.8280460834503
test acc: top1 ->  54.002 ; top5 ->  77.504  and loss:  798.4146464467049
forward train acc: top1 ->  48.4765625 ; top5 ->  71.83984375  and loss:  470.11914122104645
test acc: top1 ->  53.994 ; top5 ->  77.542  and loss:  797.7183113098145
forward train acc: top1 ->  48.81640625 ; top5 ->  72.109375  and loss:  466.3672777414322
test acc: top1 ->  54.03 ; top5 ->  77.514  and loss:  797.3890511989594
forward train acc: top1 ->  49.0 ; top5 ->  72.77734375  and loss:  465.3837798833847
test acc: top1 ->  54.004 ; top5 ->  77.59  and loss:  796.9407289028168
forward train acc: top1 ->  48.8203125 ; top5 ->  72.91015625  and loss:  465.151815533638
test acc: top1 ->  54.07 ; top5 ->  77.576  and loss:  796.5696412324905
forward train acc: top1 ->  48.6953125 ; top5 ->  72.6328125  and loss:  464.84933257102966
test acc: top1 ->  54.054 ; top5 ->  77.59  and loss:  796.4298368096352
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -120.4142062664032 , diff:  120.4142062664032
adv train loss:  -120.00571179389954 , diff:  0.4084944725036621
layer  4  adv train finish, try to retain  246
test acc: top1 ->  51.908 ; top5 ->  76.254  and loss:  832.2226721644402
forward train acc: top1 ->  47.06640625 ; top5 ->  70.9453125  and loss:  484.7317843437195
test acc: top1 ->  52.136 ; top5 ->  76.272  and loss:  827.4728500843048
forward train acc: top1 ->  46.5625 ; top5 ->  70.6171875  and loss:  488.02961242198944
test acc: top1 ->  52.03 ; top5 ->  75.978  and loss:  835.4595374464989
forward train acc: top1 ->  46.13671875 ; top5 ->  70.39453125  and loss:  492.1549174785614
test acc: top1 ->  51.868 ; top5 ->  76.176  and loss:  835.8603075742722
forward train acc: top1 ->  46.72265625 ; top5 ->  70.734375  and loss:  489.8401199579239
test acc: top1 ->  52.604 ; top5 ->  76.632  and loss:  820.8754594922066
forward train acc: top1 ->  46.9375 ; top5 ->  70.91015625  and loss:  485.126541018486
test acc: top1 ->  52.732 ; top5 ->  76.652  and loss:  820.8857933878899
forward train acc: top1 ->  47.37109375 ; top5 ->  71.68359375  and loss:  479.68569004535675
test acc: top1 ->  53.05 ; top5 ->  76.734  and loss:  816.4259243607521
forward train acc: top1 ->  48.1953125 ; top5 ->  71.95703125  and loss:  472.96840715408325
test acc: top1 ->  53.246 ; top5 ->  77.076  and loss:  809.549020588398
forward train acc: top1 ->  47.953125 ; top5 ->  71.63671875  and loss:  477.1615060567856
test acc: top1 ->  53.328 ; top5 ->  77.13  and loss:  807.3366276621819
forward train acc: top1 ->  47.87109375 ; top5 ->  71.8984375  and loss:  474.8154104948044
test acc: top1 ->  53.268 ; top5 ->  77.148  and loss:  807.1585129499435
forward train acc: top1 ->  48.00390625 ; top5 ->  71.87109375  and loss:  473.65316021442413
test acc: top1 ->  53.472 ; top5 ->  77.28  and loss:  804.3719244003296
forward train acc: top1 ->  48.390625 ; top5 ->  72.578125  and loss:  468.1531230211258
test acc: top1 ->  53.598 ; top5 ->  77.252  and loss:  804.7078152298927
forward train acc: top1 ->  48.4140625 ; top5 ->  71.9765625  and loss:  472.31008315086365
test acc: top1 ->  53.538 ; top5 ->  77.236  and loss:  804.7464405298233
forward train acc: top1 ->  48.53515625 ; top5 ->  72.26953125  and loss:  467.9199569225311
test acc: top1 ->  53.588 ; top5 ->  77.446  and loss:  801.0681619048119
forward train acc: top1 ->  48.609375 ; top5 ->  72.4140625  and loss:  464.94567370414734
test acc: top1 ->  53.568 ; top5 ->  77.462  and loss:  801.5595173835754
forward train acc: top1 ->  48.828125 ; top5 ->  72.76953125  and loss:  463.4555742740631
test acc: top1 ->  53.606 ; top5 ->  77.534  and loss:  800.8766051530838
forward train acc: top1 ->  48.828125 ; top5 ->  72.0546875  and loss:  471.7583636045456
test acc: top1 ->  53.704 ; top5 ->  77.514  and loss:  799.6159842014313
forward train acc: top1 ->  48.71484375 ; top5 ->  72.74609375  and loss:  464.42576801776886
test acc: top1 ->  53.722 ; top5 ->  77.5  and loss:  799.4851091504097
forward train acc: top1 ->  47.66015625 ; top5 ->  72.09375  and loss:  471.75912642478943
test acc: top1 ->  53.778 ; top5 ->  77.492  and loss:  798.858644425869
forward train acc: top1 ->  47.57421875 ; top5 ->  72.29296875  and loss:  472.88394820690155
test acc: top1 ->  53.76 ; top5 ->  77.504  and loss:  798.8245889544487
forward train acc: top1 ->  48.76171875 ; top5 ->  72.58203125  and loss:  467.1963950395584
test acc: top1 ->  53.794 ; top5 ->  77.502  and loss:  798.7131410241127
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -118.60796117782593 , diff:  118.60796117782593
adv train loss:  -118.18791949748993 , diff:  0.42004168033599854
layer  5  adv train finish, try to retain  9190
test acc: top1 ->  53.294 ; top5 ->  77.15  and loss:  811.7110673189163
forward train acc: top1 ->  46.8203125 ; top5 ->  70.94921875  and loss:  485.1459354162216
test acc: top1 ->  52.39 ; top5 ->  76.236  and loss:  830.7883148193359
forward train acc: top1 ->  46.7421875 ; top5 ->  70.63671875  and loss:  490.06592869758606
test acc: top1 ->  52.312 ; top5 ->  76.406  and loss:  826.5014140605927
forward train acc: top1 ->  46.75 ; top5 ->  70.90625  and loss:  486.74909806251526
test acc: top1 ->  52.49 ; top5 ->  76.488  and loss:  825.8013002276421
forward train acc: top1 ->  47.2265625 ; top5 ->  71.53125  and loss:  481.2665994167328
test acc: top1 ->  53.022 ; top5 ->  76.778  and loss:  817.5453881025314
forward train acc: top1 ->  47.88671875 ; top5 ->  71.828125  and loss:  477.3868656158447
test acc: top1 ->  52.998 ; top5 ->  76.844  and loss:  815.8162432909012
forward train acc: top1 ->  47.90625 ; top5 ->  71.8515625  and loss:  477.1050612926483
test acc: top1 ->  53.156 ; top5 ->  76.99  and loss:  812.2953845262527
forward train acc: top1 ->  48.0546875 ; top5 ->  71.60546875  and loss:  477.02697694301605
test acc: top1 ->  53.504 ; top5 ->  77.302  and loss:  806.8980977535248
forward train acc: top1 ->  48.421875 ; top5 ->  72.09375  and loss:  472.6074182987213
test acc: top1 ->  53.57 ; top5 ->  77.33  and loss:  804.761376619339
forward train acc: top1 ->  48.5078125 ; top5 ->  72.24609375  and loss:  469.8157149553299
test acc: top1 ->  53.638 ; top5 ->  77.442  and loss:  802.6627942323685
forward train acc: top1 ->  48.5 ; top5 ->  72.48046875  and loss:  470.5993196964264
test acc: top1 ->  53.746 ; top5 ->  77.44  and loss:  802.4709504246712
forward train acc: top1 ->  48.38671875 ; top5 ->  72.4140625  and loss:  470.42844450473785
test acc: top1 ->  53.726 ; top5 ->  77.372  and loss:  802.2572094202042
forward train acc: top1 ->  48.88671875 ; top5 ->  72.3515625  and loss:  468.7049217224121
test acc: top1 ->  53.742 ; top5 ->  77.412  and loss:  800.8915432691574
forward train acc: top1 ->  49.37890625 ; top5 ->  72.6640625  and loss:  465.7667261362076
test acc: top1 ->  53.848 ; top5 ->  77.474  and loss:  799.4306644797325
forward train acc: top1 ->  48.12109375 ; top5 ->  72.03125  and loss:  469.6130962371826
test acc: top1 ->  53.862 ; top5 ->  77.532  and loss:  798.4636460542679
forward train acc: top1 ->  48.66015625 ; top5 ->  72.515625  and loss:  467.8865078687668
test acc: top1 ->  53.802 ; top5 ->  77.506  and loss:  799.4318468570709
forward train acc: top1 ->  48.4453125 ; top5 ->  72.21875  and loss:  473.02572190761566
test acc: top1 ->  53.962 ; top5 ->  77.508  and loss:  799.0496667027473
forward train acc: top1 ->  48.92578125 ; top5 ->  72.2890625  and loss:  468.42364728450775
test acc: top1 ->  53.944 ; top5 ->  77.528  and loss:  797.7545151114464
forward train acc: top1 ->  48.6171875 ; top5 ->  72.14453125  and loss:  469.6377041339874
test acc: top1 ->  54.0 ; top5 ->  77.616  and loss:  797.2372155785561
forward train acc: top1 ->  48.9765625 ; top5 ->  72.4609375  and loss:  467.1536114215851
test acc: top1 ->  53.994 ; top5 ->  77.586  and loss:  796.8129531145096
forward train acc: top1 ->  48.91015625 ; top5 ->  72.66015625  and loss:  464.56231570243835
test acc: top1 ->  53.962 ; top5 ->  77.576  and loss:  797.2030847072601
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  9216 / 9216 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -116.64790880680084 , diff:  116.64790880680084
adv train loss:  -117.7699203491211 , diff:  1.1220115423202515
layer  6  adv train finish, try to retain  4064
test acc: top1 ->  53.95 ; top5 ->  77.586  and loss:  797.4206658601761
forward train acc: top1 ->  47.22265625 ; top5 ->  71.515625  and loss:  481.7167078256607
test acc: top1 ->  52.434 ; top5 ->  76.322  and loss:  827.3168106675148
forward train acc: top1 ->  47.00390625 ; top5 ->  70.8359375  and loss:  488.7623826265335
test acc: top1 ->  52.45 ; top5 ->  76.33  and loss:  830.6183012723923
forward train acc: top1 ->  46.42578125 ; top5 ->  70.8203125  and loss:  490.12895226478577
test acc: top1 ->  52.46 ; top5 ->  76.326  and loss:  828.5071498155594
forward train acc: top1 ->  47.09375 ; top5 ->  71.26953125  and loss:  484.8762822151184
test acc: top1 ->  53.018 ; top5 ->  76.826  and loss:  817.3615878224373
forward train acc: top1 ->  47.6953125 ; top5 ->  71.765625  and loss:  476.62990069389343
test acc: top1 ->  53.056 ; top5 ->  76.82  and loss:  817.2927349209785
forward train acc: top1 ->  47.984375 ; top5 ->  71.73046875  and loss:  477.26621747016907
test acc: top1 ->  52.904 ; top5 ->  76.8  and loss:  817.5763155817986
forward train acc: top1 ->  47.83984375 ; top5 ->  72.08203125  and loss:  474.8226411342621
test acc: top1 ->  53.278 ; top5 ->  77.08  and loss:  809.6733175516129
forward train acc: top1 ->  48.34765625 ; top5 ->  71.8046875  and loss:  471.815465092659
test acc: top1 ->  53.384 ; top5 ->  77.074  and loss:  809.4782584309578
forward train acc: top1 ->  48.23828125 ; top5 ->  72.03125  and loss:  474.12587916851044
test acc: top1 ->  53.406 ; top5 ->  77.166  and loss:  806.310705780983
forward train acc: top1 ->  48.16015625 ; top5 ->  72.03515625  and loss:  471.3791286945343
test acc: top1 ->  53.548 ; top5 ->  77.276  and loss:  804.3826925754547
forward train acc: top1 ->  48.0078125 ; top5 ->  72.05078125  and loss:  474.2486033439636
test acc: top1 ->  53.64 ; top5 ->  77.336  and loss:  803.8319907784462
forward train acc: top1 ->  48.80859375 ; top5 ->  72.2421875  and loss:  468.29224693775177
test acc: top1 ->  53.65 ; top5 ->  77.338  and loss:  801.8756673336029
forward train acc: top1 ->  48.2578125 ; top5 ->  72.06640625  and loss:  471.2919293642044
test acc: top1 ->  53.714 ; top5 ->  77.356  and loss:  801.4734602570534
forward train acc: top1 ->  48.49609375 ; top5 ->  72.16796875  and loss:  470.1035113334656
test acc: top1 ->  53.756 ; top5 ->  77.35  and loss:  800.7986640930176
forward train acc: top1 ->  48.46875 ; top5 ->  72.484375  and loss:  470.9832147359848
test acc: top1 ->  53.786 ; top5 ->  77.42  and loss:  800.4828433394432
forward train acc: top1 ->  48.30078125 ; top5 ->  72.05078125  and loss:  471.7974590063095
test acc: top1 ->  53.814 ; top5 ->  77.458  and loss:  800.4570828080177
forward train acc: top1 ->  48.57421875 ; top5 ->  72.43359375  and loss:  468.1704901456833
test acc: top1 ->  53.8 ; top5 ->  77.474  and loss:  799.3011326789856
forward train acc: top1 ->  48.7265625 ; top5 ->  72.87109375  and loss:  463.66219651699066
test acc: top1 ->  53.818 ; top5 ->  77.532  and loss:  799.2472346425056
forward train acc: top1 ->  48.55859375 ; top5 ->  72.515625  and loss:  469.20076978206635
test acc: top1 ->  53.85 ; top5 ->  77.482  and loss:  799.1012735366821
forward train acc: top1 ->  48.12890625 ; top5 ->  71.984375  and loss:  473.69650518894196
test acc: top1 ->  53.878 ; top5 ->  77.548  and loss:  798.6851623058319
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  4096 / 4096 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -115.39063787460327 , diff:  115.39063787460327
adv train loss:  -114.73119580745697 , diff:  0.6594420671463013
layer  7  adv train finish, try to retain  3986
test acc: top1 ->  53.776 ; top5 ->  77.534  and loss:  799.4851158261299
forward train acc: top1 ->  46.98828125 ; top5 ->  70.91015625  and loss:  487.11458706855774
test acc: top1 ->  52.328 ; top5 ->  76.24  and loss:  831.295774102211
forward train acc: top1 ->  46.66015625 ; top5 ->  70.703125  and loss:  491.0956857204437
test acc: top1 ->  52.124 ; top5 ->  76.212  and loss:  830.1250642538071
forward train acc: top1 ->  46.125 ; top5 ->  70.5625  and loss:  493.71066749095917
test acc: top1 ->  51.978 ; top5 ->  76.108  and loss:  834.6943901777267
forward train acc: top1 ->  47.078125 ; top5 ->  71.015625  and loss:  485.76366579532623
test acc: top1 ->  52.786 ; top5 ->  76.764  and loss:  818.9214125871658
forward train acc: top1 ->  47.28125 ; top5 ->  71.38671875  and loss:  481.2537508010864
test acc: top1 ->  53.144 ; top5 ->  76.756  and loss:  815.8312686681747
forward train acc: top1 ->  47.7734375 ; top5 ->  71.33984375  and loss:  481.42202484607697
test acc: top1 ->  52.988 ; top5 ->  76.932  and loss:  814.9482457637787
forward train acc: top1 ->  47.3125 ; top5 ->  71.36328125  and loss:  481.54860985279083
test acc: top1 ->  53.39 ; top5 ->  77.086  and loss:  809.5894227027893
forward train acc: top1 ->  47.7421875 ; top5 ->  71.9296875  and loss:  475.82460021972656
test acc: top1 ->  53.542 ; top5 ->  77.108  and loss:  807.7785583138466
forward train acc: top1 ->  47.87890625 ; top5 ->  71.8359375  and loss:  476.7472633123398
test acc: top1 ->  53.708 ; top5 ->  77.054  and loss:  806.4905298352242
forward train acc: top1 ->  48.69921875 ; top5 ->  72.31640625  and loss:  469.77518594264984
test acc: top1 ->  53.666 ; top5 ->  77.21  and loss:  804.3815917372704
forward train acc: top1 ->  48.04296875 ; top5 ->  72.453125  and loss:  473.68652045726776
test acc: top1 ->  53.748 ; top5 ->  77.24  and loss:  804.295973777771
forward train acc: top1 ->  48.2734375 ; top5 ->  72.26171875  and loss:  472.31289172172546
test acc: top1 ->  53.812 ; top5 ->  77.278  and loss:  802.4917989373207
forward train acc: top1 ->  48.328125 ; top5 ->  71.89453125  and loss:  472.6800791025162
test acc: top1 ->  53.868 ; top5 ->  77.338  and loss:  802.0517397522926
forward train acc: top1 ->  49.1015625 ; top5 ->  72.1484375  and loss:  468.1568785905838
test acc: top1 ->  53.946 ; top5 ->  77.384  and loss:  801.1137257218361
forward train acc: top1 ->  48.5703125 ; top5 ->  72.38671875  and loss:  470.31430172920227
test acc: top1 ->  53.96 ; top5 ->  77.41  and loss:  800.1046506762505
forward train acc: top1 ->  47.85546875 ; top5 ->  72.0234375  and loss:  474.7306076288223
test acc: top1 ->  54.01 ; top5 ->  77.396  and loss:  799.6387432217598
forward train acc: top1 ->  49.09765625 ; top5 ->  72.7421875  and loss:  462.96349942684174
test acc: top1 ->  53.992 ; top5 ->  77.4  and loss:  799.5047730207443
forward train acc: top1 ->  48.625 ; top5 ->  72.265625  and loss:  469.18443298339844
test acc: top1 ->  53.964 ; top5 ->  77.458  and loss:  799.3007289171219
forward train acc: top1 ->  48.6328125 ; top5 ->  72.65625  and loss:  469.50430023670197
test acc: top1 ->  54.038 ; top5 ->  77.408  and loss:  799.0913872122765
forward train acc: top1 ->  48.39453125 ; top5 ->  72.18359375  and loss:  470.3661826848984
test acc: top1 ->  54.04 ; top5 ->  77.404  and loss:  799.1235458254814
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  4096 / 4096 , inc:  1
layer  0  :  0.984375  ==>  63 / 64 , inc:  1
layer  1  :  0.9947916666666666  ==>  191 / 192 , inc:  1
layer  2  :  1.0  ==>  384 / 384 , inc:  1
layer  3  :  1.0  ==>  256 / 256 , inc:  1
layer  4  :  1.0  ==>  256 / 256 , inc:  1
layer  5  :  1.0  ==>  9216 / 9216 , inc:  1
layer  6  :  1.0  ==>  4096 / 4096 , inc:  1
layer  7  :  1.0  ==>  4096 / 4096 , inc:  1
eps [0.0011718750000000002, 0.000390625, 0.00014648437500000002, 0.00021972656250000003, 0.00021972656250000003, 6.103515625e-06, 1.3732910156250002e-05, 1.3732910156250002e-05]  wait [2, 2, 4, 4, 4, 4, 4, 4]  inc [1, 1, 1, 1, 1, 1, 1, 1]  tol: 0
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  2  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -123.70447254180908 , diff:  123.70447254180908
adv train loss:  -124.83774662017822 , diff:  1.1332740783691406
layer  0  adv train finish, try to retain  63
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -119.92062175273895 , diff:  119.92062175273895
adv train loss:  -118.53991425037384 , diff:  1.3807075023651123
layer  1  adv train finish, try to retain  175
test acc: top1 ->  50.31 ; top5 ->  74.36  and loss:  874.7769578695297
forward train acc: top1 ->  45.07421875 ; top5 ->  69.203125  and loss:  504.26873540878296
test acc: top1 ->  51.398 ; top5 ->  75.484  and loss:  848.3825438022614
forward train acc: top1 ->  45.4375 ; top5 ->  69.453125  and loss:  506.45043432712555
test acc: top1 ->  51.642 ; top5 ->  75.586  and loss:  845.7403354048729
forward train acc: top1 ->  44.92578125 ; top5 ->  69.26953125  and loss:  505.09355998039246
test acc: top1 ->  51.64 ; top5 ->  75.624  and loss:  846.618382036686
forward train acc: top1 ->  45.51953125 ; top5 ->  69.62109375  and loss:  502.11257243156433
test acc: top1 ->  51.976 ; top5 ->  76.008  and loss:  836.065113902092
forward train acc: top1 ->  46.73046875 ; top5 ->  70.328125  and loss:  493.21365094184875
test acc: top1 ->  52.322 ; top5 ->  76.17  and loss:  830.8817666769028
forward train acc: top1 ->  46.5390625 ; top5 ->  70.68359375  and loss:  489.08775436878204
test acc: top1 ->  52.13 ; top5 ->  76.11  and loss:  830.9483227133751
forward train acc: top1 ->  46.44921875 ; top5 ->  70.6875  and loss:  487.19175958633423
test acc: top1 ->  52.466 ; top5 ->  76.412  and loss:  823.96383279562
forward train acc: top1 ->  46.5703125 ; top5 ->  70.78515625  and loss:  489.34922528266907
test acc: top1 ->  52.592 ; top5 ->  76.482  and loss:  823.4688774943352
forward train acc: top1 ->  46.71484375 ; top5 ->  70.94140625  and loss:  488.2516220808029
test acc: top1 ->  52.626 ; top5 ->  76.554  and loss:  821.7645764946938
forward train acc: top1 ->  47.33203125 ; top5 ->  70.8359375  and loss:  484.1194967031479
test acc: top1 ->  52.806 ; top5 ->  76.624  and loss:  817.9812111854553
forward train acc: top1 ->  47.13671875 ; top5 ->  71.32421875  and loss:  484.3875033855438
test acc: top1 ->  52.76 ; top5 ->  76.66  and loss:  818.699916601181
forward train acc: top1 ->  47.1484375 ; top5 ->  70.87109375  and loss:  486.56768476963043
test acc: top1 ->  52.972 ; top5 ->  76.576  and loss:  817.5607568621635
forward train acc: top1 ->  47.59375 ; top5 ->  71.1875  and loss:  481.3096264600754
test acc: top1 ->  53.048 ; top5 ->  76.706  and loss:  815.9218742251396
forward train acc: top1 ->  47.390625 ; top5 ->  71.2109375  and loss:  481.3969415426254
test acc: top1 ->  53.076 ; top5 ->  76.75  and loss:  815.3161930441856
forward train acc: top1 ->  47.78125 ; top5 ->  71.33203125  and loss:  480.64490377902985
test acc: top1 ->  53.164 ; top5 ->  76.736  and loss:  814.3005205988884
forward train acc: top1 ->  47.359375 ; top5 ->  71.48046875  and loss:  481.7543100118637
test acc: top1 ->  53.164 ; top5 ->  76.806  and loss:  813.5305661559105
forward train acc: top1 ->  47.31640625 ; top5 ->  71.4296875  and loss:  483.5549018383026
test acc: top1 ->  53.208 ; top5 ->  76.818  and loss:  812.9030429124832
forward train acc: top1 ->  47.98828125 ; top5 ->  71.828125  and loss:  475.4013035297394
test acc: top1 ->  53.174 ; top5 ->  76.808  and loss:  813.1441938877106
forward train acc: top1 ->  47.6015625 ; top5 ->  71.52734375  and loss:  478.08571898937225
test acc: top1 ->  53.214 ; top5 ->  76.8  and loss:  812.8705059885979
forward train acc: top1 ->  47.6953125 ; top5 ->  71.78515625  and loss:  479.21069502830505
test acc: top1 ->  53.186 ; top5 ->  76.792  and loss:  812.8332473039627
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  191 / 192 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -130.0430691242218 , diff:  130.0430691242218
adv train loss:  -129.10566473007202 , diff:  0.9374043941497803
layer  2  adv train finish, try to retain  374
test acc: top1 ->  50.37 ; top5 ->  74.91  and loss:  865.5070774555206
forward train acc: top1 ->  45.87109375 ; top5 ->  70.23046875  and loss:  495.978351354599
test acc: top1 ->  52.118 ; top5 ->  76.086  and loss:  836.5032588839531
forward train acc: top1 ->  46.1015625 ; top5 ->  70.48828125  and loss:  495.3043874502182
test acc: top1 ->  52.166 ; top5 ->  76.068  and loss:  835.9879811406136
forward train acc: top1 ->  46.0703125 ; top5 ->  70.34375  and loss:  494.5493017435074
test acc: top1 ->  51.826 ; top5 ->  76.034  and loss:  838.7498952150345
forward train acc: top1 ->  46.58984375 ; top5 ->  71.515625  and loss:  485.79738759994507
test acc: top1 ->  52.552 ; top5 ->  76.612  and loss:  823.08934289217
forward train acc: top1 ->  46.7734375 ; top5 ->  70.99609375  and loss:  488.46470057964325
test acc: top1 ->  52.91 ; top5 ->  76.622  and loss:  821.524316072464
forward train acc: top1 ->  47.21875 ; top5 ->  71.0703125  and loss:  488.2945693731308
test acc: top1 ->  52.91 ; top5 ->  76.848  and loss:  817.0171597599983
forward train acc: top1 ->  46.921875 ; top5 ->  71.3125  and loss:  483.7224875688553
test acc: top1 ->  53.08 ; top5 ->  76.832  and loss:  813.3062967658043
forward train acc: top1 ->  47.16796875 ; top5 ->  71.54296875  and loss:  480.64594078063965
test acc: top1 ->  53.214 ; top5 ->  76.976  and loss:  811.2176478505135
forward train acc: top1 ->  47.8515625 ; top5 ->  71.390625  and loss:  480.2837482690811
test acc: top1 ->  53.206 ; top5 ->  76.954  and loss:  812.728112757206
forward train acc: top1 ->  47.07421875 ; top5 ->  71.40625  and loss:  483.2180805206299
test acc: top1 ->  53.28 ; top5 ->  77.05  and loss:  809.4047080874443
forward train acc: top1 ->  47.53125 ; top5 ->  71.46875  and loss:  480.78361308574677
test acc: top1 ->  53.418 ; top5 ->  77.188  and loss:  807.2670669555664
forward train acc: top1 ->  47.60546875 ; top5 ->  72.0390625  and loss:  474.5005145072937
test acc: top1 ->  53.422 ; top5 ->  77.1  and loss:  807.4974722862244
forward train acc: top1 ->  48.59765625 ; top5 ->  72.078125  and loss:  471.1281273365021
test acc: top1 ->  53.454 ; top5 ->  77.168  and loss:  806.4039863944054
forward train acc: top1 ->  47.91015625 ; top5 ->  71.72265625  and loss:  475.7486684322357
test acc: top1 ->  53.584 ; top5 ->  77.188  and loss:  805.3915376663208
forward train acc: top1 ->  47.89453125 ; top5 ->  71.44921875  and loss:  480.38084387779236
test acc: top1 ->  53.526 ; top5 ->  77.184  and loss:  805.4168601036072
forward train acc: top1 ->  47.984375 ; top5 ->  72.15625  and loss:  473.221578001976
test acc: top1 ->  53.596 ; top5 ->  77.198  and loss:  804.6517980098724
forward train acc: top1 ->  48.01953125 ; top5 ->  71.66015625  and loss:  479.5769774913788
test acc: top1 ->  53.588 ; top5 ->  77.158  and loss:  804.6066998243332
forward train acc: top1 ->  47.65625 ; top5 ->  71.72265625  and loss:  477.2914823293686
test acc: top1 ->  53.618 ; top5 ->  77.182  and loss:  804.3678797483444
forward train acc: top1 ->  48.203125 ; top5 ->  72.40234375  and loss:  471.2379854917526
test acc: top1 ->  53.584 ; top5 ->  77.198  and loss:  804.0365592837334
forward train acc: top1 ->  48.140625 ; top5 ->  71.875  and loss:  474.4522489309311
test acc: top1 ->  53.616 ; top5 ->  77.208  and loss:  803.9131481647491
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  384 / 384 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -121.05478525161743 , diff:  121.05478525161743
adv train loss:  -117.45244717597961 , diff:  3.6023380756378174
adv train loss:  -119.31263709068298 , diff:  1.8601899147033691
************ all values are small in this layer **********
layer  3  adv train finish, try to retain  255
test acc: top1 ->  53.32 ; top5 ->  77.012  and loss:  810.7812367081642
forward train acc: top1 ->  46.71875 ; top5 ->  71.14453125  and loss:  487.140274643898
test acc: top1 ->  52.068 ; top5 ->  76.068  and loss:  835.9072470664978
forward train acc: top1 ->  46.234375 ; top5 ->  70.4609375  and loss:  490.8159885406494
test acc: top1 ->  52.268 ; top5 ->  76.28  and loss:  830.4692703485489
forward train acc: top1 ->  46.14453125 ; top5 ->  70.09765625  and loss:  498.86007285118103
test acc: top1 ->  52.112 ; top5 ->  76.236  and loss:  834.3577529191971
forward train acc: top1 ->  46.93359375 ; top5 ->  70.828125  and loss:  489.9800568819046
test acc: top1 ->  52.756 ; top5 ->  76.522  and loss:  822.9878703355789
forward train acc: top1 ->  46.97265625 ; top5 ->  71.13671875  and loss:  485.7202662229538
test acc: top1 ->  52.832 ; top5 ->  76.684  and loss:  818.2498466968536
forward train acc: top1 ->  47.0234375 ; top5 ->  70.6953125  and loss:  489.32070195674896
test acc: top1 ->  52.948 ; top5 ->  76.744  and loss:  819.1500719189644
forward train acc: top1 ->  47.984375 ; top5 ->  71.53515625  and loss:  480.006374835968
test acc: top1 ->  53.208 ; top5 ->  76.8  and loss:  815.7653695344925
forward train acc: top1 ->  47.53515625 ; top5 ->  71.59375  and loss:  481.0235775709152
test acc: top1 ->  53.446 ; top5 ->  77.072  and loss:  811.2091988921165
forward train acc: top1 ->  47.77734375 ; top5 ->  71.16015625  and loss:  483.430695772171
test acc: top1 ->  53.37 ; top5 ->  76.92  and loss:  812.559231698513
forward train acc: top1 ->  48.36328125 ; top5 ->  72.03125  and loss:  476.2549458742142
test acc: top1 ->  53.368 ; top5 ->  77.082  and loss:  809.5290023684502
forward train acc: top1 ->  47.67578125 ; top5 ->  71.78125  and loss:  476.4545397758484
test acc: top1 ->  53.544 ; top5 ->  77.122  and loss:  807.2325055599213
forward train acc: top1 ->  48.109375 ; top5 ->  71.953125  and loss:  475.4562249183655
test acc: top1 ->  53.532 ; top5 ->  77.174  and loss:  807.8843120932579
forward train acc: top1 ->  48.0 ; top5 ->  72.1328125  and loss:  473.8481237888336
test acc: top1 ->  53.674 ; top5 ->  77.262  and loss:  805.5214301943779
forward train acc: top1 ->  47.91796875 ; top5 ->  71.78125  and loss:  478.8108537197113
test acc: top1 ->  53.664 ; top5 ->  77.33  and loss:  804.3413331508636
forward train acc: top1 ->  47.953125 ; top5 ->  71.8203125  and loss:  474.8205772638321
test acc: top1 ->  53.736 ; top5 ->  77.338  and loss:  803.8177733421326
forward train acc: top1 ->  48.7734375 ; top5 ->  72.40234375  and loss:  468.34380435943604
test acc: top1 ->  53.706 ; top5 ->  77.342  and loss:  803.1338784694672
forward train acc: top1 ->  47.96484375 ; top5 ->  72.1640625  and loss:  474.81499433517456
test acc: top1 ->  53.74 ; top5 ->  77.358  and loss:  803.4565111398697
forward train acc: top1 ->  48.140625 ; top5 ->  71.90234375  and loss:  475.31230103969574
test acc: top1 ->  53.722 ; top5 ->  77.354  and loss:  803.3847224712372
forward train acc: top1 ->  48.0625 ; top5 ->  71.71484375  and loss:  478.68638050556183
test acc: top1 ->  53.746 ; top5 ->  77.316  and loss:  803.0422123074532
forward train acc: top1 ->  48.90625 ; top5 ->  72.37890625  and loss:  469.11585891246796
test acc: top1 ->  53.8 ; top5 ->  77.356  and loss:  802.6261342763901
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -118.774689078331 , diff:  118.774689078331
adv train loss:  -117.79244649410248 , diff:  0.9822425842285156
************ all values are small in this layer **********
layer  4  adv train finish, try to retain  255
test acc: top1 ->  53.728 ; top5 ->  77.284  and loss:  803.9359748363495
forward train acc: top1 ->  46.71875 ; top5 ->  70.71875  and loss:  488.6688537597656
test acc: top1 ->  52.496 ; top5 ->  76.226  and loss:  828.4374713301659
forward train acc: top1 ->  46.515625 ; top5 ->  70.421875  and loss:  495.8272273540497
test acc: top1 ->  52.304 ; top5 ->  76.322  and loss:  831.4345021247864
forward train acc: top1 ->  45.6484375 ; top5 ->  69.9921875  and loss:  499.53269922733307
test acc: top1 ->  52.206 ; top5 ->  76.272  and loss:  830.0969367027283
forward train acc: top1 ->  46.75390625 ; top5 ->  70.35546875  and loss:  493.49342572689056
test acc: top1 ->  52.586 ; top5 ->  76.678  and loss:  824.9216011166573
forward train acc: top1 ->  47.09765625 ; top5 ->  70.88671875  and loss:  489.76824402809143
test acc: top1 ->  52.794 ; top5 ->  76.696  and loss:  820.9380091428757
forward train acc: top1 ->  47.328125 ; top5 ->  71.0  and loss:  483.946351647377
test acc: top1 ->  52.792 ; top5 ->  76.722  and loss:  821.0793985128403
forward train acc: top1 ->  47.58984375 ; top5 ->  71.5  and loss:  481.03563129901886
test acc: top1 ->  53.128 ; top5 ->  76.976  and loss:  813.9379033446312
forward train acc: top1 ->  48.05078125 ; top5 ->  71.5234375  and loss:  478.746723651886
test acc: top1 ->  53.268 ; top5 ->  76.956  and loss:  812.1367133259773
forward train acc: top1 ->  47.76171875 ; top5 ->  71.6640625  and loss:  475.6974515914917
test acc: top1 ->  53.276 ; top5 ->  76.992  and loss:  811.2511110901833
forward train acc: top1 ->  47.75 ; top5 ->  71.9140625  and loss:  477.27362298965454
test acc: top1 ->  53.502 ; top5 ->  77.06  and loss:  808.270035803318
forward train acc: top1 ->  48.10546875 ; top5 ->  71.671875  and loss:  476.3150380849838
test acc: top1 ->  53.446 ; top5 ->  77.196  and loss:  807.8084933161736
forward train acc: top1 ->  48.4375 ; top5 ->  71.7734375  and loss:  475.615575671196
test acc: top1 ->  53.434 ; top5 ->  77.206  and loss:  807.5267742276192
forward train acc: top1 ->  48.3515625 ; top5 ->  72.1171875  and loss:  475.5979219675064
test acc: top1 ->  53.628 ; top5 ->  77.268  and loss:  805.388904094696
forward train acc: top1 ->  48.04296875 ; top5 ->  71.515625  and loss:  480.0787218809128
test acc: top1 ->  53.524 ; top5 ->  77.282  and loss:  805.4870247244835
forward train acc: top1 ->  48.33203125 ; top5 ->  72.08984375  and loss:  477.6917027235031
test acc: top1 ->  53.688 ; top5 ->  77.324  and loss:  805.1863518953323
forward train acc: top1 ->  48.375 ; top5 ->  71.68359375  and loss:  473.36206698417664
test acc: top1 ->  53.62 ; top5 ->  77.318  and loss:  805.1309752464294
forward train acc: top1 ->  47.9375 ; top5 ->  72.10546875  and loss:  476.14308083057404
test acc: top1 ->  53.66 ; top5 ->  77.346  and loss:  804.9798213243484
forward train acc: top1 ->  47.69921875 ; top5 ->  71.7265625  and loss:  478.9723445177078
test acc: top1 ->  53.682 ; top5 ->  77.312  and loss:  805.137915790081
forward train acc: top1 ->  47.875 ; top5 ->  71.53125  and loss:  480.3878469467163
test acc: top1 ->  53.648 ; top5 ->  77.378  and loss:  804.9402834177017
forward train acc: top1 ->  48.7265625 ; top5 ->  72.3515625  and loss:  471.8737357854843
test acc: top1 ->  53.672 ; top5 ->  77.37  and loss:  804.5459768176079
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -118.64353227615356 , diff:  118.64353227615356
adv train loss:  -117.41125106811523 , diff:  1.23228120803833
************ all values are small in this layer **********
layer  5  adv train finish, try to retain  9215
test acc: top1 ->  53.654 ; top5 ->  77.284  and loss:  804.8255738019943
forward train acc: top1 ->  46.890625 ; top5 ->  70.86328125  and loss:  486.3962217569351
test acc: top1 ->  52.072 ; top5 ->  76.154  and loss:  835.9890545606613
forward train acc: top1 ->  46.00390625 ; top5 ->  70.15234375  and loss:  498.38948106765747
test acc: top1 ->  52.312 ; top5 ->  76.356  and loss:  831.2286475896835
forward train acc: top1 ->  46.12890625 ; top5 ->  69.6953125  and loss:  498.8232069015503
test acc: top1 ->  52.12 ; top5 ->  76.24  and loss:  836.7314394116402
forward train acc: top1 ->  46.953125 ; top5 ->  70.6875  and loss:  490.46029031276703
test acc: top1 ->  52.572 ; top5 ->  76.518  and loss:  826.244911134243
forward train acc: top1 ->  47.2734375 ; top5 ->  71.06640625  and loss:  486.6805499792099
test acc: top1 ->  53.014 ; top5 ->  76.814  and loss:  819.1408630013466
forward train acc: top1 ->  46.78125 ; top5 ->  70.578125  and loss:  491.1557763814926
test acc: top1 ->  52.962 ; top5 ->  76.734  and loss:  821.0414901971817
forward train acc: top1 ->  47.22265625 ; top5 ->  71.1640625  and loss:  485.82042384147644
test acc: top1 ->  53.258 ; top5 ->  76.87  and loss:  815.6690254211426
forward train acc: top1 ->  47.41015625 ; top5 ->  71.27734375  and loss:  481.14750492572784
test acc: top1 ->  53.218 ; top5 ->  76.946  and loss:  812.7400982379913
forward train acc: top1 ->  48.21484375 ; top5 ->  71.42578125  and loss:  479.17167377471924
test acc: top1 ->  53.254 ; top5 ->  76.984  and loss:  811.9270976781845
forward train acc: top1 ->  47.625 ; top5 ->  71.48828125  and loss:  479.2564789056778
test acc: top1 ->  53.498 ; top5 ->  77.104  and loss:  809.7450435161591
forward train acc: top1 ->  47.28125 ; top5 ->  71.734375  and loss:  484.4123295545578
test acc: top1 ->  53.46 ; top5 ->  77.042  and loss:  810.6651145815849
forward train acc: top1 ->  47.609375 ; top5 ->  71.36328125  and loss:  481.96555399894714
test acc: top1 ->  53.6 ; top5 ->  77.188  and loss:  807.8621493577957
forward train acc: top1 ->  48.1015625 ; top5 ->  71.71484375  and loss:  477.0913088321686
test acc: top1 ->  53.634 ; top5 ->  77.148  and loss:  806.4443782567978
forward train acc: top1 ->  48.12109375 ; top5 ->  71.77734375  and loss:  477.8089792728424
test acc: top1 ->  53.664 ; top5 ->  77.208  and loss:  806.3824966549873
forward train acc: top1 ->  47.9375 ; top5 ->  71.9765625  and loss:  480.1801961660385
test acc: top1 ->  53.626 ; top5 ->  77.234  and loss:  805.6355227828026
forward train acc: top1 ->  48.19921875 ; top5 ->  72.13671875  and loss:  475.9560388326645
test acc: top1 ->  53.776 ; top5 ->  77.21  and loss:  805.1140768527985
forward train acc: top1 ->  47.76953125 ; top5 ->  71.578125  and loss:  479.216268658638
test acc: top1 ->  53.714 ; top5 ->  77.162  and loss:  805.0471064448357
forward train acc: top1 ->  48.421875 ; top5 ->  72.3671875  and loss:  470.80294930934906
test acc: top1 ->  53.666 ; top5 ->  77.186  and loss:  805.1421167850494
forward train acc: top1 ->  48.4375 ; top5 ->  71.9765625  and loss:  476.0523121356964
test acc: top1 ->  53.74 ; top5 ->  77.206  and loss:  804.8960101008415
forward train acc: top1 ->  48.40234375 ; top5 ->  72.05078125  and loss:  470.47061121463776
test acc: top1 ->  53.748 ; top5 ->  77.202  and loss:  804.3948724865913
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  9216 / 9216 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -122.29331851005554 , diff:  122.29331851005554
adv train loss:  -115.94923150539398 , diff:  6.34408700466156
adv train loss:  -117.889857172966 , diff:  1.9406256675720215
************ all values are small in this layer **********
layer  6  adv train finish, try to retain  4095
test acc: top1 ->  53.74 ; top5 ->  77.214  and loss:  804.4056568145752
forward train acc: top1 ->  46.296875 ; top5 ->  70.453125  and loss:  495.29936814308167
test acc: top1 ->  52.548 ; top5 ->  76.39  and loss:  830.1341240406036
forward train acc: top1 ->  46.63671875 ; top5 ->  70.62109375  and loss:  491.33110094070435
test acc: top1 ->  52.386 ; top5 ->  76.418  and loss:  830.9135766029358
forward train acc: top1 ->  46.28515625 ; top5 ->  70.375  and loss:  495.655886888504
test acc: top1 ->  52.166 ; top5 ->  76.098  and loss:  836.5832081437111
forward train acc: top1 ->  46.6328125 ; top5 ->  70.49609375  and loss:  492.24998664855957
test acc: top1 ->  52.984 ; top5 ->  76.77  and loss:  820.1876890063286
forward train acc: top1 ->  46.8046875 ; top5 ->  70.5390625  and loss:  493.8434349298477
test acc: top1 ->  52.96 ; top5 ->  76.85  and loss:  818.2915612459183
forward train acc: top1 ->  46.69140625 ; top5 ->  70.4140625  and loss:  491.5327527523041
test acc: top1 ->  53.142 ; top5 ->  76.788  and loss:  819.1545297503471
forward train acc: top1 ->  47.47265625 ; top5 ->  71.3046875  and loss:  482.4879342317581
test acc: top1 ->  53.188 ; top5 ->  77.0  and loss:  815.2595660686493
forward train acc: top1 ->  47.4140625 ; top5 ->  71.15234375  and loss:  485.9881548881531
test acc: top1 ->  53.286 ; top5 ->  77.118  and loss:  811.6529462337494
forward train acc: top1 ->  47.39453125 ; top5 ->  71.53125  and loss:  479.5059026479721
test acc: top1 ->  53.398 ; top5 ->  77.124  and loss:  812.3259674906731
forward train acc: top1 ->  47.796875 ; top5 ->  71.98828125  and loss:  477.2315853834152
test acc: top1 ->  53.54 ; top5 ->  77.272  and loss:  809.3239932060242
forward train acc: top1 ->  47.69921875 ; top5 ->  71.8125  and loss:  478.4066822528839
test acc: top1 ->  53.634 ; top5 ->  77.242  and loss:  807.3520470261574
forward train acc: top1 ->  47.515625 ; top5 ->  71.58984375  and loss:  481.77128851413727
test acc: top1 ->  53.584 ; top5 ->  77.27  and loss:  807.4955766201019
forward train acc: top1 ->  48.234375 ; top5 ->  71.74609375  and loss:  475.28163051605225
test acc: top1 ->  53.716 ; top5 ->  77.314  and loss:  805.4714166522026
forward train acc: top1 ->  48.09375 ; top5 ->  71.59765625  and loss:  476.05388832092285
test acc: top1 ->  53.69 ; top5 ->  77.338  and loss:  805.6685671806335
forward train acc: top1 ->  48.0078125 ; top5 ->  71.62890625  and loss:  476.3911989927292
test acc: top1 ->  53.634 ; top5 ->  77.346  and loss:  805.1314367651939
forward train acc: top1 ->  48.4375 ; top5 ->  72.0859375  and loss:  474.8377411365509
test acc: top1 ->  53.624 ; top5 ->  77.304  and loss:  804.9099816083908
forward train acc: top1 ->  48.53515625 ; top5 ->  72.30859375  and loss:  472.5710052251816
test acc: top1 ->  53.634 ; top5 ->  77.368  and loss:  804.4081135988235
forward train acc: top1 ->  47.84765625 ; top5 ->  71.80078125  and loss:  477.87351191043854
test acc: top1 ->  53.768 ; top5 ->  77.414  and loss:  804.1751334667206
forward train acc: top1 ->  48.01953125 ; top5 ->  71.875  and loss:  478.67558443546295
test acc: top1 ->  53.746 ; top5 ->  77.4  and loss:  804.3171117305756
forward train acc: top1 ->  48.3828125 ; top5 ->  71.82421875  and loss:  476.99191534519196
test acc: top1 ->  53.724 ; top5 ->  77.406  and loss:  804.4803944826126
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  4096 / 4096 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -119.00333213806152 , diff:  119.00333213806152
adv train loss:  -117.39264059066772 , diff:  1.6106915473937988
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  4095
test acc: top1 ->  53.724 ; top5 ->  77.406  and loss:  804.4803944826126
forward train acc: top1 ->  46.22265625 ; top5 ->  70.55078125  and loss:  493.95286536216736
test acc: top1 ->  52.0 ; top5 ->  76.236  and loss:  834.5014008283615
forward train acc: top1 ->  46.95703125 ; top5 ->  70.75390625  and loss:  494.9336105585098
test acc: top1 ->  52.216 ; top5 ->  76.082  and loss:  836.3607118725777
forward train acc: top1 ->  45.96484375 ; top5 ->  69.90625  and loss:  500.692205786705
test acc: top1 ->  52.182 ; top5 ->  76.078  and loss:  836.0737095475197
forward train acc: top1 ->  47.04296875 ; top5 ->  71.12890625  and loss:  487.99360287189484
test acc: top1 ->  52.61 ; top5 ->  76.502  and loss:  826.904862344265
forward train acc: top1 ->  47.05078125 ; top5 ->  70.83984375  and loss:  490.6039856672287
test acc: top1 ->  52.746 ; top5 ->  76.464  and loss:  825.2130218744278
forward train acc: top1 ->  46.55859375 ; top5 ->  70.90625  and loss:  490.0765953063965
test acc: top1 ->  52.84 ; top5 ->  76.746  and loss:  821.991288125515
forward train acc: top1 ->  46.8828125 ; top5 ->  70.8125  and loss:  488.86280369758606
test acc: top1 ->  53.214 ; top5 ->  76.764  and loss:  815.4734885692596
forward train acc: top1 ->  47.46875 ; top5 ->  71.19921875  and loss:  485.64747858047485
test acc: top1 ->  53.272 ; top5 ->  76.946  and loss:  814.193828701973
forward train acc: top1 ->  47.29296875 ; top5 ->  71.01171875  and loss:  482.667542219162
test acc: top1 ->  53.35 ; top5 ->  77.028  and loss:  811.3620175719261
forward train acc: top1 ->  47.8046875 ; top5 ->  71.921875  and loss:  478.97853004932404
test acc: top1 ->  53.554 ; top5 ->  77.15  and loss:  808.3237975239754
forward train acc: top1 ->  48.18359375 ; top5 ->  71.2890625  and loss:  478.4130394458771
test acc: top1 ->  53.492 ; top5 ->  77.168  and loss:  808.2161080241203
forward train acc: top1 ->  47.2109375 ; top5 ->  71.66015625  and loss:  482.21861255168915
test acc: top1 ->  53.532 ; top5 ->  77.09  and loss:  809.045778632164
forward train acc: top1 ->  47.93359375 ; top5 ->  71.83984375  and loss:  476.71961581707
test acc: top1 ->  53.654 ; top5 ->  77.174  and loss:  806.3334137201309
forward train acc: top1 ->  48.13671875 ; top5 ->  71.80078125  and loss:  475.35504269599915
test acc: top1 ->  53.724 ; top5 ->  77.242  and loss:  805.9066981077194
forward train acc: top1 ->  48.21484375 ; top5 ->  71.7265625  and loss:  476.50550758838654
test acc: top1 ->  53.718 ; top5 ->  77.232  and loss:  805.3467876315117
forward train acc: top1 ->  48.4609375 ; top5 ->  71.6640625  and loss:  478.3164687156677
test acc: top1 ->  53.74 ; top5 ->  77.272  and loss:  804.8142622709274
forward train acc: top1 ->  48.33984375 ; top5 ->  71.93359375  and loss:  475.7136129140854
test acc: top1 ->  53.76 ; top5 ->  77.274  and loss:  804.7841268777847
forward train acc: top1 ->  48.328125 ; top5 ->  71.8671875  and loss:  477.0784201622009
test acc: top1 ->  53.766 ; top5 ->  77.266  and loss:  805.4926677942276
forward train acc: top1 ->  47.91796875 ; top5 ->  71.1171875  and loss:  478.56892716884613
test acc: top1 ->  53.78 ; top5 ->  77.288  and loss:  804.8890855908394
forward train acc: top1 ->  47.66015625 ; top5 ->  71.7109375  and loss:  479.25022280216217
test acc: top1 ->  53.736 ; top5 ->  77.252  and loss:  804.8702957630157
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  4096 / 4096 , inc:  1
layer  0  :  0.984375  ==>  63 / 64 , inc:  1
layer  1  :  0.9947916666666666  ==>  191 / 192 , inc:  1
layer  2  :  1.0  ==>  384 / 384 , inc:  1
layer  3  :  1.0  ==>  256 / 256 , inc:  1
layer  4  :  1.0  ==>  256 / 256 , inc:  1
layer  5  :  1.0  ==>  9216 / 9216 , inc:  1
layer  6  :  1.0  ==>  4096 / 4096 , inc:  1
layer  7  :  1.0  ==>  4096 / 4096 , inc:  1
eps [0.0023437500000000003, 0.00029296875000000004, 0.00010986328125000002, 0.000164794921875, 0.000164794921875, 4.577636718750001e-06, 1.02996826171875e-05, 1.02996826171875e-05]  wait [0, 2, 4, 4, 4, 4, 4, 4]  inc [1, 1, 1, 1, 1, 1, 1, 1]  tol: 1
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  3  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -128.90155291557312 , diff:  128.90155291557312
adv train loss:  -126.64068484306335 , diff:  2.2608680725097656
layer  0  adv train finish, try to retain  60
test acc: top1 ->  49.058 ; top5 ->  73.404  and loss:  894.7391695380211
forward train acc: top1 ->  45.44140625 ; top5 ->  69.58203125  and loss:  504.15602469444275
test acc: top1 ->  51.834 ; top5 ->  76.042  and loss:  839.562522649765
forward train acc: top1 ->  45.75 ; top5 ->  69.98828125  and loss:  502.1975029706955
test acc: top1 ->  51.33 ; top5 ->  75.914  and loss:  844.8659228682518
forward train acc: top1 ->  45.078125 ; top5 ->  69.3046875  and loss:  507.63029730319977
test acc: top1 ->  51.514 ; top5 ->  75.826  and loss:  846.1576774120331
forward train acc: top1 ->  46.40625 ; top5 ->  70.484375  and loss:  496.3130021095276
test acc: top1 ->  52.076 ; top5 ->  76.346  and loss:  834.7487297654152
forward train acc: top1 ->  46.390625 ; top5 ->  70.56640625  and loss:  492.14585185050964
test acc: top1 ->  52.212 ; top5 ->  76.344  and loss:  830.3599169254303
forward train acc: top1 ->  46.6796875 ; top5 ->  70.4375  and loss:  495.42102313041687
test acc: top1 ->  52.286 ; top5 ->  76.328  and loss:  831.1134630441666
forward train acc: top1 ->  46.23046875 ; top5 ->  70.64453125  and loss:  493.43323516845703
test acc: top1 ->  52.826 ; top5 ->  76.736  and loss:  821.3422840833664
forward train acc: top1 ->  46.84765625 ; top5 ->  70.078125  and loss:  496.9890842437744
test acc: top1 ->  52.868 ; top5 ->  76.674  and loss:  820.7200172543526
forward train acc: top1 ->  47.33984375 ; top5 ->  71.4609375  and loss:  484.81620025634766
test acc: top1 ->  52.824 ; top5 ->  76.782  and loss:  820.3153831362724
forward train acc: top1 ->  46.4765625 ; top5 ->  70.6484375  and loss:  492.39798521995544
test acc: top1 ->  53.122 ; top5 ->  76.852  and loss:  818.2704795598984
forward train acc: top1 ->  47.13671875 ; top5 ->  71.109375  and loss:  487.38003075122833
test acc: top1 ->  53.088 ; top5 ->  76.966  and loss:  816.919669508934
forward train acc: top1 ->  47.14453125 ; top5 ->  70.96875  and loss:  485.40959215164185
test acc: top1 ->  53.148 ; top5 ->  76.918  and loss:  815.7243217229843
forward train acc: top1 ->  47.55078125 ; top5 ->  71.37109375  and loss:  484.057839512825
test acc: top1 ->  53.168 ; top5 ->  76.95  and loss:  814.2403168678284
forward train acc: top1 ->  47.6953125 ; top5 ->  71.25390625  and loss:  485.1347301006317
test acc: top1 ->  53.168 ; top5 ->  76.988  and loss:  814.1498346328735
forward train acc: top1 ->  47.109375 ; top5 ->  71.06640625  and loss:  489.23962819576263
test acc: top1 ->  53.176 ; top5 ->  77.01  and loss:  813.8380696773529
forward train acc: top1 ->  47.8203125 ; top5 ->  71.49609375  and loss:  481.9370776414871
test acc: top1 ->  53.216 ; top5 ->  77.054  and loss:  812.9068261384964
forward train acc: top1 ->  47.01171875 ; top5 ->  71.40234375  and loss:  484.00507259368896
test acc: top1 ->  53.188 ; top5 ->  77.03  and loss:  813.1714553236961
forward train acc: top1 ->  47.3984375 ; top5 ->  71.0625  and loss:  488.7067903280258
test acc: top1 ->  53.218 ; top5 ->  77.052  and loss:  812.8779110908508
forward train acc: top1 ->  47.671875 ; top5 ->  71.4296875  and loss:  480.9754972457886
test acc: top1 ->  53.26 ; top5 ->  77.078  and loss:  812.7961156368256
forward train acc: top1 ->  46.83203125 ; top5 ->  71.26171875  and loss:  485.5951473712921
test acc: top1 ->  53.228 ; top5 ->  77.046  and loss:  812.2886606454849
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  63 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -134.00169944763184 , diff:  134.00169944763184
adv train loss:  -131.0835816860199 , diff:  2.9181177616119385
layer  1  adv train finish, try to retain  186
test acc: top1 ->  48.714 ; top5 ->  73.298  and loss:  904.5725781917572
forward train acc: top1 ->  45.83203125 ; top5 ->  70.3125  and loss:  500.9063036441803
test acc: top1 ->  52.008 ; top5 ->  76.024  and loss:  838.6300258040428
forward train acc: top1 ->  45.546875 ; top5 ->  70.1328125  and loss:  500.2383736371994
test acc: top1 ->  51.634 ; top5 ->  75.706  and loss:  845.5873392820358
forward train acc: top1 ->  46.078125 ; top5 ->  70.20703125  and loss:  497.62714552879333
test acc: top1 ->  51.672 ; top5 ->  75.936  and loss:  842.9096074104309
forward train acc: top1 ->  46.0390625 ; top5 ->  70.35546875  and loss:  497.5736391544342
test acc: top1 ->  52.314 ; top5 ->  76.298  and loss:  833.5060544610023
forward train acc: top1 ->  46.64453125 ; top5 ->  70.73046875  and loss:  489.9190250635147
test acc: top1 ->  52.598 ; top5 ->  76.484  and loss:  825.8615335822105
forward train acc: top1 ->  46.63671875 ; top5 ->  70.5  and loss:  492.0910174846649
test acc: top1 ->  52.628 ; top5 ->  76.548  and loss:  824.3879988193512
forward train acc: top1 ->  47.0390625 ; top5 ->  70.6484375  and loss:  491.9407078027725
test acc: top1 ->  52.762 ; top5 ->  76.794  and loss:  820.9543338418007
forward train acc: top1 ->  46.8203125 ; top5 ->  70.8515625  and loss:  491.8745733499527
test acc: top1 ->  52.81 ; top5 ->  76.744  and loss:  819.7267236113548
forward train acc: top1 ->  47.1328125 ; top5 ->  71.0625  and loss:  483.89739668369293
test acc: top1 ->  52.742 ; top5 ->  76.788  and loss:  820.8115549087524
forward train acc: top1 ->  47.40234375 ; top5 ->  71.43359375  and loss:  484.75694715976715
test acc: top1 ->  52.99 ; top5 ->  76.898  and loss:  817.3231273293495
forward train acc: top1 ->  47.6796875 ; top5 ->  70.921875  and loss:  486.0045349597931
test acc: top1 ->  53.048 ; top5 ->  76.874  and loss:  816.1366491317749
forward train acc: top1 ->  46.9375 ; top5 ->  70.375  and loss:  491.8695157766342
test acc: top1 ->  53.036 ; top5 ->  76.938  and loss:  816.6768447160721
forward train acc: top1 ->  47.37890625 ; top5 ->  71.8984375  and loss:  479.65559577941895
test acc: top1 ->  53.234 ; top5 ->  76.994  and loss:  813.824770450592
forward train acc: top1 ->  47.3515625 ; top5 ->  71.21484375  and loss:  484.62349784374237
test acc: top1 ->  53.262 ; top5 ->  77.018  and loss:  812.4801388382912
forward train acc: top1 ->  46.93359375 ; top5 ->  71.609375  and loss:  480.7702052593231
test acc: top1 ->  53.26 ; top5 ->  77.05  and loss:  812.2639144659042
forward train acc: top1 ->  47.64453125 ; top5 ->  71.5546875  and loss:  479.69476103782654
test acc: top1 ->  53.298 ; top5 ->  77.026  and loss:  811.9778644442558
forward train acc: top1 ->  48.015625 ; top5 ->  71.53515625  and loss:  480.33084893226624
test acc: top1 ->  53.308 ; top5 ->  77.082  and loss:  811.5486325621605
forward train acc: top1 ->  47.578125 ; top5 ->  71.1875  and loss:  482.80750811100006
test acc: top1 ->  53.27 ; top5 ->  77.088  and loss:  811.0875440835953
forward train acc: top1 ->  47.6484375 ; top5 ->  71.6171875  and loss:  482.0271599292755
test acc: top1 ->  53.33 ; top5 ->  77.082  and loss:  811.0957140922546
forward train acc: top1 ->  47.91015625 ; top5 ->  71.88671875  and loss:  476.42850136756897
test acc: top1 ->  53.34 ; top5 ->  77.102  and loss:  810.8687163591385
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  191 / 192 , inc:  1
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
layer  0  :  0.984375  ==>  63 / 64 , inc:  1
layer  1  :  0.9947916666666666  ==>  191 / 192 , inc:  1
layer  2  :  1.0  ==>  384 / 384 , inc:  1
layer  3  :  1.0  ==>  256 / 256 , inc:  1
layer  4  :  1.0  ==>  256 / 256 , inc:  1
layer  5  :  1.0  ==>  9216 / 9216 , inc:  1
layer  6  :  1.0  ==>  4096 / 4096 , inc:  1
layer  7  :  1.0  ==>  4096 / 4096 , inc:  1
eps [0.0017578125000000003, 0.00021972656250000003, 0.00010986328125000002, 0.000164794921875, 0.000164794921875, 4.577636718750001e-06, 1.02996826171875e-05, 1.02996826171875e-05]  wait [2, 4, 3, 3, 3, 3, 3, 3]  inc [1, 1, 1, 1, 1, 1, 1, 1]  tol: 1
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  4  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -127.33749842643738 , diff:  127.33749842643738
adv train loss:  -128.47035789489746 , diff:  1.132859468460083
layer  0  adv train finish, try to retain  61
test acc: top1 ->  49.438 ; top5 ->  73.816  and loss:  891.2241490483284
forward train acc: top1 ->  45.44140625 ; top5 ->  70.05859375  and loss:  504.56516444683075
test acc: top1 ->  52.19 ; top5 ->  76.192  and loss:  835.5071981549263
forward train acc: top1 ->  45.4921875 ; top5 ->  69.82421875  and loss:  504.5953950881958
test acc: top1 ->  51.538 ; top5 ->  75.72  and loss:  847.6003258228302
forward train acc: top1 ->  45.6171875 ; top5 ->  69.86328125  and loss:  501.71281003952026
test acc: top1 ->  51.52 ; top5 ->  75.762  and loss:  846.4191232919693
forward train acc: top1 ->  46.125 ; top5 ->  69.765625  and loss:  497.57538533210754
test acc: top1 ->  52.116 ; top5 ->  76.352  and loss:  833.9018468856812
forward train acc: top1 ->  46.18359375 ; top5 ->  70.38671875  and loss:  496.0964620113373
test acc: top1 ->  52.424 ; top5 ->  76.396  and loss:  829.7643889784813
forward train acc: top1 ->  46.953125 ; top5 ->  70.19921875  and loss:  492.92616951465607
test acc: top1 ->  52.326 ; top5 ->  76.486  and loss:  831.4988247156143
forward train acc: top1 ->  46.44921875 ; top5 ->  70.5390625  and loss:  491.70480465888977
test acc: top1 ->  52.776 ; top5 ->  76.796  and loss:  823.5363840460777
forward train acc: top1 ->  47.1171875 ; top5 ->  70.66796875  and loss:  490.49718046188354
test acc: top1 ->  52.946 ; top5 ->  76.85  and loss:  819.9567271471024
forward train acc: top1 ->  46.8515625 ; top5 ->  71.078125  and loss:  488.4098298549652
test acc: top1 ->  52.898 ; top5 ->  76.886  and loss:  820.1710525155067
forward train acc: top1 ->  46.921875 ; top5 ->  71.07421875  and loss:  486.1116647720337
test acc: top1 ->  52.978 ; top5 ->  76.842  and loss:  819.0990159511566
forward train acc: top1 ->  47.00390625 ; top5 ->  71.2265625  and loss:  486.0137711763382
test acc: top1 ->  53.14 ; top5 ->  76.94  and loss:  816.9026238918304
forward train acc: top1 ->  47.1875 ; top5 ->  71.31640625  and loss:  482.9373917579651
test acc: top1 ->  53.044 ; top5 ->  76.954  and loss:  816.5918588638306
forward train acc: top1 ->  47.44921875 ; top5 ->  71.3359375  and loss:  483.4052103757858
test acc: top1 ->  53.154 ; top5 ->  76.954  and loss:  815.7688845992088
forward train acc: top1 ->  47.53515625 ; top5 ->  70.9453125  and loss:  485.8308833837509
test acc: top1 ->  53.148 ; top5 ->  76.978  and loss:  815.5380699038506
forward train acc: top1 ->  47.51953125 ; top5 ->  71.4921875  and loss:  481.02260887622833
test acc: top1 ->  53.212 ; top5 ->  77.092  and loss:  814.5966330170631
forward train acc: top1 ->  48.01171875 ; top5 ->  71.91015625  and loss:  476.3391183614731
test acc: top1 ->  53.252 ; top5 ->  77.106  and loss:  814.0231322646141
forward train acc: top1 ->  47.08203125 ; top5 ->  71.43359375  and loss:  485.71410393714905
test acc: top1 ->  53.232 ; top5 ->  77.06  and loss:  813.5368607640266
forward train acc: top1 ->  47.1171875 ; top5 ->  71.50390625  and loss:  483.9311761856079
test acc: top1 ->  53.272 ; top5 ->  77.06  and loss:  813.3087692260742
forward train acc: top1 ->  47.66015625 ; top5 ->  71.47265625  and loss:  481.48864006996155
test acc: top1 ->  53.276 ; top5 ->  77.088  and loss:  813.0440974235535
forward train acc: top1 ->  47.09375 ; top5 ->  70.94921875  and loss:  487.9456400871277
test acc: top1 ->  53.268 ; top5 ->  77.114  and loss:  813.3458263874054
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  63 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -134.0733745098114 , diff:  134.0733745098114
adv train loss:  -130.24441003799438 , diff:  3.8289644718170166
adv train loss:  -129.01613879203796 , diff:  1.228271245956421
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  190
test acc: top1 ->  50.1 ; top5 ->  74.284  and loss:  876.741819024086
forward train acc: top1 ->  46.23046875 ; top5 ->  70.38671875  and loss:  497.72983038425446
test acc: top1 ->  52.394 ; top5 ->  76.148  and loss:  835.5573413968086
forward train acc: top1 ->  46.3125 ; top5 ->  70.48046875  and loss:  498.35962986946106
test acc: top1 ->  51.85 ; top5 ->  76.05  and loss:  842.6072573065758
forward train acc: top1 ->  45.47265625 ; top5 ->  69.5859375  and loss:  507.02053141593933
test acc: top1 ->  51.746 ; top5 ->  75.674  and loss:  850.0999621152878
forward train acc: top1 ->  45.859375 ; top5 ->  69.8125  and loss:  498.98600471019745
test acc: top1 ->  52.586 ; top5 ->  76.542  and loss:  829.7277470827103
forward train acc: top1 ->  46.16796875 ; top5 ->  70.24609375  and loss:  496.1899404525757
test acc: top1 ->  52.484 ; top5 ->  76.318  and loss:  831.7795426249504
forward train acc: top1 ->  47.08203125 ; top5 ->  70.71875  and loss:  492.9152055978775
test acc: top1 ->  52.562 ; top5 ->  76.622  and loss:  826.746316254139
forward train acc: top1 ->  47.0078125 ; top5 ->  71.02734375  and loss:  488.1551082134247
test acc: top1 ->  52.962 ; top5 ->  76.734  and loss:  820.6462215185165
forward train acc: top1 ->  47.05078125 ; top5 ->  70.98828125  and loss:  487.20032274723053
test acc: top1 ->  53.034 ; top5 ->  76.902  and loss:  818.7031232714653
forward train acc: top1 ->  46.3828125 ; top5 ->  70.96875  and loss:  490.0160001516342
test acc: top1 ->  53.04 ; top5 ->  76.834  and loss:  819.1366482377052
forward train acc: top1 ->  47.76953125 ; top5 ->  70.8828125  and loss:  484.87174010276794
test acc: top1 ->  53.214 ; top5 ->  76.978  and loss:  816.0325955748558
forward train acc: top1 ->  47.484375 ; top5 ->  71.40234375  and loss:  484.0716960430145
test acc: top1 ->  53.246 ; top5 ->  76.958  and loss:  815.0847024917603
forward train acc: top1 ->  47.10546875 ; top5 ->  70.8046875  and loss:  488.3490170240402
test acc: top1 ->  53.296 ; top5 ->  76.97  and loss:  815.1896650195122
forward train acc: top1 ->  47.8046875 ; top5 ->  71.62109375  and loss:  483.70417618751526
test acc: top1 ->  53.282 ; top5 ->  77.046  and loss:  814.0189300179482
forward train acc: top1 ->  47.33984375 ; top5 ->  71.1328125  and loss:  485.06245386600494
test acc: top1 ->  53.238 ; top5 ->  77.048  and loss:  813.8316841721535
forward train acc: top1 ->  47.203125 ; top5 ->  71.0078125  and loss:  484.87296080589294
test acc: top1 ->  53.298 ; top5 ->  77.148  and loss:  812.0694212913513
forward train acc: top1 ->  47.55078125 ; top5 ->  71.4765625  and loss:  482.20566761493683
test acc: top1 ->  53.372 ; top5 ->  77.198  and loss:  812.0652962327003
forward train acc: top1 ->  48.109375 ; top5 ->  71.69140625  and loss:  479.4258908033371
test acc: top1 ->  53.266 ; top5 ->  77.142  and loss:  811.9918787479401
forward train acc: top1 ->  47.93359375 ; top5 ->  71.83203125  and loss:  475.85178780555725
test acc: top1 ->  53.414 ; top5 ->  77.11  and loss:  811.3058820962906
forward train acc: top1 ->  47.984375 ; top5 ->  71.609375  and loss:  478.48650598526
test acc: top1 ->  53.46 ; top5 ->  77.166  and loss:  810.9797551035881
forward train acc: top1 ->  47.515625 ; top5 ->  71.4140625  and loss:  485.05267012119293
test acc: top1 ->  53.408 ; top5 ->  77.212  and loss:  811.2571029663086
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  191 / 192 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -121.17695164680481 , diff:  121.17695164680481
adv train loss:  -120.68914437294006 , diff:  0.4878072738647461
layer  2  adv train finish, try to retain  377
test acc: top1 ->  52.676 ; top5 ->  76.69  and loss:  822.2192381620407
forward train acc: top1 ->  45.78125 ; top5 ->  70.10546875  and loss:  501.2328402996063
test acc: top1 ->  51.832 ; top5 ->  76.022  and loss:  838.2037494778633
forward train acc: top1 ->  45.81640625 ; top5 ->  70.16015625  and loss:  498.16722321510315
test acc: top1 ->  51.904 ; top5 ->  75.992  and loss:  842.3249432444572
forward train acc: top1 ->  46.02734375 ; top5 ->  69.9140625  and loss:  502.8796603679657
test acc: top1 ->  51.948 ; top5 ->  75.956  and loss:  842.0564962029457
forward train acc: top1 ->  45.9140625 ; top5 ->  69.88671875  and loss:  499.8428772687912
test acc: top1 ->  52.452 ; top5 ->  76.534  and loss:  828.0221484303474
forward train acc: top1 ->  46.77734375 ; top5 ->  70.7109375  and loss:  490.9519671201706
test acc: top1 ->  52.374 ; top5 ->  76.404  and loss:  832.4982353448868
forward train acc: top1 ->  46.46484375 ; top5 ->  70.14453125  and loss:  496.43413507938385
test acc: top1 ->  52.812 ; top5 ->  76.634  and loss:  825.6726108193398
forward train acc: top1 ->  46.578125 ; top5 ->  70.80078125  and loss:  491.7709312438965
test acc: top1 ->  52.89 ; top5 ->  76.678  and loss:  822.4100899100304
forward train acc: top1 ->  46.4375 ; top5 ->  70.640625  and loss:  491.20992958545685
test acc: top1 ->  52.896 ; top5 ->  76.778  and loss:  820.0528763532639
forward train acc: top1 ->  46.50390625 ; top5 ->  70.5234375  and loss:  491.98867321014404
test acc: top1 ->  53.052 ; top5 ->  76.816  and loss:  820.5364472866058
forward train acc: top1 ->  46.7578125 ; top5 ->  71.00390625  and loss:  488.3838105201721
test acc: top1 ->  53.144 ; top5 ->  76.818  and loss:  817.7141721844673
forward train acc: top1 ->  47.13671875 ; top5 ->  70.7109375  and loss:  488.46256828308105
test acc: top1 ->  53.112 ; top5 ->  76.994  and loss:  816.1130727529526
forward train acc: top1 ->  47.50390625 ; top5 ->  71.36328125  and loss:  483.0679759979248
test acc: top1 ->  53.134 ; top5 ->  76.996  and loss:  815.614256799221
forward train acc: top1 ->  47.50390625 ; top5 ->  71.28515625  and loss:  483.83860635757446
test acc: top1 ->  53.152 ; top5 ->  77.008  and loss:  814.8974455595016
forward train acc: top1 ->  47.5390625 ; top5 ->  70.9140625  and loss:  483.6648209095001
test acc: top1 ->  53.24 ; top5 ->  77.074  and loss:  814.0348937511444
forward train acc: top1 ->  47.10546875 ; top5 ->  70.93359375  and loss:  490.3558312654495
test acc: top1 ->  53.272 ; top5 ->  76.976  and loss:  814.4039530754089
forward train acc: top1 ->  47.95703125 ; top5 ->  71.16015625  and loss:  485.27345645427704
test acc: top1 ->  53.242 ; top5 ->  76.998  and loss:  813.8966352343559
forward train acc: top1 ->  47.87109375 ; top5 ->  71.62890625  and loss:  480.9268833398819
test acc: top1 ->  53.354 ; top5 ->  76.998  and loss:  812.6777665615082
forward train acc: top1 ->  48.0 ; top5 ->  71.71484375  and loss:  479.31971979141235
test acc: top1 ->  53.334 ; top5 ->  77.036  and loss:  812.4637658596039
forward train acc: top1 ->  47.38671875 ; top5 ->  71.234375  and loss:  485.3043591976166
test acc: top1 ->  53.328 ; top5 ->  77.0  and loss:  812.5306179523468
forward train acc: top1 ->  47.03515625 ; top5 ->  71.13671875  and loss:  486.46409344673157
test acc: top1 ->  53.396 ; top5 ->  77.044  and loss:  812.3074797391891
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  384 / 384 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -121.81648707389832 , diff:  121.81648707389832
adv train loss:  -121.2247508764267 , diff:  0.5917361974716187
layer  3  adv train finish, try to retain  246
test acc: top1 ->  52.142 ; top5 ->  76.076  and loss:  833.8359404802322
forward train acc: top1 ->  45.55859375 ; top5 ->  69.63671875  and loss:  506.21548891067505
test acc: top1 ->  51.646 ; top5 ->  75.852  and loss:  847.7311903238297
forward train acc: top1 ->  45.5546875 ; top5 ->  69.72265625  and loss:  504.84115159511566
test acc: top1 ->  51.666 ; top5 ->  76.05  and loss:  846.5004658102989
forward train acc: top1 ->  44.9921875 ; top5 ->  69.484375  and loss:  505.79545307159424
test acc: top1 ->  51.5 ; top5 ->  75.792  and loss:  845.7541009187698
forward train acc: top1 ->  45.78515625 ; top5 ->  69.875  and loss:  502.6920530796051
test acc: top1 ->  52.222 ; top5 ->  76.372  and loss:  834.2867991924286
forward train acc: top1 ->  46.51953125 ; top5 ->  70.39453125  and loss:  496.21540570259094
test acc: top1 ->  52.114 ; top5 ->  76.438  and loss:  833.681823015213
forward train acc: top1 ->  45.796875 ; top5 ->  69.70703125  and loss:  504.81125378608704
test acc: top1 ->  52.476 ; top5 ->  76.678  and loss:  826.5874267816544
forward train acc: top1 ->  46.33984375 ; top5 ->  70.25  and loss:  492.9589205980301
test acc: top1 ->  52.55 ; top5 ->  76.774  and loss:  825.2395825982094
forward train acc: top1 ->  47.19921875 ; top5 ->  70.94140625  and loss:  490.57591462135315
test acc: top1 ->  52.662 ; top5 ->  76.798  and loss:  822.4946076273918
forward train acc: top1 ->  46.37109375 ; top5 ->  70.60546875  and loss:  492.00025033950806
test acc: top1 ->  52.718 ; top5 ->  76.886  and loss:  820.9747964143753
forward train acc: top1 ->  46.7890625 ; top5 ->  70.65234375  and loss:  491.7708532810211
test acc: top1 ->  52.726 ; top5 ->  76.876  and loss:  820.5572440624237
forward train acc: top1 ->  47.00390625 ; top5 ->  70.95703125  and loss:  490.82615530490875
test acc: top1 ->  52.724 ; top5 ->  76.946  and loss:  819.7153941392899
forward train acc: top1 ->  46.5703125 ; top5 ->  70.6328125  and loss:  493.25762581825256
test acc: top1 ->  52.89 ; top5 ->  76.91  and loss:  819.2975011467934
forward train acc: top1 ->  47.38671875 ; top5 ->  71.11328125  and loss:  486.26771211624146
test acc: top1 ->  53.01 ; top5 ->  77.018  and loss:  817.3510091900826
forward train acc: top1 ->  47.02734375 ; top5 ->  71.03125  and loss:  488.72018814086914
test acc: top1 ->  52.942 ; top5 ->  76.988  and loss:  817.9150171279907
forward train acc: top1 ->  47.77734375 ; top5 ->  71.2578125  and loss:  484.2442047595978
test acc: top1 ->  53.052 ; top5 ->  77.052  and loss:  816.797977745533
forward train acc: top1 ->  46.84375 ; top5 ->  70.6875  and loss:  489.4982603788376
test acc: top1 ->  53.12 ; top5 ->  77.086  and loss:  816.3755331635475
forward train acc: top1 ->  47.14453125 ; top5 ->  71.51171875  and loss:  483.0491273403168
test acc: top1 ->  53.148 ; top5 ->  77.046  and loss:  815.6829466819763
forward train acc: top1 ->  46.6875 ; top5 ->  71.125  and loss:  488.31045615673065
test acc: top1 ->  53.046 ; top5 ->  77.026  and loss:  815.9158407449722
forward train acc: top1 ->  47.02734375 ; top5 ->  71.109375  and loss:  487.4383063316345
test acc: top1 ->  53.07 ; top5 ->  77.01  and loss:  815.6633834838867
forward train acc: top1 ->  47.05078125 ; top5 ->  71.0625  and loss:  487.49278593063354
test acc: top1 ->  53.108 ; top5 ->  77.028  and loss:  815.458665728569
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -123.80487728118896 , diff:  123.80487728118896
adv train loss:  -120.1771068572998 , diff:  3.62777042388916
adv train loss:  -121.18986809253693 , diff:  1.0127612352371216
layer  4  adv train finish, try to retain  245
test acc: top1 ->  51.866 ; top5 ->  75.912  and loss:  846.2565853595734
forward train acc: top1 ->  45.80078125 ; top5 ->  70.203125  and loss:  497.89797270298004
test acc: top1 ->  51.644 ; top5 ->  76.026  and loss:  841.487447321415
forward train acc: top1 ->  44.91796875 ; top5 ->  69.21484375  and loss:  510.3407337665558
test acc: top1 ->  51.43 ; top5 ->  75.802  and loss:  849.6950420737267
forward train acc: top1 ->  45.17578125 ; top5 ->  69.79296875  and loss:  506.709401845932
test acc: top1 ->  51.52 ; top5 ->  75.844  and loss:  847.5327212810516
forward train acc: top1 ->  46.20703125 ; top5 ->  70.296875  and loss:  494.7581808567047
test acc: top1 ->  52.21 ; top5 ->  76.422  and loss:  832.0270137786865
forward train acc: top1 ->  46.48828125 ; top5 ->  70.4375  and loss:  495.04916417598724
test acc: top1 ->  52.268 ; top5 ->  76.412  and loss:  830.8692345619202
forward train acc: top1 ->  45.66015625 ; top5 ->  70.3515625  and loss:  498.6666556596756
test acc: top1 ->  52.286 ; top5 ->  76.45  and loss:  831.7194620370865
forward train acc: top1 ->  46.1171875 ; top5 ->  70.4609375  and loss:  495.612531542778
test acc: top1 ->  52.476 ; top5 ->  76.682  and loss:  825.368586242199
forward train acc: top1 ->  46.6796875 ; top5 ->  70.6484375  and loss:  492.6519113779068
test acc: top1 ->  52.588 ; top5 ->  76.628  and loss:  824.2404935359955
forward train acc: top1 ->  46.47265625 ; top5 ->  70.58203125  and loss:  493.93927562236786
test acc: top1 ->  52.714 ; top5 ->  76.754  and loss:  821.757352232933
forward train acc: top1 ->  46.859375 ; top5 ->  70.62890625  and loss:  490.6099407672882
test acc: top1 ->  52.832 ; top5 ->  76.838  and loss:  820.4909597635269
forward train acc: top1 ->  46.7109375 ; top5 ->  70.7109375  and loss:  489.56886887550354
test acc: top1 ->  52.906 ; top5 ->  76.88  and loss:  818.4976955652237
forward train acc: top1 ->  46.84765625 ; top5 ->  70.7109375  and loss:  488.41159212589264
test acc: top1 ->  52.948 ; top5 ->  76.946  and loss:  818.5128371119499
forward train acc: top1 ->  46.6328125 ; top5 ->  70.7109375  and loss:  490.6152768135071
test acc: top1 ->  53.032 ; top5 ->  77.012  and loss:  817.5653266906738
forward train acc: top1 ->  46.83203125 ; top5 ->  70.6796875  and loss:  492.2039645910263
test acc: top1 ->  52.956 ; top5 ->  76.964  and loss:  816.7252519726753
forward train acc: top1 ->  46.94140625 ; top5 ->  70.89453125  and loss:  488.4817171096802
test acc: top1 ->  53.048 ; top5 ->  76.974  and loss:  816.104851603508
forward train acc: top1 ->  47.16015625 ; top5 ->  71.19140625  and loss:  485.53815817832947
test acc: top1 ->  53.112 ; top5 ->  76.98  and loss:  815.1923265457153
forward train acc: top1 ->  46.91015625 ; top5 ->  71.06640625  and loss:  489.71355283260345
test acc: top1 ->  53.08 ; top5 ->  76.958  and loss:  815.3926547765732
forward train acc: top1 ->  47.20703125 ; top5 ->  70.8359375  and loss:  488.5270217657089
test acc: top1 ->  53.102 ; top5 ->  76.932  and loss:  815.4438979625702
forward train acc: top1 ->  47.36328125 ; top5 ->  71.00390625  and loss:  483.4541949033737
test acc: top1 ->  53.09 ; top5 ->  76.942  and loss:  815.0793393850327
forward train acc: top1 ->  47.93359375 ; top5 ->  71.3515625  and loss:  483.74269330501556
test acc: top1 ->  53.066 ; top5 ->  77.014  and loss:  814.392663538456
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -123.46694922447205 , diff:  123.46694922447205
adv train loss:  -122.89920663833618 , diff:  0.5677425861358643
layer  5  adv train finish, try to retain  9200
test acc: top1 ->  52.678 ; top5 ->  76.746  and loss:  823.2533614635468
forward train acc: top1 ->  45.51171875 ; top5 ->  69.66796875  and loss:  504.7006244659424
test acc: top1 ->  52.018 ; top5 ->  76.344  and loss:  835.3173899054527
forward train acc: top1 ->  45.609375 ; top5 ->  69.64453125  and loss:  506.7382197380066
test acc: top1 ->  51.904 ; top5 ->  75.99  and loss:  843.0230141282082
forward train acc: top1 ->  45.75 ; top5 ->  70.125  and loss:  503.91892075538635
test acc: top1 ->  51.886 ; top5 ->  76.014  and loss:  842.3505629897118
forward train acc: top1 ->  46.33203125 ; top5 ->  70.6328125  and loss:  497.7704300880432
test acc: top1 ->  52.53 ; top5 ->  76.494  and loss:  828.8765701651573
forward train acc: top1 ->  46.51953125 ; top5 ->  70.55859375  and loss:  491.85102474689484
test acc: top1 ->  52.272 ; top5 ->  76.358  and loss:  830.0437961220741
forward train acc: top1 ->  46.3984375 ; top5 ->  70.640625  and loss:  496.6157258749008
test acc: top1 ->  52.59 ; top5 ->  76.462  and loss:  831.0288400650024
forward train acc: top1 ->  46.859375 ; top5 ->  71.078125  and loss:  486.56775188446045
test acc: top1 ->  52.856 ; top5 ->  76.72  and loss:  822.6713263392448
forward train acc: top1 ->  47.15234375 ; top5 ->  70.92578125  and loss:  488.9582704305649
test acc: top1 ->  52.958 ; top5 ->  76.786  and loss:  822.0764114260674
forward train acc: top1 ->  47.1796875 ; top5 ->  70.8515625  and loss:  491.7524118423462
test acc: top1 ->  52.936 ; top5 ->  77.034  and loss:  819.2393604516983
forward train acc: top1 ->  47.0625 ; top5 ->  70.5859375  and loss:  490.84239864349365
test acc: top1 ->  53.124 ; top5 ->  76.982  and loss:  818.3945072293282
forward train acc: top1 ->  47.2734375 ; top5 ->  71.38671875  and loss:  481.5927176475525
test acc: top1 ->  53.174 ; top5 ->  77.016  and loss:  816.9807491898537
forward train acc: top1 ->  47.63671875 ; top5 ->  71.30078125  and loss:  484.65392994880676
test acc: top1 ->  53.142 ; top5 ->  77.1  and loss:  816.4753658771515
forward train acc: top1 ->  47.44921875 ; top5 ->  71.21875  and loss:  486.489026427269
test acc: top1 ->  53.282 ; top5 ->  77.094  and loss:  814.7982273101807
forward train acc: top1 ->  47.30078125 ; top5 ->  70.78515625  and loss:  489.69788777828217
test acc: top1 ->  53.19 ; top5 ->  77.056  and loss:  814.6166541576385
forward train acc: top1 ->  47.140625 ; top5 ->  70.9453125  and loss:  488.4692895412445
test acc: top1 ->  53.22 ; top5 ->  77.066  and loss:  814.5369099974632
forward train acc: top1 ->  47.4453125 ; top5 ->  71.17578125  and loss:  486.11158525943756
test acc: top1 ->  53.246 ; top5 ->  77.094  and loss:  813.8654495477676
forward train acc: top1 ->  47.62890625 ; top5 ->  71.46484375  and loss:  483.9297753572464
test acc: top1 ->  53.276 ; top5 ->  77.152  and loss:  813.0471336245537
forward train acc: top1 ->  47.453125 ; top5 ->  71.31640625  and loss:  487.84220147132874
test acc: top1 ->  53.272 ; top5 ->  77.128  and loss:  813.6648285388947
forward train acc: top1 ->  47.5546875 ; top5 ->  71.04296875  and loss:  486.6347464323044
test acc: top1 ->  53.274 ; top5 ->  77.12  and loss:  813.6087801456451
forward train acc: top1 ->  47.4765625 ; top5 ->  71.1875  and loss:  484.4679284095764
test acc: top1 ->  53.312 ; top5 ->  77.144  and loss:  813.1227601766586
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  9216 / 9216 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -119.74318671226501 , diff:  119.74318671226501
adv train loss:  -123.23893713951111 , diff:  3.4957504272460938
adv train loss:  -122.14549231529236 , diff:  1.09344482421875
layer  6  adv train finish, try to retain  4077
test acc: top1 ->  53.262 ; top5 ->  77.138  and loss:  813.1765834093094
forward train acc: top1 ->  45.671875 ; top5 ->  70.109375  and loss:  499.816002368927
test acc: top1 ->  51.562 ; top5 ->  75.942  and loss:  843.6141167283058
forward train acc: top1 ->  44.87890625 ; top5 ->  69.90625  and loss:  504.4310417175293
test acc: top1 ->  51.966 ; top5 ->  76.142  and loss:  835.7357401847839
forward train acc: top1 ->  45.11328125 ; top5 ->  69.33984375  and loss:  508.6076009273529
test acc: top1 ->  51.86 ; top5 ->  75.944  and loss:  841.6275812983513
forward train acc: top1 ->  46.48828125 ; top5 ->  70.609375  and loss:  492.55573761463165
test acc: top1 ->  52.246 ; top5 ->  76.45  and loss:  831.2002342939377
forward train acc: top1 ->  46.36328125 ; top5 ->  70.12109375  and loss:  497.88249802589417
test acc: top1 ->  52.608 ; top5 ->  76.56  and loss:  829.0483002066612
forward train acc: top1 ->  46.44921875 ; top5 ->  70.66796875  and loss:  492.4135354757309
test acc: top1 ->  52.592 ; top5 ->  76.558  and loss:  827.3974204659462
forward train acc: top1 ->  46.4140625 ; top5 ->  70.45703125  and loss:  493.8371684551239
test acc: top1 ->  52.788 ; top5 ->  76.816  and loss:  823.3452361226082
forward train acc: top1 ->  46.6015625 ; top5 ->  70.74609375  and loss:  493.988646030426
test acc: top1 ->  52.954 ; top5 ->  76.828  and loss:  821.2943522930145
forward train acc: top1 ->  46.4453125 ; top5 ->  70.7265625  and loss:  494.81132328510284
test acc: top1 ->  52.814 ; top5 ->  76.82  and loss:  821.946217238903
forward train acc: top1 ->  46.7109375 ; top5 ->  70.6953125  and loss:  494.01887106895447
test acc: top1 ->  52.976 ; top5 ->  76.954  and loss:  820.7832309007645
forward train acc: top1 ->  47.1328125 ; top5 ->  71.33203125  and loss:  484.0691522359848
test acc: top1 ->  53.06 ; top5 ->  77.006  and loss:  817.773610830307
forward train acc: top1 ->  47.1953125 ; top5 ->  71.29296875  and loss:  486.60775876045227
test acc: top1 ->  53.166 ; top5 ->  77.05  and loss:  816.5961654186249
forward train acc: top1 ->  47.46875 ; top5 ->  71.34375  and loss:  486.28147089481354
test acc: top1 ->  53.236 ; top5 ->  77.096  and loss:  815.4592269062996
forward train acc: top1 ->  47.01953125 ; top5 ->  71.2109375  and loss:  485.1016672849655
test acc: top1 ->  53.258 ; top5 ->  77.208  and loss:  814.2072233557701
forward train acc: top1 ->  47.60546875 ; top5 ->  71.1875  and loss:  486.37349927425385
test acc: top1 ->  53.226 ; top5 ->  77.17  and loss:  814.1646746993065
forward train acc: top1 ->  47.390625 ; top5 ->  71.3984375  and loss:  487.1482483148575
test acc: top1 ->  53.246 ; top5 ->  77.202  and loss:  813.7685875892639
forward train acc: top1 ->  46.546875 ; top5 ->  71.02734375  and loss:  488.6602636575699
test acc: top1 ->  53.244 ; top5 ->  77.148  and loss:  813.9284010529518
forward train acc: top1 ->  47.171875 ; top5 ->  71.4609375  and loss:  485.5287072658539
test acc: top1 ->  53.234 ; top5 ->  77.216  and loss:  813.3389366269112
forward train acc: top1 ->  47.2578125 ; top5 ->  71.15234375  and loss:  487.8174686431885
test acc: top1 ->  53.248 ; top5 ->  77.208  and loss:  813.011610686779
forward train acc: top1 ->  46.921875 ; top5 ->  71.40234375  and loss:  485.8711279630661
test acc: top1 ->  53.27 ; top5 ->  77.196  and loss:  812.7630186676979
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  4096 / 4096 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -120.47377276420593 , diff:  120.47377276420593
adv train loss:  -125.1640112400055 , diff:  4.6902384757995605
adv train loss:  -122.51651680469513 , diff:  2.6474944353103638
layer  7  adv train finish, try to retain  4008
test acc: top1 ->  53.256 ; top5 ->  77.162  and loss:  813.2801302671432
forward train acc: top1 ->  46.07421875 ; top5 ->  70.1796875  and loss:  500.5055023431778
test acc: top1 ->  51.878 ; top5 ->  76.044  and loss:  843.9377598762512
forward train acc: top1 ->  45.5 ; top5 ->  69.640625  and loss:  505.5361820459366
test acc: top1 ->  51.768 ; top5 ->  75.874  and loss:  844.2466822862625
forward train acc: top1 ->  45.22265625 ; top5 ->  69.3515625  and loss:  505.750123500824
test acc: top1 ->  51.882 ; top5 ->  76.07  and loss:  841.6130875945091
forward train acc: top1 ->  45.83984375 ; top5 ->  69.74609375  and loss:  502.60780918598175
test acc: top1 ->  52.352 ; top5 ->  76.406  and loss:  834.785228908062
forward train acc: top1 ->  45.97265625 ; top5 ->  70.15625  and loss:  500.0516986846924
test acc: top1 ->  52.4 ; top5 ->  76.53  and loss:  830.5553786754608
forward train acc: top1 ->  46.953125 ; top5 ->  70.828125  and loss:  492.2097393274307
test acc: top1 ->  52.454 ; top5 ->  76.676  and loss:  826.4505665898323
forward train acc: top1 ->  46.40234375 ; top5 ->  70.8203125  and loss:  494.06778156757355
test acc: top1 ->  52.636 ; top5 ->  76.72  and loss:  824.0367357730865
forward train acc: top1 ->  46.2890625 ; top5 ->  70.6953125  and loss:  493.8560208082199
test acc: top1 ->  52.846 ; top5 ->  76.818  and loss:  820.8829380273819
forward train acc: top1 ->  46.46875 ; top5 ->  70.578125  and loss:  492.66408038139343
test acc: top1 ->  52.9 ; top5 ->  76.844  and loss:  820.363478064537
forward train acc: top1 ->  46.796875 ; top5 ->  70.7890625  and loss:  491.9709132909775
test acc: top1 ->  52.91 ; top5 ->  76.94  and loss:  819.4009514451027
forward train acc: top1 ->  47.3046875 ; top5 ->  71.078125  and loss:  486.2733517885208
test acc: top1 ->  53.044 ; top5 ->  76.892  and loss:  818.0270361304283
forward train acc: top1 ->  47.28515625 ; top5 ->  70.9296875  and loss:  485.8305194377899
test acc: top1 ->  52.986 ; top5 ->  76.976  and loss:  817.2402105331421
forward train acc: top1 ->  47.46875 ; top5 ->  71.2265625  and loss:  487.51887488365173
test acc: top1 ->  53.042 ; top5 ->  77.0  and loss:  816.2565906643867
forward train acc: top1 ->  46.8203125 ; top5 ->  71.08203125  and loss:  485.82375943660736
test acc: top1 ->  53.178 ; top5 ->  77.048  and loss:  814.4544426202774
forward train acc: top1 ->  47.375 ; top5 ->  71.109375  and loss:  486.1814910173416
test acc: top1 ->  53.142 ; top5 ->  77.066  and loss:  814.4271612167358
forward train acc: top1 ->  47.671875 ; top5 ->  71.671875  and loss:  483.29651498794556
test acc: top1 ->  53.216 ; top5 ->  77.08  and loss:  813.4043659567833
forward train acc: top1 ->  47.61328125 ; top5 ->  71.296875  and loss:  484.1818801164627
test acc: top1 ->  53.216 ; top5 ->  77.088  and loss:  813.7697352170944
forward train acc: top1 ->  47.515625 ; top5 ->  71.3125  and loss:  483.1994061470032
test acc: top1 ->  53.218 ; top5 ->  77.09  and loss:  813.2065733671188
forward train acc: top1 ->  47.46875 ; top5 ->  70.88671875  and loss:  487.59637773036957
test acc: top1 ->  53.27 ; top5 ->  77.148  and loss:  813.0063267946243
forward train acc: top1 ->  47.28515625 ; top5 ->  71.0625  and loss:  487.1995266675949
test acc: top1 ->  53.294 ; top5 ->  77.132  and loss:  812.6188452243805
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  4096 / 4096 , inc:  1
layer  0  :  0.984375  ==>  63 / 64 , inc:  1
layer  1  :  0.9947916666666666  ==>  191 / 192 , inc:  1
layer  2  :  1.0  ==>  384 / 384 , inc:  1
layer  3  :  1.0  ==>  256 / 256 , inc:  1
layer  4  :  1.0  ==>  256 / 256 , inc:  1
layer  5  :  1.0  ==>  9216 / 9216 , inc:  1
layer  6  :  1.0  ==>  4096 / 4096 , inc:  1
layer  7  :  1.0  ==>  4096 / 4096 , inc:  1
eps [0.001318359375, 0.000164794921875, 8.23974609375e-05, 0.00012359619140625002, 0.00012359619140625002, 3.4332275390625005e-06, 7.724761962890626e-06, 7.724761962890626e-06]  wait [2, 4, 3, 3, 3, 3, 3, 3]  inc [1, 1, 1, 1, 1, 1, 1, 1]  tol: 2
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  5  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -126.47050023078918 , diff:  126.47050023078918
adv train loss:  -131.0905454158783 , diff:  4.620045185089111
adv train loss:  -127.66510534286499 , diff:  3.4254400730133057
adv train loss:  -128.43488883972168 , diff:  0.7697834968566895
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  62
test acc: top1 ->  49.958 ; top5 ->  74.344  and loss:  881.8544774651527
forward train acc: top1 ->  45.32421875 ; top5 ->  69.33984375  and loss:  509.81850492954254
test acc: top1 ->  51.868 ; top5 ->  76.212  and loss:  845.1897577643394
forward train acc: top1 ->  45.3828125 ; top5 ->  69.79296875  and loss:  504.86749625205994
test acc: top1 ->  51.432 ; top5 ->  75.794  and loss:  846.6058693528175
forward train acc: top1 ->  44.84765625 ; top5 ->  69.06640625  and loss:  512.5894355773926
test acc: top1 ->  51.702 ; top5 ->  76.012  and loss:  841.9722223877907
forward train acc: top1 ->  45.66015625 ; top5 ->  69.66015625  and loss:  503.94382333755493
test acc: top1 ->  52.242 ; top5 ->  76.544  and loss:  833.9219678640366
forward train acc: top1 ->  46.17578125 ; top5 ->  70.52734375  and loss:  495.6001422405243
test acc: top1 ->  52.24 ; top5 ->  76.52  and loss:  832.3045306801796
forward train acc: top1 ->  46.02734375 ; top5 ->  70.375  and loss:  497.64166736602783
test acc: top1 ->  52.302 ; top5 ->  76.6  and loss:  833.7032209038734
forward train acc: top1 ->  45.7265625 ; top5 ->  69.90234375  and loss:  500.8401223421097
test acc: top1 ->  52.686 ; top5 ->  76.71  and loss:  825.2659820318222
forward train acc: top1 ->  46.6484375 ; top5 ->  70.5078125  and loss:  494.1313613653183
test acc: top1 ->  52.614 ; top5 ->  76.744  and loss:  824.8580566644669
forward train acc: top1 ->  46.51953125 ; top5 ->  70.421875  and loss:  495.28335893154144
test acc: top1 ->  52.788 ; top5 ->  76.782  and loss:  822.8990936279297
forward train acc: top1 ->  46.6953125 ; top5 ->  70.20703125  and loss:  497.44003689289093
test acc: top1 ->  52.908 ; top5 ->  76.93  and loss:  822.005878508091
forward train acc: top1 ->  46.546875 ; top5 ->  70.6875  and loss:  493.991681098938
test acc: top1 ->  53.008 ; top5 ->  76.958  and loss:  819.7720222473145
forward train acc: top1 ->  46.4375 ; top5 ->  70.9609375  and loss:  491.09251832962036
test acc: top1 ->  53.042 ; top5 ->  77.042  and loss:  817.7225521802902
forward train acc: top1 ->  46.75 ; top5 ->  70.609375  and loss:  492.9785543680191
test acc: top1 ->  53.076 ; top5 ->  77.062  and loss:  817.816878259182
forward train acc: top1 ->  47.1953125 ; top5 ->  71.0625  and loss:  487.9803012609482
test acc: top1 ->  53.086 ; top5 ->  77.066  and loss:  817.1566303372383
forward train acc: top1 ->  46.66015625 ; top5 ->  70.83984375  and loss:  491.00518107414246
test acc: top1 ->  53.146 ; top5 ->  77.15  and loss:  816.3005783557892
forward train acc: top1 ->  47.16015625 ; top5 ->  70.95703125  and loss:  488.92085349559784
test acc: top1 ->  53.056 ; top5 ->  77.184  and loss:  815.9914017915726
forward train acc: top1 ->  47.66015625 ; top5 ->  71.31640625  and loss:  483.8461409807205
test acc: top1 ->  53.094 ; top5 ->  77.15  and loss:  815.6360022425652
forward train acc: top1 ->  47.3046875 ; top5 ->  71.015625  and loss:  488.4703598022461
test acc: top1 ->  53.142 ; top5 ->  77.158  and loss:  815.2600711584091
forward train acc: top1 ->  46.83984375 ; top5 ->  71.04296875  and loss:  489.4311498403549
test acc: top1 ->  53.162 ; top5 ->  77.138  and loss:  815.2059642076492
forward train acc: top1 ->  47.1484375 ; top5 ->  71.1171875  and loss:  489.14199018478394
test acc: top1 ->  53.136 ; top5 ->  77.168  and loss:  815.4577897787094
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  63 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -131.54613065719604 , diff:  131.54613065719604
adv train loss:  -130.86891174316406 , diff:  0.6772189140319824
layer  1  adv train finish, try to retain  189
test acc: top1 ->  49.916 ; top5 ->  74.12  and loss:  887.4843983054161
forward train acc: top1 ->  46.2734375 ; top5 ->  70.015625  and loss:  500.533975481987
test acc: top1 ->  51.554 ; top5 ->  76.058  and loss:  845.6667551398277
forward train acc: top1 ->  45.23828125 ; top5 ->  69.37890625  and loss:  506.56413304805756
test acc: top1 ->  51.618 ; top5 ->  76.02  and loss:  846.4400250315666
forward train acc: top1 ->  45.8671875 ; top5 ->  69.87109375  and loss:  499.7320841550827
test acc: top1 ->  51.566 ; top5 ->  75.678  and loss:  849.2922606468201
forward train acc: top1 ->  45.5546875 ; top5 ->  69.3046875  and loss:  505.16767501831055
test acc: top1 ->  52.226 ; top5 ->  76.442  and loss:  833.0352874398232
forward train acc: top1 ->  45.89453125 ; top5 ->  69.79296875  and loss:  502.0917057991028
test acc: top1 ->  52.298 ; top5 ->  76.35  and loss:  832.3481348156929
forward train acc: top1 ->  45.91796875 ; top5 ->  69.875  and loss:  502.2665432691574
test acc: top1 ->  52.568 ; top5 ->  76.462  and loss:  832.9416100382805
forward train acc: top1 ->  46.20703125 ; top5 ->  70.13671875  and loss:  495.1225392818451
test acc: top1 ->  52.776 ; top5 ->  76.838  and loss:  822.8306858539581
forward train acc: top1 ->  46.69921875 ; top5 ->  70.24609375  and loss:  493.388391494751
test acc: top1 ->  52.904 ; top5 ->  76.666  and loss:  823.3411244750023
forward train acc: top1 ->  47.0546875 ; top5 ->  70.58984375  and loss:  490.65165400505066
test acc: top1 ->  52.802 ; top5 ->  76.816  and loss:  821.2500395178795
forward train acc: top1 ->  47.0234375 ; top5 ->  71.24609375  and loss:  485.64521861076355
test acc: top1 ->  52.924 ; top5 ->  76.88  and loss:  818.6156215071678
forward train acc: top1 ->  47.046875 ; top5 ->  70.94921875  and loss:  487.11407017707825
test acc: top1 ->  52.968 ; top5 ->  76.892  and loss:  818.4634109139442
forward train acc: top1 ->  47.07421875 ; top5 ->  71.1015625  and loss:  487.64894115924835
test acc: top1 ->  53.104 ; top5 ->  76.908  and loss:  817.4433335661888
forward train acc: top1 ->  47.31640625 ; top5 ->  70.984375  and loss:  488.4929430484772
test acc: top1 ->  53.138 ; top5 ->  76.94  and loss:  816.5034728050232
forward train acc: top1 ->  47.1015625 ; top5 ->  71.296875  and loss:  484.3098202943802
test acc: top1 ->  53.134 ; top5 ->  76.988  and loss:  816.0203191637993
forward train acc: top1 ->  47.19140625 ; top5 ->  70.9453125  and loss:  491.1541875600815
test acc: top1 ->  53.1 ; top5 ->  76.95  and loss:  817.1439390182495
forward train acc: top1 ->  46.82421875 ; top5 ->  71.01171875  and loss:  485.56391751766205
test acc: top1 ->  53.2 ; top5 ->  76.974  and loss:  815.7503113150597
forward train acc: top1 ->  46.7734375 ; top5 ->  70.921875  and loss:  488.26320946216583
test acc: top1 ->  53.22 ; top5 ->  77.002  and loss:  814.9501606225967
forward train acc: top1 ->  47.28515625 ; top5 ->  71.08203125  and loss:  485.54161977767944
test acc: top1 ->  53.192 ; top5 ->  77.004  and loss:  814.7340970039368
forward train acc: top1 ->  47.21484375 ; top5 ->  70.89453125  and loss:  493.021476149559
test acc: top1 ->  53.218 ; top5 ->  77.032  and loss:  815.0365138053894
forward train acc: top1 ->  47.91015625 ; top5 ->  71.5703125  and loss:  482.5713701248169
test acc: top1 ->  53.26 ; top5 ->  77.03  and loss:  814.5315422415733
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  191 / 192 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -123.9695326089859 , diff:  123.9695326089859
adv train loss:  -121.27206516265869 , diff:  2.6974674463272095
************ all values are small in this layer **********
layer  2  adv train finish, try to retain  383
test acc: top1 ->  52.99 ; top5 ->  76.948  and loss:  818.3016702532768
forward train acc: top1 ->  46.21875 ; top5 ->  70.1484375  and loss:  498.5682338476181
test acc: top1 ->  51.838 ; top5 ->  76.036  and loss:  842.1512802243233
forward train acc: top1 ->  45.4140625 ; top5 ->  69.3671875  and loss:  507.74238300323486
test acc: top1 ->  51.872 ; top5 ->  75.912  and loss:  843.9790187478065
forward train acc: top1 ->  45.0703125 ; top5 ->  69.3359375  and loss:  505.99214804172516
test acc: top1 ->  51.608 ; top5 ->  75.838  and loss:  848.5550847053528
forward train acc: top1 ->  45.1796875 ; top5 ->  69.56640625  and loss:  505.5956838130951
test acc: top1 ->  52.354 ; top5 ->  76.384  and loss:  834.2723577022552
forward train acc: top1 ->  45.76171875 ; top5 ->  70.22265625  and loss:  496.57718884944916
test acc: top1 ->  52.426 ; top5 ->  76.56  and loss:  831.2516007423401
forward train acc: top1 ->  46.3125 ; top5 ->  70.1484375  and loss:  495.9151164293289
test acc: top1 ->  52.402 ; top5 ->  76.562  and loss:  827.9046471118927
forward train acc: top1 ->  46.328125 ; top5 ->  70.74609375  and loss:  494.8003908395767
test acc: top1 ->  52.774 ; top5 ->  76.622  and loss:  825.1861298084259
forward train acc: top1 ->  46.15625 ; top5 ->  70.31640625  and loss:  495.6558948755264
test acc: top1 ->  52.824 ; top5 ->  76.706  and loss:  823.0682087540627
forward train acc: top1 ->  46.80078125 ; top5 ->  70.77734375  and loss:  491.6109986305237
test acc: top1 ->  52.828 ; top5 ->  76.752  and loss:  822.9754646420479
forward train acc: top1 ->  47.0859375 ; top5 ->  70.94921875  and loss:  488.02388775348663
test acc: top1 ->  52.976 ; top5 ->  76.856  and loss:  819.4796838760376
forward train acc: top1 ->  47.15234375 ; top5 ->  70.8359375  and loss:  488.7186290025711
test acc: top1 ->  53.096 ; top5 ->  76.904  and loss:  817.4574929475784
forward train acc: top1 ->  46.95703125 ; top5 ->  70.61328125  and loss:  491.7257912158966
test acc: top1 ->  53.092 ; top5 ->  76.894  and loss:  817.4266920089722
forward train acc: top1 ->  46.35546875 ; top5 ->  70.5  and loss:  489.2766942977905
test acc: top1 ->  53.124 ; top5 ->  76.934  and loss:  815.4566463828087
forward train acc: top1 ->  47.22265625 ; top5 ->  70.8515625  and loss:  488.39679729938507
test acc: top1 ->  53.242 ; top5 ->  76.95  and loss:  814.6078404784203
forward train acc: top1 ->  47.328125 ; top5 ->  71.21484375  and loss:  488.03682076931
test acc: top1 ->  53.27 ; top5 ->  76.928  and loss:  815.4014804363251
forward train acc: top1 ->  46.78125 ; top5 ->  70.7734375  and loss:  490.03356647491455
test acc: top1 ->  53.306 ; top5 ->  76.996  and loss:  814.2309545874596
forward train acc: top1 ->  46.69921875 ; top5 ->  70.65234375  and loss:  490.0224827528
test acc: top1 ->  53.272 ; top5 ->  76.99  and loss:  814.3061670660973
forward train acc: top1 ->  47.26953125 ; top5 ->  71.1015625  and loss:  486.0807993412018
test acc: top1 ->  53.256 ; top5 ->  76.932  and loss:  814.2202659845352
forward train acc: top1 ->  47.4296875 ; top5 ->  70.94140625  and loss:  488.6881672143936
test acc: top1 ->  53.322 ; top5 ->  76.966  and loss:  814.2110012769699
forward train acc: top1 ->  47.375 ; top5 ->  71.61328125  and loss:  479.6326652765274
test acc: top1 ->  53.34 ; top5 ->  76.958  and loss:  813.4734563231468
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  384 / 384 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -123.17914819717407 , diff:  123.17914819717407
adv train loss:  -120.58869624137878 , diff:  2.590451955795288
layer  3  adv train finish, try to retain  247
test acc: top1 ->  52.37 ; top5 ->  76.19  and loss:  833.7338147759438
forward train acc: top1 ->  45.20703125 ; top5 ->  69.10546875  and loss:  509.1329118013382
test acc: top1 ->  51.692 ; top5 ->  75.9  and loss:  849.2896219491959
forward train acc: top1 ->  45.078125 ; top5 ->  69.0  and loss:  512.434844493866
test acc: top1 ->  51.696 ; top5 ->  75.852  and loss:  850.4655762314796
forward train acc: top1 ->  44.90625 ; top5 ->  69.0859375  and loss:  513.7940773963928
test acc: top1 ->  51.73 ; top5 ->  75.868  and loss:  848.834966301918
forward train acc: top1 ->  45.296875 ; top5 ->  69.91796875  and loss:  503.84653997421265
test acc: top1 ->  51.992 ; top5 ->  76.202  and loss:  838.5903654098511
forward train acc: top1 ->  46.2265625 ; top5 ->  70.05078125  and loss:  498.633726477623
test acc: top1 ->  52.24 ; top5 ->  76.278  and loss:  837.7770036458969
forward train acc: top1 ->  46.03515625 ; top5 ->  69.65625  and loss:  503.7885831594467
test acc: top1 ->  52.326 ; top5 ->  76.374  and loss:  833.8865277171135
forward train acc: top1 ->  46.7421875 ; top5 ->  70.6015625  and loss:  495.37558031082153
test acc: top1 ->  52.688 ; top5 ->  76.572  and loss:  829.3032977581024
forward train acc: top1 ->  46.171875 ; top5 ->  70.16015625  and loss:  497.03008484840393
test acc: top1 ->  52.708 ; top5 ->  76.59  and loss:  826.9097471237183
forward train acc: top1 ->  46.48046875 ; top5 ->  70.5390625  and loss:  494.8052730560303
test acc: top1 ->  52.724 ; top5 ->  76.694  and loss:  825.0804044008255
forward train acc: top1 ->  46.91796875 ; top5 ->  70.765625  and loss:  490.3055227994919
test acc: top1 ->  52.934 ; top5 ->  76.738  and loss:  822.9021255373955
forward train acc: top1 ->  46.1796875 ; top5 ->  70.96875  and loss:  490.9513964653015
test acc: top1 ->  52.878 ; top5 ->  76.776  and loss:  823.3655323982239
forward train acc: top1 ->  46.3359375 ; top5 ->  70.25390625  and loss:  497.27263355255127
test acc: top1 ->  53.006 ; top5 ->  76.898  and loss:  821.1715431809425
forward train acc: top1 ->  47.19921875 ; top5 ->  71.0546875  and loss:  488.5964809656143
test acc: top1 ->  52.968 ; top5 ->  76.876  and loss:  820.7676045894623
forward train acc: top1 ->  46.73828125 ; top5 ->  70.734375  and loss:  492.7443017959595
test acc: top1 ->  53.048 ; top5 ->  76.896  and loss:  819.5253957509995
forward train acc: top1 ->  46.65625 ; top5 ->  70.828125  and loss:  491.3781065940857
test acc: top1 ->  53.114 ; top5 ->  76.946  and loss:  819.4760983586311
forward train acc: top1 ->  46.78515625 ; top5 ->  70.76953125  and loss:  489.4038909673691
test acc: top1 ->  53.136 ; top5 ->  76.926  and loss:  818.9127527475357
forward train acc: top1 ->  46.67578125 ; top5 ->  70.5859375  and loss:  491.63191056251526
test acc: top1 ->  53.086 ; top5 ->  76.936  and loss:  819.2152320146561
forward train acc: top1 ->  47.08203125 ; top5 ->  70.83203125  and loss:  489.21719086170197
test acc: top1 ->  53.104 ; top5 ->  76.914  and loss:  818.3422898054123
forward train acc: top1 ->  47.1328125 ; top5 ->  70.98046875  and loss:  484.89546167850494
test acc: top1 ->  53.078 ; top5 ->  76.966  and loss:  817.7794793248177
forward train acc: top1 ->  46.57421875 ; top5 ->  70.54296875  and loss:  495.8312557935715
test acc: top1 ->  53.14 ; top5 ->  76.97  and loss:  817.6363016366959
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -123.98336982727051 , diff:  123.98336982727051
adv train loss:  -123.94416451454163 , diff:  0.039205312728881836
layer  4  adv train finish, try to retain  252
test acc: top1 ->  51.648 ; top5 ->  76.15  and loss:  839.5415706634521
forward train acc: top1 ->  45.44921875 ; top5 ->  69.62890625  and loss:  504.6366593837738
test acc: top1 ->  51.568 ; top5 ->  76.102  and loss:  842.4807354211807
forward train acc: top1 ->  44.96484375 ; top5 ->  68.99609375  and loss:  511.28558921813965
test acc: top1 ->  51.66 ; top5 ->  75.884  and loss:  844.5972630977631
forward train acc: top1 ->  45.43359375 ; top5 ->  69.30078125  and loss:  509.37813663482666
test acc: top1 ->  51.476 ; top5 ->  75.818  and loss:  847.1302253603935
forward train acc: top1 ->  45.703125 ; top5 ->  70.1328125  and loss:  500.9484124183655
test acc: top1 ->  51.868 ; top5 ->  76.272  and loss:  835.3068760037422
forward train acc: top1 ->  45.76953125 ; top5 ->  69.80859375  and loss:  502.72624254226685
test acc: top1 ->  52.212 ; top5 ->  76.33  and loss:  833.1429907083511
forward train acc: top1 ->  45.82421875 ; top5 ->  70.48828125  and loss:  499.2141749858856
test acc: top1 ->  52.328 ; top5 ->  76.464  and loss:  831.8486488461494
forward train acc: top1 ->  46.13671875 ; top5 ->  70.23046875  and loss:  496.32695829868317
test acc: top1 ->  52.346 ; top5 ->  76.6  and loss:  829.9133484363556
forward train acc: top1 ->  46.44140625 ; top5 ->  70.4296875  and loss:  491.5957270860672
test acc: top1 ->  52.5 ; top5 ->  76.602  and loss:  826.2427095770836
forward train acc: top1 ->  45.953125 ; top5 ->  70.14453125  and loss:  499.0291483402252
test acc: top1 ->  52.704 ; top5 ->  76.818  and loss:  825.43981975317
forward train acc: top1 ->  46.32421875 ; top5 ->  70.05078125  and loss:  499.1145086288452
test acc: top1 ->  52.85 ; top5 ->  76.834  and loss:  821.6292718052864
forward train acc: top1 ->  46.6953125 ; top5 ->  70.39453125  and loss:  495.3849858045578
test acc: top1 ->  52.734 ; top5 ->  76.814  and loss:  821.2582726478577
forward train acc: top1 ->  46.59375 ; top5 ->  71.10546875  and loss:  491.2552421092987
test acc: top1 ->  52.85 ; top5 ->  76.968  and loss:  819.3282188177109
forward train acc: top1 ->  46.4296875 ; top5 ->  70.88671875  and loss:  490.6535037755966
test acc: top1 ->  52.844 ; top5 ->  76.972  and loss:  818.9304673671722
forward train acc: top1 ->  46.44921875 ; top5 ->  70.3046875  and loss:  493.18535351753235
test acc: top1 ->  52.934 ; top5 ->  76.962  and loss:  817.7923675775528
forward train acc: top1 ->  47.75390625 ; top5 ->  71.2890625  and loss:  483.84175419807434
test acc: top1 ->  52.912 ; top5 ->  76.996  and loss:  817.4621430635452
forward train acc: top1 ->  46.671875 ; top5 ->  70.71875  and loss:  491.6381628513336
test acc: top1 ->  52.93 ; top5 ->  77.05  and loss:  816.9021342396736
forward train acc: top1 ->  46.29296875 ; top5 ->  70.49609375  and loss:  493.3895580768585
test acc: top1 ->  52.914 ; top5 ->  77.036  and loss:  817.4315744638443
forward train acc: top1 ->  46.3515625 ; top5 ->  70.5625  and loss:  496.262088060379
test acc: top1 ->  53.004 ; top5 ->  76.956  and loss:  817.5338082313538
forward train acc: top1 ->  46.59375 ; top5 ->  70.96875  and loss:  490.73762333393097
test acc: top1 ->  53.02 ; top5 ->  77.002  and loss:  817.1326508522034
forward train acc: top1 ->  47.16015625 ; top5 ->  71.14453125  and loss:  488.3954768180847
test acc: top1 ->  53.022 ; top5 ->  77.018  and loss:  816.8144852519035
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -123.6912168264389 , diff:  123.6912168264389
adv train loss:  -124.32588791847229 , diff:  0.6346710920333862
layer  5  adv train finish, try to retain  9202
test acc: top1 ->  52.746 ; top5 ->  76.74  and loss:  823.0443655252457
forward train acc: top1 ->  45.79296875 ; top5 ->  69.6796875  and loss:  504.50936114788055
test acc: top1 ->  51.76 ; top5 ->  75.894  and loss:  847.667424082756
forward train acc: top1 ->  44.83984375 ; top5 ->  69.18359375  and loss:  511.0317416191101
test acc: top1 ->  51.566 ; top5 ->  75.858  and loss:  849.1487397551537
forward train acc: top1 ->  45.25390625 ; top5 ->  69.26953125  and loss:  510.31462478637695
test acc: top1 ->  51.454 ; top5 ->  75.63  and loss:  851.6402307748795
forward train acc: top1 ->  45.28515625 ; top5 ->  69.58984375  and loss:  504.84470534324646
test acc: top1 ->  52.166 ; top5 ->  76.236  and loss:  834.4674605727196
forward train acc: top1 ->  45.5390625 ; top5 ->  69.84765625  and loss:  500.91111624240875
test acc: top1 ->  52.324 ; top5 ->  76.32  and loss:  834.1693248152733
forward train acc: top1 ->  45.28515625 ; top5 ->  69.32421875  and loss:  504.2317079305649
test acc: top1 ->  52.182 ; top5 ->  76.206  and loss:  837.0135273933411
forward train acc: top1 ->  46.13671875 ; top5 ->  70.1953125  and loss:  499.12008118629456
test acc: top1 ->  52.462 ; top5 ->  76.594  and loss:  828.8822691440582
forward train acc: top1 ->  46.28125 ; top5 ->  70.3046875  and loss:  496.75038027763367
test acc: top1 ->  52.564 ; top5 ->  76.624  and loss:  825.1441762447357
forward train acc: top1 ->  46.625 ; top5 ->  70.5  and loss:  494.25861608982086
test acc: top1 ->  52.742 ; top5 ->  76.706  and loss:  824.1009914278984
forward train acc: top1 ->  46.3046875 ; top5 ->  70.1796875  and loss:  495.26650536060333
test acc: top1 ->  52.768 ; top5 ->  76.768  and loss:  820.9510055184364
forward train acc: top1 ->  46.96875 ; top5 ->  71.3046875  and loss:  484.07432520389557
test acc: top1 ->  52.896 ; top5 ->  76.88  and loss:  818.5429876446724
forward train acc: top1 ->  46.9921875 ; top5 ->  70.796875  and loss:  493.22818207740784
test acc: top1 ->  52.99 ; top5 ->  76.98  and loss:  818.5445194840431
forward train acc: top1 ->  47.06640625 ; top5 ->  71.15625  and loss:  487.77929759025574
test acc: top1 ->  52.97 ; top5 ->  76.878  and loss:  818.3279396891594
forward train acc: top1 ->  46.375 ; top5 ->  70.64453125  and loss:  493.1787986755371
test acc: top1 ->  53.002 ; top5 ->  76.956  and loss:  816.9671953320503
forward train acc: top1 ->  46.71484375 ; top5 ->  70.43359375  and loss:  491.8311140537262
test acc: top1 ->  52.96 ; top5 ->  76.934  and loss:  817.1530898213387
forward train acc: top1 ->  47.21875 ; top5 ->  71.33984375  and loss:  480.23321413993835
test acc: top1 ->  52.994 ; top5 ->  76.936  and loss:  815.8785015940666
forward train acc: top1 ->  46.984375 ; top5 ->  70.73046875  and loss:  490.17883253097534
test acc: top1 ->  53.008 ; top5 ->  76.934  and loss:  816.3149636983871
forward train acc: top1 ->  46.88671875 ; top5 ->  71.0078125  and loss:  490.22528541088104
test acc: top1 ->  53.062 ; top5 ->  77.012  and loss:  815.7064350247383
forward train acc: top1 ->  46.63671875 ; top5 ->  71.0  and loss:  487.72149562835693
test acc: top1 ->  53.066 ; top5 ->  77.01  and loss:  815.5151773691177
forward train acc: top1 ->  47.3125 ; top5 ->  71.234375  and loss:  483.33206164836884
test acc: top1 ->  53.072 ; top5 ->  76.976  and loss:  815.7357988357544
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  9216 / 9216 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -121.58127069473267 , diff:  121.58127069473267
adv train loss:  -122.02297568321228 , diff:  0.44170498847961426
layer  6  adv train finish, try to retain  4078
test acc: top1 ->  53.048 ; top5 ->  76.976  and loss:  815.9033274054527
forward train acc: top1 ->  45.40234375 ; top5 ->  69.55859375  and loss:  505.1441316604614
test acc: top1 ->  51.824 ; top5 ->  76.016  and loss:  843.554897248745
forward train acc: top1 ->  45.30859375 ; top5 ->  69.546875  and loss:  507.97657537460327
test acc: top1 ->  51.476 ; top5 ->  75.862  and loss:  849.7626796960831
forward train acc: top1 ->  44.90234375 ; top5 ->  69.4453125  and loss:  507.50892424583435
test acc: top1 ->  51.558 ; top5 ->  75.838  and loss:  851.5777271986008
forward train acc: top1 ->  45.7265625 ; top5 ->  69.83984375  and loss:  503.4849600791931
test acc: top1 ->  52.224 ; top5 ->  76.286  and loss:  835.2802004814148
forward train acc: top1 ->  45.9765625 ; top5 ->  69.9765625  and loss:  498.2033038139343
test acc: top1 ->  52.376 ; top5 ->  76.432  and loss:  831.3157655000687
forward train acc: top1 ->  46.06640625 ; top5 ->  70.08984375  and loss:  499.72652411460876
test acc: top1 ->  52.4 ; top5 ->  76.378  and loss:  833.4067195057869
forward train acc: top1 ->  46.44921875 ; top5 ->  70.43359375  and loss:  494.8662533760071
test acc: top1 ->  52.67 ; top5 ->  76.638  and loss:  826.5705615282059
forward train acc: top1 ->  46.5390625 ; top5 ->  70.29296875  and loss:  499.934711933136
test acc: top1 ->  52.744 ; top5 ->  76.582  and loss:  829.2611632943153
forward train acc: top1 ->  46.2734375 ; top5 ->  70.125  and loss:  497.338849067688
test acc: top1 ->  52.762 ; top5 ->  76.648  and loss:  823.4597468972206
forward train acc: top1 ->  46.42578125 ; top5 ->  70.57421875  and loss:  493.57771825790405
test acc: top1 ->  52.9 ; top5 ->  76.706  and loss:  821.3525285124779
forward train acc: top1 ->  46.45703125 ; top5 ->  70.2421875  and loss:  495.0497770309448
test acc: top1 ->  52.966 ; top5 ->  76.752  and loss:  820.9544266462326
forward train acc: top1 ->  46.6015625 ; top5 ->  71.08984375  and loss:  490.63785231113434
test acc: top1 ->  53.024 ; top5 ->  76.79  and loss:  819.208647608757
forward train acc: top1 ->  47.046875 ; top5 ->  70.484375  and loss:  491.1752028465271
test acc: top1 ->  53.142 ; top5 ->  76.852  and loss:  817.8187835812569
forward train acc: top1 ->  46.85546875 ; top5 ->  70.62109375  and loss:  492.96827387809753
test acc: top1 ->  53.102 ; top5 ->  76.844  and loss:  818.2991688251495
forward train acc: top1 ->  46.31640625 ; top5 ->  70.25390625  and loss:  494.2560498714447
test acc: top1 ->  53.114 ; top5 ->  76.86  and loss:  817.730692923069
forward train acc: top1 ->  47.32421875 ; top5 ->  71.44140625  and loss:  484.45280170440674
test acc: top1 ->  53.2 ; top5 ->  76.88  and loss:  817.4502172470093
forward train acc: top1 ->  47.359375 ; top5 ->  70.51953125  and loss:  489.70128214359283
test acc: top1 ->  53.176 ; top5 ->  76.872  and loss:  817.0532793998718
forward train acc: top1 ->  46.859375 ; top5 ->  70.81640625  and loss:  489.8111400604248
test acc: top1 ->  53.182 ; top5 ->  76.904  and loss:  817.0291498303413
forward train acc: top1 ->  47.1796875 ; top5 ->  70.953125  and loss:  490.3211041688919
test acc: top1 ->  53.164 ; top5 ->  76.916  and loss:  816.6626288294792
forward train acc: top1 ->  47.015625 ; top5 ->  70.98046875  and loss:  489.1138412952423
test acc: top1 ->  53.15 ; top5 ->  76.884  and loss:  816.5587621331215
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  4096 / 4096 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -121.69415974617004 , diff:  121.69415974617004
adv train loss:  -122.50964629650116 , diff:  0.8154865503311157
layer  7  adv train finish, try to retain  4004
test acc: top1 ->  53.162 ; top5 ->  76.908  and loss:  816.8141717910767
forward train acc: top1 ->  45.875 ; top5 ->  69.76171875  and loss:  504.3588024377823
test acc: top1 ->  51.77 ; top5 ->  75.956  and loss:  845.2147631645203
forward train acc: top1 ->  44.921875 ; top5 ->  69.32421875  and loss:  510.8260123729706
test acc: top1 ->  51.762 ; top5 ->  75.956  and loss:  846.849346935749
forward train acc: top1 ->  44.78125 ; top5 ->  69.13671875  and loss:  511.9988558292389
test acc: top1 ->  51.526 ; top5 ->  75.892  and loss:  850.0383145213127
forward train acc: top1 ->  45.59375 ; top5 ->  69.75390625  and loss:  500.05842661857605
test acc: top1 ->  52.206 ; top5 ->  76.474  and loss:  832.3989909291267
forward train acc: top1 ->  45.74609375 ; top5 ->  70.0703125  and loss:  500.4378545284271
test acc: top1 ->  52.31 ; top5 ->  76.384  and loss:  833.2642557621002
forward train acc: top1 ->  45.91796875 ; top5 ->  69.69921875  and loss:  501.90937292575836
test acc: top1 ->  52.578 ; top5 ->  76.57  and loss:  831.4051654934883
forward train acc: top1 ->  45.5234375 ; top5 ->  70.0859375  and loss:  499.68883514404297
test acc: top1 ->  52.596 ; top5 ->  76.728  and loss:  827.3782068490982
forward train acc: top1 ->  46.3515625 ; top5 ->  70.2578125  and loss:  497.213751077652
test acc: top1 ->  52.89 ; top5 ->  76.728  and loss:  824.81410831213
forward train acc: top1 ->  46.57421875 ; top5 ->  70.2890625  and loss:  492.9652078151703
test acc: top1 ->  52.906 ; top5 ->  76.842  and loss:  823.6990339756012
forward train acc: top1 ->  46.92578125 ; top5 ->  70.6328125  and loss:  491.152845621109
test acc: top1 ->  52.946 ; top5 ->  76.866  and loss:  822.3305972218513
forward train acc: top1 ->  46.94921875 ; top5 ->  70.5546875  and loss:  491.66988575458527
test acc: top1 ->  53.04 ; top5 ->  76.888  and loss:  820.5984122753143
forward train acc: top1 ->  46.828125 ; top5 ->  70.5859375  and loss:  488.6409878730774
test acc: top1 ->  53.112 ; top5 ->  76.93  and loss:  818.9164215326309
forward train acc: top1 ->  47.29296875 ; top5 ->  71.08984375  and loss:  485.69969058036804
test acc: top1 ->  53.192 ; top5 ->  76.942  and loss:  817.4959930181503
forward train acc: top1 ->  46.4296875 ; top5 ->  70.40625  and loss:  497.54286432266235
test acc: top1 ->  53.184 ; top5 ->  76.98  and loss:  818.2109691500664
forward train acc: top1 ->  46.859375 ; top5 ->  70.828125  and loss:  491.17593264579773
test acc: top1 ->  53.168 ; top5 ->  76.996  and loss:  817.5737619996071
forward train acc: top1 ->  47.11328125 ; top5 ->  70.77734375  and loss:  490.7929356098175
test acc: top1 ->  53.17 ; top5 ->  77.018  and loss:  817.3009216785431
forward train acc: top1 ->  46.78515625 ; top5 ->  70.80859375  and loss:  487.8968348503113
test acc: top1 ->  53.25 ; top5 ->  77.04  and loss:  816.5389921069145
forward train acc: top1 ->  46.859375 ; top5 ->  70.51953125  and loss:  492.3145760297775
test acc: top1 ->  53.248 ; top5 ->  77.066  and loss:  816.6792085170746
forward train acc: top1 ->  46.95703125 ; top5 ->  71.0703125  and loss:  487.73049342632294
test acc: top1 ->  53.276 ; top5 ->  77.056  and loss:  816.5969802737236
forward train acc: top1 ->  47.08984375 ; top5 ->  70.7578125  and loss:  489.26492500305176
test acc: top1 ->  53.292 ; top5 ->  77.064  and loss:  816.3840969204903
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  4096 / 4096 , inc:  1
layer  0  :  0.984375  ==>  63 / 64 , inc:  1
layer  1  :  0.9947916666666666  ==>  191 / 192 , inc:  1
layer  2  :  1.0  ==>  384 / 384 , inc:  1
layer  3  :  1.0  ==>  256 / 256 , inc:  1
layer  4  :  1.0  ==>  256 / 256 , inc:  1
layer  5  :  1.0  ==>  9216 / 9216 , inc:  1
layer  6  :  1.0  ==>  4096 / 4096 , inc:  1
layer  7  :  1.0  ==>  4096 / 4096 , inc:  1
eps [0.0009887695312500002, 0.00012359619140625002, 6.179809570312501e-05, 9.269714355468752e-05, 9.269714355468752e-05, 2.574920654296875e-06, 5.79357147216797e-06, 5.79357147216797e-06]  wait [2, 4, 3, 3, 3, 3, 3, 3]  inc [1, 1, 1, 1, 1, 1, 1, 1]  tol: 3
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  6  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -126.2119072675705 , diff:  126.2119072675705
adv train loss:  -128.80476784706116 , diff:  2.5928605794906616
layer  0  adv train finish, try to retain  63
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -122.1480164527893 , diff:  122.1480164527893
adv train loss:  -123.14569401741028 , diff:  0.9976775646209717
layer  1  adv train finish, try to retain  188
test acc: top1 ->  52.482 ; top5 ->  76.286  and loss:  834.0681545734406
forward train acc: top1 ->  45.4140625 ; top5 ->  69.296875  and loss:  507.8086097240448
test acc: top1 ->  51.878 ; top5 ->  75.684  and loss:  848.3096278905869
forward train acc: top1 ->  44.8828125 ; top5 ->  69.54296875  and loss:  510.9020699262619
test acc: top1 ->  51.748 ; top5 ->  75.98  and loss:  841.8450983762741
forward train acc: top1 ->  44.7578125 ; top5 ->  68.546875  and loss:  516.1702477931976
test acc: top1 ->  51.538 ; top5 ->  75.8  and loss:  848.3737505674362
forward train acc: top1 ->  45.83203125 ; top5 ->  69.50390625  and loss:  503.5547137260437
test acc: top1 ->  52.132 ; top5 ->  76.344  and loss:  837.8099962472916
forward train acc: top1 ->  45.265625 ; top5 ->  69.7421875  and loss:  504.7881895303726
test acc: top1 ->  52.364 ; top5 ->  76.324  and loss:  833.9507423043251
forward train acc: top1 ->  45.55078125 ; top5 ->  69.74609375  and loss:  504.293629527092
test acc: top1 ->  52.246 ; top5 ->  76.154  and loss:  838.6231018304825
forward train acc: top1 ->  46.22265625 ; top5 ->  70.25  and loss:  501.5886170864105
test acc: top1 ->  52.798 ; top5 ->  76.636  and loss:  826.2485892176628
forward train acc: top1 ->  46.55859375 ; top5 ->  70.21484375  and loss:  497.27686858177185
test acc: top1 ->  52.822 ; top5 ->  76.554  and loss:  825.8476548194885
forward train acc: top1 ->  46.34375 ; top5 ->  70.515625  and loss:  496.9565168619156
test acc: top1 ->  52.86 ; top5 ->  76.674  and loss:  826.5269226431847
forward train acc: top1 ->  46.34375 ; top5 ->  70.4765625  and loss:  494.61623334884644
test acc: top1 ->  53.054 ; top5 ->  76.834  and loss:  821.9627785682678
forward train acc: top1 ->  46.6484375 ; top5 ->  70.296875  and loss:  495.3916599750519
test acc: top1 ->  53.014 ; top5 ->  76.794  and loss:  821.7997652292252
forward train acc: top1 ->  46.5703125 ; top5 ->  70.28125  and loss:  497.4437756538391
test acc: top1 ->  53.084 ; top5 ->  76.838  and loss:  821.4070607423782
forward train acc: top1 ->  46.48828125 ; top5 ->  70.40234375  and loss:  495.19848573207855
test acc: top1 ->  53.098 ; top5 ->  76.918  and loss:  820.4507476091385
forward train acc: top1 ->  46.83984375 ; top5 ->  71.09375  and loss:  490.01597917079926
test acc: top1 ->  53.166 ; top5 ->  76.92  and loss:  820.1433703303337
forward train acc: top1 ->  46.4609375 ; top5 ->  70.484375  and loss:  496.0812631845474
test acc: top1 ->  53.076 ; top5 ->  76.95  and loss:  819.7719422578812
forward train acc: top1 ->  46.70703125 ; top5 ->  70.63671875  and loss:  494.0126816034317
test acc: top1 ->  53.138 ; top5 ->  76.996  and loss:  819.8178881406784
forward train acc: top1 ->  46.83984375 ; top5 ->  70.8828125  and loss:  490.5511384010315
test acc: top1 ->  53.106 ; top5 ->  76.974  and loss:  819.2388745546341
forward train acc: top1 ->  47.171875 ; top5 ->  71.0390625  and loss:  488.60967564582825
test acc: top1 ->  53.174 ; top5 ->  77.008  and loss:  818.6031113266945
forward train acc: top1 ->  46.91015625 ; top5 ->  70.80078125  and loss:  490.2847818136215
test acc: top1 ->  53.168 ; top5 ->  76.986  and loss:  818.5502278208733
forward train acc: top1 ->  46.77734375 ; top5 ->  70.8515625  and loss:  493.3217490911484
test acc: top1 ->  53.164 ; top5 ->  76.97  and loss:  818.5375749468803
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  191 / 192 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -125.15610790252686 , diff:  125.15610790252686
adv train loss:  -126.83445131778717 , diff:  1.678343415260315
layer  2  adv train finish, try to retain  380
test acc: top1 ->  52.166 ; top5 ->  76.318  and loss:  836.4544912576675
forward train acc: top1 ->  45.05859375 ; top5 ->  69.5078125  and loss:  507.9703369140625
test acc: top1 ->  51.858 ; top5 ->  76.264  and loss:  839.069886803627
forward train acc: top1 ->  44.984375 ; top5 ->  69.015625  and loss:  513.5091323852539
test acc: top1 ->  51.696 ; top5 ->  76.048  and loss:  845.0366218090057
forward train acc: top1 ->  44.78125 ; top5 ->  69.42578125  and loss:  509.4514248371124
test acc: top1 ->  51.554 ; top5 ->  76.076  and loss:  847.5965093374252
forward train acc: top1 ->  45.015625 ; top5 ->  68.953125  and loss:  510.7807478904724
test acc: top1 ->  52.208 ; top5 ->  76.254  and loss:  839.0733509659767
forward train acc: top1 ->  45.55078125 ; top5 ->  69.6796875  and loss:  504.2633421421051
test acc: top1 ->  52.268 ; top5 ->  76.374  and loss:  834.105405986309
forward train acc: top1 ->  45.02734375 ; top5 ->  69.40625  and loss:  511.22424960136414
test acc: top1 ->  52.434 ; top5 ->  76.402  and loss:  839.317433655262
forward train acc: top1 ->  46.1171875 ; top5 ->  70.0  and loss:  500.449337720871
test acc: top1 ->  52.594 ; top5 ->  76.542  and loss:  828.9997996091843
forward train acc: top1 ->  46.12890625 ; top5 ->  70.05078125  and loss:  497.9603122472763
test acc: top1 ->  52.834 ; top5 ->  76.658  and loss:  827.074299633503
forward train acc: top1 ->  46.72265625 ; top5 ->  70.7890625  and loss:  489.88013219833374
test acc: top1 ->  52.788 ; top5 ->  76.712  and loss:  824.0583794116974
forward train acc: top1 ->  46.84765625 ; top5 ->  70.80078125  and loss:  491.3876098394394
test acc: top1 ->  53.006 ; top5 ->  76.82  and loss:  822.2469528913498
forward train acc: top1 ->  46.26171875 ; top5 ->  70.4765625  and loss:  494.56614542007446
test acc: top1 ->  53.004 ; top5 ->  76.84  and loss:  820.8472109436989
forward train acc: top1 ->  46.95703125 ; top5 ->  71.12890625  and loss:  489.7761574983597
test acc: top1 ->  52.98 ; top5 ->  76.812  and loss:  821.129645884037
forward train acc: top1 ->  46.52734375 ; top5 ->  70.52734375  and loss:  493.2437598705292
test acc: top1 ->  53.068 ; top5 ->  76.902  and loss:  819.2246467471123
forward train acc: top1 ->  46.78125 ; top5 ->  70.93359375  and loss:  491.4334616661072
test acc: top1 ->  53.1 ; top5 ->  76.918  and loss:  819.0743620991707
forward train acc: top1 ->  46.98046875 ; top5 ->  70.7265625  and loss:  488.7786248922348
test acc: top1 ->  53.148 ; top5 ->  76.96  and loss:  817.4929488301277
forward train acc: top1 ->  47.23828125 ; top5 ->  71.1640625  and loss:  486.869691491127
test acc: top1 ->  53.2 ; top5 ->  76.892  and loss:  818.3256684541702
forward train acc: top1 ->  46.484375 ; top5 ->  70.53515625  and loss:  493.4554966688156
test acc: top1 ->  53.124 ; top5 ->  76.924  and loss:  818.1060946583748
forward train acc: top1 ->  46.59765625 ; top5 ->  70.53125  and loss:  491.27053368091583
test acc: top1 ->  53.158 ; top5 ->  76.91  and loss:  818.2272102236748
forward train acc: top1 ->  46.90234375 ; top5 ->  70.80859375  and loss:  489.0363459587097
test acc: top1 ->  53.202 ; top5 ->  76.938  and loss:  817.9137981534004
forward train acc: top1 ->  46.6953125 ; top5 ->  70.66015625  and loss:  491.4140466451645
test acc: top1 ->  53.208 ; top5 ->  76.942  and loss:  818.119938492775
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  384 / 384 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -122.3786872625351 , diff:  122.3786872625351
adv train loss:  -122.5312967300415 , diff:  0.1526094675064087
layer  3  adv train finish, try to retain  251
test acc: top1 ->  52.258 ; top5 ->  76.274  and loss:  835.3733977079391
forward train acc: top1 ->  44.78125 ; top5 ->  68.90625  and loss:  513.8919435739517
test acc: top1 ->  51.638 ; top5 ->  75.902  and loss:  849.4571990966797
forward train acc: top1 ->  44.49609375 ; top5 ->  69.10546875  and loss:  515.2716226577759
test acc: top1 ->  51.318 ; top5 ->  75.734  and loss:  853.2443792819977
forward train acc: top1 ->  44.39453125 ; top5 ->  69.1015625  and loss:  516.453911781311
test acc: top1 ->  51.424 ; top5 ->  75.7  and loss:  849.3910266757011
forward train acc: top1 ->  45.36328125 ; top5 ->  69.55078125  and loss:  507.1980788707733
test acc: top1 ->  52.036 ; top5 ->  76.278  and loss:  839.9984366297722
forward train acc: top1 ->  45.48046875 ; top5 ->  69.41796875  and loss:  506.05021142959595
test acc: top1 ->  52.21 ; top5 ->  76.256  and loss:  840.27978515625
forward train acc: top1 ->  45.3828125 ; top5 ->  69.77734375  and loss:  502.83061051368713
test acc: top1 ->  52.134 ; top5 ->  76.398  and loss:  837.2571897506714
forward train acc: top1 ->  45.71875 ; top5 ->  70.12890625  and loss:  499.5637946128845
test acc: top1 ->  52.67 ; top5 ->  76.612  and loss:  828.780158162117
forward train acc: top1 ->  46.28125 ; top5 ->  70.04296875  and loss:  496.0523257255554
test acc: top1 ->  52.582 ; top5 ->  76.71  and loss:  827.2972655892372
forward train acc: top1 ->  46.1171875 ; top5 ->  70.41796875  and loss:  496.34760081768036
test acc: top1 ->  52.718 ; top5 ->  76.68  and loss:  827.7673519253731
forward train acc: top1 ->  46.4296875 ; top5 ->  70.58203125  and loss:  495.93446469306946
test acc: top1 ->  52.82 ; top5 ->  76.81  and loss:  826.0889512896538
forward train acc: top1 ->  46.57421875 ; top5 ->  70.35546875  and loss:  498.55743432044983
test acc: top1 ->  52.954 ; top5 ->  77.026  and loss:  822.9551906585693
forward train acc: top1 ->  46.3203125 ; top5 ->  70.44140625  and loss:  495.89935302734375
test acc: top1 ->  52.926 ; top5 ->  76.912  and loss:  823.0721307992935
forward train acc: top1 ->  46.890625 ; top5 ->  70.42578125  and loss:  496.2922487258911
test acc: top1 ->  52.924 ; top5 ->  76.95  and loss:  822.4009250998497
forward train acc: top1 ->  46.375 ; top5 ->  70.30078125  and loss:  496.0308612585068
test acc: top1 ->  52.988 ; top5 ->  76.994  and loss:  822.0395213961601
forward train acc: top1 ->  45.7890625 ; top5 ->  70.1171875  and loss:  499.24691021442413
test acc: top1 ->  52.914 ; top5 ->  76.9  and loss:  823.5976461172104
forward train acc: top1 ->  46.8984375 ; top5 ->  70.87890625  and loss:  492.0130172967911
test acc: top1 ->  52.934 ; top5 ->  76.97  and loss:  822.2524886131287
forward train acc: top1 ->  46.62890625 ; top5 ->  70.5625  and loss:  491.7385895252228
test acc: top1 ->  53.036 ; top5 ->  76.986  and loss:  821.0744152069092
forward train acc: top1 ->  46.93359375 ; top5 ->  70.91796875  and loss:  490.85392904281616
test acc: top1 ->  53.018 ; top5 ->  77.01  and loss:  820.6911134719849
forward train acc: top1 ->  47.359375 ; top5 ->  70.84765625  and loss:  490.977702498436
test acc: top1 ->  53.046 ; top5 ->  77.016  and loss:  820.4575430750847
forward train acc: top1 ->  46.8984375 ; top5 ->  70.6171875  and loss:  489.45618093013763
test acc: top1 ->  53.064 ; top5 ->  76.968  and loss:  820.2606338858604
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -122.62352323532104 , diff:  122.62352323532104
adv train loss:  -125.59345722198486 , diff:  2.9699339866638184
layer  4  adv train finish, try to retain  248
test acc: top1 ->  51.868 ; top5 ->  76.036  and loss:  842.9704931974411
forward train acc: top1 ->  44.82421875 ; top5 ->  69.3203125  and loss:  511.0589909553528
test acc: top1 ->  51.708 ; top5 ->  76.148  and loss:  843.1308563351631
forward train acc: top1 ->  45.32421875 ; top5 ->  69.1875  and loss:  507.8690667152405
test acc: top1 ->  51.342 ; top5 ->  75.66  and loss:  854.6099873185158
forward train acc: top1 ->  44.71875 ; top5 ->  68.79296875  and loss:  513.6994534730911
test acc: top1 ->  51.34 ; top5 ->  75.72  and loss:  855.9815070033073
forward train acc: top1 ->  45.47265625 ; top5 ->  69.5703125  and loss:  506.524600982666
test acc: top1 ->  52.242 ; top5 ->  76.428  and loss:  837.3720681667328
forward train acc: top1 ->  45.3828125 ; top5 ->  69.91015625  and loss:  505.0856671333313
test acc: top1 ->  52.158 ; top5 ->  76.236  and loss:  839.1506217718124
forward train acc: top1 ->  45.54296875 ; top5 ->  69.3125  and loss:  509.8356091976166
test acc: top1 ->  52.104 ; top5 ->  76.266  and loss:  841.6002246141434
forward train acc: top1 ->  45.62109375 ; top5 ->  69.94140625  and loss:  500.48473024368286
test acc: top1 ->  52.526 ; top5 ->  76.634  and loss:  830.079815030098
forward train acc: top1 ->  46.6171875 ; top5 ->  70.05078125  and loss:  497.3920031785965
test acc: top1 ->  52.556 ; top5 ->  76.766  and loss:  829.4500766396523
forward train acc: top1 ->  46.27734375 ; top5 ->  70.359375  and loss:  495.9150651693344
test acc: top1 ->  52.65 ; top5 ->  76.668  and loss:  828.4413669109344
forward train acc: top1 ->  45.4453125 ; top5 ->  69.89453125  and loss:  500.44897294044495
test acc: top1 ->  52.834 ; top5 ->  76.894  and loss:  825.8375177979469
forward train acc: top1 ->  46.84375 ; top5 ->  70.86328125  and loss:  491.4543719291687
test acc: top1 ->  52.93 ; top5 ->  76.93  and loss:  823.8786689639091
forward train acc: top1 ->  46.18359375 ; top5 ->  70.2734375  and loss:  500.2071108818054
test acc: top1 ->  52.938 ; top5 ->  76.92  and loss:  823.6855078935623
forward train acc: top1 ->  46.6875 ; top5 ->  70.6875  and loss:  488.50798177719116
test acc: top1 ->  52.922 ; top5 ->  77.022  and loss:  822.1741788983345
forward train acc: top1 ->  46.21484375 ; top5 ->  70.37890625  and loss:  497.05857491493225
test acc: top1 ->  53.01 ; top5 ->  77.004  and loss:  822.4618719816208
forward train acc: top1 ->  46.1875 ; top5 ->  70.33203125  and loss:  492.1178367137909
test acc: top1 ->  53.056 ; top5 ->  76.992  and loss:  821.4720792174339
forward train acc: top1 ->  46.35546875 ; top5 ->  70.53515625  and loss:  491.76910424232483
test acc: top1 ->  53.044 ; top5 ->  77.008  and loss:  821.0274792313576
forward train acc: top1 ->  46.984375 ; top5 ->  70.921875  and loss:  490.3880968093872
test acc: top1 ->  53.114 ; top5 ->  77.02  and loss:  820.3099269866943
forward train acc: top1 ->  46.51171875 ; top5 ->  70.43359375  and loss:  494.1298243999481
test acc: top1 ->  53.09 ; top5 ->  77.066  and loss:  819.9292803406715
forward train acc: top1 ->  46.875 ; top5 ->  70.421875  and loss:  496.067179441452
test acc: top1 ->  53.116 ; top5 ->  77.04  and loss:  820.2633092403412
forward train acc: top1 ->  46.87890625 ; top5 ->  70.21484375  and loss:  492.53977966308594
test acc: top1 ->  53.076 ; top5 ->  77.03  and loss:  820.6167531609535
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -123.95013213157654 , diff:  123.95013213157654
adv train loss:  -126.07230949401855 , diff:  2.1221773624420166
layer  5  adv train finish, try to retain  9204
test acc: top1 ->  52.6 ; top5 ->  76.658  and loss:  826.3248526453972
forward train acc: top1 ->  45.35546875 ; top5 ->  69.6953125  and loss:  506.9327669143677
test acc: top1 ->  51.658 ; top5 ->  75.88  and loss:  845.1981154680252
forward train acc: top1 ->  44.75 ; top5 ->  69.37109375  and loss:  512.4831109046936
test acc: top1 ->  51.462 ; top5 ->  75.846  and loss:  848.5810785293579
forward train acc: top1 ->  45.0390625 ; top5 ->  69.35546875  and loss:  507.26117157936096
test acc: top1 ->  51.416 ; top5 ->  75.826  and loss:  850.8916693329811
forward train acc: top1 ->  44.921875 ; top5 ->  69.24609375  and loss:  510.83852219581604
test acc: top1 ->  51.898 ; top5 ->  76.236  and loss:  841.0418131947517
forward train acc: top1 ->  45.93359375 ; top5 ->  69.87109375  and loss:  500.30232751369476
test acc: top1 ->  52.208 ; top5 ->  76.478  and loss:  834.5422044992447
forward train acc: top1 ->  45.6171875 ; top5 ->  69.390625  and loss:  506.70861196517944
test acc: top1 ->  52.226 ; top5 ->  76.458  and loss:  833.4693037867546
forward train acc: top1 ->  46.35546875 ; top5 ->  70.04296875  and loss:  500.03830993175507
test acc: top1 ->  52.678 ; top5 ->  76.662  and loss:  828.1443306803703
forward train acc: top1 ->  46.5234375 ; top5 ->  70.70703125  and loss:  493.54198348522186
test acc: top1 ->  52.85 ; top5 ->  76.692  and loss:  827.025229215622
forward train acc: top1 ->  46.51171875 ; top5 ->  70.6015625  and loss:  496.17327523231506
test acc: top1 ->  52.734 ; top5 ->  76.624  and loss:  827.5258090496063
forward train acc: top1 ->  46.6015625 ; top5 ->  70.80859375  and loss:  493.18646454811096
test acc: top1 ->  52.818 ; top5 ->  76.77  and loss:  825.477051615715
forward train acc: top1 ->  46.2890625 ; top5 ->  70.265625  and loss:  496.4283689260483
test acc: top1 ->  52.95 ; top5 ->  76.828  and loss:  824.2995536327362
forward train acc: top1 ->  46.6015625 ; top5 ->  70.4375  and loss:  493.85575449466705
test acc: top1 ->  52.948 ; top5 ->  76.842  and loss:  823.0817359685898
forward train acc: top1 ->  46.8828125 ; top5 ->  70.8125  and loss:  493.263797044754
test acc: top1 ->  53.044 ; top5 ->  76.866  and loss:  821.4336075186729
forward train acc: top1 ->  46.53125 ; top5 ->  70.6015625  and loss:  492.5337814092636
test acc: top1 ->  53.054 ; top5 ->  76.958  and loss:  820.4462928175926
forward train acc: top1 ->  46.3046875 ; top5 ->  70.0078125  and loss:  500.4656536579132
test acc: top1 ->  52.972 ; top5 ->  76.888  and loss:  822.3689628839493
forward train acc: top1 ->  46.3125 ; top5 ->  70.703125  and loss:  492.6215932369232
test acc: top1 ->  53.034 ; top5 ->  76.962  and loss:  820.3902416229248
forward train acc: top1 ->  46.55859375 ; top5 ->  70.3515625  and loss:  495.583456993103
test acc: top1 ->  53.112 ; top5 ->  76.954  and loss:  820.329229414463
forward train acc: top1 ->  46.83203125 ; top5 ->  70.6171875  and loss:  494.89779937267303
test acc: top1 ->  53.092 ; top5 ->  76.974  and loss:  820.6234582066536
forward train acc: top1 ->  47.23046875 ; top5 ->  70.65625  and loss:  489.525342464447
test acc: top1 ->  53.098 ; top5 ->  77.004  and loss:  819.9620571732521
forward train acc: top1 ->  46.40234375 ; top5 ->  70.2109375  and loss:  494.7171559333801
test acc: top1 ->  53.07 ; top5 ->  77.004  and loss:  819.7996450066566
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  9216 / 9216 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -123.34018087387085 , diff:  123.34018087387085
adv train loss:  -122.95343494415283 , diff:  0.3867459297180176
layer  6  adv train finish, try to retain  4084
test acc: top1 ->  53.082 ; top5 ->  77.008  and loss:  819.8711470365524
forward train acc: top1 ->  45.1796875 ; top5 ->  69.70703125  and loss:  505.05985033512115
test acc: top1 ->  51.648 ; top5 ->  75.888  and loss:  846.6520485877991
forward train acc: top1 ->  45.02734375 ; top5 ->  69.26171875  and loss:  508.52465438842773
test acc: top1 ->  51.828 ; top5 ->  75.956  and loss:  844.1380085945129
forward train acc: top1 ->  44.9921875 ; top5 ->  68.93359375  and loss:  513.372542142868
test acc: top1 ->  51.672 ; top5 ->  75.88  and loss:  853.436288356781
forward train acc: top1 ->  45.51953125 ; top5 ->  69.84765625  and loss:  503.71797800064087
test acc: top1 ->  52.024 ; top5 ->  76.258  and loss:  839.3345649242401
forward train acc: top1 ->  45.28515625 ; top5 ->  69.67578125  and loss:  507.7584059238434
test acc: top1 ->  52.302 ; top5 ->  76.234  and loss:  839.7376484870911
forward train acc: top1 ->  46.0390625 ; top5 ->  69.953125  and loss:  501.1010055541992
test acc: top1 ->  52.294 ; top5 ->  76.386  and loss:  836.6596007347107
forward train acc: top1 ->  45.64453125 ; top5 ->  69.89453125  and loss:  498.33770394325256
test acc: top1 ->  52.692 ; top5 ->  76.566  and loss:  829.6105144619942
forward train acc: top1 ->  46.31640625 ; top5 ->  70.46875  and loss:  495.3153884410858
test acc: top1 ->  52.692 ; top5 ->  76.732  and loss:  827.7963387966156
forward train acc: top1 ->  46.70703125 ; top5 ->  70.8671875  and loss:  494.23869800567627
test acc: top1 ->  52.672 ; top5 ->  76.806  and loss:  826.3308410644531
forward train acc: top1 ->  46.64453125 ; top5 ->  71.1015625  and loss:  490.141246676445
test acc: top1 ->  52.772 ; top5 ->  76.87  and loss:  824.4400642514229
forward train acc: top1 ->  46.70703125 ; top5 ->  71.0078125  and loss:  492.0065139532089
test acc: top1 ->  52.876 ; top5 ->  76.916  and loss:  821.9096530079842
forward train acc: top1 ->  46.7734375 ; top5 ->  71.14453125  and loss:  489.3172769546509
test acc: top1 ->  52.826 ; top5 ->  76.944  and loss:  822.5031599998474
forward train acc: top1 ->  47.05078125 ; top5 ->  71.0390625  and loss:  489.1799900531769
test acc: top1 ->  52.964 ; top5 ->  76.964  and loss:  820.9346167445183
forward train acc: top1 ->  46.84765625 ; top5 ->  70.84765625  and loss:  492.45508766174316
test acc: top1 ->  52.944 ; top5 ->  76.938  and loss:  820.694663643837
forward train acc: top1 ->  46.8671875 ; top5 ->  70.70703125  and loss:  490.5087447166443
test acc: top1 ->  53.044 ; top5 ->  77.068  and loss:  819.794403731823
forward train acc: top1 ->  46.73828125 ; top5 ->  70.578125  and loss:  493.63664960861206
test acc: top1 ->  53.046 ; top5 ->  77.1  and loss:  819.4104292988777
forward train acc: top1 ->  46.79296875 ; top5 ->  70.90234375  and loss:  488.21806740760803
test acc: top1 ->  53.056 ; top5 ->  77.084  and loss:  819.4370841383934
forward train acc: top1 ->  47.24609375 ; top5 ->  70.8828125  and loss:  492.9189693927765
test acc: top1 ->  53.044 ; top5 ->  77.09  and loss:  819.3861939311028
forward train acc: top1 ->  46.61328125 ; top5 ->  70.51953125  and loss:  490.60116016864777
test acc: top1 ->  53.036 ; top5 ->  77.07  and loss:  818.481934607029
forward train acc: top1 ->  47.0546875 ; top5 ->  70.953125  and loss:  488.7236533164978
test acc: top1 ->  53.036 ; top5 ->  77.088  and loss:  818.5357431769371
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  4096 / 4096 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -121.39329254627228 , diff:  121.39329254627228
adv train loss:  -124.40779232978821 , diff:  3.01449978351593
adv train loss:  -122.84821248054504 , diff:  1.559579849243164
layer  7  adv train finish, try to retain  4010
test acc: top1 ->  53.054 ; top5 ->  77.104  and loss:  819.1820244789124
forward train acc: top1 ->  44.890625 ; top5 ->  69.625  and loss:  506.95967614650726
test acc: top1 ->  51.584 ; top5 ->  75.846  and loss:  850.1329563856125
forward train acc: top1 ->  45.3125 ; top5 ->  69.44921875  and loss:  509.34499192237854
test acc: top1 ->  51.472 ; top5 ->  75.96  and loss:  849.9008889198303
forward train acc: top1 ->  44.56640625 ; top5 ->  69.03515625  and loss:  514.4956433773041
test acc: top1 ->  51.756 ; top5 ->  75.95  and loss:  848.7282426953316
forward train acc: top1 ->  45.73046875 ; top5 ->  69.703125  and loss:  503.6888505220413
test acc: top1 ->  52.248 ; top5 ->  76.25  and loss:  836.324100613594
forward train acc: top1 ->  45.4296875 ; top5 ->  69.9609375  and loss:  502.5490868091583
test acc: top1 ->  52.168 ; top5 ->  76.428  and loss:  834.7235333323479
forward train acc: top1 ->  45.08984375 ; top5 ->  69.5625  and loss:  507.7641670703888
test acc: top1 ->  52.176 ; top5 ->  76.43  and loss:  837.5000430941582
forward train acc: top1 ->  45.9375 ; top5 ->  70.05078125  and loss:  496.2271534204483
test acc: top1 ->  52.578 ; top5 ->  76.766  and loss:  830.0072798728943
forward train acc: top1 ->  46.6640625 ; top5 ->  70.69921875  and loss:  489.2522712945938
test acc: top1 ->  52.63 ; top5 ->  76.892  and loss:  825.1862415671349
forward train acc: top1 ->  46.40234375 ; top5 ->  70.47265625  and loss:  496.3515977859497
test acc: top1 ->  52.682 ; top5 ->  76.824  and loss:  826.3872749209404
forward train acc: top1 ->  46.01953125 ; top5 ->  69.55078125  and loss:  504.0852255821228
test acc: top1 ->  52.84 ; top5 ->  76.922  and loss:  823.5770618319511
forward train acc: top1 ->  46.76171875 ; top5 ->  70.5859375  and loss:  491.74374198913574
test acc: top1 ->  52.866 ; top5 ->  76.93  and loss:  822.9873994588852
forward train acc: top1 ->  46.3359375 ; top5 ->  70.21875  and loss:  494.3165020942688
test acc: top1 ->  52.858 ; top5 ->  76.954  and loss:  821.8676601648331
forward train acc: top1 ->  46.27734375 ; top5 ->  70.12890625  and loss:  497.5836079120636
test acc: top1 ->  52.94 ; top5 ->  77.026  and loss:  821.4425294995308
forward train acc: top1 ->  46.5859375 ; top5 ->  70.47265625  and loss:  491.5695081949234
test acc: top1 ->  53.018 ; top5 ->  77.012  and loss:  820.572836458683
forward train acc: top1 ->  46.171875 ; top5 ->  70.671875  and loss:  494.117883682251
test acc: top1 ->  52.928 ; top5 ->  76.982  and loss:  820.804602265358
forward train acc: top1 ->  46.6328125 ; top5 ->  70.65625  and loss:  492.28030693531036
test acc: top1 ->  53.016 ; top5 ->  77.036  and loss:  819.5692892670631
forward train acc: top1 ->  46.48828125 ; top5 ->  70.4140625  and loss:  491.5002180337906
test acc: top1 ->  52.954 ; top5 ->  76.99  and loss:  819.8021973371506
forward train acc: top1 ->  47.0078125 ; top5 ->  71.0859375  and loss:  491.2796425819397
test acc: top1 ->  53.042 ; top5 ->  76.998  and loss:  818.980651319027
forward train acc: top1 ->  46.76171875 ; top5 ->  70.59375  and loss:  492.8768073320389
test acc: top1 ->  53.01 ; top5 ->  76.998  and loss:  819.3674477338791
forward train acc: top1 ->  46.85546875 ; top5 ->  71.05859375  and loss:  488.35899913311005
test acc: top1 ->  53.032 ; top5 ->  77.006  and loss:  818.9433372020721
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  4096 / 4096 , inc:  1
layer  0  :  0.984375  ==>  63 / 64 , inc:  1
layer  1  :  0.9947916666666666  ==>  191 / 192 , inc:  1
layer  2  :  1.0  ==>  384 / 384 , inc:  1
layer  3  :  1.0  ==>  256 / 256 , inc:  1
layer  4  :  1.0  ==>  256 / 256 , inc:  1
layer  5  :  1.0  ==>  9216 / 9216 , inc:  1
layer  6  :  1.0  ==>  4096 / 4096 , inc:  1
layer  7  :  1.0  ==>  4096 / 4096 , inc:  1
eps [0.0019775390625000003, 9.269714355468752e-05, 4.634857177734376e-05, 6.952285766601563e-05, 6.952285766601563e-05, 1.9311904907226566e-06, 4.345178604125977e-06, 4.345178604125977e-06]  wait [0, 4, 3, 3, 3, 3, 3, 3]  inc [1, 1, 1, 1, 1, 1, 1, 1]  tol: 4
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  7  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -124.37980508804321 , diff:  124.37980508804321
adv train loss:  -125.95367336273193 , diff:  1.5738682746887207
layer  0  adv train finish, try to retain  59
test acc: top1 ->  46.368 ; top5 ->  71.158  and loss:  958.5882098674774
forward train acc: top1 ->  43.72265625 ; top5 ->  67.79296875  and loss:  523.1136379241943
test acc: top1 ->  50.956 ; top5 ->  75.432  and loss:  860.0230169892311
forward train acc: top1 ->  44.5625 ; top5 ->  68.6015625  and loss:  521.708530664444
test acc: top1 ->  51.358 ; top5 ->  75.484  and loss:  858.358316719532
forward train acc: top1 ->  44.66015625 ; top5 ->  68.64453125  and loss:  519.2871646881104
test acc: top1 ->  51.026 ; top5 ->  75.21  and loss:  860.8153379559517
forward train acc: top1 ->  44.546875 ; top5 ->  69.0546875  and loss:  512.8203947544098
test acc: top1 ->  51.838 ; top5 ->  76.002  and loss:  846.734813272953
forward train acc: top1 ->  45.06640625 ; top5 ->  69.06640625  and loss:  511.5346132516861
test acc: top1 ->  51.88 ; top5 ->  75.926  and loss:  845.1500005126
forward train acc: top1 ->  45.12109375 ; top5 ->  69.4296875  and loss:  511.4952790737152
test acc: top1 ->  51.902 ; top5 ->  76.112  and loss:  845.0333051085472
forward train acc: top1 ->  45.140625 ; top5 ->  69.01953125  and loss:  512.6064827442169
test acc: top1 ->  52.29 ; top5 ->  76.406  and loss:  837.5779283046722
forward train acc: top1 ->  45.58203125 ; top5 ->  69.625  and loss:  503.5665922164917
test acc: top1 ->  52.428 ; top5 ->  76.474  and loss:  834.1367167234421
forward train acc: top1 ->  45.90625 ; top5 ->  70.09375  and loss:  501.01973509788513
test acc: top1 ->  52.434 ; top5 ->  76.418  and loss:  833.9608309864998
forward train acc: top1 ->  45.91796875 ; top5 ->  70.046875  and loss:  501.836763381958
test acc: top1 ->  52.606 ; top5 ->  76.544  and loss:  831.5362015366554
forward train acc: top1 ->  46.05859375 ; top5 ->  69.5234375  and loss:  502.8220543861389
test acc: top1 ->  52.69 ; top5 ->  76.604  and loss:  830.0898059606552
forward train acc: top1 ->  46.10546875 ; top5 ->  70.0703125  and loss:  498.84523379802704
test acc: top1 ->  52.54 ; top5 ->  76.434  and loss:  832.3341438770294
forward train acc: top1 ->  46.375 ; top5 ->  70.1328125  and loss:  498.4615557193756
test acc: top1 ->  52.686 ; top5 ->  76.612  and loss:  828.8217943310738
forward train acc: top1 ->  45.734375 ; top5 ->  70.23046875  and loss:  499.3982354402542
test acc: top1 ->  52.644 ; top5 ->  76.594  and loss:  828.7273657917976
forward train acc: top1 ->  46.359375 ; top5 ->  70.4765625  and loss:  497.5354573726654
test acc: top1 ->  52.712 ; top5 ->  76.546  and loss:  829.2017242312431
forward train acc: top1 ->  46.76171875 ; top5 ->  70.28125  and loss:  495.20561695098877
test acc: top1 ->  52.742 ; top5 ->  76.628  and loss:  828.7132586836815
forward train acc: top1 ->  46.12109375 ; top5 ->  70.53125  and loss:  497.10913622379303
test acc: top1 ->  52.728 ; top5 ->  76.714  and loss:  827.8618538975716
forward train acc: top1 ->  46.17578125 ; top5 ->  70.3046875  and loss:  496.72556257247925
test acc: top1 ->  52.708 ; top5 ->  76.644  and loss:  827.6737444996834
forward train acc: top1 ->  46.09765625 ; top5 ->  70.1875  and loss:  499.0704652070999
test acc: top1 ->  52.672 ; top5 ->  76.63  and loss:  827.9428828954697
forward train acc: top1 ->  45.86328125 ; top5 ->  69.9921875  and loss:  499.95557141304016
test acc: top1 ->  52.686 ; top5 ->  76.658  and loss:  827.8407890200615
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  63 / 64 , inc:  1
---------------- start layer  1  ---------------
### skip layer  1 wait:  4  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  3  ###
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
layer  0  :  0.984375  ==>  63 / 64 , inc:  1
layer  1  :  0.9947916666666666  ==>  191 / 192 , inc:  1
layer  2  :  1.0  ==>  384 / 384 , inc:  1
layer  3  :  1.0  ==>  256 / 256 , inc:  1
layer  4  :  1.0  ==>  256 / 256 , inc:  1
layer  5  :  1.0  ==>  9216 / 9216 , inc:  1
layer  6  :  1.0  ==>  4096 / 4096 , inc:  1
layer  7  :  1.0  ==>  4096 / 4096 , inc:  1
eps [0.0014831542968750003, 9.269714355468752e-05, 4.634857177734376e-05, 6.952285766601563e-05, 6.952285766601563e-05, 1.9311904907226566e-06, 4.345178604125977e-06, 4.345178604125977e-06]  wait [2, 3, 2, 2, 2, 2, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1]  tol: 4
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  8  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -131.01503777503967 , diff:  131.01503777503967
adv train loss:  -132.37987303733826 , diff:  1.364835262298584
layer  0  adv train finish, try to retain  62
test acc: top1 ->  40.212 ; top5 ->  65.126  and loss:  1100.0998284220695
forward train acc: top1 ->  44.01953125 ; top5 ->  67.66015625  and loss:  527.7934522628784
test acc: top1 ->  50.784 ; top5 ->  75.278  and loss:  869.1806260943413
forward train acc: top1 ->  44.38671875 ; top5 ->  68.60546875  and loss:  519.4456112384796
test acc: top1 ->  50.85 ; top5 ->  75.15  and loss:  870.8790190815926
forward train acc: top1 ->  44.28515625 ; top5 ->  68.98828125  and loss:  517.5686178207397
test acc: top1 ->  50.722 ; top5 ->  75.032  and loss:  869.4586896896362
forward train acc: top1 ->  44.83203125 ; top5 ->  69.16796875  and loss:  511.605619430542
test acc: top1 ->  51.81 ; top5 ->  75.918  and loss:  848.6272980570793
forward train acc: top1 ->  45.01953125 ; top5 ->  68.9921875  and loss:  509.00719118118286
test acc: top1 ->  51.87 ; top5 ->  76.092  and loss:  843.8824122548103
forward train acc: top1 ->  45.45703125 ; top5 ->  69.55078125  and loss:  505.34177696704865
test acc: top1 ->  51.978 ; top5 ->  76.124  and loss:  842.9919463396072
forward train acc: top1 ->  45.640625 ; top5 ->  69.8828125  and loss:  504.0968713760376
test acc: top1 ->  52.194 ; top5 ->  76.266  and loss:  838.4951065182686
forward train acc: top1 ->  45.57421875 ; top5 ->  69.4609375  and loss:  504.94477915763855
test acc: top1 ->  52.292 ; top5 ->  76.252  and loss:  837.9442149400711
forward train acc: top1 ->  45.984375 ; top5 ->  69.89453125  and loss:  505.1632077693939
test acc: top1 ->  52.438 ; top5 ->  76.364  and loss:  838.120413005352
forward train acc: top1 ->  45.7109375 ; top5 ->  70.08984375  and loss:  503.37653493881226
test acc: top1 ->  52.544 ; top5 ->  76.502  and loss:  834.6200369596481
forward train acc: top1 ->  45.421875 ; top5 ->  69.1875  and loss:  508.3195412158966
test acc: top1 ->  52.524 ; top5 ->  76.532  and loss:  834.0784076452255
forward train acc: top1 ->  45.67578125 ; top5 ->  69.6328125  and loss:  502.8897601366043
test acc: top1 ->  52.722 ; top5 ->  76.642  and loss:  831.1546420454979
forward train acc: top1 ->  46.12890625 ; top5 ->  70.3125  and loss:  495.2142320871353
test acc: top1 ->  52.732 ; top5 ->  76.702  and loss:  829.6917697191238
forward train acc: top1 ->  46.51171875 ; top5 ->  70.31640625  and loss:  496.2807568311691
test acc: top1 ->  52.722 ; top5 ->  76.698  and loss:  829.4016545414925
forward train acc: top1 ->  46.6875 ; top5 ->  69.7734375  and loss:  501.30266523361206
test acc: top1 ->  52.686 ; top5 ->  76.728  and loss:  829.7235780358315
forward train acc: top1 ->  46.1875 ; top5 ->  69.9140625  and loss:  499.25839614868164
test acc: top1 ->  52.726 ; top5 ->  76.718  and loss:  829.0869358778
forward train acc: top1 ->  46.27734375 ; top5 ->  70.17578125  and loss:  497.83272099494934
test acc: top1 ->  52.694 ; top5 ->  76.746  and loss:  829.188112616539
forward train acc: top1 ->  46.41015625 ; top5 ->  70.26953125  and loss:  498.2787883281708
test acc: top1 ->  52.722 ; top5 ->  76.746  and loss:  828.7632619738579
forward train acc: top1 ->  45.81640625 ; top5 ->  69.9453125  and loss:  501.91247153282166
test acc: top1 ->  52.766 ; top5 ->  76.722  and loss:  828.7555058002472
forward train acc: top1 ->  46.26171875 ; top5 ->  70.2265625  and loss:  495.1381505727768
test acc: top1 ->  52.766 ; top5 ->  76.718  and loss:  828.4093946218491
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  63 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -142.61075496673584 , diff:  142.61075496673584
adv train loss:  -140.02752876281738 , diff:  2.583226203918457
layer  1  adv train finish, try to retain  192
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -141.11437392234802 , diff:  141.11437392234802
adv train loss:  -142.7270166873932 , diff:  1.612642765045166
layer  2  adv train finish, try to retain  383
test acc: top1 ->  47.146 ; top5 ->  71.938  and loss:  940.9829696416855
forward train acc: top1 ->  44.12890625 ; top5 ->  68.5703125  and loss:  518.8862898349762
test acc: top1 ->  51.712 ; top5 ->  76.17  and loss:  844.997031390667
forward train acc: top1 ->  44.56640625 ; top5 ->  69.09765625  and loss:  515.3104665279388
test acc: top1 ->  51.642 ; top5 ->  75.878  and loss:  845.8950024843216
forward train acc: top1 ->  44.45703125 ; top5 ->  68.5390625  and loss:  517.8512568473816
test acc: top1 ->  51.466 ; top5 ->  75.84  and loss:  851.6798770427704
forward train acc: top1 ->  45.88671875 ; top5 ->  69.59375  and loss:  503.7778857946396
test acc: top1 ->  52.164 ; top5 ->  76.394  and loss:  840.7023247480392
forward train acc: top1 ->  46.2734375 ; top5 ->  70.03125  and loss:  499.07377457618713
test acc: top1 ->  52.272 ; top5 ->  76.408  and loss:  836.5538322329521
forward train acc: top1 ->  45.90234375 ; top5 ->  69.93359375  and loss:  502.85282027721405
test acc: top1 ->  52.382 ; top5 ->  76.468  and loss:  837.7810436487198
forward train acc: top1 ->  46.01953125 ; top5 ->  70.19921875  and loss:  502.6517502069473
test acc: top1 ->  52.648 ; top5 ->  76.62  and loss:  832.7150715589523
forward train acc: top1 ->  45.87890625 ; top5 ->  69.73828125  and loss:  499.7244622707367
test acc: top1 ->  52.736 ; top5 ->  76.68  and loss:  829.9998773932457
forward train acc: top1 ->  45.87890625 ; top5 ->  70.16015625  and loss:  500.329793214798
test acc: top1 ->  52.756 ; top5 ->  76.706  and loss:  829.4064452648163
forward train acc: top1 ->  46.03125 ; top5 ->  70.18359375  and loss:  498.07934749126434
test acc: top1 ->  52.892 ; top5 ->  76.83  and loss:  826.7224350571632
forward train acc: top1 ->  46.45703125 ; top5 ->  70.19921875  and loss:  496.7313503026962
test acc: top1 ->  52.806 ; top5 ->  76.912  and loss:  824.623064994812
forward train acc: top1 ->  46.28125 ; top5 ->  70.140625  and loss:  495.7381123304367
test acc: top1 ->  52.886 ; top5 ->  76.838  and loss:  825.8227915763855
forward train acc: top1 ->  46.49609375 ; top5 ->  70.375  and loss:  495.3917747735977
test acc: top1 ->  52.988 ; top5 ->  76.92  and loss:  823.1569687128067
forward train acc: top1 ->  46.13671875 ; top5 ->  70.53125  and loss:  495.6812640428543
test acc: top1 ->  53.034 ; top5 ->  76.97  and loss:  821.9642615914345
forward train acc: top1 ->  46.65234375 ; top5 ->  70.56640625  and loss:  493.05866158008575
test acc: top1 ->  53.046 ; top5 ->  76.986  and loss:  822.2910143136978
forward train acc: top1 ->  46.3203125 ; top5 ->  70.65234375  and loss:  493.04912436008453
test acc: top1 ->  53.164 ; top5 ->  76.982  and loss:  821.5563896894455
forward train acc: top1 ->  46.40625 ; top5 ->  70.17578125  and loss:  494.3085378408432
test acc: top1 ->  53.148 ; top5 ->  77.03  and loss:  821.2415263652802
forward train acc: top1 ->  46.9296875 ; top5 ->  70.6171875  and loss:  494.2676668167114
test acc: top1 ->  53.108 ; top5 ->  77.012  and loss:  821.2606849074364
forward train acc: top1 ->  46.55078125 ; top5 ->  70.96484375  and loss:  494.7861671447754
test acc: top1 ->  53.08 ; top5 ->  77.02  and loss:  821.1919528245926
forward train acc: top1 ->  46.62890625 ; top5 ->  70.67578125  and loss:  493.71287846565247
test acc: top1 ->  53.104 ; top5 ->  77.056  and loss:  821.0967405438423
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  384 / 384 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -126.60202431678772 , diff:  126.60202431678772
adv train loss:  -121.93761086463928 , diff:  4.6644134521484375
adv train loss:  -123.66628205776215 , diff:  1.7286711931228638
************ all values are small in this layer **********
layer  3  adv train finish, try to retain  255
test acc: top1 ->  53.094 ; top5 ->  77.028  and loss:  823.9109589457512
forward train acc: top1 ->  45.55078125 ; top5 ->  70.0703125  and loss:  498.77035081386566
test acc: top1 ->  51.762 ; top5 ->  75.984  and loss:  848.1382269859314
forward train acc: top1 ->  44.71484375 ; top5 ->  68.68359375  and loss:  518.0066395998001
test acc: top1 ->  51.602 ; top5 ->  75.758  and loss:  849.8849441409111
forward train acc: top1 ->  45.13671875 ; top5 ->  69.19921875  and loss:  511.6404342651367
test acc: top1 ->  51.334 ; top5 ->  75.62  and loss:  858.4174545407295
forward train acc: top1 ->  45.4609375 ; top5 ->  69.84375  and loss:  501.45050573349
test acc: top1 ->  52.212 ; top5 ->  76.318  and loss:  834.9232051968575
forward train acc: top1 ->  44.828125 ; top5 ->  69.15625  and loss:  509.88447761535645
test acc: top1 ->  52.348 ; top5 ->  76.336  and loss:  834.6291772127151
forward train acc: top1 ->  45.76171875 ; top5 ->  70.0  and loss:  501.3800426721573
test acc: top1 ->  52.412 ; top5 ->  76.524  and loss:  831.154029905796
forward train acc: top1 ->  45.8046875 ; top5 ->  70.32421875  and loss:  499.8106827735901
test acc: top1 ->  52.592 ; top5 ->  76.654  and loss:  829.004404604435
forward train acc: top1 ->  46.34375 ; top5 ->  70.58984375  and loss:  497.92933177948
test acc: top1 ->  52.72 ; top5 ->  76.58  and loss:  829.3562167882919
forward train acc: top1 ->  45.9375 ; top5 ->  69.66796875  and loss:  498.37198066711426
test acc: top1 ->  52.688 ; top5 ->  76.912  and loss:  826.8892164230347
forward train acc: top1 ->  45.97265625 ; top5 ->  70.19140625  and loss:  498.03281033039093
test acc: top1 ->  52.966 ; top5 ->  76.912  and loss:  825.0333833098412
forward train acc: top1 ->  45.921875 ; top5 ->  70.203125  and loss:  498.36612844467163
test acc: top1 ->  52.894 ; top5 ->  76.888  and loss:  823.3676186800003
forward train acc: top1 ->  46.4609375 ; top5 ->  70.3359375  and loss:  495.6093319654465
test acc: top1 ->  52.884 ; top5 ->  76.888  and loss:  824.1095516085625
forward train acc: top1 ->  46.85546875 ; top5 ->  70.48046875  and loss:  493.39358246326447
test acc: top1 ->  52.944 ; top5 ->  77.036  and loss:  820.8943817019463
forward train acc: top1 ->  46.984375 ; top5 ->  70.74609375  and loss:  492.0126168727875
test acc: top1 ->  52.984 ; top5 ->  76.986  and loss:  821.8342225551605
forward train acc: top1 ->  45.8828125 ; top5 ->  69.92578125  and loss:  499.85524928569794
test acc: top1 ->  52.984 ; top5 ->  77.01  and loss:  822.0005511045456
forward train acc: top1 ->  47.07421875 ; top5 ->  70.7578125  and loss:  489.31633734703064
test acc: top1 ->  52.93 ; top5 ->  77.044  and loss:  821.2915459275246
forward train acc: top1 ->  46.1328125 ; top5 ->  70.44921875  and loss:  494.4801940917969
test acc: top1 ->  53.028 ; top5 ->  77.076  and loss:  820.6748319268227
forward train acc: top1 ->  46.28125 ; top5 ->  70.34375  and loss:  496.06009566783905
test acc: top1 ->  53.04 ; top5 ->  77.052  and loss:  821.202324450016
forward train acc: top1 ->  46.61328125 ; top5 ->  70.4921875  and loss:  493.8258546590805
test acc: top1 ->  53.074 ; top5 ->  77.1  and loss:  820.8886971473694
forward train acc: top1 ->  46.0 ; top5 ->  70.25  and loss:  494.78405380249023
test acc: top1 ->  53.076 ; top5 ->  77.062  and loss:  820.5147423148155
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -125.88855171203613 , diff:  125.88855171203613
adv train loss:  -123.76097679138184 , diff:  2.127574920654297
************ all values are small in this layer **********
layer  4  adv train finish, try to retain  255
test acc: top1 ->  52.904 ; top5 ->  76.826  and loss:  823.6200675964355
forward train acc: top1 ->  44.9296875 ; top5 ->  69.52734375  and loss:  507.5975071191788
test acc: top1 ->  51.522 ; top5 ->  75.872  and loss:  847.7211029529572
forward train acc: top1 ->  44.57421875 ; top5 ->  69.359375  and loss:  510.83440935611725
test acc: top1 ->  51.34 ; top5 ->  75.702  and loss:  852.8298058509827
forward train acc: top1 ->  44.640625 ; top5 ->  68.93359375  and loss:  512.1990789175034
test acc: top1 ->  51.214 ; top5 ->  75.842  and loss:  856.6036775112152
forward train acc: top1 ->  44.8359375 ; top5 ->  69.02734375  and loss:  510.39558839797974
test acc: top1 ->  52.018 ; top5 ->  76.408  and loss:  838.5476441979408
forward train acc: top1 ->  45.51171875 ; top5 ->  69.38671875  and loss:  506.0984808206558
test acc: top1 ->  52.088 ; top5 ->  76.338  and loss:  841.6940116286278
forward train acc: top1 ->  45.75390625 ; top5 ->  69.7578125  and loss:  503.0131449699402
test acc: top1 ->  52.094 ; top5 ->  76.264  and loss:  839.9684889316559
forward train acc: top1 ->  45.6171875 ; top5 ->  69.37890625  and loss:  506.2061858177185
test acc: top1 ->  52.518 ; top5 ->  76.73  and loss:  830.5540697574615
forward train acc: top1 ->  45.53125 ; top5 ->  69.3828125  and loss:  505.51978743076324
test acc: top1 ->  52.556 ; top5 ->  76.746  and loss:  829.0581310987473
forward train acc: top1 ->  45.95703125 ; top5 ->  70.359375  and loss:  498.74281311035156
test acc: top1 ->  52.744 ; top5 ->  76.818  and loss:  827.8971417546272
forward train acc: top1 ->  46.01953125 ; top5 ->  70.44140625  and loss:  496.4295310974121
test acc: top1 ->  52.7 ; top5 ->  76.912  and loss:  826.331121981144
forward train acc: top1 ->  46.6328125 ; top5 ->  70.44921875  and loss:  495.8088948726654
test acc: top1 ->  52.894 ; top5 ->  76.942  and loss:  822.4044970273972
forward train acc: top1 ->  46.171875 ; top5 ->  70.1640625  and loss:  494.64217603206635
test acc: top1 ->  52.844 ; top5 ->  76.874  and loss:  824.3432008624077
forward train acc: top1 ->  45.859375 ; top5 ->  70.0546875  and loss:  502.49796175956726
test acc: top1 ->  52.794 ; top5 ->  77.002  and loss:  822.1732398271561
forward train acc: top1 ->  46.46484375 ; top5 ->  70.22265625  and loss:  497.4777532815933
test acc: top1 ->  52.888 ; top5 ->  77.038  and loss:  822.1416885852814
forward train acc: top1 ->  46.69921875 ; top5 ->  70.85546875  and loss:  493.08948493003845
test acc: top1 ->  52.892 ; top5 ->  76.984  and loss:  821.5607565641403
forward train acc: top1 ->  46.8671875 ; top5 ->  70.56640625  and loss:  491.84130454063416
test acc: top1 ->  52.932 ; top5 ->  77.03  and loss:  821.0438213348389
forward train acc: top1 ->  46.48828125 ; top5 ->  71.05078125  and loss:  488.8427492380142
test acc: top1 ->  52.954 ; top5 ->  77.052  and loss:  819.874272942543
forward train acc: top1 ->  46.19140625 ; top5 ->  70.50390625  and loss:  495.6120240688324
test acc: top1 ->  52.908 ; top5 ->  77.012  and loss:  820.4675018191338
forward train acc: top1 ->  46.69140625 ; top5 ->  70.37890625  and loss:  493.98665511608124
test acc: top1 ->  52.948 ; top5 ->  77.044  and loss:  820.7248203754425
forward train acc: top1 ->  46.078125 ; top5 ->  70.43359375  and loss:  497.4761699438095
test acc: top1 ->  52.968 ; top5 ->  77.028  and loss:  820.8616669774055
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -121.1564953327179 , diff:  121.1564953327179
adv train loss:  -119.17854297161102 , diff:  1.9779523611068726
************ all values are small in this layer **********
layer  5  adv train finish, try to retain  9215
test acc: top1 ->  53.018 ; top5 ->  76.946  and loss:  820.3363220691681
forward train acc: top1 ->  45.265625 ; top5 ->  68.71484375  and loss:  511.79120206832886
test acc: top1 ->  51.932 ; top5 ->  76.014  and loss:  848.6231516003609
forward train acc: top1 ->  44.6953125 ; top5 ->  69.0234375  and loss:  513.4281804561615
test acc: top1 ->  51.658 ; top5 ->  75.888  and loss:  848.4298389554024
forward train acc: top1 ->  45.2890625 ; top5 ->  69.19140625  and loss:  508.0889096260071
test acc: top1 ->  52.03 ; top5 ->  76.004  and loss:  842.2756484746933
forward train acc: top1 ->  45.0078125 ; top5 ->  69.44921875  and loss:  509.33919417858124
test acc: top1 ->  52.282 ; top5 ->  76.244  and loss:  838.9881505966187
forward train acc: top1 ->  45.58984375 ; top5 ->  69.83203125  and loss:  504.62719678878784
test acc: top1 ->  52.398 ; top5 ->  76.4  and loss:  834.356342792511
forward train acc: top1 ->  45.203125 ; top5 ->  69.62109375  and loss:  505.5321372747421
test acc: top1 ->  52.432 ; top5 ->  76.652  and loss:  833.9133321642876
forward train acc: top1 ->  45.37109375 ; top5 ->  69.3515625  and loss:  507.759659409523
test acc: top1 ->  52.734 ; top5 ->  76.806  and loss:  828.1943231225014
forward train acc: top1 ->  45.72265625 ; top5 ->  69.90234375  and loss:  501.9970281124115
test acc: top1 ->  52.762 ; top5 ->  76.798  and loss:  827.0256015658379
forward train acc: top1 ->  45.74609375 ; top5 ->  69.94921875  and loss:  500.8012592792511
test acc: top1 ->  52.772 ; top5 ->  76.792  and loss:  826.0183668732643
forward train acc: top1 ->  45.8125 ; top5 ->  69.9765625  and loss:  498.82656013965607
test acc: top1 ->  52.956 ; top5 ->  76.904  and loss:  825.1408258080482
forward train acc: top1 ->  46.46875 ; top5 ->  70.2734375  and loss:  495.6500816345215
test acc: top1 ->  53.07 ; top5 ->  76.936  and loss:  823.0294651389122
forward train acc: top1 ->  46.64453125 ; top5 ->  70.2734375  and loss:  497.06812286376953
test acc: top1 ->  53.094 ; top5 ->  76.92  and loss:  822.5958105921745
forward train acc: top1 ->  46.171875 ; top5 ->  70.3515625  and loss:  495.9720585346222
test acc: top1 ->  53.092 ; top5 ->  76.938  and loss:  822.5778470039368
forward train acc: top1 ->  45.8359375 ; top5 ->  70.23046875  and loss:  501.0145891904831
test acc: top1 ->  53.06 ; top5 ->  77.0  and loss:  822.7968324422836
forward train acc: top1 ->  45.98828125 ; top5 ->  70.12109375  and loss:  498.1546186208725
test acc: top1 ->  53.182 ; top5 ->  76.97  and loss:  821.3167234063148
forward train acc: top1 ->  46.0546875 ; top5 ->  70.546875  and loss:  494.6479823589325
test acc: top1 ->  53.142 ; top5 ->  76.976  and loss:  820.7101551294327
forward train acc: top1 ->  46.54296875 ; top5 ->  70.4609375  and loss:  495.6821573972702
test acc: top1 ->  53.18 ; top5 ->  76.99  and loss:  820.4277913570404
forward train acc: top1 ->  47.0703125 ; top5 ->  70.54296875  and loss:  491.36630976200104
test acc: top1 ->  53.184 ; top5 ->  77.03  and loss:  820.5497052073479
forward train acc: top1 ->  47.07421875 ; top5 ->  70.23828125  and loss:  492.11630868911743
test acc: top1 ->  53.212 ; top5 ->  77.016  and loss:  820.2992382645607
forward train acc: top1 ->  46.5234375 ; top5 ->  70.12109375  and loss:  494.21248400211334
test acc: top1 ->  53.22 ; top5 ->  76.998  and loss:  820.1151098608971
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  9216 / 9216 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -125.67971754074097 , diff:  125.67971754074097
adv train loss:  -122.96052086353302 , diff:  2.7191966772079468
layer  6  adv train finish, try to retain  4081
test acc: top1 ->  53.214 ; top5 ->  76.994  and loss:  820.2972922325134
forward train acc: top1 ->  44.98828125 ; top5 ->  69.58203125  and loss:  508.67869114875793
test acc: top1 ->  51.648 ; top5 ->  75.964  and loss:  851.3225759267807
forward train acc: top1 ->  45.00390625 ; top5 ->  69.3203125  and loss:  513.6487557888031
test acc: top1 ->  51.49 ; top5 ->  75.828  and loss:  859.2722111940384
forward train acc: top1 ->  44.328125 ; top5 ->  68.75  and loss:  517.6765854358673
test acc: top1 ->  51.684 ; top5 ->  75.904  and loss:  851.9506939649582
forward train acc: top1 ->  45.44921875 ; top5 ->  69.66796875  and loss:  507.3560435771942
test acc: top1 ->  52.182 ; top5 ->  76.324  and loss:  840.734399497509
forward train acc: top1 ->  44.8359375 ; top5 ->  69.25  and loss:  511.18043291568756
test acc: top1 ->  52.268 ; top5 ->  76.194  and loss:  840.9959862232208
forward train acc: top1 ->  45.42578125 ; top5 ->  69.42578125  and loss:  505.59287548065186
test acc: top1 ->  52.496 ; top5 ->  76.484  and loss:  836.706797003746
forward train acc: top1 ->  46.1640625 ; top5 ->  70.109375  and loss:  499.44566440582275
test acc: top1 ->  52.61 ; top5 ->  76.548  and loss:  832.9676820039749
forward train acc: top1 ->  45.703125 ; top5 ->  69.78125  and loss:  503.5566952228546
test acc: top1 ->  52.662 ; top5 ->  76.598  and loss:  833.4277902841568
forward train acc: top1 ->  45.5859375 ; top5 ->  69.9140625  and loss:  501.59311521053314
test acc: top1 ->  52.79 ; top5 ->  76.644  and loss:  829.9244194626808
forward train acc: top1 ->  46.19921875 ; top5 ->  70.16015625  and loss:  498.1790112257004
test acc: top1 ->  52.892 ; top5 ->  76.822  and loss:  827.6210372447968
forward train acc: top1 ->  46.1328125 ; top5 ->  70.21484375  and loss:  499.53231716156006
test acc: top1 ->  52.924 ; top5 ->  76.834  and loss:  827.2846602201462
forward train acc: top1 ->  46.125 ; top5 ->  70.34765625  and loss:  496.5369380712509
test acc: top1 ->  52.952 ; top5 ->  76.82  and loss:  826.1815893650055
forward train acc: top1 ->  46.07421875 ; top5 ->  70.0703125  and loss:  498.88782501220703
test acc: top1 ->  53.084 ; top5 ->  76.788  and loss:  824.9796990156174
forward train acc: top1 ->  46.2734375 ; top5 ->  70.88671875  and loss:  492.7485910654068
test acc: top1 ->  53.086 ; top5 ->  76.804  and loss:  824.6099677681923
forward train acc: top1 ->  46.55078125 ; top5 ->  70.0859375  and loss:  494.1678057909012
test acc: top1 ->  53.1 ; top5 ->  76.942  and loss:  823.1180843710899
forward train acc: top1 ->  46.78515625 ; top5 ->  70.46875  and loss:  493.4709757566452
test acc: top1 ->  53.07 ; top5 ->  76.894  and loss:  823.3235968351364
forward train acc: top1 ->  46.453125 ; top5 ->  70.42578125  and loss:  497.39039266109467
test acc: top1 ->  53.126 ; top5 ->  76.888  and loss:  823.3529597520828
forward train acc: top1 ->  46.1640625 ; top5 ->  70.40234375  and loss:  497.25644278526306
test acc: top1 ->  53.088 ; top5 ->  76.946  and loss:  823.5312317609787
forward train acc: top1 ->  46.578125 ; top5 ->  70.859375  and loss:  491.78469812870026
test acc: top1 ->  53.088 ; top5 ->  76.916  and loss:  822.6475079059601
forward train acc: top1 ->  46.2890625 ; top5 ->  70.26171875  and loss:  497.29887104034424
test acc: top1 ->  53.152 ; top5 ->  76.988  and loss:  822.5746424794197
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  4096 / 4096 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -125.17074179649353 , diff:  125.17074179649353
adv train loss:  -121.42632961273193 , diff:  3.7444121837615967
adv train loss:  -122.63358640670776 , diff:  1.20725679397583
layer  7  adv train finish, try to retain  4012
test acc: top1 ->  53.11 ; top5 ->  76.92  and loss:  822.5960288643837
forward train acc: top1 ->  45.24609375 ; top5 ->  69.1796875  and loss:  508.158216714859
test acc: top1 ->  51.794 ; top5 ->  76.094  and loss:  846.7803049683571
forward train acc: top1 ->  44.42578125 ; top5 ->  69.296875  and loss:  513.6526141166687
test acc: top1 ->  51.548 ; top5 ->  75.854  and loss:  852.9774966835976
forward train acc: top1 ->  44.671875 ; top5 ->  69.01953125  and loss:  512.1030658483505
test acc: top1 ->  51.772 ; top5 ->  75.954  and loss:  849.7814979553223
forward train acc: top1 ->  44.82421875 ; top5 ->  69.578125  and loss:  508.13844764232635
test acc: top1 ->  52.134 ; top5 ->  76.448  and loss:  839.7014374732971
forward train acc: top1 ->  45.91796875 ; top5 ->  69.90234375  and loss:  501.66384875774384
test acc: top1 ->  52.56 ; top5 ->  76.446  and loss:  834.8523772954941
forward train acc: top1 ->  46.0 ; top5 ->  70.0078125  and loss:  501.1433701515198
test acc: top1 ->  52.354 ; top5 ->  76.398  and loss:  836.4806693792343
forward train acc: top1 ->  45.6796875 ; top5 ->  69.890625  and loss:  500.62358129024506
test acc: top1 ->  52.718 ; top5 ->  76.612  and loss:  831.877571284771
forward train acc: top1 ->  45.98828125 ; top5 ->  69.9765625  and loss:  501.88340973854065
test acc: top1 ->  52.694 ; top5 ->  76.658  and loss:  830.1634343862534
forward train acc: top1 ->  46.1640625 ; top5 ->  70.4296875  and loss:  497.0967803001404
test acc: top1 ->  52.814 ; top5 ->  76.772  and loss:  825.5728332400322
forward train acc: top1 ->  46.7265625 ; top5 ->  70.26953125  and loss:  496.7678141593933
test acc: top1 ->  52.964 ; top5 ->  76.902  and loss:  824.7440955638885
forward train acc: top1 ->  46.37890625 ; top5 ->  70.37890625  and loss:  495.795108795166
test acc: top1 ->  53.052 ; top5 ->  76.872  and loss:  824.1693969964981
forward train acc: top1 ->  45.82421875 ; top5 ->  70.5078125  and loss:  495.65908920764923
test acc: top1 ->  53.16 ; top5 ->  76.878  and loss:  824.0098749995232
forward train acc: top1 ->  46.23046875 ; top5 ->  70.18359375  and loss:  498.12360525131226
test acc: top1 ->  53.11 ; top5 ->  76.908  and loss:  824.0252642631531
forward train acc: top1 ->  46.3515625 ; top5 ->  70.72265625  and loss:  494.9149292707443
test acc: top1 ->  53.082 ; top5 ->  76.97  and loss:  822.9144241213799
forward train acc: top1 ->  46.73046875 ; top5 ->  70.7578125  and loss:  491.1105934381485
test acc: top1 ->  53.186 ; top5 ->  77.02  and loss:  821.8993865847588
forward train acc: top1 ->  46.484375 ; top5 ->  70.20703125  and loss:  495.1925435066223
test acc: top1 ->  53.152 ; top5 ->  76.962  and loss:  822.0527786612511
forward train acc: top1 ->  46.6328125 ; top5 ->  70.578125  and loss:  495.30529713630676
test acc: top1 ->  53.204 ; top5 ->  76.974  and loss:  821.9883647561073
forward train acc: top1 ->  46.36328125 ; top5 ->  70.36328125  and loss:  495.529212474823
test acc: top1 ->  53.232 ; top5 ->  77.012  and loss:  820.9874285459518
forward train acc: top1 ->  46.62890625 ; top5 ->  70.63671875  and loss:  490.89151525497437
test acc: top1 ->  53.104 ; top5 ->  77.018  and loss:  820.8003525137901
forward train acc: top1 ->  45.94921875 ; top5 ->  69.92578125  and loss:  500.10898995399475
test acc: top1 ->  53.152 ; top5 ->  76.996  and loss:  821.3674417734146
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  4096 / 4096 , inc:  1
layer  0  :  0.984375  ==>  63 / 64 , inc:  1
layer  1  :  0.9947916666666666  ==>  191 / 192 , inc:  1
layer  2  :  1.0  ==>  384 / 384 , inc:  1
layer  3  :  1.0  ==>  256 / 256 , inc:  1
layer  4  :  1.0  ==>  256 / 256 , inc:  1
layer  5  :  1.0  ==>  9216 / 9216 , inc:  1
layer  6  :  1.0  ==>  4096 / 4096 , inc:  1
layer  7  :  1.0  ==>  4096 / 4096 , inc:  1
eps [0.00111236572265625, 0.00018539428710937503, 3.4761428833007815e-05, 5.214214324951172e-05, 5.214214324951172e-05, 1.4483928680419924e-06, 3.2588839530944827e-06, 3.2588839530944827e-06]  wait [2, 1, 2, 2, 2, 2, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1]  tol: 5
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  9  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -126.44350981712341 , diff:  126.44350981712341
adv train loss:  -126.68890285491943 , diff:  0.2453930377960205
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  62
test acc: top1 ->  49.92 ; top5 ->  74.358  and loss:  884.3055964708328
forward train acc: top1 ->  44.59765625 ; top5 ->  68.80078125  and loss:  515.877299785614
test acc: top1 ->  51.506 ; top5 ->  75.778  and loss:  854.2066842913628
forward train acc: top1 ->  44.60546875 ; top5 ->  68.90234375  and loss:  513.1895048618317
test acc: top1 ->  51.516 ; top5 ->  75.824  and loss:  853.4761424064636
forward train acc: top1 ->  43.9375 ; top5 ->  68.24609375  and loss:  523.5342721939087
test acc: top1 ->  51.29 ; top5 ->  75.514  and loss:  858.4936201572418
forward train acc: top1 ->  45.296875 ; top5 ->  69.4609375  and loss:  509.7244602441788
test acc: top1 ->  51.9 ; top5 ->  76.23  and loss:  842.3928630948067
forward train acc: top1 ->  45.0625 ; top5 ->  68.9453125  and loss:  511.0249960422516
test acc: top1 ->  52.194 ; top5 ->  76.324  and loss:  842.209104180336
forward train acc: top1 ->  45.2890625 ; top5 ->  70.0234375  and loss:  504.73060858249664
test acc: top1 ->  52.152 ; top5 ->  76.34  and loss:  839.8178427219391
forward train acc: top1 ->  46.34375 ; top5 ->  70.39453125  and loss:  496.2415224313736
test acc: top1 ->  52.446 ; top5 ->  76.604  and loss:  834.7103333473206
forward train acc: top1 ->  46.04296875 ; top5 ->  70.21875  and loss:  499.2374094724655
test acc: top1 ->  52.478 ; top5 ->  76.618  and loss:  831.7088534832001
forward train acc: top1 ->  46.06640625 ; top5 ->  70.02734375  and loss:  500.0326941013336
test acc: top1 ->  52.454 ; top5 ->  76.83  and loss:  830.9849850535393
forward train acc: top1 ->  45.8203125 ; top5 ->  70.05078125  and loss:  499.7708537578583
test acc: top1 ->  52.654 ; top5 ->  76.834  and loss:  829.5460555553436
forward train acc: top1 ->  45.8203125 ; top5 ->  70.1171875  and loss:  499.7263550758362
test acc: top1 ->  52.524 ; top5 ->  76.828  and loss:  831.5155316591263
forward train acc: top1 ->  46.57421875 ; top5 ->  70.3984375  and loss:  496.18707406520844
test acc: top1 ->  52.65 ; top5 ->  76.794  and loss:  828.1807109117508
forward train acc: top1 ->  45.71875 ; top5 ->  70.0  and loss:  501.01210498809814
test acc: top1 ->  52.722 ; top5 ->  76.876  and loss:  827.9530055522919
forward train acc: top1 ->  46.109375 ; top5 ->  70.24609375  and loss:  497.82458782196045
test acc: top1 ->  52.748 ; top5 ->  76.864  and loss:  826.342834353447
forward train acc: top1 ->  45.7890625 ; top5 ->  69.33203125  and loss:  504.87036323547363
test acc: top1 ->  52.702 ; top5 ->  76.886  and loss:  828.0520424842834
forward train acc: top1 ->  46.15234375 ; top5 ->  69.98828125  and loss:  499.412145614624
test acc: top1 ->  52.784 ; top5 ->  76.892  and loss:  826.731655061245
forward train acc: top1 ->  46.39453125 ; top5 ->  70.16015625  and loss:  497.0944045782089
test acc: top1 ->  52.766 ; top5 ->  76.92  and loss:  825.7599936723709
forward train acc: top1 ->  45.921875 ; top5 ->  69.875  and loss:  502.33536434173584
test acc: top1 ->  52.81 ; top5 ->  76.95  and loss:  825.9499800205231
forward train acc: top1 ->  46.34375 ; top5 ->  70.41015625  and loss:  496.4889786243439
test acc: top1 ->  52.75 ; top5 ->  76.904  and loss:  825.3535740971565
forward train acc: top1 ->  46.7890625 ; top5 ->  70.41796875  and loss:  497.2958896160126
test acc: top1 ->  52.766 ; top5 ->  76.944  and loss:  825.4270243644714
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  63 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -130.6090705394745 , diff:  130.6090705394745
adv train loss:  -133.22218775749207 , diff:  2.613117218017578
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  190
test acc: top1 ->  49.362 ; top5 ->  73.924  and loss:  901.4084813594818
forward train acc: top1 ->  44.79296875 ; top5 ->  68.8203125  and loss:  511.5576469898224
test acc: top1 ->  51.64 ; top5 ->  75.834  and loss:  848.4776622056961
forward train acc: top1 ->  44.33984375 ; top5 ->  68.87890625  and loss:  517.0590236186981
test acc: top1 ->  51.44 ; top5 ->  75.724  and loss:  854.3085077404976
forward train acc: top1 ->  43.9765625 ; top5 ->  68.18359375  and loss:  523.9079389572144
test acc: top1 ->  51.116 ; top5 ->  75.456  and loss:  863.6248496174812
forward train acc: top1 ->  45.0 ; top5 ->  69.5078125  and loss:  506.70759081840515
test acc: top1 ->  52.08 ; top5 ->  76.27  and loss:  838.6285366415977
forward train acc: top1 ->  45.578125 ; top5 ->  69.72265625  and loss:  507.1676844358444
test acc: top1 ->  52.118 ; top5 ->  76.118  and loss:  842.092033803463
forward train acc: top1 ->  45.125 ; top5 ->  69.2734375  and loss:  508.17599964141846
test acc: top1 ->  52.268 ; top5 ->  76.378  and loss:  839.4244156479836
forward train acc: top1 ->  45.3046875 ; top5 ->  69.51171875  and loss:  505.5738968849182
test acc: top1 ->  52.49 ; top5 ->  76.496  and loss:  834.9085708856583
forward train acc: top1 ->  46.17578125 ; top5 ->  70.1640625  and loss:  500.4699239730835
test acc: top1 ->  52.708 ; top5 ->  76.614  and loss:  829.884158551693
forward train acc: top1 ->  45.78125 ; top5 ->  69.9765625  and loss:  502.38701033592224
test acc: top1 ->  52.548 ; top5 ->  76.664  and loss:  829.9049160480499
forward train acc: top1 ->  45.91796875 ; top5 ->  70.390625  and loss:  496.6352585554123
test acc: top1 ->  52.718 ; top5 ->  76.776  and loss:  827.596319437027
forward train acc: top1 ->  46.1328125 ; top5 ->  69.9453125  and loss:  498.8358277082443
test acc: top1 ->  52.722 ; top5 ->  76.754  and loss:  829.2835212349892
forward train acc: top1 ->  46.421875 ; top5 ->  70.671875  and loss:  492.05286955833435
test acc: top1 ->  52.832 ; top5 ->  76.852  and loss:  826.9460328817368
forward train acc: top1 ->  46.14453125 ; top5 ->  70.21484375  and loss:  499.1254073381424
test acc: top1 ->  52.832 ; top5 ->  76.826  and loss:  826.7900626063347
forward train acc: top1 ->  46.43359375 ; top5 ->  70.3515625  and loss:  496.3537098169327
test acc: top1 ->  52.852 ; top5 ->  76.784  and loss:  826.7393705248833
forward train acc: top1 ->  46.5 ; top5 ->  70.34765625  and loss:  494.02071475982666
test acc: top1 ->  52.998 ; top5 ->  76.84  and loss:  824.6321488618851
forward train acc: top1 ->  46.00390625 ; top5 ->  69.99609375  and loss:  498.54394936561584
test acc: top1 ->  52.956 ; top5 ->  76.814  and loss:  825.1044835448265
forward train acc: top1 ->  46.16796875 ; top5 ->  70.3125  and loss:  495.59232807159424
test acc: top1 ->  52.978 ; top5 ->  76.854  and loss:  824.221003472805
forward train acc: top1 ->  46.265625 ; top5 ->  69.90625  and loss:  500.05503833293915
test acc: top1 ->  53.012 ; top5 ->  76.844  and loss:  823.908321082592
forward train acc: top1 ->  46.828125 ; top5 ->  70.5625  and loss:  494.84289491176605
test acc: top1 ->  53.03 ; top5 ->  76.876  and loss:  823.9019221067429
forward train acc: top1 ->  46.546875 ; top5 ->  70.2578125  and loss:  495.65528762340546
test acc: top1 ->  53.02 ; top5 ->  76.908  and loss:  823.9076619744301
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  191 / 192 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -124.61144614219666 , diff:  124.61144614219666
adv train loss:  -125.1552300453186 , diff:  0.5437839031219482
layer  2  adv train finish, try to retain  380
test acc: top1 ->  52.518 ; top5 ->  76.56  and loss:  832.1568674445152
forward train acc: top1 ->  44.4296875 ; top5 ->  68.6171875  and loss:  515.6661949157715
test acc: top1 ->  51.518 ; top5 ->  75.712  and loss:  858.6601635813713
forward train acc: top1 ->  44.93359375 ; top5 ->  68.34765625  and loss:  518.0624656677246
test acc: top1 ->  51.32 ; top5 ->  75.728  and loss:  857.0328765511513
forward train acc: top1 ->  44.46484375 ; top5 ->  68.68359375  and loss:  515.4683222770691
test acc: top1 ->  51.34 ; top5 ->  75.614  and loss:  857.505820274353
forward train acc: top1 ->  45.5078125 ; top5 ->  69.02734375  and loss:  509.9381465911865
test acc: top1 ->  52.252 ; top5 ->  76.314  and loss:  837.994390130043
forward train acc: top1 ->  45.32421875 ; top5 ->  69.51171875  and loss:  508.71140456199646
test acc: top1 ->  52.218 ; top5 ->  76.416  and loss:  838.0231981873512
forward train acc: top1 ->  45.42578125 ; top5 ->  69.828125  and loss:  502.96780955791473
test acc: top1 ->  52.15 ; top5 ->  76.31  and loss:  837.4443384408951
forward train acc: top1 ->  45.01171875 ; top5 ->  69.6640625  and loss:  507.6618618965149
test acc: top1 ->  52.474 ; top5 ->  76.528  and loss:  832.0044117569923
forward train acc: top1 ->  45.265625 ; top5 ->  69.65625  and loss:  505.40122866630554
test acc: top1 ->  52.46 ; top5 ->  76.67  and loss:  830.9473032951355
forward train acc: top1 ->  45.84375 ; top5 ->  70.1875  and loss:  500.31244564056396
test acc: top1 ->  52.642 ; top5 ->  76.724  and loss:  828.7263780236244
forward train acc: top1 ->  46.203125 ; top5 ->  70.4140625  and loss:  496.73212802410126
test acc: top1 ->  52.752 ; top5 ->  76.732  and loss:  829.5872863531113
forward train acc: top1 ->  46.13671875 ; top5 ->  70.45703125  and loss:  497.7904291152954
test acc: top1 ->  52.72 ; top5 ->  76.788  and loss:  828.5731219649315
forward train acc: top1 ->  46.17578125 ; top5 ->  70.015625  and loss:  498.8764524459839
test acc: top1 ->  52.79 ; top5 ->  76.778  and loss:  828.1365906596184
forward train acc: top1 ->  46.4921875 ; top5 ->  69.68359375  and loss:  497.34305143356323
test acc: top1 ->  52.92 ; top5 ->  76.882  and loss:  825.3003159165382
forward train acc: top1 ->  46.75390625 ; top5 ->  70.48828125  and loss:  493.6865813732147
test acc: top1 ->  52.894 ; top5 ->  76.908  and loss:  825.126569211483
forward train acc: top1 ->  46.16015625 ; top5 ->  69.9375  and loss:  499.13299894332886
test acc: top1 ->  52.904 ; top5 ->  76.962  and loss:  825.0776765346527
forward train acc: top1 ->  45.91015625 ; top5 ->  69.75  and loss:  502.32479310035706
test acc: top1 ->  52.9 ; top5 ->  76.93  and loss:  825.1104323267937
forward train acc: top1 ->  46.21875 ; top5 ->  70.42578125  and loss:  496.78210186958313
test acc: top1 ->  52.94 ; top5 ->  76.922  and loss:  824.3338489532471
forward train acc: top1 ->  46.4296875 ; top5 ->  70.1953125  and loss:  497.71665024757385
test acc: top1 ->  52.976 ; top5 ->  76.854  and loss:  823.8571889400482
forward train acc: top1 ->  46.41796875 ; top5 ->  70.15234375  and loss:  499.0555441379547
test acc: top1 ->  52.982 ; top5 ->  76.908  and loss:  823.773741543293
forward train acc: top1 ->  45.8671875 ; top5 ->  69.97265625  and loss:  497.8919584751129
test acc: top1 ->  52.992 ; top5 ->  76.94  and loss:  823.8762568235397
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  384 / 384 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -124.06399869918823 , diff:  124.06399869918823
adv train loss:  -124.96680057048798 , diff:  0.9028018712997437
layer  3  adv train finish, try to retain  253
test acc: top1 ->  52.178 ; top5 ->  76.322  and loss:  834.2269966602325
forward train acc: top1 ->  44.6640625 ; top5 ->  69.01953125  and loss:  512.7818648815155
test acc: top1 ->  51.452 ; top5 ->  75.892  and loss:  851.3682254552841
forward train acc: top1 ->  44.21484375 ; top5 ->  68.84375  and loss:  516.278243303299
test acc: top1 ->  51.298 ; top5 ->  75.634  and loss:  856.8654807209969
forward train acc: top1 ->  44.3828125 ; top5 ->  68.734375  and loss:  516.6696741580963
test acc: top1 ->  51.294 ; top5 ->  75.606  and loss:  859.4035009741783
forward train acc: top1 ->  44.39453125 ; top5 ->  69.0859375  and loss:  515.0792558193207
test acc: top1 ->  51.98 ; top5 ->  76.206  and loss:  845.0813288092613
forward train acc: top1 ->  45.046875 ; top5 ->  68.8046875  and loss:  509.7600588798523
test acc: top1 ->  52.136 ; top5 ->  76.266  and loss:  841.4615157842636
forward train acc: top1 ->  45.57421875 ; top5 ->  70.046875  and loss:  501.87460577487946
test acc: top1 ->  52.1 ; top5 ->  76.312  and loss:  839.0051869153976
forward train acc: top1 ->  44.85546875 ; top5 ->  69.30859375  and loss:  509.24293410778046
test acc: top1 ->  52.5 ; top5 ->  76.644  and loss:  832.5804440379143
forward train acc: top1 ->  45.4609375 ; top5 ->  69.5390625  and loss:  504.65573382377625
test acc: top1 ->  52.488 ; top5 ->  76.566  and loss:  833.150189101696
forward train acc: top1 ->  46.15625 ; top5 ->  70.19921875  and loss:  500.6838490962982
test acc: top1 ->  52.614 ; top5 ->  76.646  and loss:  831.5111778378487
forward train acc: top1 ->  45.89453125 ; top5 ->  69.5546875  and loss:  501.66474211215973
test acc: top1 ->  52.772 ; top5 ->  76.698  and loss:  827.5117147564888
forward train acc: top1 ->  46.16796875 ; top5 ->  70.28125  and loss:  496.7007089853287
test acc: top1 ->  52.754 ; top5 ->  76.708  and loss:  829.6154618263245
forward train acc: top1 ->  46.60546875 ; top5 ->  70.265625  and loss:  496.72326016426086
test acc: top1 ->  52.742 ; top5 ->  76.7  and loss:  827.4468272924423
forward train acc: top1 ->  46.046875 ; top5 ->  70.265625  and loss:  495.71821093559265
test acc: top1 ->  52.814 ; top5 ->  76.684  and loss:  826.2089157104492
forward train acc: top1 ->  45.8203125 ; top5 ->  69.70703125  and loss:  502.07125449180603
test acc: top1 ->  52.792 ; top5 ->  76.794  and loss:  826.1457608938217
forward train acc: top1 ->  46.30859375 ; top5 ->  70.1953125  and loss:  499.90960717201233
test acc: top1 ->  52.946 ; top5 ->  76.766  and loss:  826.375481903553
forward train acc: top1 ->  46.3671875 ; top5 ->  69.89453125  and loss:  498.6096034049988
test acc: top1 ->  52.964 ; top5 ->  76.818  and loss:  824.8995642066002
forward train acc: top1 ->  45.9375 ; top5 ->  69.86328125  and loss:  502.2054091691971
test acc: top1 ->  52.988 ; top5 ->  76.874  and loss:  825.6161397695541
forward train acc: top1 ->  46.0390625 ; top5 ->  70.609375  and loss:  495.0319176912308
test acc: top1 ->  52.962 ; top5 ->  76.88  and loss:  825.3022872805595
forward train acc: top1 ->  45.8828125 ; top5 ->  70.34765625  and loss:  496.90036511421204
test acc: top1 ->  53.028 ; top5 ->  76.884  and loss:  824.7206301093102
forward train acc: top1 ->  46.078125 ; top5 ->  69.875  and loss:  500.51705038547516
test acc: top1 ->  53.034 ; top5 ->  76.924  and loss:  824.6997636556625
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -125.7378556728363 , diff:  125.7378556728363
adv train loss:  -126.34115266799927 , diff:  0.6032969951629639
layer  4  adv train finish, try to retain  252
test acc: top1 ->  52.52 ; top5 ->  76.468  and loss:  836.6219471693039
forward train acc: top1 ->  45.13671875 ; top5 ->  69.4140625  and loss:  511.881228685379
test acc: top1 ->  51.568 ; top5 ->  75.988  and loss:  847.8671231269836
forward train acc: top1 ->  44.34375 ; top5 ->  68.6953125  and loss:  516.7015364170074
test acc: top1 ->  51.498 ; top5 ->  75.732  and loss:  854.3457860946655
forward train acc: top1 ->  44.62109375 ; top5 ->  68.6953125  and loss:  516.0869460105896
test acc: top1 ->  51.526 ; top5 ->  75.814  and loss:  855.8855535387993
forward train acc: top1 ->  45.4296875 ; top5 ->  69.57421875  and loss:  510.2658483982086
test acc: top1 ->  51.894 ; top5 ->  76.168  and loss:  846.9811123013496
forward train acc: top1 ->  45.796875 ; top5 ->  69.83984375  and loss:  504.55612349510193
test acc: top1 ->  52.296 ; top5 ->  76.42  and loss:  839.5778439640999
forward train acc: top1 ->  44.7265625 ; top5 ->  69.30078125  and loss:  511.863317489624
test acc: top1 ->  52.222 ; top5 ->  76.376  and loss:  841.0591169595718
forward train acc: top1 ->  45.375 ; top5 ->  69.33984375  and loss:  508.0877672433853
test acc: top1 ->  52.52 ; top5 ->  76.588  and loss:  832.9909833073616
forward train acc: top1 ->  45.51953125 ; top5 ->  69.53515625  and loss:  508.0755432844162
test acc: top1 ->  52.584 ; top5 ->  76.658  and loss:  832.3805747032166
forward train acc: top1 ->  46.0234375 ; top5 ->  69.734375  and loss:  502.8646230697632
test acc: top1 ->  52.802 ; top5 ->  76.756  and loss:  829.2885138988495
forward train acc: top1 ->  45.62109375 ; top5 ->  69.9453125  and loss:  502.0861645936966
test acc: top1 ->  52.812 ; top5 ->  76.74  and loss:  827.8279024362564
forward train acc: top1 ->  45.74609375 ; top5 ->  69.95703125  and loss:  498.47167789936066
test acc: top1 ->  52.752 ; top5 ->  76.728  and loss:  828.3124008774757
forward train acc: top1 ->  45.6875 ; top5 ->  70.1796875  and loss:  499.7703061103821
test acc: top1 ->  52.88 ; top5 ->  76.878  and loss:  826.4735642671585
forward train acc: top1 ->  46.0 ; top5 ->  70.4609375  and loss:  495.7525727748871
test acc: top1 ->  52.824 ; top5 ->  76.77  and loss:  826.5224168300629
forward train acc: top1 ->  45.7890625 ; top5 ->  69.8203125  and loss:  504.27501225471497
test acc: top1 ->  52.744 ; top5 ->  76.864  and loss:  827.3588203191757
forward train acc: top1 ->  45.71484375 ; top5 ->  70.3125  and loss:  497.26146376132965
test acc: top1 ->  52.954 ; top5 ->  76.922  and loss:  824.9986335039139
forward train acc: top1 ->  46.0625 ; top5 ->  70.25390625  and loss:  496.2427896261215
test acc: top1 ->  52.894 ; top5 ->  76.868  and loss:  824.8196396827698
forward train acc: top1 ->  45.73046875 ; top5 ->  69.9921875  and loss:  498.79870414733887
test acc: top1 ->  52.93 ; top5 ->  76.878  and loss:  824.3663245439529
forward train acc: top1 ->  46.6015625 ; top5 ->  70.23046875  and loss:  497.8065470457077
test acc: top1 ->  52.884 ; top5 ->  76.864  and loss:  824.0711041688919
forward train acc: top1 ->  45.890625 ; top5 ->  69.7421875  and loss:  500.62812972068787
test acc: top1 ->  52.94 ; top5 ->  76.864  and loss:  824.3729110956192
forward train acc: top1 ->  45.86328125 ; top5 ->  70.0859375  and loss:  499.79755902290344
test acc: top1 ->  52.896 ; top5 ->  76.874  and loss:  824.2715464234352
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -124.80365443229675 , diff:  124.80365443229675
adv train loss:  -125.66144704818726 , diff:  0.8577926158905029
************ all values are small in this layer **********
layer  5  adv train finish, try to retain  9215
test acc: top1 ->  52.876 ; top5 ->  76.794  and loss:  823.9742931723595
forward train acc: top1 ->  45.1796875 ; top5 ->  69.140625  and loss:  512.1046007871628
test acc: top1 ->  51.73 ; top5 ->  75.914  and loss:  849.8739466667175
forward train acc: top1 ->  44.87109375 ; top5 ->  69.0546875  and loss:  510.04762172698975
test acc: top1 ->  51.99 ; top5 ->  76.208  and loss:  842.7194700241089
forward train acc: top1 ->  44.6796875 ; top5 ->  68.3828125  and loss:  514.021958231926
test acc: top1 ->  51.522 ; top5 ->  75.836  and loss:  853.4921931028366
forward train acc: top1 ->  44.85546875 ; top5 ->  69.1796875  and loss:  514.6711381673813
test acc: top1 ->  51.836 ; top5 ->  76.264  and loss:  844.7477989792824
forward train acc: top1 ->  45.9296875 ; top5 ->  69.69921875  and loss:  503.09398126602173
test acc: top1 ->  51.968 ; top5 ->  76.408  and loss:  839.6828108429909
forward train acc: top1 ->  45.2890625 ; top5 ->  69.8828125  and loss:  503.1894357204437
test acc: top1 ->  52.264 ; top5 ->  76.474  and loss:  839.6805422902107
forward train acc: top1 ->  45.8359375 ; top5 ->  70.21484375  and loss:  499.5222773551941
test acc: top1 ->  52.472 ; top5 ->  76.642  and loss:  832.5727836489677
forward train acc: top1 ->  45.55078125 ; top5 ->  69.98828125  and loss:  503.93485617637634
test acc: top1 ->  52.594 ; top5 ->  76.678  and loss:  832.3593217134476
forward train acc: top1 ->  45.8671875 ; top5 ->  70.1953125  and loss:  499.1741166114807
test acc: top1 ->  52.806 ; top5 ->  76.836  and loss:  827.4838021397591
forward train acc: top1 ->  45.59375 ; top5 ->  69.98046875  and loss:  505.64067804813385
test acc: top1 ->  52.73 ; top5 ->  76.798  and loss:  829.3907935619354
forward train acc: top1 ->  46.140625 ; top5 ->  70.0625  and loss:  498.70451867580414
test acc: top1 ->  52.844 ; top5 ->  76.856  and loss:  827.6599968075752
forward train acc: top1 ->  46.3515625 ; top5 ->  70.50390625  and loss:  496.3326542377472
test acc: top1 ->  52.866 ; top5 ->  76.96  and loss:  825.3883712291718
forward train acc: top1 ->  46.07421875 ; top5 ->  69.9921875  and loss:  500.28523886203766
test acc: top1 ->  52.936 ; top5 ->  76.97  and loss:  825.3326662778854
forward train acc: top1 ->  46.44140625 ; top5 ->  70.50390625  and loss:  496.41847038269043
test acc: top1 ->  52.948 ; top5 ->  76.998  and loss:  825.3447501659393
forward train acc: top1 ->  46.30859375 ; top5 ->  70.23046875  and loss:  495.6237471103668
test acc: top1 ->  52.968 ; top5 ->  76.942  and loss:  825.142967402935
forward train acc: top1 ->  46.58984375 ; top5 ->  70.60546875  and loss:  493.6177899837494
test acc: top1 ->  52.974 ; top5 ->  76.936  and loss:  824.8159885406494
forward train acc: top1 ->  46.8046875 ; top5 ->  70.4765625  and loss:  493.1017333269119
test acc: top1 ->  53.014 ; top5 ->  76.976  and loss:  824.0466050505638
forward train acc: top1 ->  45.76953125 ; top5 ->  70.18359375  and loss:  497.86134803295135
test acc: top1 ->  52.98 ; top5 ->  77.044  and loss:  824.0454072356224
forward train acc: top1 ->  47.0390625 ; top5 ->  70.99609375  and loss:  492.8967092037201
test acc: top1 ->  52.98 ; top5 ->  76.986  and loss:  823.8228432536125
forward train acc: top1 ->  46.0390625 ; top5 ->  69.94921875  and loss:  500.22286438941956
test acc: top1 ->  53.032 ; top5 ->  77.026  and loss:  823.84706813097
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  9216 / 9216 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -125.26563215255737 , diff:  125.26563215255737
adv train loss:  -122.51689577102661 , diff:  2.7487363815307617
layer  6  adv train finish, try to retain  4087
test acc: top1 ->  53.048 ; top5 ->  77.028  and loss:  823.9524717926979
forward train acc: top1 ->  44.15234375 ; top5 ->  68.734375  and loss:  518.2282152175903
test acc: top1 ->  52.082 ; top5 ->  76.142  and loss:  846.4875562787056
forward train acc: top1 ->  44.6640625 ; top5 ->  69.21875  and loss:  512.7731578350067
test acc: top1 ->  51.72 ; top5 ->  76.02  and loss:  850.1502385139465
forward train acc: top1 ->  44.1796875 ; top5 ->  68.5390625  and loss:  519.7513728141785
test acc: top1 ->  51.694 ; top5 ->  76.034  and loss:  847.1543626785278
forward train acc: top1 ->  44.88671875 ; top5 ->  69.19140625  and loss:  511.5427579879761
test acc: top1 ->  52.022 ; top5 ->  76.23  and loss:  842.5398387312889
forward train acc: top1 ->  45.25 ; top5 ->  69.546875  and loss:  508.33352506160736
test acc: top1 ->  52.304 ; top5 ->  76.332  and loss:  837.409971177578
forward train acc: top1 ->  45.15234375 ; top5 ->  68.9453125  and loss:  509.9265310764313
test acc: top1 ->  52.216 ; top5 ->  76.354  and loss:  839.3489307165146
forward train acc: top1 ->  45.77734375 ; top5 ->  69.9296875  and loss:  500.05357336997986
test acc: top1 ->  52.65 ; top5 ->  76.63  and loss:  832.0840345025063
forward train acc: top1 ->  46.3203125 ; top5 ->  70.6953125  and loss:  493.58091127872467
test acc: top1 ->  52.716 ; top5 ->  76.618  and loss:  829.5247836709023
forward train acc: top1 ->  46.23828125 ; top5 ->  70.1953125  and loss:  499.2371007204056
test acc: top1 ->  52.798 ; top5 ->  76.8  and loss:  828.1152052879333
forward train acc: top1 ->  46.05859375 ; top5 ->  70.0625  and loss:  501.2256369590759
test acc: top1 ->  52.888 ; top5 ->  76.728  and loss:  827.84610247612
forward train acc: top1 ->  46.12109375 ; top5 ->  69.76171875  and loss:  500.13256549835205
test acc: top1 ->  52.756 ; top5 ->  76.83  and loss:  828.6844226717949
forward train acc: top1 ->  46.1015625 ; top5 ->  70.4375  and loss:  496.3880949020386
test acc: top1 ->  52.85 ; top5 ->  76.838  and loss:  826.7573632001877
forward train acc: top1 ->  45.90625 ; top5 ->  70.04296875  and loss:  501.0765166282654
test acc: top1 ->  52.982 ; top5 ->  76.89  and loss:  824.773815870285
forward train acc: top1 ->  46.20703125 ; top5 ->  70.00390625  and loss:  497.79129934310913
test acc: top1 ->  52.974 ; top5 ->  76.914  and loss:  824.6097912788391
forward train acc: top1 ->  46.23046875 ; top5 ->  70.4453125  and loss:  494.47606694698334
test acc: top1 ->  53.044 ; top5 ->  76.918  and loss:  822.6620553135872
forward train acc: top1 ->  46.56640625 ; top5 ->  70.40234375  and loss:  496.83853101730347
test acc: top1 ->  53.044 ; top5 ->  76.972  and loss:  822.8848590254784
forward train acc: top1 ->  46.3359375 ; top5 ->  70.15234375  and loss:  495.9638910293579
test acc: top1 ->  53.068 ; top5 ->  76.996  and loss:  822.5647051930428
forward train acc: top1 ->  45.76171875 ; top5 ->  70.0  and loss:  499.41327953338623
test acc: top1 ->  53.112 ; top5 ->  76.996  and loss:  822.3310388326645
forward train acc: top1 ->  46.33203125 ; top5 ->  70.4453125  and loss:  495.3257945775986
test acc: top1 ->  53.032 ; top5 ->  77.022  and loss:  822.3647534251213
forward train acc: top1 ->  46.0625 ; top5 ->  69.890625  and loss:  499.5494210720062
test acc: top1 ->  53.074 ; top5 ->  77.012  and loss:  822.1837936043739
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  4096 / 4096 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -126.351527094841 , diff:  126.351527094841
adv train loss:  -121.09065508842468 , diff:  5.260872006416321
adv train loss:  -121.15592515468597 , diff:  0.0652700662612915
layer  7  adv train finish, try to retain  4014
test acc: top1 ->  53.042 ; top5 ->  76.988  and loss:  822.4731624126434
forward train acc: top1 ->  44.44140625 ; top5 ->  68.609375  and loss:  517.4265038967133
test acc: top1 ->  51.53 ; top5 ->  75.856  and loss:  850.4846986532211
forward train acc: top1 ->  44.42578125 ; top5 ->  69.3125  and loss:  510.3517277240753
test acc: top1 ->  51.74 ; top5 ->  76.092  and loss:  844.3584942221642
forward train acc: top1 ->  44.3125 ; top5 ->  69.09375  and loss:  514.0446450710297
test acc: top1 ->  51.37 ; top5 ->  75.708  and loss:  856.5739527344704
forward train acc: top1 ->  44.94921875 ; top5 ->  68.9765625  and loss:  513.4597308635712
test acc: top1 ->  52.0 ; top5 ->  76.252  and loss:  846.9103674292564
forward train acc: top1 ->  44.64453125 ; top5 ->  69.16015625  and loss:  510.1742789745331
test acc: top1 ->  52.336 ; top5 ->  76.378  and loss:  836.8925094604492
forward train acc: top1 ->  44.890625 ; top5 ->  69.29296875  and loss:  507.8755991458893
test acc: top1 ->  52.106 ; top5 ->  76.364  and loss:  839.2818167805672
forward train acc: top1 ->  45.34375 ; top5 ->  69.33203125  and loss:  504.9280912876129
test acc: top1 ->  52.564 ; top5 ->  76.722  and loss:  830.2232680916786
forward train acc: top1 ->  45.71484375 ; top5 ->  69.69921875  and loss:  503.3869535923004
test acc: top1 ->  52.556 ; top5 ->  76.664  and loss:  831.8772272467613
forward train acc: top1 ->  46.328125 ; top5 ->  70.6015625  and loss:  494.214870929718
test acc: top1 ->  52.62 ; top5 ->  76.796  and loss:  828.4411627054214
forward train acc: top1 ->  45.640625 ; top5 ->  70.28125  and loss:  500.80259144306183
test acc: top1 ->  52.618 ; top5 ->  76.78  and loss:  830.3533871769905
forward train acc: top1 ->  45.6953125 ; top5 ->  69.90234375  and loss:  502.62589979171753
test acc: top1 ->  52.658 ; top5 ->  76.866  and loss:  828.0716685652733
forward train acc: top1 ->  46.0 ; top5 ->  69.921875  and loss:  498.1014542579651
test acc: top1 ->  52.718 ; top5 ->  76.902  and loss:  826.5711590051651
forward train acc: top1 ->  46.57421875 ; top5 ->  70.28515625  and loss:  496.3280109167099
test acc: top1 ->  52.876 ; top5 ->  76.958  and loss:  825.5936476588249
forward train acc: top1 ->  45.984375 ; top5 ->  70.03515625  and loss:  498.1500002145767
test acc: top1 ->  52.85 ; top5 ->  76.97  and loss:  824.6285046339035
forward train acc: top1 ->  46.4375 ; top5 ->  70.39453125  and loss:  498.42266488075256
test acc: top1 ->  52.818 ; top5 ->  76.974  and loss:  824.4474229812622
forward train acc: top1 ->  46.10546875 ; top5 ->  70.328125  and loss:  499.038268327713
test acc: top1 ->  52.848 ; top5 ->  76.988  and loss:  824.6582203507423
forward train acc: top1 ->  45.90234375 ; top5 ->  70.0  and loss:  498.5365699529648
test acc: top1 ->  52.862 ; top5 ->  77.024  and loss:  824.2096694111824
forward train acc: top1 ->  46.51953125 ; top5 ->  70.6328125  and loss:  495.28969967365265
test acc: top1 ->  52.904 ; top5 ->  77.056  and loss:  823.737189412117
forward train acc: top1 ->  46.2265625 ; top5 ->  69.8515625  and loss:  497.10154163837433
test acc: top1 ->  52.89 ; top5 ->  77.034  and loss:  823.538868188858
forward train acc: top1 ->  46.3203125 ; top5 ->  70.68359375  and loss:  492.651425242424
test acc: top1 ->  52.878 ; top5 ->  77.052  and loss:  823.1297624111176
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  4096 / 4096 , inc:  1
layer  0  :  0.984375  ==>  63 / 64 , inc:  1
layer  1  :  0.9947916666666666  ==>  191 / 192 , inc:  1
layer  2  :  1.0  ==>  384 / 384 , inc:  1
layer  3  :  1.0  ==>  256 / 256 , inc:  1
layer  4  :  1.0  ==>  256 / 256 , inc:  1
layer  5  :  1.0  ==>  9216 / 9216 , inc:  1
layer  6  :  1.0  ==>  4096 / 4096 , inc:  1
layer  7  :  1.0  ==>  4096 / 4096 , inc:  1
eps [0.0008342742919921876, 0.00013904571533203126, 2.607107162475586e-05, 3.910660743713379e-05, 3.910660743713379e-05, 1.0862946510314942e-06, 2.444162964820862e-06, 2.444162964820862e-06]  wait [4, 3, 4, 4, 4, 4, 4, 4]  inc [1, 1, 1, 1, 1, 1, 1, 1]  tol: 5
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  10  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -125.21390867233276 , diff:  125.21390867233276
adv train loss:  -129.30251622200012 , diff:  4.088607549667358
adv train loss:  -129.20894694328308 , diff:  0.09356927871704102
layer  0  adv train finish, try to retain  63
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -124.38938593864441 , diff:  124.38938593864441
adv train loss:  -121.69119691848755 , diff:  2.6981890201568604
layer  1  adv train finish, try to retain  181
test acc: top1 ->  51.5 ; top5 ->  75.694  and loss:  854.8873544931412
forward train acc: top1 ->  44.03515625 ; top5 ->  67.953125  and loss:  526.1045200824738
test acc: top1 ->  51.19 ; top5 ->  75.656  and loss:  858.2366098165512
forward train acc: top1 ->  43.6328125 ; top5 ->  68.3125  and loss:  521.2710602283478
test acc: top1 ->  50.88 ; top5 ->  75.314  and loss:  865.5249288082123
forward train acc: top1 ->  44.01171875 ; top5 ->  67.90625  and loss:  522.6317706108093
test acc: top1 ->  51.026 ; top5 ->  75.484  and loss:  861.4745684862137
forward train acc: top1 ->  44.51953125 ; top5 ->  68.64453125  and loss:  515.7550010681152
test acc: top1 ->  51.722 ; top5 ->  75.832  and loss:  849.2088313698769
forward train acc: top1 ->  45.1796875 ; top5 ->  69.0625  and loss:  511.25696325302124
test acc: top1 ->  52.028 ; top5 ->  76.242  and loss:  846.8758189082146
forward train acc: top1 ->  44.5859375 ; top5 ->  69.0390625  and loss:  515.281172990799
test acc: top1 ->  52.18 ; top5 ->  76.244  and loss:  844.0486419200897
forward train acc: top1 ->  44.74609375 ; top5 ->  69.484375  and loss:  507.7703334093094
test acc: top1 ->  52.342 ; top5 ->  76.372  and loss:  841.5070576071739
forward train acc: top1 ->  45.04296875 ; top5 ->  69.3984375  and loss:  509.1744632720947
test acc: top1 ->  52.518 ; top5 ->  76.482  and loss:  837.4101174473763
forward train acc: top1 ->  45.16796875 ; top5 ->  69.5703125  and loss:  507.4987864494324
test acc: top1 ->  52.514 ; top5 ->  76.62  and loss:  836.76288408041
forward train acc: top1 ->  45.46484375 ; top5 ->  69.31640625  and loss:  507.5248975753784
test acc: top1 ->  52.612 ; top5 ->  76.676  and loss:  834.0802945494652
forward train acc: top1 ->  44.92578125 ; top5 ->  68.99609375  and loss:  509.741051197052
test acc: top1 ->  52.572 ; top5 ->  76.744  and loss:  835.9343380331993
forward train acc: top1 ->  46.21484375 ; top5 ->  69.60546875  and loss:  505.0606850385666
test acc: top1 ->  52.582 ; top5 ->  76.71  and loss:  834.591969370842
forward train acc: top1 ->  45.53515625 ; top5 ->  69.57421875  and loss:  503.8361197710037
test acc: top1 ->  52.604 ; top5 ->  76.754  and loss:  831.8746755123138
forward train acc: top1 ->  45.66015625 ; top5 ->  70.00390625  and loss:  502.90210258960724
test acc: top1 ->  52.7 ; top5 ->  76.804  and loss:  831.7114608883858
forward train acc: top1 ->  45.7421875 ; top5 ->  69.41015625  and loss:  502.47601091861725
test acc: top1 ->  52.566 ; top5 ->  76.778  and loss:  831.7106549143791
forward train acc: top1 ->  45.70703125 ; top5 ->  69.6171875  and loss:  500.3037316799164
test acc: top1 ->  52.704 ; top5 ->  76.792  and loss:  830.6839464306831
forward train acc: top1 ->  46.25390625 ; top5 ->  70.046875  and loss:  500.8752394914627
test acc: top1 ->  52.704 ; top5 ->  76.806  and loss:  829.914864897728
forward train acc: top1 ->  45.84375 ; top5 ->  69.9453125  and loss:  499.84238719940186
test acc: top1 ->  52.696 ; top5 ->  76.822  and loss:  829.8250131011009
forward train acc: top1 ->  45.41796875 ; top5 ->  69.64453125  and loss:  500.2825086116791
test acc: top1 ->  52.7 ; top5 ->  76.83  and loss:  829.3448503613472
forward train acc: top1 ->  46.29296875 ; top5 ->  70.1015625  and loss:  498.46196830272675
test acc: top1 ->  52.68 ; top5 ->  76.826  and loss:  829.0588740110397
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  191 / 192 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -128.2740684747696 , diff:  128.2740684747696
adv train loss:  -126.98791074752808 , diff:  1.2861577272415161
layer  2  adv train finish, try to retain  384
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -129.7208149433136 , diff:  129.7208149433136
adv train loss:  -129.7768702507019 , diff:  0.056055307388305664
************ all values are small in this layer **********
layer  3  adv train finish, try to retain  255
test acc: top1 ->  51.402 ; top5 ->  75.794  and loss:  854.5161755681038
forward train acc: top1 ->  45.25390625 ; top5 ->  69.34375  and loss:  512.6641733646393
test acc: top1 ->  51.6 ; top5 ->  75.93  and loss:  853.9057934880257
forward train acc: top1 ->  44.8203125 ; top5 ->  69.0546875  and loss:  515.4560662508011
test acc: top1 ->  51.408 ; top5 ->  75.75  and loss:  857.875392973423
forward train acc: top1 ->  44.6484375 ; top5 ->  69.03125  and loss:  514.0331391096115
test acc: top1 ->  51.096 ; top5 ->  75.602  and loss:  857.4429974555969
forward train acc: top1 ->  45.0 ; top5 ->  69.30078125  and loss:  510.93500208854675
test acc: top1 ->  52.086 ; top5 ->  76.354  and loss:  841.0159704089165
forward train acc: top1 ->  45.6015625 ; top5 ->  69.43359375  and loss:  508.9035601615906
test acc: top1 ->  51.992 ; top5 ->  76.05  and loss:  844.2920795083046
forward train acc: top1 ->  45.296875 ; top5 ->  69.3359375  and loss:  508.0749499797821
test acc: top1 ->  52.106 ; top5 ->  76.282  and loss:  843.1412692666054
forward train acc: top1 ->  45.6796875 ; top5 ->  70.26171875  and loss:  501.1591407060623
test acc: top1 ->  52.602 ; top5 ->  76.524  and loss:  836.8862674832344
forward train acc: top1 ->  45.421875 ; top5 ->  69.93359375  and loss:  503.95125913619995
test acc: top1 ->  52.66 ; top5 ->  76.582  and loss:  833.8247833848
forward train acc: top1 ->  46.1484375 ; top5 ->  69.6328125  and loss:  504.30768406391144
test acc: top1 ->  52.72 ; top5 ->  76.628  and loss:  833.9763658642769
forward train acc: top1 ->  46.1953125 ; top5 ->  70.01953125  and loss:  498.732045173645
test acc: top1 ->  52.736 ; top5 ->  76.74  and loss:  830.2308659553528
forward train acc: top1 ->  46.1875 ; top5 ->  69.80078125  and loss:  503.12637090682983
test acc: top1 ->  52.694 ; top5 ->  76.73  and loss:  830.5176930427551
forward train acc: top1 ->  46.0546875 ; top5 ->  70.1875  and loss:  498.44555258750916
test acc: top1 ->  52.884 ; top5 ->  76.774  and loss:  830.3752940297127
forward train acc: top1 ->  46.09765625 ; top5 ->  69.953125  and loss:  499.43846917152405
test acc: top1 ->  52.858 ; top5 ->  76.886  and loss:  828.6174898147583
forward train acc: top1 ->  45.75 ; top5 ->  69.9609375  and loss:  500.1010152101517
test acc: top1 ->  53.016 ; top5 ->  76.884  and loss:  827.9904193282127
forward train acc: top1 ->  46.578125 ; top5 ->  70.4453125  and loss:  494.4479355812073
test acc: top1 ->  52.944 ; top5 ->  76.89  and loss:  826.9292387366295
forward train acc: top1 ->  45.8671875 ; top5 ->  69.9765625  and loss:  499.5731688737869
test acc: top1 ->  52.944 ; top5 ->  76.886  and loss:  826.2436410188675
forward train acc: top1 ->  46.12109375 ; top5 ->  70.04296875  and loss:  497.90251636505127
test acc: top1 ->  52.924 ; top5 ->  76.92  and loss:  825.5179518461227
forward train acc: top1 ->  46.24609375 ; top5 ->  70.38671875  and loss:  495.407737493515
test acc: top1 ->  52.964 ; top5 ->  76.942  and loss:  825.5793515443802
forward train acc: top1 ->  46.1015625 ; top5 ->  70.59765625  and loss:  495.520272731781
test acc: top1 ->  52.964 ; top5 ->  76.94  and loss:  825.3844240307808
forward train acc: top1 ->  46.07421875 ; top5 ->  70.01953125  and loss:  498.1341919898987
test acc: top1 ->  52.968 ; top5 ->  76.92  and loss:  825.5103306770325
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -123.30648040771484 , diff:  123.30648040771484
adv train loss:  -124.32034623622894 , diff:  1.0138658285140991
************ all values are small in this layer **********
layer  4  adv train finish, try to retain  255
test acc: top1 ->  52.804 ; top5 ->  76.798  and loss:  828.5232191681862
forward train acc: top1 ->  44.91796875 ; top5 ->  69.05859375  and loss:  511.6973121166229
test acc: top1 ->  51.572 ; top5 ->  75.826  and loss:  855.8903493881226
forward train acc: top1 ->  44.140625 ; top5 ->  68.515625  and loss:  519.0359848737717
test acc: top1 ->  51.476 ; top5 ->  75.882  and loss:  849.6174119710922
forward train acc: top1 ->  44.23046875 ; top5 ->  68.546875  and loss:  520.5524652004242
test acc: top1 ->  51.562 ; top5 ->  75.836  and loss:  852.6246852278709
forward train acc: top1 ->  44.5546875 ; top5 ->  68.95703125  and loss:  513.403836607933
test acc: top1 ->  52.152 ; top5 ->  76.118  and loss:  844.2367990016937
forward train acc: top1 ->  45.19140625 ; top5 ->  69.4375  and loss:  508.7620322704315
test acc: top1 ->  52.134 ; top5 ->  76.378  and loss:  840.7503705620766
forward train acc: top1 ->  45.33203125 ; top5 ->  69.40625  and loss:  507.5107834339142
test acc: top1 ->  52.066 ; top5 ->  76.256  and loss:  842.690470635891
forward train acc: top1 ->  45.4375 ; top5 ->  70.14453125  and loss:  501.2011390924454
test acc: top1 ->  52.364 ; top5 ->  76.446  and loss:  833.5410690307617
forward train acc: top1 ->  45.9765625 ; top5 ->  69.72265625  and loss:  503.7203314304352
test acc: top1 ->  52.552 ; top5 ->  76.562  and loss:  833.8641495108604
forward train acc: top1 ->  45.27734375 ; top5 ->  69.375  and loss:  507.75478410720825
test acc: top1 ->  52.612 ; top5 ->  76.51  and loss:  833.1796173453331
forward train acc: top1 ->  45.8515625 ; top5 ->  70.11328125  and loss:  499.33689069747925
test acc: top1 ->  52.714 ; top5 ->  76.7  and loss:  828.7604234814644
forward train acc: top1 ->  45.90625 ; top5 ->  70.171875  and loss:  497.63001561164856
test acc: top1 ->  52.728 ; top5 ->  76.702  and loss:  828.5736744999886
forward train acc: top1 ->  46.0390625 ; top5 ->  70.30859375  and loss:  498.65764915943146
test acc: top1 ->  52.876 ; top5 ->  76.774  and loss:  827.0944637060165
forward train acc: top1 ->  46.43359375 ; top5 ->  70.265625  and loss:  499.2261891365051
test acc: top1 ->  52.92 ; top5 ->  76.8  and loss:  826.5116241574287
forward train acc: top1 ->  45.8828125 ; top5 ->  70.1171875  and loss:  497.9841026067734
test acc: top1 ->  52.984 ; top5 ->  76.78  and loss:  826.6677886247635
forward train acc: top1 ->  46.59765625 ; top5 ->  70.4765625  and loss:  496.60842049121857
test acc: top1 ->  52.928 ; top5 ->  76.786  and loss:  826.5669276714325
forward train acc: top1 ->  45.890625 ; top5 ->  69.94921875  and loss:  497.59100461006165
test acc: top1 ->  52.936 ; top5 ->  76.842  and loss:  825.7986513972282
forward train acc: top1 ->  45.984375 ; top5 ->  70.25  and loss:  498.21195328235626
test acc: top1 ->  52.928 ; top5 ->  76.834  and loss:  825.2627946138382
forward train acc: top1 ->  46.01953125 ; top5 ->  70.33203125  and loss:  498.76612281799316
test acc: top1 ->  52.938 ; top5 ->  76.774  and loss:  825.6460862159729
forward train acc: top1 ->  46.2890625 ; top5 ->  69.96484375  and loss:  499.5496201515198
test acc: top1 ->  52.916 ; top5 ->  76.822  and loss:  825.6380071640015
forward train acc: top1 ->  46.21484375 ; top5 ->  70.52734375  and loss:  495.90778160095215
test acc: top1 ->  52.996 ; top5 ->  76.834  and loss:  824.7605931758881
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -122.72484147548676 , diff:  122.72484147548676
adv train loss:  -126.13317084312439 , diff:  3.4083293676376343
adv train loss:  -127.16400957107544 , diff:  1.0308387279510498
************ all values are small in this layer **********
layer  5  adv train finish, try to retain  9215
test acc: top1 ->  52.882 ; top5 ->  76.734  and loss:  826.116948902607
forward train acc: top1 ->  44.80859375 ; top5 ->  68.8671875  and loss:  513.0664467811584
test acc: top1 ->  51.47 ; top5 ->  75.766  and loss:  858.6866602897644
forward train acc: top1 ->  44.95703125 ; top5 ->  69.34765625  and loss:  507.4211724996567
test acc: top1 ->  51.702 ; top5 ->  75.796  and loss:  852.4960720539093
forward train acc: top1 ->  44.3046875 ; top5 ->  68.57421875  and loss:  519.5896818637848
test acc: top1 ->  51.206 ; top5 ->  75.694  and loss:  859.1920486092567
forward train acc: top1 ->  44.875 ; top5 ->  68.97265625  and loss:  513.1043610572815
test acc: top1 ->  52.14 ; top5 ->  76.126  and loss:  843.7879021763802
forward train acc: top1 ->  45.2109375 ; top5 ->  69.3203125  and loss:  508.62854623794556
test acc: top1 ->  52.098 ; top5 ->  76.31  and loss:  846.2358455061913
forward train acc: top1 ->  45.546875 ; top5 ->  69.51953125  and loss:  505.2968295812607
test acc: top1 ->  52.16 ; top5 ->  76.18  and loss:  842.212722837925
forward train acc: top1 ->  45.36328125 ; top5 ->  69.8046875  and loss:  506.09716415405273
test acc: top1 ->  52.51 ; top5 ->  76.568  and loss:  835.7831754684448
forward train acc: top1 ->  45.7734375 ; top5 ->  69.78515625  and loss:  505.15911769866943
test acc: top1 ->  52.464 ; top5 ->  76.536  and loss:  836.7200465202332
forward train acc: top1 ->  45.58984375 ; top5 ->  69.66015625  and loss:  501.9734597206116
test acc: top1 ->  52.66 ; top5 ->  76.694  and loss:  832.1191101670265
forward train acc: top1 ->  46.125 ; top5 ->  70.328125  and loss:  500.0773582458496
test acc: top1 ->  52.804 ; top5 ->  76.712  and loss:  827.0599880814552
forward train acc: top1 ->  45.83984375 ; top5 ->  69.890625  and loss:  502.3148076534271
test acc: top1 ->  52.732 ; top5 ->  76.762  and loss:  828.7289388775826
forward train acc: top1 ->  46.14453125 ; top5 ->  70.515625  and loss:  496.9003643989563
test acc: top1 ->  52.764 ; top5 ->  76.812  and loss:  828.4184465408325
forward train acc: top1 ->  46.734375 ; top5 ->  70.28125  and loss:  495.131400346756
test acc: top1 ->  52.926 ; top5 ->  76.824  and loss:  826.0764763355255
forward train acc: top1 ->  46.15625 ; top5 ->  70.42578125  and loss:  490.80047833919525
test acc: top1 ->  53.006 ; top5 ->  76.858  and loss:  824.3522154688835
forward train acc: top1 ->  45.75390625 ; top5 ->  69.6953125  and loss:  500.0553380250931
test acc: top1 ->  53.064 ; top5 ->  76.85  and loss:  824.6509832143784
forward train acc: top1 ->  45.70703125 ; top5 ->  70.046875  and loss:  501.14013373851776
test acc: top1 ->  53.036 ; top5 ->  76.864  and loss:  824.6111240386963
forward train acc: top1 ->  46.3203125 ; top5 ->  70.75390625  and loss:  492.7808231115341
test acc: top1 ->  52.986 ; top5 ->  76.924  and loss:  824.3520513176918
forward train acc: top1 ->  46.4140625 ; top5 ->  70.39453125  and loss:  493.48142445087433
test acc: top1 ->  53.036 ; top5 ->  76.922  and loss:  823.2168715000153
forward train acc: top1 ->  46.19140625 ; top5 ->  69.9140625  and loss:  498.8028585910797
test acc: top1 ->  53.042 ; top5 ->  76.946  and loss:  823.6302605271339
forward train acc: top1 ->  46.38671875 ; top5 ->  70.40234375  and loss:  495.3670496940613
test acc: top1 ->  53.016 ; top5 ->  76.934  and loss:  823.729918897152
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  9216 / 9216 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -122.1390974521637 , diff:  122.1390974521637
adv train loss:  -124.04695200920105 , diff:  1.9078545570373535
layer  6  adv train finish, try to retain  4091
test acc: top1 ->  53.01 ; top5 ->  76.936  and loss:  823.7146419286728
forward train acc: top1 ->  44.61328125 ; top5 ->  68.9765625  and loss:  513.9615194797516
test acc: top1 ->  51.956 ; top5 ->  76.0  and loss:  847.8849555253983
forward train acc: top1 ->  44.14453125 ; top5 ->  68.328125  and loss:  521.0328466892242
test acc: top1 ->  51.344 ; top5 ->  75.83  and loss:  857.4017211198807
forward train acc: top1 ->  44.0859375 ; top5 ->  68.33984375  and loss:  520.26149559021
test acc: top1 ->  51.104 ; top5 ->  75.574  and loss:  868.9372070431709
forward train acc: top1 ->  44.84765625 ; top5 ->  69.41015625  and loss:  510.4036592245102
test acc: top1 ->  52.182 ; top5 ->  76.358  and loss:  840.5106924176216
forward train acc: top1 ->  45.015625 ; top5 ->  69.6640625  and loss:  507.251336812973
test acc: top1 ->  51.95 ; top5 ->  76.196  and loss:  845.1430309414864
forward train acc: top1 ->  45.50390625 ; top5 ->  69.6015625  and loss:  505.577006816864
test acc: top1 ->  51.934 ; top5 ->  76.266  and loss:  843.2975959181786
forward train acc: top1 ->  45.14453125 ; top5 ->  69.45703125  and loss:  510.204185962677
test acc: top1 ->  52.342 ; top5 ->  76.406  and loss:  838.9340087175369
forward train acc: top1 ->  45.18359375 ; top5 ->  69.13671875  and loss:  509.6101665496826
test acc: top1 ->  52.438 ; top5 ->  76.46  and loss:  838.0299090147018
forward train acc: top1 ->  45.76171875 ; top5 ->  69.64453125  and loss:  505.9451632499695
test acc: top1 ->  52.536 ; top5 ->  76.566  and loss:  836.4034162163734
forward train acc: top1 ->  45.90625 ; top5 ->  69.8515625  and loss:  501.2215133905411
test acc: top1 ->  52.754 ; top5 ->  76.726  and loss:  831.1249226927757
forward train acc: top1 ->  46.6796875 ; top5 ->  70.60546875  and loss:  492.3379373550415
test acc: top1 ->  52.772 ; top5 ->  76.836  and loss:  826.9335460662842
forward train acc: top1 ->  45.82421875 ; top5 ->  69.92578125  and loss:  504.2449361085892
test acc: top1 ->  52.74 ; top5 ->  76.786  and loss:  828.5974757671356
forward train acc: top1 ->  46.4140625 ; top5 ->  70.0234375  and loss:  497.37605035305023
test acc: top1 ->  52.766 ; top5 ->  76.798  and loss:  827.963852584362
forward train acc: top1 ->  46.89453125 ; top5 ->  70.1875  and loss:  495.5741877555847
test acc: top1 ->  52.856 ; top5 ->  76.804  and loss:  827.6266803741455
forward train acc: top1 ->  46.20703125 ; top5 ->  70.26171875  and loss:  501.6643520593643
test acc: top1 ->  52.836 ; top5 ->  76.852  and loss:  827.4158541560173
forward train acc: top1 ->  46.12890625 ; top5 ->  70.30859375  and loss:  496.86186504364014
test acc: top1 ->  52.888 ; top5 ->  76.922  and loss:  826.8006363511086
forward train acc: top1 ->  45.73828125 ; top5 ->  70.00390625  and loss:  500.37441849708557
test acc: top1 ->  52.85 ; top5 ->  76.874  and loss:  826.7679067254066
forward train acc: top1 ->  46.08984375 ; top5 ->  70.2421875  and loss:  498.6942265033722
test acc: top1 ->  52.878 ; top5 ->  76.892  and loss:  826.7607239484787
forward train acc: top1 ->  46.7109375 ; top5 ->  70.53125  and loss:  493.69168651103973
test acc: top1 ->  52.866 ; top5 ->  76.868  and loss:  826.2872014045715
forward train acc: top1 ->  46.265625 ; top5 ->  70.1015625  and loss:  500.70539832115173
test acc: top1 ->  52.872 ; top5 ->  76.872  and loss:  825.8666974306107
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  4096 / 4096 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -124.41647148132324 , diff:  124.41647148132324
adv train loss:  -125.3408442735672 , diff:  0.9243727922439575
layer  7  adv train finish, try to retain  4022
test acc: top1 ->  52.872 ; top5 ->  76.872  and loss:  825.8666974306107
forward train acc: top1 ->  44.78515625 ; top5 ->  68.8515625  and loss:  515.8138806819916
test acc: top1 ->  51.9 ; top5 ->  75.99  and loss:  851.9094998836517
forward train acc: top1 ->  44.30078125 ; top5 ->  68.96875  and loss:  516.6834437847137
test acc: top1 ->  51.494 ; top5 ->  75.796  and loss:  856.2650647759438
forward train acc: top1 ->  44.4375 ; top5 ->  68.84765625  and loss:  516.906857252121
test acc: top1 ->  51.334 ; top5 ->  75.852  and loss:  856.4064149856567
forward train acc: top1 ->  44.80078125 ; top5 ->  69.15625  and loss:  508.0356310606003
test acc: top1 ->  51.772 ; top5 ->  76.076  and loss:  843.8039849996567
forward train acc: top1 ->  44.84375 ; top5 ->  68.796875  and loss:  513.6406046152115
test acc: top1 ->  52.0 ; top5 ->  76.13  and loss:  844.4531745314598
forward train acc: top1 ->  45.03125 ; top5 ->  69.40625  and loss:  511.42634630203247
test acc: top1 ->  52.188 ; top5 ->  76.216  and loss:  846.5157893896103
forward train acc: top1 ->  45.2421875 ; top5 ->  69.4296875  and loss:  506.8649524450302
test acc: top1 ->  52.478 ; top5 ->  76.502  and loss:  834.7474356889725
forward train acc: top1 ->  44.96484375 ; top5 ->  69.453125  and loss:  508.3686126470566
test acc: top1 ->  52.56 ; top5 ->  76.512  and loss:  837.5217192769051
forward train acc: top1 ->  45.625 ; top5 ->  70.0390625  and loss:  502.4940433502197
test acc: top1 ->  52.678 ; top5 ->  76.682  and loss:  832.4683251976967
forward train acc: top1 ->  45.50390625 ; top5 ->  69.9609375  and loss:  500.66749358177185
test acc: top1 ->  52.708 ; top5 ->  76.692  and loss:  830.4497694373131
forward train acc: top1 ->  45.9296875 ; top5 ->  69.57421875  and loss:  503.8474854230881
test acc: top1 ->  52.864 ; top5 ->  76.762  and loss:  830.7742413282394
forward train acc: top1 ->  46.140625 ; top5 ->  70.1796875  and loss:  497.75199007987976
test acc: top1 ->  52.742 ; top5 ->  76.748  and loss:  829.5762358307838
forward train acc: top1 ->  45.890625 ; top5 ->  69.9140625  and loss:  499.1085205078125
test acc: top1 ->  52.824 ; top5 ->  76.792  and loss:  828.6419520974159
forward train acc: top1 ->  46.22265625 ; top5 ->  70.34765625  and loss:  498.2335125207901
test acc: top1 ->  52.9 ; top5 ->  76.886  and loss:  827.3077819347382
forward train acc: top1 ->  46.2421875 ; top5 ->  69.85546875  and loss:  498.62041997909546
test acc: top1 ->  52.972 ; top5 ->  76.824  and loss:  826.8509835004807
forward train acc: top1 ->  45.7265625 ; top5 ->  70.44921875  and loss:  496.64726650714874
test acc: top1 ->  52.968 ; top5 ->  76.814  and loss:  826.7037352919579
forward train acc: top1 ->  45.99609375 ; top5 ->  70.17578125  and loss:  498.4068670272827
test acc: top1 ->  52.996 ; top5 ->  76.858  and loss:  826.1485306620598
forward train acc: top1 ->  46.3203125 ; top5 ->  70.27734375  and loss:  495.66489481925964
test acc: top1 ->  52.984 ; top5 ->  76.848  and loss:  825.9133768677711
forward train acc: top1 ->  46.10546875 ; top5 ->  70.39453125  and loss:  498.4950478076935
test acc: top1 ->  53.014 ; top5 ->  76.85  and loss:  826.5489203333855
forward train acc: top1 ->  46.859375 ; top5 ->  70.7421875  and loss:  491.397971868515
test acc: top1 ->  53.018 ; top5 ->  76.906  and loss:  826.0578910708427
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  4096 / 4096 , inc:  1
layer  0  :  0.984375  ==>  63 / 64 , inc:  1
layer  1  :  0.9947916666666666  ==>  191 / 192 , inc:  1
layer  2  :  1.0  ==>  384 / 384 , inc:  1
layer  3  :  1.0  ==>  256 / 256 , inc:  1
layer  4  :  1.0  ==>  256 / 256 , inc:  1
layer  5  :  1.0  ==>  9216 / 9216 , inc:  1
layer  6  :  1.0  ==>  4096 / 4096 , inc:  1
layer  7  :  1.0  ==>  4096 / 4096 , inc:  1
eps [0.0016685485839843751, 0.00010428428649902345, 5.214214324951172e-05, 2.9329955577850345e-05, 2.9329955577850345e-05, 8.147209882736207e-07, 1.8331222236156465e-06, 1.8331222236156465e-06]  wait [1, 2, 1, 3, 3, 3, 3, 3]  inc [1, 1, 1, 1, 1, 1, 1, 1]  tol: 6
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  11  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -126.69820547103882 , diff:  126.69820547103882
adv train loss:  -128.2908729314804 , diff:  1.5926674604415894
layer  0  adv train finish, try to retain  61
test acc: top1 ->  46.952 ; top5 ->  71.57  and loss:  951.2382290959358
forward train acc: top1 ->  42.66015625 ; top5 ->  66.83203125  and loss:  539.0288484096527
test acc: top1 ->  50.648 ; top5 ->  75.24  and loss:  871.2492381334305
forward train acc: top1 ->  43.61328125 ; top5 ->  67.53125  and loss:  529.5571660995483
test acc: top1 ->  50.636 ; top5 ->  75.206  and loss:  872.8674618005753
forward train acc: top1 ->  43.140625 ; top5 ->  67.3359375  and loss:  530.3263261318207
test acc: top1 ->  50.72 ; top5 ->  75.196  and loss:  869.3229464888573
forward train acc: top1 ->  44.58984375 ; top5 ->  68.625  and loss:  516.3724935054779
test acc: top1 ->  51.464 ; top5 ->  75.928  and loss:  854.3950839042664
forward train acc: top1 ->  44.7109375 ; top5 ->  68.80078125  and loss:  514.8502918481827
test acc: top1 ->  51.538 ; top5 ->  75.852  and loss:  853.207770049572
forward train acc: top1 ->  44.75 ; top5 ->  69.0390625  and loss:  513.9716838598251
test acc: top1 ->  51.732 ; top5 ->  75.8  and loss:  852.9000549912453
forward train acc: top1 ->  44.8515625 ; top5 ->  69.00390625  and loss:  512.2644150257111
test acc: top1 ->  51.972 ; top5 ->  76.058  and loss:  847.0694988965988
forward train acc: top1 ->  44.8984375 ; top5 ->  69.484375  and loss:  506.2690739631653
test acc: top1 ->  52.166 ; top5 ->  76.226  and loss:  842.6839435696602
forward train acc: top1 ->  45.78515625 ; top5 ->  69.65625  and loss:  500.56751215457916
test acc: top1 ->  52.092 ; top5 ->  76.274  and loss:  841.0033902525902
forward train acc: top1 ->  45.33203125 ; top5 ->  69.4609375  and loss:  506.6868269443512
test acc: top1 ->  52.278 ; top5 ->  76.368  and loss:  838.9956594705582
forward train acc: top1 ->  45.91796875 ; top5 ->  69.8359375  and loss:  504.9169294834137
test acc: top1 ->  52.354 ; top5 ->  76.508  and loss:  838.7109805345535
forward train acc: top1 ->  45.16015625 ; top5 ->  69.44140625  and loss:  509.67198729515076
test acc: top1 ->  52.348 ; top5 ->  76.416  and loss:  837.3464990258217
forward train acc: top1 ->  45.296875 ; top5 ->  69.51953125  and loss:  510.3683843612671
test acc: top1 ->  52.35 ; top5 ->  76.496  and loss:  837.9176578521729
forward train acc: top1 ->  45.3203125 ; top5 ->  69.60546875  and loss:  504.7885522842407
test acc: top1 ->  52.41 ; top5 ->  76.532  and loss:  837.9857801198959
forward train acc: top1 ->  45.44921875 ; top5 ->  69.671875  and loss:  506.90784645080566
test acc: top1 ->  52.514 ; top5 ->  76.568  and loss:  836.9782176017761
forward train acc: top1 ->  45.3515625 ; top5 ->  69.3671875  and loss:  506.9452918767929
test acc: top1 ->  52.442 ; top5 ->  76.546  and loss:  836.544703066349
forward train acc: top1 ->  45.8359375 ; top5 ->  69.703125  and loss:  503.67826545238495
test acc: top1 ->  52.528 ; top5 ->  76.616  and loss:  836.1705050468445
forward train acc: top1 ->  45.421875 ; top5 ->  69.80859375  and loss:  503.39373314380646
test acc: top1 ->  52.534 ; top5 ->  76.566  and loss:  836.4550242424011
forward train acc: top1 ->  46.09375 ; top5 ->  69.98046875  and loss:  498.12767469882965
test acc: top1 ->  52.556 ; top5 ->  76.646  and loss:  835.0643050074577
forward train acc: top1 ->  45.32421875 ; top5 ->  69.3203125  and loss:  509.43639063835144
test acc: top1 ->  52.532 ; top5 ->  76.624  and loss:  835.1112063527107
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  63 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -144.5855791568756 , diff:  144.5855791568756
adv train loss:  -146.18830490112305 , diff:  1.6027257442474365
layer  1  adv train finish, try to retain  190
test acc: top1 ->  45.182 ; top5 ->  70.108  and loss:  987.4978743195534
forward train acc: top1 ->  43.7421875 ; top5 ->  68.40234375  and loss:  524.3887929916382
test acc: top1 ->  51.218 ; top5 ->  75.764  and loss:  860.6129941344261
forward train acc: top1 ->  44.265625 ; top5 ->  69.1640625  and loss:  512.8292264938354
