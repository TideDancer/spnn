Files already downloaded and verified
Files already downloaded and verified
CONV_Mask(
  (net): VGG(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace)
      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (9): ReLU(inplace)
      (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (12): ReLU(inplace)
      (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (16): ReLU(inplace)
      (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (19): ReLU(inplace)
      (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (22): ReLU(inplace)
      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (26): ReLU(inplace)
      (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (29): ReLU(inplace)
      (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (32): ReLU(inplace)
      (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (36): ReLU(inplace)
      (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (39): ReLU(inplace)
      (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (42): ReLU(inplace)
      (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (44): AvgPool2d(kernel_size=1, stride=1, padding=0)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=512, bias=True)
      (1): ReLU(inplace)
      (2): Linear(in_features=512, out_features=10, bias=True)
    )
  )
  (mask): ParameterList(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 128 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 128 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
      (5): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
      (6): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
      (7): Parameter containing: [torch.cuda.FloatTensor of size 512 (GPU 0)]
      (8): Parameter containing: [torch.cuda.FloatTensor of size 512 (GPU 0)]
      (9): Parameter containing: [torch.cuda.FloatTensor of size 512 (GPU 0)]
      (10): Parameter containing: [torch.cuda.FloatTensor of size 512 (GPU 0)]
      (11): Parameter containing: [torch.cuda.FloatTensor of size 512 (GPU 0)]
      (12): Parameter containing: [torch.cuda.FloatTensor of size 512 (GPU 0)]
      (13): Parameter containing: [torch.cuda.FloatTensor of size 512 (GPU 0)]
    
  )
)
$$$$$$$$$$$$$ epoch  0  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -0.03476562594300958 , diff:  0.03476562594300958
adv train loss:  -0.024650766543345526 , diff:  0.010114859399664056
adv train loss:  -0.01964621522165544 , diff:  0.005004551321690087
layer  0  adv train finish, try to retain  32
test acc: top1 ->  91.91 ; top5 ->  99.12  and loss:  73.92063456773758
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.02525353637884109
test acc: top1 ->  91.92 ; top5 ->  99.12  and loss:  75.37740978598595
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.027062862269303878
test acc: top1 ->  91.86 ; top5 ->  99.09  and loss:  77.72602519392967
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.03550104494206607
test acc: top1 ->  91.91 ; top5 ->  99.08  and loss:  78.36499129235744
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03602937417144858
test acc: top1 ->  91.87 ; top5 ->  99.13  and loss:  78.85933436453342
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  64 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -0.04736949958169134 , diff:  0.04736949958169134
adv train loss:  -0.040176251181037514 , diff:  0.007193248400653829
layer  1  adv train finish, try to retain  55
test acc: top1 ->  91.87 ; top5 ->  99.12  and loss:  78.772779494524
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.026701810063968878
test acc: top1 ->  91.77 ; top5 ->  99.11  and loss:  80.95218397676945
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.013767037038633134
test acc: top1 ->  91.82 ; top5 ->  99.11  and loss:  82.44399669021368
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.0902902273046493
test acc: top1 ->  91.78 ; top5 ->  99.13  and loss:  80.81104677915573
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.03304103565005789
test acc: top1 ->  91.77 ; top5 ->  99.05  and loss:  81.54871974885464
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.04144399859796977
test acc: top1 ->  91.91 ; top5 ->  99.13  and loss:  80.54369157552719
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03216086259635631
test acc: top1 ->  91.84 ; top5 ->  99.12  and loss:  81.4322950989008
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.030565060758817708
test acc: top1 ->  91.91 ; top5 ->  99.11  and loss:  82.1489938646555
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.035699725383892655
test acc: top1 ->  91.89 ; top5 ->  99.1  and loss:  81.91054343432188
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.044543539099322516
test acc: top1 ->  91.78 ; top5 ->  99.14  and loss:  81.1030892431736
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.025059341070118535
test acc: top1 ->  91.78 ; top5 ->  99.13  and loss:  81.38669542968273
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  64 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -0.022670691228995565 , diff:  0.022670691228995565
adv train loss:  -0.03589342873192436 , diff:  0.013222737502928794
adv train loss:  -0.06051054284307611 , diff:  0.02461711411115175
adv train loss:  -0.02439240059925396 , diff:  0.03611814224382215
adv train loss:  -0.051730882445554016 , diff:  0.027338481846300056
adv train loss:  -0.025610615201003384 , diff:  0.026120267244550632
adv train loss:  -0.029144313009965117 , diff:  0.0035336978089617332
layer  2  adv train finish, try to retain  100
test acc: top1 ->  91.83 ; top5 ->  99.1  and loss:  82.02627782523632
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.034738312735498766
test acc: top1 ->  91.97 ; top5 ->  99.02  and loss:  84.75379639863968
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.05304791020898847
test acc: top1 ->  91.92 ; top5 ->  99.05  and loss:  84.36410351097584
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.025875878432998434
test acc: top1 ->  91.93 ; top5 ->  99.03  and loss:  84.24571141600609
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.027160961624758784
test acc: top1 ->  91.91 ; top5 ->  99.03  and loss:  84.41529473662376
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.05009131380870713
test acc: top1 ->  91.84 ; top5 ->  98.99  and loss:  85.41188733279705
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.042378236907097744
test acc: top1 ->  91.84 ; top5 ->  99.01  and loss:  83.51401886343956
forward train acc: top1 ->  99.97599997558594 ; top5 ->  100.0  and loss:  0.052472102106548846
test acc: top1 ->  91.94 ; top5 ->  99.01  and loss:  84.0101018100977
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.05375936457858188
test acc: top1 ->  92.04 ; top5 ->  99.05  and loss:  82.42119207978249
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.03558588671148755
test acc: top1 ->  92.01 ; top5 ->  99.04  and loss:  83.18162393569946
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.026369642738245602
test acc: top1 ->  92.0 ; top5 ->  99.05  and loss:  83.8590686917305
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  128 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -0.034753628555336036 , diff:  0.034753628555336036
adv train loss:  -0.028534014543765807 , diff:  0.006219614011570229
layer  3  adv train finish, try to retain  87
test acc: top1 ->  91.97 ; top5 ->  99.02  and loss:  83.55278444290161
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.03509562868566718
test acc: top1 ->  92.04 ; top5 ->  99.09  and loss:  84.0676833242178
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.06786015709803905
test acc: top1 ->  91.86 ; top5 ->  99.09  and loss:  83.82157178223133
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.0593106811902544
test acc: top1 ->  91.83 ; top5 ->  99.04  and loss:  84.40511628985405
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.02726732598966919
test acc: top1 ->  91.88 ; top5 ->  99.05  and loss:  84.85156482458115
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.052559006897354266
test acc: top1 ->  91.85 ; top5 ->  99.05  and loss:  83.52369871735573
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.032775475876405835
test acc: top1 ->  91.77 ; top5 ->  99.04  and loss:  83.77103744447231
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.02390193949577224
test acc: top1 ->  91.78 ; top5 ->  99.02  and loss:  84.1942979246378
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.04461301574337995
test acc: top1 ->  91.76 ; top5 ->  99.06  and loss:  82.69753213226795
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.026066140733689735
test acc: top1 ->  91.81 ; top5 ->  99.07  and loss:  83.23891055583954
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.0474137632700149
test acc: top1 ->  91.92 ; top5 ->  99.09  and loss:  83.15171141922474
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  128 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -0.024530934671929572 , diff:  0.024530934671929572
adv train loss:  -0.028544774955662433 , diff:  0.004013840283732861
layer  4  adv train finish, try to retain  154
test acc: top1 ->  91.75 ; top5 ->  99.12  and loss:  84.50141514837742
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.1297177936148728
test acc: top1 ->  91.84 ; top5 ->  99.04  and loss:  84.865915954113
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.09300102073757444
test acc: top1 ->  91.77 ; top5 ->  99.05  and loss:  83.70976503193378
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.0945828173244081
test acc: top1 ->  91.83 ; top5 ->  99.05  and loss:  83.9899090975523
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.062432877108221874
test acc: top1 ->  91.89 ; top5 ->  99.08  and loss:  84.42948822677135
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.06198156923528586
test acc: top1 ->  91.8 ; top5 ->  99.1  and loss:  83.339320987463
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -0.052539667802875556 , diff:  0.052539667802875556
adv train loss:  -0.08096855039184447 , diff:  0.028428882588968918
adv train loss:  -0.04857318496215157 , diff:  0.0323953654296929
adv train loss:  -0.07821060032574678 , diff:  0.02963741536359521
adv train loss:  -0.07442839434952475 , diff:  0.0037822059762220306
layer  5  adv train finish, try to retain  149
test acc: top1 ->  91.72 ; top5 ->  99.09  and loss:  83.63606449961662
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.02729110098152887
test acc: top1 ->  91.8 ; top5 ->  99.05  and loss:  85.60120779275894
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.02284535897342721
test acc: top1 ->  91.93 ; top5 ->  99.06  and loss:  85.71076004207134
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.03809017606181442
test acc: top1 ->  91.86 ; top5 ->  99.12  and loss:  86.72858625650406
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.024080331478899097
test acc: top1 ->  91.85 ; top5 ->  99.06  and loss:  87.41718478500843
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.017696076956781326
test acc: top1 ->  92.01 ; top5 ->  99.07  and loss:  86.00111661851406
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.0733140082004411
test acc: top1 ->  92.09 ; top5 ->  99.06  and loss:  84.87087355554104
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.03432836544561724
test acc: top1 ->  92.01 ; top5 ->  99.08  and loss:  85.21376478672028
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.02533871982450364
test acc: top1 ->  91.99 ; top5 ->  99.08  and loss:  85.12610998749733
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.03766294000934067
test acc: top1 ->  92.07 ; top5 ->  99.06  and loss:  84.64624665677547
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.0202737073705066
test acc: top1 ->  92.03 ; top5 ->  99.06  and loss:  85.06247788667679
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -0.03129231568527757 , diff:  0.03129231568527757
adv train loss:  -0.035492997245455626 , diff:  0.0042006815601780545
************ all values are small in this layer **********
layer  6  adv train finish, try to retain  255
test acc: top1 ->  92.02 ; top5 ->  99.08  and loss:  84.11253276467323
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.042832127037399914
test acc: top1 ->  91.93 ; top5 ->  99.08  and loss:  86.0029059201479
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.03230796347634168
test acc: top1 ->  91.84 ; top5 ->  99.05  and loss:  87.32777538895607
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.03917623946836102
test acc: top1 ->  92.09 ; top5 ->  99.05  and loss:  88.57659205794334
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.03432216939245336
test acc: top1 ->  92.02 ; top5 ->  99.08  and loss:  88.01662574708462
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.048453882249646085
test acc: top1 ->  91.9 ; top5 ->  99.05  and loss:  87.25504432618618
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.03995828286997494
test acc: top1 ->  91.92 ; top5 ->  99.07  and loss:  87.60583636164665
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.02287364591302321
test acc: top1 ->  91.95 ; top5 ->  99.04  and loss:  87.75892588496208
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.051524106616852805
test acc: top1 ->  92.01 ; top5 ->  99.1  and loss:  86.57241417467594
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.036874218632874545
test acc: top1 ->  92.02 ; top5 ->  99.1  and loss:  86.27251547574997
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.02691197035528603
test acc: top1 ->  91.99 ; top5 ->  99.09  and loss:  87.0130195170641
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -0.025035386275703786 , diff:  0.025035386275703786
adv train loss:  -0.04164168674122948 , diff:  0.016606300465525692
adv train loss:  -0.026127143249595974 , diff:  0.015514543491633503
adv train loss:  -0.03136401371375541 , diff:  0.005236870464159438
layer  7  adv train finish, try to retain  29
test acc: top1 ->  34.87 ; top5 ->  82.01  and loss:  338.90307331085205
forward train acc: top1 ->  99.13599997558593 ; top5 ->  99.99  and loss:  3.2022654260508716
test acc: top1 ->  90.75 ; top5 ->  98.73  and loss:  88.18346640467644
forward train acc: top1 ->  99.6300000024414 ; top5 ->  100.0  and loss:  1.3326624532346614
test acc: top1 ->  90.93 ; top5 ->  98.86  and loss:  85.67462649941444
forward train acc: top1 ->  99.6860000024414 ; top5 ->  99.998  and loss:  1.0795621337601915
test acc: top1 ->  91.09 ; top5 ->  98.99  and loss:  82.53329263627529
forward train acc: top1 ->  99.724 ; top5 ->  100.0  and loss:  0.8872497383854352
test acc: top1 ->  91.16 ; top5 ->  99.0  and loss:  80.91349086165428
forward train acc: top1 ->  99.77599997558593 ; top5 ->  100.0  and loss:  0.7133341333828866
test acc: top1 ->  91.29 ; top5 ->  99.02  and loss:  79.68129779398441
forward train acc: top1 ->  99.8 ; top5 ->  99.998  and loss:  0.6480971677810885
test acc: top1 ->  91.26 ; top5 ->  99.1  and loss:  78.86043855547905
forward train acc: top1 ->  99.808 ; top5 ->  100.0  and loss:  0.5561275199870579
test acc: top1 ->  91.23 ; top5 ->  99.04  and loss:  78.15686404705048
forward train acc: top1 ->  99.83199997558594 ; top5 ->  100.0  and loss:  0.5190982770291157
test acc: top1 ->  91.18 ; top5 ->  99.09  and loss:  78.19152647256851
forward train acc: top1 ->  99.81999997558594 ; top5 ->  99.998  and loss:  0.5319685677241068
test acc: top1 ->  91.29 ; top5 ->  99.1  and loss:  77.48344475030899
forward train acc: top1 ->  99.82599997558594 ; top5 ->  100.0  and loss:  0.5114014284627046
test acc: top1 ->  91.23 ; top5 ->  99.07  and loss:  77.13467782735825
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -0.12293166127346922 , diff:  0.12293166127346922
adv train loss:  -0.13618106124340557 , diff:  0.013249399969936348
adv train loss:  -0.13771422510035336 , diff:  0.0015331638569477946
layer  8  adv train finish, try to retain  15
test acc: top1 ->  37.8 ; top5 ->  89.05  and loss:  294.2111625671387
forward train acc: top1 ->  97.5760000024414 ; top5 ->  100.0  and loss:  8.198295117123052
test acc: top1 ->  90.71 ; top5 ->  98.98  and loss:  87.40139032900333
forward train acc: top1 ->  99.7 ; top5 ->  100.0  and loss:  0.9149843567283824
test acc: top1 ->  91.15 ; top5 ->  99.07  and loss:  85.29939769208431
forward train acc: top1 ->  99.804 ; top5 ->  100.0  and loss:  0.643231691501569
test acc: top1 ->  91.31 ; top5 ->  99.1  and loss:  84.20115900039673
forward train acc: top1 ->  99.84 ; top5 ->  100.0  and loss:  0.4656441828701645
test acc: top1 ->  91.28 ; top5 ->  99.09  and loss:  84.016129180789
forward train acc: top1 ->  99.87199997558594 ; top5 ->  100.0  and loss:  0.4149257843964733
test acc: top1 ->  91.44 ; top5 ->  99.12  and loss:  83.9947007894516
forward train acc: top1 ->  99.884 ; top5 ->  99.998  and loss:  0.43674080475466326
test acc: top1 ->  91.41 ; top5 ->  99.12  and loss:  83.89710196852684
forward train acc: top1 ->  99.88599997558593 ; top5 ->  100.0  and loss:  0.3600236285419669
test acc: top1 ->  91.45 ; top5 ->  99.12  and loss:  82.5204755961895
forward train acc: top1 ->  99.884 ; top5 ->  100.0  and loss:  0.37201484391698614
test acc: top1 ->  91.54 ; top5 ->  99.17  and loss:  82.5346786826849
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.24770528325461783
test acc: top1 ->  91.5 ; top5 ->  99.15  and loss:  82.64771321415901
forward train acc: top1 ->  99.914 ; top5 ->  100.0  and loss:  0.2603668336523697
test acc: top1 ->  91.51 ; top5 ->  99.17  and loss:  82.55915127694607
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -0.5197120030643418 , diff:  0.5197120030643418
adv train loss:  -0.47933660526177846 , diff:  0.040375397802563384
adv train loss:  -0.5439476212195586 , diff:  0.06461101595778018
adv train loss:  -0.5984684792347252 , diff:  0.054520858015166596
adv train loss:  -0.5493873382729362 , diff:  0.04908114096178906
adv train loss:  -0.5432274440245237 , diff:  0.006159894248412456
layer  9  adv train finish, try to retain  19
test acc: top1 ->  27.17 ; top5 ->  74.18  and loss:  767.0553245544434
forward train acc: top1 ->  99.89199997558593 ; top5 ->  100.0  and loss:  0.35023261394235305
test acc: top1 ->  91.56 ; top5 ->  99.1  and loss:  80.46255362033844
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.1029257238842547
test acc: top1 ->  91.74 ; top5 ->  99.07  and loss:  80.6727492660284
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.09211881268129218
test acc: top1 ->  91.72 ; top5 ->  99.09  and loss:  80.12285801768303
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.08748968516738387
test acc: top1 ->  91.78 ; top5 ->  99.11  and loss:  80.66821609437466
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.05009488711584709
test acc: top1 ->  91.74 ; top5 ->  99.1  and loss:  80.61386732757092
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.06587216423940845
test acc: top1 ->  91.77 ; top5 ->  99.07  and loss:  80.69383281469345
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.052697671388159506
test acc: top1 ->  91.75 ; top5 ->  99.03  and loss:  81.25972324609756
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.06644798417710263
test acc: top1 ->  91.76 ; top5 ->  99.06  and loss:  81.14196437597275
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.03999942484369967
test acc: top1 ->  91.79 ; top5 ->  99.11  and loss:  81.0485276132822
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.08737776690031751
test acc: top1 ->  91.83 ; top5 ->  99.1  and loss:  80.12340952455997
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -0.4054338685818948 , diff:  0.4054338685818948
adv train loss:  -0.4124004592304118 , diff:  0.006966590648517013
layer  10  adv train finish, try to retain  15
test acc: top1 ->  11.63 ; top5 ->  78.15  and loss:  941.5263752937317
forward train acc: top1 ->  99.516 ; top5 ->  100.0  and loss:  1.597463905258337
test acc: top1 ->  91.29 ; top5 ->  99.01  and loss:  86.86793600022793
forward train acc: top1 ->  99.94999997558594 ; top5 ->  100.0  and loss:  0.14522593362198677
test acc: top1 ->  91.55 ; top5 ->  99.05  and loss:  88.2519479393959
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.10510511876782402
test acc: top1 ->  91.64 ; top5 ->  99.03  and loss:  88.02082544565201
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.05661450305706239
test acc: top1 ->  91.74 ; top5 ->  99.06  and loss:  87.56511117517948
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.07261013604875188
test acc: top1 ->  91.7 ; top5 ->  99.04  and loss:  88.52302615344524
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.039317846476478735
test acc: top1 ->  91.73 ; top5 ->  99.07  and loss:  88.22995790839195
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.027765217782871332
test acc: top1 ->  91.7 ; top5 ->  99.03  and loss:  88.65423618257046
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.0746029252113658
test acc: top1 ->  91.7 ; top5 ->  99.07  and loss:  88.85489919781685
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.05281980417112209
test acc: top1 ->  91.72 ; top5 ->  99.05  and loss:  88.34153497219086
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.08153265140208532
test acc: top1 ->  91.77 ; top5 ->  99.07  and loss:  88.14904986321926
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -0.24457791885652114 , diff:  0.24457791885652114
adv train loss:  -0.20999576348549454 , diff:  0.0345821553710266
adv train loss:  -0.22519942526560044 , diff:  0.015203661780105904
adv train loss:  -0.22729176304346765 , diff:  0.0020923377778672148
layer  11  adv train finish, try to retain  26
test acc: top1 ->  66.97 ; top5 ->  96.65  and loss:  199.26715779304504
forward train acc: top1 ->  98.90599997558594 ; top5 ->  100.0  and loss:  3.444378709187731
test acc: top1 ->  90.72 ; top5 ->  98.72  and loss:  89.88119441270828
forward train acc: top1 ->  99.814 ; top5 ->  100.0  and loss:  0.5929407792864367
test acc: top1 ->  91.15 ; top5 ->  98.83  and loss:  86.45736932754517
forward train acc: top1 ->  99.898 ; top5 ->  100.0  and loss:  0.3000033467251342
test acc: top1 ->  91.15 ; top5 ->  98.79  and loss:  85.44908332824707
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.25050571153406054
test acc: top1 ->  91.28 ; top5 ->  98.76  and loss:  85.36141926050186
forward train acc: top1 ->  99.942 ; top5 ->  100.0  and loss:  0.1818295201228466
test acc: top1 ->  91.37 ; top5 ->  98.85  and loss:  86.38134889304638
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.15027652980643325
test acc: top1 ->  91.37 ; top5 ->  98.82  and loss:  86.51243583858013
forward train acc: top1 ->  99.952 ; top5 ->  99.998  and loss:  0.1965724742913153
test acc: top1 ->  91.29 ; top5 ->  98.84  and loss:  86.14640575647354
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.13436776866728906
test acc: top1 ->  91.45 ; top5 ->  98.91  and loss:  85.46871528029442
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.15392904794134665
test acc: top1 ->  91.41 ; top5 ->  98.85  and loss:  85.2303577363491
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.15794454812566983
test acc: top1 ->  91.38 ; top5 ->  98.86  and loss:  85.65286032855511
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -0.21506185795442434 , diff:  0.21506185795442434
adv train loss:  -0.33812361187301576 , diff:  0.12306175391859142
adv train loss:  -0.2002355274016736 , diff:  0.13788808447134215
adv train loss:  -0.24029157129552914 , diff:  0.04005604389385553
adv train loss:  -0.21766565773577895 , diff:  0.022625913559750188
adv train loss:  -0.26422272603849706 , diff:  0.04655706830271811
adv train loss:  -0.22054596496673184 , diff:  0.04367676107176521
adv train loss:  -0.2838072806916898 , diff:  0.06326131572495797
adv train loss:  -0.31699805811513215 , diff:  0.03319077742344234
adv train loss:  -0.2435050906788092 , diff:  0.07349296743632294
layer  12  adv train finish, try to retain  15
test acc: top1 ->  82.3 ; top5 ->  98.36  and loss:  108.69870591163635
forward train acc: top1 ->  98.7660000024414 ; top5 ->  100.0  and loss:  3.8257391061633825
test acc: top1 ->  91.08 ; top5 ->  98.86  and loss:  68.67474621534348
forward train acc: top1 ->  99.912 ; top5 ->  100.0  and loss:  0.35854697169270366
test acc: top1 ->  91.31 ; top5 ->  98.92  and loss:  66.30549024045467
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.18004621934960596
test acc: top1 ->  91.31 ; top5 ->  98.89  and loss:  66.51100013405085
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.12290078477235511
test acc: top1 ->  91.5 ; top5 ->  98.9  and loss:  66.42395845800638
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.1008491399261402
test acc: top1 ->  91.48 ; top5 ->  98.89  and loss:  66.53201299160719
forward train acc: top1 ->  99.97799997558593 ; top5 ->  100.0  and loss:  0.10265346025698818
test acc: top1 ->  91.49 ; top5 ->  98.85  and loss:  66.76906479150057
forward train acc: top1 ->  99.97199997558593 ; top5 ->  100.0  and loss:  0.09203371942567173
test acc: top1 ->  91.46 ; top5 ->  98.87  and loss:  67.44247134029865
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.09952911373693496
test acc: top1 ->  91.51 ; top5 ->  98.91  and loss:  67.06095598638058
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.07733021328749601
test acc: top1 ->  91.5 ; top5 ->  98.86  and loss:  67.37215461581945
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.07043598737800494
test acc: top1 ->  91.6 ; top5 ->  98.86  and loss:  67.88302944600582
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -0.30356063008912315 , diff:  0.30356063008912315
adv train loss:  -0.24587485848996948 , diff:  0.05768577159915367
adv train loss:  -0.2957354163299897 , diff:  0.04986055784002019
adv train loss:  -0.27066885735985124 , diff:  0.02506655897013843
adv train loss:  -0.311221313833812 , diff:  0.040552456473960774
adv train loss:  -0.26673349600605434 , diff:  0.04448781782775768
adv train loss:  -0.3625415486530983 , diff:  0.09580805264704395
adv train loss:  -0.3551160859205993 , diff:  0.007425462732499
layer  13  adv train finish, try to retain  20
test acc: top1 ->  88.31 ; top5 ->  98.35  and loss:  108.20594128966331
forward train acc: top1 ->  99.618 ; top5 ->  100.0  and loss:  1.0697565853479318
test acc: top1 ->  91.45 ; top5 ->  98.78  and loss:  83.00600092113018
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.13252752920379862
test acc: top1 ->  91.68 ; top5 ->  98.78  and loss:  81.89798770844936
forward train acc: top1 ->  99.97999997558594 ; top5 ->  100.0  and loss:  0.09228656411869451
test acc: top1 ->  91.75 ; top5 ->  98.79  and loss:  80.48111359775066
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.053956496485625394
test acc: top1 ->  91.77 ; top5 ->  98.77  and loss:  80.64699724316597
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.06270329171456979
test acc: top1 ->  91.7 ; top5 ->  98.73  and loss:  81.7258501201868
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.05867206127368263
test acc: top1 ->  91.73 ; top5 ->  98.74  and loss:  81.11984023451805
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.02643280877964571
test acc: top1 ->  91.73 ; top5 ->  98.74  and loss:  81.34923581779003
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.03393813293223502
test acc: top1 ->  91.71 ; top5 ->  98.74  and loss:  81.8624302148819
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.025350281212013215
test acc: top1 ->  91.78 ; top5 ->  98.73  and loss:  82.35757833719254
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.048812366534548346
test acc: top1 ->  91.77 ; top5 ->  98.76  and loss:  82.7567441612482
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
layer  0  :  0  ==>  64 / 64 , inc:  1
layer  1  :  0  ==>  64 / 64 , inc:  1
layer  2  :  0  ==>  128 / 128 , inc:  1
layer  3  :  0  ==>  128 / 128 , inc:  1
layer  4  :  0  ==>  256 / 256 , inc:  1
layer  5  :  0  ==>  256 / 256 , inc:  1
layer  6  :  0  ==>  256 / 256 , inc:  1
layer  7  :  0  ==>  512 / 512 , inc:  1
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0  ==>  512 / 512 , inc:  1
layer  11  :  0  ==>  512 / 512 , inc:  1
layer  12  :  0  ==>  512 / 512 , inc:  1
layer  13  :  0  ==>  512 / 512 , inc:  1
eps [0.00075, 0.00075, 0.00075, 0.00075, 0.00075, 0.00075, 0.00075, 0.00075, 0.00075, 0.00075, 0.00075, 0.00075, 0.00075, 0.00075]  wait [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 0
$$$$$$$$$$$$$ epoch  1  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -0.3671416693687206 , diff:  0.3671416693687206
adv train loss:  -0.3986699061788386 , diff:  0.03152823681011796
adv train loss:  -0.3603041573878727 , diff:  0.03836574879096588
adv train loss:  -0.3752003522859013 , diff:  0.014896194898028625
adv train loss:  -0.2989164312612047 , diff:  0.07628392102469661
adv train loss:  -0.38289402523332683 , diff:  0.08397759397212212
adv train loss:  -0.31344389909281745 , diff:  0.06945012614050938
adv train loss:  -0.36367964453529567 , diff:  0.05023574544247822
adv train loss:  -0.3074530475519168 , diff:  0.05622659698337884
adv train loss:  -0.34851949581934605 , diff:  0.04106644826742922
layer  0  adv train finish, try to retain  32
test acc: top1 ->  91.12 ; top5 ->  98.88  and loss:  105.48283022642136
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.09915787782483676
test acc: top1 ->  91.7 ; top5 ->  98.9  and loss:  100.86432406306267
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.03360433117086359
test acc: top1 ->  91.81 ; top5 ->  98.92  and loss:  100.792201846838
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.03798598574576317
test acc: top1 ->  91.86 ; top5 ->  98.93  and loss:  99.25210577249527
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.0295648735773284
test acc: top1 ->  91.96 ; top5 ->  98.91  and loss:  101.0640075057745
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.019870160689606564
test acc: top1 ->  91.88 ; top5 ->  98.94  and loss:  100.26369625329971
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.037641665074261255
test acc: top1 ->  91.93 ; top5 ->  98.9  and loss:  100.15758065879345
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.008982083340015379
test acc: top1 ->  91.86 ; top5 ->  98.87  and loss:  100.76336824893951
forward train acc: top1 ->  99.99199997558594 ; top5 ->  100.0  and loss:  0.024440601344394963
test acc: top1 ->  91.95 ; top5 ->  98.89  and loss:  99.99457147717476
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.021508235466171755
test acc: top1 ->  91.94 ; top5 ->  98.92  and loss:  99.5538788586855
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.036547019582940266
test acc: top1 ->  91.97 ; top5 ->  98.96  and loss:  98.82237823307514
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  64 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -0.022867924912134185 , diff:  0.022867924912134185
adv train loss:  -0.027804492247014423 , diff:  0.0049365673348802375
layer  1  adv train finish, try to retain  55
test acc: top1 ->  91.95 ; top5 ->  98.95  and loss:  98.47541715204716
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.02283822677918579
test acc: top1 ->  91.91 ; top5 ->  98.85  and loss:  100.2891126871109
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.018207890879622823
test acc: top1 ->  91.8 ; top5 ->  98.89  and loss:  99.93099583685398
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.026649866988009308
test acc: top1 ->  91.86 ; top5 ->  98.94  and loss:  99.46688711643219
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.014385189124482167
test acc: top1 ->  91.87 ; top5 ->  98.93  and loss:  100.89840489625931
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.020026458371148692
test acc: top1 ->  91.86 ; top5 ->  99.02  and loss:  99.90798836946487
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.01580810767723051
test acc: top1 ->  91.93 ; top5 ->  99.02  and loss:  99.60084868967533
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.04476949170907574
test acc: top1 ->  92.01 ; top5 ->  98.98  and loss:  99.83637769520283
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.010870409676272175
test acc: top1 ->  91.92 ; top5 ->  98.97  and loss:  99.61804327368736
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.020285294769564644
test acc: top1 ->  91.92 ; top5 ->  98.93  and loss:  99.30498103797436
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.018887056861785823
test acc: top1 ->  91.97 ; top5 ->  98.95  and loss:  98.90840338170528
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  64 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -0.022036608432244975 , diff:  0.022036608432244975
adv train loss:  -0.019346549023907755 , diff:  0.00269005940833722
layer  2  adv train finish, try to retain  99
test acc: top1 ->  91.71 ; top5 ->  98.9  and loss:  100.6145058721304
forward train acc: top1 ->  99.996 ; top5 ->  99.998  and loss:  0.034488402015995234
test acc: top1 ->  91.84 ; top5 ->  98.92  and loss:  101.3584311157465
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.01536978101648856
test acc: top1 ->  91.77 ; top5 ->  98.94  and loss:  100.47989657521248
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.10247195437705159
test acc: top1 ->  91.71 ; top5 ->  98.99  and loss:  98.27625826001167
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.033577780342966435
test acc: top1 ->  91.78 ; top5 ->  99.0  and loss:  98.05389633774757
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.0376512497132353
test acc: top1 ->  91.84 ; top5 ->  99.07  and loss:  97.63988932967186
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.032539455266487494
test acc: top1 ->  91.81 ; top5 ->  99.03  and loss:  97.81022547185421
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.01708410024036766
test acc: top1 ->  91.78 ; top5 ->  99.0  and loss:  98.63165977597237
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.046884208755272994
test acc: top1 ->  91.81 ; top5 ->  98.97  and loss:  98.1248095035553
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.023656219102122122
test acc: top1 ->  91.9 ; top5 ->  99.0  and loss:  96.84069260954857
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.048495167760847835
test acc: top1 ->  91.88 ; top5 ->  99.02  and loss:  96.27065958082676
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  128 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -0.030128115751722362 , diff:  0.030128115751722362
adv train loss:  -0.019394770177314058 , diff:  0.010733345574408304
adv train loss:  -0.012953618522260513 , diff:  0.006441151655053545
layer  3  adv train finish, try to retain  87
test acc: top1 ->  91.98 ; top5 ->  99.05  and loss:  95.90723723173141
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.025352154914571656
test acc: top1 ->  91.82 ; top5 ->  99.05  and loss:  96.55427618324757
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.012506658593110842
test acc: top1 ->  92.02 ; top5 ->  98.97  and loss:  99.08600617945194
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.04101911116123347
test acc: top1 ->  91.94 ; top5 ->  98.95  and loss:  98.47968383133411
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.01874822052923264
test acc: top1 ->  91.86 ; top5 ->  98.96  and loss:  97.48830349743366
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.025102891951974016
test acc: top1 ->  91.96 ; top5 ->  98.98  and loss:  97.45005922019482
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.015146693901215258
test acc: top1 ->  92.0 ; top5 ->  98.99  and loss:  97.07204401493073
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.01082648426609012
test acc: top1 ->  91.99 ; top5 ->  99.06  and loss:  96.89870582520962
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.01659920208840049
test acc: top1 ->  92.03 ; top5 ->  99.01  and loss:  97.45909205079079
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.022773269591652934
test acc: top1 ->  91.95 ; top5 ->  98.95  and loss:  97.70454858243465
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.03995885554468259
test acc: top1 ->  92.0 ; top5 ->  99.02  and loss:  97.50200806558132
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  128 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -0.021379083633974005 , diff:  0.021379083633974005
adv train loss:  -0.02505344262624476 , diff:  0.0036743589922707542
layer  4  adv train finish, try to retain  155
test acc: top1 ->  91.98 ; top5 ->  99.01  and loss:  97.81956507265568
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.02950375584032372
test acc: top1 ->  91.9 ; top5 ->  98.97  and loss:  99.7135883718729
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.026378597889561206
test acc: top1 ->  91.94 ; top5 ->  98.94  and loss:  100.7823994755745
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.019330085182446055
test acc: top1 ->  91.8 ; top5 ->  99.0  and loss:  99.60119995474815
forward train acc: top1 ->  99.98799997558594 ; top5 ->  100.0  and loss:  0.04854372733825585
test acc: top1 ->  91.86 ; top5 ->  98.98  and loss:  100.34743796288967
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.030644128250969516
test acc: top1 ->  91.93 ; top5 ->  98.97  and loss:  98.10174478590488
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03892394236754626
test acc: top1 ->  91.92 ; top5 ->  98.99  and loss:  96.53753574192524
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.02223459499555247
test acc: top1 ->  91.9 ; top5 ->  98.98  and loss:  96.49863322079182
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.02778639550905382
test acc: top1 ->  91.97 ; top5 ->  98.96  and loss:  96.07534018158913
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.009484329122869894
test acc: top1 ->  91.94 ; top5 ->  98.98  and loss:  97.81693989038467
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.03827636105279453
test acc: top1 ->  91.95 ; top5 ->  98.99  and loss:  97.44626165926456
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -0.030825140407614526 , diff:  0.030825140407614526
adv train loss:  -0.015243824433127884 , diff:  0.015581315974486643
adv train loss:  -0.03416406990163523 , diff:  0.018920245468507346
adv train loss:  -0.018198735496753216 , diff:  0.015965334404882014
adv train loss:  -0.020496736717177555 , diff:  0.0022980012204243394
layer  5  adv train finish, try to retain  149
test acc: top1 ->  91.91 ; top5 ->  98.96  and loss:  97.79362012445927
forward train acc: top1 ->  99.98999997558593 ; top5 ->  100.0  and loss:  0.04273383060353808
test acc: top1 ->  91.9 ; top5 ->  99.01  and loss:  96.99926888942719
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.020903375575926475
test acc: top1 ->  92.0 ; top5 ->  98.98  and loss:  97.2239129692316
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.018389029547904556
test acc: top1 ->  91.92 ; top5 ->  99.0  and loss:  97.97657904028893
forward train acc: top1 ->  99.99199997558594 ; top5 ->  100.0  and loss:  0.020038720620505046
test acc: top1 ->  91.84 ; top5 ->  99.02  and loss:  98.14371255040169
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.01571318849346426
test acc: top1 ->  91.81 ; top5 ->  99.0  and loss:  102.13380861282349
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.027107686757517513
test acc: top1 ->  91.84 ; top5 ->  99.02  and loss:  102.47515493631363
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.017871698334147368
test acc: top1 ->  91.85 ; top5 ->  98.98  and loss:  101.69390995800495
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.029559825621163327
test acc: top1 ->  92.01 ; top5 ->  98.99  and loss:  100.76326866447926
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.030234621284762397
test acc: top1 ->  91.97 ; top5 ->  99.03  and loss:  100.31022062897682
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -0.02281089621413912 , diff:  0.02281089621413912
adv train loss:  -0.03169904971207416 , diff:  0.008888153497935036
layer  6  adv train finish, try to retain  144
test acc: top1 ->  91.9 ; top5 ->  99.0  and loss:  100.89272259175777
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.01682928912487114
test acc: top1 ->  91.99 ; top5 ->  99.0  and loss:  101.55515293776989
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.011223554884054465
test acc: top1 ->  91.91 ; top5 ->  99.03  and loss:  102.30196969211102
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.014047893366864628
test acc: top1 ->  91.96 ; top5 ->  99.03  and loss:  102.19143162667751
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.0754112059594263
test acc: top1 ->  91.67 ; top5 ->  99.11  and loss:  99.03750666975975
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.0336547174825057
test acc: top1 ->  91.79 ; top5 ->  99.08  and loss:  96.7824239730835
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.012527648913533085
test acc: top1 ->  91.84 ; top5 ->  99.1  and loss:  97.63236555457115
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.020252962020094856
test acc: top1 ->  91.93 ; top5 ->  99.11  and loss:  96.24465724825859
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.01326028253424738
test acc: top1 ->  91.95 ; top5 ->  99.06  and loss:  96.58001761138439
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.010158394390145986
test acc: top1 ->  91.96 ; top5 ->  99.09  and loss:  97.12278774380684
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.01972457918913051
test acc: top1 ->  91.92 ; top5 ->  99.1  and loss:  97.98546519875526
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -0.025393277159309946 , diff:  0.025393277159309946
adv train loss:  -0.016369467707590957 , diff:  0.00902380945171899
layer  7  adv train finish, try to retain  14
test acc: top1 ->  20.98 ; top5 ->  51.63  and loss:  887.1144967079163
forward train acc: top1 ->  92.552 ; top5 ->  99.83  and loss:  48.550579354166985
test acc: top1 ->  86.78 ; top5 ->  98.44  and loss:  125.73680719733238
forward train acc: top1 ->  96.30999998291016 ; top5 ->  99.956  and loss:  16.271156780421734
test acc: top1 ->  87.75 ; top5 ->  98.61  and loss:  113.11516696214676
forward train acc: top1 ->  97.24000001220703 ; top5 ->  99.968  and loss:  10.542541600763798
test acc: top1 ->  88.31 ; top5 ->  98.77  and loss:  101.74035221338272
forward train acc: top1 ->  97.64199997802734 ; top5 ->  99.974  and loss:  8.62713992036879
test acc: top1 ->  88.75 ; top5 ->  98.77  and loss:  97.6317945420742
forward train acc: top1 ->  97.97799998046875 ; top5 ->  99.98  and loss:  6.993457173928618
test acc: top1 ->  89.01 ; top5 ->  98.82  and loss:  92.88110679388046
forward train acc: top1 ->  98.17400000976562 ; top5 ->  99.992  and loss:  6.435169596225023
test acc: top1 ->  89.22 ; top5 ->  98.8  and loss:  92.14428175985813
forward train acc: top1 ->  98.28999998291016 ; top5 ->  99.986  and loss:  5.963609183207154
test acc: top1 ->  89.32 ; top5 ->  98.79  and loss:  88.4767586439848
forward train acc: top1 ->  98.28799998046875 ; top5 ->  99.986  and loss:  5.582264108583331
test acc: top1 ->  89.49 ; top5 ->  98.81  and loss:  86.91624300181866
forward train acc: top1 ->  98.50599998046874 ; top5 ->  99.982  and loss:  4.95528235565871
test acc: top1 ->  89.53 ; top5 ->  98.8  and loss:  85.93694259226322
forward train acc: top1 ->  98.55000000488282 ; top5 ->  99.994  and loss:  4.739227489568293
test acc: top1 ->  89.61 ; top5 ->  98.82  and loss:  84.47723205387592
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -2.3179050104226917 , diff:  2.3179050104226917
adv train loss:  -2.384039265336469 , diff:  0.06613425491377711
adv train loss:  -2.2157747955061495 , diff:  0.16826446983031929
adv train loss:  -2.3179250604007393 , diff:  0.10215026489458978
adv train loss:  -2.2520828979322687 , diff:  0.06584216246847063
adv train loss:  -2.1625467299018055 , diff:  0.08953616803046316
adv train loss:  -2.0814930761116557 , diff:  0.08105365379014984
adv train loss:  -2.2673376303864643 , diff:  0.18584455427480862
adv train loss:  -2.2449972457252443 , diff:  0.022340384661220014
adv train loss:  -2.2804111672448926 , diff:  0.035413921519648284
layer  8  adv train finish, try to retain  58
test acc: top1 ->  87.0 ; top5 ->  99.06  and loss:  78.51082046329975
forward train acc: top1 ->  99.86 ; top5 ->  100.0  and loss:  0.41836444531872985
test acc: top1 ->  91.51 ; top5 ->  99.0  and loss:  91.28010995686054
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.11669079847706598
test acc: top1 ->  91.61 ; top5 ->  99.02  and loss:  91.34554995596409
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.0969395583451842
test acc: top1 ->  91.72 ; top5 ->  99.0  and loss:  89.77233968675137
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.10167411499787704
test acc: top1 ->  91.75 ; top5 ->  99.1  and loss:  88.39060062170029
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.059243517870527285
test acc: top1 ->  91.8 ; top5 ->  99.11  and loss:  88.63725933432579
forward train acc: top1 ->  99.97399997558594 ; top5 ->  100.0  and loss:  0.093788532479266
test acc: top1 ->  91.8 ; top5 ->  99.14  and loss:  88.6851739436388
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.041141661880828906
test acc: top1 ->  91.76 ; top5 ->  99.14  and loss:  89.36606100201607
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.08244951409142232
test acc: top1 ->  91.73 ; top5 ->  99.14  and loss:  89.63955183327198
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.054499800156918354
test acc: top1 ->  91.78 ; top5 ->  99.12  and loss:  88.84847483038902
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.02913963182072621
test acc: top1 ->  91.86 ; top5 ->  99.1  and loss:  89.57927599549294
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -0.12271973055612762 , diff:  0.12271973055612762
adv train loss:  -0.12295874200935941 , diff:  0.00023901145323179662
layer  9  adv train finish, try to retain  16
test acc: top1 ->  27.78 ; top5 ->  83.74  and loss:  909.3372778892517
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.27364413736358983
test acc: top1 ->  91.59 ; top5 ->  99.04  and loss:  90.9207576662302
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.04977549604154774
test acc: top1 ->  91.73 ; top5 ->  99.08  and loss:  89.50999449193478
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.06519931172806537
test acc: top1 ->  91.8 ; top5 ->  99.03  and loss:  88.95148459076881
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.04919422782057836
test acc: top1 ->  91.81 ; top5 ->  99.05  and loss:  88.39168141782284
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.05219910324194643
test acc: top1 ->  91.75 ; top5 ->  99.04  and loss:  88.81077495217323
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.03384676594805569
test acc: top1 ->  91.83 ; top5 ->  99.03  and loss:  88.52271945774555
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.04401075019268319
test acc: top1 ->  91.81 ; top5 ->  99.04  and loss:  88.66249576210976
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.027194226793653797
test acc: top1 ->  91.91 ; top5 ->  99.05  and loss:  88.41444231569767
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.0332237327547773
test acc: top1 ->  91.87 ; top5 ->  99.04  and loss:  87.80569434165955
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.03484094116356573
test acc: top1 ->  91.83 ; top5 ->  99.01  and loss:  88.08317126333714
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -0.1159634184996321 , diff:  0.1159634184996321
adv train loss:  -0.08094519191217842 , diff:  0.03501822658745368
adv train loss:  -0.10588270581138204 , diff:  0.024937513899203623
adv train loss:  -0.09106818380496406 , diff:  0.014814522006417974
adv train loss:  -0.14721419244051503 , diff:  0.05614600863555097
adv train loss:  -0.11304516919517482 , diff:  0.03416902324534021
adv train loss:  -0.0858814579760292 , diff:  0.027163711219145625
adv train loss:  -0.10534010634910373 , diff:  0.01945864837307454
adv train loss:  -0.10897895775269717 , diff:  0.003638851403593435
layer  10  adv train finish, try to retain  18
test acc: top1 ->  10.27 ; top5 ->  71.99  and loss:  949.2965121269226
forward train acc: top1 ->  99.928 ; top5 ->  100.0  and loss:  0.20454609256557887
test acc: top1 ->  91.86 ; top5 ->  99.12  and loss:  79.03980143368244
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.0422197061998304
test acc: top1 ->  91.8 ; top5 ->  99.12  and loss:  80.36867117881775
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.026267816827385104
test acc: top1 ->  91.87 ; top5 ->  99.07  and loss:  79.8792902380228
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.024239913474957575
test acc: top1 ->  91.88 ; top5 ->  99.07  and loss:  80.73809397220612
forward train acc: top1 ->  99.98599997558594 ; top5 ->  100.0  and loss:  0.048668789268049295
test acc: top1 ->  91.88 ; top5 ->  99.08  and loss:  81.24962286651134
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.052956536561396206
test acc: top1 ->  91.93 ; top5 ->  99.05  and loss:  81.30676831305027
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.02367571911554478
test acc: top1 ->  91.86 ; top5 ->  99.05  and loss:  81.80410942435265
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.045027564994597924
test acc: top1 ->  91.9 ; top5 ->  99.09  and loss:  81.58613181114197
forward train acc: top1 ->  99.98599997558594 ; top5 ->  100.0  and loss:  0.0287229231580568
test acc: top1 ->  91.81 ; top5 ->  99.07  and loss:  82.24748581647873
forward train acc: top1 ->  99.98999997558593 ; top5 ->  100.0  and loss:  0.044171278573230666
test acc: top1 ->  91.86 ; top5 ->  99.04  and loss:  81.94946590065956
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -0.15834149158763466 , diff:  0.15834149158763466
adv train loss:  -0.143365837253441 , diff:  0.014975654334193678
adv train loss:  -0.15402756210096413 , diff:  0.010661724847523146
adv train loss:  -0.15190260021336144 , diff:  0.0021249618876026943
layer  11  adv train finish, try to retain  33
test acc: top1 ->  59.91 ; top5 ->  96.13  and loss:  187.56404840946198
forward train acc: top1 ->  98.78600000244141 ; top5 ->  99.99  and loss:  4.791833010036498
test acc: top1 ->  90.8 ; top5 ->  98.44  and loss:  99.10601109266281
forward train acc: top1 ->  99.832 ; top5 ->  99.998  and loss:  0.5174885098313098
test acc: top1 ->  91.18 ; top5 ->  98.58  and loss:  93.01577246189117
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.23519826056508464
test acc: top1 ->  91.34 ; top5 ->  98.63  and loss:  92.18126249313354
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.11058887131002848
test acc: top1 ->  91.44 ; top5 ->  98.8  and loss:  90.80524358153343
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.11782321719329047
test acc: top1 ->  91.42 ; top5 ->  98.87  and loss:  90.08674931526184
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.07511378099297872
test acc: top1 ->  91.44 ; top5 ->  98.86  and loss:  89.58127543330193
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.0674542147826287
test acc: top1 ->  91.45 ; top5 ->  98.86  and loss:  89.3961209654808
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.09525334702630062
test acc: top1 ->  91.48 ; top5 ->  98.88  and loss:  88.92689797282219
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.0538759359769756
test acc: top1 ->  91.49 ; top5 ->  98.87  and loss:  88.35699588060379
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.03937053411937086
test acc: top1 ->  91.51 ; top5 ->  98.9  and loss:  88.04533216357231
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -0.3463033878360875 , diff:  0.3463033878360875
adv train loss:  -0.3954502991764457 , diff:  0.0491469113403582
adv train loss:  -0.43366983684609295 , diff:  0.03821953766964725
adv train loss:  -0.4266450482646178 , diff:  0.00702478858147515
layer  12  adv train finish, try to retain  19
test acc: top1 ->  80.93 ; top5 ->  98.57  and loss:  113.28016424179077
forward train acc: top1 ->  98.42000000244141 ; top5 ->  99.994  and loss:  4.98982112063095
test acc: top1 ->  91.35 ; top5 ->  98.98  and loss:  63.9871686771512
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.5146424660924822
test acc: top1 ->  91.65 ; top5 ->  98.92  and loss:  61.47458288446069
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.26179656526073813
test acc: top1 ->  91.79 ; top5 ->  98.85  and loss:  62.47119815647602
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.1560407960205339
test acc: top1 ->  91.81 ; top5 ->  98.89  and loss:  63.28268210589886
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.11330418905708939
test acc: top1 ->  91.69 ; top5 ->  98.91  and loss:  64.01814833283424
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.10678285380708985
test acc: top1 ->  91.64 ; top5 ->  98.9  and loss:  64.68905065953732
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.099980126178707
test acc: top1 ->  91.69 ; top5 ->  98.87  and loss:  65.12946982681751
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.06482101636356674
test acc: top1 ->  91.8 ; top5 ->  98.87  and loss:  65.86308584362268
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.06986678256362211
test acc: top1 ->  91.8 ; top5 ->  98.86  and loss:  65.95614837110043
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.05942845113168005
test acc: top1 ->  91.7 ; top5 ->  98.89  and loss:  66.37585461139679
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -0.28614497694798047 , diff:  0.28614497694798047
adv train loss:  -0.24336226962623186 , diff:  0.0427827073217486
adv train loss:  -0.25599015913758194 , diff:  0.012627889511350077
adv train loss:  -0.28144450685613265 , diff:  0.025454347718550707
adv train loss:  -0.22099782476652763 , diff:  0.06044668208960502
adv train loss:  -0.22391190209054912 , diff:  0.0029140773240214912
layer  13  adv train finish, try to retain  28
test acc: top1 ->  91.35 ; top5 ->  98.98  and loss:  104.37348452210426
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.057971257436292944
test acc: top1 ->  91.86 ; top5 ->  99.0  and loss:  99.57230231165886
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.012977016064724012
test acc: top1 ->  91.9 ; top5 ->  99.01  and loss:  99.489303201437
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.024115713982610032
test acc: top1 ->  91.67 ; top5 ->  98.97  and loss:  102.35073974728584
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.02078654655599621
test acc: top1 ->  91.69 ; top5 ->  98.91  and loss:  102.78924116492271
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.0169785036008534
test acc: top1 ->  91.71 ; top5 ->  99.0  and loss:  100.55644726753235
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.023049532887853275
test acc: top1 ->  91.68 ; top5 ->  98.96  and loss:  101.51846419274807
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.015332920651417226
test acc: top1 ->  91.84 ; top5 ->  98.98  and loss:  100.6839115768671
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.005052565135429177
test acc: top1 ->  91.92 ; top5 ->  98.96  and loss:  101.47488342225552
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.026275723383150762
test acc: top1 ->  91.85 ; top5 ->  98.97  and loss:  100.2231997102499
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03818124923316191
test acc: top1 ->  91.87 ; top5 ->  98.95  and loss:  100.8275852650404
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
layer  0  :  0  ==>  64 / 64 , inc:  1
layer  1  :  0  ==>  64 / 64 , inc:  1
layer  2  :  0  ==>  128 / 128 , inc:  1
layer  3  :  0  ==>  128 / 128 , inc:  1
layer  4  :  0  ==>  256 / 256 , inc:  1
layer  5  :  0  ==>  256 / 256 , inc:  1
layer  6  :  0  ==>  256 / 256 , inc:  1
layer  7  :  0  ==>  512 / 512 , inc:  1
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0  ==>  512 / 512 , inc:  1
layer  11  :  0  ==>  512 / 512 , inc:  1
layer  12  :  0  ==>  512 / 512 , inc:  1
layer  13  :  0  ==>  512 / 512 , inc:  1
eps [0.0005625000000000001, 0.0005625000000000001, 0.0005625000000000001, 0.0005625000000000001, 0.0005625000000000001, 0.0005625000000000001, 0.0005625000000000001, 0.0005625000000000001, 0.0005625000000000001, 0.0005625000000000001, 0.0005625000000000001, 0.0005625000000000001, 0.0005625000000000001, 0.0005625000000000001]  wait [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 1
$$$$$$$$$$$$$ epoch  2  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -0.019332946488702873 , diff:  0.019332946488702873
adv train loss:  -0.017702740405752593 , diff:  0.0016302060829502807
layer  0  adv train finish, try to retain  32
test acc: top1 ->  91.86 ; top5 ->  98.96  and loss:  101.4544090628624
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.033783223592763534
test acc: top1 ->  91.83 ; top5 ->  98.97  and loss:  101.08395074307919
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.012246898606917966
test acc: top1 ->  91.69 ; top5 ->  98.95  and loss:  101.60864594578743
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.003896213556799921
test acc: top1 ->  91.76 ; top5 ->  99.0  and loss:  101.72643792629242
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.020560621031563642
test acc: top1 ->  91.98 ; top5 ->  99.07  and loss:  100.7106205523014
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.024417557305241644
test acc: top1 ->  92.0 ; top5 ->  99.01  and loss:  101.21955269575119
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.01678488422976443
test acc: top1 ->  92.0 ; top5 ->  99.03  and loss:  101.98592734336853
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.03466569758211335
test acc: top1 ->  91.99 ; top5 ->  99.04  and loss:  99.98759686946869
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.014176949259308458
test acc: top1 ->  91.92 ; top5 ->  98.98  and loss:  101.2057950347662
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.02103720038417123
test acc: top1 ->  91.92 ; top5 ->  98.97  and loss:  100.62135510146618
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.011077139574126704
test acc: top1 ->  91.92 ; top5 ->  99.0  and loss:  99.7452530413866
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  64 / 64 , inc:  1
---------------- start layer  1  ---------------
layer  1  adv train finish, try to retain  55
test acc: top1 ->  91.94 ; top5 ->  98.99  and loss:  100.08937296271324
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.009904330170684261
test acc: top1 ->  91.88 ; top5 ->  98.97  and loss:  102.92648568749428
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.04619012093947106
test acc: top1 ->  91.95 ; top5 ->  99.0  and loss:  102.6946403235197
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.011363211877498003
test acc: top1 ->  91.96 ; top5 ->  98.97  and loss:  102.2478578388691
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.006426630394599897
test acc: top1 ->  91.93 ; top5 ->  98.97  and loss:  102.26571680605412
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.016456217737868428
test acc: top1 ->  91.99 ; top5 ->  99.0  and loss:  101.39212414622307
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.02660774692844825
test acc: top1 ->  91.85 ; top5 ->  98.99  and loss:  101.41429874300957
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.0123887904737785
test acc: top1 ->  91.97 ; top5 ->  98.99  and loss:  101.7222934961319
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.017373176971887005
test acc: top1 ->  91.95 ; top5 ->  98.98  and loss:  101.57804667949677
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.018461605672897008
test acc: top1 ->  91.93 ; top5 ->  99.0  and loss:  101.55566474795341
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.011412834040470443
test acc: top1 ->  92.0 ; top5 ->  98.99  and loss:  101.78758779168129
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  64 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -0.012561705105781584 , diff:  0.012561705105781584
adv train loss:  -0.005898699439853772 , diff:  0.0066630056659278125
layer  2  adv train finish, try to retain  100
test acc: top1 ->  91.9 ; top5 ->  99.02  and loss:  101.00523206591606
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.026786455844217016
test acc: top1 ->  91.88 ; top5 ->  99.01  and loss:  101.18978598713875
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.011644508158497047
test acc: top1 ->  91.9 ; top5 ->  99.07  and loss:  100.11247193813324
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.005078321713654077
test acc: top1 ->  91.85 ; top5 ->  99.06  and loss:  100.4575924873352
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.02152300119314532
test acc: top1 ->  91.94 ; top5 ->  99.05  and loss:  99.43786406517029
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.014013720187961098
test acc: top1 ->  91.91 ; top5 ->  99.04  and loss:  101.05834820866585
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.018872644274779304
test acc: top1 ->  91.84 ; top5 ->  99.01  and loss:  101.70485419034958
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.01537559750067885
test acc: top1 ->  91.88 ; top5 ->  99.0  and loss:  101.51947811245918
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03668139622493527
test acc: top1 ->  91.82 ; top5 ->  98.98  and loss:  102.22097384929657
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.005998696301190876
test acc: top1 ->  91.93 ; top5 ->  98.99  and loss:  102.97573581337929
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.016119356547278585
test acc: top1 ->  91.88 ; top5 ->  99.0  and loss:  101.84076038002968
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  128 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -0.011070909614360858 , diff:  0.011070909614360858
adv train loss:  -0.004673848454331164 , diff:  0.006397061160029693
layer  3  adv train finish, try to retain  87
test acc: top1 ->  91.82 ; top5 ->  98.99  and loss:  103.2678627371788
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.010796830523759127
test acc: top1 ->  91.92 ; top5 ->  98.94  and loss:  104.11467710137367
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.005549141384790346
test acc: top1 ->  91.88 ; top5 ->  98.94  and loss:  105.64901515841484
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.014119060327175248
test acc: top1 ->  91.87 ; top5 ->  98.97  and loss:  105.65780185163021
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.024416674016492834
test acc: top1 ->  91.85 ; top5 ->  98.99  and loss:  105.15610830485821
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.019819029203972605
test acc: top1 ->  91.89 ; top5 ->  99.03  and loss:  105.09212097525597
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.024960849714261713
test acc: top1 ->  91.93 ; top5 ->  99.0  and loss:  104.11826133728027
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.0125313255398396
test acc: top1 ->  91.92 ; top5 ->  99.01  and loss:  104.6946809887886
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03842082229539301
test acc: top1 ->  91.88 ; top5 ->  99.0  and loss:  104.20572811365128
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.009399007423780859
test acc: top1 ->  91.87 ; top5 ->  98.99  and loss:  103.72956627607346
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.016622506214588384
test acc: top1 ->  91.86 ; top5 ->  98.96  and loss:  103.54969203472137
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  128 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -0.004794447791425682 , diff:  0.004794447791425682
layer  4  adv train finish, try to retain  155
test acc: top1 ->  91.88 ; top5 ->  98.98  and loss:  103.56531688570976
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.007983757467755481
test acc: top1 ->  91.89 ; top5 ->  98.94  and loss:  106.63106861710548
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.017385011824359253
test acc: top1 ->  91.89 ; top5 ->  99.03  and loss:  104.45784112811089
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.02927098638056691
test acc: top1 ->  92.06 ; top5 ->  98.95  and loss:  105.3310971558094
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.03130987091390125
test acc: top1 ->  91.94 ; top5 ->  98.99  and loss:  103.60894215106964
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.015327538877542679
test acc: top1 ->  91.91 ; top5 ->  99.02  and loss:  101.9628276526928
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.01023602375153132
test acc: top1 ->  91.97 ; top5 ->  99.03  and loss:  102.40193328261375
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.028126512832130857
test acc: top1 ->  91.97 ; top5 ->  98.96  and loss:  102.79080203175545
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.005855381830770057
test acc: top1 ->  91.96 ; top5 ->  99.01  and loss:  102.4286795258522
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.017065545192963327
test acc: top1 ->  91.93 ; top5 ->  99.02  and loss:  101.53618988394737
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03255003038378135
test acc: top1 ->  91.87 ; top5 ->  98.98  and loss:  102.80447667837143
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -0.018495285376047832 , diff:  0.018495285376047832
adv train loss:  -0.02899509478896789 , diff:  0.010499809412920058
adv train loss:  -0.009735857925761593 , diff:  0.019259236863206297
adv train loss:  -0.009715397577565454 , diff:  2.046034819613851e-05
layer  5  adv train finish, try to retain  149
test acc: top1 ->  91.99 ; top5 ->  98.98  and loss:  103.47766825556755
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.01649813688982249
test acc: top1 ->  91.88 ; top5 ->  98.98  and loss:  102.4309221804142
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.006962039558857214
test acc: top1 ->  92.06 ; top5 ->  99.01  and loss:  103.30749428272247
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.024362425281651667
test acc: top1 ->  91.91 ; top5 ->  98.98  and loss:  103.83685645461082
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.0454489712810755
test acc: top1 ->  91.95 ; top5 ->  98.99  and loss:  104.10032980144024
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.023496398520364892
test acc: top1 ->  91.95 ; top5 ->  99.0  and loss:  103.51076221466064
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.039972949277398584
test acc: top1 ->  91.92 ; top5 ->  99.06  and loss:  102.62813583016396
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.026073100252688164
test acc: top1 ->  91.95 ; top5 ->  99.04  and loss:  101.12708668410778
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.015107199822523398
test acc: top1 ->  91.94 ; top5 ->  99.03  and loss:  101.4684402346611
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.010273170314917479
test acc: top1 ->  91.94 ; top5 ->  99.07  and loss:  101.40223044157028
forward train acc: top1 ->  99.99599997558593 ; top5 ->  100.0  and loss:  0.011009158266460872
test acc: top1 ->  91.89 ; top5 ->  99.06  and loss:  102.08531752228737
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  6  ---------------
************ all values are small in this layer **********
layer  6  adv train finish, try to retain  255
test acc: top1 ->  91.91 ; top5 ->  99.04  and loss:  101.98123237490654
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.01207078521942151
test acc: top1 ->  91.88 ; top5 ->  99.03  and loss:  102.57007804512978
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.019840721504806424
test acc: top1 ->  91.97 ; top5 ->  99.07  and loss:  102.14039126038551
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.025165470885212926
test acc: top1 ->  91.98 ; top5 ->  99.06  and loss:  101.6535479426384
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.048895146193899564
test acc: top1 ->  91.89 ; top5 ->  99.0  and loss:  102.1445178091526
forward train acc: top1 ->  99.98599997558594 ; top5 ->  100.0  and loss:  0.03608288497161993
test acc: top1 ->  91.95 ; top5 ->  99.02  and loss:  101.18755818903446
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.01757896649860413
test acc: top1 ->  91.86 ; top5 ->  98.97  and loss:  101.32733575999737
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.0058943081975968425
test acc: top1 ->  91.93 ; top5 ->  99.0  and loss:  101.85038466751575
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.005397227267422977
test acc: top1 ->  91.99 ; top5 ->  98.97  and loss:  102.39179830253124
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  7  ---------------
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  511
test acc: top1 ->  91.99 ; top5 ->  98.99  and loss:  101.1249732375145
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.007709045753088617
test acc: top1 ->  91.87 ; top5 ->  98.98  and loss:  102.90944318473339
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.004419902265780706
test acc: top1 ->  91.89 ; top5 ->  98.95  and loss:  105.10959742963314
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.027257052654022118
test acc: top1 ->  91.94 ; top5 ->  99.01  and loss:  104.30916561186314
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.015292673953808844
test acc: top1 ->  91.96 ; top5 ->  98.94  and loss:  104.81784290075302
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.008539130329836553
test acc: top1 ->  91.94 ; top5 ->  98.97  and loss:  104.09729769825935
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.027346469758867897
test acc: top1 ->  91.9 ; top5 ->  99.0  and loss:  104.05058771371841
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.011222384986126599
test acc: top1 ->  91.96 ; top5 ->  98.97  and loss:  104.01538853347301
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.011441940675922524
test acc: top1 ->  91.93 ; top5 ->  98.99  and loss:  104.38846170902252
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  8  ---------------
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  511
test acc: top1 ->  91.97 ; top5 ->  99.03  and loss:  104.45402131974697
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.021893066849543175
test acc: top1 ->  91.91 ; top5 ->  98.95  and loss:  104.51446703076363
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.010376050948480042
test acc: top1 ->  91.95 ; top5 ->  98.95  and loss:  103.16226544976234
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.009615153545382782
test acc: top1 ->  92.02 ; top5 ->  99.0  and loss:  103.79939594864845
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  9  ---------------
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  511
test acc: top1 ->  92.0 ; top5 ->  99.04  and loss:  103.47212091088295
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.024059715898545164
test acc: top1 ->  91.9 ; top5 ->  99.03  and loss:  103.17685714364052
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.02015707233840658
test acc: top1 ->  91.9 ; top5 ->  98.97  and loss:  104.27334490418434
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.00607069642865099
test acc: top1 ->  91.92 ; top5 ->  98.97  and loss:  105.28713661432266
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.01737825425925621
test acc: top1 ->  91.85 ; top5 ->  98.98  and loss:  104.13951432704926
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.005065674240199769
test acc: top1 ->  91.85 ; top5 ->  98.94  and loss:  105.35041885077953
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.016505249880083284
test acc: top1 ->  91.87 ; top5 ->  98.97  and loss:  104.46022653579712
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03390591780998875
test acc: top1 ->  91.9 ; top5 ->  98.96  and loss:  103.4573321044445
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.010330023946153233
test acc: top1 ->  91.95 ; top5 ->  99.0  and loss:  102.74001717567444
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.01998325275053503
test acc: top1 ->  91.85 ; top5 ->  98.99  and loss:  102.50813919305801
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.01435805370840626
test acc: top1 ->  91.88 ; top5 ->  99.02  and loss:  102.55060887336731
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -0.030869524284753425 , diff:  0.030869524284753425
adv train loss:  -0.005432211297375034 , diff:  0.02543731298737839
adv train loss:  -0.01216187805403024 , diff:  0.006729666756655206
layer  10  adv train finish, try to retain  2
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  2114.7434101104736
forward train acc: top1 ->  61.64199998046875 ; top5 ->  94.45  and loss:  257.4706234931946
test acc: top1 ->  63.98 ; top5 ->  96.9  and loss:  178.6780310869217
forward train acc: top1 ->  90.53800000244141 ; top5 ->  99.982  and loss:  25.86539439857006
test acc: top1 ->  83.95 ; top5 ->  97.53  and loss:  108.61218723654747
forward train acc: top1 ->  95.55600000488282 ; top5 ->  99.984  and loss:  15.389536239206791
test acc: top1 ->  85.36 ; top5 ->  97.7  and loss:  101.92690834403038
forward train acc: top1 ->  96.81799998046876 ; top5 ->  99.988  and loss:  11.784797497093678
test acc: top1 ->  86.07 ; top5 ->  97.72  and loss:  98.52066054940224
forward train acc: top1 ->  97.52799997802734 ; top5 ->  99.996  and loss:  9.294220447540283
test acc: top1 ->  86.44 ; top5 ->  97.76  and loss:  99.12770107388496
forward train acc: top1 ->  97.80199998046875 ; top5 ->  100.0  and loss:  7.881147105246782
test acc: top1 ->  86.79 ; top5 ->  97.72  and loss:  99.2117371559143
forward train acc: top1 ->  97.96999998779297 ; top5 ->  99.99399997558594  and loss:  7.213283155113459
test acc: top1 ->  86.92 ; top5 ->  97.67  and loss:  100.00791418552399
forward train acc: top1 ->  98.04799998535157 ; top5 ->  99.996  and loss:  6.717729281634092
test acc: top1 ->  87.16 ; top5 ->  97.66  and loss:  98.75801402330399
forward train acc: top1 ->  98.15399998046875 ; top5 ->  99.998  and loss:  6.206928335130215
test acc: top1 ->  87.26 ; top5 ->  97.65  and loss:  98.67601254582405
forward train acc: top1 ->  98.16200000488281 ; top5 ->  99.996  and loss:  6.084904436022043
test acc: top1 ->  87.35 ; top5 ->  97.63  and loss:  98.19735065102577
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -2.1994921007426456 , diff:  2.1994921007426456
adv train loss:  -2.269014566205442 , diff:  0.06952246546279639
adv train loss:  -2.2719774262513965 , diff:  0.002962860045954585
layer  11  adv train finish, try to retain  61
test acc: top1 ->  72.62 ; top5 ->  97.55  and loss:  215.88253313302994
forward train acc: top1 ->  99.12199997558594 ; top5 ->  100.0  and loss:  2.847186955914367
test acc: top1 ->  91.16 ; top5 ->  98.91  and loss:  93.83681002259254
forward train acc: top1 ->  99.88599997558593 ; top5 ->  99.998  and loss:  0.3628991172736278
test acc: top1 ->  91.45 ; top5 ->  98.93  and loss:  91.3800947368145
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.19740119987909566
test acc: top1 ->  91.68 ; top5 ->  98.96  and loss:  91.17493817210197
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.11635698560712626
test acc: top1 ->  91.79 ; top5 ->  98.87  and loss:  91.00427332520485
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.052366053312653094
test acc: top1 ->  91.83 ; top5 ->  98.97  and loss:  90.99352736771107
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.05299620756886725
test acc: top1 ->  91.72 ; top5 ->  98.9  and loss:  91.34761463105679
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -0.09517708274995584 , diff:  0.09517708274995584
adv train loss:  -0.03255751032656917 , diff:  0.06261957242338667
adv train loss:  -0.027347365449713834 , diff:  0.005210144876855338
layer  12  adv train finish, try to retain  14
test acc: top1 ->  79.78 ; top5 ->  98.62  and loss:  102.12038713693619
forward train acc: top1 ->  98.53799997558593 ; top5 ->  100.0  and loss:  4.583600822370499
test acc: top1 ->  90.97 ; top5 ->  99.09  and loss:  62.43735148012638
forward train acc: top1 ->  99.894 ; top5 ->  100.0  and loss:  0.6233896794728935
test acc: top1 ->  91.25 ; top5 ->  99.1  and loss:  60.00623834133148
forward train acc: top1 ->  99.94599997558593 ; top5 ->  99.998  and loss:  0.325669088633731
test acc: top1 ->  91.51 ; top5 ->  99.06  and loss:  60.478018432855606
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.19161528936820105
test acc: top1 ->  91.44 ; top5 ->  99.08  and loss:  60.74134624004364
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.13344639941351488
test acc: top1 ->  91.57 ; top5 ->  99.07  and loss:  61.98009990155697
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.12344860623124987
test acc: top1 ->  91.74 ; top5 ->  99.08  and loss:  62.21374835073948
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.12476493322174065
test acc: top1 ->  91.61 ; top5 ->  99.08  and loss:  62.592518851161
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.09181515115778893
test acc: top1 ->  91.69 ; top5 ->  99.06  and loss:  62.83054183423519
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.09952191483171191
test acc: top1 ->  91.71 ; top5 ->  99.08  and loss:  62.90075635910034
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.09409001568565145
test acc: top1 ->  91.67 ; top5 ->  99.06  and loss:  63.73662705719471
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -0.44198211398907006 , diff:  0.44198211398907006
adv train loss:  -0.5484874660614878 , diff:  0.10650535207241774
adv train loss:  -0.3922529968403978 , diff:  0.15623446922108997
adv train loss:  -0.43483181021292694 , diff:  0.04257881337252911
adv train loss:  -0.3908869420411065 , diff:  0.04394486817182042
adv train loss:  -0.4473109738610219 , diff:  0.0564240318199154
adv train loss:  -0.4063841250317637 , diff:  0.04092684882925823
adv train loss:  -0.5126105934032239 , diff:  0.10622646837146021
adv train loss:  -0.48455313029990066 , diff:  0.028057463103323244
adv train loss:  -0.3639233750946005 , diff:  0.12062975520530017
layer  13  adv train finish, try to retain  29
test acc: top1 ->  91.47 ; top5 ->  98.7  and loss:  127.92731697112322
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.10462630099419812
test acc: top1 ->  91.87 ; top5 ->  98.94  and loss:  113.85301914066076
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.05762003217387246
test acc: top1 ->  91.82 ; top5 ->  99.01  and loss:  111.61213317513466
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.019565858469832165
test acc: top1 ->  91.95 ; top5 ->  98.96  and loss:  110.711868673563
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.008046376581944514
test acc: top1 ->  91.94 ; top5 ->  98.94  and loss:  110.55346766114235
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.008373404940357432
test acc: top1 ->  91.91 ; top5 ->  98.96  and loss:  110.36379685997963
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
layer  0  :  0  ==>  64 / 64 , inc:  1
layer  1  :  0  ==>  64 / 64 , inc:  1
layer  2  :  0  ==>  128 / 128 , inc:  1
layer  3  :  0  ==>  128 / 128 , inc:  1
layer  4  :  0  ==>  256 / 256 , inc:  1
layer  5  :  0  ==>  256 / 256 , inc:  1
layer  6  :  0  ==>  256 / 256 , inc:  1
layer  7  :  0  ==>  512 / 512 , inc:  1
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0  ==>  512 / 512 , inc:  1
layer  11  :  0  ==>  512 / 512 , inc:  1
layer  12  :  0  ==>  512 / 512 , inc:  1
layer  13  :  0  ==>  512 / 512 , inc:  1
eps [0.00042187500000000005, 0.00042187500000000005, 0.00042187500000000005, 0.00042187500000000005, 0.00042187500000000005, 0.00042187500000000005, 0.00042187500000000005, 0.00042187500000000005, 0.00042187500000000005, 0.00042187500000000005, 0.00042187500000000005, 0.00042187500000000005, 0.00042187500000000005, 0.00042187500000000005]  wait [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 2
$$$$$$$$$$$$$ epoch  3  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -0.007419446533504015 , diff:  0.007419446533504015
layer  0  adv train finish, try to retain  32
test acc: top1 ->  91.88 ; top5 ->  98.95  and loss:  110.32367128133774
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.019653902068967
test acc: top1 ->  91.98 ; top5 ->  98.98  and loss:  111.47664493322372
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.023393159091938287
test acc: top1 ->  92.08 ; top5 ->  98.9  and loss:  113.215914234519
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.02580131172385336
test acc: top1 ->  92.12 ; top5 ->  98.98  and loss:  110.60094904899597
==> this epoch:  32 / 64
---------------- start layer  1  ---------------
adv train loss:  -0.022732544780410535 , diff:  0.022732544780410535
adv train loss:  -0.007766850308598805 , diff:  0.01496569447181173
adv train loss:  -0.024805400458191684 , diff:  0.01703855014959288
adv train loss:  -0.018895451614213243 , diff:  0.005909948843978441
layer  1  adv train finish, try to retain  55
test acc: top1 ->  92.04 ; top5 ->  99.0  and loss:  110.36456742882729
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.021950112689012258
test acc: top1 ->  92.03 ; top5 ->  98.89  and loss:  112.29587611556053
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.02942425540095428
test acc: top1 ->  91.91 ; top5 ->  98.98  and loss:  110.3381282389164
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.019249806512561918
test acc: top1 ->  91.9 ; top5 ->  98.95  and loss:  111.19203653931618
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.0067036809265346164
test acc: top1 ->  91.87 ; top5 ->  98.95  and loss:  111.14277133345604
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.013009175181423416
test acc: top1 ->  91.95 ; top5 ->  98.98  and loss:  112.21561166644096
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.010428226441945299
test acc: top1 ->  91.89 ; top5 ->  99.0  and loss:  111.26334235072136
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.0375037903431803
test acc: top1 ->  91.92 ; top5 ->  98.93  and loss:  112.71503925323486
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.008021804267102084
test acc: top1 ->  91.89 ; top5 ->  98.94  and loss:  110.9146160185337
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.007509620815170592
test acc: top1 ->  91.88 ; top5 ->  98.97  and loss:  111.19910717010498
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  64 / 64 , inc:  1
---------------- start layer  2  ---------------
layer  2  adv train finish, try to retain  98
test acc: top1 ->  91.85 ; top5 ->  98.93  and loss:  111.78163447976112
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.025883426525979303
test acc: top1 ->  91.9 ; top5 ->  98.95  and loss:  111.70944413542747
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.05125089855914666
test acc: top1 ->  91.72 ; top5 ->  98.89  and loss:  111.91124525666237
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.0252958267719805
test acc: top1 ->  91.84 ; top5 ->  98.84  and loss:  112.73135370016098
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.017236122883673488
test acc: top1 ->  91.75 ; top5 ->  98.9  and loss:  112.14413332939148
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.057992697998315634
test acc: top1 ->  91.92 ; top5 ->  98.88  and loss:  111.15307578444481
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.016050657082814723
test acc: top1 ->  91.86 ; top5 ->  98.86  and loss:  110.7948996424675
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.042090500171070744
test acc: top1 ->  91.88 ; top5 ->  98.96  and loss:  109.01184436678886
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.021998084836923226
test acc: top1 ->  91.83 ; top5 ->  98.95  and loss:  110.35915103554726
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.006224038130312692
test acc: top1 ->  91.94 ; top5 ->  98.94  and loss:  110.89218212664127
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.01273358804792224
test acc: top1 ->  91.91 ; top5 ->  98.95  and loss:  110.54421958327293
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  128 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -0.01859200621021273 , diff:  0.01859200621021273
adv train loss:  -0.0039505051598638374 , diff:  0.014641501050348893
adv train loss:  -0.0017332133016338958 , diff:  0.0022172918582299417
layer  3  adv train finish, try to retain  87
test acc: top1 ->  91.89 ; top5 ->  98.97  and loss:  110.3606818318367
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.025992269647304056
test acc: top1 ->  91.96 ; top5 ->  98.83  and loss:  112.43438792228699
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.00979281922627706
test acc: top1 ->  91.89 ; top5 ->  98.95  and loss:  111.6575503051281
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.022339613352414744
test acc: top1 ->  91.86 ; top5 ->  98.95  and loss:  111.47885051369667
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.010709397327445913
test acc: top1 ->  92.05 ; top5 ->  98.93  and loss:  112.82359603047371
forward train acc: top1 ->  99.99599997558593 ; top5 ->  100.0  and loss:  0.020614086592104286
test acc: top1 ->  91.84 ; top5 ->  98.87  and loss:  112.91745430231094
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.013890098885720903
test acc: top1 ->  91.97 ; top5 ->  98.92  and loss:  112.77288046479225
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.040287887668227995
test acc: top1 ->  92.0 ; top5 ->  98.94  and loss:  111.02921164035797
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.011350333821610548
test acc: top1 ->  91.9 ; top5 ->  98.96  and loss:  110.70105966925621
forward train acc: top1 ->  99.99199997558594 ; top5 ->  100.0  and loss:  0.043489744362886995
test acc: top1 ->  91.84 ; top5 ->  98.95  and loss:  110.60661056637764
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.0075782272490982905
test acc: top1 ->  91.87 ; top5 ->  98.92  and loss:  110.69545570015907
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  128 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -0.015945016819472357 , diff:  0.015945016819472357
adv train loss:  -0.006890457795918792 , diff:  0.009054559023553566
layer  4  adv train finish, try to retain  155
test acc: top1 ->  91.93 ; top5 ->  98.97  and loss:  111.54895943403244
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.023897363254377524
test acc: top1 ->  91.81 ; top5 ->  98.91  and loss:  111.23419046401978
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.011117109611404885
test acc: top1 ->  91.76 ; top5 ->  98.91  and loss:  111.56897781789303
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.027037198969210863
test acc: top1 ->  91.81 ; top5 ->  98.98  and loss:  111.49054193496704
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.016620719574348186
test acc: top1 ->  91.8 ; top5 ->  98.97  and loss:  110.95880495011806
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.007812074962203042
test acc: top1 ->  91.76 ; top5 ->  98.94  and loss:  111.25203862786293
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.007279989671324927
test acc: top1 ->  91.78 ; top5 ->  98.94  and loss:  111.41152039170265
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -0.01191656148460396 , diff:  0.01191656148460396
adv train loss:  -0.01568515444523655 , diff:  0.0037685929606325885
layer  5  adv train finish, try to retain  149
test acc: top1 ->  91.78 ; top5 ->  98.94  and loss:  110.77999368309975
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.011923021154530034
test acc: top1 ->  91.82 ; top5 ->  98.92  and loss:  112.1040068268776
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.019402048565790864
test acc: top1 ->  91.9 ; top5 ->  98.96  and loss:  111.56634682416916
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.027739647194493955
test acc: top1 ->  91.82 ; top5 ->  98.95  and loss:  110.58759272098541
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.03775285009552931
test acc: top1 ->  91.93 ; top5 ->  98.93  and loss:  111.39371508359909
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.017334247257736024
test acc: top1 ->  91.89 ; top5 ->  98.95  and loss:  111.1866894364357
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.02592549361725105
test acc: top1 ->  91.82 ; top5 ->  98.91  and loss:  111.45746430754662
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.014622225837058522
test acc: top1 ->  91.87 ; top5 ->  98.9  and loss:  112.22256349027157
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.008881196509946676
test acc: top1 ->  91.86 ; top5 ->  98.91  and loss:  111.57698257267475
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.008797527202204947
test acc: top1 ->  91.97 ; top5 ->  98.92  and loss:  111.29838904738426
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -0.02008924025358283 , diff:  0.02008924025358283
adv train loss:  -0.01852506550355315 , diff:  0.0015641747500296788
layer  6  adv train finish, try to retain  143
test acc: top1 ->  92.02 ; top5 ->  98.98  and loss:  111.06390404701233
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.017412140393389564
test acc: top1 ->  91.97 ; top5 ->  98.9  and loss:  111.23663912713528
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.014534775178617565
test acc: top1 ->  91.91 ; top5 ->  98.9  and loss:  111.88186584413052
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.015603753796312958
test acc: top1 ->  91.87 ; top5 ->  98.96  and loss:  110.10043230652809
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.011122792616561128
test acc: top1 ->  92.03 ; top5 ->  98.95  and loss:  108.94734960794449
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.024143529993182256
test acc: top1 ->  91.93 ; top5 ->  98.93  and loss:  109.81213307380676
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.018569371481135022
test acc: top1 ->  91.99 ; top5 ->  98.92  and loss:  108.95240803062916
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.011544310114459222
test acc: top1 ->  91.88 ; top5 ->  98.97  and loss:  108.86419650912285
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.013832134165568277
test acc: top1 ->  91.92 ; top5 ->  98.99  and loss:  108.51816236972809
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.009390385461301776
test acc: top1 ->  91.96 ; top5 ->  99.0  and loss:  108.09879177808762
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03526437376490321
test acc: top1 ->  91.96 ; top5 ->  98.96  and loss:  109.36995987594128
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -0.01493699250931968 , diff:  0.01493699250931968
adv train loss:  -0.005673379055224359 , diff:  0.00926361345409532
layer  7  adv train finish, try to retain  11
test acc: top1 ->  11.36 ; top5 ->  71.33  and loss:  977.1450414657593
forward train acc: top1 ->  80.98800001953126 ; top5 ->  99.26999997558593  and loss:  91.16027063131332
test acc: top1 ->  83.05 ; top5 ->  98.08  and loss:  124.21825844049454
forward train acc: top1 ->  92.73799999023437 ; top5 ->  99.924  and loss:  24.888232424855232
test acc: top1 ->  86.35 ; top5 ->  98.41  and loss:  97.2209597080946
forward train acc: top1 ->  95.36799999023438 ; top5 ->  99.95  and loss:  15.009791366755962
test acc: top1 ->  87.48 ; top5 ->  98.54  and loss:  87.57907536625862
forward train acc: top1 ->  96.26799998535157 ; top5 ->  99.96  and loss:  11.734127655625343
test acc: top1 ->  88.2 ; top5 ->  98.65  and loss:  83.2830944955349
forward train acc: top1 ->  96.94199999267578 ; top5 ->  99.966  and loss:  9.566488798707724
test acc: top1 ->  88.61 ; top5 ->  98.7  and loss:  79.45495784282684
forward train acc: top1 ->  97.54199998291016 ; top5 ->  99.982  and loss:  8.14172058366239
test acc: top1 ->  88.64 ; top5 ->  98.75  and loss:  79.44121587276459
forward train acc: top1 ->  97.62799997802735 ; top5 ->  99.986  and loss:  7.339215971529484
test acc: top1 ->  89.0 ; top5 ->  98.76  and loss:  77.81541430950165
forward train acc: top1 ->  97.83600000244141 ; top5 ->  99.982  and loss:  6.871508521959186
test acc: top1 ->  89.04 ; top5 ->  98.79  and loss:  76.21306182444096
forward train acc: top1 ->  97.95399998291016 ; top5 ->  99.982  and loss:  6.3781413827091455
test acc: top1 ->  89.26 ; top5 ->  98.77  and loss:  75.62509126961231
forward train acc: top1 ->  98.09199998291015 ; top5 ->  99.984  and loss:  5.88170718587935
test acc: top1 ->  89.4 ; top5 ->  98.8  and loss:  75.3018616437912
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -6.463721560314298 , diff:  6.463721560314298
adv train loss:  -5.971629718318582 , diff:  0.4920918419957161
adv train loss:  -6.260927643626928 , diff:  0.28929792530834675
adv train loss:  -6.014756887219846 , diff:  0.24617075640708208
adv train loss:  -6.370483469218016 , diff:  0.3557265819981694
adv train loss:  -6.171091319527477 , diff:  0.19939214969053864
adv train loss:  -6.481659410521388 , diff:  0.31056809099391103
adv train loss:  -6.4302323423326015 , diff:  0.05142706818878651
adv train loss:  -6.205427207285538 , diff:  0.22480513504706323
adv train loss:  -6.256454836577177 , diff:  0.05102762929163873
layer  8  adv train finish, try to retain  105
test acc: top1 ->  85.5 ; top5 ->  98.87  and loss:  114.52496787905693
forward train acc: top1 ->  99.618 ; top5 ->  100.0  and loss:  1.30091393955081
test acc: top1 ->  91.13 ; top5 ->  99.11  and loss:  105.87833496928215
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.1913577045634156
test acc: top1 ->  91.43 ; top5 ->  99.08  and loss:  103.55820912122726
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.11010258719761623
test acc: top1 ->  91.43 ; top5 ->  99.09  and loss:  102.60395151376724
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.09309787693928229
test acc: top1 ->  91.57 ; top5 ->  99.1  and loss:  102.54583050310612
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.07442691763208131
test acc: top1 ->  91.54 ; top5 ->  99.04  and loss:  101.94748556613922
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.02636631946916168
test acc: top1 ->  91.6 ; top5 ->  99.07  and loss:  101.62950529158115
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.03182429217031313
test acc: top1 ->  91.56 ; top5 ->  99.09  and loss:  102.78391183912754
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.03970737365489185
test acc: top1 ->  91.59 ; top5 ->  99.07  and loss:  102.80982708930969
forward train acc: top1 ->  99.97999997558594 ; top5 ->  100.0  and loss:  0.06331044879334513
test acc: top1 ->  91.72 ; top5 ->  99.04  and loss:  101.34309810400009
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.03897417509324441
test acc: top1 ->  91.72 ; top5 ->  99.01  and loss:  102.06034590303898
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -0.07137772891110217 , diff:  0.07137772891110217
adv train loss:  -0.11600445123804093 , diff:  0.04462672232693876
adv train loss:  -0.04554031555949223 , diff:  0.0704641356785487
adv train loss:  -0.08055606262860238 , diff:  0.03501574706911015
adv train loss:  -0.0614611592154688 , diff:  0.01909490341313358
adv train loss:  -0.03682908469636459 , diff:  0.024632074519104208
adv train loss:  -0.04064527425327924 , diff:  0.003816189556914651
layer  9  adv train finish, try to retain  31
test acc: top1 ->  46.98 ; top5 ->  94.86  and loss:  356.2678894996643
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.04224415763746947
test acc: top1 ->  91.65 ; top5 ->  99.13  and loss:  99.96775740385056
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.03273439534677891
test acc: top1 ->  91.78 ; top5 ->  99.06  and loss:  98.63836026191711
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.033794591048149414
test acc: top1 ->  91.86 ; top5 ->  99.09  and loss:  98.35986407101154
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.03413606849790085
test acc: top1 ->  91.92 ; top5 ->  99.09  and loss:  97.53992500901222
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -0.025486909794381063 , diff:  0.025486909794381063
adv train loss:  -0.02733046881746759 , diff:  0.0018435590230865273
layer  10  adv train finish, try to retain  10
test acc: top1 ->  11.43 ; top5 ->  59.45  and loss:  1329.564561843872
forward train acc: top1 ->  99.87 ; top5 ->  100.0  and loss:  0.4613844266632441
test acc: top1 ->  91.23 ; top5 ->  98.92  and loss:  119.70958161354065
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.10631539454527683
test acc: top1 ->  91.61 ; top5 ->  98.96  and loss:  119.56014257669449
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.06963991308987261
test acc: top1 ->  91.61 ; top5 ->  98.98  and loss:  118.9355117380619
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.04189919363784611
test acc: top1 ->  91.64 ; top5 ->  98.96  and loss:  118.83826199173927
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.05150623775659824
test acc: top1 ->  91.84 ; top5 ->  98.97  and loss:  118.53321972489357
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.026214653112617725
test acc: top1 ->  91.73 ; top5 ->  98.94  and loss:  117.99369731545448
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.01994850791106728
test acc: top1 ->  91.78 ; top5 ->  98.97  and loss:  118.39599473774433
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.0452810884671635
test acc: top1 ->  91.66 ; top5 ->  99.0  and loss:  117.76891422271729
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.03346861444049409
test acc: top1 ->  91.64 ; top5 ->  98.93  and loss:  118.31885346770287
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.015199966175714508
test acc: top1 ->  91.66 ; top5 ->  98.93  and loss:  117.18341171741486
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -0.6478519985394087 , diff:  0.6478519985394087
adv train loss:  -0.6716664803388994 , diff:  0.02381448179949075
adv train loss:  -0.6266446115914732 , diff:  0.0450218687474262
adv train loss:  -0.6349256172688911 , diff:  0.008281005677417852
layer  11  adv train finish, try to retain  48
test acc: top1 ->  85.72 ; top5 ->  98.37  and loss:  88.15633389353752
forward train acc: top1 ->  99.914 ; top5 ->  99.998  and loss:  0.35433145628121565
test acc: top1 ->  91.85 ; top5 ->  99.03  and loss:  110.90973383188248
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.021386730892118067
test acc: top1 ->  91.95 ; top5 ->  99.04  and loss:  108.09494639933109
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.023721051753454958
test acc: top1 ->  91.92 ; top5 ->  98.94  and loss:  108.84928084909916
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.04245235420722793
test acc: top1 ->  91.83 ; top5 ->  98.96  and loss:  107.9719190299511
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.010870624992776357
test acc: top1 ->  91.9 ; top5 ->  99.01  and loss:  108.23971231281757
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.02102995167433619
test acc: top1 ->  91.85 ; top5 ->  99.01  and loss:  107.85699562728405
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.02192251150950142
test acc: top1 ->  91.97 ; top5 ->  98.97  and loss:  107.53671541810036
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -0.018276705046218922 , diff:  0.018276705046218922
adv train loss:  -0.01657047917251475 , diff:  0.0017062258737041702
layer  12  adv train finish, try to retain  11
test acc: top1 ->  48.28 ; top5 ->  94.65  and loss:  378.7369556427002
forward train acc: top1 ->  81.11400000976562 ; top5 ->  99.954  and loss:  78.21870070695877
test acc: top1 ->  87.62 ; top5 ->  99.19  and loss:  63.883355021476746
forward train acc: top1 ->  99.274 ; top5 ->  100.0  and loss:  7.017564093694091
test acc: top1 ->  90.77 ; top5 ->  99.15  and loss:  48.48912379145622
forward train acc: top1 ->  99.822 ; top5 ->  100.0  and loss:  2.1339801363646984
test acc: top1 ->  91.26 ; top5 ->  99.1  and loss:  46.39886122941971
forward train acc: top1 ->  99.914 ; top5 ->  100.0  and loss:  1.0652318317443132
test acc: top1 ->  91.39 ; top5 ->  99.06  and loss:  46.15534106642008
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.6300308317877352
test acc: top1 ->  91.47 ; top5 ->  99.05  and loss:  47.231082767248154
forward train acc: top1 ->  99.96399997558593 ; top5 ->  100.0  and loss:  0.44179197563789785
test acc: top1 ->  91.5 ; top5 ->  99.01  and loss:  47.3402976244688
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.36765152239240706
test acc: top1 ->  91.48 ; top5 ->  99.01  and loss:  47.68606351315975
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.30799375055357814
test acc: top1 ->  91.55 ; top5 ->  98.99  and loss:  47.755010798573494
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.27888624486513436
test acc: top1 ->  91.62 ; top5 ->  99.01  and loss:  48.23933669179678
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.2632186844712123
test acc: top1 ->  91.58 ; top5 ->  99.05  and loss:  49.266743898391724
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -1.183667597011663 , diff:  1.183667597011663
adv train loss:  -1.5791571015724912 , diff:  0.3954895045608282
adv train loss:  -1.5595275876694359 , diff:  0.01962951390305534
adv train loss:  -1.4846412115730345 , diff:  0.07488637609640136
adv train loss:  -1.267695638583973 , diff:  0.21694557298906147
adv train loss:  -1.2554096053354442 , diff:  0.012286033248528838
adv train loss:  -1.411324452899862 , diff:  0.15591484756441787
adv train loss:  -1.3468950382084586 , diff:  0.06442941469140351
adv train loss:  -1.6387442294799257 , diff:  0.29184919127146713
adv train loss:  -1.4609878111514263 , diff:  0.17775641832849942
layer  13  adv train finish, try to retain  45
test acc: top1 ->  90.77 ; top5 ->  98.38  and loss:  129.2810284793377
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.23940362685129912
test acc: top1 ->  91.78 ; top5 ->  98.79  and loss:  112.490856975317
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.06774446942745271
test acc: top1 ->  91.87 ; top5 ->  98.85  and loss:  110.04851660132408
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.03789343118582167
test acc: top1 ->  91.77 ; top5 ->  98.99  and loss:  107.98446017503738
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.04906275676887617
test acc: top1 ->  91.91 ; top5 ->  99.0  and loss:  106.2609221637249
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.00720857775559125
test acc: top1 ->  91.94 ; top5 ->  98.95  and loss:  106.41820573806763
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.01740102558005674
test acc: top1 ->  91.91 ; top5 ->  98.95  and loss:  105.89772257208824
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.021029739824371063
test acc: top1 ->  91.98 ; top5 ->  98.99  and loss:  105.21049731969833
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.01160753494571054
test acc: top1 ->  92.03 ; top5 ->  98.94  and loss:  106.73201720416546
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.020576597196850344
test acc: top1 ->  91.96 ; top5 ->  98.97  and loss:  105.9213095754385
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.026577415718747943
test acc: top1 ->  92.01 ; top5 ->  98.99  and loss:  105.5164146721363
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
layer  0  :  0.5  ==>  32 / 64 , inc:  2
layer  1  :  0  ==>  64 / 64 , inc:  1
layer  2  :  0  ==>  128 / 128 , inc:  1
layer  3  :  0  ==>  128 / 128 , inc:  1
layer  4  :  0  ==>  256 / 256 , inc:  1
layer  5  :  0  ==>  256 / 256 , inc:  1
layer  6  :  0  ==>  256 / 256 , inc:  1
layer  7  :  0  ==>  512 / 512 , inc:  1
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0  ==>  512 / 512 , inc:  1
layer  11  :  0  ==>  512 / 512 , inc:  1
layer  12  :  0  ==>  512 / 512 , inc:  1
layer  13  :  0  ==>  512 / 512 , inc:  1
eps [0.00042187500000000005, 0.00031640625000000006, 0.00031640625000000006, 0.00031640625000000006, 0.00031640625000000006, 0.00031640625000000006, 0.00031640625000000006, 0.00031640625000000006, 0.00031640625000000006, 0.00031640625000000006, 0.00031640625000000006, 0.00031640625000000006, 0.00031640625000000006, 0.00031640625000000006]  wait [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]  inc [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 3
$$$$$$$$$$$$$ epoch  4  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -0.01900646562611996 , diff:  0.01900646562611996
adv train loss:  -0.017514764066618227 , diff:  0.0014917015595017347
layer  0  adv train finish, try to retain  64
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -0.03164451666407331 , diff:  0.03164451666407331
adv train loss:  -0.028406542140146485 , diff:  0.003237974523926823
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  63
test acc: top1 ->  91.94 ; top5 ->  99.01  and loss:  105.42479258775711
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.022605713889959134
test acc: top1 ->  91.89 ; top5 ->  98.76  and loss:  109.43626880645752
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.027037162731133435
test acc: top1 ->  91.89 ; top5 ->  98.94  and loss:  109.7721017897129
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.031249907828168944
test acc: top1 ->  91.94 ; top5 ->  98.9  and loss:  109.54710972309113
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.020900825318221905
test acc: top1 ->  91.89 ; top5 ->  98.86  and loss:  109.95614790916443
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.008535983641358769
test acc: top1 ->  92.03 ; top5 ->  98.92  and loss:  108.40287479758263
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.013801656300302056
test acc: top1 ->  92.0 ; top5 ->  98.95  and loss:  108.37942464649677
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.014562569327154051
test acc: top1 ->  91.92 ; top5 ->  98.9  and loss:  108.05043959617615
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  64 / 64 , inc:  1
---------------- start layer  2  ---------------
************ all values are small in this layer **********
layer  2  adv train finish, try to retain  127
test acc: top1 ->  92.02 ; top5 ->  98.91  and loss:  108.49764451384544
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.020021325740685825
test acc: top1 ->  91.99 ; top5 ->  98.94  and loss:  110.96374629437923
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.010830248855199898
test acc: top1 ->  91.99 ; top5 ->  98.82  and loss:  111.50241401791573
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.00804898050319025
test acc: top1 ->  91.89 ; top5 ->  98.9  and loss:  112.8856874704361
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.028462903143633866
test acc: top1 ->  91.88 ; top5 ->  98.87  and loss:  111.84237226843834
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.01599813658655691
test acc: top1 ->  91.96 ; top5 ->  98.88  and loss:  111.86258879303932
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.008456580257359292
test acc: top1 ->  91.77 ; top5 ->  99.01  and loss:  110.88606530427933
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.028630427113967016
test acc: top1 ->  91.89 ; top5 ->  98.98  and loss:  109.78865006566048
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.014313002391219243
test acc: top1 ->  91.92 ; top5 ->  98.95  and loss:  109.35772152245045
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.023362990787973104
test acc: top1 ->  91.88 ; top5 ->  98.94  and loss:  110.7696810811758
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.008619907794411574
test acc: top1 ->  91.93 ; top5 ->  98.98  and loss:  109.88512805104256
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  128 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -0.01402566936093308 , diff:  0.01402566936093308
adv train loss:  -0.01972654345445335 , diff:  0.0057008740935202695
************ all values are small in this layer **********
layer  3  adv train finish, try to retain  127
test acc: top1 ->  91.91 ; top5 ->  98.97  and loss:  110.422303378582
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.03771647868329353
test acc: top1 ->  91.96 ; top5 ->  98.98  and loss:  110.0302626490593
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.02856687628662513
test acc: top1 ->  91.85 ; top5 ->  99.0  and loss:  109.74637892842293
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.024251463776238324
test acc: top1 ->  91.88 ; top5 ->  98.99  and loss:  110.46745800971985
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.008003272266535078
test acc: top1 ->  92.01 ; top5 ->  98.87  and loss:  110.76947529613972
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.025136741258393158
test acc: top1 ->  91.92 ; top5 ->  99.01  and loss:  109.26301927864552
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.019495385315224212
test acc: top1 ->  91.92 ; top5 ->  98.93  and loss:  111.94895523786545
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.03207774614782011
test acc: top1 ->  91.88 ; top5 ->  98.95  and loss:  110.78380812704563
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.007448351643688511
test acc: top1 ->  91.87 ; top5 ->  98.98  and loss:  108.57140043377876
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.02027214117870635
test acc: top1 ->  91.95 ; top5 ->  99.0  and loss:  107.2565900683403
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.008514287534580944
test acc: top1 ->  91.98 ; top5 ->  98.97  and loss:  108.38848030567169
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  128 / 128 , inc:  1
---------------- start layer  4  ---------------
************ all values are small in this layer **********
layer  4  adv train finish, try to retain  255
test acc: top1 ->  91.95 ; top5 ->  98.98  and loss:  108.41597664356232
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.021567527916090512
test acc: top1 ->  91.93 ; top5 ->  98.98  and loss:  108.60255244374275
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.01535310778741561
test acc: top1 ->  91.99 ; top5 ->  98.89  and loss:  111.76053507626057
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.017301659755048604
test acc: top1 ->  91.98 ; top5 ->  98.95  and loss:  109.40210917592049
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.03573888570235795
test acc: top1 ->  92.01 ; top5 ->  98.94  and loss:  109.03418917953968
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.012246126203535823
test acc: top1 ->  91.94 ; top5 ->  98.92  and loss:  110.73239849507809
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.01984722901320879
test acc: top1 ->  91.89 ; top5 ->  99.0  and loss:  109.10262763500214
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.0182958282783261
test acc: top1 ->  91.89 ; top5 ->  99.02  and loss:  108.53430277109146
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.004106724449314925
test acc: top1 ->  91.93 ; top5 ->  99.02  and loss:  108.98371560871601
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.02514012173651281
test acc: top1 ->  91.86 ; top5 ->  98.99  and loss:  109.03237956762314
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.0059921488915506416
test acc: top1 ->  91.84 ; top5 ->  98.95  and loss:  108.45112046599388
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -0.013791024933759388 , diff:  0.013791024933759388
adv train loss:  -0.02161570955559 , diff:  0.007824684621830613
************ all values are small in this layer **********
layer  5  adv train finish, try to retain  255
test acc: top1 ->  91.86 ; top5 ->  98.96  and loss:  109.18865925073624
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.03257367679630363
test acc: top1 ->  91.94 ; top5 ->  98.94  and loss:  109.62994968891144
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.017488768441864977
test acc: top1 ->  91.92 ; top5 ->  99.05  and loss:  109.24088729918003
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.008768034568632288
test acc: top1 ->  91.9 ; top5 ->  98.91  and loss:  111.41722519695759
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.00944134282049447
test acc: top1 ->  91.89 ; top5 ->  98.9  and loss:  112.66324074566364
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  6  ---------------
************ all values are small in this layer **********
layer  6  adv train finish, try to retain  255
test acc: top1 ->  91.91 ; top5 ->  98.91  and loss:  112.14648948609829
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.021497186072394925
test acc: top1 ->  92.0 ; top5 ->  98.99  and loss:  111.98686634004116
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.009339738719518209
test acc: top1 ->  91.91 ; top5 ->  99.01  and loss:  111.21961395442486
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.012522204820925253
test acc: top1 ->  92.01 ; top5 ->  99.03  and loss:  110.13197189569473
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.03269424284303568
test acc: top1 ->  91.92 ; top5 ->  98.98  and loss:  110.22984659671783
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.027395110689212743
test acc: top1 ->  92.03 ; top5 ->  99.0  and loss:  109.2595442533493
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.01508928662224207
test acc: top1 ->  92.01 ; top5 ->  99.02  and loss:  108.17874492704868
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.01849105959922781
test acc: top1 ->  91.99 ; top5 ->  98.99  and loss:  109.32816362380981
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.006710826057314989
test acc: top1 ->  91.98 ; top5 ->  99.02  and loss:  108.67904798686504
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.005393474214088201
test acc: top1 ->  91.98 ; top5 ->  99.01  and loss:  108.75857605040073
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.006140674001017032
test acc: top1 ->  92.03 ; top5 ->  99.02  and loss:  108.19502727687359
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  7  ---------------
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  511
test acc: top1 ->  92.02 ; top5 ->  98.97  and loss:  109.45817637443542
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.0030951955714044743
test acc: top1 ->  91.95 ; top5 ->  99.01  and loss:  110.52439552545547
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.006856863795064783
test acc: top1 ->  91.88 ; top5 ->  98.99  and loss:  112.1068505346775
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.004782539783718676
test acc: top1 ->  92.01 ; top5 ->  98.98  and loss:  115.76926209032536
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.002396532605871471
test acc: top1 ->  91.99 ; top5 ->  98.99  and loss:  115.43288831412792
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.0036854142590208028
test acc: top1 ->  91.96 ; top5 ->  98.97  and loss:  116.90548133850098
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.02116588947544784
test acc: top1 ->  92.03 ; top5 ->  99.0  and loss:  118.04729035496712
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.01912887778027539
test acc: top1 ->  91.93 ; top5 ->  98.94  and loss:  116.90107347071171
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.016879529642324087
test acc: top1 ->  91.99 ; top5 ->  99.02  and loss:  114.83258785307407
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.007735958415139521
test acc: top1 ->  91.98 ; top5 ->  98.97  and loss:  116.55290760099888
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.011960265688685467
test acc: top1 ->  91.91 ; top5 ->  99.01  and loss:  114.67721408605576
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -0.015565457884463285 , diff:  0.015565457884463285
adv train loss:  -0.020941420349117834 , diff:  0.005375962464654549
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  511
test acc: top1 ->  92.02 ; top5 ->  98.99  and loss:  114.93820589780807
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.008489576865827075
test acc: top1 ->  91.95 ; top5 ->  99.02  and loss:  117.1553515791893
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.017169994753885476
test acc: top1 ->  92.01 ; top5 ->  98.99  and loss:  117.17466807365417
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.00736310625961778
test acc: top1 ->  91.96 ; top5 ->  98.92  and loss:  119.46927528083324
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.037492801179723756
test acc: top1 ->  91.84 ; top5 ->  98.86  and loss:  118.63045656681061
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.0195321953838814
test acc: top1 ->  91.73 ; top5 ->  98.9  and loss:  118.06439989805222
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.023580144113338974
test acc: top1 ->  91.7 ; top5 ->  98.97  and loss:  116.28029239177704
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.01138027552622134
test acc: top1 ->  91.76 ; top5 ->  98.95  and loss:  117.17029613256454
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.009694765030872077
test acc: top1 ->  91.83 ; top5 ->  98.94  and loss:  116.94939461350441
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.003165413949915319
test acc: top1 ->  91.88 ; top5 ->  98.96  and loss:  117.1496784389019
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.006078492470351193
test acc: top1 ->  91.91 ; top5 ->  98.96  and loss:  116.74605169892311
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -0.00618155465451764 , diff:  0.00618155465451764
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  511
test acc: top1 ->  91.82 ; top5 ->  98.95  and loss:  117.08365604281425
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.04458184848954261
test acc: top1 ->  91.82 ; top5 ->  98.92  and loss:  115.16442650556564
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.005421553810482038
test acc: top1 ->  91.87 ; top5 ->  98.94  and loss:  115.75778537988663
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.012759019052850817
test acc: top1 ->  91.71 ; top5 ->  98.89  and loss:  116.50575232505798
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.01308446856741341
test acc: top1 ->  91.79 ; top5 ->  98.89  and loss:  117.35848128795624
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -0.004011100712887128 , diff:  0.004011100712887128
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  511
test acc: top1 ->  91.78 ; top5 ->  98.89  and loss:  117.66903755068779
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.02910348033765331
test acc: top1 ->  91.83 ; top5 ->  98.99  and loss:  115.45107036828995
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.0046230952416834725
test acc: top1 ->  91.84 ; top5 ->  98.97  and loss:  116.85628390312195
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.00851366460784675
test acc: top1 ->  91.74 ; top5 ->  98.96  and loss:  119.88167533278465
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.013654265689183376
test acc: top1 ->  91.8 ; top5 ->  98.94  and loss:  119.59026369452477
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.02565796127419162
test acc: top1 ->  91.85 ; top5 ->  98.95  and loss:  119.04580354690552
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.0066253987770323874
test acc: top1 ->  91.89 ; top5 ->  98.94  and loss:  118.57323989272118
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.01950571470175788
test acc: top1 ->  91.93 ; top5 ->  98.98  and loss:  118.46525084972382
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.00790489935388905
test acc: top1 ->  91.94 ; top5 ->  98.96  and loss:  118.17764389514923
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.019951760028561694
test acc: top1 ->  91.88 ; top5 ->  98.97  and loss:  117.54677253961563
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.006379163585307879
test acc: top1 ->  91.87 ; top5 ->  98.98  and loss:  117.14619137346745
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -0.0042736885873040364 , diff:  0.0042736885873040364
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  511
test acc: top1 ->  91.93 ; top5 ->  98.93  and loss:  117.50891581177711
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.012456161524823983
test acc: top1 ->  91.89 ; top5 ->  98.96  and loss:  118.17277285456657
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.013327440600818363
test acc: top1 ->  91.95 ; top5 ->  98.99  and loss:  118.08218988776207
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -0.058223530381968036 , diff:  0.058223530381968036
adv train loss:  -0.01026402063871501 , diff:  0.047959509743253026
adv train loss:  -0.029351158372492137 , diff:  0.019087137733777126
adv train loss:  -0.017591528507182375 , diff:  0.011759629865309762
adv train loss:  -0.022201323030458298 , diff:  0.004609794523275923
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  511
test acc: top1 ->  91.94 ; top5 ->  98.99  and loss:  118.45471450686455
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03523543383233374
test acc: top1 ->  91.81 ; top5 ->  99.02  and loss:  119.81220817565918
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.011589459175453953
test acc: top1 ->  91.84 ; top5 ->  99.01  and loss:  119.9040110707283
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.003907894506795628
test acc: top1 ->  91.77 ; top5 ->  98.99  and loss:  119.75799143314362
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.009437889944479139
test acc: top1 ->  91.8 ; top5 ->  98.96  and loss:  117.64436060190201
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.005602684551504922
test acc: top1 ->  91.86 ; top5 ->  98.92  and loss:  117.55738523602486
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.011260222484452242
test acc: top1 ->  91.91 ; top5 ->  98.93  and loss:  117.3872600197792
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.00800352669986637
test acc: top1 ->  91.88 ; top5 ->  98.93  and loss:  117.02269810438156
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.013586136114358993
test acc: top1 ->  91.92 ; top5 ->  98.95  and loss:  115.77833607792854
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.010166080148849233
test acc: top1 ->  91.88 ; top5 ->  98.93  and loss:  117.56920951604843
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.005621854015251948
test acc: top1 ->  91.81 ; top5 ->  98.95  and loss:  117.29859444499016
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  13  ---------------
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  511
test acc: top1 ->  91.87 ; top5 ->  98.92  and loss:  117.46663129329681
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.015129507826713962
test acc: top1 ->  91.89 ; top5 ->  98.98  and loss:  119.40178173780441
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.007504255667818427
test acc: top1 ->  91.95 ; top5 ->  98.95  and loss:  117.86701306700706
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.02074274532100162
test acc: top1 ->  91.9 ; top5 ->  98.96  and loss:  116.42158228158951
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.024276030429973616
test acc: top1 ->  91.92 ; top5 ->  99.0  and loss:  115.63374768197536
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.00448419954250312
test acc: top1 ->  91.94 ; top5 ->  98.96  and loss:  117.1593358963728
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.014531102041928534
test acc: top1 ->  91.96 ; top5 ->  98.94  and loss:  117.34190183877945
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.01841533398481232
test acc: top1 ->  91.98 ; top5 ->  99.0  and loss:  116.75654612481594
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.01192671387789801
test acc: top1 ->  91.87 ; top5 ->  98.96  and loss:  116.7885652333498
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.003857920493032907
test acc: top1 ->  91.91 ; top5 ->  98.96  and loss:  117.68475258350372
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.023508945608185172
test acc: top1 ->  92.01 ; top5 ->  98.97  and loss:  117.88183772563934
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
layer  0  :  0.5  ==>  32 / 64 , inc:  2
layer  1  :  0  ==>  64 / 64 , inc:  1
layer  2  :  0  ==>  128 / 128 , inc:  1
layer  3  :  0  ==>  128 / 128 , inc:  1
layer  4  :  0  ==>  256 / 256 , inc:  1
layer  5  :  0  ==>  256 / 256 , inc:  1
layer  6  :  0  ==>  256 / 256 , inc:  1
layer  7  :  0  ==>  512 / 512 , inc:  1
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0  ==>  512 / 512 , inc:  1
layer  11  :  0  ==>  512 / 512 , inc:  1
layer  12  :  0  ==>  512 / 512 , inc:  1
layer  13  :  0  ==>  512 / 512 , inc:  1
eps [0.0008437500000000001, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005]  wait [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]  inc [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 3
$$$$$$$$$$$$$ epoch  5  $$$$$$$$$$$$
---------------- start layer  0  ---------------
layer  0  adv train finish, try to retain  64
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
### skip layer  1 wait:  4  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
### skip layer  8 wait:  4  ###
---------------- start layer  9  ---------------
### skip layer  9 wait:  4  ###
---------------- start layer  10  ---------------
### skip layer  10 wait:  4  ###
---------------- start layer  11  ---------------
### skip layer  11 wait:  4  ###
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
### skip layer  13 wait:  4  ###
layer  0  :  0.5  ==>  32 / 64 , inc:  2
layer  1  :  0  ==>  64 / 64 , inc:  1
layer  2  :  0  ==>  128 / 128 , inc:  1
layer  3  :  0  ==>  128 / 128 , inc:  1
layer  4  :  0  ==>  256 / 256 , inc:  1
layer  5  :  0  ==>  256 / 256 , inc:  1
layer  6  :  0  ==>  256 / 256 , inc:  1
layer  7  :  0  ==>  512 / 512 , inc:  1
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0  ==>  512 / 512 , inc:  1
layer  11  :  0  ==>  512 / 512 , inc:  1
layer  12  :  0  ==>  512 / 512 , inc:  1
layer  13  :  0  ==>  512 / 512 , inc:  1
eps [0.0016875000000000002, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005]  wait [0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]  inc [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 3
$$$$$$$$$$$$$ epoch  6  $$$$$$$$$$$$
---------------- start layer  0  ---------------
layer  0  adv train finish, try to retain  64
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
### skip layer  1 wait:  3  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  3  ###
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
### skip layer  8 wait:  3  ###
---------------- start layer  9  ---------------
### skip layer  9 wait:  3  ###
---------------- start layer  10  ---------------
### skip layer  10 wait:  3  ###
---------------- start layer  11  ---------------
### skip layer  11 wait:  3  ###
---------------- start layer  12  ---------------
### skip layer  12 wait:  3  ###
---------------- start layer  13  ---------------
### skip layer  13 wait:  3  ###
layer  0  :  0.5  ==>  32 / 64 , inc:  2
layer  1  :  0  ==>  64 / 64 , inc:  1
layer  2  :  0  ==>  128 / 128 , inc:  1
layer  3  :  0  ==>  128 / 128 , inc:  1
layer  4  :  0  ==>  256 / 256 , inc:  1
layer  5  :  0  ==>  256 / 256 , inc:  1
layer  6  :  0  ==>  256 / 256 , inc:  1
layer  7  :  0  ==>  512 / 512 , inc:  1
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0  ==>  512 / 512 , inc:  1
layer  11  :  0  ==>  512 / 512 , inc:  1
layer  12  :  0  ==>  512 / 512 , inc:  1
layer  13  :  0  ==>  512 / 512 , inc:  1
eps [0.0033750000000000004, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005, 0.00023730468750000005]  wait [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]  inc [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 3
$$$$$$$$$$$$$ epoch  7  $$$$$$$$$$$$
---------------- start layer  0  ---------------
layer  0  adv train finish, try to retain  64
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
layer  1  adv train finish, try to retain  55
test acc: top1 ->  91.98 ; top5 ->  98.97  and loss:  117.26465454697609
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.016422721875187563
test acc: top1 ->  91.89 ; top5 ->  98.98  and loss:  118.39164474606514
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.004489852177357534
test acc: top1 ->  91.91 ; top5 ->  98.94  and loss:  119.73921473324299
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.01621513994905399
test acc: top1 ->  91.82 ; top5 ->  98.94  and loss:  120.81109650433064
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.012441692528568637
test acc: top1 ->  91.83 ; top5 ->  98.96  and loss:  118.85041064023972
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.00455288854858793
test acc: top1 ->  91.84 ; top5 ->  98.91  and loss:  118.29036588966846
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.0014848771546285278
test acc: top1 ->  91.9 ; top5 ->  98.93  and loss:  119.20636720955372
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.024033819419855718
test acc: top1 ->  91.75 ; top5 ->  99.01  and loss:  119.16542296111584
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.012777365479593072
test acc: top1 ->  91.93 ; top5 ->  99.0  and loss:  120.31394158303738
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.007101661349484978
test acc: top1 ->  91.87 ; top5 ->  98.97  and loss:  120.2121903449297
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.008690059273277484
test acc: top1 ->  91.88 ; top5 ->  99.0  and loss:  119.34693433344364
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  64 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -0.01878793706629267 , diff:  0.01878793706629267
adv train loss:  -0.012659154623612778 , diff:  0.006128782442679892
layer  2  adv train finish, try to retain  99
test acc: top1 ->  91.89 ; top5 ->  99.0  and loss:  119.46234986186028
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.007128183389795595
test acc: top1 ->  91.93 ; top5 ->  99.0  and loss:  121.28286221623421
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.026683468711866
test acc: top1 ->  91.87 ; top5 ->  98.96  and loss:  119.20633780956268
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.01552521745594504
test acc: top1 ->  91.89 ; top5 ->  99.0  and loss:  120.57918381690979
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.02343983100096647
test acc: top1 ->  91.84 ; top5 ->  98.99  and loss:  120.69337752461433
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.023648864876577136
test acc: top1 ->  91.99 ; top5 ->  99.0  and loss:  119.63060067594051
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  128 / 128 , inc:  1
---------------- start layer  3  ---------------
layer  3  adv train finish, try to retain  87
test acc: top1 ->  91.96 ; top5 ->  98.99  and loss:  120.15098743140697
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.004979122797521995
test acc: top1 ->  91.94 ; top5 ->  99.01  and loss:  121.13449946045876
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.01828482432011569
test acc: top1 ->  91.95 ; top5 ->  98.99  and loss:  120.31126508116722
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.02762889640064259
test acc: top1 ->  91.93 ; top5 ->  98.9  and loss:  121.5688610970974
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.03427846640366283
test acc: top1 ->  91.82 ; top5 ->  98.99  and loss:  119.66764813661575
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03351536310719894
test acc: top1 ->  91.68 ; top5 ->  99.0  and loss:  119.1808906942606
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  128 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -0.0253782373138165 , diff:  0.0253782373138165
adv train loss:  -0.027581912956261334 , diff:  0.0022036756424448356
layer  4  adv train finish, try to retain  154
test acc: top1 ->  91.84 ; top5 ->  99.03  and loss:  119.31081362068653
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.059277133500700074
test acc: top1 ->  91.93 ; top5 ->  98.95  and loss:  118.15386037528515
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.052874230471516626
test acc: top1 ->  91.87 ; top5 ->  98.92  and loss:  118.49099960923195
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.03342833442911797
test acc: top1 ->  92.0 ; top5 ->  98.93  and loss:  118.19913007318974
forward train acc: top1 ->  99.98399997558593 ; top5 ->  100.0  and loss:  0.04070421456890472
test acc: top1 ->  91.93 ; top5 ->  98.96  and loss:  116.98630617558956
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.01719562965718069
test acc: top1 ->  91.94 ; top5 ->  98.94  and loss:  117.39953105151653
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.03099661383566854
test acc: top1 ->  91.91 ; top5 ->  98.98  and loss:  117.48689852654934
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.01868688042247868
test acc: top1 ->  92.0 ; top5 ->  98.93  and loss:  117.59712003171444
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.04070206605047133
test acc: top1 ->  91.89 ; top5 ->  98.99  and loss:  115.50288279354572
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.04565088931485661
test acc: top1 ->  91.92 ; top5 ->  98.97  and loss:  116.24389988183975
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.03110807848028685
test acc: top1 ->  91.87 ; top5 ->  99.0  and loss:  115.77113653719425
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -0.027790841281195355 , diff:  0.027790841281195355
adv train loss:  -0.023171659660874866 , diff:  0.004619181620320489
layer  5  adv train finish, try to retain  149
test acc: top1 ->  91.86 ; top5 ->  99.07  and loss:  114.46564896404743
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.020193291766709365
test acc: top1 ->  91.91 ; top5 ->  98.97  and loss:  116.42648336291313
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.017311139625235228
test acc: top1 ->  91.83 ; top5 ->  98.94  and loss:  115.24792693555355
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.021472435780324872
test acc: top1 ->  91.84 ; top5 ->  98.93  and loss:  115.10601499676704
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.01057776779998676
test acc: top1 ->  91.82 ; top5 ->  98.96  and loss:  115.06545624136925
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.018884023599639477
test acc: top1 ->  91.87 ; top5 ->  99.01  and loss:  114.02149000763893
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.00831080556690722
test acc: top1 ->  91.89 ; top5 ->  98.96  and loss:  112.88202512264252
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.0081287394586127
test acc: top1 ->  91.93 ; top5 ->  99.02  and loss:  112.9999870955944
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  6  ---------------
************ all values are small in this layer **********
layer  6  adv train finish, try to retain  255
test acc: top1 ->  91.97 ; top5 ->  99.02  and loss:  113.72914837300777
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.003062167515185621
test acc: top1 ->  91.92 ; top5 ->  99.03  and loss:  115.64594411849976
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.0026884810061673647
test acc: top1 ->  91.82 ; top5 ->  99.01  and loss:  117.46202629804611
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  7  ---------------
layer  7  adv train finish, try to retain  5
test acc: top1 ->  12.94 ; top5 ->  58.63  and loss:  786.8526921272278
forward train acc: top1 ->  52.73799999633789 ; top5 ->  92.62799998535156  and loss:  368.61550867557526
test acc: top1 ->  55.62 ; top5 ->  94.93  and loss:  268.82537150382996
forward train acc: top1 ->  65.67199997314454 ; top5 ->  98.18799998046875  and loss:  122.57430183887482
test acc: top1 ->  64.66 ; top5 ->  96.76  and loss:  179.68356084823608
forward train acc: top1 ->  72.80199999267577 ; top5 ->  99.11800000488282  and loss:  82.17001366615295
test acc: top1 ->  71.09 ; top5 ->  97.63  and loss:  142.97457855939865
forward train acc: top1 ->  78.73399997558593 ; top5 ->  99.40200000244141  and loss:  63.77449396252632
test acc: top1 ->  76.31 ; top5 ->  97.87  and loss:  120.45498871803284
forward train acc: top1 ->  83.18600000732422 ; top5 ->  99.5880000024414  and loss:  50.67527595162392
test acc: top1 ->  78.98 ; top5 ->  98.08  and loss:  108.56305760145187
forward train acc: top1 ->  85.80400000732422 ; top5 ->  99.66400000244141  and loss:  43.487248450517654
test acc: top1 ->  79.95 ; top5 ->  98.18  and loss:  101.82999715209007
forward train acc: top1 ->  86.64200001464843 ; top5 ->  99.70799997558593  and loss:  40.08064606785774
test acc: top1 ->  80.77 ; top5 ->  98.22  and loss:  97.64735183119774
forward train acc: top1 ->  88.05400001953124 ; top5 ->  99.728  and loss:  36.517571181058884
test acc: top1 ->  81.74 ; top5 ->  98.24  and loss:  94.39788821339607
forward train acc: top1 ->  89.08400000732422 ; top5 ->  99.7660000024414  and loss:  33.349017426371574
test acc: top1 ->  82.46 ; top5 ->  98.32  and loss:  92.87882795929909
forward train acc: top1 ->  89.93000001220703 ; top5 ->  99.81  and loss:  30.976247176527977
test acc: top1 ->  83.17 ; top5 ->  98.36  and loss:  90.5128892660141
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -12.676155231893063 , diff:  12.676155231893063
adv train loss:  -12.819508450105786 , diff:  0.14335321821272373
adv train loss:  -12.80378944426775 , diff:  0.015719005838036537
layer  8  adv train finish, try to retain  126
test acc: top1 ->  86.01 ; top5 ->  97.74  and loss:  116.61586648225784
forward train acc: top1 ->  99.654 ; top5 ->  100.0  and loss:  1.303298421044019
test acc: top1 ->  91.39 ; top5 ->  98.89  and loss:  93.72843798995018
forward train acc: top1 ->  99.93599997558594 ; top5 ->  100.0  and loss:  0.19797902816935675
test acc: top1 ->  91.62 ; top5 ->  98.92  and loss:  89.20720583200455
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.136116520196083
test acc: top1 ->  91.7 ; top5 ->  98.94  and loss:  87.61285097897053
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.1262514761856437
test acc: top1 ->  91.71 ; top5 ->  98.91  and loss:  88.32090599834919
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.07096098290094233
test acc: top1 ->  91.76 ; top5 ->  98.97  and loss:  87.32636806368828
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.05287274787951901
test acc: top1 ->  91.78 ; top5 ->  98.97  and loss:  87.52008570730686
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.09066927517346812
test acc: top1 ->  91.7 ; top5 ->  98.93  and loss:  87.48174186050892
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.04576781880314229
test acc: top1 ->  91.72 ; top5 ->  98.97  and loss:  87.27339006960392
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.058907074201670184
test acc: top1 ->  91.73 ; top5 ->  99.0  and loss:  86.81454421579838
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.08494717900066462
test acc: top1 ->  91.73 ; top5 ->  98.92  and loss:  87.25979673862457
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -0.08572964646737091 , diff:  0.08572964646737091
adv train loss:  -0.08522706780058797 , diff:  0.0005025786667829379
layer  9  adv train finish, try to retain  24
test acc: top1 ->  25.87 ; top5 ->  70.36  and loss:  609.7233552932739
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.21160434173361864
test acc: top1 ->  91.63 ; top5 ->  99.04  and loss:  88.06220453977585
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.03748147841542959
test acc: top1 ->  91.76 ; top5 ->  99.07  and loss:  89.65651908516884
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.0752853317944755
test acc: top1 ->  91.91 ; top5 ->  99.04  and loss:  87.93348768353462
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.060231996510765384
test acc: top1 ->  91.79 ; top5 ->  99.04  and loss:  88.27388320863247
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.02804456736521388
test acc: top1 ->  91.83 ; top5 ->  99.06  and loss:  88.06588259339333
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.029540753588662483
test acc: top1 ->  91.81 ; top5 ->  99.03  and loss:  88.71891567111015
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.030522143734287965
test acc: top1 ->  91.74 ; top5 ->  99.0  and loss:  88.78768263757229
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -0.08274410021931544 , diff:  0.08274410021931544
adv train loss:  -0.05103938761203608 , diff:  0.03170471260727936
adv train loss:  -0.08345427776839642 , diff:  0.03241489015636034
adv train loss:  -0.0669258955758778 , diff:  0.01652838219251862
adv train loss:  -0.11128229790483601 , diff:  0.04435640232895821
adv train loss:  -0.05748115059577685 , diff:  0.05380114730905916
adv train loss:  -0.09914548404412926 , diff:  0.04166433344835241
adv train loss:  -0.09344733798116067 , diff:  0.00569814606296859
layer  10  adv train finish, try to retain  24
test acc: top1 ->  32.35 ; top5 ->  71.03  and loss:  885.7594985961914
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.04766772262064478
test acc: top1 ->  91.75 ; top5 ->  99.05  and loss:  92.67311675846577
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.05519651027134387
test acc: top1 ->  91.74 ; top5 ->  98.98  and loss:  92.66627898812294
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03828041731026133
test acc: top1 ->  91.85 ; top5 ->  98.94  and loss:  93.48138332366943
forward train acc: top1 ->  99.98199997558594 ; top5 ->  100.0  and loss:  0.03785979880831292
test acc: top1 ->  91.91 ; top5 ->  99.06  and loss:  93.91721315681934
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -0.10599684455519309 , diff:  0.10599684455519309
adv train loss:  -0.05432907683461963 , diff:  0.051667767720573465
adv train loss:  -0.10507516680445406 , diff:  0.05074608996983443
adv train loss:  -0.09002212251834862 , diff:  0.015053044286105433
adv train loss:  -0.08407131858075445 , diff:  0.0059508039375941735
layer  11  adv train finish, try to retain  53
test acc: top1 ->  79.42 ; top5 ->  96.11  and loss:  131.88447362184525
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.1945911185684963
test acc: top1 ->  91.63 ; top5 ->  98.74  and loss:  113.00641703605652
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.04156667121787905
test acc: top1 ->  91.64 ; top5 ->  98.79  and loss:  112.47258657217026
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.029480614444764797
test acc: top1 ->  91.65 ; top5 ->  98.82  and loss:  112.06217673420906
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.03466786577541825
test acc: top1 ->  91.69 ; top5 ->  98.81  and loss:  110.05226877331734
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.08812117385571128
test acc: top1 ->  91.67 ; top5 ->  98.87  and loss:  107.71201810240746
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.024159382797733997
test acc: top1 ->  91.7 ; top5 ->  98.85  and loss:  107.19802522659302
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.021227296319835887
test acc: top1 ->  91.7 ; top5 ->  98.89  and loss:  106.48268668353558
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.017765738352125027
test acc: top1 ->  91.68 ; top5 ->  98.9  and loss:  107.07136391103268
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.03303586117044688
test acc: top1 ->  91.67 ; top5 ->  98.88  and loss:  106.96984009444714
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.03480176061123075
test acc: top1 ->  91.79 ; top5 ->  98.86  and loss:  106.86576789617538
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -0.2874009022361861 , diff:  0.2874009022361861
adv train loss:  -0.2749845417783945 , diff:  0.012416360457791598
adv train loss:  -0.36555261036846787 , diff:  0.09056806859007338
adv train loss:  -0.2412550822391495 , diff:  0.12429752812931838
adv train loss:  -0.2256894447018567 , diff:  0.015565637537292787
adv train loss:  -0.21026762590918224 , diff:  0.015421818792674458
adv train loss:  -0.2978700215844583 , diff:  0.08760239567527606
adv train loss:  -0.22550619635148905 , diff:  0.07236382523296925
adv train loss:  -0.2631023797148373 , diff:  0.037596183363348246
adv train loss:  -0.26106523776616086 , diff:  0.0020371419486764353
layer  12  adv train finish, try to retain  28
test acc: top1 ->  79.74 ; top5 ->  98.57  and loss:  87.51086515188217
forward train acc: top1 ->  99.24 ; top5 ->  99.998  and loss:  2.359442758373916
test acc: top1 ->  91.49 ; top5 ->  98.58  and loss:  54.89133482426405
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.2773301522247493
test acc: top1 ->  91.67 ; top5 ->  98.6  and loss:  57.50283531099558
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.1653367553371936
test acc: top1 ->  91.86 ; top5 ->  98.61  and loss:  60.12827269732952
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.10009235002507921
test acc: top1 ->  91.92 ; top5 ->  98.61  and loss:  63.47222585976124
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.061116119803045876
test acc: top1 ->  91.84 ; top5 ->  98.64  and loss:  64.09315330535173
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.06237528384372126
test acc: top1 ->  91.87 ; top5 ->  98.62  and loss:  64.67596703767776
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.06562603870406747
test acc: top1 ->  91.95 ; top5 ->  98.62  and loss:  65.48993622511625
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.053542650523013435
test acc: top1 ->  91.96 ; top5 ->  98.62  and loss:  65.54140456020832
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.054663232011080254
test acc: top1 ->  91.92 ; top5 ->  98.59  and loss:  67.34343411028385
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.04310108920617495
test acc: top1 ->  91.94 ; top5 ->  98.58  and loss:  67.83580431342125
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -0.08057244874339631 , diff:  0.08057244874339631
adv train loss:  -0.0517610067915939 , diff:  0.028811441951802408
adv train loss:  -0.039289789177018974 , diff:  0.012471217614574925
adv train loss:  -0.047289367284975015 , diff:  0.00799957810795604
layer  13  adv train finish, try to retain  43
test acc: top1 ->  91.49 ; top5 ->  98.88  and loss:  104.61868453025818
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.058029252528285724
test acc: top1 ->  91.91 ; top5 ->  98.92  and loss:  101.83353221416473
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.029507254075269884
test acc: top1 ->  91.93 ; top5 ->  98.97  and loss:  99.87775711715221
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.012434307907824405
test acc: top1 ->  92.14 ; top5 ->  98.93  and loss:  101.56406772136688
==> this epoch:  43 / 512
layer  0  :  0.5  ==>  32 / 64 , inc:  2
layer  1  :  0  ==>  64 / 64 , inc:  1
layer  2  :  0  ==>  128 / 128 , inc:  1
layer  3  :  0  ==>  128 / 128 , inc:  1
layer  4  :  0  ==>  256 / 256 , inc:  1
layer  5  :  0  ==>  256 / 256 , inc:  1
layer  6  :  0  ==>  256 / 256 , inc:  1
layer  7  :  0  ==>  512 / 512 , inc:  1
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0  ==>  512 / 512 , inc:  1
layer  11  :  0  ==>  512 / 512 , inc:  1
layer  12  :  0  ==>  512 / 512 , inc:  1
layer  13  :  0.083984375  ==>  43 / 512 , inc:  2
eps [0.006750000000000001, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.00023730468750000005]  wait [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0]  inc [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]  tol: 3
$$$$$$$$$$$$$ epoch  8  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -0.007648808735439161 , diff:  0.007648808735439161
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  30
test acc: top1 ->  27.85 ; top5 ->  69.93  and loss:  1017.6135454177856
forward train acc: top1 ->  90.94399998291016 ; top5 ->  99.18399997558593  and loss:  65.58483281731606
test acc: top1 ->  77.67 ; top5 ->  94.68  and loss:  197.91045320034027
forward train acc: top1 ->  93.16999997070313 ; top5 ->  99.51599997558594  and loss:  30.144013211131096
test acc: top1 ->  86.57 ; top5 ->  98.59  and loss:  79.01597189903259
forward train acc: top1 ->  93.94999999511718 ; top5 ->  99.64999997558594  and loss:  21.49797982722521
test acc: top1 ->  87.01 ; top5 ->  98.7  and loss:  66.88122552633286
forward train acc: top1 ->  94.81799999267578 ; top5 ->  99.71  and loss:  18.003895789384842
test acc: top1 ->  87.44 ; top5 ->  98.8  and loss:  60.08675613999367
forward train acc: top1 ->  95.38599998535156 ; top5 ->  99.76199997558594  and loss:  14.951595269143581
test acc: top1 ->  87.94 ; top5 ->  98.88  and loss:  55.86923275887966
forward train acc: top1 ->  95.77999997070313 ; top5 ->  99.78999997558594  and loss:  13.537159699946642
test acc: top1 ->  88.02 ; top5 ->  98.81  and loss:  55.349707186222076
forward train acc: top1 ->  96.02200001953125 ; top5 ->  99.818  and loss:  12.94657577201724
test acc: top1 ->  88.25 ; top5 ->  98.86  and loss:  53.75796191394329
forward train acc: top1 ->  96.15200001953124 ; top5 ->  99.8560000024414  and loss:  12.073367170989513
test acc: top1 ->  88.36 ; top5 ->  98.84  and loss:  53.20446467399597
forward train acc: top1 ->  96.33399998291016 ; top5 ->  99.87  and loss:  11.489844020456076
test acc: top1 ->  88.54 ; top5 ->  98.85  and loss:  52.1160983145237
forward train acc: top1 ->  96.64600000732422 ; top5 ->  99.862  and loss:  10.57729546353221
test acc: top1 ->  88.52 ; top5 ->  98.9  and loss:  51.906837582588196
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  2
---------------- start layer  1  ---------------
### skip layer  1 wait:  4  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
### skip layer  8 wait:  4  ###
---------------- start layer  9  ---------------
### skip layer  9 wait:  4  ###
---------------- start layer  10  ---------------
### skip layer  10 wait:  4  ###
---------------- start layer  11  ---------------
### skip layer  11 wait:  4  ###
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
adv train loss:  -1.5315783696714789 , diff:  1.5315783696714789
adv train loss:  -1.6012563507538289 , diff:  0.06967798108235002
adv train loss:  -1.6016777008771896 , diff:  0.00042135012336075306
layer  13  adv train finish, try to retain  492
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0  ==>  64 / 64 , inc:  1
layer  2  :  0  ==>  128 / 128 , inc:  1
layer  3  :  0  ==>  128 / 128 , inc:  1
layer  4  :  0  ==>  256 / 256 , inc:  1
layer  5  :  0  ==>  256 / 256 , inc:  1
layer  6  :  0  ==>  256 / 256 , inc:  1
layer  7  :  0  ==>  512 / 512 , inc:  1
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0  ==>  512 / 512 , inc:  1
layer  11  :  0  ==>  512 / 512 , inc:  1
layer  12  :  0  ==>  512 / 512 , inc:  1
layer  13  :  0.083984375  ==>  43 / 512 , inc:  2
eps [0.005062500000000001, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.0004746093750000001]  wait [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]  tol: 3
$$$$$$$$$$$$$ epoch  9  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -0.8415659042075276 , diff:  0.8415659042075276
adv train loss:  -0.8121878774836659 , diff:  0.029378026723861694
adv train loss:  -0.8483050353825092 , diff:  0.03611715789884329
adv train loss:  -0.8234860247466713 , diff:  0.024819010635837913
adv train loss:  -0.7780947939027101 , diff:  0.04539123084396124
adv train loss:  -0.8103284733369946 , diff:  0.03223367943428457
adv train loss:  -0.8005455951206386 , diff:  0.009782878216356039
layer  0  adv train finish, try to retain  64
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
### skip layer  1 wait:  3  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  3  ###
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
### skip layer  8 wait:  3  ###
---------------- start layer  9  ---------------
### skip layer  9 wait:  3  ###
---------------- start layer  10  ---------------
### skip layer  10 wait:  3  ###
---------------- start layer  11  ---------------
### skip layer  11 wait:  3  ###
---------------- start layer  12  ---------------
### skip layer  12 wait:  3  ###
---------------- start layer  13  ---------------
adv train loss:  -1.5559695588890463 , diff:  1.5559695588890463
adv train loss:  -1.6598537229001522 , diff:  0.1038841640111059
adv train loss:  -1.654170117340982 , diff:  0.005683605559170246
layer  13  adv train finish, try to retain  506
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0  ==>  64 / 64 , inc:  1
layer  2  :  0  ==>  128 / 128 , inc:  1
layer  3  :  0  ==>  128 / 128 , inc:  1
layer  4  :  0  ==>  256 / 256 , inc:  1
layer  5  :  0  ==>  256 / 256 , inc:  1
layer  6  :  0  ==>  256 / 256 , inc:  1
layer  7  :  0  ==>  512 / 512 , inc:  1
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0  ==>  512 / 512 , inc:  1
layer  11  :  0  ==>  512 / 512 , inc:  1
layer  12  :  0  ==>  512 / 512 , inc:  1
layer  13  :  0.083984375  ==>  43 / 512 , inc:  2
eps [0.010125000000000002, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.00017797851562500002, 0.0009492187500000002]  wait [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]  tol: 3
$$$$$$$$$$$$$ epoch  10  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -0.8149459580890834 , diff:  0.8149459580890834
adv train loss:  -0.8126851245760918 , diff:  0.002260833512991667
layer  0  adv train finish, try to retain  64
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -0.7970566754229367 , diff:  0.7970566754229367
adv train loss:  -0.8176175062544644 , diff:  0.02056083083152771
adv train loss:  -0.8143656707834452 , diff:  0.003251835471019149
layer  1  adv train finish, try to retain  56
test acc: top1 ->  91.38 ; top5 ->  99.19  and loss:  37.10054913908243
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.26250229105062317
test acc: top1 ->  91.92 ; top5 ->  99.24  and loss:  49.477505937218666
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.09675622275244677
test acc: top1 ->  91.83 ; top5 ->  99.17  and loss:  54.397839814424515
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.060257976525463164
test acc: top1 ->  91.91 ; top5 ->  99.09  and loss:  58.48209174722433
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.04495207657055289
test acc: top1 ->  91.91 ; top5 ->  99.1  and loss:  61.650020979344845
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.058118495868257014
test acc: top1 ->  92.0 ; top5 ->  99.12  and loss:  64.15153784304857
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.03315825483241497
test acc: top1 ->  91.92 ; top5 ->  99.12  and loss:  65.09650614857674
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.031268852323592
test acc: top1 ->  92.01 ; top5 ->  99.08  and loss:  67.2275920137763
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.029612139171149465
test acc: top1 ->  92.05 ; top5 ->  99.09  and loss:  66.87860860675573
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.02312349660314794
test acc: top1 ->  91.94 ; top5 ->  99.06  and loss:  68.20373959094286
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.027857837842020672
test acc: top1 ->  92.01 ; top5 ->  99.07  and loss:  67.9874817430973
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  64 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -0.02171962046122644 , diff:  0.02171962046122644
adv train loss:  -0.030293589364191575 , diff:  0.008573968902965134
layer  2  adv train finish, try to retain  100
test acc: top1 ->  91.99 ; top5 ->  99.05  and loss:  67.58089999109507
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.015023419210592692
test acc: top1 ->  91.93 ; top5 ->  99.05  and loss:  72.57642011344433
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.017002856026010704
test acc: top1 ->  91.96 ; top5 ->  99.09  and loss:  74.45477586239576
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.024207011730140948
test acc: top1 ->  91.98 ; top5 ->  99.05  and loss:  75.69010062515736
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.02305091199832532
test acc: top1 ->  92.01 ; top5 ->  99.05  and loss:  77.46959159523249
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.01787944653096929
test acc: top1 ->  91.98 ; top5 ->  99.12  and loss:  76.92767421156168
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.016775692272631204
test acc: top1 ->  92.04 ; top5 ->  99.14  and loss:  78.03447245806456
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.008083377695584204
test acc: top1 ->  92.1 ; top5 ->  99.11  and loss:  79.01624156534672
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.02368723880408652
test acc: top1 ->  91.93 ; top5 ->  99.15  and loss:  78.66566397249699
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.004390106972209651
test acc: top1 ->  91.9 ; top5 ->  99.13  and loss:  79.10577347874641
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.010700117190935998
test acc: top1 ->  91.92 ; top5 ->  99.14  and loss:  79.1617286503315
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  128 / 128 , inc:  1
---------------- start layer  3  ---------------
layer  3  adv train finish, try to retain  87
test acc: top1 ->  91.93 ; top5 ->  99.15  and loss:  79.65204514563084
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.016047952095959772
test acc: top1 ->  91.91 ; top5 ->  99.05  and loss:  83.26152575016022
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.004208794263149684
test acc: top1 ->  91.94 ; top5 ->  99.06  and loss:  85.49831838905811
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.012177418509054405
test acc: top1 ->  91.86 ; top5 ->  99.1  and loss:  86.15796610713005
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.004588656632677157
test acc: top1 ->  91.95 ; top5 ->  99.06  and loss:  87.37886671721935
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.037122414070836385
test acc: top1 ->  91.89 ; top5 ->  98.99  and loss:  86.00528982281685
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.018321858723538753
test acc: top1 ->  91.84 ; top5 ->  99.01  and loss:  86.836410805583
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.005264431957357374
test acc: top1 ->  91.88 ; top5 ->  98.99  and loss:  86.95188198983669
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.024460128059217823
test acc: top1 ->  91.97 ; top5 ->  99.02  and loss:  87.59210275113583
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.010376248632837815
test acc: top1 ->  92.02 ; top5 ->  99.02  and loss:  86.9858311265707
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.00549522095377597
test acc: top1 ->  92.01 ; top5 ->  99.05  and loss:  86.89278960227966
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  128 / 128 , inc:  1
---------------- start layer  4  ---------------
layer  4  adv train finish, try to retain  156
test acc: top1 ->  91.96 ; top5 ->  99.03  and loss:  87.46498607099056
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.006633354332734598
test acc: top1 ->  92.02 ; top5 ->  99.09  and loss:  89.6092841476202
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.02170454270526534
test acc: top1 ->  91.92 ; top5 ->  99.04  and loss:  90.8183931261301
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.019505943459535047
test acc: top1 ->  92.07 ; top5 ->  98.95  and loss:  91.59087605029345
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.004285768081899732
test acc: top1 ->  92.03 ; top5 ->  99.0  and loss:  90.40519294142723
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.014361408704189671
test acc: top1 ->  92.03 ; top5 ->  99.0  and loss:  93.76441642642021
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.009482872287662758
test acc: top1 ->  92.1 ; top5 ->  99.06  and loss:  93.34371310472488
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.013365440322104405
test acc: top1 ->  92.1 ; top5 ->  99.06  and loss:  91.90580432116985
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.010263014936754189
test acc: top1 ->  92.07 ; top5 ->  99.05  and loss:  91.74801932275295
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.009586077906988066
test acc: top1 ->  92.06 ; top5 ->  99.07  and loss:  91.46668510138988
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -0.004101644098227553 , diff:  0.004101644098227553
layer  5  adv train finish, try to retain  149
test acc: top1 ->  92.08 ; top5 ->  99.05  and loss:  91.22773218154907
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.002946008180060744
test acc: top1 ->  92.03 ; top5 ->  98.99  and loss:  93.43016171455383
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.004168382266470871
test acc: top1 ->  92.13 ; top5 ->  99.01  and loss:  95.23441934585571
==> this epoch:  149 / 256
---------------- start layer  6  ---------------
layer  6  adv train finish, try to retain  144
test acc: top1 ->  92.08 ; top5 ->  99.01  and loss:  95.75181291997433
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.0076856571638472815
test acc: top1 ->  92.05 ; top5 ->  99.01  and loss:  96.6608483940363
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.005554553836788045
test acc: top1 ->  91.96 ; top5 ->  99.01  and loss:  95.12232783436775
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.029845920402522097
test acc: top1 ->  92.11 ; top5 ->  99.0  and loss:  97.13348615169525
==> this epoch:  144 / 256
---------------- start layer  7  ---------------
adv train loss:  -0.013311053711731802 , diff:  0.013311053711731802
adv train loss:  -0.020842300780941514 , diff:  0.007531247069209712
layer  7  adv train finish, try to retain  49
test acc: top1 ->  70.91 ; top5 ->  92.57  and loss:  221.0581247806549
forward train acc: top1 ->  99.774 ; top5 ->  100.0  and loss:  0.7757064328616252
test acc: top1 ->  91.12 ; top5 ->  98.76  and loss:  90.64622548222542
forward train acc: top1 ->  99.89 ; top5 ->  100.0  and loss:  0.3121840738458559
test acc: top1 ->  91.3 ; top5 ->  98.77  and loss:  86.47237426042557
forward train acc: top1 ->  99.912 ; top5 ->  100.0  and loss:  0.26364139054203406
test acc: top1 ->  91.31 ; top5 ->  98.86  and loss:  84.49263593554497
forward train acc: top1 ->  99.916 ; top5 ->  99.998  and loss:  0.27089647031971253
test acc: top1 ->  91.29 ; top5 ->  98.93  and loss:  81.92529015243053
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.1866508511120628
test acc: top1 ->  91.43 ; top5 ->  98.89  and loss:  86.11287948489189
forward train acc: top1 ->  99.94799997558594 ; top5 ->  100.0  and loss:  0.17314140421513002
test acc: top1 ->  91.38 ; top5 ->  98.9  and loss:  84.02706319093704
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.10117578450444853
test acc: top1 ->  91.56 ; top5 ->  98.96  and loss:  85.18322785198689
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.147708114105626
test acc: top1 ->  91.58 ; top5 ->  98.98  and loss:  84.23752731084824
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.1023050955045619
test acc: top1 ->  91.52 ; top5 ->  98.97  and loss:  83.41010604798794
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.1888173580809962
test acc: top1 ->  91.61 ; top5 ->  98.94  and loss:  85.3463102132082
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -0.13899674179685917 , diff:  0.13899674179685917
adv train loss:  -0.18192922571142844 , diff:  0.042932483914569275
adv train loss:  -0.13911114653637924 , diff:  0.042818079175049206
adv train loss:  -0.18333530824020272 , diff:  0.044224161703823484
adv train loss:  -0.15592835439747432 , diff:  0.0274069538427284
adv train loss:  -0.1349860285149589 , diff:  0.020942325882515433
adv train loss:  -0.16140831751545193 , diff:  0.026422289000493038
adv train loss:  -0.2494409924147476 , diff:  0.08803267489929567
adv train loss:  -0.19026534966542386 , diff:  0.05917564274932374
adv train loss:  -0.11346171230434265 , diff:  0.0768036373610812
layer  8  adv train finish, try to retain  72
test acc: top1 ->  88.11 ; top5 ->  98.2  and loss:  77.96581426262856
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.09552778707018206
test acc: top1 ->  91.92 ; top5 ->  98.96  and loss:  102.35773721337318
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.026030973180240835
test acc: top1 ->  91.89 ; top5 ->  99.0  and loss:  100.6985094845295
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.03175299168606216
test acc: top1 ->  91.9 ; top5 ->  99.05  and loss:  101.21747639775276
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.011236360415125546
test acc: top1 ->  92.02 ; top5 ->  98.99  and loss:  102.25760239362717
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.025895213805142703
test acc: top1 ->  91.87 ; top5 ->  98.97  and loss:  103.05562375485897
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.03248152191997633
test acc: top1 ->  91.99 ; top5 ->  99.03  and loss:  100.96746833622456
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.022153077185180337
test acc: top1 ->  91.89 ; top5 ->  99.0  and loss:  101.03373911976814
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.047015758042107336
test acc: top1 ->  91.84 ; top5 ->  98.99  and loss:  100.8106357306242
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.01082731670476278
test acc: top1 ->  92.0 ; top5 ->  99.0  and loss:  101.31903840601444
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.01934322460124349
test acc: top1 ->  91.99 ; top5 ->  98.99  and loss:  101.65377324819565
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -0.053791238423627874 , diff:  0.053791238423627874
adv train loss:  -0.018604675663937087 , diff:  0.03518656275969079
adv train loss:  -0.025418133627681527 , diff:  0.00681345796374444
layer  9  adv train finish, try to retain  21
test acc: top1 ->  33.77 ; top5 ->  76.74  and loss:  563.9675087928772
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03788037500089558
test acc: top1 ->  91.9 ; top5 ->  99.04  and loss:  94.35843490064144
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.013245321966167012
test acc: top1 ->  91.92 ; top5 ->  99.06  and loss:  93.41141293942928
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.010563516040974719
test acc: top1 ->  91.94 ; top5 ->  99.05  and loss:  94.51497492194176
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.01770224337496984
test acc: top1 ->  91.92 ; top5 ->  99.01  and loss:  96.12020573019981
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.015021628939337006
test acc: top1 ->  91.95 ; top5 ->  99.05  and loss:  95.48920522630215
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.01690050762363171
test acc: top1 ->  91.99 ; top5 ->  99.04  and loss:  95.73200853168964
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.01649489005626492
test acc: top1 ->  92.01 ; top5 ->  99.03  and loss:  96.51244202256203
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -0.00812381809356566 , diff:  0.00812381809356566
layer  10  adv train finish, try to retain  7
test acc: top1 ->  10.6 ; top5 ->  59.58  and loss:  1411.3540897369385
forward train acc: top1 ->  99.86 ; top5 ->  100.0  and loss:  0.45718702860176563
test acc: top1 ->  91.49 ; top5 ->  98.59  and loss:  111.56115835905075
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.07411425872760447
test acc: top1 ->  91.69 ; top5 ->  98.59  and loss:  110.9669160246849
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03507349123901804
test acc: top1 ->  91.73 ; top5 ->  98.54  and loss:  111.67059397697449
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.024322959829987667
test acc: top1 ->  91.76 ; top5 ->  98.56  and loss:  112.82078212499619
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.012911303188502643
test acc: top1 ->  91.83 ; top5 ->  98.62  and loss:  110.45863515138626
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.026541119792511836
test acc: top1 ->  91.84 ; top5 ->  98.62  and loss:  111.83295553922653
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.03649495702848071
test acc: top1 ->  91.79 ; top5 ->  98.67  and loss:  111.25175175070763
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.02071766935114283
test acc: top1 ->  91.95 ; top5 ->  98.66  and loss:  110.53435522317886
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.01655541310475428
test acc: top1 ->  91.91 ; top5 ->  98.63  and loss:  110.71314835548401
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.02321461242809164
test acc: top1 ->  91.88 ; top5 ->  98.62  and loss:  111.22929471731186
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -0.24812240345363534 , diff:  0.24812240345363534
adv train loss:  -0.33764950635668356 , diff:  0.08952710290304822
adv train loss:  -0.2823540418903576 , diff:  0.05529546446632594
adv train loss:  -0.2970221134228268 , diff:  0.014668071532469185
adv train loss:  -0.21951840941983392 , diff:  0.07750370400299289
adv train loss:  -0.22281595282038325 , diff:  0.00329754340054933
layer  11  adv train finish, try to retain  86
test acc: top1 ->  88.51 ; top5 ->  98.59  and loss:  96.28181254863739
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.06235977159667527
test acc: top1 ->  92.17 ; top5 ->  98.95  and loss:  97.93461962044239
==> this epoch:  86 / 512
---------------- start layer  12  ---------------
layer  12  adv train finish, try to retain  7
test acc: top1 ->  33.26 ; top5 ->  77.82  and loss:  326.86999464035034
forward train acc: top1 ->  75.08599998046876 ; top5 ->  99.222  and loss:  69.34041668474674
test acc: top1 ->  83.32 ; top5 ->  98.78  and loss:  52.581311374902725
forward train acc: top1 ->  98.672 ; top5 ->  100.0  and loss:  13.771477080881596
test acc: top1 ->  90.29 ; top5 ->  98.74  and loss:  40.55798111855984
forward train acc: top1 ->  99.822 ; top5 ->  100.0  and loss:  5.362035192549229
test acc: top1 ->  90.95 ; top5 ->  98.75  and loss:  38.09838210046291
forward train acc: top1 ->  99.914 ; top5 ->  100.0  and loss:  2.567905841395259
test acc: top1 ->  91.29 ; top5 ->  98.75  and loss:  37.73928984999657
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  1.4198376452550292
test acc: top1 ->  91.55 ; top5 ->  98.88  and loss:  38.275837890803814
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.9398793317377567
test acc: top1 ->  91.68 ; top5 ->  98.91  and loss:  38.80770414322615
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.7253367961384356
test acc: top1 ->  91.74 ; top5 ->  98.97  and loss:  39.234415128827095
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.5929762918967754
test acc: top1 ->  91.76 ; top5 ->  98.99  and loss:  39.847545593976974
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.4722234704531729
test acc: top1 ->  91.84 ; top5 ->  98.99  and loss:  40.646858006715775
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.40255822823382914
test acc: top1 ->  91.82 ; top5 ->  99.0  and loss:  40.88329263776541
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -16.429407477378845 , diff:  16.429407477378845
adv train loss:  -15.808626810088754 , diff:  0.6207806672900915
adv train loss:  -16.528112150728703 , diff:  0.7194853406399488
adv train loss:  -16.07120724581182 , diff:  0.4569049049168825
adv train loss:  -15.8319630920887 , diff:  0.2392441537231207
adv train loss:  -15.023270945996046 , diff:  0.8086921460926533
adv train loss:  -15.741399077698588 , diff:  0.7181281317025423
adv train loss:  -16.234627436846495 , diff:  0.4932283591479063
adv train loss:  -15.978870246559381 , diff:  0.2557571902871132
adv train loss:  -15.870652446523309 , diff:  0.10821780003607273
layer  13  adv train finish, try to retain  509
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0  ==>  64 / 64 , inc:  1
layer  2  :  0  ==>  128 / 128 , inc:  1
layer  3  :  0  ==>  128 / 128 , inc:  1
layer  4  :  0  ==>  256 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  2
layer  6  :  0.5625  ==>  144 / 256 , inc:  2
layer  7  :  0  ==>  512 / 512 , inc:  1
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0  ==>  512 / 512 , inc:  1
layer  11  :  0.16796875  ==>  86 / 512 , inc:  2
layer  12  :  0  ==>  512 / 512 , inc:  1
layer  13  :  0.083984375  ==>  43 / 512 , inc:  2
eps [0.020250000000000004, 0.00013348388671875002, 0.00013348388671875002, 0.00013348388671875002, 0.00013348388671875002, 0.00017797851562500002, 0.00017797851562500002, 0.00013348388671875002, 0.00013348388671875002, 0.00013348388671875002, 0.00013348388671875002, 0.00017797851562500002, 0.00013348388671875002, 0.0018984375000000004]  wait [2, 4, 4, 4, 4, 0, 0, 4, 4, 4, 4, 0, 4, 0]  inc [1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2]  tol: 3
$$$$$$$$$$$$$ epoch  11  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -2.0796288489364088 , diff:  2.0796288489364088
adv train loss:  -2.063785047736019 , diff:  0.015843801200389862
adv train loss:  -2.00464788347017 , diff:  0.059137164265848696
adv train loss:  -2.124498510354897 , diff:  0.11985062688472681
adv train loss:  -2.2434832083527 , diff:  0.11898469799780287
adv train loss:  -2.0847350154072046 , diff:  0.15874819294549525
adv train loss:  -2.1603751161601394 , diff:  0.07564010075293481
adv train loss:  -2.2311752651585266 , diff:  0.07080014899838716
adv train loss:  -2.1425801769364625 , diff:  0.08859508822206408
adv train loss:  -2.1845343312015757 , diff:  0.041954154265113175
layer  0  adv train finish, try to retain  64
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
### skip layer  1 wait:  4  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
adv train loss:  -2.012521047727205 , diff:  2.012521047727205
adv train loss:  -2.2155029932036996 , diff:  0.20298194547649473
adv train loss:  -2.1790464244550094 , diff:  0.03645656874869019
adv train loss:  -2.2474257613066584 , diff:  0.06837933685164899
adv train loss:  -2.249613895895891 , diff:  0.0021881345892325044
layer  5  adv train finish, try to retain  256
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -2.230089112999849 , diff:  2.230089112999849
adv train loss:  -2.0916259753284976 , diff:  0.13846313767135143
adv train loss:  -2.228922030539252 , diff:  0.13729605521075428
adv train loss:  -2.1807730307336897 , diff:  0.0481489998055622
adv train loss:  -2.0022192442556843 , diff:  0.17855378647800535
adv train loss:  -2.1640157715883106 , diff:  0.16179652733262628
adv train loss:  -2.1388648946303874 , diff:  0.025150876957923174
adv train loss:  -2.1459731857758015 , diff:  0.007108291145414114
layer  6  adv train finish, try to retain  256
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
### skip layer  8 wait:  4  ###
---------------- start layer  9  ---------------
### skip layer  9 wait:  4  ###
---------------- start layer  10  ---------------
### skip layer  10 wait:  4  ###
---------------- start layer  11  ---------------
adv train loss:  -0.9235039612976834 , diff:  0.9235039612976834
adv train loss:  -1.0223279717029072 , diff:  0.09882401040522382
adv train loss:  -0.9457118707941845 , diff:  0.07661610090872273
adv train loss:  -0.843779799353797 , diff:  0.10193207144038752
adv train loss:  -0.8954299206088763 , diff:  0.051650121255079284
adv train loss:  -0.8442357564927079 , diff:  0.051194164116168395
adv train loss:  -0.8962536769686267 , diff:  0.0520179204759188
adv train loss:  -0.9543803342385218 , diff:  0.05812665726989508
adv train loss:  -0.8468911985401064 , diff:  0.10748913569841534
adv train loss:  -0.9055841820663773 , diff:  0.05869298352627084
layer  11  adv train finish, try to retain  454
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
adv train loss:  -16.966482911258936 , diff:  16.966482911258936
adv train loss:  -15.308711223304272 , diff:  1.6577716879546642
adv train loss:  -16.20911179855466 , diff:  0.9004005752503872
adv train loss:  -16.146632993593812 , diff:  0.0624788049608469
adv train loss:  -16.093476746231318 , diff:  0.05315624736249447
adv train loss:  -16.224331688135862 , diff:  0.13085494190454483
adv train loss:  -16.063597410917282 , diff:  0.16073427721858025
adv train loss:  -16.152062725275755 , diff:  0.08846531435847282
adv train loss:  -17.002395782619715 , diff:  0.8503330573439598
adv train loss:  -16.37183253467083 , diff:  0.630563247948885
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  41
test acc: top1 ->  13.06 ; top5 ->  69.07  and loss:  536.0094513893127
forward train acc: top1 ->  29.26600000854492 ; top5 ->  73.77599997802734  and loss:  314.05235266685486
test acc: top1 ->  41.42 ; top5 ->  78.67  and loss:  209.6050579547882
forward train acc: top1 ->  58.69399999267578 ; top5 ->  94.868  and loss:  123.47991061210632
test acc: top1 ->  71.63 ; top5 ->  98.09  and loss:  85.86283099651337
forward train acc: top1 ->  93.692 ; top5 ->  99.996  and loss:  42.18983879685402
test acc: top1 ->  90.31 ; top5 ->  98.53  and loss:  52.36165779829025
forward train acc: top1 ->  99.872 ; top5 ->  100.0  and loss:  17.03370653092861
test acc: top1 ->  91.17 ; top5 ->  98.47  and loss:  39.280688136816025
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  7.340002428740263
test acc: top1 ->  91.37 ; top5 ->  98.5  and loss:  36.331398248672485
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  4.35003137588501
test acc: top1 ->  91.37 ; top5 ->  98.47  and loss:  36.11202481389046
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  3.2892976999282837
test acc: top1 ->  91.45 ; top5 ->  98.46  and loss:  36.1390675753355
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  2.5039889831095934
test acc: top1 ->  91.53 ; top5 ->  98.41  and loss:  36.57076606154442
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  1.910591971129179
test acc: top1 ->  91.6 ; top5 ->  98.41  and loss:  36.86097255349159
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  1.470268290489912
test acc: top1 ->  91.53 ; top5 ->  98.37  and loss:  37.33631022274494
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  43 / 512 , inc:  2
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0  ==>  64 / 64 , inc:  1
layer  2  :  0  ==>  128 / 128 , inc:  1
layer  3  :  0  ==>  128 / 128 , inc:  1
layer  4  :  0  ==>  256 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  2
layer  6  :  0.5625  ==>  144 / 256 , inc:  2
layer  7  :  0  ==>  512 / 512 , inc:  1
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0  ==>  512 / 512 , inc:  1
layer  11  :  0.16796875  ==>  86 / 512 , inc:  2
layer  12  :  0  ==>  512 / 512 , inc:  1
layer  13  :  0.083984375  ==>  43 / 512 , inc:  1
eps [0.04050000000000001, 0.00013348388671875002, 0.00013348388671875002, 0.00013348388671875002, 0.00013348388671875002, 0.00035595703125000005, 0.00035595703125000005, 0.00013348388671875002, 0.00013348388671875002, 0.00013348388671875002, 0.00013348388671875002, 0.00035595703125000005, 0.00013348388671875002, 0.0014238281250000002]  wait [2, 3, 3, 3, 3, 0, 0, 3, 3, 3, 3, 0, 3, 2]  inc [1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1]  tol: 3
$$$$$$$$$$$$$ epoch  12  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1.9622391785960644 , diff:  1.9622391785960644
adv train loss:  -2.0492147477634717 , diff:  0.08697556916740723
adv train loss:  -1.795966439298354 , diff:  0.2532483084651176
adv train loss:  -1.843238431843929 , diff:  0.047271992545574903
adv train loss:  -2.038904204033315 , diff:  0.1956657721893862
adv train loss:  -1.8159314012737013 , diff:  0.22297280275961384
adv train loss:  -1.8977471766120289 , diff:  0.08181577533832751
adv train loss:  -1.9391425969042757 , diff:  0.041395420292246854
adv train loss:  -1.9200534595074714 , diff:  0.01908913739680429
adv train loss:  -1.9016091638768557 , diff:  0.01844429563061567
layer  0  adv train finish, try to retain  64
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
### skip layer  1 wait:  3  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
adv train loss:  -1.79472353332676 , diff:  1.79472353332676
adv train loss:  -2.0520447077578865 , diff:  0.25732117443112656
adv train loss:  -2.055936520337127 , diff:  0.00389181257924065
layer  5  adv train finish, try to retain  256
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -2.214417913230136 , diff:  2.214417913230136
adv train loss:  -1.7992888444568962 , diff:  0.41512906877323985
adv train loss:  -1.917254584375769 , diff:  0.11796573991887271
adv train loss:  -1.568620399106294 , diff:  0.348634185269475
adv train loss:  -1.8496342651196755 , diff:  0.28101386601338163
adv train loss:  -1.9630820979364216 , diff:  0.11344783281674609
adv train loss:  -1.9702312406734563 , diff:  0.007149142737034708
layer  6  adv train finish, try to retain  255
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
### skip layer  8 wait:  3  ###
---------------- start layer  9  ---------------
### skip layer  9 wait:  3  ###
---------------- start layer  10  ---------------
### skip layer  10 wait:  3  ###
---------------- start layer  11  ---------------
adv train loss:  -0.9835311430433649 , diff:  0.9835311430433649
adv train loss:  -0.9568713289336301 , diff:  0.026659814109734725
adv train loss:  -0.9093759129755199 , diff:  0.04749541595811024
adv train loss:  -1.0122147742076777 , diff:  0.1028388612321578
adv train loss:  -1.0634969668462873 , diff:  0.05128219263860956
adv train loss:  -0.896058624915895 , diff:  0.1674383419303922
adv train loss:  -0.8063880489789881 , diff:  0.08967057593690697
adv train loss:  -1.0390713965753093 , diff:  0.23268334759632125
adv train loss:  -0.8733498915680684 , diff:  0.16572150500724092
adv train loss:  -1.0590582169825211 , diff:  0.1857083254144527
layer  11  adv train finish, try to retain  451
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
### skip layer  12 wait:  3  ###
---------------- start layer  13  ---------------
adv train loss:  -1.469814928015694 , diff:  1.469814928015694
adv train loss:  -1.6217716397950426 , diff:  0.15195671177934855
adv train loss:  -1.7864720885409042 , diff:  0.16470044874586165
adv train loss:  -1.5594141447327274 , diff:  0.2270579438081768
adv train loss:  -1.8440962043241598 , diff:  0.28468205959143233
adv train loss:  -1.5713770545553416 , diff:  0.27271914976881817
adv train loss:  -1.3374475113814697 , diff:  0.23392954317387193
adv train loss:  -1.8274215643759817 , diff:  0.489974052994512
adv train loss:  -1.377532179991249 , diff:  0.4498893843847327
adv train loss:  -1.8436568592042022 , diff:  0.4661246792129532
layer  13  adv train finish, try to retain  504
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0  ==>  64 / 64 , inc:  1
layer  2  :  0  ==>  128 / 128 , inc:  1
layer  3  :  0  ==>  128 / 128 , inc:  1
layer  4  :  0  ==>  256 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  2
layer  6  :  0.5625  ==>  144 / 256 , inc:  2
layer  7  :  0  ==>  512 / 512 , inc:  1
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0  ==>  512 / 512 , inc:  1
layer  11  :  0.16796875  ==>  86 / 512 , inc:  2
layer  12  :  0  ==>  512 / 512 , inc:  1
layer  13  :  0.083984375  ==>  43 / 512 , inc:  1
eps [0.08100000000000002, 0.00013348388671875002, 0.00013348388671875002, 0.00013348388671875002, 0.00013348388671875002, 0.0007119140625000001, 0.0007119140625000001, 0.00013348388671875002, 0.00013348388671875002, 0.00013348388671875002, 0.00013348388671875002, 0.0007119140625000001, 0.00013348388671875002, 0.0028476562500000004]  wait [2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2]  inc [1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1]  tol: 3
$$$$$$$$$$$$$ epoch  13  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1.3169991467148066 , diff:  1.3169991467148066
adv train loss:  -1.8666026888531633 , diff:  0.5496035421383567
adv train loss:  -1.900481833028607 , diff:  0.0338791441754438
adv train loss:  -2.0893891588784754 , diff:  0.18890732584986836
adv train loss:  -2.268074741063174 , diff:  0.17868558218469843
adv train loss:  -1.959976035868749 , diff:  0.30809870519442484
adv train loss:  -1.8352394569083117 , diff:  0.12473657896043733
adv train loss:  -2.2867001574486494 , diff:  0.4514607005403377
adv train loss:  -1.924582908366574 , diff:  0.36211724908207543
adv train loss:  -1.9304239874472842 , diff:  0.0058410790807101876
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  31
test acc: top1 ->  15.25 ; top5 ->  52.21  and loss:  3391.357376098633
forward train acc: top1 ->  94.04999998291015 ; top5 ->  99.45800000244141  and loss:  69.22221985459328
test acc: top1 ->  80.15 ; top5 ->  95.37  and loss:  293.5268552303314
forward train acc: top1 ->  95.27999998046874 ; top5 ->  99.728  and loss:  30.403211764991283
test acc: top1 ->  88.04 ; top5 ->  98.76  and loss:  112.39068925380707
forward train acc: top1 ->  95.94200001708984 ; top5 ->  99.808  and loss:  20.097890123724937
test acc: top1 ->  88.33 ; top5 ->  98.7  and loss:  89.66915836930275
forward train acc: top1 ->  96.36200001464844 ; top5 ->  99.8140000024414  and loss:  15.404905378818512
test acc: top1 ->  88.4 ; top5 ->  98.73  and loss:  77.2887214422226
forward train acc: top1 ->  96.68599999023438 ; top5 ->  99.87  and loss:  12.11946752294898
test acc: top1 ->  88.52 ; top5 ->  98.82  and loss:  70.1601380109787
forward train acc: top1 ->  96.71999998779297 ; top5 ->  99.882  and loss:  11.33760453760624
test acc: top1 ->  88.61 ; top5 ->  98.84  and loss:  68.71552300453186
forward train acc: top1 ->  96.95599998291016 ; top5 ->  99.918  and loss:  10.34250684082508
test acc: top1 ->  88.66 ; top5 ->  98.88  and loss:  65.65711060166359
forward train acc: top1 ->  97.13999998779298 ; top5 ->  99.906  and loss:  9.73005486652255
test acc: top1 ->  88.81 ; top5 ->  98.82  and loss:  63.68982046842575
forward train acc: top1 ->  97.18200001220703 ; top5 ->  99.86799997558593  and loss:  9.659745462238789
test acc: top1 ->  88.82 ; top5 ->  98.87  and loss:  61.460434287786484
forward train acc: top1 ->  97.29600000976562 ; top5 ->  99.92  and loss:  9.173137985169888
test acc: top1 ->  88.91 ; top5 ->  98.85  and loss:  60.76315048336983
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -0.40605301747564226 , diff:  0.40605301747564226
adv train loss:  -0.42058242240455 , diff:  0.014529404928907752
adv train loss:  -0.43448295537382364 , diff:  0.013900532969273627
adv train loss:  -0.514287680038251 , diff:  0.07980472466442734
adv train loss:  -0.4457581932656467 , diff:  0.06852948677260429
adv train loss:  -0.4470710048917681 , diff:  0.0013128116261214018
layer  1  adv train finish, try to retain  56
test acc: top1 ->  91.36 ; top5 ->  99.31  and loss:  43.948951072990894
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.15421108304144582
test acc: top1 ->  91.73 ; top5 ->  99.19  and loss:  57.07581576704979
forward train acc: top1 ->  99.98399997558593 ; top5 ->  100.0  and loss:  0.05572130508517148
test acc: top1 ->  91.85 ; top5 ->  99.15  and loss:  63.096635922789574
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.04380872943875147
test acc: top1 ->  91.88 ; top5 ->  99.18  and loss:  64.51802092790604
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.034336314325628337
test acc: top1 ->  91.75 ; top5 ->  99.17  and loss:  68.70712815225124
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.022465372281658347
test acc: top1 ->  91.82 ; top5 ->  99.18  and loss:  70.98082636296749
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.037876833735026594
test acc: top1 ->  91.83 ; top5 ->  99.21  and loss:  71.04407896101475
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.02365371819178108
test acc: top1 ->  91.94 ; top5 ->  99.13  and loss:  72.44925650954247
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.017003902297801687
test acc: top1 ->  91.95 ; top5 ->  99.15  and loss:  72.652384147048
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03799424258249928
test acc: top1 ->  91.87 ; top5 ->  99.17  and loss:  73.60495921969414
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.01919092388197896
test acc: top1 ->  91.91 ; top5 ->  99.15  and loss:  75.22835555672646
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  64 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -0.023267052301434887 , diff:  0.023267052301434887
adv train loss:  -0.021300245810834895 , diff:  0.0019668064905999927
layer  2  adv train finish, try to retain  100
test acc: top1 ->  91.89 ; top5 ->  99.18  and loss:  74.2443853020668
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.012444475335996685
test acc: top1 ->  91.91 ; top5 ->  99.1  and loss:  82.14329911768436
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.015270681082256488
test acc: top1 ->  91.98 ; top5 ->  99.14  and loss:  82.64744184911251
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.013251429629235645
test acc: top1 ->  91.76 ; top5 ->  99.08  and loss:  86.00330206751823
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.040712834258783914
test acc: top1 ->  91.88 ; top5 ->  99.07  and loss:  83.25305259972811
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.013815068275334852
test acc: top1 ->  91.86 ; top5 ->  99.05  and loss:  86.28894877433777
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.015181843352252145
test acc: top1 ->  91.98 ; top5 ->  99.12  and loss:  86.6299682110548
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.007320277020880894
test acc: top1 ->  91.9 ; top5 ->  99.11  and loss:  86.0606761276722
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.005178058362389493
test acc: top1 ->  91.92 ; top5 ->  99.06  and loss:  86.4724530428648
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.005744281261286233
test acc: top1 ->  91.98 ; top5 ->  99.09  and loss:  87.10781462490559
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  128 / 128 , inc:  1
---------------- start layer  3  ---------------
layer  3  adv train finish, try to retain  88
test acc: top1 ->  91.96 ; top5 ->  99.1  and loss:  87.22013275325298
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.013339248410829896
test acc: top1 ->  91.83 ; top5 ->  99.15  and loss:  89.99335771799088
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.012179340664260963
test acc: top1 ->  91.83 ; top5 ->  99.07  and loss:  91.1975777298212
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.00720617610750196
test acc: top1 ->  91.87 ; top5 ->  99.01  and loss:  93.00967817008495
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.01030866001974573
test acc: top1 ->  92.06 ; top5 ->  99.14  and loss:  90.41481509804726
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.04037721861004684
test acc: top1 ->  92.07 ; top5 ->  99.12  and loss:  90.18538005650043
forward train acc: top1 ->  99.98399997558593 ; top5 ->  100.0  and loss:  0.030142047609842848
test acc: top1 ->  91.95 ; top5 ->  99.11  and loss:  90.94233658909798
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.009118108278926229
test acc: top1 ->  92.01 ; top5 ->  99.13  and loss:  90.17242956161499
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.010348147496301863
test acc: top1 ->  91.93 ; top5 ->  99.13  and loss:  90.22022089362144
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.0028430303809727775
test acc: top1 ->  91.96 ; top5 ->  99.11  and loss:  90.41858266294003
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.0107641783042709
test acc: top1 ->  91.95 ; top5 ->  99.12  and loss:  90.71871456503868
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  128 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -0.007264542738084856 , diff:  0.007264542738084856
layer  4  adv train finish, try to retain  155
test acc: top1 ->  91.72 ; top5 ->  99.12  and loss:  91.06672276556492
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.01264501689752251
test acc: top1 ->  91.89 ; top5 ->  99.16  and loss:  92.764270439744
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.007557884954564997
test acc: top1 ->  91.87 ; top5 ->  99.13  and loss:  92.46802838146687
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.01039906009873448
test acc: top1 ->  91.93 ; top5 ->  99.11  and loss:  95.10384467244148
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.018678981392213245
test acc: top1 ->  91.93 ; top5 ->  99.06  and loss:  94.64970171451569
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.04772165857727373
test acc: top1 ->  91.88 ; top5 ->  99.09  and loss:  91.76041205227375
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.021721309727581684
test acc: top1 ->  91.95 ; top5 ->  99.09  and loss:  92.56734248250723
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.04042858282218731
test acc: top1 ->  91.89 ; top5 ->  99.07  and loss:  93.06748086214066
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.01845473052981106
test acc: top1 ->  91.89 ; top5 ->  99.14  and loss:  91.5887931138277
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.011806058684442178
test acc: top1 ->  91.95 ; top5 ->  99.08  and loss:  92.09622675180435
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.041745284499313584
test acc: top1 ->  91.94 ; top5 ->  99.1  and loss:  91.99689210951328
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  256 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -0.03291717212934486 , diff:  0.03291717212934486
adv train loss:  -0.009788670401121635 , diff:  0.023128501728223227
adv train loss:  -0.021132172778379754 , diff:  0.011343502377258119
adv train loss:  -0.03922644356219962 , diff:  0.018094270783819866
adv train loss:  -0.011058202589197208 , diff:  0.028168240973002412
adv train loss:  -0.03223580567092199 , diff:  0.021177603081724783
adv train loss:  -0.017658867402133183 , diff:  0.014576938268788808
adv train loss:  -0.010276785081941853 , diff:  0.00738208232019133
layer  5  adv train finish, try to retain  256
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -0.02017704643185425 , diff:  0.02017704643185425
adv train loss:  -0.026543370825493184 , diff:  0.0063663243936389335
layer  6  adv train finish, try to retain  256
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
layer  7  adv train finish, try to retain  12
test acc: top1 ->  25.07 ; top5 ->  59.06  and loss:  954.2419023513794
forward train acc: top1 ->  89.22399997070312 ; top5 ->  99.69  and loss:  55.53791756927967
test acc: top1 ->  85.7 ; top5 ->  98.11  and loss:  112.92591628432274
forward train acc: top1 ->  95.77000001220703 ; top5 ->  99.908  and loss:  15.491098619997501
test acc: top1 ->  87.16 ; top5 ->  98.51  and loss:  94.99738527834415
forward train acc: top1 ->  96.97200001464844 ; top5 ->  99.94399997558594  and loss:  10.585673954337835
test acc: top1 ->  88.18 ; top5 ->  98.64  and loss:  85.57909753918648
forward train acc: top1 ->  97.79199998046874 ; top5 ->  99.958  and loss:  7.4753711093217134
test acc: top1 ->  88.55 ; top5 ->  98.64  and loss:  79.19689263403416
forward train acc: top1 ->  98.03400000488281 ; top5 ->  99.956  and loss:  6.709858441725373
test acc: top1 ->  88.69 ; top5 ->  98.76  and loss:  78.4621102064848
forward train acc: top1 ->  98.17599998291016 ; top5 ->  99.972  and loss:  5.782256426289678
test acc: top1 ->  88.87 ; top5 ->  98.74  and loss:  75.09349024295807
forward train acc: top1 ->  98.54799998291016 ; top5 ->  99.978  and loss:  4.67760400287807
test acc: top1 ->  89.21 ; top5 ->  98.8  and loss:  74.28949339687824
forward train acc: top1 ->  98.53199998779297 ; top5 ->  99.97799997558593  and loss:  4.62644568644464
test acc: top1 ->  89.43 ; top5 ->  98.8  and loss:  72.63580738008022
forward train acc: top1 ->  98.66800000732422 ; top5 ->  99.982  and loss:  4.132690115831792
test acc: top1 ->  89.56 ; top5 ->  98.74  and loss:  71.88723076879978
forward train acc: top1 ->  98.74600000488282 ; top5 ->  99.99  and loss:  4.005847852677107
test acc: top1 ->  89.54 ; top5 ->  98.79  and loss:  71.49366120994091
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -2.606596039608121 , diff:  2.606596039608121
adv train loss:  -2.331176138482988 , diff:  0.27541990112513304
adv train loss:  -2.5150556524749845 , diff:  0.18387951399199665
adv train loss:  -2.818076374474913 , diff:  0.30302072199992836
adv train loss:  -2.5587218885775656 , diff:  0.25935448589734733
adv train loss:  -2.477467800024897 , diff:  0.08125408855266869
adv train loss:  -2.3580362345092 , diff:  0.119431565515697
adv train loss:  -2.4685206138528883 , diff:  0.11048437934368849
adv train loss:  -2.410193902440369 , diff:  0.05832671141251922
adv train loss:  -2.4811052336590365 , diff:  0.07091133121866733
layer  8  adv train finish, try to retain  117
test acc: top1 ->  75.69 ; top5 ->  97.54  and loss:  130.300044298172
forward train acc: top1 ->  99.758 ; top5 ->  100.0  and loss:  0.665568842079665
test acc: top1 ->  91.62 ; top5 ->  99.04  and loss:  82.10546799004078
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.05954018751435797
test acc: top1 ->  91.72 ; top5 ->  99.04  and loss:  82.73820953071117
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.05858839452957909
test acc: top1 ->  91.82 ; top5 ->  99.09  and loss:  83.46664544939995
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -0.06558933174892445 , diff:  0.06558933174892445
adv train loss:  -0.07899691155398614 , diff:  0.013407579805061687
adv train loss:  -0.09547106423087826 , diff:  0.016474152676892118
adv train loss:  -0.0903865466825664 , diff:  0.005084517548311851
layer  9  adv train finish, try to retain  44
test acc: top1 ->  71.31 ; top5 ->  96.97  and loss:  173.68694496154785
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.05059395736680017
test acc: top1 ->  91.77 ; top5 ->  99.11  and loss:  82.3618098422885
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.025334997835670947
test acc: top1 ->  91.96 ; top5 ->  99.06  and loss:  85.48489923030138
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.035651449557008164
test acc: top1 ->  91.99 ; top5 ->  99.12  and loss:  86.62088651955128
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.01991879616616643
test acc: top1 ->  91.92 ; top5 ->  99.1  and loss:  86.67064567655325
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.021546511401538737
test acc: top1 ->  91.93 ; top5 ->  99.03  and loss:  89.69277139008045
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.02718913871399309
test acc: top1 ->  91.97 ; top5 ->  99.05  and loss:  90.6411289870739
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.013733652081441505
test acc: top1 ->  91.94 ; top5 ->  99.1  and loss:  89.43756563961506
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.016574906916503096
test acc: top1 ->  91.88 ; top5 ->  99.07  and loss:  88.69125358760357
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.012930000105427553
test acc: top1 ->  91.86 ; top5 ->  99.1  and loss:  89.4725634381175
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.02328066363133985
test acc: top1 ->  91.95 ; top5 ->  99.1  and loss:  88.94194959104061
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -0.01752221425795142 , diff:  0.01752221425795142
adv train loss:  -0.02160665814380991 , diff:  0.004084443885858491
layer  10  adv train finish, try to retain  22
test acc: top1 ->  48.93 ; top5 ->  93.82  and loss:  424.65850496292114
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.022361020721291425
test acc: top1 ->  91.83 ; top5 ->  98.96  and loss:  96.19060797989368
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.03564791763892572
test acc: top1 ->  91.91 ; top5 ->  98.9  and loss:  94.50215171277523
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.010206367442833653
test acc: top1 ->  91.87 ; top5 ->  98.92  and loss:  96.67676812410355
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.024453978212477523
test acc: top1 ->  91.88 ; top5 ->  99.03  and loss:  95.95415590703487
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.011090892308402545
test acc: top1 ->  91.93 ; top5 ->  99.05  and loss:  96.43082338571548
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.012378196330246283
test acc: top1 ->  91.92 ; top5 ->  99.01  and loss:  96.21713801473379
forward train acc: top1 ->  99.99199997558594 ; top5 ->  100.0  and loss:  0.02902368420473067
test acc: top1 ->  91.94 ; top5 ->  99.02  and loss:  95.17957267165184
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.010502842347705155
test acc: top1 ->  91.96 ; top5 ->  99.01  and loss:  97.24591581523418
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.02845302111472847
test acc: top1 ->  91.91 ; top5 ->  99.03  and loss:  97.39758896827698
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.027613822159764823
test acc: top1 ->  92.11 ; top5 ->  99.03  and loss:  94.80238384008408
==> this epoch:  22 / 512
---------------- start layer  11  ---------------
adv train loss:  -0.041643663902277694 , diff:  0.041643663902277694
adv train loss:  -0.04370439839021856 , diff:  0.002060734487940863
layer  11  adv train finish, try to retain  424
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -0.005187684746715604 , diff:  0.005187684746715604
layer  12  adv train finish, try to retain  10
test acc: top1 ->  63.41 ; top5 ->  98.66  and loss:  231.28963553905487
forward train acc: top1 ->  94.2340000024414 ; top5 ->  100.0  and loss:  19.171140618622303
test acc: top1 ->  90.97 ; top5 ->  99.02  and loss:  52.34343433380127
forward train acc: top1 ->  99.924 ; top5 ->  100.0  and loss:  1.020883491029963
test acc: top1 ->  91.62 ; top5 ->  98.84  and loss:  49.37393392622471
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.465320804156363
test acc: top1 ->  91.78 ; top5 ->  98.8  and loss:  49.82611083984375
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.2844578995136544
test acc: top1 ->  91.78 ; top5 ->  98.74  and loss:  51.38172955811024
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.19800032780040056
test acc: top1 ->  91.79 ; top5 ->  98.74  and loss:  52.80298963934183
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.17838723654858768
test acc: top1 ->  91.78 ; top5 ->  98.74  and loss:  53.34713006019592
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.15614493563771248
test acc: top1 ->  91.81 ; top5 ->  98.69  and loss:  53.439150132238865
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.13048891810467467
test acc: top1 ->  91.78 ; top5 ->  98.72  and loss:  53.73657160997391
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.13741925795329735
test acc: top1 ->  91.92 ; top5 ->  98.76  and loss:  54.297029837965965
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.12069255215465091
test acc: top1 ->  91.88 ; top5 ->  98.73  and loss:  54.545834079384804
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -0.7462978593448497 , diff:  0.7462978593448497
adv train loss:  -0.9140742045638035 , diff:  0.1677763452189538
adv train loss:  -0.9780088576480921 , diff:  0.06393465308428858
adv train loss:  -0.8407652726746164 , diff:  0.13724358497347566
adv train loss:  -0.8795615984563483 , diff:  0.038796325781731866
adv train loss:  -0.9333381777105387 , diff:  0.05377657925419044
adv train loss:  -0.6433394319201398 , diff:  0.28999874579039897
adv train loss:  -0.8015338597506343 , diff:  0.15819442783049453
adv train loss:  -1.0235149378968345 , diff:  0.22198107814620016
adv train loss:  -1.0275732644367963 , diff:  0.004058326539961854
layer  13  adv train finish, try to retain  506
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0  ==>  64 / 64 , inc:  1
layer  2  :  0  ==>  128 / 128 , inc:  1
layer  3  :  0  ==>  128 / 128 , inc:  1
layer  4  :  0  ==>  256 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  2
layer  6  :  0.5625  ==>  144 / 256 , inc:  2
layer  7  :  0  ==>  512 / 512 , inc:  1
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  2
layer  11  :  0.16796875  ==>  86 / 512 , inc:  2
layer  12  :  0  ==>  512 / 512 , inc:  1
layer  13  :  0.083984375  ==>  43 / 512 , inc:  1
eps [0.06075000000000001, 0.00010011291503906251, 0.00010011291503906251, 0.00010011291503906251, 0.00010011291503906251, 0.0014238281250000002, 0.0014238281250000002, 0.00010011291503906251, 0.00010011291503906251, 0.00010011291503906251, 0.00013348388671875002, 0.0014238281250000002, 0.00010011291503906251, 0.005695312500000001]  wait [4, 4, 4, 4, 4, 0, 0, 4, 4, 4, 0, 0, 4, 2]  inc [1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1]  tol: 3
$$$$$$$$$$$$$ epoch  14  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
### skip layer  1 wait:  4  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
adv train loss:  -0.22168632467946736 , diff:  0.22168632467946736
adv train loss:  -0.08361333128050319 , diff:  0.13807299339896417
adv train loss:  -0.16970733897278478 , diff:  0.0860940076922816
adv train loss:  -0.10129124099694309 , diff:  0.0684160979758417
adv train loss:  -0.17440590621117735 , diff:  0.07311466521423426
adv train loss:  -0.12971474965343077 , diff:  0.04469115655774658
adv train loss:  -0.12173473127313628 , diff:  0.007980018380294496
layer  5  adv train finish, try to retain  256
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -0.154397229352071 , diff:  0.154397229352071
adv train loss:  -0.21333110116142961 , diff:  0.05893387180935861
adv train loss:  -0.09561223921741657 , diff:  0.11771886194401304
adv train loss:  -0.18002996095310664 , diff:  0.08441772173569007
adv train loss:  -0.13163937354647715 , diff:  0.04839058740662949
adv train loss:  -0.15417728946340503 , diff:  0.022537915916927886
adv train loss:  -0.1620878793000884 , diff:  0.007910589836683357
layer  6  adv train finish, try to retain  256
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
### skip layer  8 wait:  4  ###
---------------- start layer  9  ---------------
### skip layer  9 wait:  4  ###
---------------- start layer  10  ---------------
adv train loss:  -0.23080487635888858 , diff:  0.23080487635888858
adv train loss:  -0.3014798521071498 , diff:  0.0706749757482612
adv train loss:  -0.24029136749959434 , diff:  0.06118848460755544
adv train loss:  -0.25163972166046733 , diff:  0.011348354160872987
adv train loss:  -0.2622471801532811 , diff:  0.010607458492813748
adv train loss:  -0.2701844679977512 , diff:  0.00793728784447012
layer  10  adv train finish, try to retain  479
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -0.3278609729350137 , diff:  0.3278609729350137
adv train loss:  -0.2196264569865889 , diff:  0.10823451594842481
adv train loss:  -0.302248252805839 , diff:  0.08262179581925011
adv train loss:  -0.2282164665966775 , diff:  0.07403178620916151
adv train loss:  -0.34009293793860707 , diff:  0.11187647134192957
adv train loss:  -0.25162291919696145 , diff:  0.08847001874164562
adv train loss:  -0.36220871883597283 , diff:  0.11058579963901138
adv train loss:  -0.2972510348618016 , diff:  0.06495768397417123
adv train loss:  -0.31803917726210784 , diff:  0.020788142400306242
adv train loss:  -0.31300544956320664 , diff:  0.0050337276989012025
layer  11  adv train finish, try to retain  435
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
adv train loss:  -0.6700129668097361 , diff:  0.6700129668097361
adv train loss:  -0.9028499698470114 , diff:  0.23283700303727528
adv train loss:  -0.8921036375486437 , diff:  0.010746332298367633
adv train loss:  -1.0257649274353753 , diff:  0.13366128988673154
adv train loss:  -0.9737450020911638 , diff:  0.052019925344211515
adv train loss:  -1.1571486952962005 , diff:  0.1834036932050367
adv train loss:  -1.1408864006443764 , diff:  0.01626229465182405
adv train loss:  -0.909802003272489 , diff:  0.23108439737188746
adv train loss:  -0.8360518468243754 , diff:  0.07375015644811356
adv train loss:  -0.7927484054816887 , diff:  0.04330344134268671
layer  13  adv train finish, try to retain  508
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0  ==>  64 / 64 , inc:  1
layer  2  :  0  ==>  128 / 128 , inc:  1
layer  3  :  0  ==>  128 / 128 , inc:  1
layer  4  :  0  ==>  256 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  2
layer  6  :  0.5625  ==>  144 / 256 , inc:  2
layer  7  :  0  ==>  512 / 512 , inc:  1
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  2
layer  11  :  0.16796875  ==>  86 / 512 , inc:  2
layer  12  :  0  ==>  512 / 512 , inc:  1
layer  13  :  0.083984375  ==>  43 / 512 , inc:  1
eps [0.06075000000000001, 0.00010011291503906251, 0.00010011291503906251, 0.00010011291503906251, 0.00010011291503906251, 0.0028476562500000004, 0.0028476562500000004, 0.00010011291503906251, 0.00010011291503906251, 0.00010011291503906251, 0.00026696777343750003, 0.0028476562500000004, 0.00010011291503906251, 0.011390625000000001]  wait [3, 3, 3, 3, 3, 0, 0, 3, 3, 3, 0, 0, 3, 2]  inc [1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1]  tol: 3
$$$$$$$$$$$$$ epoch  15  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
### skip layer  1 wait:  3  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
adv train loss:  -0.14117254197708462 , diff:  0.14117254197708462
adv train loss:  -0.08652007049659005 , diff:  0.05465247148049457
adv train loss:  -0.12804457001402625 , diff:  0.0415244995174362
adv train loss:  -0.10112813870910031 , diff:  0.026916431304925936
adv train loss:  -0.15575633820753865 , diff:  0.054628199498438335
adv train loss:  -0.13022147886204039 , diff:  0.025534859345498262
adv train loss:  -0.13446986281633144 , diff:  0.004248383954291057
layer  5  adv train finish, try to retain  256
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -0.14014361634394845 , diff:  0.14014361634394845
adv train loss:  -0.14936022867277643 , diff:  0.009216612328827978
layer  6  adv train finish, try to retain  256
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
### skip layer  8 wait:  3  ###
---------------- start layer  9  ---------------
### skip layer  9 wait:  3  ###
---------------- start layer  10  ---------------
adv train loss:  -0.26590820321871433 , diff:  0.26590820321871433
adv train loss:  -0.3383493507280946 , diff:  0.07244114750938024
adv train loss:  -0.27138866879977286 , diff:  0.06696068192832172
adv train loss:  -0.35113316740171285 , diff:  0.07974449860194
adv train loss:  -0.2668588528758846 , diff:  0.08427431452582823
adv train loss:  -0.344113092898624 , diff:  0.0772542400227394
adv train loss:  -0.17387675064674113 , diff:  0.1702363422518829
adv train loss:  -0.20168070860017906 , diff:  0.02780395795343793
adv train loss:  -0.27642548681615153 , diff:  0.07474477821597247
adv train loss:  -0.22709590687918535 , diff:  0.049329579936966184
layer  10  adv train finish, try to retain  481
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -0.4235489230413805 , diff:  0.4235489230413805
adv train loss:  -0.2973204575046111 , diff:  0.12622846553676936
adv train loss:  -0.3201797760993941 , diff:  0.022859318594782962
adv train loss:  -0.29746109820189304 , diff:  0.02271867789750104
adv train loss:  -0.2974008751953079 , diff:  6.022300658514723e-05
layer  11  adv train finish, try to retain  431
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
### skip layer  12 wait:  3  ###
---------------- start layer  13  ---------------
adv train loss:  -1.148809963142412 , diff:  1.148809963142412
adv train loss:  -1.031024533273012 , diff:  0.1177854298694001
adv train loss:  -0.9662554340320639 , diff:  0.06476909924094798
adv train loss:  -0.9005647361336742 , diff:  0.06569069789838977
adv train loss:  -0.9411738164853887 , diff:  0.04060908035171451
adv train loss:  -0.9197857195831602 , diff:  0.021388096902228426
adv train loss:  -1.0987683084676974 , diff:  0.1789825888845371
adv train loss:  -0.8526869303168496 , diff:  0.24608137815084774
adv train loss:  -1.0616876187450544 , diff:  0.20900068842820474
adv train loss:  -0.9528931340755662 , diff:  0.10879448466948816
layer  13  adv train finish, try to retain  502
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0  ==>  64 / 64 , inc:  1
layer  2  :  0  ==>  128 / 128 , inc:  1
layer  3  :  0  ==>  128 / 128 , inc:  1
layer  4  :  0  ==>  256 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  2
layer  6  :  0.5625  ==>  144 / 256 , inc:  2
layer  7  :  0  ==>  512 / 512 , inc:  1
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  2
layer  11  :  0.16796875  ==>  86 / 512 , inc:  2
layer  12  :  0  ==>  512 / 512 , inc:  1
layer  13  :  0.083984375  ==>  43 / 512 , inc:  1
eps [0.06075000000000001, 0.00010011291503906251, 0.00010011291503906251, 0.00010011291503906251, 0.00010011291503906251, 0.005695312500000001, 0.005695312500000001, 0.00010011291503906251, 0.00010011291503906251, 0.00010011291503906251, 0.0005339355468750001, 0.005695312500000001, 0.00010011291503906251, 0.022781250000000003]  wait [2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 0, 0, 2, 2]  inc [1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1]  tol: 3
$$$$$$$$$$$$$ epoch  16  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -0.25500486539567646 , diff:  0.25500486539567646
adv train loss:  -0.10452914145571413 , diff:  0.15047572393996234
adv train loss:  -0.10373674097763796 , diff:  0.0007924004780761607
layer  0  adv train finish, try to retain  63
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -0.15083802852859662 , diff:  0.15083802852859662
adv train loss:  -0.1833429343150783 , diff:  0.032504905786481686
adv train loss:  -0.10541550248490239 , diff:  0.07792743183017592
adv train loss:  -0.13726467854826296 , diff:  0.031849176063360574
adv train loss:  -0.1216589449395542 , diff:  0.01560573360870876
adv train loss:  -0.18194447865244 , diff:  0.06028553371288581
adv train loss:  -0.08351017484665135 , diff:  0.09843430380578866
adv train loss:  -0.1904786273762511 , diff:  0.10696845252959974
adv train loss:  -0.11829757724080991 , diff:  0.07218105013544118
adv train loss:  -0.1524606867778857 , diff:  0.0341631095370758
layer  1  adv train finish, try to retain  56
test acc: top1 ->  91.97 ; top5 ->  98.72  and loss:  113.06754542887211
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.05693377531315491
test acc: top1 ->  92.07 ; top5 ->  98.74  and loss:  110.08441309630871
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.020609428317541756
test acc: top1 ->  91.95 ; top5 ->  98.84  and loss:  111.04887589812279
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.009739285469201775
test acc: top1 ->  91.96 ; top5 ->  98.86  and loss:  108.81150510907173
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.0023107161880204785
test acc: top1 ->  92.04 ; top5 ->  98.81  and loss:  109.70812013745308
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.005665624415996717
test acc: top1 ->  92.02 ; top5 ->  98.87  and loss:  110.11584728956223
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.0026235135089898165
test acc: top1 ->  91.95 ; top5 ->  98.85  and loss:  111.20447707176208
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.008579814025182486
test acc: top1 ->  92.05 ; top5 ->  98.88  and loss:  110.21630463004112
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.006662406407826893
test acc: top1 ->  91.88 ; top5 ->  98.89  and loss:  110.32359862327576
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.00904721578059764
test acc: top1 ->  91.98 ; top5 ->  98.94  and loss:  108.9428169131279
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.013596389161648403
test acc: top1 ->  92.09 ; top5 ->  98.95  and loss:  107.78408922255039
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  64 / 64 , inc:  1
---------------- start layer  2  ---------------
layer  2  adv train finish, try to retain  100
test acc: top1 ->  92.11 ; top5 ->  98.94  and loss:  108.18273504078388
==> this epoch:  100 / 128
---------------- start layer  3  ---------------
layer  3  adv train finish, try to retain  87
test acc: top1 ->  92.11 ; top5 ->  98.92  and loss:  107.75742368400097
==> this epoch:  87 / 128
---------------- start layer  4  ---------------
adv train loss:  -0.02648384522035485 , diff:  0.02648384522035485
adv train loss:  -0.030555435828091504 , diff:  0.004071590607736653
layer  4  adv train finish, try to retain  156
test acc: top1 ->  92.09 ; top5 ->  98.94  and loss:  107.89131827652454
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.004577402191671354
test acc: top1 ->  92.02 ; top5 ->  98.98  and loss:  111.22415624558926
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.011933600104038433
test acc: top1 ->  92.15 ; top5 ->  98.85  and loss:  112.10990560054779
==> this epoch:  156 / 256
---------------- start layer  5  ---------------
adv train loss:  -0.001990381452612766 , diff:  0.001990381452612766
layer  5  adv train finish, try to retain  256
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -0.004950746642862214 , diff:  0.004950746642862214
layer  6  adv train finish, try to retain  256
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
layer  7  adv train finish, try to retain  28
test acc: top1 ->  55.99 ; top5 ->  94.44  and loss:  182.43031454086304
forward train acc: top1 ->  97.25599997802735 ; top5 ->  99.968  and loss:  15.722439154982567
test acc: top1 ->  89.97 ; top5 ->  98.73  and loss:  108.24152034521103
forward train acc: top1 ->  98.996 ; top5 ->  99.99  and loss:  4.053028899244964
test acc: top1 ->  90.51 ; top5 ->  98.75  and loss:  102.52379381656647
forward train acc: top1 ->  99.31999997558594 ; top5 ->  99.988  and loss:  2.6087569462833926
test acc: top1 ->  90.57 ; top5 ->  98.73  and loss:  100.27966579794884
forward train acc: top1 ->  99.44599997558593 ; top5 ->  99.994  and loss:  2.0194690921343863
test acc: top1 ->  90.86 ; top5 ->  98.83  and loss:  96.43271000683308
forward train acc: top1 ->  99.54599997802734 ; top5 ->  99.998  and loss:  1.5752825628151186
test acc: top1 ->  90.89 ; top5 ->  98.84  and loss:  93.45053079724312
forward train acc: top1 ->  99.59599997558594 ; top5 ->  100.0  and loss:  1.357156488520559
test acc: top1 ->  90.85 ; top5 ->  98.81  and loss:  92.67452868819237
forward train acc: top1 ->  99.55599997802734 ; top5 ->  99.998  and loss:  1.370714873453835
test acc: top1 ->  90.93 ; top5 ->  98.86  and loss:  92.09262701869011
forward train acc: top1 ->  99.65 ; top5 ->  99.994  and loss:  1.1660977606079541
test acc: top1 ->  91.07 ; top5 ->  98.86  and loss:  90.24610082805157
forward train acc: top1 ->  99.69000000244141 ; top5 ->  100.0  and loss:  1.0592065647360869
test acc: top1 ->  91.07 ; top5 ->  98.89  and loss:  89.20018097758293
forward train acc: top1 ->  99.67199997558593 ; top5 ->  99.996  and loss:  1.1380708782817237
test acc: top1 ->  91.08 ; top5 ->  98.89  and loss:  88.08768863976002
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -0.7625979687727522 , diff:  0.7625979687727522
adv train loss:  -0.7358590021613054 , diff:  0.026738966611446813
adv train loss:  -0.6903017577133141 , diff:  0.04555724444799125
adv train loss:  -0.7275102069834247 , diff:  0.03720844927011058
adv train loss:  -0.6577866498264484 , diff:  0.06972355715697631
adv train loss:  -0.8391847215243615 , diff:  0.1813980716979131
adv train loss:  -0.7751468597562052 , diff:  0.06403786176815629
adv train loss:  -0.744041471843957 , diff:  0.03110538791224826
adv train loss:  -0.7419286828153417 , diff:  0.002112789028615225
layer  8  adv train finish, try to retain  104
test acc: top1 ->  87.47 ; top5 ->  98.24  and loss:  113.833381742239
forward train acc: top1 ->  99.924 ; top5 ->  100.0  and loss:  0.28489719795345536
test acc: top1 ->  91.7 ; top5 ->  98.71  and loss:  116.74387991428375
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.05188215737950941
test acc: top1 ->  91.73 ; top5 ->  98.81  and loss:  116.75806909799576
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.101433045981139
test acc: top1 ->  91.8 ; top5 ->  98.87  and loss:  112.62985821068287
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.03127891098847613
test acc: top1 ->  91.8 ; top5 ->  98.91  and loss:  114.4134392440319
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.04062066016626886
test acc: top1 ->  91.78 ; top5 ->  98.87  and loss:  113.01454181969166
forward train acc: top1 ->  99.98799997558594 ; top5 ->  100.0  and loss:  0.03616373431827924
test acc: top1 ->  91.76 ; top5 ->  98.88  and loss:  112.31093227863312
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.024023760673429706
test acc: top1 ->  91.79 ; top5 ->  98.92  and loss:  112.70212763547897
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.01959998323218315
test acc: top1 ->  91.74 ; top5 ->  98.97  and loss:  113.47231125831604
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.011779943497913337
test acc: top1 ->  91.75 ; top5 ->  98.95  and loss:  113.06643033027649
forward train acc: top1 ->  99.99599997558593 ; top5 ->  100.0  and loss:  0.021856151312931615
test acc: top1 ->  91.75 ; top5 ->  98.97  and loss:  112.73809087276459
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -0.02899158580112271 , diff:  0.02899158580112271
adv train loss:  -0.023321001338445058 , diff:  0.0056705844626776525
layer  9  adv train finish, try to retain  31
test acc: top1 ->  62.82 ; top5 ->  94.97  and loss:  272.4781836271286
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.11932341059946339
test acc: top1 ->  91.92 ; top5 ->  98.97  and loss:  98.67281113564968
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.011176992255059304
test acc: top1 ->  92.01 ; top5 ->  99.04  and loss:  97.42146530747414
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.006978771487524682
test acc: top1 ->  91.99 ; top5 ->  99.06  and loss:  98.00567089021206
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.0075714636313932715
test acc: top1 ->  91.94 ; top5 ->  99.01  and loss:  98.88255913555622
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -0.39562485941132763 , diff:  0.39562485941132763
adv train loss:  -0.40932148225329 , diff:  0.01369662284196238
adv train loss:  -0.4700784069718793 , diff:  0.06075692471858929
adv train loss:  -0.5139816097744188 , diff:  0.043903202802539454
adv train loss:  -0.2992794120000326 , diff:  0.21470219777438615
adv train loss:  -0.3827771717296855 , diff:  0.0834977597296529
adv train loss:  -0.31960483464354184 , diff:  0.06317233708614367
adv train loss:  -0.3048486314510228 , diff:  0.014756203192519024
adv train loss:  -0.3403158490837086 , diff:  0.03546721763268579
adv train loss:  -0.3479389129424817 , diff:  0.007623063858773094
layer  10  adv train finish, try to retain  481
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -0.09531308603982325 , diff:  0.09531308603982325
adv train loss:  -0.10447216975171614 , diff:  0.00915908371189289
layer  11  adv train finish, try to retain  420
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -0.08529844118345409 , diff:  0.08529844118345409
adv train loss:  -0.10046912059283386 , diff:  0.015170679409379773
adv train loss:  -0.052742057910109 , diff:  0.04772706268272486
adv train loss:  -0.05372722565607546 , diff:  0.0009851677459664643
layer  12  adv train finish, try to retain  30
test acc: top1 ->  89.96 ; top5 ->  98.88  and loss:  84.14519657194614
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.25134615968272556
test acc: top1 ->  91.9 ; top5 ->  99.12  and loss:  73.53461849689484
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.039460601430619135
test acc: top1 ->  91.86 ; top5 ->  99.13  and loss:  74.54922215640545
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.029644376461874344
test acc: top1 ->  92.03 ; top5 ->  99.07  and loss:  75.5534298568964
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.02345126941872877
test acc: top1 ->  92.09 ; top5 ->  99.09  and loss:  76.11134541034698
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.020814905221413937
test acc: top1 ->  92.03 ; top5 ->  99.1  and loss:  78.06904818117619
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.01344402929134958
test acc: top1 ->  91.99 ; top5 ->  99.09  and loss:  78.53750021755695
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.017112178678871715
test acc: top1 ->  92.07 ; top5 ->  99.13  and loss:  78.85221593081951
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.008647614075925958
test acc: top1 ->  92.09 ; top5 ->  99.09  and loss:  79.74998508393764
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.022371035623109492
test acc: top1 ->  92.1 ; top5 ->  99.05  and loss:  79.42487397789955
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.015954714928739122
test acc: top1 ->  92.05 ; top5 ->  99.08  and loss:  80.03040735423565
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -0.37206608571258926 , diff:  0.37206608571258926
adv train loss:  -0.37779570581551525 , diff:  0.005729620102925992
layer  13  adv train finish, try to retain  503
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0  ==>  64 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  2
layer  3  :  0.6796875  ==>  87 / 128 , inc:  2
layer  4  :  0.609375  ==>  156 / 256 , inc:  2
layer  5  :  0.58203125  ==>  149 / 256 , inc:  2
layer  6  :  0.5625  ==>  144 / 256 , inc:  2
layer  7  :  0  ==>  512 / 512 , inc:  1
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  2
layer  11  :  0.16796875  ==>  86 / 512 , inc:  2
layer  12  :  0  ==>  512 / 512 , inc:  1
layer  13  :  0.083984375  ==>  43 / 512 , inc:  1
eps [0.12150000000000002, 7.508468627929689e-05, 0.00010011291503906251, 0.00010011291503906251, 0.00010011291503906251, 0.011390625000000001, 0.011390625000000001, 7.508468627929689e-05, 7.508468627929689e-05, 7.508468627929689e-05, 0.0010678710937500001, 0.011390625000000001, 7.508468627929689e-05, 0.045562500000000006]  wait [2, 4, 0, 0, 0, 0, 0, 4, 4, 4, 0, 0, 4, 2]  inc [1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1]  tol: 3
$$$$$$$$$$$$$ epoch  17  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -0.10726508218795061 , diff:  0.10726508218795061
adv train loss:  -0.1730510588058678 , diff:  0.06578597661791719
adv train loss:  -0.11628914896573406 , diff:  0.05676190984013374
adv train loss:  -0.0902487131243106 , diff:  0.02604043584142346
adv train loss:  -0.18268259028263856 , diff:  0.09243387715832796
adv train loss:  -0.10077610203370568 , diff:  0.08190648824893287
adv train loss:  -0.1794060168285796 , diff:  0.07862991479487391
adv train loss:  -0.13261771489487728 , diff:  0.046788301933702314
adv train loss:  -0.11973868383574882 , diff:  0.012879031059128465
adv train loss:  -0.09156762492784765 , diff:  0.02817105890790117
layer  0  adv train finish, try to retain  64
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
### skip layer  1 wait:  4  ###
---------------- start layer  2  ---------------
adv train loss:  -0.18496694573332206 , diff:  0.18496694573332206
adv train loss:  -0.16378991297096945 , diff:  0.02117703276235261
adv train loss:  -0.14193507449817844 , diff:  0.021854838472791016
adv train loss:  -0.09339461646959535 , diff:  0.04854045802858309
adv train loss:  -0.13017715882961056 , diff:  0.036782542360015213
adv train loss:  -0.0973472846576442 , diff:  0.03282987417196637
adv train loss:  -0.08882069088394928 , diff:  0.008526593773694913
layer  2  adv train finish, try to retain  128
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -0.12523739946482237 , diff:  0.12523739946482237
adv train loss:  -0.13833380707364995 , diff:  0.013096407608827576
adv train loss:  -0.1550561373387609 , diff:  0.01672233026511094
adv train loss:  -0.1483510770922294 , diff:  0.006705060246531502
layer  3  adv train finish, try to retain  128
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -0.1306157555518439 , diff:  0.1306157555518439
adv train loss:  -0.11451717242198356 , diff:  0.016098583129860344
adv train loss:  -0.11952562923761434 , diff:  0.00500845681563078
layer  4  adv train finish, try to retain  256
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -0.057510640847795 , diff:  0.057510640847795
adv train loss:  -0.06828745544771664 , diff:  0.010776814599921636
adv train loss:  -0.1640867209789576 , diff:  0.09579926553124096
adv train loss:  -0.15705416518176207 , diff:  0.007032555797195528
layer  5  adv train finish, try to retain  256
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -0.09491585591354124 , diff:  0.09491585591354124
adv train loss:  -0.11752960922240163 , diff:  0.022613753308860396
adv train loss:  -0.13705149628367508 , diff:  0.019521887061273446
adv train loss:  -0.148993491930014 , diff:  0.01194199564633891
adv train loss:  -0.08209274502223707 , diff:  0.06690074690777692
adv train loss:  -0.16786412897272385 , diff:  0.08577138395048678
adv train loss:  -0.084998634421936 , diff:  0.08286549455078784
adv train loss:  -0.09589527559728594 , diff:  0.010896641175349941
adv train loss:  -0.1079023149504792 , diff:  0.01200703935319325
adv train loss:  -0.1270065552325832 , diff:  0.019104240282104
layer  6  adv train finish, try to retain  256
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
### skip layer  8 wait:  4  ###
---------------- start layer  9  ---------------
### skip layer  9 wait:  4  ###
---------------- start layer  10  ---------------
adv train loss:  -0.11297264875292967 , diff:  0.11297264875292967
adv train loss:  -0.09316909921608385 , diff:  0.01980354953684582
adv train loss:  -0.10437460854518577 , diff:  0.011205509329101915
adv train loss:  -0.13207351918845234 , diff:  0.027698910643266572
adv train loss:  -0.13520832001086092 , diff:  0.003134800822408579
layer  10  adv train finish, try to retain  486
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -0.6823181768122595 , diff:  0.6823181768122595
adv train loss:  -0.5883910343400203 , diff:  0.09392714247223921
adv train loss:  -0.7178793330531335 , diff:  0.12948829871311318
adv train loss:  -0.7052304682583781 , diff:  0.012648864794755355
adv train loss:  -0.6253974494757131 , diff:  0.07983301878266502
adv train loss:  -0.6722443071921589 , diff:  0.04684685771644581
adv train loss:  -0.7300703780492768 , diff:  0.057826070857117884
adv train loss:  -0.5928203915245831 , diff:  0.13724998652469367
adv train loss:  -0.6036196760396706 , diff:  0.010799284515087493
adv train loss:  -0.6741716233445914 , diff:  0.07055194730492076
layer  11  adv train finish, try to retain  443
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
adv train loss:  -0.3276519288712194 , diff:  0.3276519288712194
adv train loss:  -0.42553557048086077 , diff:  0.09788364160964136
adv train loss:  -0.5891985248308629 , diff:  0.1636629543500021
adv train loss:  -0.5293549505986448 , diff:  0.05984357423221809
adv train loss:  -0.35793849757828866 , diff:  0.17141645302035613
adv train loss:  -0.3921543563683372 , diff:  0.034215858790048514
adv train loss:  -0.4245989546034252 , diff:  0.03244459823508805
adv train loss:  -0.4545739848545054 , diff:  0.02997503025108017
adv train loss:  -0.36439779840847564 , diff:  0.09017618644602976
adv train loss:  -0.5228767753093848 , diff:  0.15847897690090917
layer  13  adv train finish, try to retain  501
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0  ==>  64 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  2
layer  3  :  0.6796875  ==>  87 / 128 , inc:  2
layer  4  :  0.609375  ==>  156 / 256 , inc:  2
layer  5  :  0.58203125  ==>  149 / 256 , inc:  2
layer  6  :  0.5625  ==>  144 / 256 , inc:  2
layer  7  :  0  ==>  512 / 512 , inc:  1
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  2
layer  11  :  0.16796875  ==>  86 / 512 , inc:  2
layer  12  :  0  ==>  512 / 512 , inc:  1
layer  13  :  0.083984375  ==>  43 / 512 , inc:  1
eps [0.24300000000000005, 7.508468627929689e-05, 0.00020022583007812503, 0.00020022583007812503, 0.00020022583007812503, 0.022781250000000003, 0.022781250000000003, 7.508468627929689e-05, 7.508468627929689e-05, 7.508468627929689e-05, 0.0021357421875000003, 0.022781250000000003, 7.508468627929689e-05, 0.09112500000000001]  wait [2, 3, 0, 0, 0, 0, 0, 3, 3, 3, 0, 0, 3, 2]  inc [1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1]  tol: 3
$$$$$$$$$$$$$ epoch  18  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -0.15264536860922817 , diff:  0.15264536860922817
adv train loss:  -0.12284465976733827 , diff:  0.0298007088418899
adv train loss:  -0.07929179004258913 , diff:  0.043552869724749144
adv train loss:  -0.17894545132367057 , diff:  0.09965366128108144
adv train loss:  -0.12812789032614091 , diff:  0.050817560997529654
adv train loss:  -0.09772152734876727 , diff:  0.030406362977373647
adv train loss:  -0.08721261177925044 , diff:  0.01050891556951683
adv train loss:  -0.14176278353625094 , diff:  0.0545501717570005
adv train loss:  -0.11923470362398803 , diff:  0.02252807991226291
adv train loss:  -0.1506344852869006 , diff:  0.031399781662912574
layer  0  adv train finish, try to retain  64
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
### skip layer  1 wait:  3  ###
---------------- start layer  2  ---------------
adv train loss:  -0.10940303273491736 , diff:  0.10940303273491736
adv train loss:  -0.12501319764487562 , diff:  0.015610164909958257
adv train loss:  -0.07578957993246149 , diff:  0.049223617712414125
adv train loss:  -0.1368313602724811 , diff:  0.061041780340019614
adv train loss:  -0.1282277576119668 , diff:  0.008603602660514298
layer  2  adv train finish, try to retain  128
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -0.14892567618790054 , diff:  0.14892567618790054
adv train loss:  -0.1358245197297947 , diff:  0.013101156458105834
adv train loss:  -0.11434476403519511 , diff:  0.02147975569459959
adv train loss:  -0.1414958837613085 , diff:  0.027151119726113393
adv train loss:  -0.11748566610185662 , diff:  0.024010217659451882
adv train loss:  -0.17180022569846187 , diff:  0.05431455959660525
adv train loss:  -0.12597248052043142 , diff:  0.045827745178030455
adv train loss:  -0.16312967360499897 , diff:  0.03715719308456755
adv train loss:  -0.11230747568788502 , diff:  0.050822197917113954
adv train loss:  -0.09590241228943341 , diff:  0.016405063398451603
layer  3  adv train finish, try to retain  128
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -0.1414781133798897 , diff:  0.1414781133798897
adv train loss:  -0.14004469408655495 , diff:  0.0014334192933347367
layer  4  adv train finish, try to retain  256
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -0.1371041393294945 , diff:  0.1371041393294945
adv train loss:  -0.10581370576801419 , diff:  0.03129043356148031
adv train loss:  -0.12267020494982717 , diff:  0.01685649918181298
adv train loss:  -0.08274806749795971 , diff:  0.03992213745186746
adv train loss:  -0.08890756489927298 , diff:  0.006159497401313274
layer  5  adv train finish, try to retain  256
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -0.17938918267191184 , diff:  0.17938918267191184
adv train loss:  -0.1126259864140593 , diff:  0.06676319625785254
adv train loss:  -0.12458673374931095 , diff:  0.011960747335251654
adv train loss:  -0.11436671024785028 , diff:  0.010220023501460673
adv train loss:  -0.11053731959327706 , diff:  0.003829390654573217
layer  6  adv train finish, try to retain  256
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
### skip layer  8 wait:  3  ###
---------------- start layer  9  ---------------
### skip layer  9 wait:  3  ###
---------------- start layer  10  ---------------
adv train loss:  -0.13259505120367976 , diff:  0.13259505120367976
adv train loss:  -0.0818243438952777 , diff:  0.05077070730840205
adv train loss:  -0.09972858765468118 , diff:  0.01790424375940347
adv train loss:  -0.16185334663896356 , diff:  0.06212475898428238
adv train loss:  -0.10530164558721822 , diff:  0.05655170105174534
adv train loss:  -0.12010944965504677 , diff:  0.014807804067828556
adv train loss:  -0.08969180466283433 , diff:  0.030417644992212445
adv train loss:  -0.09532991239757393 , diff:  0.005638107734739606
layer  10  adv train finish, try to retain  481
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -0.6520838793148869 , diff:  0.6520838793148869
adv train loss:  -0.6466366156091681 , diff:  0.005447263705718797
layer  11  adv train finish, try to retain  423
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
### skip layer  12 wait:  3  ###
---------------- start layer  13  ---------------
adv train loss:  -0.6714880898070987 , diff:  0.6714880898070987
adv train loss:  -0.6369711541565266 , diff:  0.03451693565057212
adv train loss:  -0.6894138380957884 , diff:  0.05244268393926177
adv train loss:  -0.6168284800478432 , diff:  0.07258535804794519
adv train loss:  -0.6935379246860975 , diff:  0.07670944463825435
adv train loss:  -0.7433377125344123 , diff:  0.049799787848314736
adv train loss:  -0.4993675253517722 , diff:  0.24397018718264007
adv train loss:  -0.7021347366498958 , diff:  0.20276721129812358
adv train loss:  -0.6217729950294597 , diff:  0.08036174162043608
adv train loss:  -0.5841215130058117 , diff:  0.037651482023647986
layer  13  adv train finish, try to retain  503
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0  ==>  64 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  2
layer  3  :  0.6796875  ==>  87 / 128 , inc:  2
layer  4  :  0.609375  ==>  156 / 256 , inc:  2
layer  5  :  0.58203125  ==>  149 / 256 , inc:  2
layer  6  :  0.5625  ==>  144 / 256 , inc:  2
layer  7  :  0  ==>  512 / 512 , inc:  1
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  2
layer  11  :  0.16796875  ==>  86 / 512 , inc:  2
layer  12  :  0  ==>  512 / 512 , inc:  1
layer  13  :  0.083984375  ==>  43 / 512 , inc:  1
eps [0.4860000000000001, 7.508468627929689e-05, 0.00040045166015625005, 0.00040045166015625005, 0.00040045166015625005, 0.045562500000000006, 0.045562500000000006, 7.508468627929689e-05, 7.508468627929689e-05, 7.508468627929689e-05, 0.0042714843750000005, 0.045562500000000006, 7.508468627929689e-05, 0.18225000000000002]  wait [2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 2, 2]  inc [1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1]  tol: 3
$$$$$$$$$$$$$ epoch  19  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -0.13920985673757968 , diff:  0.13920985673757968
adv train loss:  -0.08697932196173497 , diff:  0.05223053477584472
adv train loss:  -0.12032544730573136 , diff:  0.03334612534399639
adv train loss:  -0.10912813676259248 , diff:  0.011197310543138883
adv train loss:  -0.15776421281407238 , diff:  0.048636076051479904
adv train loss:  -0.09213094851247661 , diff:  0.06563326430159577
adv train loss:  -0.12161203370487783 , diff:  0.029481085192401224
adv train loss:  -0.11230383334623184 , diff:  0.00930820035864599
layer  0  adv train finish, try to retain  64
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -0.12465232951581129 , diff:  0.12465232951581129
adv train loss:  -0.1576843989751069 , diff:  0.0330320694592956
adv train loss:  -0.11099330167053267 , diff:  0.046691097304574214
adv train loss:  -0.16488874148126342 , diff:  0.05389543981073075
adv train loss:  -0.16436742240694002 , diff:  0.0005213190743234009
layer  1  adv train finish, try to retain  56
test acc: top1 ->  91.73 ; top5 ->  98.97  and loss:  116.61892265081406
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.015169138534247395
test acc: top1 ->  92.03 ; top5 ->  99.02  and loss:  111.4519484937191
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.00913733653405302
test acc: top1 ->  92.0 ; top5 ->  99.05  and loss:  110.35556134581566
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.004968252291249087
test acc: top1 ->  92.05 ; top5 ->  98.96  and loss:  111.56940920650959
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.0244486684059666
test acc: top1 ->  91.89 ; top5 ->  98.97  and loss:  114.35502403974533
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.003387411989933753
test acc: top1 ->  91.85 ; top5 ->  98.93  and loss:  117.05126643180847
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.020591198312104098
test acc: top1 ->  91.96 ; top5 ->  98.88  and loss:  116.35763588547707
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.012077405709220557
test acc: top1 ->  91.84 ; top5 ->  98.87  and loss:  116.3941163122654
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.003187147119788847
test acc: top1 ->  91.99 ; top5 ->  98.94  and loss:  116.13700744509697
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.0071908512409208925
test acc: top1 ->  91.95 ; top5 ->  99.0  and loss:  115.31208597123623
forward train acc: top1 ->  100.0 ; top5 ->  100.0  and loss:  0.001308069431701142
test acc: top1 ->  91.93 ; top5 ->  98.95  and loss:  116.43284229934216
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  64 / 64 , inc:  1
---------------- start layer  2  ---------------
************ all values are small in this layer **********
layer  2  adv train finish, try to retain  98
test acc: top1 ->  77.23 ; top5 ->  95.22  and loss:  389.22399163246155
forward train acc: top1 ->  99.40599997802734 ; top5 ->  99.99  and loss:  3.5590451779135037
test acc: top1 ->  90.58 ; top5 ->  99.07  and loss:  112.63974758982658
forward train acc: top1 ->  99.5020000024414 ; top5 ->  99.996  and loss:  2.3930528296041302
test acc: top1 ->  90.75 ; top5 ->  99.17  and loss:  97.89902827143669
forward train acc: top1 ->  99.586 ; top5 ->  99.996  and loss:  1.7077735908897012
test acc: top1 ->  90.91 ; top5 ->  99.22  and loss:  87.99113151431084
forward train acc: top1 ->  99.60399997558594 ; top5 ->  100.0  and loss:  1.3962440992181655
test acc: top1 ->  90.9 ; top5 ->  99.2  and loss:  81.49223707616329
forward train acc: top1 ->  99.63 ; top5 ->  99.998  and loss:  1.269347911758814
test acc: top1 ->  90.97 ; top5 ->  99.26  and loss:  75.72773534059525
forward train acc: top1 ->  99.624 ; top5 ->  99.996  and loss:  1.2264737418154255
test acc: top1 ->  90.95 ; top5 ->  99.28  and loss:  74.85286538302898
forward train acc: top1 ->  99.70399997558594 ; top5 ->  100.0  and loss:  0.8965138191124424
test acc: top1 ->  91.01 ; top5 ->  99.29  and loss:  72.16423477232456
forward train acc: top1 ->  99.69599997558593 ; top5 ->  99.998  and loss:  1.0342352066654712
test acc: top1 ->  91.14 ; top5 ->  99.26  and loss:  71.08860605955124
forward train acc: top1 ->  99.71599997802734 ; top5 ->  100.0  and loss:  0.8586815883172676
test acc: top1 ->  91.24 ; top5 ->  99.28  and loss:  69.87609401345253
forward train acc: top1 ->  99.738 ; top5 ->  99.998  and loss:  0.7748820491833612
test acc: top1 ->  91.11 ; top5 ->  99.34  and loss:  69.49961470067501
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  2
---------------- start layer  3  ---------------
adv train loss:  -0.044885319515742594 , diff:  0.044885319515742594
adv train loss:  -0.03380429406024632 , diff:  0.011081025455496274
adv train loss:  -0.02936313829923165 , diff:  0.00444115576101467
************ all values are small in this layer **********
layer  3  adv train finish, try to retain  85
test acc: top1 ->  80.52 ; top5 ->  97.43  and loss:  160.21564584970474
forward train acc: top1 ->  97.63800001708984 ; top5 ->  99.958  and loss:  9.362782876938581
test acc: top1 ->  88.97 ; top5 ->  98.97  and loss:  65.27634328603745
forward train acc: top1 ->  98.13199998535156 ; top5 ->  99.978  and loss:  6.1048860512673855
test acc: top1 ->  89.44 ; top5 ->  98.98  and loss:  57.24174624681473
forward train acc: top1 ->  98.43199998291016 ; top5 ->  99.964  and loss:  5.149563011713326
test acc: top1 ->  89.59 ; top5 ->  99.09  and loss:  55.072781302034855
forward train acc: top1 ->  98.64400000488281 ; top5 ->  99.97999997558594  and loss:  4.264948529191315
test acc: top1 ->  89.71 ; top5 ->  99.08  and loss:  53.31490196287632
forward train acc: top1 ->  98.70800000244141 ; top5 ->  99.976  and loss:  3.952704107388854
test acc: top1 ->  89.81 ; top5 ->  99.11  and loss:  52.62382762134075
forward train acc: top1 ->  98.89000000488281 ; top5 ->  99.982  and loss:  3.490442634560168
test acc: top1 ->  89.89 ; top5 ->  99.11  and loss:  52.141729190945625
forward train acc: top1 ->  98.87800000976563 ; top5 ->  99.98  and loss:  3.3754453724250197
test acc: top1 ->  90.05 ; top5 ->  99.13  and loss:  51.98489920049906
forward train acc: top1 ->  98.8920000024414 ; top5 ->  99.99  and loss:  3.3077873419970274
test acc: top1 ->  90.01 ; top5 ->  99.13  and loss:  52.47156370431185
forward train acc: top1 ->  98.93400000488282 ; top5 ->  99.988  and loss:  3.2753816097974777
test acc: top1 ->  89.99 ; top5 ->  99.13  and loss:  52.323067501187325
forward train acc: top1 ->  99.04800000244141 ; top5 ->  99.986  and loss:  2.948071936145425
test acc: top1 ->  90.09 ; top5 ->  99.1  and loss:  52.27379000931978
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  2
---------------- start layer  4  ---------------
adv train loss:  -0.3213021047413349 , diff:  0.3213021047413349
adv train loss:  -0.3967144946800545 , diff:  0.07541238993871957
adv train loss:  -0.339505958952941 , diff:  0.057208535727113485
adv train loss:  -0.3223668642458506 , diff:  0.017139094707090408
adv train loss:  -0.3182305700611323 , diff:  0.004136294184718281
************ all values are small in this layer **********
layer  4  adv train finish, try to retain  154
test acc: top1 ->  43.68 ; top5 ->  76.82  and loss:  284.35245633125305
forward train acc: top1 ->  93.01999998535156 ; top5 ->  99.722  and loss:  24.349163189530373
test acc: top1 ->  86.59 ; top5 ->  98.8  and loss:  58.67409060895443
forward train acc: top1 ->  94.3660000048828 ; top5 ->  99.848  and loss:  17.11804573237896
test acc: top1 ->  87.36 ; top5 ->  98.89  and loss:  52.91821911931038
forward train acc: top1 ->  95.02799999267579 ; top5 ->  99.898  and loss:  14.793048202991486
test acc: top1 ->  87.9 ; top5 ->  98.93  and loss:  50.16187062859535
forward train acc: top1 ->  95.50399999267579 ; top5 ->  99.91  and loss:  13.48221768438816
test acc: top1 ->  88.34 ; top5 ->  99.0  and loss:  48.737473875284195
forward train acc: top1 ->  95.87200001464844 ; top5 ->  99.924  and loss:  12.209065906703472
test acc: top1 ->  88.69 ; top5 ->  99.04  and loss:  47.66344910860062
forward train acc: top1 ->  96.14000001708985 ; top5 ->  99.922  and loss:  11.37027045339346
test acc: top1 ->  88.64 ; top5 ->  99.0  and loss:  48.07136383652687
forward train acc: top1 ->  96.32600001953125 ; top5 ->  99.922  and loss:  10.8627397865057
test acc: top1 ->  88.75 ; top5 ->  99.06  and loss:  46.86132353544235
forward train acc: top1 ->  96.42399998779297 ; top5 ->  99.93799997558594  and loss:  10.460155181586742
test acc: top1 ->  88.85 ; top5 ->  99.0  and loss:  47.65738271176815
forward train acc: top1 ->  96.56200001220704 ; top5 ->  99.954  and loss:  10.004796672612429
test acc: top1 ->  89.05 ; top5 ->  99.02  and loss:  47.236855536699295
forward train acc: top1 ->  96.49600001708984 ; top5 ->  99.956  and loss:  9.981150012463331
test acc: top1 ->  89.13 ; top5 ->  99.01  and loss:  46.720799669623375
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  156 / 256 , inc:  2
---------------- start layer  5  ---------------
adv train loss:  -1.0008600847795606 , diff:  1.0008600847795606
adv train loss:  -0.956864848267287 , diff:  0.04399523651227355
adv train loss:  -0.9860738292336464 , diff:  0.029208980966359377
adv train loss:  -1.0319629930891097 , diff:  0.045889163855463266
adv train loss:  -0.9893042966723442 , diff:  0.04265869641676545
adv train loss:  -1.0591724454425275 , diff:  0.06986814877018332
adv train loss:  -0.9877706724219024 , diff:  0.07140177302062511
adv train loss:  -0.9677084102295339 , diff:  0.020062262192368507
adv train loss:  -0.9538668794557452 , diff:  0.01384153077378869
adv train loss:  -0.9804631266742945 , diff:  0.02659624721854925
layer  5  adv train finish, try to retain  255
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -1.0096827682573348 , diff:  1.0096827682573348
adv train loss:  -1.0250460258685052 , diff:  0.015363257611170411
adv train loss:  -0.9729830166324973 , diff:  0.05206300923600793
adv train loss:  -1.052794115850702 , diff:  0.07981109921820462
adv train loss:  -0.9668157703708857 , diff:  0.0859783454798162
adv train loss:  -0.9894189089536667 , diff:  0.022603138582780957
adv train loss:  -1.0092828799970448 , diff:  0.019863971043378115
adv train loss:  -0.9208590264897794 , diff:  0.08842385350726545
adv train loss:  -0.9905735407955945 , diff:  0.0697145143058151
adv train loss:  -1.0573970419354737 , diff:  0.06682350113987923
layer  6  adv train finish, try to retain  255
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -0.9204207328148186 , diff:  0.9204207328148186
adv train loss:  -1.050141496118158 , diff:  0.12972076330333948
adv train loss:  -0.9930208413861692 , diff:  0.05712065473198891
adv train loss:  -1.0049806276801974 , diff:  0.011959786294028163
adv train loss:  -1.0005350965075195 , diff:  0.0044455311726778746
layer  7  adv train finish, try to retain  104
test acc: top1 ->  88.55 ; top5 ->  98.84  and loss:  39.83940176665783
forward train acc: top1 ->  99.914 ; top5 ->  100.0  and loss:  0.38231786439428106
test acc: top1 ->  91.86 ; top5 ->  99.27  and loss:  49.94232513010502
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.13865530230395962
test acc: top1 ->  92.06 ; top5 ->  99.25  and loss:  55.411198168992996
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.10863656220317353
test acc: top1 ->  92.08 ; top5 ->  99.2  and loss:  58.74359031021595
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.06658448904272518
test acc: top1 ->  92.07 ; top5 ->  99.19  and loss:  60.24207119643688
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.06174645505598164
test acc: top1 ->  92.08 ; top5 ->  99.18  and loss:  63.2623979896307
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.062336339452485845
test acc: top1 ->  92.11 ; top5 ->  99.18  and loss:  63.30366373062134
==> this epoch:  104 / 512
---------------- start layer  8  ---------------
adv train loss:  -0.05054438790011773 , diff:  0.05054438790011773
adv train loss:  -0.0579992832026619 , diff:  0.007454895302544173
layer  8  adv train finish, try to retain  64
test acc: top1 ->  91.05 ; top5 ->  99.02  and loss:  55.37612684071064
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.09116088429072988
test acc: top1 ->  91.9 ; top5 ->  98.95  and loss:  74.97712056338787
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.055206771150551504
test acc: top1 ->  91.89 ; top5 ->  99.09  and loss:  77.03502662479877
forward train acc: top1 ->  99.97599997558594 ; top5 ->  100.0  and loss:  0.05331562274659518
test acc: top1 ->  91.96 ; top5 ->  99.07  and loss:  75.86160144209862
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.055224651463504415
test acc: top1 ->  91.87 ; top5 ->  99.02  and loss:  79.35364097356796
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.020569376951243612
test acc: top1 ->  91.93 ; top5 ->  99.01  and loss:  79.62484160065651
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.031037207059853245
test acc: top1 ->  91.84 ; top5 ->  99.02  and loss:  81.4833302795887
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.030272670439103422
test acc: top1 ->  91.9 ; top5 ->  99.01  and loss:  82.05762027204037
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -0.11309632264692482 , diff:  0.11309632264692482
adv train loss:  -0.10737002011592267 , diff:  0.005726302531002148
layer  9  adv train finish, try to retain  35
test acc: top1 ->  51.59 ; top5 ->  96.55  and loss:  322.16322779655457
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.06855608241221489
test acc: top1 ->  91.92 ; top5 ->  99.1  and loss:  79.85887214541435
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.034542567102562316
test acc: top1 ->  91.97 ; top5 ->  99.11  and loss:  78.83371898531914
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.02134571758688253
test acc: top1 ->  91.94 ; top5 ->  99.1  and loss:  81.68750973045826
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03293775480960903
test acc: top1 ->  91.88 ; top5 ->  99.08  and loss:  81.94313913583755
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.0347728483411629
test acc: top1 ->  91.93 ; top5 ->  99.12  and loss:  81.16012789309025
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.037070311158458935
test acc: top1 ->  91.91 ; top5 ->  99.13  and loss:  79.85267016291618
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.014555015139649186
test acc: top1 ->  91.95 ; top5 ->  99.12  and loss:  81.43220192193985
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.015108222758499323
test acc: top1 ->  92.0 ; top5 ->  99.1  and loss:  81.33684888482094
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -0.14753017977636773 , diff:  0.14753017977636773
adv train loss:  -0.1539337245212664 , diff:  0.0064035447448986815
layer  10  adv train finish, try to retain  487
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -0.16771876590428292 , diff:  0.16771876590428292
adv train loss:  -0.10993045117174916 , diff:  0.05778831473253376
adv train loss:  -0.12231514276118105 , diff:  0.012384691589431895
adv train loss:  -0.09007000931705988 , diff:  0.032245133444121166
adv train loss:  -0.11463413396722899 , diff:  0.024564124650169106
adv train loss:  -0.1726812857286859 , diff:  0.05804715176145692
adv train loss:  -0.12991019391120062 , diff:  0.042771091817485285
adv train loss:  -0.09819064608836925 , diff:  0.03171954782283137
adv train loss:  -0.11218097872824728 , diff:  0.013990332639878034
adv train loss:  -0.1420955956527905 , diff:  0.029914616924543225
layer  11  adv train finish, try to retain  414
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -0.04921899803321139 , diff:  0.04921899803321139
adv train loss:  -0.04789757511753123 , diff:  0.0013214229156801593
layer  12  adv train finish, try to retain  32
test acc: top1 ->  91.06 ; top5 ->  99.21  and loss:  61.607002168893814
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.10789081852999516
test acc: top1 ->  92.01 ; top5 ->  99.16  and loss:  59.27248552441597
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.05390030157650472
test acc: top1 ->  92.09 ; top5 ->  99.08  and loss:  61.79799522459507
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.023754328562063165
test acc: top1 ->  91.98 ; top5 ->  99.11  and loss:  63.84849616885185
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.028782146924640983
test acc: top1 ->  91.98 ; top5 ->  99.03  and loss:  66.46862250566483
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.028466299771935155
test acc: top1 ->  92.04 ; top5 ->  98.95  and loss:  69.30502979457378
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -2.029466555075487 , diff:  2.029466555075487
adv train loss:  -2.140254495316185 , diff:  0.11078794024069794
adv train loss:  -2.2662378381355666 , diff:  0.1259833428193815
adv train loss:  -2.2410679899621755 , diff:  0.025169848173391074
adv train loss:  -2.3006149343564175 , diff:  0.05954694439424202
adv train loss:  -2.1509552908246405 , diff:  0.14965964353177696
adv train loss:  -2.200235572454403 , diff:  0.04928028162976261
adv train loss:  -2.233723667450249 , diff:  0.03348809499584604
adv train loss:  -2.0988832218572497 , diff:  0.13484044559299946
adv train loss:  -2.0876319539238466 , diff:  0.011251267933403142
layer  13  adv train finish, try to retain  510
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0  ==>  64 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  2
layer  6  :  0.5625  ==>  144 / 256 , inc:  2
layer  7  :  0.203125  ==>  104 / 512 , inc:  2
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  2
layer  11  :  0.16796875  ==>  86 / 512 , inc:  2
layer  12  :  0  ==>  512 / 512 , inc:  1
layer  13  :  0.083984375  ==>  43 / 512 , inc:  1
eps [0.9720000000000002, 5.631351470947266e-05, 0.00030033874511718755, 0.00030033874511718755, 0.00030033874511718755, 0.09112500000000001, 0.09112500000000001, 7.508468627929689e-05, 5.631351470947266e-05, 5.631351470947266e-05, 0.008542968750000001, 0.09112500000000001, 5.631351470947266e-05, 0.36450000000000005]  wait [2, 4, 2, 2, 2, 0, 0, 0, 4, 4, 0, 0, 4, 2]  inc [1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 2, 1, 1]  tol: 3
$$$$$$$$$$$$$ epoch  20  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -0.04294786386162741 , diff:  0.04294786386162741
adv train loss:  -0.0661516049635793 , diff:  0.02320374110195189
adv train loss:  -0.022141826890674565 , diff:  0.04400977807290474
adv train loss:  -0.09228067248159277 , diff:  0.07013884559091821
adv train loss:  -0.030425817577224734 , diff:  0.06185485490436804
adv train loss:  -0.05199951173216277 , diff:  0.02157369415493804
adv train loss:  -0.036574816263055254 , diff:  0.015424695469107519
adv train loss:  -0.06340577610990294 , diff:  0.02683095984684769
adv train loss:  -0.020238674131178414 , diff:  0.04316710197872453
adv train loss:  -0.028559572573612968 , diff:  0.008320898442434554
layer  0  adv train finish, try to retain  64
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
### skip layer  1 wait:  4  ###
---------------- start layer  2  ---------------
layer  2  adv train finish, try to retain  128
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -0.035862918697148416 , diff:  0.035862918697148416
adv train loss:  -0.01813210809359589 , diff:  0.017730810603552527
adv train loss:  -0.04344316235574297 , diff:  0.02531105426214708
adv train loss:  -0.027403872376680738 , diff:  0.016039289979062232
adv train loss:  -0.02212427673930506 , diff:  0.005279595637375678
layer  3  adv train finish, try to retain  128
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -0.02720883975052857 , diff:  0.02720883975052857
adv train loss:  -0.03401358756764239 , diff:  0.006804747817113821
layer  4  adv train finish, try to retain  255
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -0.01828005248671616 , diff:  0.01828005248671616
adv train loss:  -0.0649831180644469 , diff:  0.04670306557773074
adv train loss:  -0.035107500008507486 , diff:  0.029875618055939412
adv train loss:  -0.042225163198963855 , diff:  0.007117663190456369
layer  5  adv train finish, try to retain  256
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -0.032199454773945035 , diff:  0.032199454773945035
adv train loss:  -0.056317074467415296 , diff:  0.02411761969347026
adv train loss:  -0.02506401641596767 , diff:  0.031253058051447624
adv train loss:  -0.02094374537927024 , diff:  0.0041202710366974316
layer  6  adv train finish, try to retain  256
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -0.054852970364663634 , diff:  0.054852970364663634
adv train loss:  -0.07297534992540022 , diff:  0.01812237956073659
adv train loss:  -0.07414348179554509 , diff:  0.0011681318701448618
layer  7  adv train finish, try to retain  428
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
### skip layer  8 wait:  4  ###
---------------- start layer  9  ---------------
### skip layer  9 wait:  4  ###
---------------- start layer  10  ---------------
adv train loss:  -0.10415463980461936 , diff:  0.10415463980461936
adv train loss:  -0.16299726228044165 , diff:  0.05884262247582228
adv train loss:  -0.09785611773986602 , diff:  0.06514114454057562
adv train loss:  -0.06987482612021267 , diff:  0.027981291619653348
adv train loss:  -0.09586384647809609 , diff:  0.02598902035788342
adv train loss:  -0.09680936177392141 , diff:  0.0009455152958253166
layer  10  adv train finish, try to retain  485
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -0.08865659994444286 , diff:  0.08865659994444286
adv train loss:  -0.15735178944669315 , diff:  0.06869518950225029
adv train loss:  -0.15569656401203247 , diff:  0.0016552254346606787
layer  11  adv train finish, try to retain  423
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
adv train loss:  -49.505621790885925 , diff:  49.505621790885925
adv train loss:  -49.94099396467209 , diff:  0.43537217378616333
adv train loss:  -48.35831367969513 , diff:  1.5826802849769592
adv train loss:  -49.547197073698044 , diff:  1.1888833940029144
adv train loss:  -48.48077067732811 , diff:  1.066426396369934
adv train loss:  -49.94405646622181 , diff:  1.4632857888936996
adv train loss:  -49.206848084926605 , diff:  0.7372083812952042
adv train loss:  -50.98243373632431 , diff:  1.775585651397705
adv train loss:  -49.065826654434204 , diff:  1.9166070818901062
adv train loss:  -50.00536361336708 , diff:  0.9395369589328766
layer  13  adv train finish, try to retain  511
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0  ==>  64 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  2
layer  6  :  0.5625  ==>  144 / 256 , inc:  2
layer  7  :  0.203125  ==>  104 / 512 , inc:  2
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  2
layer  11  :  0.16796875  ==>  86 / 512 , inc:  2
layer  12  :  0  ==>  512 / 512 , inc:  1
layer  13  :  0.083984375  ==>  43 / 512 , inc:  1
eps [1.9440000000000004, 5.631351470947266e-05, 0.0006006774902343751, 0.0006006774902343751, 0.0006006774902343751, 0.18225000000000002, 0.18225000000000002, 0.00015016937255859378, 5.631351470947266e-05, 5.631351470947266e-05, 0.017085937500000002, 0.18225000000000002, 5.631351470947266e-05, 0.7290000000000001]  wait [2, 3, 2, 2, 2, 0, 0, 0, 3, 3, 0, 0, 3, 2]  inc [1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 2, 1, 1]  tol: 3
$$$$$$$$$$$$$ epoch  21  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -2239.315890789032 , diff:  2239.315890789032
adv train loss:  -2208.7021713256836 , diff:  30.61371946334839
adv train loss:  -1900.1934413909912 , diff:  308.5087299346924
adv train loss:  -1879.377594947815 , diff:  20.81584644317627
adv train loss:  -1895.5705299377441 , diff:  16.1929349899292
adv train loss:  -1896.681978225708 , diff:  1.1114482879638672
adv train loss:  -1885.5607089996338 , diff:  11.121269226074219
adv train loss:  -1876.1658430099487 , diff:  9.394865989685059
adv train loss:  -1864.9059085845947 , diff:  11.259934425354004
adv train loss:  -1877.1175603866577 , diff:  12.211651802062988
layer  0  adv train finish, try to retain  2
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  43099.27700805664
forward train acc: top1 ->  20.786000003051758 ; top5 ->  69.70000001953125  and loss:  1310.7317733764648
test acc: top1 ->  9.72 ; top5 ->  47.95  and loss:  10509.922813415527
forward train acc: top1 ->  21.587999997558594 ; top5 ->  73.79400000244141  and loss:  450.04138803482056
test acc: top1 ->  21.27 ; top5 ->  74.02  and loss:  336.9321880340576
forward train acc: top1 ->  20.39799999328613 ; top5 ->  72.94999998535157  and loss:  287.24925565719604
test acc: top1 ->  21.17 ; top5 ->  73.69  and loss:  257.9402859210968
forward train acc: top1 ->  21.507999994506836 ; top5 ->  74.21999999755859  and loss:  241.27421689033508
test acc: top1 ->  23.71 ; top5 ->  75.94  and loss:  231.38214576244354
forward train acc: top1 ->  22.645999996337892 ; top5 ->  76.12999997070312  and loss:  221.29542422294617
test acc: top1 ->  24.55 ; top5 ->  77.88  and loss:  215.98719763755798
forward train acc: top1 ->  24.008000004882813 ; top5 ->  77.41000000732421  and loss:  211.50310516357422
test acc: top1 ->  25.49 ; top5 ->  78.86  and loss:  211.2841054201126
forward train acc: top1 ->  24.529999995117187 ; top5 ->  77.95  and loss:  208.87019765377045
test acc: top1 ->  26.32 ; top5 ->  79.57  and loss:  208.69861447811127
forward train acc: top1 ->  25.138000002441405 ; top5 ->  78.796  and loss:  206.4785360097885
test acc: top1 ->  26.97 ; top5 ->  80.01  and loss:  204.52908492088318
forward train acc: top1 ->  25.547999992065428 ; top5 ->  79.42399997070312  and loss:  202.64972126483917
test acc: top1 ->  27.67 ; top5 ->  80.26  and loss:  200.96154940128326
forward train acc: top1 ->  26.176000003051758 ; top5 ->  79.16000000976562  and loss:  201.69798362255096
test acc: top1 ->  28.09 ; top5 ->  81.03  and loss:  199.80479657649994
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
### skip layer  1 wait:  3  ###
---------------- start layer  2  ---------------
adv train loss:  -80.96477448940277 , diff:  80.96477448940277
adv train loss:  -80.42624241113663 , diff:  0.5385320782661438
adv train loss:  -81.09319990873337 , diff:  0.6669574975967407
adv train loss:  -80.9815114736557 , diff:  0.11168843507766724
layer  2  adv train finish, try to retain  128
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -80.92183774709702 , diff:  80.92183774709702
adv train loss:  -80.9862277507782 , diff:  0.06439000368118286
layer  3  adv train finish, try to retain  126
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -80.5050031542778 , diff:  80.5050031542778
adv train loss:  -80.75989735126495 , diff:  0.2548941969871521
adv train loss:  -80.69244384765625 , diff:  0.06745350360870361
layer  4  adv train finish, try to retain  254
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -81.13069522380829 , diff:  81.13069522380829
adv train loss:  -80.57012152671814 , diff:  0.5605736970901489
adv train loss:  -80.89400923252106 , diff:  0.3238877058029175
adv train loss:  -80.8224071264267 , diff:  0.07160210609436035
layer  5  adv train finish, try to retain  256
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -80.7062155008316 , diff:  80.7062155008316
adv train loss:  -80.86327189207077 , diff:  0.15705639123916626
layer  6  adv train finish, try to retain  253
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -92.93588906526566 , diff:  92.93588906526566
adv train loss:  -92.81639176607132 , diff:  0.11949729919433594
layer  7  adv train finish, try to retain  473
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
### skip layer  8 wait:  3  ###
---------------- start layer  9  ---------------
### skip layer  9 wait:  3  ###
---------------- start layer  10  ---------------
adv train loss:  -72.98091787099838 , diff:  72.98091787099838
adv train loss:  -72.69112640619278 , diff:  0.289791464805603
adv train loss:  -72.36217331886292 , diff:  0.3289530873298645
adv train loss:  -73.2539513707161 , diff:  0.8917780518531799
adv train loss:  -72.67954671382904 , diff:  0.5744046568870544
adv train loss:  -72.87076705694199 , diff:  0.19122034311294556
adv train loss:  -72.52685642242432 , diff:  0.3439106345176697
adv train loss:  -72.8973777294159 , diff:  0.37052130699157715
adv train loss:  -72.70775681734085 , diff:  0.18962091207504272
adv train loss:  -72.82851326465607 , diff:  0.12075644731521606
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  20
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  392.819717168808
forward train acc: top1 ->  39.87399998779297 ; top5 ->  86.86  and loss:  247.71579015254974
test acc: top1 ->  28.67 ; top5 ->  76.13  and loss:  277.588561296463
forward train acc: top1 ->  88.88800000732422 ; top5 ->  99.804  and loss:  60.8552689999342
test acc: top1 ->  85.54 ; top5 ->  98.21  and loss:  54.82625496387482
forward train acc: top1 ->  99.496 ; top5 ->  99.992  and loss:  6.919806910678744
test acc: top1 ->  89.51 ; top5 ->  98.72  and loss:  45.70050773024559
forward train acc: top1 ->  99.736 ; top5 ->  99.998  and loss:  2.270718727260828
test acc: top1 ->  90.56 ; top5 ->  98.76  and loss:  44.846971079707146
forward train acc: top1 ->  99.808 ; top5 ->  99.996  and loss:  1.3491195063106716
test acc: top1 ->  91.16 ; top5 ->  98.7  and loss:  45.9147527590394
forward train acc: top1 ->  99.84 ; top5 ->  99.998  and loss:  1.0314867100678384
test acc: top1 ->  91.11 ; top5 ->  98.68  and loss:  47.26546382904053
forward train acc: top1 ->  99.844 ; top5 ->  99.998  and loss:  0.8220847542397678
test acc: top1 ->  91.16 ; top5 ->  98.71  and loss:  50.0419704541564
forward train acc: top1 ->  99.89 ; top5 ->  99.998  and loss:  0.6996343056671321
test acc: top1 ->  91.21 ; top5 ->  98.74  and loss:  49.52796324342489
forward train acc: top1 ->  99.87199997558594 ; top5 ->  99.994  and loss:  0.6669252514839172
test acc: top1 ->  91.38 ; top5 ->  98.78  and loss:  50.399403020739555
forward train acc: top1 ->  99.912 ; top5 ->  100.0  and loss:  0.5396069190464914
test acc: top1 ->  91.53 ; top5 ->  98.79  and loss:  50.48782154545188
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  22 / 512 , inc:  2
---------------- start layer  11  ---------------
adv train loss:  -145.94224452972412 , diff:  145.94224452972412
adv train loss:  -145.4279658794403 , diff:  0.5142786502838135
adv train loss:  -145.93156468868256 , diff:  0.5035988092422485
adv train loss:  -146.16527032852173 , diff:  0.23370563983917236
layer  11  adv train finish, try to retain  449
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
### skip layer  12 wait:  3  ###
---------------- start layer  13  ---------------
adv train loss:  -3189.192358016968 , diff:  3189.192358016968
adv train loss:  -3248.7832794189453 , diff:  59.59092140197754
adv train loss:  -3252.382942199707 , diff:  3.5996627807617188
layer  13  adv train finish, try to retain  492
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0  ==>  64 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  2
layer  6  :  0.5625  ==>  144 / 256 , inc:  2
layer  7  :  0.203125  ==>  104 / 512 , inc:  2
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.16796875  ==>  86 / 512 , inc:  2
layer  12  :  0  ==>  512 / 512 , inc:  1
layer  13  :  0.083984375  ==>  43 / 512 , inc:  1
eps [1.4580000000000002, 5.631351470947266e-05, 0.0012013549804687502, 0.0012013549804687502, 0.0012013549804687502, 0.36450000000000005, 0.36450000000000005, 0.00030033874511718755, 5.631351470947266e-05, 5.631351470947266e-05, 0.012814453125000002, 0.36450000000000005, 5.631351470947266e-05, 1.4580000000000002]  wait [4, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 0, 2, 2]  inc [1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1]  tol: 3
$$$$$$$$$$$$$ epoch  22  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
adv train loss:  -304.8010938167572 , diff:  304.8010938167572
adv train loss:  -304.246595621109 , diff:  0.5544981956481934
layer  1  adv train finish, try to retain  56
test acc: top1 ->  54.87 ; top5 ->  89.71  and loss:  320.0133924484253
forward train acc: top1 ->  96.476 ; top5 ->  99.668  and loss:  23.047649243846536
test acc: top1 ->  91.65 ; top5 ->  99.05  and loss:  33.65425372868776
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.5013588932342827
test acc: top1 ->  91.81 ; top5 ->  98.95  and loss:  39.48542072623968
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.22437898186035454
test acc: top1 ->  91.84 ; top5 ->  98.93  and loss:  42.8490027859807
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.14419822249328718
test acc: top1 ->  91.79 ; top5 ->  98.87  and loss:  45.586042329669
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.11531854179338552
test acc: top1 ->  91.81 ; top5 ->  98.9  and loss:  47.2794943228364
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.08121030670008622
test acc: top1 ->  91.96 ; top5 ->  98.91  and loss:  48.67737567424774
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.0702967592224013
test acc: top1 ->  91.81 ; top5 ->  98.86  and loss:  49.63397812843323
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.0810537513170857
test acc: top1 ->  91.78 ; top5 ->  98.79  and loss:  51.00731893628836
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.0717557954340009
test acc: top1 ->  91.75 ; top5 ->  98.86  and loss:  51.41349997371435
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.06055942671082448
test acc: top1 ->  91.88 ; top5 ->  98.82  and loss:  51.96384885907173
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  64 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -0.058310102082032245 , diff:  0.058310102082032245
adv train loss:  -0.058609538318705745 , diff:  0.0002994362366735004
layer  2  adv train finish, try to retain  127
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -0.06657476715918165 , diff:  0.06657476715918165
adv train loss:  -0.060810528229922056 , diff:  0.005764238929259591
layer  3  adv train finish, try to retain  128
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -0.06227416357432958 , diff:  0.06227416357432958
adv train loss:  -0.06193555405479856 , diff:  0.00033860951953101903
layer  4  adv train finish, try to retain  256
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -0.060508954513352364 , diff:  0.060508954513352364
adv train loss:  -0.06068689620587975 , diff:  0.00017794169252738357
layer  5  adv train finish, try to retain  255
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -0.06072563148336485 , diff:  0.06072563148336485
adv train loss:  -0.05621383438119665 , diff:  0.004511797102168202
layer  6  adv train finish, try to retain  255
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -0.10265595209784806 , diff:  0.10265595209784806
adv train loss:  -0.09292665286920965 , diff:  0.00972929922863841
layer  7  adv train finish, try to retain  375
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -0.06461598971509375 , diff:  0.06461598971509375
adv train loss:  -0.0595567665877752 , diff:  0.005059223127318546
layer  8  adv train finish, try to retain  53
test acc: top1 ->  77.05 ; top5 ->  98.09  and loss:  88.38158869743347
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.12570838051033206
test acc: top1 ->  91.71 ; top5 ->  98.54  and loss:  69.93090376257896
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.1056972969963681
test acc: top1 ->  91.67 ; top5 ->  98.5  and loss:  70.77791705727577
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.06567321365218959
test acc: top1 ->  91.78 ; top5 ->  98.65  and loss:  69.96685554087162
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.03141493330622325
test acc: top1 ->  91.7 ; top5 ->  98.71  and loss:  74.79929940402508
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.09352044659317471
test acc: top1 ->  91.82 ; top5 ->  98.61  and loss:  75.06319736689329
forward train acc: top1 ->  99.978 ; top5 ->  100.0  and loss:  0.05624289308798325
test acc: top1 ->  91.88 ; top5 ->  98.68  and loss:  73.76168179512024
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.037983513035214855
test acc: top1 ->  91.91 ; top5 ->  98.66  and loss:  75.03130139410496
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.04044728147891874
test acc: top1 ->  91.78 ; top5 ->  98.68  and loss:  75.77255803346634
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.04285621264989459
test acc: top1 ->  91.86 ; top5 ->  98.66  and loss:  75.71985487639904
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.060045728056138614
test acc: top1 ->  91.8 ; top5 ->  98.72  and loss:  75.95400243997574
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -0.14070844357047463 , diff:  0.14070844357047463
adv train loss:  -0.1557580760709243 , diff:  0.015049632500449661
adv train loss:  -0.14910688642703462 , diff:  0.006651189643889666
layer  9  adv train finish, try to retain  65
test acc: top1 ->  81.17 ; top5 ->  98.47  and loss:  76.21039804816246
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.026429757537698606
test acc: top1 ->  91.97 ; top5 ->  98.72  and loss:  70.99944554269314
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.04098307091589959
test acc: top1 ->  91.78 ; top5 ->  98.78  and loss:  74.14128053188324
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.01584003357174879
test acc: top1 ->  92.05 ; top5 ->  98.69  and loss:  76.28434513509274
forward train acc: top1 ->  99.98999997558593 ; top5 ->  100.0  and loss:  0.03354401147998942
test acc: top1 ->  91.95 ; top5 ->  98.89  and loss:  77.43893705308437
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.02794820581220847
test acc: top1 ->  91.91 ; top5 ->  98.82  and loss:  77.96407118439674
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.040190653191530146
test acc: top1 ->  91.85 ; top5 ->  98.77  and loss:  77.91451942920685
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.02254825023965168
test acc: top1 ->  92.02 ; top5 ->  98.89  and loss:  78.48098169267178
forward train acc: top1 ->  99.98599997558594 ; top5 ->  100.0  and loss:  0.04290047641734418
test acc: top1 ->  91.92 ; top5 ->  98.94  and loss:  77.84594422578812
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.023684677228175133
test acc: top1 ->  91.92 ; top5 ->  98.97  and loss:  76.78876794874668
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.011502879597173887
test acc: top1 ->  92.0 ; top5 ->  99.0  and loss:  77.29674719274044
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -0.04799035226460546 , diff:  0.04799035226460546
adv train loss:  -0.06732922243827488 , diff:  0.01933887017366942
adv train loss:  -0.0628501474402583 , diff:  0.004479074998016586
layer  10  adv train finish, try to retain  489
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -0.1427578021393856 , diff:  0.1427578021393856
adv train loss:  -0.06726762139260245 , diff:  0.07549018074678315
adv train loss:  -0.12270756426732987 , diff:  0.05543994287472742
adv train loss:  -0.06114739433542127 , diff:  0.061560169931908604
adv train loss:  -0.10395285434060497 , diff:  0.042805460005183704
adv train loss:  -0.1089407269532785 , diff:  0.004987872612673527
layer  11  adv train finish, try to retain  422
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -0.016460156304674456 , diff:  0.016460156304674456
adv train loss:  -0.01934742469302364 , diff:  0.0028872683883491845
layer  12  adv train finish, try to retain  30
test acc: top1 ->  91.85 ; top5 ->  99.18  and loss:  65.08652876317501
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03842922375770286
test acc: top1 ->  91.97 ; top5 ->  98.98  and loss:  70.62970879673958
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.014685261411614192
test acc: top1 ->  92.03 ; top5 ->  99.05  and loss:  74.07113391160965
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.014826351055035047
test acc: top1 ->  91.82 ; top5 ->  99.05  and loss:  76.13622090220451
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -4029.69734954834 , diff:  4029.69734954834
adv train loss:  -4822.439064025879 , diff:  792.7417144775391
adv train loss:  -5102.067543029785 , diff:  279.62847900390625
adv train loss:  -5118.038761138916 , diff:  15.97121810913086
adv train loss:  -5133.518337249756 , diff:  15.479576110839844
adv train loss:  -5158.055255889893 , diff:  24.53691864013672
adv train loss:  -5164.703701019287 , diff:  6.648445129394531
adv train loss:  -5164.087856292725 , diff:  0.6158447265625
layer  13  adv train finish, try to retain  100
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0  ==>  64 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  2
layer  6  :  0.5625  ==>  144 / 256 , inc:  2
layer  7  :  0.203125  ==>  104 / 512 , inc:  2
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.16796875  ==>  86 / 512 , inc:  2
layer  12  :  0  ==>  512 / 512 , inc:  1
layer  13  :  0.083984375  ==>  43 / 512 , inc:  1
eps [1.4580000000000002, 4.22351360321045e-05, 0.0024027099609375004, 0.0024027099609375004, 0.0024027099609375004, 0.7290000000000001, 0.7290000000000001, 0.0006006774902343751, 4.22351360321045e-05, 4.22351360321045e-05, 0.025628906250000003, 0.7290000000000001, 4.22351360321045e-05, 2.9160000000000004]  wait [3, 4, 2, 2, 2, 0, 0, 0, 4, 4, 2, 0, 4, 2]  inc [1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1]  tol: 3
$$$$$$$$$$$$$ epoch  23  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
### skip layer  1 wait:  4  ###
---------------- start layer  2  ---------------
adv train loss:  -0.012803266510672984 , diff:  0.012803266510672984
adv train loss:  -0.023103976833226625 , diff:  0.010300710322553641
adv train loss:  -0.014284911852200821 , diff:  0.008819064981025804
layer  2  adv train finish, try to retain  127
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -0.02357526971536572 , diff:  0.02357526971536572
adv train loss:  -0.03403670844045337 , diff:  0.010461438725087646
adv train loss:  -0.02351199976897078 , diff:  0.010524708671482585
adv train loss:  -0.030209853068754455 , diff:  0.006697853299783674
layer  3  adv train finish, try to retain  128
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -0.03171221814727687 , diff:  0.03171221814727687
adv train loss:  -0.009902384883389459 , diff:  0.021809833263887413
adv train loss:  -0.045323140445816534 , diff:  0.035420755562427075
adv train loss:  -0.050529491065390175 , diff:  0.005206350619573641
layer  4  adv train finish, try to retain  256
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -0.03702875534145278 , diff:  0.03702875534145278
adv train loss:  -0.034929008263134165 , diff:  0.0020997470783186145
layer  5  adv train finish, try to retain  251
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -0.018548328853285057 , diff:  0.018548328853285057
adv train loss:  -0.02334191574846045 , diff:  0.004793586895175395
layer  6  adv train finish, try to retain  254
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -0.0181752489511382 , diff:  0.0181752489511382
adv train loss:  -0.049064189468481345 , diff:  0.030888940517343144
adv train loss:  -0.059868033158636536 , diff:  0.01080384369015519
adv train loss:  -0.016675697404934908 , diff:  0.04319233575370163
adv train loss:  -0.042113745239475975 , diff:  0.025438047834541067
adv train loss:  -0.056497196522286686 , diff:  0.01438345128281071
adv train loss:  -0.042491984566368046 , diff:  0.01400521195591864
adv train loss:  -0.08958267132948095 , diff:  0.0470906867631129
adv train loss:  -0.03876741420572216 , diff:  0.050815257123758784
adv train loss:  -0.033444730723203975 , diff:  0.005322683482518187
layer  7  adv train finish, try to retain  393
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
### skip layer  8 wait:  4  ###
---------------- start layer  9  ---------------
### skip layer  9 wait:  4  ###
---------------- start layer  10  ---------------
adv train loss:  -0.1067144146454666 , diff:  0.1067144146454666
adv train loss:  -0.09223007144100848 , diff:  0.014484343204458128
adv train loss:  -0.07963738450598612 , diff:  0.012592686935022357
adv train loss:  -0.09687614441463666 , diff:  0.017238759908650536
adv train loss:  -0.09300189974237583 , diff:  0.0038742446722608292
layer  10  adv train finish, try to retain  490
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -0.10764238761476008 , diff:  0.10764238761476008
adv train loss:  -0.06593292625393588 , diff:  0.04170946136082421
adv train loss:  -0.0687820856264807 , diff:  0.0028491593725448183
layer  11  adv train finish, try to retain  426
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
adv train loss:  -12029.337409973145 , diff:  12029.337409973145
adv train loss:  -22259.254028320312 , diff:  10229.916618347168
adv train loss:  -24341.051788330078 , diff:  2081.7977600097656
adv train loss:  -24464.204162597656 , diff:  123.15237426757812
adv train loss:  -24481.026733398438 , diff:  16.82257080078125
adv train loss:  -24478.658279418945 , diff:  2.3684539794921875
adv train loss:  -24492.980834960938 , diff:  14.322555541992188
adv train loss:  -24507.665802001953 , diff:  14.684967041015625
adv train loss:  -24520.58497619629 , diff:  12.919174194335938
adv train loss:  -24508.25096130371 , diff:  12.334014892578125
layer  13  adv train finish, try to retain  93
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0  ==>  64 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  2
layer  6  :  0.5625  ==>  144 / 256 , inc:  2
layer  7  :  0.203125  ==>  104 / 512 , inc:  2
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.16796875  ==>  86 / 512 , inc:  2
layer  12  :  0  ==>  512 / 512 , inc:  1
layer  13  :  0.083984375  ==>  43 / 512 , inc:  1
eps [1.4580000000000002, 4.22351360321045e-05, 0.004805419921875001, 0.004805419921875001, 0.004805419921875001, 1.4580000000000002, 1.4580000000000002, 0.0012013549804687502, 4.22351360321045e-05, 4.22351360321045e-05, 0.051257812500000006, 1.4580000000000002, 4.22351360321045e-05, 5.832000000000001]  wait [2, 3, 2, 2, 2, 0, 0, 0, 3, 3, 2, 0, 3, 2]  inc [1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1]  tol: 3
$$$$$$$$$$$$$ epoch  24  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1646.5419590473175 , diff:  1646.5419590473175
adv train loss:  -1785.541395187378 , diff:  138.99943614006042
adv train loss:  -1785.780993461609 , diff:  0.23959827423095703
adv train loss:  -1798.6964254379272 , diff:  12.91543197631836
adv train loss:  -1784.787504196167 , diff:  13.908921241760254
adv train loss:  -1790.496109008789 , diff:  5.70860481262207
adv train loss:  -1799.0429821014404 , diff:  8.546873092651367
adv train loss:  -1760.3508195877075 , diff:  38.69216251373291
adv train loss:  -1162.938069343567 , diff:  597.4127502441406
adv train loss:  -1159.7730712890625 , diff:  3.1649980545043945
layer  0  adv train finish, try to retain  5
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  25651.62303161621
forward train acc: top1 ->  65.54800000244141 ; top5 ->  93.33200001708984  and loss:  203.08349812030792
test acc: top1 ->  51.75 ; top5 ->  86.44  and loss:  178.2460035085678
forward train acc: top1 ->  69.21199997070312 ; top5 ->  94.82199997314453  and loss:  100.48330694437027
test acc: top1 ->  70.29 ; top5 ->  95.45  and loss:  98.43968480825424
forward train acc: top1 ->  71.97799999023438 ; top5 ->  95.81600001464844  and loss:  88.52865809202194
test acc: top1 ->  72.27 ; top5 ->  95.89  and loss:  91.35952627658844
forward train acc: top1 ->  74.03000001708985 ; top5 ->  96.47800001220703  and loss:  81.50221383571625
test acc: top1 ->  73.69 ; top5 ->  96.26  and loss:  85.45353817939758
forward train acc: top1 ->  75.76600001708984 ; top5 ->  96.87199998291015  and loss:  75.3674789071083
test acc: top1 ->  75.07 ; top5 ->  96.51  and loss:  80.71415919065475
forward train acc: top1 ->  77.18599999511719 ; top5 ->  97.05999998535157  and loss:  70.9991706609726
test acc: top1 ->  75.64 ; top5 ->  96.74  and loss:  78.5799258351326
forward train acc: top1 ->  77.99599999023438 ; top5 ->  97.44199998535156  and loss:  67.99597144126892
test acc: top1 ->  76.37 ; top5 ->  96.89  and loss:  76.95698571205139
forward train acc: top1 ->  78.40799998046874 ; top5 ->  97.43599997802734  and loss:  66.9088534116745
test acc: top1 ->  76.55 ; top5 ->  96.95  and loss:  74.79619398713112
forward train acc: top1 ->  79.18199998291016 ; top5 ->  97.67799998779297  and loss:  64.6340502500534
test acc: top1 ->  77.14 ; top5 ->  97.25  and loss:  73.5417628288269
forward train acc: top1 ->  79.85400000976563 ; top5 ->  97.75800000732421  and loss:  62.27540189027786
test acc: top1 ->  77.61 ; top5 ->  97.26  and loss:  72.06992456316948
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  32 / 64 , inc:  1
---------------- start layer  1  ---------------
### skip layer  1 wait:  3  ###
---------------- start layer  2  ---------------
adv train loss:  -6.453305695205927 , diff:  6.453305695205927
adv train loss:  -6.446297813206911 , diff:  0.007007881999015808
layer  2  adv train finish, try to retain  128
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -6.470750343054533 , diff:  6.470750343054533
adv train loss:  -6.421820864081383 , diff:  0.04892947897315025
adv train loss:  -6.468530036509037 , diff:  0.046709172427654266
adv train loss:  -6.4133074432611465 , diff:  0.05522259324789047
adv train loss:  -6.438381757587194 , diff:  0.025074314326047897
adv train loss:  -6.4862901121377945 , diff:  0.04790835455060005
adv train loss:  -6.4594114646315575 , diff:  0.02687864750623703
adv train loss:  -6.423605106770992 , diff:  0.035806357860565186
adv train loss:  -6.396301340311766 , diff:  0.02730376645922661
adv train loss:  -6.436907820403576 , diff:  0.040606480091810226
layer  3  adv train finish, try to retain  128
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -6.439863242208958 , diff:  6.439863242208958
adv train loss:  -6.424411244690418 , diff:  0.015451997518539429
adv train loss:  -6.3685536831617355 , diff:  0.05585756152868271
adv train loss:  -6.433047875761986 , diff:  0.06449419260025024
adv train loss:  -6.4552440121769905 , diff:  0.02219613641500473
adv train loss:  -6.416705165058374 , diff:  0.038538847118616104
adv train loss:  -6.391676791012287 , diff:  0.025028374046087265
adv train loss:  -6.4828458949923515 , diff:  0.09116910398006439
adv train loss:  -6.4469990357756615 , diff:  0.035846859216690063
adv train loss:  -6.439031191170216 , diff:  0.007967844605445862
layer  4  adv train finish, try to retain  256
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -305.83363261446357 , diff:  305.83363261446357
adv train loss:  -1112.6491136550903 , diff:  806.8154810406268
adv train loss:  -1175.0707502365112 , diff:  62.4216365814209
adv train loss:  -1192.3864574432373 , diff:  17.315707206726074
adv train loss:  -1218.5333194732666 , diff:  26.146862030029297
adv train loss:  -1224.6583766937256 , diff:  6.125057220458984
adv train loss:  -1227.8227558135986 , diff:  3.164379119873047
adv train loss:  -1224.0791778564453 , diff:  3.7435779571533203
adv train loss:  -1200.3822135925293 , diff:  23.696964263916016
adv train loss:  -1184.811858177185 , diff:  15.570355415344238
layer  5  adv train finish, try to retain  50
test acc: top1 ->  20.15 ; top5 ->  60.42  and loss:  1793.0347623825073
forward train acc: top1 ->  86.93399997558593 ; top5 ->  99.32199997558594  and loss:  38.52123096585274
test acc: top1 ->  82.68 ; top5 ->  98.62  and loss:  57.110545217990875
forward train acc: top1 ->  90.86599998291015 ; top5 ->  99.63199997558594  and loss:  26.928503185510635
test acc: top1 ->  84.53 ; top5 ->  98.71  and loss:  53.08467039465904
forward train acc: top1 ->  92.3380000024414 ; top5 ->  99.74399997558594  and loss:  22.32392828166485
test acc: top1 ->  85.53 ; top5 ->  98.79  and loss:  50.72221238911152
forward train acc: top1 ->  93.30999997558594 ; top5 ->  99.808  and loss:  19.447933577001095
test acc: top1 ->  86.28 ; top5 ->  98.91  and loss:  49.04559734463692
forward train acc: top1 ->  93.83399997070312 ; top5 ->  99.846  and loss:  17.802718326449394
test acc: top1 ->  86.86 ; top5 ->  98.93  and loss:  48.53038823604584
forward train acc: top1 ->  94.21999997558594 ; top5 ->  99.848  and loss:  16.79788912087679
test acc: top1 ->  86.86 ; top5 ->  98.97  and loss:  48.03923198580742
forward train acc: top1 ->  94.55199999267577 ; top5 ->  99.852  and loss:  15.849416740238667
test acc: top1 ->  87.14 ; top5 ->  98.95  and loss:  47.85762260854244
forward train acc: top1 ->  94.75399999267579 ; top5 ->  99.878  and loss:  15.215290926396847
test acc: top1 ->  87.22 ; top5 ->  99.04  and loss:  47.59894897043705
forward train acc: top1 ->  94.80799999511719 ; top5 ->  99.88  and loss:  14.932765446603298
test acc: top1 ->  87.39 ; top5 ->  99.04  and loss:  47.35223324596882
forward train acc: top1 ->  94.96999997070313 ; top5 ->  99.892  and loss:  14.426389992237091
test acc: top1 ->  87.58 ; top5 ->  99.03  and loss:  46.92337340116501
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  2
---------------- start layer  6  ---------------
adv train loss:  -408.29169935174286 , diff:  408.29169935174286
adv train loss:  -1861.2797603607178 , diff:  1452.988061008975
adv train loss:  -1912.6609725952148 , diff:  51.38121223449707
adv train loss:  -1932.5498580932617 , diff:  19.888885498046875
adv train loss:  -1929.6542263031006 , diff:  2.895631790161133
adv train loss:  -1933.4760189056396 , diff:  3.8217926025390625
adv train loss:  -1932.4216327667236 , diff:  1.0543861389160156
adv train loss:  -1936.4951992034912 , diff:  4.073566436767578
adv train loss:  -1924.766710281372 , diff:  11.72848892211914
adv train loss:  -1927.9162139892578 , diff:  3.149503707885742
layer  6  adv train finish, try to retain  35
test acc: top1 ->  41.74 ; top5 ->  83.31  and loss:  913.6546730995178
forward train acc: top1 ->  95.52199998535156 ; top5 ->  99.92  and loss:  13.649165779352188
test acc: top1 ->  88.05 ; top5 ->  99.1  and loss:  46.686867117881775
forward train acc: top1 ->  96.66800001220703 ; top5 ->  99.974  and loss:  9.548895690590143
test acc: top1 ->  88.71 ; top5 ->  99.23  and loss:  46.94299377501011
forward train acc: top1 ->  97.24600000976562 ; top5 ->  99.97  and loss:  7.925750754773617
test acc: top1 ->  88.96 ; top5 ->  99.21  and loss:  47.36039309203625
forward train acc: top1 ->  97.59000001220703 ; top5 ->  99.98  and loss:  7.087391763925552
test acc: top1 ->  89.1 ; top5 ->  99.21  and loss:  46.54029056429863
forward train acc: top1 ->  97.79999998046875 ; top5 ->  99.99  and loss:  6.373146273195744
test acc: top1 ->  89.34 ; top5 ->  99.21  and loss:  48.56087298691273
forward train acc: top1 ->  98.03600000976563 ; top5 ->  99.998  and loss:  5.502211121842265
test acc: top1 ->  89.38 ; top5 ->  99.19  and loss:  47.695160418748856
forward train acc: top1 ->  98.11399998535157 ; top5 ->  99.996  and loss:  5.537632828578353
test acc: top1 ->  89.43 ; top5 ->  99.24  and loss:  47.630506470799446
forward train acc: top1 ->  98.13000000732421 ; top5 ->  99.998  and loss:  5.25677983276546
test acc: top1 ->  89.57 ; top5 ->  99.26  and loss:  47.73937150835991
forward train acc: top1 ->  98.31600000732422 ; top5 ->  99.996  and loss:  4.822085704654455
test acc: top1 ->  89.75 ; top5 ->  99.26  and loss:  48.314959958195686
forward train acc: top1 ->  98.33399998535157 ; top5 ->  99.984  and loss:  4.84242401085794
test acc: top1 ->  89.82 ; top5 ->  99.24  and loss:  47.749150186777115
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  144 / 256 , inc:  2
---------------- start layer  7  ---------------
adv train loss:  -1.5141255385242403 , diff:  1.5141255385242403
adv train loss:  -1.5262793679721653 , diff:  0.01215382944792509
adv train loss:  -1.436430494301021 , diff:  0.08984887367114425
adv train loss:  -1.4044496952556074 , diff:  0.03198079904541373
adv train loss:  -1.3939311914145947 , diff:  0.010518503841012716
adv train loss:  -1.517730672378093 , diff:  0.12379948096349835
adv train loss:  -1.4478238699957728 , diff:  0.06990680238232017
adv train loss:  -1.4906195783987641 , diff:  0.042795708402991295
adv train loss:  -1.3696021400392056 , diff:  0.12101743835955858
adv train loss:  -1.4336973174940795 , diff:  0.06409517745487392
layer  7  adv train finish, try to retain  395
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
### skip layer  8 wait:  3  ###
---------------- start layer  9  ---------------
### skip layer  9 wait:  3  ###
---------------- start layer  10  ---------------
adv train loss:  -1.5431450959295034 , diff:  1.5431450959295034
adv train loss:  -1.6491826511919498 , diff:  0.1060375552624464
adv train loss:  -1.6190864266827703 , diff:  0.030096224509179592
adv train loss:  -1.4714575437828898 , diff:  0.1476288828998804
adv train loss:  -1.6168233435600996 , diff:  0.14536579977720976
adv train loss:  -1.667973249219358 , diff:  0.051149905659258366
adv train loss:  -1.6308916830457747 , diff:  0.03708156617358327
adv train loss:  -1.6371616208925843 , diff:  0.006269937846809626
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  21
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  910.8541350364685
forward train acc: top1 ->  10.206000001831054 ; top5 ->  50.04600000854492  and loss:  355.20117712020874
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  508.0907955169678
forward train acc: top1 ->  9.819999999084473 ; top5 ->  49.9  and loss:  225.9168140888214
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  232.8481662273407
forward train acc: top1 ->  10.247999999694825 ; top5 ->  50.227999986572264  and loss:  226.0373077392578
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.84747052192688
forward train acc: top1 ->  9.729999998168946 ; top5 ->  49.94399998901367  and loss:  226.02570295333862
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.9769411087036
forward train acc: top1 ->  9.990000001220704 ; top5 ->  49.762  and loss:  226.10131216049194
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  231.61368703842163
forward train acc: top1 ->  10.079999999084473 ; top5 ->  49.97799998535156  and loss:  225.94427514076233
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  232.95707607269287
forward train acc: top1 ->  9.949999999694825 ; top5 ->  50.081999998779295  and loss:  225.9469187259674
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  231.40369391441345
forward train acc: top1 ->  9.935999999389649 ; top5 ->  49.890000008544924  and loss:  225.92330169677734
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.508691072464
forward train acc: top1 ->  9.729999999084473 ; top5 ->  49.84599999267578  and loss:  225.91328310966492
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.84445667266846
forward train acc: top1 ->  10.070000000915527 ; top5 ->  49.906000001220704  and loss:  226.02815628051758
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.8580446243286
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  22 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -25.36651538312435 , diff:  25.36651538312435
adv train loss:  -27.93416130542755 , diff:  2.5676459223031998
adv train loss:  -30.19509407877922 , diff:  2.2609327733516693
adv train loss:  -31.832717090845108 , diff:  1.6376230120658875
adv train loss:  -32.18242126703262 , diff:  0.34970417618751526
adv train loss:  -32.10722288489342 , diff:  0.07519838213920593
adv train loss:  -35.539885491132736 , diff:  3.432662606239319
adv train loss:  -35.77504551410675 , diff:  0.23516002297401428
adv train loss:  -35.73488211631775 , diff:  0.040163397789001465
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  84
test acc: top1 ->  21.82 ; top5 ->  56.93  and loss:  13595459.09375
forward train acc: top1 ->  94.718 ; top5 ->  99.966  and loss:  25.501702835783362
test acc: top1 ->  90.54 ; top5 ->  98.43  and loss:  67.6432958766818
forward train acc: top1 ->  99.54799997558594 ; top5 ->  99.996  and loss:  2.073684356175363
test acc: top1 ->  91.29 ; top5 ->  98.47  and loss:  64.0388153642416
forward train acc: top1 ->  99.7780000024414 ; top5 ->  100.0  and loss:  1.0417489409446716
test acc: top1 ->  91.5 ; top5 ->  98.53  and loss:  63.0543157979846
forward train acc: top1 ->  99.84799997558594 ; top5 ->  100.0  and loss:  0.6507389363832772
test acc: top1 ->  91.66 ; top5 ->  98.56  and loss:  62.55602145195007
forward train acc: top1 ->  99.88999997558594 ; top5 ->  100.0  and loss:  0.5186084653250873
test acc: top1 ->  91.82 ; top5 ->  98.56  and loss:  61.873985193669796
forward train acc: top1 ->  99.902 ; top5 ->  100.0  and loss:  0.4173687675502151
test acc: top1 ->  91.86 ; top5 ->  98.57  and loss:  62.35806430131197
forward train acc: top1 ->  99.902 ; top5 ->  100.0  and loss:  0.3569926463533193
test acc: top1 ->  91.87 ; top5 ->  98.55  and loss:  62.56365706771612
forward train acc: top1 ->  99.906 ; top5 ->  100.0  and loss:  0.37723098532296717
test acc: top1 ->  91.91 ; top5 ->  98.65  and loss:  62.049678690731525
forward train acc: top1 ->  99.924 ; top5 ->  100.0  and loss:  0.3011376978829503
test acc: top1 ->  91.88 ; top5 ->  98.62  and loss:  62.91132755577564
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.23910553159657866
test acc: top1 ->  91.96 ; top5 ->  98.61  and loss:  62.53953582048416
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  86 / 512 , inc:  2
---------------- start layer  12  ---------------
### skip layer  12 wait:  3  ###
---------------- start layer  13  ---------------
adv train loss:  -5369.649066925049 , diff:  5369.649066925049
adv train loss:  -9325.400482177734 , diff:  3955.7514152526855
adv train loss:  -15438.797424316406 , diff:  6113.396942138672
adv train loss:  -28625.327239990234 , diff:  13186.529815673828
adv train loss:  -41267.4235534668 , diff:  12642.096313476562
adv train loss:  -46872.93811035156 , diff:  5605.514556884766
adv train loss:  -49925.33792114258 , diff:  3052.3998107910156
adv train loss:  -52288.78826904297 , diff:  2363.4503479003906
adv train loss:  -52903.60922241211 , diff:  614.8209533691406
adv train loss:  -53254.95654296875 , diff:  351.3473205566406
layer  13  adv train finish, try to retain  77
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0  ==>  64 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  2
layer  8  :  0  ==>  512 / 512 , inc:  1
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.16796875  ==>  86 / 512 , inc:  1
layer  12  :  0  ==>  512 / 512 , inc:  1
layer  13  :  0.083984375  ==>  43 / 512 , inc:  1
eps [1.0935000000000001, 4.22351360321045e-05, 0.009610839843750002, 0.009610839843750002, 0.009610839843750002, 1.0935000000000001, 1.0935000000000001, 0.0024027099609375004, 4.22351360321045e-05, 4.22351360321045e-05, 0.03844335937500001, 1.0935000000000001, 4.22351360321045e-05, 11.664000000000001]  wait [4, 2, 2, 2, 2, 2, 2, 0, 2, 2, 4, 2, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1]  tol: 3
$$$$$$$$$$$$$ epoch  25  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
adv train loss:  -2.720839756540954 , diff:  2.720839756540954
adv train loss:  -2.694501619786024 , diff:  0.02633813675493002
adv train loss:  -2.8280357364565134 , diff:  0.1335341166704893
adv train loss:  -2.9530205335468054 , diff:  0.12498479709029198
adv train loss:  -2.7545080427080393 , diff:  0.1985124908387661
adv train loss:  -2.7023392524570227 , diff:  0.05216879025101662
adv train loss:  -2.803865062072873 , diff:  0.10152580961585045
adv train loss:  -2.7062601284123957 , diff:  0.0976049336604774
adv train loss:  -2.646879526786506 , diff:  0.05938060162588954
adv train loss:  -2.8501110542565584 , diff:  0.20323152747005224
layer  1  adv train finish, try to retain  56
test acc: top1 ->  90.32 ; top5 ->  99.16  and loss:  50.778750240802765
forward train acc: top1 ->  99.916 ; top5 ->  100.0  and loss:  0.26357221136277076
test acc: top1 ->  92.02 ; top5 ->  99.21  and loss:  54.04585790634155
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.0900841894072073
test acc: top1 ->  92.01 ; top5 ->  99.21  and loss:  57.605593889951706
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.0700865692269872
test acc: top1 ->  92.05 ; top5 ->  99.21  and loss:  60.82827592641115
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.05872495508810971
test acc: top1 ->  91.97 ; top5 ->  99.16  and loss:  63.53371299058199
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.05502358399644436
test acc: top1 ->  92.05 ; top5 ->  99.23  and loss:  64.95495001971722
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.05248786617198675
test acc: top1 ->  92.06 ; top5 ->  99.22  and loss:  66.51691550761461
forward train acc: top1 ->  99.996 ; top5 ->  100.0  and loss:  0.0315179081981114
test acc: top1 ->  92.02 ; top5 ->  99.24  and loss:  66.25302433222532
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.048853047330339905
test acc: top1 ->  92.11 ; top5 ->  99.22  and loss:  66.9587242603302
==> this epoch:  56 / 64
---------------- start layer  2  ---------------
adv train loss:  -0.05754611203883542 , diff:  0.05754611203883542
adv train loss:  -0.03280385684956855 , diff:  0.02474225518926687
adv train loss:  -0.03730719580562436 , diff:  0.0045033389560558135
layer  2  adv train finish, try to retain  128
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -0.06281778011543793 , diff:  0.06281778011543793
adv train loss:  -0.04573196800902224 , diff:  0.017085812106415688
adv train loss:  -0.04137540948067908 , diff:  0.004356558528343157
layer  3  adv train finish, try to retain  128
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -0.0404859204109016 , diff:  0.0404859204109016
adv train loss:  -0.02680311241056188 , diff:  0.013682808000339719
adv train loss:  -0.054440061934656114 , diff:  0.027636949524094234
adv train loss:  -0.031336464584455825 , diff:  0.02310359735020029
adv train loss:  -0.057447208570010844 , diff:  0.02611074398555502
adv train loss:  -0.044617658295464935 , diff:  0.012829550274545909
adv train loss:  -0.02918985917676764 , diff:  0.015427799118697294
adv train loss:  -0.04336220305231109 , diff:  0.014172343875543447
adv train loss:  -0.029766775091957243 , diff:  0.013595427960353845
adv train loss:  -0.03131255590369619 , diff:  0.0015457808117389504
layer  4  adv train finish, try to retain  255
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -1020.1875053929443 , diff:  1020.1875053929443
adv train loss:  -2768.690299987793 , diff:  1748.5027945948486
adv train loss:  -2843.868881225586 , diff:  75.17858123779297
adv train loss:  -2853.825786590576 , diff:  9.956905364990234
adv train loss:  -2862.8231716156006 , diff:  8.997385025024414
adv train loss:  -2885.3323879241943 , diff:  22.50921630859375
adv train loss:  -2902.1949825286865 , diff:  16.862594604492188
adv train loss:  -2908.762533187866 , diff:  6.5675506591796875
adv train loss:  -2900.7923374176025 , diff:  7.970195770263672
adv train loss:  -2916.9840240478516 , diff:  16.191686630249023
layer  5  adv train finish, try to retain  226
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -828.7563931093027 , diff:  828.7563931093027
adv train loss:  -2869.377384185791 , diff:  2040.6209910764883
adv train loss:  -2942.7797107696533 , diff:  73.4023265838623
adv train loss:  -2961.2401580810547 , diff:  18.460447311401367
adv train loss:  -2959.837158203125 , diff:  1.4029998779296875
adv train loss:  -2987.573154449463 , diff:  27.73599624633789
adv train loss:  -3017.1025371551514 , diff:  29.529382705688477
adv train loss:  -3010.8432292938232 , diff:  6.259307861328125
adv train loss:  -3010.1980056762695 , diff:  0.6452236175537109
adv train loss:  -3011.827533721924 , diff:  1.6295280456542969
layer  6  adv train finish, try to retain  229
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -0.0822488193934987 , diff:  0.0822488193934987
adv train loss:  -0.08548142788276891 , diff:  0.0032326084892702056
layer  7  adv train finish, try to retain  387
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -0.033808879232310574 , diff:  0.033808879232310574
adv train loss:  -0.046789777759840945 , diff:  0.012980898527530371
adv train loss:  -0.048149316004128195 , diff:  0.0013595382442872506
layer  8  adv train finish, try to retain  73
test acc: top1 ->  88.67 ; top5 ->  99.01  and loss:  56.6531720161438
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.11745854193213745
test acc: top1 ->  92.01 ; top5 ->  99.07  and loss:  73.96181322261691
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.05991493442343199
test acc: top1 ->  92.03 ; top5 ->  99.0  and loss:  78.0800383836031
forward train acc: top1 ->  99.97399997558594 ; top5 ->  100.0  and loss:  0.07590847143364954
test acc: top1 ->  92.01 ; top5 ->  99.08  and loss:  74.62232872843742
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.08167158295327681
test acc: top1 ->  92.02 ; top5 ->  99.01  and loss:  75.51424895226955
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.05569368246506201
test acc: top1 ->  92.06 ; top5 ->  99.09  and loss:  74.59210906922817
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.04140321047634643
test acc: top1 ->  92.13 ; top5 ->  99.08  and loss:  77.59320156276226
==> this epoch:  73 / 512
---------------- start layer  9  ---------------
adv train loss:  -0.03939725342070233 , diff:  0.03939725342070233
adv train loss:  -0.042603905605574255 , diff:  0.0032066521848719276
layer  9  adv train finish, try to retain  55
test acc: top1 ->  70.95 ; top5 ->  98.28  and loss:  162.07892775535583
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.06678032451600302
test acc: top1 ->  92.02 ; top5 ->  99.03  and loss:  76.8098089247942
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.03953513513260987
test acc: top1 ->  91.91 ; top5 ->  99.1  and loss:  78.60617759823799
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.055891478411922435
test acc: top1 ->  91.88 ; top5 ->  99.04  and loss:  81.81089594215155
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.06374748535563413
test acc: top1 ->  92.03 ; top5 ->  99.01  and loss:  80.44585065543652
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.03896896123296756
test acc: top1 ->  92.04 ; top5 ->  99.07  and loss:  82.58197452872992
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.0322602301112056
test acc: top1 ->  92.07 ; top5 ->  99.08  and loss:  81.00650887191296
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.03156095051463126
test acc: top1 ->  91.93 ; top5 ->  99.05  and loss:  81.69552229344845
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  512 / 512 , inc:  1
---------------- start layer  10  ---------------
### skip layer  10 wait:  4  ###
---------------- start layer  11  ---------------
adv train loss:  -0.13222161098383367 , diff:  0.13222161098383367
adv train loss:  -0.09672578531171894 , diff:  0.035495825672114734
adv train loss:  -0.11931542330239608 , diff:  0.022589637990677147
adv train loss:  -0.12368582256749505 , diff:  0.004370399265098968
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  85
test acc: top1 ->  29.52 ; top5 ->  55.76  and loss:  13316761.765625
forward train acc: top1 ->  96.24199997558594 ; top5 ->  100.0  and loss:  17.428905859589577
test acc: top1 ->  91.35 ; top5 ->  98.87  and loss:  72.79419255256653
forward train acc: top1 ->  99.85999997558594 ; top5 ->  100.0  and loss:  0.9804740799590945
test acc: top1 ->  91.59 ; top5 ->  98.9  and loss:  71.26073549687862
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.45913662668317556
test acc: top1 ->  91.69 ; top5 ->  98.86  and loss:  70.70289941877127
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.32173812889959663
test acc: top1 ->  91.71 ; top5 ->  98.93  and loss:  71.18644408136606
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.20425079282722436
test acc: top1 ->  91.81 ; top5 ->  98.94  and loss:  70.85975293070078
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.17291395826032385
test acc: top1 ->  91.77 ; top5 ->  98.95  and loss:  71.3517944291234
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.1582737280987203
test acc: top1 ->  91.92 ; top5 ->  98.95  and loss:  71.38624616712332
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.14764896559063345
test acc: top1 ->  91.92 ; top5 ->  98.92  and loss:  72.26644665747881
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.12629665195709094
test acc: top1 ->  91.95 ; top5 ->  98.93  and loss:  72.55412367731333
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.1308193346048938
test acc: top1 ->  92.0 ; top5 ->  98.92  and loss:  72.39564277231693
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  86 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -0.1998552694749378 , diff:  0.1998552694749378
adv train loss:  -0.17749225418447168 , diff:  0.022363015290466137
adv train loss:  -0.15078118036035448 , diff:  0.026711073824117193
adv train loss:  -0.18560553198039997 , diff:  0.03482435162004549
adv train loss:  -0.19494281627703458 , diff:  0.009337284296634607
layer  12  adv train finish, try to retain  41
test acc: top1 ->  92.07 ; top5 ->  99.12  and loss:  88.12148536741734
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.043350296821699885
test acc: top1 ->  92.01 ; top5 ->  99.16  and loss:  84.59169924259186
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.030186871385012637
test acc: top1 ->  91.95 ; top5 ->  99.11  and loss:  84.16186761856079
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.04084106585014524
test acc: top1 ->  91.92 ; top5 ->  99.11  and loss:  86.62363724410534
forward train acc: top1 ->  99.99 ; top5 ->  100.0  and loss:  0.028427623156858317
test acc: top1 ->  92.07 ; top5 ->  99.09  and loss:  87.52141304314137
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.044196883085533045
test acc: top1 ->  91.96 ; top5 ->  99.15  and loss:  85.45835331082344
forward train acc: top1 ->  99.992 ; top5 ->  100.0  and loss:  0.018255526505527087
test acc: top1 ->  91.93 ; top5 ->  99.12  and loss:  86.4136975556612
forward train acc: top1 ->  99.998 ; top5 ->  100.0  and loss:  0.010680863995276013
test acc: top1 ->  91.99 ; top5 ->  99.06  and loss:  85.99371954798698
forward train acc: top1 ->  99.986 ; top5 ->  100.0  and loss:  0.024509262779247365
test acc: top1 ->  92.02 ; top5 ->  99.05  and loss:  86.80276399105787
forward train acc: top1 ->  99.988 ; top5 ->  100.0  and loss:  0.02415227598976344
test acc: top1 ->  92.12 ; top5 ->  99.05  and loss:  86.00620664656162
==> this epoch:  41 / 512
---------------- start layer  13  ---------------
adv train loss:  -3769.453042984009 , diff:  3769.453042984009
adv train loss:  -7004.452507019043 , diff:  3234.999464035034
adv train loss:  -13356.416374206543 , diff:  6351.9638671875
adv train loss:  -29009.51545715332 , diff:  15653.099082946777
adv train loss:  -42343.86022949219 , diff:  13334.344772338867
adv train loss:  -53238.14273071289 , diff:  10894.282501220703
adv train loss:  -62398.04431152344 , diff:  9159.901580810547
adv train loss:  -69947.81030273438 , diff:  7549.7659912109375
adv train loss:  -75319.9521484375 , diff:  5372.141845703125
adv train loss:  -79053.40051269531 , diff:  3733.4483642578125
layer  13  adv train finish, try to retain  24
test acc: top1 ->  69.55 ; top5 ->  98.38  and loss:  277.59999883174896
forward train acc: top1 ->  97.032 ; top5 ->  100.0  and loss:  18.057984340703115
test acc: top1 ->  91.57 ; top5 ->  98.26  and loss:  67.34611749649048
forward train acc: top1 ->  99.894 ; top5 ->  100.0  and loss:  0.46833318064454943
test acc: top1 ->  91.72 ; top5 ->  98.38  and loss:  65.29257528483868
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.20449291070690379
test acc: top1 ->  91.84 ; top5 ->  98.34  and loss:  65.2179312556982
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.16996581119019538
test acc: top1 ->  91.8 ; top5 ->  98.42  and loss:  65.53043306618929
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.13481569473515265
test acc: top1 ->  91.84 ; top5 ->  98.38  and loss:  65.85216111689806
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.11830412410199642
test acc: top1 ->  91.93 ; top5 ->  98.36  and loss:  65.91672383993864
forward train acc: top1 ->  99.98 ; top5 ->  100.0  and loss:  0.10194113178295083
test acc: top1 ->  91.83 ; top5 ->  98.41  and loss:  65.94736142456532
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.09478129919443745
test acc: top1 ->  91.89 ; top5 ->  98.43  and loss:  66.64620645344257
forward train acc: top1 ->  99.994 ; top5 ->  100.0  and loss:  0.07097732655529398
test acc: top1 ->  91.87 ; top5 ->  98.4  and loss:  66.28064754605293
forward train acc: top1 ->  99.984 ; top5 ->  100.0  and loss:  0.09860427143576089
test acc: top1 ->  91.92 ; top5 ->  98.46  and loss:  66.6118471994996
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  43 / 512 , inc:  1
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.875  ==>  56 / 64 , inc:  2
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  2
layer  8  :  0.142578125  ==>  73 / 512 , inc:  2
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.16796875  ==>  86 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  2
layer  13  :  0.083984375  ==>  43 / 512 , inc:  1
eps [1.0935000000000001, 4.22351360321045e-05, 0.019221679687500003, 0.019221679687500003, 0.019221679687500003, 2.1870000000000003, 2.1870000000000003, 0.004805419921875001, 4.22351360321045e-05, 3.167635202407837e-05, 0.03844335937500001, 0.8201250000000001, 4.22351360321045e-05, 8.748000000000001]  wait [3, 0, 2, 2, 2, 2, 2, 0, 0, 4, 3, 4, 0, 4]  inc [1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1]  tol: 3
$$$$$$$$$$$$$ epoch  26  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
adv train loss:  -0.20211131056566956 , diff:  0.20211131056566956
adv train loss:  -0.19642101280624047 , diff:  0.005690297759429086
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -0.24354408627186785 , diff:  0.24354408627186785
adv train loss:  -0.21833285953835002 , diff:  0.025211226733517833
adv train loss:  -0.21935859066798002 , diff:  0.0010257311296300031
************ all values are small in this layer **********
layer  2  adv train finish, try to retain  99
test acc: top1 ->  75.67 ; top5 ->  95.37  and loss:  287.13480949401855
forward train acc: top1 ->  99.3920000024414 ; top5 ->  99.986  and loss:  2.8736066177953035
test acc: top1 ->  90.79 ; top5 ->  99.02  and loss:  89.06516614556313
forward train acc: top1 ->  99.50999997558594 ; top5 ->  99.996  and loss:  1.9013850769260898
test acc: top1 ->  90.93 ; top5 ->  99.06  and loss:  77.50512300431728
forward train acc: top1 ->  99.5820000024414 ; top5 ->  99.994  and loss:  1.4165279838489369
test acc: top1 ->  90.74 ; top5 ->  99.12  and loss:  70.26815795898438
forward train acc: top1 ->  99.58199997558594 ; top5 ->  99.996  and loss:  1.2729699870105833
test acc: top1 ->  90.88 ; top5 ->  99.18  and loss:  65.91465225815773
forward train acc: top1 ->  99.66999997558594 ; top5 ->  100.0  and loss:  0.9933581142104231
test acc: top1 ->  91.04 ; top5 ->  99.16  and loss:  64.46990418434143
forward train acc: top1 ->  99.718 ; top5 ->  99.996  and loss:  0.8414624032448046
test acc: top1 ->  91.07 ; top5 ->  99.12  and loss:  64.87161563336849
forward train acc: top1 ->  99.72600000244141 ; top5 ->  100.0  and loss:  0.86500326715759
test acc: top1 ->  91.1 ; top5 ->  99.12  and loss:  63.60262617468834
forward train acc: top1 ->  99.7060000024414 ; top5 ->  99.998  and loss:  0.870409738534363
test acc: top1 ->  90.96 ; top5 ->  99.14  and loss:  62.42821127176285
forward train acc: top1 ->  99.7700000024414 ; top5 ->  100.0  and loss:  0.7347248191945255
test acc: top1 ->  91.13 ; top5 ->  99.17  and loss:  62.529405295848846
forward train acc: top1 ->  99.7580000024414 ; top5 ->  99.998  and loss:  0.6684730489505455
test acc: top1 ->  91.18 ; top5 ->  99.2  and loss:  62.81824243068695
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -0.047849876591499196 , diff:  0.047849876591499196
adv train loss:  -0.07107354507388663 , diff:  0.02322366848238744
adv train loss:  -0.05480321583308978 , diff:  0.016270329240796855
adv train loss:  -0.07785636511107441 , diff:  0.02305314927798463
adv train loss:  -0.052956139770685695 , diff:  0.024900225340388715
adv train loss:  -0.08529080988955684 , diff:  0.032334670118871145
adv train loss:  -0.07305845173686976 , diff:  0.012232358152687084
adv train loss:  -0.079810057861323 , diff:  0.006751606124453247
************ all values are small in this layer **********
layer  3  adv train finish, try to retain  86
test acc: top1 ->  82.41 ; top5 ->  97.46  and loss:  133.46198868751526
forward train acc: top1 ->  98.13800000732422 ; top5 ->  99.966  and loss:  6.071587907150388
test acc: top1 ->  89.54 ; top5 ->  99.16  and loss:  53.80853904783726
forward train acc: top1 ->  98.57800000488281 ; top5 ->  99.982  and loss:  4.388606123626232
test acc: top1 ->  89.81 ; top5 ->  99.14  and loss:  50.11164033412933
forward train acc: top1 ->  98.79000000488281 ; top5 ->  99.99  and loss:  3.624174197204411
test acc: top1 ->  90.12 ; top5 ->  99.13  and loss:  49.60963752865791
forward train acc: top1 ->  98.93400000244141 ; top5 ->  99.99  and loss:  3.1190129118040204
test acc: top1 ->  90.09 ; top5 ->  99.18  and loss:  49.58988404273987
forward train acc: top1 ->  99.0720000024414 ; top5 ->  99.992  and loss:  2.847821891773492
test acc: top1 ->  90.29 ; top5 ->  99.17  and loss:  49.27253481000662
forward train acc: top1 ->  99.14600000488281 ; top5 ->  99.998  and loss:  2.515673330053687
test acc: top1 ->  90.33 ; top5 ->  99.18  and loss:  50.246068492531776
forward train acc: top1 ->  99.17 ; top5 ->  99.996  and loss:  2.4559094577562064
test acc: top1 ->  90.36 ; top5 ->  99.16  and loss:  50.27363624423742
forward train acc: top1 ->  99.2420000024414 ; top5 ->  99.996  and loss:  2.314931537490338
test acc: top1 ->  90.42 ; top5 ->  99.16  and loss:  49.86098611354828
forward train acc: top1 ->  99.25799997558593 ; top5 ->  99.994  and loss:  2.1759634097106755
test acc: top1 ->  90.42 ; top5 ->  99.13  and loss:  50.41891074180603
forward train acc: top1 ->  99.29399997802734 ; top5 ->  99.994  and loss:  2.0984316812828183
test acc: top1 ->  90.42 ; top5 ->  99.17  and loss:  50.823877438902855
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -0.27145171869779006 , diff:  0.27145171869779006
adv train loss:  -0.300384523201501 , diff:  0.02893280450371094
adv train loss:  -0.29133094678400084 , diff:  0.009053576417500153
************ all values are small in this layer **********
layer  4  adv train finish, try to retain  155
test acc: top1 ->  49.89 ; top5 ->  80.54  and loss:  264.4973769187927
forward train acc: top1 ->  94.90600001708984 ; top5 ->  99.85  and loss:  16.30907190591097
test acc: top1 ->  88.0 ; top5 ->  98.97  and loss:  50.64324803650379
forward train acc: top1 ->  95.73199999511719 ; top5 ->  99.926  and loss:  12.58731185644865
test acc: top1 ->  88.47 ; top5 ->  99.05  and loss:  47.607493460178375
forward train acc: top1 ->  96.42600000732422 ; top5 ->  99.926  and loss:  10.69330033659935
test acc: top1 ->  88.82 ; top5 ->  99.12  and loss:  46.26907791942358
forward train acc: top1 ->  96.51800001220703 ; top5 ->  99.95  and loss:  10.052675176411867
test acc: top1 ->  89.21 ; top5 ->  99.15  and loss:  46.31937201321125
forward train acc: top1 ->  96.87999998291015 ; top5 ->  99.964  and loss:  8.994663137942553
test acc: top1 ->  89.28 ; top5 ->  99.15  and loss:  45.35902713984251
forward train acc: top1 ->  96.99999998046874 ; top5 ->  99.992  and loss:  8.39780743047595
test acc: top1 ->  89.44 ; top5 ->  99.16  and loss:  45.54364217072725
forward train acc: top1 ->  97.17600000976563 ; top5 ->  99.966  and loss:  8.224288076162338
test acc: top1 ->  89.32 ; top5 ->  99.17  and loss:  45.391412034630775
forward train acc: top1 ->  97.20799998779297 ; top5 ->  99.956  and loss:  8.007811587303877
test acc: top1 ->  89.42 ; top5 ->  99.15  and loss:  45.570062935352325
forward train acc: top1 ->  97.22399998535157 ; top5 ->  99.976  and loss:  7.793653275817633
test acc: top1 ->  89.4 ; top5 ->  99.16  and loss:  44.94444026052952
forward train acc: top1 ->  97.37599998779297 ; top5 ->  99.968  and loss:  7.689239412546158
test acc: top1 ->  89.65 ; top5 ->  99.19  and loss:  44.75577552616596
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  156 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -522.0739927508403 , diff:  522.0739927508403
adv train loss:  -1945.3137378692627 , diff:  1423.2397451184224
adv train loss:  -2066.4649200439453 , diff:  121.15118217468262
adv train loss:  -2147.5610733032227 , diff:  81.09615325927734
adv train loss:  -2164.6224403381348 , diff:  17.06136703491211
adv train loss:  -2155.5947608947754 , diff:  9.027679443359375
adv train loss:  -2160.5552291870117 , diff:  4.960468292236328
adv train loss:  -2163.922201156616 , diff:  3.366971969604492
adv train loss:  -2197.610990524292 , diff:  33.68878936767578
adv train loss:  -2224.0530891418457 , diff:  26.44209861755371
layer  5  adv train finish, try to retain  8
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  4608.144687652588
forward train acc: top1 ->  36.61399999633789 ; top5 ->  79.73599998046875  and loss:  285.56760907173157
test acc: top1 ->  41.24 ; top5 ->  85.8  and loss:  178.09046506881714
forward train acc: top1 ->  44.23800000366211 ; top5 ->  87.27399999023437  and loss:  156.342426776886
test acc: top1 ->  47.59 ; top5 ->  90.35  and loss:  150.45762312412262
forward train acc: top1 ->  49.613999990234376 ; top5 ->  90.68799998535157  and loss:  139.48560738563538
test acc: top1 ->  51.78 ; top5 ->  92.45  and loss:  136.37854504585266
forward train acc: top1 ->  54.38799998901367 ; top5 ->  93.02399997314453  and loss:  125.64215815067291
test acc: top1 ->  55.82 ; top5 ->  93.98  and loss:  124.82419818639755
forward train acc: top1 ->  57.74599998413086 ; top5 ->  94.56200001464843  and loss:  115.36284220218658
test acc: top1 ->  58.57 ; top5 ->  94.92  and loss:  116.56357049942017
forward train acc: top1 ->  59.72199999389648 ; top5 ->  95.29800001708985  and loss:  109.65925860404968
test acc: top1 ->  60.04 ; top5 ->  95.3  and loss:  113.09119272232056
forward train acc: top1 ->  60.564000001220705 ; top5 ->  95.43799998779296  and loss:  106.76014798879623
test acc: top1 ->  60.76 ; top5 ->  95.49  and loss:  110.4882362484932
forward train acc: top1 ->  61.714000001220704 ; top5 ->  96.0339999975586  and loss:  103.25472384691238
test acc: top1 ->  61.94 ; top5 ->  95.87  and loss:  107.46339583396912
forward train acc: top1 ->  63.2119999987793 ; top5 ->  96.32199998779296  and loss:  99.93721401691437
test acc: top1 ->  62.86 ; top5 ->  96.15  and loss:  105.27262610197067
forward train acc: top1 ->  64.168 ; top5 ->  96.38200001953125  and loss:  97.76812916994095
test acc: top1 ->  63.36 ; top5 ->  96.23  and loss:  103.91925704479218
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -86.01942309737206 , diff:  86.01942309737206
adv train loss:  -734.56511926651 , diff:  648.545696169138
adv train loss:  -1267.7638969421387 , diff:  533.1987776756287
adv train loss:  -1381.240511894226 , diff:  113.4766149520874
adv train loss:  -1458.6093282699585 , diff:  77.36881637573242
adv train loss:  -1458.1044282913208 , diff:  0.5048999786376953
adv train loss:  -1455.8608903884888 , diff:  2.2435379028320312
adv train loss:  -1459.6459560394287 , diff:  3.7850656509399414
adv train loss:  -1450.734782218933 , diff:  8.911173820495605
adv train loss:  -1442.835153579712 , diff:  7.899628639221191
layer  6  adv train finish, try to retain  2
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  1486.9500169754028
forward train acc: top1 ->  27.91400000732422 ; top5 ->  75.66999999755859  and loss:  253.57276666164398
test acc: top1 ->  34.2 ; top5 ->  82.34  and loss:  183.7740638256073
forward train acc: top1 ->  34.384000002441404 ; top5 ->  82.86199997314453  and loss:  173.71440482139587
test acc: top1 ->  39.17 ; top5 ->  85.52  and loss:  169.83080542087555
forward train acc: top1 ->  38.33799999511719 ; top5 ->  87.23000001220703  and loss:  160.1084145307541
test acc: top1 ->  42.15 ; top5 ->  88.3  and loss:  157.61240458488464
forward train acc: top1 ->  41.761999995117186 ; top5 ->  89.72  and loss:  150.00758981704712
test acc: top1 ->  44.62 ; top5 ->  90.55  and loss:  147.24848771095276
forward train acc: top1 ->  45.57800000366211 ; top5 ->  92.02999997558594  and loss:  139.8091902732849
test acc: top1 ->  48.48 ; top5 ->  92.27  and loss:  139.3880363702774
forward train acc: top1 ->  47.942000002441404 ; top5 ->  93.01799998046874  and loss:  134.0226262807846
test acc: top1 ->  49.34 ; top5 ->  92.93  and loss:  135.71736431121826
forward train acc: top1 ->  49.222 ; top5 ->  93.59199997314452  and loss:  130.59285938739777
test acc: top1 ->  50.43 ; top5 ->  93.52  and loss:  132.26423501968384
forward train acc: top1 ->  50.146000001220706 ; top5 ->  94.09199999755859  and loss:  127.12781655788422
test acc: top1 ->  51.89 ; top5 ->  93.91  and loss:  128.94879859685898
forward train acc: top1 ->  51.14199998901367 ; top5 ->  94.2939999975586  and loss:  124.57498729228973
test acc: top1 ->  53.03 ; top5 ->  94.42  and loss:  126.25849443674088
forward train acc: top1 ->  52.025999993896484 ; top5 ->  94.82000001708984  and loss:  121.90836310386658
test acc: top1 ->  53.68 ; top5 ->  94.79  and loss:  123.4303879737854
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  144 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -77.95595633983612 , diff:  77.95595633983612
adv train loss:  -77.95152282714844 , diff:  0.0044335126876831055
layer  7  adv train finish, try to retain  415
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -145.91816985607147 , diff:  145.91816985607147
adv train loss:  -145.89169764518738 , diff:  0.02647221088409424
layer  8  adv train finish, try to retain  485
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
### skip layer  9 wait:  4  ###
---------------- start layer  10  ---------------
### skip layer  10 wait:  3  ###
---------------- start layer  11  ---------------
### skip layer  11 wait:  4  ###
---------------- start layer  12  ---------------
adv train loss:  -112.558977663517 , diff:  112.558977663517
adv train loss:  -112.87025356292725 , diff:  0.3112758994102478
adv train loss:  -112.85930013656616 , diff:  0.010953426361083984
layer  12  adv train finish, try to retain  510
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
### skip layer  13 wait:  4  ###
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.875  ==>  56 / 64 , inc:  2
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  2
layer  8  :  0.142578125  ==>  73 / 512 , inc:  2
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.16796875  ==>  86 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  2
layer  13  :  0.083984375  ==>  43 / 512 , inc:  1
eps [1.0935000000000001, 8.4470272064209e-05, 0.014416259765625002, 0.014416259765625002, 0.014416259765625002, 1.6402500000000002, 1.6402500000000002, 0.009610839843750002, 8.4470272064209e-05, 3.167635202407837e-05, 0.03844335937500001, 0.8201250000000001, 8.4470272064209e-05, 8.748000000000001]  wait [2, 0, 4, 4, 4, 4, 4, 0, 0, 3, 2, 3, 0, 3]  inc [1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1]  tol: 3
$$$$$$$$$$$$$ epoch  27  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -581.1399266719818 , diff:  581.1399266719818
adv train loss:  -585.5054903030396 , diff:  4.365563631057739
adv train loss:  -587.4217829704285 , diff:  1.916292667388916
adv train loss:  -594.3553557395935 , diff:  6.933572769165039
adv train loss:  -593.1324644088745 , diff:  1.2228913307189941
adv train loss:  -592.4525208473206 , diff:  0.6799435615539551
adv train loss:  -594.8133497238159 , diff:  2.3608288764953613
adv train loss:  -596.9409132003784 , diff:  2.1275634765625
adv train loss:  -595.5695719718933 , diff:  1.3713412284851074
adv train loss:  -595.6209626197815 , diff:  0.051390647888183594
layer  0  adv train finish, try to retain  59
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -88.73841893672943 , diff:  88.73841893672943
adv train loss:  -89.11130821704865 , diff:  0.37288928031921387
adv train loss:  -88.77146601676941 , diff:  0.33984220027923584
adv train loss:  -88.90854960680008 , diff:  0.13708359003067017
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
adv train loss:  -77.74203103780746 , diff:  77.74203103780746
adv train loss:  -77.69741135835648 , diff:  0.04461967945098877
layer  7  adv train finish, try to retain  409
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -145.42949068546295 , diff:  145.42949068546295
adv train loss:  -145.32420313358307 , diff:  0.10528755187988281
layer  8  adv train finish, try to retain  478
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
### skip layer  9 wait:  3  ###
---------------- start layer  10  ---------------
adv train loss:  -49.86732915043831 , diff:  49.86732915043831
adv train loss:  -49.782318472862244 , diff:  0.08501067757606506
layer  10  adv train finish, try to retain  476
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
### skip layer  11 wait:  3  ###
---------------- start layer  12  ---------------
adv train loss:  -112.59867340326309 , diff:  112.59867340326309
adv train loss:  -112.35450232028961 , diff:  0.24417108297348022
adv train loss:  -112.69717073440552 , diff:  0.34266841411590576
adv train loss:  -112.98293828964233 , diff:  0.2857675552368164
adv train loss:  -112.65249878168106 , diff:  0.3304395079612732
adv train loss:  -112.58361154794693 , diff:  0.06888723373413086
layer  12  adv train finish, try to retain  511
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
### skip layer  13 wait:  3  ###
layer  0  :  0.5  ==>  32 / 64 , inc:  1
layer  1  :  0.875  ==>  56 / 64 , inc:  2
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  2
layer  8  :  0.142578125  ==>  73 / 512 , inc:  2
layer  9  :  0  ==>  512 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.16796875  ==>  86 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  2
layer  13  :  0.083984375  ==>  43 / 512 , inc:  1
eps [2.1870000000000003, 0.000168940544128418, 0.014416259765625002, 0.014416259765625002, 0.014416259765625002, 1.6402500000000002, 1.6402500000000002, 0.019221679687500003, 0.000168940544128418, 3.167635202407837e-05, 0.07688671875000001, 0.8201250000000001, 0.000168940544128418, 8.748000000000001]  wait [2, 0, 3, 3, 3, 3, 3, 0, 0, 2, 2, 2, 0, 2]  inc [1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1]  tol: 3
$$$$$$$$$$$$$ epoch  28  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -410.7611093521118 , diff:  410.7611093521118
adv train loss:  -407.65383863449097 , diff:  3.1072707176208496
adv train loss:  -408.8598954677582 , diff:  1.206056833267212
adv train loss:  -409.2144434452057 , diff:  0.35454797744750977
adv train loss:  -409.0494456291199 , diff:  0.16499781608581543
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  31
test acc: top1 ->  9.24 ; top5 ->  49.82  and loss:  12311.575592041016
forward train acc: top1 ->  97.14599997558594 ; top5 ->  99.93  and loss:  10.24700086005032
test acc: top1 ->  90.83 ; top5 ->  99.1  and loss:  38.0243668705225
forward train acc: top1 ->  99.4900000024414 ; top5 ->  99.994  and loss:  1.8486356204375625
test acc: top1 ->  91.67 ; top5 ->  99.16  and loss:  42.36188814043999
forward train acc: top1 ->  99.67799997558593 ; top5 ->  99.998  and loss:  1.0042370794108137
test acc: top1 ->  91.69 ; top5 ->  99.19  and loss:  44.305887550115585
forward train acc: top1 ->  99.758 ; top5 ->  100.0  and loss:  0.7220523385331035
test acc: top1 ->  91.88 ; top5 ->  99.18  and loss:  46.15046511590481
forward train acc: top1 ->  99.848 ; top5 ->  100.0  and loss:  0.5429350081831217
test acc: top1 ->  92.11 ; top5 ->  99.16  and loss:  46.871504820883274
==> this epoch:  31 / 64
---------------- start layer  1  ---------------
adv train loss:  -0.4625320148188621 , diff:  0.4625320148188621
adv train loss:  -0.4554618909023702 , diff:  0.007070123916491866
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  54
test acc: top1 ->  64.11 ; top5 ->  90.8  and loss:  224.27299213409424
forward train acc: top1 ->  99.1740000024414 ; top5 ->  99.986  and loss:  2.525251941755414
test acc: top1 ->  90.9 ; top5 ->  99.13  and loss:  48.06241550296545
forward train acc: top1 ->  99.44999997558594 ; top5 ->  99.996  and loss:  1.661315391305834
test acc: top1 ->  91.02 ; top5 ->  99.16  and loss:  49.739039089530706
forward train acc: top1 ->  99.528 ; top5 ->  99.998  and loss:  1.4121676788199693
test acc: top1 ->  91.06 ; top5 ->  99.19  and loss:  50.07678206264973
forward train acc: top1 ->  99.61599997558594 ; top5 ->  100.0  and loss:  1.1134922333294526
test acc: top1 ->  91.21 ; top5 ->  99.16  and loss:  51.309878293424845
forward train acc: top1 ->  99.71399997558593 ; top5 ->  99.996  and loss:  0.8716370921465568
test acc: top1 ->  91.32 ; top5 ->  99.17  and loss:  52.98727799206972
forward train acc: top1 ->  99.70999997558594 ; top5 ->  100.0  and loss:  0.8491238493006676
test acc: top1 ->  91.3 ; top5 ->  99.22  and loss:  52.60098544880748
forward train acc: top1 ->  99.7360000024414 ; top5 ->  99.998  and loss:  0.7720958690624684
test acc: top1 ->  91.4 ; top5 ->  99.21  and loss:  53.75849371403456
forward train acc: top1 ->  99.74799997558594 ; top5 ->  99.998  and loss:  0.6970656726043671
test acc: top1 ->  91.35 ; top5 ->  99.24  and loss:  53.16114594042301
forward train acc: top1 ->  99.76199997802735 ; top5 ->  99.998  and loss:  0.7366996989003383
test acc: top1 ->  91.5 ; top5 ->  99.22  and loss:  53.91123306006193
forward train acc: top1 ->  99.80399997558594 ; top5 ->  100.0  and loss:  0.5770570287131704
test acc: top1 ->  91.47 ; top5 ->  99.19  and loss:  54.46743117272854
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  56 / 64 , inc:  2
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  3  ###
---------------- start layer  7  ---------------
adv train loss:  -0.5659625481057446 , diff:  0.5659625481057446
adv train loss:  -0.47664966963930055 , diff:  0.08931287846644409
adv train loss:  -0.539841439051088 , diff:  0.06319176941178739
adv train loss:  -0.4756424894439988 , diff:  0.06419894960708916
adv train loss:  -0.5972836373257451 , diff:  0.12164114788174629
adv train loss:  -0.5382626965292729 , diff:  0.05902094079647213
adv train loss:  -0.5374448286020197 , diff:  0.0008178679272532463
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  102
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  619.5587000846863
forward train acc: top1 ->  36.04199999389648 ; top5 ->  92.92  and loss:  297.5054290294647
test acc: top1 ->  44.98 ; top5 ->  96.61  and loss:  148.07607650756836
forward train acc: top1 ->  49.62000000732422 ; top5 ->  98.29400000488282  and loss:  123.93658304214478
test acc: top1 ->  53.5 ; top5 ->  97.39  and loss:  127.80793714523315
forward train acc: top1 ->  56.62800000732422 ; top5 ->  98.90199997802735  and loss:  106.54562479257584
test acc: top1 ->  58.68 ; top5 ->  97.78  and loss:  113.7147787809372
forward train acc: top1 ->  63.34400000732422 ; top5 ->  99.29599997558594  and loss:  91.38601958751678
test acc: top1 ->  63.41 ; top5 ->  98.12  and loss:  100.54550105333328
forward train acc: top1 ->  74.58999999267579 ; top5 ->  99.5260000024414  and loss:  67.66721230745316
test acc: top1 ->  75.19 ; top5 ->  98.58  and loss:  78.12299692630768
forward train acc: top1 ->  81.67199997558593 ; top5 ->  99.774  and loss:  51.137089401483536
test acc: top1 ->  76.99 ; top5 ->  98.75  and loss:  73.65791523456573
forward train acc: top1 ->  83.63599999267578 ; top5 ->  99.772  and loss:  45.02358487248421
test acc: top1 ->  78.2 ; top5 ->  98.81  and loss:  70.49883350729942
forward train acc: top1 ->  85.37599997558594 ; top5 ->  99.82  and loss:  40.31093317270279
test acc: top1 ->  79.34 ; top5 ->  98.83  and loss:  67.70928445458412
forward train acc: top1 ->  86.41800001708984 ; top5 ->  99.86199997558593  and loss:  37.13309946656227
test acc: top1 ->  79.95 ; top5 ->  98.85  and loss:  66.70533537864685
forward train acc: top1 ->  87.51199997070313 ; top5 ->  99.886  and loss:  34.58545646071434
test acc: top1 ->  80.85 ; top5 ->  98.88  and loss:  65.62235763669014
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  104 / 512 , inc:  2
---------------- start layer  8  ---------------
adv train loss:  -31.40195867419243 , diff:  31.40195867419243
adv train loss:  -31.2169132232666 , diff:  0.18504545092582703
adv train loss:  -31.381105855107307 , diff:  0.16419263184070587
adv train loss:  -31.68518005311489 , diff:  0.3040741980075836
adv train loss:  -31.281260073184967 , diff:  0.403919979929924
adv train loss:  -30.911731362342834 , diff:  0.36952871084213257
adv train loss:  -31.756967142224312 , diff:  0.8452357798814774
adv train loss:  -31.141875118017197 , diff:  0.6150920242071152
adv train loss:  -31.186127841472626 , diff:  0.04425272345542908
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  71
test acc: top1 ->  10.41 ; top5 ->  57.46  and loss:  697.8582787513733
forward train acc: top1 ->  95.46599998046875 ; top5 ->  99.796  and loss:  17.898759552277625
test acc: top1 ->  90.48 ; top5 ->  98.91  and loss:  51.292264461517334
forward train acc: top1 ->  99.43 ; top5 ->  99.99  and loss:  2.027263119351119
test acc: top1 ->  90.75 ; top5 ->  98.94  and loss:  53.259528145194054
forward train acc: top1 ->  99.648 ; top5 ->  99.99  and loss:  1.339065591339022
test acc: top1 ->  91.14 ; top5 ->  98.98  and loss:  55.76986985653639
forward train acc: top1 ->  99.74399997558594 ; top5 ->  100.0  and loss:  0.9358855839818716
test acc: top1 ->  91.33 ; top5 ->  99.01  and loss:  56.814397037029266
forward train acc: top1 ->  99.75599997558594 ; top5 ->  99.998  and loss:  0.81832215632312
test acc: top1 ->  91.39 ; top5 ->  99.01  and loss:  57.875260174274445
forward train acc: top1 ->  99.836 ; top5 ->  100.0  and loss:  0.5828464068472385
test acc: top1 ->  91.45 ; top5 ->  99.01  and loss:  58.41367680579424
forward train acc: top1 ->  99.806 ; top5 ->  100.0  and loss:  0.611122390255332
test acc: top1 ->  91.51 ; top5 ->  99.04  and loss:  58.71220774948597
forward train acc: top1 ->  99.8640000024414 ; top5 ->  100.0  and loss:  0.4931322305928916
test acc: top1 ->  91.52 ; top5 ->  99.06  and loss:  59.38394382596016
forward train acc: top1 ->  99.8260000024414 ; top5 ->  100.0  and loss:  0.5470024291425943
test acc: top1 ->  91.5 ; top5 ->  99.09  and loss:  58.46670052409172
forward train acc: top1 ->  99.876 ; top5 ->  100.0  and loss:  0.4274788429029286
test acc: top1 ->  91.53 ; top5 ->  99.03  and loss:  60.22344918549061
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  73 / 512 , inc:  2
---------------- start layer  9  ---------------
adv train loss:  -0.7610887507908046 , diff:  0.7610887507908046
adv train loss:  -0.7263237158185802 , diff:  0.034765034972224385
adv train loss:  -0.7437458186759613 , diff:  0.017422102857381105
adv train loss:  -0.722990479611326 , diff:  0.020755339064635336
adv train loss:  -0.6776103163138032 , diff:  0.04538016329752281
adv train loss:  -0.6641780829522759 , diff:  0.013432233361527324
adv train loss:  -0.7510005655931309 , diff:  0.08682248264085501
adv train loss:  -0.7370090195909142 , diff:  0.013991546002216637
adv train loss:  -0.7022923064650968 , diff:  0.03471671312581748
adv train loss:  -0.6176645007217303 , diff:  0.08462780574336648
layer  9  adv train finish, try to retain  96
test acc: top1 ->  85.8 ; top5 ->  98.78  and loss:  73.28446853160858
forward train acc: top1 ->  99.91 ; top5 ->  99.998  and loss:  0.28710471452359343
test acc: top1 ->  92.02 ; top5 ->  99.2  and loss:  59.720547050237656
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.20500536757754162
test acc: top1 ->  92.16 ; top5 ->  99.21  and loss:  61.340009950101376
==> this epoch:  96 / 512
---------------- start layer  10  ---------------
adv train loss:  -0.2652481274562888 , diff:  0.2652481274562888
adv train loss:  -0.24335687162238173 , diff:  0.021891255833907053
adv train loss:  -0.26313542552816216 , diff:  0.019778553905780427
adv train loss:  -0.2546843100572005 , diff:  0.008451115470961668
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  21
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  2444.1305751800537
forward train acc: top1 ->  10.083999998474122 ; top5 ->  50.13399999389648  and loss:  362.7257573604584
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  664.6128969192505
forward train acc: top1 ->  10.098000000915528 ; top5 ->  49.89399998901367  and loss:  225.97772121429443
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.48014521598816
forward train acc: top1 ->  9.919999998321533 ; top5 ->  50.061999993896485  and loss:  225.9820806980133
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.58975172042847
forward train acc: top1 ->  9.854000000305176 ; top5 ->  49.79200000732422  and loss:  226.1387484073639
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  231.45923137664795
forward train acc: top1 ->  9.802000000610352 ; top5 ->  49.80199998779297  and loss:  226.0871181488037
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.4378547668457
forward train acc: top1 ->  10.042 ; top5 ->  49.86400000488281  and loss:  225.90627598762512
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.74008703231812
forward train acc: top1 ->  10.03400000213623 ; top5 ->  49.77999999511719  and loss:  225.93431973457336
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.52741742134094
forward train acc: top1 ->  9.910000001831055 ; top5 ->  50.036000002441405  and loss:  225.88676834106445
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.3664791584015
forward train acc: top1 ->  9.855999999084473 ; top5 ->  50.15399998901367  and loss:  225.87423181533813
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  230.505863904953
forward train acc: top1 ->  9.816000001831055 ; top5 ->  50.00600000366211  and loss:  225.94093990325928
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  231.0639946460724
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  22 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -0.7343580041051609 , diff:  0.7343580041051609
adv train loss:  -0.8099662002059631 , diff:  0.07560819610080216
adv train loss:  -0.6779089318588376 , diff:  0.1320572683471255
adv train loss:  -0.8006756143877283 , diff:  0.12276668252889067
adv train loss:  -0.7255397697445005 , diff:  0.07513584464322776
adv train loss:  -0.7579416744410992 , diff:  0.03240190469659865
adv train loss:  -0.6354513051919639 , diff:  0.12249036924913526
adv train loss:  -0.6975233686389402 , diff:  0.062072063446976244
adv train loss:  -0.7302978089137468 , diff:  0.03277444027480669
adv train loss:  -0.7288653792929836 , diff:  0.001432429620763287
layer  11  adv train finish, try to retain  415
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -0.5794593840255402 , diff:  0.5794593840255402
adv train loss:  -0.5448358517023735 , diff:  0.03462353232316673
adv train loss:  -0.5699022755725309 , diff:  0.02506642387015745
adv train loss:  -0.5650207041762769 , diff:  0.004881571396254003
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  39
test acc: top1 ->  10.05 ; top5 ->  43.59  and loss:  1278.4731492996216
forward train acc: top1 ->  32.239999990234374 ; top5 ->  57.72199997558594  and loss:  531.0539047718048
test acc: top1 ->  43.41 ; top5 ->  75.67  and loss:  196.8615484237671
forward train acc: top1 ->  83.21200000488281 ; top5 ->  97.46  and loss:  73.36526638269424
test acc: top1 ->  88.59 ; top5 ->  96.89  and loss:  62.7494757771492
forward train acc: top1 ->  98.89599997802735 ; top5 ->  99.96  and loss:  25.38952372968197
test acc: top1 ->  89.75 ; top5 ->  97.25  and loss:  50.289467453956604
forward train acc: top1 ->  99.49599997802734 ; top5 ->  99.99  and loss:  14.7821144759655
test acc: top1 ->  90.3 ; top5 ->  97.49  and loss:  44.49001184105873
forward train acc: top1 ->  99.636 ; top5 ->  99.992  and loss:  9.294623605906963
test acc: top1 ->  90.53 ; top5 ->  97.74  and loss:  42.62531180679798
forward train acc: top1 ->  99.71400000244141 ; top5 ->  99.988  and loss:  6.970388509333134
test acc: top1 ->  90.65 ; top5 ->  97.68  and loss:  42.07012642920017
forward train acc: top1 ->  99.744 ; top5 ->  99.994  and loss:  5.536395609378815
test acc: top1 ->  90.78 ; top5 ->  97.8  and loss:  41.408246487379074
forward train acc: top1 ->  99.776 ; top5 ->  99.996  and loss:  4.588635116815567
test acc: top1 ->  90.9 ; top5 ->  97.78  and loss:  41.324408173561096
forward train acc: top1 ->  99.82 ; top5 ->  99.998  and loss:  3.9274602197110653
test acc: top1 ->  91.02 ; top5 ->  97.83  and loss:  41.43353508412838
forward train acc: top1 ->  99.83399997558594 ; top5 ->  100.0  and loss:  3.3773356564342976
test acc: top1 ->  91.04 ; top5 ->  97.81  and loss:  41.87273623049259
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  41 / 512 , inc:  2
---------------- start layer  13  ---------------
adv train loss:  -6933.441493988037 , diff:  6933.441493988037
adv train loss:  -10780.58586883545 , diff:  3847.144374847412
adv train loss:  -14938.051940917969 , diff:  4157.4660720825195
adv train loss:  -19503.315155029297 , diff:  4565.263214111328
adv train loss:  -24032.773834228516 , diff:  4529.458679199219
adv train loss:  -28411.213409423828 , diff:  4378.4395751953125
adv train loss:  -32673.836547851562 , diff:  4262.623138427734
adv train loss:  -35905.147033691406 , diff:  3231.3104858398438
adv train loss:  -37373.14855957031 , diff:  1468.0015258789062
adv train loss:  -37867.97927856445 , diff:  494.8307189941406
layer  13  adv train finish, try to retain  31
test acc: top1 ->  35.36 ; top5 ->  85.68  and loss:  815.8515200614929
forward train acc: top1 ->  89.63399997558594 ; top5 ->  98.012  and loss:  93.75199857680127
test acc: top1 ->  91.18 ; top5 ->  98.51  and loss:  61.7693285793066
forward train acc: top1 ->  99.81 ; top5 ->  99.998  and loss:  0.773715796880424
test acc: top1 ->  91.58 ; top5 ->  98.63  and loss:  59.919042371213436
forward train acc: top1 ->  99.90799997558594 ; top5 ->  100.0  and loss:  0.4593412126414478
test acc: top1 ->  91.79 ; top5 ->  98.68  and loss:  59.33900383859873
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.3571732802083716
test acc: top1 ->  91.93 ; top5 ->  98.73  and loss:  58.09756841510534
forward train acc: top1 ->  99.94 ; top5 ->  100.0  and loss:  0.2729443694697693
test acc: top1 ->  91.93 ; top5 ->  98.78  and loss:  59.11386586725712
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.2569832322187722
test acc: top1 ->  92.03 ; top5 ->  98.8  and loss:  58.956066466867924
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.23419482295867056
test acc: top1 ->  92.01 ; top5 ->  98.79  and loss:  59.46881438046694
forward train acc: top1 ->  99.95399997558594 ; top5 ->  100.0  and loss:  0.20532041223486885
test acc: top1 ->  92.0 ; top5 ->  98.82  and loss:  59.81904659420252
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.20318477629916742
test acc: top1 ->  92.04 ; top5 ->  98.81  and loss:  59.84202767908573
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.17956536918063648
test acc: top1 ->  92.01 ; top5 ->  98.83  and loss:  60.59613557159901
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  43 / 512 , inc:  1
layer  0  :  0.484375  ==>  31 / 64 , inc:  2
layer  1  :  0.875  ==>  56 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.142578125  ==>  73 / 512 , inc:  1
layer  9  :  0.1875  ==>  96 / 512 , inc:  2
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.16796875  ==>  86 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.083984375  ==>  43 / 512 , inc:  1
eps [2.1870000000000003, 0.00012670540809631348, 0.014416259765625002, 0.014416259765625002, 0.014416259765625002, 1.6402500000000002, 1.6402500000000002, 0.014416259765625002, 0.00012670540809631348, 3.167635202407837e-05, 0.057665039062500006, 1.6402500000000002, 0.00012670540809631348, 6.561000000000001]  wait [0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 4, 2, 2, 4]  inc [2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]  tol: 3
$$$$$$$$$$$$$ epoch  29  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1581.5748829841614 , diff:  1581.5748829841614
adv train loss:  -1652.4877672195435 , diff:  70.91288423538208
adv train loss:  -1661.5322618484497 , diff:  9.04449462890625
adv train loss:  -1536.0227336883545 , diff:  125.50952816009521
adv train loss:  -1492.2953872680664 , diff:  43.727346420288086
adv train loss:  -1498.3221950531006 , diff:  6.02680778503418
adv train loss:  -1498.545503616333 , diff:  0.22330856323242188
adv train loss:  -1489.1594305038452 , diff:  9.386073112487793
adv train loss:  -1496.4634437561035 , diff:  7.304013252258301
adv train loss:  -1486.098720550537 , diff:  10.364723205566406
layer  0  adv train finish, try to retain  1
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  16042.621810913086
forward train acc: top1 ->  13.333999996643067 ; top5 ->  56.44799999633789  and loss:  893.8464465141296
test acc: top1 ->  10.53 ; top5 ->  48.65  and loss:  806.4509482383728
forward train acc: top1 ->  13.525999998779296 ; top5 ->  58.46399998779297  and loss:  285.841534614563
test acc: top1 ->  13.11 ; top5 ->  60.14  and loss:  252.2177426815033
forward train acc: top1 ->  14.366000000305176 ; top5 ->  60.241999993896485  and loss:  242.294775724411
test acc: top1 ->  13.75 ; top5 ->  61.6  and loss:  235.9995312690735
forward train acc: top1 ->  15.463999999084473 ; top5 ->  62.3159999987793  and loss:  231.164626121521
test acc: top1 ->  16.56 ; top5 ->  63.96  and loss:  229.3160457611084
forward train acc: top1 ->  16.335999996948242 ; top5 ->  64.3560000024414  and loss:  225.3182282447815
test acc: top1 ->  16.53 ; top5 ->  65.51  and loss:  224.70465302467346
forward train acc: top1 ->  16.97199999938965 ; top5 ->  65.6540000024414  and loss:  222.01180815696716
test acc: top1 ->  16.69 ; top5 ->  65.94  and loss:  223.69724917411804
forward train acc: top1 ->  17.301999994506836 ; top5 ->  66.40599998046875  and loss:  220.53897643089294
test acc: top1 ->  16.77 ; top5 ->  67.08  and loss:  222.55751061439514
forward train acc: top1 ->  17.893999996643068 ; top5 ->  67.20800000732422  and loss:  219.32059407234192
test acc: top1 ->  16.49 ; top5 ->  67.55  and loss:  222.64718556404114
forward train acc: top1 ->  18.02800000366211 ; top5 ->  68.15799997802735  and loss:  218.0113627910614
test acc: top1 ->  17.23 ; top5 ->  68.7  and loss:  220.01250195503235
forward train acc: top1 ->  18.79200000427246 ; top5 ->  68.40399998291015  and loss:  216.98996353149414
test acc: top1 ->  18.27 ; top5 ->  69.38  and loss:  218.3400273323059
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  31 / 64 , inc:  2
---------------- start layer  1  ---------------
adv train loss:  -33.41583213210106 , diff:  33.41583213210106
adv train loss:  -33.47356501221657 , diff:  0.05773288011550903
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -33.2532939016819 , diff:  33.2532939016819
adv train loss:  -33.418202221393585 , diff:  0.16490831971168518
adv train loss:  -33.53225600719452 , diff:  0.11405378580093384
adv train loss:  -33.44588130712509 , diff:  0.08637470006942749
adv train loss:  -33.42261880636215 , diff:  0.023262500762939453
layer  2  adv train finish, try to retain  128
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -33.67653627693653 , diff:  33.67653627693653
adv train loss:  -33.62248420715332 , diff:  0.054052069783210754
layer  3  adv train finish, try to retain  128
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -33.3105494081974 , diff:  33.3105494081974
adv train loss:  -33.45586997270584 , diff:  0.1453205645084381
adv train loss:  -33.44080001115799 , diff:  0.015069961547851562
layer  4  adv train finish, try to retain  254
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -79.68731927871704 , diff:  79.68731927871704
adv train loss:  -580.2564961910248 , diff:  500.56917691230774
adv train loss:  -706.1486744880676 , diff:  125.89217829704285
adv train loss:  -705.7433838844299 , diff:  0.4052906036376953
adv train loss:  -693.5244045257568 , diff:  12.218979358673096
adv train loss:  -686.8501772880554 , diff:  6.674227237701416
adv train loss:  -686.0476846694946 , diff:  0.802492618560791
adv train loss:  -682.5937600135803 , diff:  3.4539246559143066
adv train loss:  -688.5223278999329 , diff:  5.928567886352539
adv train loss:  -681.7562403678894 , diff:  6.766087532043457
layer  5  adv train finish, try to retain  33
test acc: top1 ->  18.17 ; top5 ->  58.81  and loss:  626.8702130317688
forward train acc: top1 ->  78.98800000244141 ; top5 ->  97.93999998046876  and loss:  65.36911377310753
test acc: top1 ->  77.32 ; top5 ->  97.91  and loss:  75.1990357041359
forward train acc: top1 ->  84.10799998046875 ; top5 ->  98.93999998291015  and loss:  47.73178651928902
test acc: top1 ->  80.06 ; top5 ->  98.29  and loss:  65.8840266764164
forward train acc: top1 ->  86.41800001220703 ; top5 ->  99.33599997558593  and loss:  40.3465980887413
test acc: top1 ->  81.28 ; top5 ->  98.41  and loss:  62.51430717110634
forward train acc: top1 ->  87.98599999511718 ; top5 ->  99.42799997802734  and loss:  35.92890700697899
test acc: top1 ->  82.66 ; top5 ->  98.7  and loss:  58.059573978185654
forward train acc: top1 ->  88.66399997558594 ; top5 ->  99.562  and loss:  33.27237689495087
test acc: top1 ->  83.48 ; top5 ->  98.65  and loss:  57.18880099058151
forward train acc: top1 ->  89.53200001464843 ; top5 ->  99.61799997558593  and loss:  31.1006977558136
test acc: top1 ->  83.92 ; top5 ->  98.78  and loss:  54.29557517170906
forward train acc: top1 ->  89.93399998535156 ; top5 ->  99.624  and loss:  29.78404602408409
test acc: top1 ->  84.23 ; top5 ->  98.77  and loss:  54.49104171991348
forward train acc: top1 ->  90.07400001220704 ; top5 ->  99.646  and loss:  29.02965533733368
test acc: top1 ->  84.25 ; top5 ->  98.77  and loss:  53.6133106648922
forward train acc: top1 ->  90.51600000732422 ; top5 ->  99.64999997802734  and loss:  27.888605788350105
test acc: top1 ->  84.62 ; top5 ->  98.85  and loss:  52.30750185251236
forward train acc: top1 ->  90.66800000732422 ; top5 ->  99.65999997558593  and loss:  27.38844856619835
test acc: top1 ->  84.6 ; top5 ->  98.85  and loss:  52.137758046388626
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -267.16066435724497 , diff:  267.16066435724497
adv train loss:  -1217.6268577575684 , diff:  950.4661934003234
adv train loss:  -1315.1789846420288 , diff:  97.55212688446045
adv train loss:  -1320.0145139694214 , diff:  4.835529327392578
adv train loss:  -1321.2699060440063 , diff:  1.255392074584961
adv train loss:  -1323.3935632705688 , diff:  2.1236572265625
adv train loss:  -1327.2504358291626 , diff:  3.85687255859375
adv train loss:  -1321.0754442214966 , diff:  6.174991607666016
adv train loss:  -1315.2466306686401 , diff:  5.828813552856445
adv train loss:  -1302.0165214538574 , diff:  13.230109214782715
layer  6  adv train finish, try to retain  17
test acc: top1 ->  18.13 ; top5 ->  51.6  and loss:  778.424421787262
forward train acc: top1 ->  80.85199998291016 ; top5 ->  99.1260000024414  and loss:  54.49441060423851
test acc: top1 ->  79.01 ; top5 ->  98.27  and loss:  71.42175179719925
forward train acc: top1 ->  86.0699999975586 ; top5 ->  99.43200000244141  and loss:  40.1857967376709
test acc: top1 ->  81.67 ; top5 ->  98.57  and loss:  63.16406863927841
forward train acc: top1 ->  88.67199998535156 ; top5 ->  99.62399997558593  and loss:  32.559764459729195
test acc: top1 ->  83.04 ; top5 ->  98.67  and loss:  59.4967582821846
forward train acc: top1 ->  90.33799998535156 ; top5 ->  99.73399997802734  and loss:  28.163103938102722
test acc: top1 ->  84.37 ; top5 ->  98.76  and loss:  56.27546286582947
forward train acc: top1 ->  91.44200000488281 ; top5 ->  99.72  and loss:  25.28174266219139
test acc: top1 ->  84.96 ; top5 ->  98.85  and loss:  54.10416167974472
forward train acc: top1 ->  92.04399997802734 ; top5 ->  99.77399997558594  and loss:  23.06061464548111
test acc: top1 ->  85.13 ; top5 ->  98.87  and loss:  53.71840211749077
forward train acc: top1 ->  92.26199997314453 ; top5 ->  99.778  and loss:  22.62982629239559
test acc: top1 ->  85.42 ; top5 ->  98.96  and loss:  53.186130702495575
forward train acc: top1 ->  92.76799998535157 ; top5 ->  99.792  and loss:  21.425769925117493
test acc: top1 ->  85.81 ; top5 ->  98.9  and loss:  51.56887252628803
forward train acc: top1 ->  92.91400000732422 ; top5 ->  99.83199997558594  and loss:  20.649400755763054
test acc: top1 ->  85.78 ; top5 ->  98.98  and loss:  51.43825148046017
forward train acc: top1 ->  93.15600000732422 ; top5 ->  99.826  and loss:  20.086920082569122
test acc: top1 ->  86.08 ; top5 ->  99.04  and loss:  50.59051947295666
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  144 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -4.890824448317289 , diff:  4.890824448317289
adv train loss:  -4.876418098807335 , diff:  0.014406349509954453
adv train loss:  -4.778019584715366 , diff:  0.09839851409196854
adv train loss:  -4.833540113642812 , diff:  0.05552052892744541
adv train loss:  -4.880287952721119 , diff:  0.04674783907830715
adv train loss:  -4.8753392193466425 , diff:  0.004948733374476433
layer  7  adv train finish, try to retain  392
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -8.124105118215084 , diff:  8.124105118215084
adv train loss:  -8.250568572431803 , diff:  0.12646345421671867
adv train loss:  -8.234520398080349 , diff:  0.01604817435145378
layer  8  adv train finish, try to retain  440
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -8.473421759903431 , diff:  8.473421759903431
adv train loss:  -8.515673898160458 , diff:  0.04225213825702667
adv train loss:  -8.404775369912386 , diff:  0.11089852824807167
adv train loss:  -8.464422300457954 , diff:  0.059646930545568466
adv train loss:  -8.339278478175402 , diff:  0.12514382228255272
adv train loss:  -8.253695040941238 , diff:  0.08558343723416328
adv train loss:  -8.420657400041819 , diff:  0.16696235910058022
adv train loss:  -8.323693331331015 , diff:  0.09696406871080399
adv train loss:  -8.518915999680758 , diff:  0.1952226683497429
adv train loss:  -8.364882364869118 , diff:  0.15403363481163979
layer  9  adv train finish, try to retain  489
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
### skip layer  10 wait:  4  ###
---------------- start layer  11  ---------------
adv train loss:  -115.60364496707916 , diff:  115.60364496707916
adv train loss:  -140.5401246547699 , diff:  24.936479687690735
adv train loss:  -158.1565523147583 , diff:  17.616427659988403
adv train loss:  -175.08151590824127 , diff:  16.92496359348297
adv train loss:  -174.59761703014374 , diff:  0.4838988780975342
adv train loss:  -245.84163415431976 , diff:  71.24401712417603
adv train loss:  -327.1701967716217 , diff:  81.32856261730194
adv train loss:  -432.8733801841736 , diff:  105.70318341255188
adv train loss:  -1075.2714080810547 , diff:  642.3980278968811
adv train loss:  -1643.409267425537 , diff:  568.1378593444824
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  85
test acc: top1 ->  10.89 ; top5 ->  65.28  and loss:  3508068.990234375
forward train acc: top1 ->  98.23999997558593 ; top5 ->  99.946  and loss:  6.427760446909815
test acc: top1 ->  91.71 ; top5 ->  98.7  and loss:  55.749575689435005
forward train acc: top1 ->  99.76 ; top5 ->  99.996  and loss:  0.8514769654721022
test acc: top1 ->  91.65 ; top5 ->  98.75  and loss:  58.09533950686455
forward train acc: top1 ->  99.814 ; top5 ->  100.0  and loss:  0.6325740386382677
test acc: top1 ->  91.83 ; top5 ->  98.84  and loss:  59.705622494220734
forward train acc: top1 ->  99.836 ; top5 ->  100.0  and loss:  0.5066168279154226
test acc: top1 ->  91.83 ; top5 ->  98.88  and loss:  61.5468497723341
forward train acc: top1 ->  99.87399997558593 ; top5 ->  100.0  and loss:  0.37551376852206886
test acc: top1 ->  92.07 ; top5 ->  98.91  and loss:  62.0595378279686
forward train acc: top1 ->  99.904 ; top5 ->  100.0  and loss:  0.3490642267279327
test acc: top1 ->  92.03 ; top5 ->  98.86  and loss:  63.16876199841499
forward train acc: top1 ->  99.92200000244141 ; top5 ->  100.0  and loss:  0.2552252933382988
test acc: top1 ->  91.92 ; top5 ->  98.89  and loss:  63.48395782709122
forward train acc: top1 ->  99.914 ; top5 ->  100.0  and loss:  0.2631571216043085
test acc: top1 ->  91.95 ; top5 ->  98.89  and loss:  63.84404522180557
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.23803453787695616
test acc: top1 ->  91.98 ; top5 ->  98.88  and loss:  64.57163175940514
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.25598158524371684
test acc: top1 ->  91.95 ; top5 ->  98.89  and loss:  65.00207258760929
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  86 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -10.777078799903393 , diff:  10.777078799903393
adv train loss:  -11.176552627235651 , diff:  0.3994738273322582
adv train loss:  -10.782786436378956 , diff:  0.3937661908566952
adv train loss:  -11.038984227925539 , diff:  0.2561977915465832
adv train loss:  -10.865052547305822 , diff:  0.17393168061971664
adv train loss:  -11.016638781875372 , diff:  0.15158623456954956
adv train loss:  -10.875854298472404 , diff:  0.14078448340296745
adv train loss:  -10.820517420768738 , diff:  0.05533687770366669
adv train loss:  -10.787916708737612 , diff:  0.03260071203112602
adv train loss:  -10.876314975321293 , diff:  0.0883982665836811
layer  12  adv train finish, try to retain  509
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
### skip layer  13 wait:  4  ###
layer  0  :  0.484375  ==>  31 / 64 , inc:  1
layer  1  :  0.875  ==>  56 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.142578125  ==>  73 / 512 , inc:  1
layer  9  :  0.1875  ==>  96 / 512 , inc:  2
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.16796875  ==>  86 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.083984375  ==>  43 / 512 , inc:  1
eps [1.6402500000000002, 0.00025341081619262697, 0.028832519531250003, 0.028832519531250003, 0.028832519531250003, 1.2301875000000002, 1.2301875000000002, 0.028832519531250003, 0.00025341081619262697, 6.335270404815674e-05, 0.057665039062500006, 1.2301875000000002, 0.00025341081619262697, 6.561000000000001]  wait [2, 2, 2, 2, 2, 4, 4, 2, 2, 0, 3, 4, 2, 3]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]  tol: 3
$$$$$$$$$$$$$ epoch  30  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -905.7389559745789 , diff:  905.7389559745789
adv train loss:  -942.9139385223389 , diff:  37.17498254776001
adv train loss:  -940.403938293457 , diff:  2.510000228881836
adv train loss:  -770.7802977561951 , diff:  169.62364053726196
adv train loss:  -659.0361194610596 , diff:  111.7441782951355
adv train loss:  -646.2412214279175 , diff:  12.79489803314209
adv train loss:  -538.8657674789429 , diff:  107.37545394897461
adv train loss:  -517.3519315719604 , diff:  21.513835906982422
adv train loss:  -521.5983905792236 , diff:  4.246459007263184
adv train loss:  -525.3604221343994 , diff:  3.7620315551757812
layer  0  adv train finish, try to retain  4
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  7061.961757659912
forward train acc: top1 ->  78.51400000732421 ; top5 ->  96.94200000732422  and loss:  77.37603032588959
test acc: top1 ->  55.87 ; top5 ->  85.74  and loss:  175.8446445465088
forward train acc: top1 ->  82.57599998535156 ; top5 ->  98.15200000732422  and loss:  53.742772191762924
test acc: top1 ->  80.42 ; top5 ->  97.62  and loss:  63.740208595991135
forward train acc: top1 ->  84.90799998535157 ; top5 ->  98.67799997802734  and loss:  46.274947971105576
test acc: top1 ->  81.79 ; top5 ->  97.95  and loss:  59.68562996387482
forward train acc: top1 ->  85.90800001464844 ; top5 ->  98.87999997802734  and loss:  42.4706307053566
test acc: top1 ->  82.76 ; top5 ->  98.24  and loss:  56.2038137614727
forward train acc: top1 ->  87.58399997802735 ; top5 ->  99.04200000488281  and loss:  38.06588378548622
test acc: top1 ->  83.4 ; top5 ->  98.46  and loss:  54.149992257356644
forward train acc: top1 ->  88.254 ; top5 ->  99.2320000048828  and loss:  35.455874532461166
test acc: top1 ->  83.68 ; top5 ->  98.5  and loss:  53.781783789396286
forward train acc: top1 ->  88.55800000976562 ; top5 ->  99.2580000024414  and loss:  34.44375768303871
test acc: top1 ->  83.96 ; top5 ->  98.54  and loss:  53.09644666314125
forward train acc: top1 ->  88.97000001708984 ; top5 ->  99.28000000488281  and loss:  33.310972303152084
test acc: top1 ->  84.21 ; top5 ->  98.56  and loss:  52.06182187795639
forward train acc: top1 ->  89.42399997070312 ; top5 ->  99.35599997802734  and loss:  32.02889432013035
test acc: top1 ->  84.48 ; top5 ->  98.61  and loss:  51.67834734916687
forward train acc: top1 ->  89.83199997558594 ; top5 ->  99.39999997802734  and loss:  30.75520470738411
test acc: top1 ->  84.7 ; top5 ->  98.6  and loss:  50.71000763773918
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  31 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -3.7363905124366283 , diff:  3.7363905124366283
adv train loss:  -3.739741086959839 , diff:  0.0033505745232105255
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -3.7030812948942184 , diff:  3.7030812948942184
adv train loss:  -3.621524417772889 , diff:  0.08155687712132931
adv train loss:  -3.67206846550107 , diff:  0.050544047728180885
adv train loss:  -3.731439596042037 , diff:  0.05937113054096699
adv train loss:  -3.681634047999978 , diff:  0.049805548042058945
adv train loss:  -3.774911629036069 , diff:  0.09327758103609085
adv train loss:  -3.7276157066226006 , diff:  0.04729592241346836
adv train loss:  -3.7696724720299244 , diff:  0.04205676540732384
adv train loss:  -3.6695348285138607 , diff:  0.10013764351606369
adv train loss:  -3.6596912499517202 , diff:  0.009843578562140465
layer  2  adv train finish, try to retain  128
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -3.6663634832948446 , diff:  3.6663634832948446
adv train loss:  -3.7146432269364595 , diff:  0.048279743641614914
adv train loss:  -3.6918852645903826 , diff:  0.022757962346076965
adv train loss:  -3.7064159475266933 , diff:  0.014530682936310768
adv train loss:  -3.6491053849458694 , diff:  0.0573105625808239
adv train loss:  -3.721013305708766 , diff:  0.07190792076289654
adv train loss:  -3.6539489664137363 , diff:  0.06706433929502964
adv train loss:  -3.731660857796669 , diff:  0.07771189138293266
adv train loss:  -3.724252240732312 , diff:  0.007408617064356804
layer  3  adv train finish, try to retain  128
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -3.6923779156059027 , diff:  3.6923779156059027
adv train loss:  -3.67063619941473 , diff:  0.0217417161911726
adv train loss:  -3.6665539294481277 , diff:  0.0040822699666023254
layer  4  adv train finish, try to retain  256
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
adv train loss:  -4.582422621548176 , diff:  4.582422621548176
adv train loss:  -4.558138784021139 , diff:  0.024283837527036667
adv train loss:  -4.693421915173531 , diff:  0.13528313115239143
adv train loss:  -4.5961502976715565 , diff:  0.0972716175019741
adv train loss:  -4.557307304814458 , diff:  0.03884299285709858
adv train loss:  -4.682654671370983 , diff:  0.12534736655652523
adv train loss:  -4.589659351855516 , diff:  0.09299531951546669
adv train loss:  -4.653890643268824 , diff:  0.06423129141330719
adv train loss:  -4.653912354260683 , diff:  2.1710991859436035e-05
layer  7  adv train finish, try to retain  398
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -5.335609931498766 , diff:  5.335609931498766
adv train loss:  -5.422223903238773 , diff:  0.0866139717400074
adv train loss:  -5.3246816620230675 , diff:  0.09754224121570587
adv train loss:  -5.463395543396473 , diff:  0.13871388137340546
adv train loss:  -5.291714891791344 , diff:  0.17168065160512924
adv train loss:  -5.309415776282549 , diff:  0.017700884491205215
adv train loss:  -5.388424132019281 , diff:  0.07900835573673248
adv train loss:  -5.410579569637775 , diff:  0.022155437618494034
adv train loss:  -5.267920933663845 , diff:  0.14265863597393036
adv train loss:  -5.386947367340326 , diff:  0.11902643367648125
layer  8  adv train finish, try to retain  411
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -5.183974474668503 , diff:  5.183974474668503
adv train loss:  -5.203945979475975 , diff:  0.01997150480747223
adv train loss:  -5.252054072916508 , diff:  0.048108093440532684
adv train loss:  -5.2342835776507854 , diff:  0.017770495265722275
adv train loss:  -5.209805641323328 , diff:  0.024477936327457428
adv train loss:  -5.094487350434065 , diff:  0.11531829088926315
adv train loss:  -5.317707922309637 , diff:  0.2232205718755722
adv train loss:  -5.264126587659121 , diff:  0.05358133465051651
adv train loss:  -5.180100165307522 , diff:  0.08402642235159874
adv train loss:  -5.189478673040867 , diff:  0.009378507733345032
layer  9  adv train finish, try to retain  480
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
### skip layer  10 wait:  3  ###
---------------- start layer  11  ---------------
### skip layer  11 wait:  4  ###
---------------- start layer  12  ---------------
adv train loss:  -4.107107371091843 , diff:  4.107107371091843
adv train loss:  -4.117501463741064 , diff:  0.01039409264922142
adv train loss:  -4.063213095068932 , diff:  0.05428836867213249
adv train loss:  -4.200054939836264 , diff:  0.13684184476733208
adv train loss:  -4.170652333647013 , diff:  0.029402606189250946
adv train loss:  -4.159112984314561 , diff:  0.01153934933245182
adv train loss:  -4.1809914987534285 , diff:  0.02187851443886757
adv train loss:  -4.099263848736882 , diff:  0.08172765001654625
adv train loss:  -4.091449543833733 , diff:  0.007814304903149605
layer  12  adv train finish, try to retain  506
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
### skip layer  13 wait:  3  ###
layer  0  :  0.484375  ==>  31 / 64 , inc:  1
layer  1  :  0.875  ==>  56 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.142578125  ==>  73 / 512 , inc:  1
layer  9  :  0.1875  ==>  96 / 512 , inc:  2
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.16796875  ==>  86 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.083984375  ==>  43 / 512 , inc:  1
eps [1.2301875000000002, 0.0005068216323852539, 0.057665039062500006, 0.057665039062500006, 0.057665039062500006, 1.2301875000000002, 1.2301875000000002, 0.057665039062500006, 0.0005068216323852539, 0.00012670540809631348, 0.057665039062500006, 1.2301875000000002, 0.0005068216323852539, 6.561000000000001]  wait [4, 2, 2, 2, 2, 3, 3, 2, 2, 0, 2, 3, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]  tol: 3
$$$$$$$$$$$$$ epoch  31  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
adv train loss:  -3.74378321133554 , diff:  3.74378321133554
adv train loss:  -3.6537210159003735 , diff:  0.09006219543516636
adv train loss:  -3.7463980726897717 , diff:  0.0926770567893982
adv train loss:  -3.641518710181117 , diff:  0.1048793625086546
adv train loss:  -3.6310453228652477 , diff:  0.010473387315869331
adv train loss:  -3.724436266347766 , diff:  0.0933909434825182
adv train loss:  -3.641091810539365 , diff:  0.08334445580840111
adv train loss:  -3.674995021894574 , diff:  0.03390321135520935
adv train loss:  -3.6744326278567314 , diff:  0.0005623940378427505
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -3.681418750435114 , diff:  3.681418750435114
adv train loss:  -3.634079296141863 , diff:  0.04733945429325104
adv train loss:  -3.691711112856865 , diff:  0.05763181671500206
adv train loss:  -3.6606348734349012 , diff:  0.03107623942196369
adv train loss:  -3.6821504440158606 , diff:  0.02151557058095932
adv train loss:  -3.666298655793071 , diff:  0.015851788222789764
adv train loss:  -3.7463326640427113 , diff:  0.08003400824964046
adv train loss:  -3.7476407065987587 , diff:  0.0013080425560474396
layer  2  adv train finish, try to retain  127
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -3.658484546467662 , diff:  3.658484546467662
adv train loss:  -3.703184351325035 , diff:  0.04469980485737324
adv train loss:  -3.661606853827834 , diff:  0.041577497497200966
adv train loss:  -3.75765741802752 , diff:  0.09605056419968605
adv train loss:  -3.6789475604891777 , diff:  0.07870985753834248
adv train loss:  -3.680884215980768 , diff:  0.0019366554915904999
layer  3  adv train finish, try to retain  128
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -3.6777363419532776 , diff:  3.6777363419532776
adv train loss:  -3.6809725612401962 , diff:  0.00323621928691864
layer  4  adv train finish, try to retain  256
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  3  ###
---------------- start layer  7  ---------------
adv train loss:  -4.670478954911232 , diff:  4.670478954911232
adv train loss:  -4.662170831114054 , diff:  0.008308123797178268
layer  7  adv train finish, try to retain  405
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -5.463172167539597 , diff:  5.463172167539597
adv train loss:  -5.367976933717728 , diff:  0.0951952338218689
adv train loss:  -5.360646426677704 , diff:  0.007330507040023804
layer  8  adv train finish, try to retain  419
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -5.284731756895781 , diff:  5.284731756895781
adv train loss:  -5.209564361721277 , diff:  0.07516739517450333
adv train loss:  -5.211289044469595 , diff:  0.0017246827483177185
layer  9  adv train finish, try to retain  466
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -7.637394890189171 , diff:  7.637394890189171
adv train loss:  -7.723188910633326 , diff:  0.08579402044415474
adv train loss:  -7.752900939434767 , diff:  0.029712028801441193
adv train loss:  -7.7130980640649796 , diff:  0.039802875369787216
adv train loss:  -7.478528033941984 , diff:  0.23457003012299538
adv train loss:  -7.680797591805458 , diff:  0.2022695578634739
adv train loss:  -7.628246136009693 , diff:  0.05255145579576492
adv train loss:  -7.562191974371672 , diff:  0.06605416163802147
adv train loss:  -7.6061623096466064 , diff:  0.04397033527493477
adv train loss:  -7.691960737109184 , diff:  0.08579842746257782
layer  10  adv train finish, try to retain  468
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
### skip layer  11 wait:  3  ###
---------------- start layer  12  ---------------
adv train loss:  -4.1290218234062195 , diff:  4.1290218234062195
adv train loss:  -4.088799804449081 , diff:  0.04022201895713806
adv train loss:  -4.155551904812455 , diff:  0.06675210036337376
adv train loss:  -4.132589157670736 , diff:  0.022962747141718864
adv train loss:  -4.106348905712366 , diff:  0.02624025195837021
adv train loss:  -4.171437129378319 , diff:  0.06508822366595268
adv train loss:  -4.178750146180391 , diff:  0.007313016802072525
layer  12  adv train finish, try to retain  506
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -5080.417713165283 , diff:  5080.417713165283
adv train loss:  -8482.58765411377 , diff:  3402.1699409484863
adv train loss:  -12327.809310913086 , diff:  3845.2216567993164
adv train loss:  -16023.751831054688 , diff:  3695.9425201416016
adv train loss:  -19552.617141723633 , diff:  3528.8653106689453
adv train loss:  -21843.324310302734 , diff:  2290.7071685791016
adv train loss:  -22568.940383911133 , diff:  725.6160736083984
adv train loss:  -22858.161560058594 , diff:  289.22117614746094
adv train loss:  -23039.427032470703 , diff:  181.26547241210938
adv train loss:  -23118.346939086914 , diff:  78.91990661621094
layer  13  adv train finish, try to retain  35
test acc: top1 ->  36.66 ; top5 ->  78.99  and loss:  534.7003421783447
forward train acc: top1 ->  90.562 ; top5 ->  98.496  and loss:  51.69642882794142
test acc: top1 ->  91.53 ; top5 ->  99.01  and loss:  35.35314653068781
forward train acc: top1 ->  99.73 ; top5 ->  99.998  and loss:  1.3090571388602257
test acc: top1 ->  91.94 ; top5 ->  99.07  and loss:  38.92971163243055
forward train acc: top1 ->  99.886 ; top5 ->  100.0  and loss:  0.6568197677843273
test acc: top1 ->  92.1 ; top5 ->  99.09  and loss:  41.884162686765194
forward train acc: top1 ->  99.86799997558593 ; top5 ->  100.0  and loss:  0.5808115322142839
test acc: top1 ->  92.23 ; top5 ->  99.11  and loss:  44.362990736961365
==> this epoch:  35 / 512
layer  0  :  0.484375  ==>  31 / 64 , inc:  1
layer  1  :  0.875  ==>  56 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.5625  ==>  144 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.142578125  ==>  73 / 512 , inc:  1
layer  9  :  0.1875  ==>  96 / 512 , inc:  2
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.16796875  ==>  86 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.068359375  ==>  35 / 512 , inc:  2
eps [1.2301875000000002, 0.0010136432647705079, 0.11533007812500001, 0.11533007812500001, 0.11533007812500001, 1.2301875000000002, 1.2301875000000002, 0.11533007812500001, 0.0010136432647705079, 0.00025341081619262697, 0.11533007812500001, 1.2301875000000002, 0.0010136432647705079, 6.561000000000001]  wait [3, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2]  tol: 3
$$$$$$$$$$$$$ epoch  32  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
adv train loss:  -0.4309510716702789 , diff:  0.4309510716702789
adv train loss:  -0.423565567471087 , diff:  0.007385504199191928
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  55
test acc: top1 ->  66.58 ; top5 ->  90.7  and loss:  186.73045766353607
forward train acc: top1 ->  99.46999997802735 ; top5 ->  99.994  and loss:  1.6957887357566506
test acc: top1 ->  91.55 ; top5 ->  99.02  and loss:  51.96204616129398
forward train acc: top1 ->  99.65999997558593 ; top5 ->  99.996  and loss:  1.151322462130338
test acc: top1 ->  91.65 ; top5 ->  99.1  and loss:  51.57613284885883
forward train acc: top1 ->  99.69 ; top5 ->  99.996  and loss:  0.9076160518452525
test acc: top1 ->  91.53 ; top5 ->  99.19  and loss:  53.485836781561375
forward train acc: top1 ->  99.7580000024414 ; top5 ->  99.998  and loss:  0.7499866706784815
test acc: top1 ->  91.63 ; top5 ->  99.2  and loss:  53.29394952952862
forward train acc: top1 ->  99.75999997558594 ; top5 ->  99.998  and loss:  0.6966462542768568
test acc: top1 ->  91.72 ; top5 ->  99.19  and loss:  54.01849181950092
forward train acc: top1 ->  99.752 ; top5 ->  99.998  and loss:  0.7794691183371469
test acc: top1 ->  91.7 ; top5 ->  99.07  and loss:  53.178804986178875
forward train acc: top1 ->  99.82199997558594 ; top5 ->  100.0  and loss:  0.5097293303115293
test acc: top1 ->  91.82 ; top5 ->  99.19  and loss:  53.87409824132919
forward train acc: top1 ->  99.78 ; top5 ->  100.0  and loss:  0.6185812501935288
test acc: top1 ->  91.79 ; top5 ->  99.21  and loss:  53.67587722837925
forward train acc: top1 ->  99.83 ; top5 ->  100.0  and loss:  0.48368954221950844
test acc: top1 ->  91.81 ; top5 ->  99.22  and loss:  55.008091889321804
forward train acc: top1 ->  99.81 ; top5 ->  100.0  and loss:  0.5447633813600987
test acc: top1 ->  91.88 ; top5 ->  99.21  and loss:  54.835344806313515
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  56 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -0.2103714457480237 , diff:  0.2103714457480237
adv train loss:  -0.2913619027240202 , diff:  0.0809904569759965
adv train loss:  -0.3016044945688918 , diff:  0.01024259184487164
adv train loss:  -0.2642399712931365 , diff:  0.037364523275755346
adv train loss:  -0.2862634096236434 , diff:  0.022023438330506906
adv train loss:  -0.23782124288845807 , diff:  0.04844216673518531
adv train loss:  -0.25884800910716876 , diff:  0.02102676621871069
adv train loss:  -0.29064311580441426 , diff:  0.0317951066972455
adv train loss:  -0.28024358104448766 , diff:  0.01039953475992661
adv train loss:  -0.26987676799762994 , diff:  0.010366813046857715
layer  2  adv train finish, try to retain  128
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -0.31535820045974106 , diff:  0.31535820045974106
adv train loss:  -0.28609366243472323 , diff:  0.029264538025017828
adv train loss:  -0.30413376254728064 , diff:  0.01804010011255741
adv train loss:  -0.2582499642157927 , diff:  0.045883798331487924
adv train loss:  -0.23840993375051767 , diff:  0.01984003046527505
adv train loss:  -0.26528854423668236 , diff:  0.02687861048616469
adv train loss:  -0.2683178238803521 , diff:  0.0030292796436697245
layer  3  adv train finish, try to retain  128
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -0.2715738343031262 , diff:  0.2715738343031262
adv train loss:  -0.27715167170390487 , diff:  0.005577837400778662
layer  4  adv train finish, try to retain  254
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -588.5832272502594 , diff:  588.5832272502594
adv train loss:  -1905.151023864746 , diff:  1316.5677966144867
adv train loss:  -1937.9422340393066 , diff:  32.79121017456055
adv train loss:  -1976.596103668213 , diff:  38.65386962890625
adv train loss:  -1972.6454963684082 , diff:  3.9506072998046875
adv train loss:  -1983.1442241668701 , diff:  10.498727798461914
adv train loss:  -1976.6104850769043 , diff:  6.53373908996582
adv train loss:  -1998.4997901916504 , diff:  21.889305114746094
adv train loss:  -1990.5810623168945 , diff:  7.918727874755859
adv train loss:  -1998.7420616149902 , diff:  8.160999298095703
layer  5  adv train finish, try to retain  132
test acc: top1 ->  47.6 ; top5 ->  86.91  and loss:  2854.612823486328
forward train acc: top1 ->  99.66600000244141 ; top5 ->  99.996  and loss:  1.0554048536578193
test acc: top1 ->  91.53 ; top5 ->  99.1  and loss:  52.347336530685425
forward train acc: top1 ->  99.74 ; top5 ->  100.0  and loss:  0.698297506722156
test acc: top1 ->  91.63 ; top5 ->  99.27  and loss:  52.442820727825165
forward train acc: top1 ->  99.826 ; top5 ->  100.0  and loss:  0.5524535970762372
test acc: top1 ->  91.67 ; top5 ->  99.18  and loss:  54.13172818720341
forward train acc: top1 ->  99.848 ; top5 ->  100.0  and loss:  0.4399225463857874
test acc: top1 ->  91.72 ; top5 ->  99.15  and loss:  56.12428238987923
forward train acc: top1 ->  99.884 ; top5 ->  100.0  and loss:  0.37992319103796035
test acc: top1 ->  91.63 ; top5 ->  99.1  and loss:  59.203938871622086
forward train acc: top1 ->  99.87999997558593 ; top5 ->  99.998  and loss:  0.35395713441539556
test acc: top1 ->  91.82 ; top5 ->  99.11  and loss:  58.81387774646282
forward train acc: top1 ->  99.886 ; top5 ->  100.0  and loss:  0.30132303736172616
test acc: top1 ->  91.83 ; top5 ->  99.16  and loss:  58.602681040763855
forward train acc: top1 ->  99.918 ; top5 ->  100.0  and loss:  0.2649246034998214
test acc: top1 ->  91.82 ; top5 ->  99.13  and loss:  58.65840396285057
forward train acc: top1 ->  99.896 ; top5 ->  100.0  and loss:  0.28761636739363894
test acc: top1 ->  91.82 ; top5 ->  99.1  and loss:  58.93931132555008
forward train acc: top1 ->  99.88399997558594 ; top5 ->  100.0  and loss:  0.30983040228602476
test acc: top1 ->  91.89 ; top5 ->  99.17  and loss:  59.25049069523811
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -635.9282702122 , diff:  635.9282702122
adv train loss:  -2049.7434787750244 , diff:  1413.8152085628244
adv train loss:  -2075.273275375366 , diff:  25.529796600341797
adv train loss:  -2090.28501701355 , diff:  15.011741638183594
adv train loss:  -2094.2050495147705 , diff:  3.920032501220703
adv train loss:  -2098.7373657226562 , diff:  4.532316207885742
adv train loss:  -2102.9970626831055 , diff:  4.259696960449219
adv train loss:  -2107.8858013153076 , diff:  4.888738632202148
adv train loss:  -2108.961669921875 , diff:  1.0758686065673828
adv train loss:  -2099.065809249878 , diff:  9.89586067199707
layer  6  adv train finish, try to retain  118
test acc: top1 ->  60.38 ; top5 ->  94.07  and loss:  7472.300411224365
forward train acc: top1 ->  99.89799997802734 ; top5 ->  100.0  and loss:  0.3274045748403296
test acc: top1 ->  92.03 ; top5 ->  99.2  and loss:  61.71188327670097
forward train acc: top1 ->  99.904 ; top5 ->  100.0  and loss:  0.282215596329479
test acc: top1 ->  92.13 ; top5 ->  99.1  and loss:  61.32382030785084
==> this epoch:  118 / 256
---------------- start layer  7  ---------------
adv train loss:  -0.2645233930670656 , diff:  0.2645233930670656
adv train loss:  -0.3106849706819048 , diff:  0.04616157761483919
adv train loss:  -0.3254853234102484 , diff:  0.014800352728343569
adv train loss:  -0.26295290832058527 , diff:  0.06253241508966312
adv train loss:  -0.3946200445643626 , diff:  0.13166713624377735
adv train loss:  -0.34377551803481765 , diff:  0.050844526529544964
adv train loss:  -0.3059159642434679 , diff:  0.037859553791349754
adv train loss:  -0.30533521343022585 , diff:  0.000580750813242048
layer  7  adv train finish, try to retain  406
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -0.7615757789462805 , diff:  0.7615757789462805
adv train loss:  -0.6676190986763686 , diff:  0.09395668026991189
adv train loss:  -0.7464820735622197 , diff:  0.07886297488585114
adv train loss:  -0.8173578226706013 , diff:  0.07087574910838157
adv train loss:  -0.745335049694404 , diff:  0.0720227729761973
adv train loss:  -0.6767900123377331 , diff:  0.06854503735667095
adv train loss:  -0.615277634235099 , diff:  0.0615123781026341
adv train loss:  -0.7215522341430187 , diff:  0.10627459990791976
adv train loss:  -0.7551115518435836 , diff:  0.03355931770056486
adv train loss:  -0.7103564946446568 , diff:  0.044755057198926806
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  72
test acc: top1 ->  11.53 ; top5 ->  61.29  and loss:  771.9584674835205
forward train acc: top1 ->  70.81000000244141 ; top5 ->  94.978  and loss:  138.04942286014557
test acc: top1 ->  84.96 ; top5 ->  98.95  and loss:  52.307815462350845
forward train acc: top1 ->  96.26599999023438 ; top5 ->  99.96  and loss:  16.12415011227131
test acc: top1 ->  88.76 ; top5 ->  99.08  and loss:  44.90866473317146
forward train acc: top1 ->  98.24399997558594 ; top5 ->  99.976  and loss:  6.853267677128315
test acc: top1 ->  89.66 ; top5 ->  99.15  and loss:  43.401228457689285
forward train acc: top1 ->  98.99800000488281 ; top5 ->  99.988  and loss:  3.9766553230583668
test acc: top1 ->  90.27 ; top5 ->  99.1  and loss:  44.51685281097889
forward train acc: top1 ->  99.2600000024414 ; top5 ->  99.994  and loss:  2.737698085606098
test acc: top1 ->  90.56 ; top5 ->  99.1  and loss:  45.937019392848015
forward train acc: top1 ->  99.38599997558593 ; top5 ->  99.99  and loss:  2.3057869505137205
test acc: top1 ->  90.7 ; top5 ->  99.17  and loss:  46.80408684909344
forward train acc: top1 ->  99.424 ; top5 ->  99.988  and loss:  2.043130129110068
test acc: top1 ->  90.79 ; top5 ->  99.12  and loss:  46.60032320022583
forward train acc: top1 ->  99.46599997558594 ; top5 ->  99.992  and loss:  1.7920473720878363
test acc: top1 ->  90.72 ; top5 ->  99.14  and loss:  47.785417437553406
forward train acc: top1 ->  99.56199997802734 ; top5 ->  99.996  and loss:  1.5151684074662626
test acc: top1 ->  90.91 ; top5 ->  99.2  and loss:  48.13248498737812
forward train acc: top1 ->  99.55799997558594 ; top5 ->  100.0  and loss:  1.4717826405540109
test acc: top1 ->  90.9 ; top5 ->  99.25  and loss:  47.99872782826424
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  73 / 512 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -3.313050896860659 , diff:  3.313050896860659
adv train loss:  -3.4862794091459364 , diff:  0.17322851228527725
adv train loss:  -3.5469855749979615 , diff:  0.06070616585202515
adv train loss:  -3.9224565369077027 , diff:  0.37547096190974116
adv train loss:  -4.274719761218876 , diff:  0.35226322431117296
adv train loss:  -3.6570760859176517 , diff:  0.617643675301224
adv train loss:  -3.7242674748413265 , diff:  0.06719138892367482
adv train loss:  -3.5981791825033724 , diff:  0.12608829233795404
adv train loss:  -3.8427862096577883 , diff:  0.24460702715441585
adv train loss:  -4.174104746431112 , diff:  0.331318536773324
layer  9  adv train finish, try to retain  465
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -3.895797065459192 , diff:  3.895797065459192
adv train loss:  -4.324404692742974 , diff:  0.42860762728378177
adv train loss:  -4.243923903442919 , diff:  0.08048078930005431
adv train loss:  -4.028733399230987 , diff:  0.21519050421193242
adv train loss:  -3.6810879381373525 , diff:  0.34764546109363437
adv train loss:  -3.960632372647524 , diff:  0.2795444345101714
adv train loss:  -3.9382784329354763 , diff:  0.022353939712047577
adv train loss:  -3.467921486357227 , diff:  0.47035694657824934
adv train loss:  -3.773871730081737 , diff:  0.3059502437245101
adv train loss:  -3.9888057000935078 , diff:  0.21493397001177073
layer  10  adv train finish, try to retain  475
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -17.76739326119423 , diff:  17.76739326119423
adv train loss:  -28.20616589486599 , diff:  10.43877263367176
adv train loss:  -53.760419964790344 , diff:  25.554254069924355
adv train loss:  -109.53439432382584 , diff:  55.77397435903549
adv train loss:  -147.5021914243698 , diff:  37.967797100543976
adv train loss:  -148.61993026733398 , diff:  1.1177388429641724
adv train loss:  -180.2938756942749 , diff:  31.673945426940918
adv train loss:  -206.2304619550705 , diff:  25.936586260795593
adv train loss:  -255.2852132320404 , diff:  49.05475127696991
adv train loss:  -357.2374761104584 , diff:  101.95226287841797
layer  11  adv train finish, try to retain  28
test acc: top1 ->  28.6 ; top5 ->  77.93  and loss:  2987.597858428955
forward train acc: top1 ->  96.554 ; top5 ->  99.998  and loss:  17.558051832718775
test acc: top1 ->  91.59 ; top5 ->  99.12  and loss:  48.884450525045395
forward train acc: top1 ->  99.81 ; top5 ->  100.0  and loss:  0.836036421591416
test acc: top1 ->  91.69 ; top5 ->  99.06  and loss:  49.51887218654156
forward train acc: top1 ->  99.82799997558594 ; top5 ->  100.0  and loss:  0.6750843287445605
test acc: top1 ->  91.91 ; top5 ->  99.02  and loss:  51.10892766714096
forward train acc: top1 ->  99.8860000024414 ; top5 ->  100.0  and loss:  0.5190316140651703
test acc: top1 ->  92.07 ; top5 ->  99.03  and loss:  52.205044358968735
forward train acc: top1 ->  99.898 ; top5 ->  100.0  and loss:  0.36828387016430497
test acc: top1 ->  92.05 ; top5 ->  98.94  and loss:  53.497008085250854
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.2746975304908119
test acc: top1 ->  92.1 ; top5 ->  98.94  and loss:  53.89273148775101
forward train acc: top1 ->  99.91199997558594 ; top5 ->  100.0  and loss:  0.31780682830139995
test acc: top1 ->  92.07 ; top5 ->  98.93  and loss:  54.7232603430748
forward train acc: top1 ->  99.898 ; top5 ->  100.0  and loss:  0.33578846941236407
test acc: top1 ->  92.11 ; top5 ->  98.92  and loss:  55.38601786643267
==> this epoch:  28 / 512
---------------- start layer  12  ---------------
adv train loss:  -141.70345413684845 , diff:  141.70345413684845
adv train loss:  -141.75599259138107 , diff:  0.05253845453262329
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  40
test acc: top1 ->  18.35 ; top5 ->  50.0  and loss:  2095.3423652648926
forward train acc: top1 ->  75.05600001953125 ; top5 ->  97.618  and loss:  88.67217573523521
test acc: top1 ->  86.29 ; top5 ->  98.42  and loss:  58.60284870862961
forward train acc: top1 ->  98.18999998046876 ; top5 ->  99.996  and loss:  16.98759748786688
test acc: top1 ->  90.28 ; top5 ->  98.58  and loss:  45.79500864446163
forward train acc: top1 ->  99.5320000024414 ; top5 ->  99.994  and loss:  8.271382961422205
test acc: top1 ->  90.47 ; top5 ->  98.58  and loss:  43.06194196641445
forward train acc: top1 ->  99.71000000488282 ; top5 ->  100.0  and loss:  4.824092913419008
test acc: top1 ->  90.53 ; top5 ->  98.47  and loss:  43.55520501732826
forward train acc: top1 ->  99.7640000024414 ; top5 ->  99.996  and loss:  3.3652739357203245
test acc: top1 ->  90.62 ; top5 ->  98.44  and loss:  44.65329824388027
forward train acc: top1 ->  99.78799997558593 ; top5 ->  100.0  and loss:  2.6640565805137157
test acc: top1 ->  90.87 ; top5 ->  98.37  and loss:  45.3283616900444
forward train acc: top1 ->  99.78799997558593 ; top5 ->  99.998  and loss:  2.3722195979207754
test acc: top1 ->  90.88 ; top5 ->  98.35  and loss:  45.86848156154156
forward train acc: top1 ->  99.81 ; top5 ->  99.994  and loss:  2.140989577397704
test acc: top1 ->  90.9 ; top5 ->  98.34  and loss:  46.329761534929276
forward train acc: top1 ->  99.83399997558594 ; top5 ->  100.0  and loss:  1.8126872964203358
test acc: top1 ->  90.93 ; top5 ->  98.26  and loss:  47.0796265155077
forward train acc: top1 ->  99.8280000024414 ; top5 ->  99.998  and loss:  1.6865363726392388
test acc: top1 ->  91.04 ; top5 ->  98.28  and loss:  47.414150923490524
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  41 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -9424.098209381104 , diff:  9424.098209381104
adv train loss:  -19942.52508544922 , diff:  10518.426876068115
adv train loss:  -35793.20376586914 , diff:  15850.678680419922
adv train loss:  -53445.58630371094 , diff:  17652.382537841797
adv train loss:  -65102.55322265625 , diff:  11656.966918945312
adv train loss:  -70555.56066894531 , diff:  5453.0074462890625
adv train loss:  -73179.95764160156 , diff:  2624.39697265625
adv train loss:  -74681.16973876953 , diff:  1501.2120971679688
adv train loss:  -75854.60571289062 , diff:  1173.4359741210938
adv train loss:  -76837.05114746094 , diff:  982.4454345703125
layer  13  adv train finish, try to retain  37
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.484375  ==>  31 / 64 , inc:  1
layer  1  :  0.875  ==>  56 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.4609375  ==>  118 / 256 , inc:  2
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.142578125  ==>  73 / 512 , inc:  1
layer  9  :  0.1875  ==>  96 / 512 , inc:  2
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.0546875  ==>  28 / 512 , inc:  2
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.068359375  ==>  35 / 512 , inc:  2
eps [1.2301875000000002, 0.0007602324485778809, 0.23066015625000003, 0.23066015625000003, 0.23066015625000003, 0.9226406250000001, 1.2301875000000002, 0.23066015625000003, 0.0007602324485778809, 0.0005068216323852539, 0.23066015625000003, 1.2301875000000002, 0.0007602324485778809, 13.122000000000002]  wait [2, 4, 2, 2, 2, 4, 0, 2, 4, 0, 2, 0, 4, 0]  inc [1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2]  tol: 3
$$$$$$$$$$$$$ epoch  33  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1511.7702541351318 , diff:  1511.7702541351318
adv train loss:  -1521.5087060928345 , diff:  9.738451957702637
adv train loss:  -1522.7873058319092 , diff:  1.278599739074707
adv train loss:  -1534.712529182434 , diff:  11.925223350524902
adv train loss:  -1525.9911642074585 , diff:  8.721364974975586
adv train loss:  -1526.1827983856201 , diff:  0.1916341781616211
layer  0  adv train finish, try to retain  14
test acc: top1 ->  11.68 ; top5 ->  48.52  and loss:  1667.3410367965698
forward train acc: top1 ->  91.36 ; top5 ->  99.198  and loss:  61.972424402832985
test acc: top1 ->  85.08 ; top5 ->  97.46  and loss:  99.84759652614594
forward train acc: top1 ->  94.42599999755859 ; top5 ->  99.57999997558593  and loss:  23.082280725240707
test acc: top1 ->  87.32 ; top5 ->  98.46  and loss:  59.79869222640991
forward train acc: top1 ->  95.18000001953125 ; top5 ->  99.71999997558594  and loss:  16.884767435491085
test acc: top1 ->  87.74 ; top5 ->  98.59  and loss:  51.44303499162197
forward train acc: top1 ->  95.71400001464843 ; top5 ->  99.774  and loss:  14.555856823921204
test acc: top1 ->  88.08 ; top5 ->  98.73  and loss:  49.12340900301933
forward train acc: top1 ->  95.87199999023437 ; top5 ->  99.82199997558594  and loss:  13.164500243961811
test acc: top1 ->  88.29 ; top5 ->  98.75  and loss:  47.33134187757969
forward train acc: top1 ->  96.04999999267578 ; top5 ->  99.862  and loss:  11.980325661599636
test acc: top1 ->  88.41 ; top5 ->  98.81  and loss:  47.07490935921669
forward train acc: top1 ->  96.57199998535157 ; top5 ->  99.87200000244141  and loss:  10.92951725050807
test acc: top1 ->  88.44 ; top5 ->  98.81  and loss:  46.7119000852108
forward train acc: top1 ->  96.55000001953125 ; top5 ->  99.858  and loss:  10.76046846061945
test acc: top1 ->  88.51 ; top5 ->  98.81  and loss:  46.677250161767006
forward train acc: top1 ->  96.68599998535156 ; top5 ->  99.896  and loss:  10.005113322287798
test acc: top1 ->  88.7 ; top5 ->  98.83  and loss:  46.35494887828827
forward train acc: top1 ->  96.73400001220703 ; top5 ->  99.87799997558594  and loss:  10.299903430044651
test acc: top1 ->  88.81 ; top5 ->  98.79  and loss:  45.55558079481125
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  31 / 64 , inc:  1
---------------- start layer  1  ---------------
### skip layer  1 wait:  4  ###
---------------- start layer  2  ---------------
adv train loss:  -1.0211725202389061 , diff:  1.0211725202389061
adv train loss:  -1.0689987121149898 , diff:  0.04782619187608361
adv train loss:  -1.1220017466694117 , diff:  0.0530030345544219
adv train loss:  -1.0954609084874392 , diff:  0.026540838181972504
adv train loss:  -1.0315696420148015 , diff:  0.06389126647263765
adv train loss:  -1.0932557783089578 , diff:  0.06168613629415631
adv train loss:  -1.0942942406982183 , diff:  0.0010384623892605305
layer  2  adv train finish, try to retain  127
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -1.0641784220933914 , diff:  1.0641784220933914
adv train loss:  -1.1116822017356753 , diff:  0.047503779642283916
adv train loss:  -1.069206661079079 , diff:  0.04247554065659642
adv train loss:  -1.074181825388223 , diff:  0.00497516430914402
layer  3  adv train finish, try to retain  128
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -1.0609295088797808 , diff:  1.0609295088797808
adv train loss:  -1.1197826326824725 , diff:  0.0588531238026917
adv train loss:  -1.0881957681849599 , diff:  0.03158686449751258
adv train loss:  -1.057828564196825 , diff:  0.03036720398813486
adv train loss:  -1.0910044754855335 , diff:  0.03317591128870845
adv train loss:  -1.0556465503759682 , diff:  0.03535792510956526
adv train loss:  -1.0244268109090626 , diff:  0.031219739466905594
adv train loss:  -1.0539775188080966 , diff:  0.029550707899034023
adv train loss:  -1.0443331226706505 , diff:  0.009644396137446165
layer  4  adv train finish, try to retain  254
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
adv train loss:  -467.9355031340383 , diff:  467.9355031340383
adv train loss:  -1544.6482038497925 , diff:  1076.7127007157542
adv train loss:  -1580.8370990753174 , diff:  36.1888952255249
adv train loss:  -1588.7650260925293 , diff:  7.927927017211914
adv train loss:  -1589.6663055419922 , diff:  0.9012794494628906
adv train loss:  -1599.6845397949219 , diff:  10.018234252929688
adv train loss:  -1592.6562194824219 , diff:  7.0283203125
adv train loss:  -1595.4784288406372 , diff:  2.822209358215332
adv train loss:  -1596.0652875900269 , diff:  0.5868587493896484
adv train loss:  -1597.0472507476807 , diff:  0.9819631576538086
layer  6  adv train finish, try to retain  105
test acc: top1 ->  47.96 ; top5 ->  92.22  and loss:  2061.2087144851685
forward train acc: top1 ->  99.8280000024414 ; top5 ->  99.998  and loss:  0.598573130031582
test acc: top1 ->  91.89 ; top5 ->  99.12  and loss:  54.87259902060032
forward train acc: top1 ->  99.876 ; top5 ->  100.0  and loss:  0.3407253693731036
test acc: top1 ->  91.92 ; top5 ->  99.06  and loss:  59.90509995818138
forward train acc: top1 ->  99.8940000024414 ; top5 ->  100.0  and loss:  0.3005643699725624
test acc: top1 ->  91.97 ; top5 ->  99.03  and loss:  62.57897515594959
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.285769269507
test acc: top1 ->  92.07 ; top5 ->  99.07  and loss:  63.438018046319485
forward train acc: top1 ->  99.88 ; top5 ->  100.0  and loss:  0.31542612350312993
test acc: top1 ->  91.9 ; top5 ->  99.16  and loss:  63.412075623869896
forward train acc: top1 ->  99.918 ; top5 ->  100.0  and loss:  0.24070628370827762
test acc: top1 ->  92.14 ; top5 ->  99.13  and loss:  62.427565433084965
==> this epoch:  105 / 256
---------------- start layer  7  ---------------
adv train loss:  -0.46085926942760125 , diff:  0.46085926942760125
adv train loss:  -0.45524541998747736 , diff:  0.005613849440123886
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  103
test acc: top1 ->  10.0 ; top5 ->  50.17  and loss:  821.6309700012207
forward train acc: top1 ->  65.3320000024414 ; top5 ->  97.16999997558594  and loss:  118.32167181372643
test acc: top1 ->  76.8 ; top5 ->  98.29  and loss:  76.21083116531372
forward train acc: top1 ->  88.31200000244141 ; top5 ->  99.778  and loss:  34.39625680446625
test acc: top1 ->  84.37 ; top5 ->  98.51  and loss:  59.28918665647507
forward train acc: top1 ->  92.45199997314454 ; top5 ->  99.896  and loss:  22.074348106980324
test acc: top1 ->  86.18 ; top5 ->  98.7  and loss:  55.95023596286774
forward train acc: top1 ->  94.00600000488281 ; top5 ->  99.924  and loss:  17.341579154133797
test acc: top1 ->  87.05 ; top5 ->  98.72  and loss:  54.2818284034729
forward train acc: top1 ->  95.22800001953125 ; top5 ->  99.9000000024414  and loss:  14.46154760569334
test acc: top1 ->  87.55 ; top5 ->  98.82  and loss:  54.75459614396095
forward train acc: top1 ->  95.61799997070312 ; top5 ->  99.932  and loss:  12.846041806042194
test acc: top1 ->  87.75 ; top5 ->  98.78  and loss:  54.42858934402466
forward train acc: top1 ->  95.92799997070313 ; top5 ->  99.956  and loss:  11.83934435248375
test acc: top1 ->  87.96 ; top5 ->  98.81  and loss:  54.08239644765854
forward train acc: top1 ->  96.08200000976562 ; top5 ->  99.96  and loss:  11.399628445506096
test acc: top1 ->  88.17 ; top5 ->  98.85  and loss:  53.6299904435873
forward train acc: top1 ->  96.40799998535157 ; top5 ->  99.97  and loss:  10.605941139161587
test acc: top1 ->  88.51 ; top5 ->  98.89  and loss:  52.661304980516434
forward train acc: top1 ->  96.52999999023437 ; top5 ->  99.974  and loss:  10.019741475582123
test acc: top1 ->  88.59 ; top5 ->  98.87  and loss:  53.8908404558897
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  104 / 512 , inc:  1
---------------- start layer  8  ---------------
### skip layer  8 wait:  4  ###
---------------- start layer  9  ---------------
adv train loss:  -7.513154946267605 , diff:  7.513154946267605
adv train loss:  -7.786740999668837 , diff:  0.27358605340123177
adv train loss:  -8.106991620734334 , diff:  0.3202506210654974
adv train loss:  -7.879098724573851 , diff:  0.22789289616048336
adv train loss:  -7.805378034710884 , diff:  0.07372068986296654
adv train loss:  -7.639753909781575 , diff:  0.1656241249293089
adv train loss:  -7.90712851844728 , diff:  0.2673746086657047
adv train loss:  -7.573731176555157 , diff:  0.3333973418921232
adv train loss:  -8.094691600650549 , diff:  0.5209604240953922
adv train loss:  -8.045320942997932 , diff:  0.0493706576526165
layer  9  adv train finish, try to retain  471
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -10.610055465251207 , diff:  10.610055465251207
adv train loss:  -10.632671998813748 , diff:  0.022616533562541008
adv train loss:  -10.847179032862186 , diff:  0.21450703404843807
adv train loss:  -10.435245011001825 , diff:  0.4119340218603611
adv train loss:  -10.570206329226494 , diff:  0.1349613182246685
adv train loss:  -10.397561497986317 , diff:  0.17264483124017715
adv train loss:  -10.691243276000023 , diff:  0.2936817780137062
adv train loss:  -10.592012479901314 , diff:  0.0992307960987091
adv train loss:  -10.703766889870167 , diff:  0.111754409968853
adv train loss:  -9.78614324517548 , diff:  0.9176236446946859
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  21
test acc: top1 ->  10.0 ; top5 ->  50.79  and loss:  1442.9884309768677
forward train acc: top1 ->  74.73199998535156 ; top5 ->  92.06599997558594  and loss:  110.63187000155449
test acc: top1 ->  65.09 ; top5 ->  97.15  and loss:  101.71178990602493
forward train acc: top1 ->  98.65200000244141 ; top5 ->  99.978  and loss:  7.64336546510458
test acc: top1 ->  90.42 ; top5 ->  98.04  and loss:  53.90388762950897
forward train acc: top1 ->  99.35399997558594 ; top5 ->  99.988  and loss:  2.916530128568411
test acc: top1 ->  90.66 ; top5 ->  98.18  and loss:  55.32523827254772
forward train acc: top1 ->  99.50000000244141 ; top5 ->  99.988  and loss:  2.175634052604437
test acc: top1 ->  90.68 ; top5 ->  98.29  and loss:  57.07703626155853
forward train acc: top1 ->  99.5580000024414 ; top5 ->  99.994  and loss:  1.7039798200130463
test acc: top1 ->  90.9 ; top5 ->  98.35  and loss:  57.11326511204243
forward train acc: top1 ->  99.68 ; top5 ->  99.996  and loss:  1.239423990715295
test acc: top1 ->  90.87 ; top5 ->  98.4  and loss:  56.63941828906536
forward train acc: top1 ->  99.71399997558593 ; top5 ->  100.0  and loss:  1.0477752555161715
test acc: top1 ->  91.07 ; top5 ->  98.35  and loss:  57.79658651351929
forward train acc: top1 ->  99.73399997558593 ; top5 ->  99.998  and loss:  0.9703094540163875
test acc: top1 ->  91.14 ; top5 ->  98.37  and loss:  57.704809814691544
forward train acc: top1 ->  99.75000000244141 ; top5 ->  99.994  and loss:  0.9085874855518341
test acc: top1 ->  91.12 ; top5 ->  98.37  and loss:  58.9377708286047
forward train acc: top1 ->  99.74599997558593 ; top5 ->  99.996  and loss:  0.9330528303980827
test acc: top1 ->  91.06 ; top5 ->  98.41  and loss:  58.08191351592541
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  22 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -3226.2427139282227 , diff:  3226.2427139282227
adv train loss:  -3447.1901664733887 , diff:  220.94745254516602
adv train loss:  -3474.253646850586 , diff:  27.063480377197266
adv train loss:  -3472.3756828308105 , diff:  1.8779640197753906
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  26
test acc: top1 ->  25.03 ; top5 ->  82.29  and loss:  47101.81784057617
forward train acc: top1 ->  99.2540000024414 ; top5 ->  100.0  and loss:  2.8830962972715497
test acc: top1 ->  91.96 ; top5 ->  98.63  and loss:  91.86671848595142
forward train acc: top1 ->  99.90199997558594 ; top5 ->  100.0  and loss:  0.3282388146035373
test acc: top1 ->  92.0 ; top5 ->  98.69  and loss:  89.60589717328548
forward train acc: top1 ->  99.912 ; top5 ->  100.0  and loss:  0.27749550054431893
test acc: top1 ->  91.99 ; top5 ->  98.7  and loss:  87.30165323615074
forward train acc: top1 ->  99.924 ; top5 ->  100.0  and loss:  0.2432189371320419
test acc: top1 ->  92.04 ; top5 ->  98.63  and loss:  88.81998427212238
forward train acc: top1 ->  99.9 ; top5 ->  100.0  and loss:  0.2320795020496007
test acc: top1 ->  92.11 ; top5 ->  98.82  and loss:  85.3954749405384
==> this epoch:  26 / 512
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
adv train loss:  -7497.447917938232 , diff:  7497.447917938232
adv train loss:  -14293.995880126953 , diff:  6796.547962188721
adv train loss:  -22455.193893432617 , diff:  8161.198013305664
adv train loss:  -37305.65447998047 , diff:  14850.460586547852
adv train loss:  -56435.061767578125 , diff:  19129.407287597656
adv train loss:  -72352.50677490234 , diff:  15917.445007324219
adv train loss:  -86677.9116821289 , diff:  14325.404907226562
adv train loss:  -99899.20556640625 , diff:  13221.293884277344
adv train loss:  -112537.82238769531 , diff:  12638.616821289062
adv train loss:  -122557.35693359375 , diff:  10019.534545898438
layer  13  adv train finish, try to retain  17
test acc: top1 ->  69.46 ; top5 ->  98.03  and loss:  286.13320994377136
forward train acc: top1 ->  96.3440000024414 ; top5 ->  99.996  and loss:  20.302885624580085
test acc: top1 ->  91.45 ; top5 ->  98.28  and loss:  72.59995374083519
forward train acc: top1 ->  99.84799997558594 ; top5 ->  100.0  and loss:  0.7438656520098448
test acc: top1 ->  91.53 ; top5 ->  98.4  and loss:  71.93408878147602
forward train acc: top1 ->  99.874 ; top5 ->  100.0  and loss:  0.49878232297487557
test acc: top1 ->  91.71 ; top5 ->  98.39  and loss:  72.36755143851042
forward train acc: top1 ->  99.898 ; top5 ->  99.998  and loss:  0.4400682288687676
test acc: top1 ->  91.85 ; top5 ->  98.39  and loss:  72.77919315546751
forward train acc: top1 ->  99.928 ; top5 ->  99.998  and loss:  0.30561167164705694
test acc: top1 ->  91.75 ; top5 ->  98.43  and loss:  73.18151786923409
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.24998143839184195
test acc: top1 ->  91.84 ; top5 ->  98.43  and loss:  74.05042335391045
forward train acc: top1 ->  99.93999997558593 ; top5 ->  100.0  and loss:  0.2566602863371372
test acc: top1 ->  91.82 ; top5 ->  98.41  and loss:  73.94954496622086
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.24273295747116208
test acc: top1 ->  91.82 ; top5 ->  98.47  and loss:  73.60920866578817
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.24798704008571804
test acc: top1 ->  91.85 ; top5 ->  98.54  and loss:  74.27055351436138
forward train acc: top1 ->  99.942 ; top5 ->  100.0  and loss:  0.23833252233453095
test acc: top1 ->  91.93 ; top5 ->  98.44  and loss:  75.33014644682407
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  35 / 512 , inc:  2
layer  0  :  0.484375  ==>  31 / 64 , inc:  1
layer  1  :  0.875  ==>  56 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.41015625  ==>  105 / 256 , inc:  4
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.142578125  ==>  73 / 512 , inc:  1
layer  9  :  0.1875  ==>  96 / 512 , inc:  2
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.05078125  ==>  26 / 512 , inc:  4
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.068359375  ==>  35 / 512 , inc:  1
eps [0.9226406250000001, 0.0007602324485778809, 0.46132031250000005, 0.46132031250000005, 0.46132031250000005, 0.9226406250000001, 1.2301875000000002, 0.17299511718750002, 0.0007602324485778809, 0.0010136432647705079, 0.17299511718750002, 1.2301875000000002, 0.0007602324485778809, 9.841500000000002]  wait [4, 3, 2, 2, 2, 3, 0, 4, 3, 0, 4, 0, 3, 2]  inc [1, 1, 1, 1, 1, 1, 4, 1, 1, 2, 1, 4, 1, 1]  tol: 3
$$$$$$$$$$$$$ epoch  34  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
### skip layer  1 wait:  3  ###
---------------- start layer  2  ---------------
adv train loss:  -2.329992196522653 , diff:  2.329992196522653
adv train loss:  -2.2034956514835358 , diff:  0.12649654503911734
adv train loss:  -2.4637608071789145 , diff:  0.2602651556953788
adv train loss:  -2.168609853833914 , diff:  0.29515095334500074
adv train loss:  -2.3806863771751523 , diff:  0.2120765233412385
adv train loss:  -2.3500678641721606 , diff:  0.030618513002991676
adv train loss:  -2.2327071875333786 , diff:  0.11736067663878202
adv train loss:  -2.576057717669755 , diff:  0.3433505301363766
adv train loss:  -2.264076257124543 , diff:  0.31198146054521203
adv train loss:  -2.4067672295495868 , diff:  0.14269097242504358
layer  2  adv train finish, try to retain  127
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -2.110285874456167 , diff:  2.110285874456167
adv train loss:  -2.1514929151162505 , diff:  0.041207040660083294
adv train loss:  -2.3010503398254514 , diff:  0.14955742470920086
adv train loss:  -2.1975135477259755 , diff:  0.10353679209947586
adv train loss:  -2.0504504162818193 , diff:  0.14706313144415617
adv train loss:  -2.117325627245009 , diff:  0.0668752109631896
adv train loss:  -2.3901302432641387 , diff:  0.27280461601912975
adv train loss:  -2.2040049135684967 , diff:  0.186125329695642
adv train loss:  -2.30398522131145 , diff:  0.0999803077429533
adv train loss:  -2.2844129959121346 , diff:  0.019572225399315357
layer  3  adv train finish, try to retain  128
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -2.261950266547501 , diff:  2.261950266547501
adv train loss:  -2.2422171300277114 , diff:  0.019733136519789696
adv train loss:  -2.2589640598744154 , diff:  0.016746929846704006
adv train loss:  -2.1242676344700158 , diff:  0.13469642540439963
adv train loss:  -2.328790576197207 , diff:  0.2045229417271912
adv train loss:  -2.283242695964873 , diff:  0.04554788023233414
adv train loss:  -2.2106533143669367 , diff:  0.07258938159793615
adv train loss:  -2.3862914610654116 , diff:  0.17563814669847488
adv train loss:  -2.487956137396395 , diff:  0.10166467633098364
adv train loss:  -2.2633949918672442 , diff:  0.22456114552915096
layer  4  adv train finish, try to retain  255
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
adv train loss:  -1006.747280722484 , diff:  1006.747280722484
adv train loss:  -3087.5507068634033 , diff:  2080.8034261409193
adv train loss:  -3108.581069946289 , diff:  21.030363082885742
adv train loss:  -3149.8660068511963 , diff:  41.28493690490723
adv train loss:  -3166.0617637634277 , diff:  16.195756912231445
adv train loss:  -3155.922887802124 , diff:  10.138875961303711
adv train loss:  -3165.6107654571533 , diff:  9.687877655029297
adv train loss:  -3158.44708442688 , diff:  7.1636810302734375
adv train loss:  -3166.7755794525146 , diff:  8.328495025634766
adv train loss:  -3172.942434310913 , diff:  6.1668548583984375
layer  6  adv train finish, try to retain  88
test acc: top1 ->  62.96 ; top5 ->  94.84  and loss:  1762.3963050842285
forward train acc: top1 ->  99.756 ; top5 ->  100.0  and loss:  0.8987813311687205
test acc: top1 ->  91.66 ; top5 ->  98.72  and loss:  98.810537815094
forward train acc: top1 ->  99.83799997558594 ; top5 ->  99.998  and loss:  0.5085325792897493
test acc: top1 ->  91.68 ; top5 ->  98.76  and loss:  94.50467500090599
forward train acc: top1 ->  99.85 ; top5 ->  100.0  and loss:  0.5220553987892345
test acc: top1 ->  91.66 ; top5 ->  98.85  and loss:  92.17727300524712
forward train acc: top1 ->  99.87799997558594 ; top5 ->  100.0  and loss:  0.39010506751947105
test acc: top1 ->  91.79 ; top5 ->  98.9  and loss:  89.19848105311394
forward train acc: top1 ->  99.88 ; top5 ->  100.0  and loss:  0.4176989089464769
test acc: top1 ->  91.91 ; top5 ->  98.85  and loss:  89.90087349712849
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.30080347020339104
test acc: top1 ->  91.9 ; top5 ->  98.83  and loss:  89.678009390831
forward train acc: top1 ->  99.88999997558594 ; top5 ->  100.0  and loss:  0.32010208582505584
test acc: top1 ->  91.78 ; top5 ->  98.84  and loss:  88.44514191150665
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.27299041536753066
test acc: top1 ->  91.83 ; top5 ->  98.9  and loss:  86.39575159549713
forward train acc: top1 ->  99.894 ; top5 ->  100.0  and loss:  0.33302180701866746
test acc: top1 ->  91.75 ; top5 ->  98.88  and loss:  86.82865034043789
forward train acc: top1 ->  99.91399997558594 ; top5 ->  100.0  and loss:  0.2924664719030261
test acc: top1 ->  91.86 ; top5 ->  98.91  and loss:  87.67495113611221
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  105 / 256 , inc:  4
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
### skip layer  8 wait:  3  ###
---------------- start layer  9  ---------------
adv train loss:  -0.30027850810438395 , diff:  0.30027850810438395
adv train loss:  -0.27837629267014563 , diff:  0.021902215434238315
adv train loss:  -0.2753419139771722 , diff:  0.003034378692973405
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  94
test acc: top1 ->  10.01 ; top5 ->  64.96  and loss:  1843.7306489944458
forward train acc: top1 ->  99.402 ; top5 ->  99.996  and loss:  1.7854660102166235
test acc: top1 ->  91.42 ; top5 ->  98.33  and loss:  79.59806749224663
forward train acc: top1 ->  99.878 ; top5 ->  100.0  and loss:  0.45506750186905265
test acc: top1 ->  91.69 ; top5 ->  98.32  and loss:  78.22069887816906
forward train acc: top1 ->  99.886 ; top5 ->  99.998  and loss:  0.39819359709508717
test acc: top1 ->  91.86 ; top5 ->  98.39  and loss:  78.2860698401928
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.2245566206984222
test acc: top1 ->  91.83 ; top5 ->  98.38  and loss:  78.95273053646088
forward train acc: top1 ->  99.934 ; top5 ->  99.998  and loss:  0.2716485144919716
test acc: top1 ->  91.9 ; top5 ->  98.45  and loss:  77.2711600214243
forward train acc: top1 ->  99.958 ; top5 ->  99.998  and loss:  0.17880126158706844
test acc: top1 ->  91.97 ; top5 ->  98.49  and loss:  77.3808854073286
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.18494348583044484
test acc: top1 ->  92.02 ; top5 ->  98.55  and loss:  78.03264798223972
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.239959876271314
test acc: top1 ->  92.05 ; top5 ->  98.49  and loss:  77.36498139798641
forward train acc: top1 ->  99.912 ; top5 ->  100.0  and loss:  0.18697755731409416
test acc: top1 ->  92.01 ; top5 ->  98.53  and loss:  77.88980546593666
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.14956753549631685
test acc: top1 ->  92.01 ; top5 ->  98.55  and loss:  79.14682507514954
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  96 / 512 , inc:  2
---------------- start layer  10  ---------------
### skip layer  10 wait:  4  ###
---------------- start layer  11  ---------------
adv train loss:  -586.0511908531189 , diff:  586.0511908531189
adv train loss:  -815.1980834007263 , diff:  229.14689254760742
adv train loss:  -819.8997101783752 , diff:  4.701626777648926
adv train loss:  -873.8231558799744 , diff:  53.92344570159912
adv train loss:  -1346.0419435501099 , diff:  472.2187876701355
adv train loss:  -1350.8980340957642 , diff:  4.856090545654297
adv train loss:  -1411.1290874481201 , diff:  60.23105335235596
adv train loss:  -1430.9234352111816 , diff:  19.794347763061523
adv train loss:  -1426.8253726959229 , diff:  4.098062515258789
adv train loss:  -1636.3746147155762 , diff:  209.54924201965332
layer  11  adv train finish, try to retain  16
test acc: top1 ->  18.89 ; top5 ->  76.21  and loss:  10472.906547546387
forward train acc: top1 ->  99.478 ; top5 ->  99.998  and loss:  1.7452900291827973
test acc: top1 ->  91.92 ; top5 ->  98.87  and loss:  69.29253688454628
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.2102346309402492
test acc: top1 ->  92.02 ; top5 ->  98.78  and loss:  69.16661234200001
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.21304020390380174
test acc: top1 ->  92.02 ; top5 ->  98.87  and loss:  69.73871618509293
forward train acc: top1 ->  99.95399997558594 ; top5 ->  100.0  and loss:  0.14041333319619298
test acc: top1 ->  92.03 ; top5 ->  98.95  and loss:  70.19434258341789
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.15400919737294316
test acc: top1 ->  91.81 ; top5 ->  98.96  and loss:  72.31066296994686
forward train acc: top1 ->  99.95599997558594 ; top5 ->  100.0  and loss:  0.1353013520129025
test acc: top1 ->  92.1 ; top5 ->  98.96  and loss:  73.75181789696217
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.10768961063877214
test acc: top1 ->  92.03 ; top5 ->  99.02  and loss:  73.583290733397
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.131152488735097
test acc: top1 ->  92.02 ; top5 ->  98.96  and loss:  74.17642852663994
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.12154735386138782
test acc: top1 ->  92.11 ; top5 ->  99.01  and loss:  72.63127620518208
==> this epoch:  16 / 512
---------------- start layer  12  ---------------
### skip layer  12 wait:  3  ###
---------------- start layer  13  ---------------
adv train loss:  -10727.902557373047 , diff:  10727.902557373047
adv train loss:  -20736.6173248291 , diff:  10008.714767456055
adv train loss:  -29350.38722229004 , diff:  8613.769897460938
adv train loss:  -37462.361907958984 , diff:  8111.974685668945
adv train loss:  -45336.8850402832 , diff:  7874.523132324219
adv train loss:  -53109.42337036133 , diff:  7772.538330078125
adv train loss:  -60812.620178222656 , diff:  7703.196807861328
adv train loss:  -68140.14379882812 , diff:  7327.523620605469
adv train loss:  -73010.56536865234 , diff:  4870.421569824219
adv train loss:  -74886.97143554688 , diff:  1876.4060668945312
layer  13  adv train finish, try to retain  30
test acc: top1 ->  62.58 ; top5 ->  98.14  and loss:  435.1982721090317
forward train acc: top1 ->  96.664 ; top5 ->  99.998  and loss:  24.459513983223587
test acc: top1 ->  91.95 ; top5 ->  98.96  and loss:  53.19810938835144
forward train acc: top1 ->  99.93599997558594 ; top5 ->  100.0  and loss:  0.3329066466540098
test acc: top1 ->  92.07 ; top5 ->  98.98  and loss:  54.83672084659338
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.2418381158495322
test acc: top1 ->  91.98 ; top5 ->  99.01  and loss:  56.020911142230034
forward train acc: top1 ->  99.958 ; top5 ->  99.998  and loss:  0.2130735218524933
test acc: top1 ->  91.86 ; top5 ->  99.05  and loss:  57.055468171834946
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.20520781551022083
test acc: top1 ->  91.77 ; top5 ->  99.04  and loss:  58.03163640201092
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.1762959063053131
test acc: top1 ->  91.95 ; top5 ->  99.13  and loss:  58.11079567670822
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.187046543112956
test acc: top1 ->  91.93 ; top5 ->  99.08  and loss:  58.533620432019234
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.16818437229085248
test acc: top1 ->  91.91 ; top5 ->  99.04  and loss:  58.157763451337814
forward train acc: top1 ->  99.94 ; top5 ->  100.0  and loss:  0.19836332771228626
test acc: top1 ->  91.86 ; top5 ->  99.06  and loss:  59.31395502388477
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.14022125233896077
test acc: top1 ->  92.02 ; top5 ->  99.09  and loss:  59.7811025083065
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  35 / 512 , inc:  1
layer  0  :  0.484375  ==>  31 / 64 , inc:  1
layer  1  :  0.875  ==>  56 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.41015625  ==>  105 / 256 , inc:  2
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.142578125  ==>  73 / 512 , inc:  1
layer  9  :  0.1875  ==>  96 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.03125  ==>  16 / 512 , inc:  8
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.068359375  ==>  35 / 512 , inc:  1
eps [0.9226406250000001, 0.0007602324485778809, 0.9226406250000001, 0.9226406250000001, 0.9226406250000001, 0.9226406250000001, 0.9226406250000001, 0.17299511718750002, 0.0007602324485778809, 0.0007602324485778809, 0.17299511718750002, 1.2301875000000002, 0.0007602324485778809, 7.381125000000001]  wait [3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 0, 2, 4]  inc [1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 8, 1, 1]  tol: 3
$$$$$$$$$$$$$ epoch  35  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
adv train loss:  -54.39401450753212 , diff:  54.39401450753212
adv train loss:  -54.289676666259766 , diff:  0.10433784127235413
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -54.69137018918991 , diff:  54.69137018918991
adv train loss:  -54.78460890054703 , diff:  0.0932387113571167
layer  2  adv train finish, try to retain  122
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -54.74427151679993 , diff:  54.74427151679993
adv train loss:  -54.56145861744881 , diff:  0.18281289935112
adv train loss:  -54.92290261387825 , diff:  0.36144399642944336
adv train loss:  -55.06110268831253 , diff:  0.1382000744342804
adv train loss:  -54.65310826897621 , diff:  0.40799441933631897
adv train loss:  -54.845538437366486 , diff:  0.19243016839027405
adv train loss:  -53.968522518873215 , diff:  0.8770159184932709
adv train loss:  -54.36442467570305 , diff:  0.395902156829834
adv train loss:  -54.65374606847763 , diff:  0.2893213927745819
adv train loss:  -54.78979870676994 , diff:  0.13605263829231262
layer  3  adv train finish, try to retain  124
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -54.832056909799576 , diff:  54.832056909799576
adv train loss:  -54.38718166947365 , diff:  0.44487524032592773
adv train loss:  -54.5239272415638 , diff:  0.13674557209014893
adv train loss:  -54.915491461753845 , diff:  0.3915642201900482
adv train loss:  -54.563391625881195 , diff:  0.35209983587265015
adv train loss:  -54.803654462099075 , diff:  0.24026283621788025
adv train loss:  -54.74356549978256 , diff:  0.06008896231651306
layer  4  adv train finish, try to retain  246
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -54.703197598457336 , diff:  54.703197598457336
adv train loss:  -54.72171914577484 , diff:  0.018521547317504883
layer  5  adv train finish, try to retain  246
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -59.00539466738701 , diff:  59.00539466738701
adv train loss:  -58.703323781490326 , diff:  0.30207088589668274
adv train loss:  -58.978657722473145 , diff:  0.2753339409828186
adv train loss:  -58.72682026028633 , diff:  0.25183746218681335
adv train loss:  -59.0617852807045 , diff:  0.3349650204181671
adv train loss:  -58.584104180336 , diff:  0.47768110036849976
adv train loss:  -58.98188453912735 , diff:  0.3977803587913513
adv train loss:  -59.43145224452019 , diff:  0.4495677053928375
adv train loss:  -58.28405383229256 , diff:  1.1473984122276306
adv train loss:  -58.95256155729294 , diff:  0.6685077250003815
layer  6  adv train finish, try to retain  254
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
adv train loss:  -28.89026491343975 , diff:  28.89026491343975
adv train loss:  -28.951399207115173 , diff:  0.06113429367542267
adv train loss:  -29.493348747491837 , diff:  0.5419495403766632
adv train loss:  -28.979061543941498 , diff:  0.5142872035503387
adv train loss:  -28.902555406093597 , diff:  0.07650613784790039
adv train loss:  -28.83016426116228 , diff:  0.07239114493131638
adv train loss:  -29.005160361528397 , diff:  0.17499610036611557
adv train loss:  -28.68901628255844 , diff:  0.31614407896995544
adv train loss:  -29.446521192789078 , diff:  0.7575049102306366
adv train loss:  -29.05397018790245 , diff:  0.3925510048866272
layer  8  adv train finish, try to retain  439
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -65.12489324808121 , diff:  65.12489324808121
adv train loss:  -65.13329169154167 , diff:  0.008398443460464478
layer  9  adv train finish, try to retain  473
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
### skip layer  10 wait:  3  ###
---------------- start layer  11  ---------------
adv train loss:  -3149.591953277588 , diff:  3149.591953277588
adv train loss:  -3283.3727378845215 , diff:  133.7807846069336
adv train loss:  -3284.710433959961 , diff:  1.3376960754394531
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  8
test acc: top1 ->  10.02 ; top5 ->  51.19  and loss:  14730.045196533203
forward train acc: top1 ->  94.394 ; top5 ->  99.616  and loss:  37.20910253422335
test acc: top1 ->  91.06 ; top5 ->  98.59  and loss:  59.43409013748169
forward train acc: top1 ->  99.886 ; top5 ->  99.998  and loss:  0.5118366098031402
test acc: top1 ->  91.71 ; top5 ->  98.9  and loss:  55.41781106591225
forward train acc: top1 ->  99.928 ; top5 ->  100.0  and loss:  0.311418897472322
test acc: top1 ->  91.92 ; top5 ->  98.94  and loss:  55.82885003089905
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.21833856153534725
test acc: top1 ->  91.94 ; top5 ->  98.94  and loss:  56.342920169234276
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.17544601965346374
test acc: top1 ->  91.95 ; top5 ->  98.97  and loss:  56.5539345741272
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.21105358586646616
test acc: top1 ->  92.06 ; top5 ->  98.97  and loss:  56.83554030954838
forward train acc: top1 ->  99.952 ; top5 ->  99.998  and loss:  0.19983454351313412
test acc: top1 ->  92.01 ; top5 ->  98.95  and loss:  57.25268064439297
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.16402383451350033
test acc: top1 ->  92.02 ; top5 ->  99.01  and loss:  57.16229112446308
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.14642127993283793
test acc: top1 ->  92.1 ; top5 ->  98.99  and loss:  57.50862519443035
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.12852600077167153
test acc: top1 ->  92.08 ; top5 ->  98.99  and loss:  57.89374330639839
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  16 / 512 , inc:  8
---------------- start layer  12  ---------------
adv train loss:  -169.3388547897339 , diff:  169.3388547897339
adv train loss:  -169.31473851203918 , diff:  0.02411627769470215
layer  12  adv train finish, try to retain  505
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
### skip layer  13 wait:  4  ###
layer  0  :  0.484375  ==>  31 / 64 , inc:  1
layer  1  :  0.875  ==>  56 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.41015625  ==>  105 / 256 , inc:  2
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.142578125  ==>  73 / 512 , inc:  1
layer  9  :  0.1875  ==>  96 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.03125  ==>  16 / 512 , inc:  4
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.068359375  ==>  35 / 512 , inc:  1
eps [0.9226406250000001, 0.0015204648971557618, 1.8452812500000002, 1.8452812500000002, 1.8452812500000002, 1.8452812500000002, 1.8452812500000002, 0.17299511718750002, 0.0015204648971557618, 0.0015204648971557618, 0.17299511718750002, 0.9226406250000001, 0.0015204648971557618, 7.381125000000001]  wait [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3]  inc [1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 4, 1, 1]  tol: 3
$$$$$$$$$$$$$ epoch  36  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -20.666406333446503 , diff:  20.666406333446503
adv train loss:  -20.580418691039085 , diff:  0.0859876424074173
adv train loss:  -20.629918448626995 , diff:  0.0494997575879097
adv train loss:  -20.696740746498108 , diff:  0.06682229787111282
adv train loss:  -21.20060645788908 , diff:  0.5038657113909721
adv train loss:  -21.182730205357075 , diff:  0.01787625253200531
layer  0  adv train finish, try to retain  57
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -15.518146488815546 , diff:  15.518146488815546
adv train loss:  -16.25830740481615 , diff:  0.7401609160006046
adv train loss:  -15.31609695404768 , diff:  0.9422104507684708
adv train loss:  -15.732220133766532 , diff:  0.41612317971885204
adv train loss:  -16.035143442451954 , diff:  0.30292330868542194
adv train loss:  -16.347775354981422 , diff:  0.31263191252946854
adv train loss:  -15.644067265093327 , diff:  0.7037080898880959
adv train loss:  -15.4884599968791 , diff:  0.15560726821422577
adv train loss:  -16.114385429769754 , diff:  0.6259254328906536
adv train loss:  -15.46402321010828 , diff:  0.6503622196614742
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -1516.329232364893 , diff:  1516.329232364893
adv train loss:  -2241.741865158081 , diff:  725.4126327931881
adv train loss:  -2250.1320819854736 , diff:  8.390216827392578
adv train loss:  -2250.3443908691406 , diff:  0.2123088836669922
adv train loss:  -2255.264471054077 , diff:  4.920080184936523
adv train loss:  -2252.3056602478027 , diff:  2.958810806274414
adv train loss:  -2255.343065261841 , diff:  3.037405014038086
adv train loss:  -2251.0123558044434 , diff:  4.330709457397461
adv train loss:  -2248.8799686431885 , diff:  2.132387161254883
adv train loss:  -2248.291040420532 , diff:  0.58892822265625
layer  2  adv train finish, try to retain  35
test acc: top1 ->  10.0 ; top5 ->  50.16  and loss:  2113.610601425171
forward train acc: top1 ->  77.28799998535156 ; top5 ->  96.10799998535157  and loss:  157.16773921251297
test acc: top1 ->  75.24 ; top5 ->  95.65  and loss:  95.88159757852554
forward train acc: top1 ->  80.44399997558594 ; top5 ->  97.40800000488281  and loss:  64.5644822716713
test acc: top1 ->  77.13 ; top5 ->  96.78  and loss:  78.70223432779312
forward train acc: top1 ->  82.352 ; top5 ->  97.93599998535156  and loss:  56.15574890375137
test acc: top1 ->  78.52 ; top5 ->  97.28  and loss:  71.72392240166664
forward train acc: top1 ->  83.87599998779297 ; top5 ->  98.41200000488281  and loss:  50.27661135792732
test acc: top1 ->  79.79 ; top5 ->  97.69  and loss:  67.85870534181595
forward train acc: top1 ->  84.98400001953125 ; top5 ->  98.62599997558594  and loss:  46.13088622689247
test acc: top1 ->  80.36 ; top5 ->  97.85  and loss:  64.50203678011894
forward train acc: top1 ->  85.9299999975586 ; top5 ->  98.76400000976562  and loss:  42.870932787656784
test acc: top1 ->  80.87 ; top5 ->  97.99  and loss:  63.40248113870621
forward train acc: top1 ->  86.40399998535156 ; top5 ->  98.97199997802734  and loss:  41.3447790145874
test acc: top1 ->  81.31 ; top5 ->  98.06  and loss:  60.97783121466637
forward train acc: top1 ->  86.936 ; top5 ->  98.93200000488281  and loss:  39.97493952512741
test acc: top1 ->  81.84 ; top5 ->  98.17  and loss:  60.200532376766205
forward train acc: top1 ->  87.58799997314453 ; top5 ->  99.06199997802734  and loss:  37.85346961021423
test acc: top1 ->  82.03 ; top5 ->  98.2  and loss:  59.51266196370125
forward train acc: top1 ->  87.78999997314453 ; top5 ->  99.03400000488281  and loss:  37.10394886136055
test acc: top1 ->  82.56 ; top5 ->  98.27  and loss:  57.91519382596016
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -438.88146547600627 , diff:  438.88146547600627
adv train loss:  -703.4568214416504 , diff:  264.5753559656441
adv train loss:  -710.0673499107361 , diff:  6.610528469085693
adv train loss:  -710.1202936172485 , diff:  0.05294370651245117
adv train loss:  -713.1768851280212 , diff:  3.056591510772705
adv train loss:  -710.6272530555725 , diff:  2.5496320724487305
adv train loss:  -709.3985133171082 , diff:  1.2287397384643555
adv train loss:  -711.9576387405396 , diff:  2.5591254234313965
adv train loss:  -708.4756865501404 , diff:  3.48195219039917
adv train loss:  -711.3461232185364 , diff:  2.870436668395996
layer  3  adv train finish, try to retain  30
test acc: top1 ->  9.96 ; top5 ->  50.0  and loss:  489.1642723083496
forward train acc: top1 ->  77.42400001953125 ; top5 ->  97.78199998046875  and loss:  68.15667909383774
test acc: top1 ->  76.34 ; top5 ->  97.53  and loss:  74.42928820848465
forward train acc: top1 ->  81.93599997802734 ; top5 ->  98.61600000244141  and loss:  53.44480738043785
test acc: top1 ->  79.38 ; top5 ->  97.86  and loss:  66.31371304392815
forward train acc: top1 ->  83.97599998291015 ; top5 ->  98.96399997802735  and loss:  46.85211071372032
test acc: top1 ->  80.72 ; top5 ->  98.08  and loss:  62.37440264225006
forward train acc: top1 ->  85.19399997802735 ; top5 ->  99.0760000024414  and loss:  42.95047304034233
test acc: top1 ->  81.66 ; top5 ->  98.29  and loss:  59.14472150802612
forward train acc: top1 ->  86.35599997558593 ; top5 ->  99.18000000488281  and loss:  39.98896840214729
test acc: top1 ->  82.15 ; top5 ->  98.44  and loss:  57.10521790385246
forward train acc: top1 ->  87.05999997314453 ; top5 ->  99.268  and loss:  37.95341283082962
test acc: top1 ->  82.6 ; top5 ->  98.48  and loss:  56.245265662670135
forward train acc: top1 ->  87.35599998291016 ; top5 ->  99.36199997802734  and loss:  36.81664243340492
test acc: top1 ->  82.92 ; top5 ->  98.55  and loss:  55.38089960813522
forward train acc: top1 ->  87.52999997802735 ; top5 ->  99.3720000048828  and loss:  36.238670855760574
test acc: top1 ->  83.13 ; top5 ->  98.58  and loss:  54.58596834540367
forward train acc: top1 ->  88.16999999511718 ; top5 ->  99.36999997558594  and loss:  34.86086794734001
test acc: top1 ->  83.22 ; top5 ->  98.62  and loss:  54.44103518128395
forward train acc: top1 ->  88.10599999023438 ; top5 ->  99.41000000244141  and loss:  34.600570648908615
test acc: top1 ->  83.41 ; top5 ->  98.61  and loss:  53.96327903866768
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -114.00209653005004 , diff:  114.00209653005004
adv train loss:  -536.1446304321289 , diff:  422.14253390207887
adv train loss:  -594.0807089805603 , diff:  57.9360785484314
adv train loss:  -636.1606945991516 , diff:  42.07998561859131
adv train loss:  -678.5999894142151 , diff:  42.43929481506348
adv train loss:  -730.2484498023987 , diff:  51.648460388183594
adv train loss:  -738.3708543777466 , diff:  8.1224045753479
adv train loss:  -741.0046472549438 , diff:  2.6337928771972656
adv train loss:  -745.040093421936 , diff:  4.0354461669921875
adv train loss:  -749.3810186386108 , diff:  4.340925216674805
layer  4  adv train finish, try to retain  23
test acc: top1 ->  11.68 ; top5 ->  50.96  and loss:  1706.2956819534302
forward train acc: top1 ->  65.81999998291016 ; top5 ->  95.72400001708985  and loss:  98.25440716743469
test acc: top1 ->  67.69 ; top5 ->  95.84  and loss:  96.77690345048904
forward train acc: top1 ->  71.85199997070312 ; top5 ->  97.42399998291016  and loss:  79.84406781196594
test acc: top1 ->  71.27 ; top5 ->  96.93  and loss:  85.98311376571655
forward train acc: top1 ->  74.93399997314454 ; top5 ->  97.94800000488281  and loss:  71.5621178150177
test acc: top1 ->  73.3 ; top5 ->  97.25  and loss:  80.48002398014069
forward train acc: top1 ->  76.51000000488281 ; top5 ->  98.22800000488282  and loss:  66.18945997953415
test acc: top1 ->  75.0 ; top5 ->  97.65  and loss:  75.96667885780334
forward train acc: top1 ->  78.10999998779297 ; top5 ->  98.41399998046874  and loss:  62.12812900543213
test acc: top1 ->  75.76 ; top5 ->  97.63  and loss:  72.93233895301819
forward train acc: top1 ->  79.16600001953125 ; top5 ->  98.50000000244141  and loss:  59.48033964633942
test acc: top1 ->  76.54 ; top5 ->  97.83  and loss:  70.53246119618416
forward train acc: top1 ->  80.03399999267579 ; top5 ->  98.57000000732423  and loss:  57.47385975718498
test acc: top1 ->  77.21 ; top5 ->  97.89  and loss:  69.45389753580093
forward train acc: top1 ->  80.26399998535156 ; top5 ->  98.64799998046875  and loss:  56.94616001844406
test acc: top1 ->  77.59 ; top5 ->  97.87  and loss:  68.2909606397152
forward train acc: top1 ->  80.54199999755859 ; top5 ->  98.62799998535156  and loss:  55.67963328957558
test acc: top1 ->  78.1 ; top5 ->  97.9  and loss:  67.34684494137764
forward train acc: top1 ->  81.02599997070313 ; top5 ->  98.75399998291016  and loss:  54.48270636796951
test acc: top1 ->  78.34 ; top5 ->  98.03  and loss:  66.26664772629738
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  156 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -176.33431831002235 , diff:  176.33431831002235
adv train loss:  -813.1548552513123 , diff:  636.8205369412899
adv train loss:  -895.8955526351929 , diff:  82.74069738388062
adv train loss:  -952.7910118103027 , diff:  56.89545917510986
adv train loss:  -970.282172203064 , diff:  17.49116039276123
adv train loss:  -974.7459468841553 , diff:  4.463774681091309
adv train loss:  -974.5325193405151 , diff:  0.21342754364013672
adv train loss:  -975.9319763183594 , diff:  1.3994569778442383
adv train loss:  -975.4695453643799 , diff:  0.4624309539794922
adv train loss:  -973.3722677230835 , diff:  2.0972776412963867
layer  5  adv train finish, try to retain  27
test acc: top1 ->  10.77 ; top5 ->  53.11  and loss:  661.6204690933228
forward train acc: top1 ->  81.86600001708985 ; top5 ->  98.3200000024414  and loss:  54.316518634557724
test acc: top1 ->  79.9 ; top5 ->  98.17  and loss:  64.01075235009193
forward train acc: top1 ->  85.42800001953125 ; top5 ->  99.03399997802734  and loss:  42.49493581056595
test acc: top1 ->  81.54 ; top5 ->  98.31  and loss:  58.650474578142166
forward train acc: top1 ->  87.05799997314453 ; top5 ->  99.18399997802734  and loss:  37.825810343027115
test acc: top1 ->  82.35 ; top5 ->  98.66  and loss:  56.96921908855438
forward train acc: top1 ->  88.42999998779297 ; top5 ->  99.32399997802735  and loss:  34.077421963214874
test acc: top1 ->  83.43 ; top5 ->  98.77  and loss:  53.60998249053955
forward train acc: top1 ->  88.99399997558594 ; top5 ->  99.458  and loss:  31.966326162219048
test acc: top1 ->  83.86 ; top5 ->  98.7  and loss:  53.75999107956886
forward train acc: top1 ->  89.46399998779297 ; top5 ->  99.47000000244141  and loss:  30.34467300772667
test acc: top1 ->  84.43 ; top5 ->  98.81  and loss:  50.94427900016308
forward train acc: top1 ->  89.77400001464844 ; top5 ->  99.50400000488281  and loss:  29.673909336328506
test acc: top1 ->  84.54 ; top5 ->  98.76  and loss:  50.996594578027725
forward train acc: top1 ->  90.27400001708985 ; top5 ->  99.56200000244141  and loss:  28.365284845232964
test acc: top1 ->  84.66 ; top5 ->  98.8  and loss:  50.24311849474907
forward train acc: top1 ->  90.38000001464843 ; top5 ->  99.59599997802735  and loss:  28.01420982182026
test acc: top1 ->  84.81 ; top5 ->  98.83  and loss:  49.87245148420334
forward train acc: top1 ->  90.67199998535156 ; top5 ->  99.53799997558593  and loss:  27.334749445319176
test acc: top1 ->  84.94 ; top5 ->  98.86  and loss:  49.1975264698267
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -96.7813470698893 , diff:  96.7813470698893
adv train loss:  -591.8147797584534 , diff:  495.03343268856406
adv train loss:  -752.611982345581 , diff:  160.79720258712769
adv train loss:  -907.2489166259766 , diff:  154.6369342803955
adv train loss:  -915.3120641708374 , diff:  8.06314754486084
adv train loss:  -933.93039894104 , diff:  18.618334770202637
adv train loss:  -980.0810527801514 , diff:  46.15065383911133
adv train loss:  -1003.6456251144409 , diff:  23.56457233428955
adv train loss:  -1007.0502595901489 , diff:  3.404634475708008
adv train loss:  -1012.3484582901001 , diff:  5.298198699951172
layer  6  adv train finish, try to retain  10
test acc: top1 ->  19.89 ; top5 ->  54.31  and loss:  974.7605586051941
forward train acc: top1 ->  81.77000001464843 ; top5 ->  98.86199997802734  and loss:  53.78897961974144
test acc: top1 ->  79.8 ; top5 ->  98.34  and loss:  64.52846425771713
forward train acc: top1 ->  86.22799999267578 ; top5 ->  99.34200000244141  and loss:  39.90336614847183
test acc: top1 ->  81.88 ; top5 ->  98.65  and loss:  60.04026398062706
forward train acc: top1 ->  88.26999998535156 ; top5 ->  99.4900000024414  and loss:  34.008514970541
test acc: top1 ->  83.14 ; top5 ->  98.85  and loss:  56.8464330136776
forward train acc: top1 ->  89.40000000488281 ; top5 ->  99.566  and loss:  30.846645087003708
test acc: top1 ->  83.6 ; top5 ->  98.83  and loss:  55.41404542326927
forward train acc: top1 ->  90.45399998535156 ; top5 ->  99.64000000244141  and loss:  27.581984981894493
test acc: top1 ->  84.33 ; top5 ->  98.95  and loss:  53.2778625190258
forward train acc: top1 ->  90.94999998779296 ; top5 ->  99.712  and loss:  26.00045156478882
test acc: top1 ->  84.49 ; top5 ->  98.97  and loss:  53.10844811797142
forward train acc: top1 ->  91.28599999023437 ; top5 ->  99.68999997558593  and loss:  24.992894425988197
test acc: top1 ->  84.81 ; top5 ->  98.99  and loss:  52.68684509396553
forward train acc: top1 ->  91.6480000024414 ; top5 ->  99.728  and loss:  23.932740539312363
test acc: top1 ->  84.75 ; top5 ->  98.95  and loss:  52.66089928150177
forward train acc: top1 ->  91.98800001464843 ; top5 ->  99.75999997802734  and loss:  23.325377970933914
test acc: top1 ->  85.26 ; top5 ->  99.0  and loss:  51.50973206758499
forward train acc: top1 ->  92.26200001220703 ; top5 ->  99.7580000024414  and loss:  22.087499246001244
test acc: top1 ->  85.1 ; top5 ->  99.01  and loss:  52.68719270825386
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  105 / 256 , inc:  2
---------------- start layer  7  ---------------
adv train loss:  -5.476899351924658 , diff:  5.476899351924658
adv train loss:  -5.307540740817785 , diff:  0.16935861110687256
adv train loss:  -5.416147759184241 , diff:  0.10860701836645603
adv train loss:  -5.34735163487494 , diff:  0.06879612430930138
adv train loss:  -5.500941459089518 , diff:  0.15358982421457767
adv train loss:  -5.432100661098957 , diff:  0.06884079799056053
adv train loss:  -5.361609732732177 , diff:  0.07049092836678028
adv train loss:  -5.424367185682058 , diff:  0.06275745294988155
adv train loss:  -5.478110279887915 , diff:  0.05374309420585632
adv train loss:  -5.355149861425161 , diff:  0.1229604184627533
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  103
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  439.22095680236816
forward train acc: top1 ->  66.67199998291015 ; top5 ->  98.79000000732422  and loss:  93.42072314023972
test acc: top1 ->  77.65 ; top5 ->  98.86  and loss:  68.94326049089432
forward train acc: top1 ->  84.58999998291016 ; top5 ->  99.68399997558593  and loss:  44.36238068342209
test acc: top1 ->  82.08 ; top5 ->  99.07  and loss:  60.15073022246361
forward train acc: top1 ->  89.14400001708984 ; top5 ->  99.808  and loss:  31.05063332617283
test acc: top1 ->  85.01 ; top5 ->  99.17  and loss:  52.69169959425926
forward train acc: top1 ->  92.17800000976563 ; top5 ->  99.84199997558594  and loss:  22.704249992966652
test acc: top1 ->  86.82 ; top5 ->  99.25  and loss:  49.03160548210144
forward train acc: top1 ->  93.94399999511718 ; top5 ->  99.92199997558593  and loss:  17.638397961854935
test acc: top1 ->  87.6 ; top5 ->  99.19  and loss:  46.98486889898777
forward train acc: top1 ->  94.85399999755859 ; top5 ->  99.916  and loss:  14.972476780414581
test acc: top1 ->  87.55 ; top5 ->  99.27  and loss:  48.853027015924454
forward train acc: top1 ->  95.34199999267578 ; top5 ->  99.92399997558594  and loss:  13.686462014913559
test acc: top1 ->  88.01 ; top5 ->  99.32  and loss:  46.96423763036728
forward train acc: top1 ->  95.6779999975586 ; top5 ->  99.952  and loss:  12.736566469073296
test acc: top1 ->  88.21 ; top5 ->  99.36  and loss:  47.48110614717007
forward train acc: top1 ->  96.00999998779297 ; top5 ->  99.93999997558593  and loss:  11.649508841335773
test acc: top1 ->  88.53 ; top5 ->  99.38  and loss:  46.859737649559975
forward train acc: top1 ->  96.29999997070313 ; top5 ->  99.974  and loss:  10.82911717146635
test acc: top1 ->  88.59 ; top5 ->  99.34  and loss:  47.1461521089077
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  104 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -23.42524243891239 , diff:  23.42524243891239
adv train loss:  -23.41712425649166 , diff:  0.00811818242073059
layer  8  adv train finish, try to retain  413
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -27.96162350475788 , diff:  27.96162350475788
adv train loss:  -27.935864999890327 , diff:  0.02575850486755371
layer  9  adv train finish, try to retain  463
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -31.573871344327927 , diff:  31.573871344327927
adv train loss:  -31.83045695722103 , diff:  0.25658561289310455
adv train loss:  -31.524011477828026 , diff:  0.30644547939300537
adv train loss:  -31.385825037956238 , diff:  0.13818643987178802
adv train loss:  -31.76654863357544 , diff:  0.38072359561920166
adv train loss:  -30.840777561068535 , diff:  0.9257710725069046
adv train loss:  -30.972782626748085 , diff:  0.13200506567955017
adv train loss:  -31.52023057639599 , diff:  0.5474479496479034
adv train loss:  -30.801887214183807 , diff:  0.7183433622121811
adv train loss:  -31.87911979854107 , diff:  1.0772325843572617
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  21
test acc: top1 ->  10.0 ; top5 ->  55.72  and loss:  1007.3819751739502
forward train acc: top1 ->  68.79599998779297 ; top5 ->  94.83  and loss:  129.95873826742172
test acc: top1 ->  80.18 ; top5 ->  98.93  and loss:  62.05302831530571
forward train acc: top1 ->  97.7320000048828 ; top5 ->  99.97  and loss:  15.000349842011929
test acc: top1 ->  89.29 ; top5 ->  98.43  and loss:  46.72913295030594
forward train acc: top1 ->  98.67800000488282 ; top5 ->  99.984  and loss:  6.38886646181345
test acc: top1 ->  89.78 ; top5 ->  98.09  and loss:  49.51789245009422
forward train acc: top1 ->  98.98799997802735 ; top5 ->  99.996  and loss:  4.0707124173641205
test acc: top1 ->  90.06 ; top5 ->  98.43  and loss:  50.668551817536354
forward train acc: top1 ->  99.17800000244141 ; top5 ->  99.998  and loss:  3.177133619785309
test acc: top1 ->  90.4 ; top5 ->  98.56  and loss:  51.6210178732872
forward train acc: top1 ->  99.30599997802734 ; top5 ->  99.988  and loss:  2.604770753532648
test acc: top1 ->  90.39 ; top5 ->  98.32  and loss:  51.291145607829094
forward train acc: top1 ->  99.36000000244141 ; top5 ->  99.998  and loss:  2.3660367969423532
test acc: top1 ->  90.39 ; top5 ->  98.25  and loss:  52.85953912138939
forward train acc: top1 ->  99.42399997558594 ; top5 ->  99.994  and loss:  2.1967142652720213
test acc: top1 ->  90.57 ; top5 ->  98.43  and loss:  53.26837719976902
forward train acc: top1 ->  99.518 ; top5 ->  99.998  and loss:  1.9076391095295548
test acc: top1 ->  90.54 ; top5 ->  98.46  and loss:  52.60304605960846
forward train acc: top1 ->  99.552 ; top5 ->  99.988  and loss:  1.7184042446315289
test acc: top1 ->  90.6 ; top5 ->  98.41  and loss:  53.922659650444984
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  22 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -4.485636128112674 , diff:  4.485636128112674
adv train loss:  -4.824891906231642 , diff:  0.339255778118968
adv train loss:  -4.602475775405765 , diff:  0.2224161308258772
adv train loss:  -4.601235177367926 , diff:  0.0012405980378389359
layer  11  adv train finish, try to retain  427
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -92.37467765808105 , diff:  92.37467765808105
adv train loss:  -92.23013311624527 , diff:  0.1445445418357849
layer  12  adv train finish, try to retain  502
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -9333.415840148926 , diff:  9333.415840148926
adv train loss:  -15342.568450927734 , diff:  6009.152610778809
adv train loss:  -21226.245193481445 , diff:  5883.676742553711
adv train loss:  -27096.217086791992 , diff:  5869.971893310547
adv train loss:  -33039.00466918945 , diff:  5942.787582397461
adv train loss:  -39090.725982666016 , diff:  6051.7213134765625
adv train loss:  -43880.10971069336 , diff:  4789.383728027344
adv train loss:  -45882.50372314453 , diff:  2002.3940124511719
adv train loss:  -46821.5915222168 , diff:  939.0877990722656
adv train loss:  -47357.34567260742 , diff:  535.754150390625
layer  13  adv train finish, try to retain  34
test acc: top1 ->  53.02 ; top5 ->  95.6  and loss:  411.812876701355
forward train acc: top1 ->  95.56000000244141 ; top5 ->  99.944  and loss:  26.14129552990198
test acc: top1 ->  91.79 ; top5 ->  99.15  and loss:  56.23899846524
forward train acc: top1 ->  99.7460000024414 ; top5 ->  100.0  and loss:  0.8420141735114157
test acc: top1 ->  91.87 ; top5 ->  99.16  and loss:  55.27403649687767
forward train acc: top1 ->  99.806 ; top5 ->  100.0  and loss:  0.5898122357903048
test acc: top1 ->  91.92 ; top5 ->  99.22  and loss:  55.896589905023575
forward train acc: top1 ->  99.842 ; top5 ->  100.0  and loss:  0.46227448782883584
test acc: top1 ->  92.03 ; top5 ->  99.2  and loss:  56.20368714630604
forward train acc: top1 ->  99.854 ; top5 ->  100.0  and loss:  0.4213860896998085
test acc: top1 ->  91.99 ; top5 ->  99.25  and loss:  57.52171687036753
forward train acc: top1 ->  99.878 ; top5 ->  100.0  and loss:  0.38491868067649193
test acc: top1 ->  91.94 ; top5 ->  99.22  and loss:  57.151488818228245
forward train acc: top1 ->  99.87 ; top5 ->  100.0  and loss:  0.364162951183971
test acc: top1 ->  92.04 ; top5 ->  99.23  and loss:  56.70641803741455
forward train acc: top1 ->  99.9 ; top5 ->  100.0  and loss:  0.3670678903290536
test acc: top1 ->  92.01 ; top5 ->  99.22  and loss:  57.27772706001997
forward train acc: top1 ->  99.89799997558593 ; top5 ->  99.998  and loss:  0.3204179457388818
test acc: top1 ->  92.11 ; top5 ->  99.2  and loss:  56.70651577413082
==> this epoch:  34 / 512
layer  0  :  0.484375  ==>  31 / 64 , inc:  1
layer  1  :  0.875  ==>  56 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.41015625  ==>  105 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.142578125  ==>  73 / 512 , inc:  1
layer  9  :  0.1875  ==>  96 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.03125  ==>  16 / 512 , inc:  4
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.06640625  ==>  34 / 512 , inc:  2
eps [1.8452812500000002, 0.0030409297943115236, 1.3839609375000002, 1.3839609375000002, 1.3839609375000002, 1.3839609375000002, 1.3839609375000002, 0.129746337890625, 0.0030409297943115236, 0.0030409297943115236, 0.129746337890625, 1.8452812500000002, 0.0030409297943115236, 7.381125000000001]  wait [0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0, 0, 0]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 2]  tol: 4
$$$$$$$$$$$$$ epoch  37  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1457.8173940181732 , diff:  1457.8173940181732
adv train loss:  -1336.1221170425415 , diff:  121.69527697563171
adv train loss:  -1313.90904712677 , diff:  22.213069915771484
adv train loss:  -1312.7674016952515 , diff:  1.1416454315185547
adv train loss:  -1370.320447921753 , diff:  57.553046226501465
adv train loss:  -1379.4421014785767 , diff:  9.12165355682373
adv train loss:  -1385.9241161346436 , diff:  6.4820146560668945
adv train loss:  -1386.1903076171875 , diff:  0.2661914825439453
adv train loss:  -1386.0317106246948 , diff:  0.15859699249267578
adv train loss:  -1385.5638666152954 , diff:  0.46784400939941406
layer  0  adv train finish, try to retain  4
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  8942.239151000977
forward train acc: top1 ->  38.42199998535156 ; top5 ->  80.13000001464843  and loss:  355.69728899002075
test acc: top1 ->  11.79 ; top5 ->  50.35  and loss:  495.5296607017517
forward train acc: top1 ->  46.0680000012207 ; top5 ->  86.10399998046876  and loss:  160.26783967018127
test acc: top1 ->  50.64 ; top5 ->  88.61  and loss:  148.41148960590363
forward train acc: top1 ->  51.88799999267578 ; top5 ->  89.36599998535156  and loss:  142.28273665905
test acc: top1 ->  55.13 ; top5 ->  91.04  and loss:  135.20384812355042
forward train acc: top1 ->  55.75200000732422 ; top5 ->  91.27800000976562  and loss:  130.53016757965088
test acc: top1 ->  58.57 ; top5 ->  92.79  and loss:  124.47485107183456
forward train acc: top1 ->  58.904 ; top5 ->  92.4940000048828  and loss:  121.2836229801178
test acc: top1 ->  61.06 ; top5 ->  93.42  and loss:  118.38522243499756
forward train acc: top1 ->  61.04399998657227 ; top5 ->  93.13200000732422  and loss:  115.32118511199951
test acc: top1 ->  62.65 ; top5 ->  94.2  and loss:  112.37358009815216
forward train acc: top1 ->  62.3339999987793 ; top5 ->  93.794  and loss:  111.07203215360641
test acc: top1 ->  63.89 ; top5 ->  94.41  and loss:  109.32641953229904
forward train acc: top1 ->  63.390000008544924 ; top5 ->  94.02399999755859  and loss:  108.37303471565247
test acc: top1 ->  64.4 ; top5 ->  94.79  and loss:  106.7150628566742
forward train acc: top1 ->  64.56600000732422 ; top5 ->  94.35599999267578  and loss:  105.06944406032562
test acc: top1 ->  65.89 ; top5 ->  94.85  and loss:  103.93500727415085
forward train acc: top1 ->  65.4879999987793 ; top5 ->  94.70800000976563  and loss:  101.89961278438568
test acc: top1 ->  66.43 ; top5 ->  95.19  and loss:  101.34528011083603
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  31 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -25.21716460585594 , diff:  25.21716460585594
adv train loss:  -25.134969174861908 , diff:  0.08219543099403381
adv train loss:  -25.083388701081276 , diff:  0.05158047378063202
adv train loss:  -25.178494781255722 , diff:  0.0951060801744461
adv train loss:  -25.203251108527184 , diff:  0.024756327271461487
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  55
test acc: top1 ->  38.49 ; top5 ->  79.08  and loss:  192.7569179534912
forward train acc: top1 ->  98.71399998291015 ; top5 ->  99.97  and loss:  5.678524158895016
test acc: top1 ->  90.93 ; top5 ->  99.19  and loss:  40.08579749986529
forward train acc: top1 ->  99.31800000244141 ; top5 ->  99.998  and loss:  2.075238471850753
test acc: top1 ->  91.21 ; top5 ->  99.16  and loss:  43.35191110149026
forward train acc: top1 ->  99.456 ; top5 ->  99.998  and loss:  1.6771673345938325
test acc: top1 ->  91.42 ; top5 ->  99.17  and loss:  44.63922160118818
forward train acc: top1 ->  99.54999997802734 ; top5 ->  100.0  and loss:  1.3370453286916018
test acc: top1 ->  91.45 ; top5 ->  99.2  and loss:  47.08392835408449
forward train acc: top1 ->  99.62000000488281 ; top5 ->  99.996  and loss:  1.0343900620937347
test acc: top1 ->  91.47 ; top5 ->  99.18  and loss:  48.961161747574806
forward train acc: top1 ->  99.6920000024414 ; top5 ->  100.0  and loss:  0.900303078815341
test acc: top1 ->  91.47 ; top5 ->  99.15  and loss:  49.177495919167995
forward train acc: top1 ->  99.69199997558594 ; top5 ->  100.0  and loss:  0.8753447122871876
test acc: top1 ->  91.46 ; top5 ->  99.23  and loss:  50.15239043533802
forward train acc: top1 ->  99.7280000024414 ; top5 ->  100.0  and loss:  0.7713467446155846
test acc: top1 ->  91.47 ; top5 ->  99.23  and loss:  50.60952256992459
forward train acc: top1 ->  99.7240000024414 ; top5 ->  99.998  and loss:  0.8347413595765829
test acc: top1 ->  91.6 ; top5 ->  99.22  and loss:  51.1316728964448
forward train acc: top1 ->  99.764 ; top5 ->  100.0  and loss:  0.6487658795667812
test acc: top1 ->  91.61 ; top5 ->  99.23  and loss:  51.498099967837334
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  56 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -829.8304486386478 , diff:  829.8304486386478
adv train loss:  -1322.714635848999 , diff:  492.88418721035123
adv train loss:  -1337.4608125686646 , diff:  14.746176719665527
adv train loss:  -1349.366943359375 , diff:  11.90613079071045
adv train loss:  -1351.2838945388794 , diff:  1.9169511795043945
adv train loss:  -1347.6747751235962 , diff:  3.609119415283203
adv train loss:  -1350.50559425354 , diff:  2.8308191299438477
adv train loss:  -1349.6104516983032 , diff:  0.8951425552368164
adv train loss:  -1345.7690267562866 , diff:  3.8414249420166016
adv train loss:  -1354.81001663208 , diff:  9.040989875793457
layer  2  adv train finish, try to retain  55
test acc: top1 ->  25.36 ; top5 ->  71.92  and loss:  1263.366985321045
forward train acc: top1 ->  93.47999999511718 ; top5 ->  99.7100000024414  and loss:  22.1093839854002
test acc: top1 ->  87.1 ; top5 ->  98.86  and loss:  48.45885622501373
forward train acc: top1 ->  95.09999997070312 ; top5 ->  99.86  and loss:  14.916818521916866
test acc: top1 ->  87.62 ; top5 ->  99.04  and loss:  46.225512221455574
forward train acc: top1 ->  95.70399999511719 ; top5 ->  99.88  and loss:  12.673944555222988
test acc: top1 ->  88.36 ; top5 ->  99.01  and loss:  44.94377437233925
forward train acc: top1 ->  96.20399998535156 ; top5 ->  99.926  and loss:  11.047532483935356
test acc: top1 ->  88.76 ; top5 ->  99.1  and loss:  44.409798085689545
forward train acc: top1 ->  96.63999998779298 ; top5 ->  99.942  and loss:  9.955770496279001
test acc: top1 ->  88.96 ; top5 ->  99.1  and loss:  44.05394068360329
forward train acc: top1 ->  96.98799998291015 ; top5 ->  99.934  and loss:  8.875215698033571
test acc: top1 ->  89.06 ; top5 ->  99.11  and loss:  43.85313564538956
forward train acc: top1 ->  97.17399998291016 ; top5 ->  99.956  and loss:  8.239477559924126
test acc: top1 ->  89.16 ; top5 ->  99.17  and loss:  44.534738689661026
forward train acc: top1 ->  97.17600001220703 ; top5 ->  99.946  and loss:  7.946703668683767
test acc: top1 ->  89.15 ; top5 ->  99.23  and loss:  44.534665524959564
forward train acc: top1 ->  97.32400001220704 ; top5 ->  99.952  and loss:  7.895287282764912
test acc: top1 ->  89.38 ; top5 ->  99.18  and loss:  43.87706908583641
forward train acc: top1 ->  97.44599998046876 ; top5 ->  99.96  and loss:  7.438388865441084
test acc: top1 ->  89.35 ; top5 ->  99.16  and loss:  44.1639434248209
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -627.283718649298 , diff:  627.283718649298
adv train loss:  -1034.8723993301392 , diff:  407.5886806808412
adv train loss:  -1043.6452798843384 , diff:  8.772880554199219
adv train loss:  -1041.765468597412 , diff:  1.8798112869262695
adv train loss:  -1043.3600568771362 , diff:  1.594588279724121
adv train loss:  -1046.0441265106201 , diff:  2.6840696334838867
adv train loss:  -1055.3064069747925 , diff:  9.262280464172363
adv train loss:  -1058.9581689834595 , diff:  3.651762008666992
adv train loss:  -1051.8308010101318 , diff:  7.127367973327637
adv train loss:  -1059.8192949295044 , diff:  7.988493919372559
layer  3  adv train finish, try to retain  59
test acc: top1 ->  34.19 ; top5 ->  76.66  and loss:  605.028395652771
forward train acc: top1 ->  97.33199998535156 ; top5 ->  99.98  and loss:  7.736714072525501
test acc: top1 ->  90.01 ; top5 ->  99.25  and loss:  41.91110298037529
forward train acc: top1 ->  97.90399998046875 ; top5 ->  99.96199997558594  and loss:  6.181470587849617
test acc: top1 ->  90.22 ; top5 ->  99.21  and loss:  42.09984127432108
forward train acc: top1 ->  98.13199998291016 ; top5 ->  99.984  and loss:  5.356732562184334
test acc: top1 ->  90.46 ; top5 ->  99.34  and loss:  42.256395258009434
forward train acc: top1 ->  98.43399998535156 ; top5 ->  99.986  and loss:  4.453047286719084
test acc: top1 ->  90.54 ; top5 ->  99.32  and loss:  43.594144493341446
forward train acc: top1 ->  98.48000000976562 ; top5 ->  99.984  and loss:  4.292739026248455
test acc: top1 ->  90.54 ; top5 ->  99.31  and loss:  43.54137109220028
forward train acc: top1 ->  98.60199998535157 ; top5 ->  99.994  and loss:  4.028662338852882
test acc: top1 ->  90.65 ; top5 ->  99.33  and loss:  43.35455621778965
forward train acc: top1 ->  98.62200000732422 ; top5 ->  99.986  and loss:  3.908175628632307
test acc: top1 ->  90.48 ; top5 ->  99.3  and loss:  43.43552003055811
forward train acc: top1 ->  98.76600000488281 ; top5 ->  99.982  and loss:  3.5755332075059414
test acc: top1 ->  90.59 ; top5 ->  99.3  and loss:  43.74111347645521
forward train acc: top1 ->  98.78200000732421 ; top5 ->  99.992  and loss:  3.548946011811495
test acc: top1 ->  90.6 ; top5 ->  99.26  and loss:  44.2318494990468
forward train acc: top1 ->  98.76399998046875 ; top5 ->  99.994  and loss:  3.487028770148754
test acc: top1 ->  90.53 ; top5 ->  99.27  and loss:  44.95365356653929
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -152.25847994536161 , diff:  152.25847994536161
adv train loss:  -905.6151452064514 , diff:  753.3566652610898
adv train loss:  -1216.4017162322998 , diff:  310.7865710258484
adv train loss:  -1274.5945310592651 , diff:  58.19281482696533
adv train loss:  -1308.4263095855713 , diff:  33.83177852630615
adv train loss:  -1345.7540788650513 , diff:  37.32776927947998
adv train loss:  -1363.5516395568848 , diff:  17.797560691833496
adv train loss:  -1368.2888746261597 , diff:  4.737235069274902
adv train loss:  -1372.691204071045 , diff:  4.402329444885254
adv train loss:  -1373.3223905563354 , diff:  0.6311864852905273
layer  4  adv train finish, try to retain  69
test acc: top1 ->  51.85 ; top5 ->  89.31  and loss:  443.038387298584
forward train acc: top1 ->  91.222 ; top5 ->  99.68199997558594  and loss:  26.801693007349968
test acc: top1 ->  85.01 ; top5 ->  98.83  and loss:  52.32513424754143
forward train acc: top1 ->  92.66599997070313 ; top5 ->  99.76399997558593  and loss:  21.124387241899967
test acc: top1 ->  85.97 ; top5 ->  99.03  and loss:  48.90323354303837
forward train acc: top1 ->  93.448 ; top5 ->  99.8220000024414  and loss:  18.734002143144608
test acc: top1 ->  86.37 ; top5 ->  99.1  and loss:  47.9046188890934
forward train acc: top1 ->  94.04199997314453 ; top5 ->  99.83999997558594  and loss:  17.142742663621902
test acc: top1 ->  86.93 ; top5 ->  99.12  and loss:  46.97336433827877
forward train acc: top1 ->  94.45599999511718 ; top5 ->  99.884  and loss:  15.860067427158356
test acc: top1 ->  87.2 ; top5 ->  99.13  and loss:  46.23911912739277
forward train acc: top1 ->  94.76000000732422 ; top5 ->  99.872  and loss:  14.794282637536526
test acc: top1 ->  87.37 ; top5 ->  99.16  and loss:  46.0045026242733
forward train acc: top1 ->  94.92200001464843 ; top5 ->  99.89  and loss:  14.42749858647585
test acc: top1 ->  87.49 ; top5 ->  99.12  and loss:  45.38641329109669
forward train acc: top1 ->  94.97799997070312 ; top5 ->  99.89399997558594  and loss:  14.285533428192139
test acc: top1 ->  87.48 ; top5 ->  99.15  and loss:  46.18510615825653
forward train acc: top1 ->  95.27999999755859 ; top5 ->  99.9  and loss:  13.472079820930958
test acc: top1 ->  87.73 ; top5 ->  99.2  and loss:  45.86947837471962
forward train acc: top1 ->  95.24199997558594 ; top5 ->  99.878  and loss:  13.732464864850044
test acc: top1 ->  87.69 ; top5 ->  99.22  and loss:  45.248321160674095
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  156 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -375.6018298268318 , diff:  375.6018298268318
adv train loss:  -1247.914834022522 , diff:  872.3130041956902
adv train loss:  -1297.9515829086304 , diff:  50.0367488861084
adv train loss:  -1296.8997831344604 , diff:  1.0517997741699219
adv train loss:  -1307.7988786697388 , diff:  10.89909553527832
adv train loss:  -1309.3045902252197 , diff:  1.505711555480957
adv train loss:  -1336.941686630249 , diff:  27.637096405029297
adv train loss:  -1344.5423946380615 , diff:  7.6007080078125
adv train loss:  -1345.4475889205933 , diff:  0.9051942825317383
adv train loss:  -1342.689953804016 , diff:  2.7576351165771484
layer  5  adv train finish, try to retain  60
test acc: top1 ->  31.97 ; top5 ->  74.27  and loss:  1538.9761800765991
forward train acc: top1 ->  93.99 ; top5 ->  99.79800000244141  and loss:  17.857065588235855
test acc: top1 ->  87.11 ; top5 ->  99.13  and loss:  45.600238502025604
forward train acc: top1 ->  95.13199999267579 ; top5 ->  99.8800000024414  and loss:  13.961068406701088
test acc: top1 ->  88.11 ; top5 ->  99.19  and loss:  43.25205545127392
forward train acc: top1 ->  95.96599998779297 ; top5 ->  99.904  and loss:  11.735233411192894
test acc: top1 ->  88.59 ; top5 ->  99.24  and loss:  43.50186865031719
forward train acc: top1 ->  96.29400001464843 ; top5 ->  99.92799997558593  and loss:  10.696110542863607
test acc: top1 ->  89.12 ; top5 ->  99.24  and loss:  42.719867557287216
forward train acc: top1 ->  96.61799998291016 ; top5 ->  99.952  and loss:  9.64256203174591
test acc: top1 ->  89.25 ; top5 ->  99.37  and loss:  42.37591019272804
forward train acc: top1 ->  96.89199997314454 ; top5 ->  99.93999997558593  and loss:  8.994948234409094
test acc: top1 ->  89.29 ; top5 ->  99.33  and loss:  42.402110278606415
forward train acc: top1 ->  96.9479999975586 ; top5 ->  99.958  and loss:  8.660338044166565
test acc: top1 ->  89.6 ; top5 ->  99.37  and loss:  41.80556917190552
forward train acc: top1 ->  97.07600000732423 ; top5 ->  99.948  and loss:  8.366771198809147
test acc: top1 ->  89.6 ; top5 ->  99.34  and loss:  42.14126142859459
forward train acc: top1 ->  97.14199998779297 ; top5 ->  99.948  and loss:  8.001236151903868
test acc: top1 ->  89.74 ; top5 ->  99.41  and loss:  42.38842776417732
forward train acc: top1 ->  97.31599998779296 ; top5 ->  99.962  and loss:  7.6845801174640656
test acc: top1 ->  89.63 ; top5 ->  99.34  and loss:  42.475006490945816
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -352.6834872029722 , diff:  352.6834872029722
adv train loss:  -1464.0472736358643 , diff:  1111.363786432892
adv train loss:  -1525.0178499221802 , diff:  60.97057628631592
adv train loss:  -1545.8750610351562 , diff:  20.857211112976074
adv train loss:  -1563.004677772522 , diff:  17.129616737365723
adv train loss:  -1565.5880422592163 , diff:  2.583364486694336
adv train loss:  -1556.7032976150513 , diff:  8.884744644165039
adv train loss:  -1560.162015914917 , diff:  3.4587182998657227
adv train loss:  -1561.6604852676392 , diff:  1.498469352722168
adv train loss:  -1558.670039176941 , diff:  2.990446090698242
layer  6  adv train finish, try to retain  35
test acc: top1 ->  67.59 ; top5 ->  93.73  and loss:  250.75641798973083
forward train acc: top1 ->  97.23000000488281 ; top5 ->  99.952  and loss:  8.137173164635897
test acc: top1 ->  89.62 ; top5 ->  99.07  and loss:  44.3714814633131
forward train acc: top1 ->  97.95200000732422 ; top5 ->  99.992  and loss:  5.8227711990475655
test acc: top1 ->  89.79 ; top5 ->  99.16  and loss:  44.67805904895067
forward train acc: top1 ->  98.32200000976563 ; top5 ->  99.976  and loss:  4.730167176574469
test acc: top1 ->  90.13 ; top5 ->  99.17  and loss:  45.328957699239254
forward train acc: top1 ->  98.53199998046875 ; top5 ->  99.98599997558594  and loss:  4.176555328071117
test acc: top1 ->  89.99 ; top5 ->  99.22  and loss:  46.432860016822815
forward train acc: top1 ->  98.71000000732423 ; top5 ->  99.988  and loss:  3.868156597018242
test acc: top1 ->  90.46 ; top5 ->  99.21  and loss:  46.35565970093012
forward train acc: top1 ->  98.78600000488281 ; top5 ->  99.992  and loss:  3.4263477846980095
test acc: top1 ->  90.46 ; top5 ->  99.22  and loss:  47.0659601688385
forward train acc: top1 ->  98.88000000732421 ; top5 ->  99.996  and loss:  3.230335809290409
test acc: top1 ->  90.58 ; top5 ->  99.24  and loss:  46.60483853518963
forward train acc: top1 ->  98.85800000488281 ; top5 ->  99.998  and loss:  3.200229797512293
test acc: top1 ->  90.56 ; top5 ->  99.2  and loss:  47.025430992245674
forward train acc: top1 ->  99.00600000488281 ; top5 ->  99.998  and loss:  2.9201511777937412
test acc: top1 ->  90.62 ; top5 ->  99.21  and loss:  47.25282935798168
forward train acc: top1 ->  98.88599998046875 ; top5 ->  99.992  and loss:  3.138927087187767
test acc: top1 ->  90.45 ; top5 ->  99.16  and loss:  47.445376716554165
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  105 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -1.4845445044338703 , diff:  1.4845445044338703
adv train loss:  -1.5129443611949682 , diff:  0.028399856761097908
adv train loss:  -1.4756447868421674 , diff:  0.037299574352800846
adv train loss:  -1.3772552143782377 , diff:  0.09838957246392965
adv train loss:  -1.3935905080288649 , diff:  0.016335293650627136
adv train loss:  -1.46468056878075 , diff:  0.07109006075188518
adv train loss:  -1.3329811915755272 , diff:  0.13169937720522285
adv train loss:  -1.3770734146237373 , diff:  0.044092223048210144
adv train loss:  -1.493570189923048 , diff:  0.11649677529931068
adv train loss:  -1.4437930923886597 , diff:  0.049777097534388304
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  103
test acc: top1 ->  10.0 ; top5 ->  56.84  and loss:  583.6660289764404
forward train acc: top1 ->  80.25599999755859 ; top5 ->  99.668  and loss:  58.44562703371048
test acc: top1 ->  81.03 ; top5 ->  98.97  and loss:  63.57746088504791
forward train acc: top1 ->  91.36399998535157 ; top5 ->  99.894  and loss:  25.27553680539131
test acc: top1 ->  85.86 ; top5 ->  99.13  and loss:  54.28680941462517
forward train acc: top1 ->  94.14599998046874 ; top5 ->  99.926  and loss:  17.539409659802914
test acc: top1 ->  87.22 ; top5 ->  99.17  and loss:  52.01887845993042
forward train acc: top1 ->  95.19199999267578 ; top5 ->  99.948  and loss:  14.055442340672016
test acc: top1 ->  87.85 ; top5 ->  99.22  and loss:  49.986991718411446
forward train acc: top1 ->  95.88799999267579 ; top5 ->  99.962  and loss:  11.931601971387863
test acc: top1 ->  88.33 ; top5 ->  99.16  and loss:  49.33047758042812
forward train acc: top1 ->  96.52799999267579 ; top5 ->  99.962  and loss:  10.350143313407898
test acc: top1 ->  88.69 ; top5 ->  99.21  and loss:  48.94346942007542
forward train acc: top1 ->  96.78200001708984 ; top5 ->  99.95599997558594  and loss:  9.50812641903758
test acc: top1 ->  88.53 ; top5 ->  99.21  and loss:  48.96064992249012
forward train acc: top1 ->  96.86599998779297 ; top5 ->  99.954  and loss:  9.154201697558165
test acc: top1 ->  88.78 ; top5 ->  99.24  and loss:  49.39866867661476
forward train acc: top1 ->  97.13399998779298 ; top5 ->  99.962  and loss:  8.652809012681246
test acc: top1 ->  88.73 ; top5 ->  99.19  and loss:  49.91984984278679
forward train acc: top1 ->  97.03399998535156 ; top5 ->  99.966  and loss:  8.503443647176027
test acc: top1 ->  88.91 ; top5 ->  99.18  and loss:  50.486692532896996
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  104 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -26.849805280566216 , diff:  26.849805280566216
adv train loss:  -27.40402427315712 , diff:  0.5542189925909042
adv train loss:  -26.821382448077202 , diff:  0.5826418250799179
adv train loss:  -26.309955276548862 , diff:  0.5114271715283394
adv train loss:  -27.121016517281532 , diff:  0.8110612407326698
adv train loss:  -26.884656697511673 , diff:  0.23635981976985931
adv train loss:  -27.689355358481407 , diff:  0.8046986609697342
adv train loss:  -27.16567572951317 , diff:  0.5236796289682388
adv train loss:  -27.09303693473339 , diff:  0.07263879477977753
adv train loss:  -26.82547613978386 , diff:  0.26756079494953156
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  72
test acc: top1 ->  14.06 ; top5 ->  92.7  and loss:  709.3430967330933
forward train acc: top1 ->  98.3920000024414 ; top5 ->  99.978  and loss:  5.297407295554876
test acc: top1 ->  91.6 ; top5 ->  98.92  and loss:  55.47267787158489
forward train acc: top1 ->  99.5960000024414 ; top5 ->  99.998  and loss:  1.3583236075937748
test acc: top1 ->  91.74 ; top5 ->  98.95  and loss:  55.28002578765154
forward train acc: top1 ->  99.68599997558594 ; top5 ->  100.0  and loss:  0.9788161828182638
test acc: top1 ->  91.84 ; top5 ->  99.1  and loss:  54.84006603807211
forward train acc: top1 ->  99.78000000244141 ; top5 ->  99.998  and loss:  0.6942605618387461
test acc: top1 ->  92.04 ; top5 ->  99.12  and loss:  55.92127403616905
forward train acc: top1 ->  99.784 ; top5 ->  99.996  and loss:  0.7061515430686995
test acc: top1 ->  91.95 ; top5 ->  99.16  and loss:  55.71631743758917
forward train acc: top1 ->  99.82999997558593 ; top5 ->  100.0  and loss:  0.5334097375161946
test acc: top1 ->  92.05 ; top5 ->  99.12  and loss:  56.652479372918606
forward train acc: top1 ->  99.836 ; top5 ->  100.0  and loss:  0.48807960376143456
test acc: top1 ->  92.14 ; top5 ->  99.1  and loss:  56.61574046313763
==> this epoch:  72 / 512
---------------- start layer  9  ---------------
adv train loss:  -0.928774768486619 , diff:  0.928774768486619
adv train loss:  -0.8931603413075209 , diff:  0.03561442717909813
adv train loss:  -1.0196196287870407 , diff:  0.12645928747951984
adv train loss:  -0.877393008209765 , diff:  0.14222662057727575
adv train loss:  -0.8796674041077495 , diff:  0.0022743958979845047
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  95
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  3960.1337394714355
forward train acc: top1 ->  90.95800001464843 ; top5 ->  98.828  and loss:  36.19504729285836
test acc: top1 ->  86.58 ; top5 ->  98.61  and loss:  63.89129197597504
forward train acc: top1 ->  99.00799997802734 ; top5 ->  99.988  and loss:  3.731978502124548
test acc: top1 ->  90.78 ; top5 ->  98.87  and loss:  51.26458728313446
forward train acc: top1 ->  99.422 ; top5 ->  99.988  and loss:  2.1332110166549683
test acc: top1 ->  91.17 ; top5 ->  98.84  and loss:  51.62166564166546
forward train acc: top1 ->  99.592 ; top5 ->  99.998  and loss:  1.4242936484515667
test acc: top1 ->  91.45 ; top5 ->  98.94  and loss:  52.80944313108921
forward train acc: top1 ->  99.656 ; top5 ->  99.998  and loss:  1.1429469212889671
test acc: top1 ->  91.51 ; top5 ->  99.01  and loss:  55.063316002488136
forward train acc: top1 ->  99.76200000244141 ; top5 ->  100.0  and loss:  0.8900365717709064
test acc: top1 ->  91.77 ; top5 ->  98.95  and loss:  53.45354683697224
forward train acc: top1 ->  99.77399997802735 ; top5 ->  100.0  and loss:  0.7865045480430126
test acc: top1 ->  91.76 ; top5 ->  99.03  and loss:  54.27731578052044
forward train acc: top1 ->  99.83 ; top5 ->  100.0  and loss:  0.6694050561636686
test acc: top1 ->  91.84 ; top5 ->  99.01  and loss:  54.638627380132675
forward train acc: top1 ->  99.794 ; top5 ->  99.998  and loss:  0.7095520943403244
test acc: top1 ->  91.86 ; top5 ->  99.01  and loss:  55.31478637456894
forward train acc: top1 ->  99.848 ; top5 ->  100.0  and loss:  0.5619236105121672
test acc: top1 ->  91.87 ; top5 ->  99.02  and loss:  56.47212980687618
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  96 / 512 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -78.30625778436661 , diff:  78.30625778436661
adv train loss:  -77.88705664873123 , diff:  0.419201135635376
adv train loss:  -78.31095498800278 , diff:  0.4238983392715454
adv train loss:  -78.03911679983139 , diff:  0.2718381881713867
adv train loss:  -78.28617471456528 , diff:  0.24705791473388672
adv train loss:  -77.31649005413055 , diff:  0.9696846604347229
adv train loss:  -78.17711907625198 , diff:  0.8606290221214294
adv train loss:  -78.41525799036026 , diff:  0.23813891410827637
adv train loss:  -78.14287883043289 , diff:  0.27237915992736816
adv train loss:  -78.93790173530579 , diff:  0.7950229048728943
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  21
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  1279.7548351287842
forward train acc: top1 ->  13.767999993286132 ; top5 ->  62.165999990234376  and loss:  268.6169228553772
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  668.4735250473022
forward train acc: top1 ->  38.71799999145508 ; top5 ->  96.57400000488282  and loss:  140.7998799085617
test acc: top1 ->  33.11 ; top5 ->  92.98  and loss:  167.2590743303299
forward train acc: top1 ->  55.64800000488281 ; top5 ->  99.52599997558593  and loss:  107.25132548809052
test acc: top1 ->  45.38 ; top5 ->  94.95  and loss:  145.81356346607208
forward train acc: top1 ->  61.489999992675784 ; top5 ->  99.70199997558593  and loss:  95.80199491977692
test acc: top1 ->  39.12 ; top5 ->  94.47  and loss:  153.0060350894928
forward train acc: top1 ->  67.70400000732423 ; top5 ->  99.82999997558593  and loss:  87.50086003541946
test acc: top1 ->  37.17 ; top5 ->  94.6  and loss:  162.47168219089508
forward train acc: top1 ->  72.28799997070313 ; top5 ->  99.824  and loss:  82.5995118021965
test acc: top1 ->  26.87 ; top5 ->  94.33  and loss:  182.85738945007324
forward train acc: top1 ->  76.15000000488281 ; top5 ->  99.852  and loss:  77.94455635547638
test acc: top1 ->  27.58 ; top5 ->  94.75  and loss:  174.40199375152588
forward train acc: top1 ->  76.44000001953125 ; top5 ->  99.88999997558594  and loss:  76.1598339676857
test acc: top1 ->  29.31 ; top5 ->  94.94  and loss:  170.33757400512695
forward train acc: top1 ->  78.68399997558593 ; top5 ->  99.894  and loss:  72.67051076889038
test acc: top1 ->  39.54 ; top5 ->  95.45  and loss:  150.86836206912994
forward train acc: top1 ->  80.23599997802734 ; top5 ->  99.898  and loss:  69.85198605060577
test acc: top1 ->  70.77 ; top5 ->  96.3  and loss:  117.52130770683289
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  22 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -4219.964744567871 , diff:  4219.964744567871
adv train loss:  -4401.940841674805 , diff:  181.9760971069336
adv train loss:  -4434.830429077148 , diff:  32.88958740234375
adv train loss:  -4486.5948486328125 , diff:  51.76441955566406
adv train loss:  -4668.006214141846 , diff:  181.4113655090332
adv train loss:  -4771.342247009277 , diff:  103.33603286743164
adv train loss:  -4770.776874542236 , diff:  0.5653724670410156
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  12
test acc: top1 ->  10.0 ; top5 ->  59.54  and loss:  45091.80908203125
forward train acc: top1 ->  98.06399997558594 ; top5 ->  99.99  and loss:  7.433128035161644
test acc: top1 ->  91.19 ; top5 ->  99.08  and loss:  55.26188336312771
forward train acc: top1 ->  99.704 ; top5 ->  99.996  and loss:  1.0432764606084675
test acc: top1 ->  91.61 ; top5 ->  99.04  and loss:  55.15937779843807
forward train acc: top1 ->  99.77999997558594 ; top5 ->  100.0  and loss:  0.7866619070991874
test acc: top1 ->  91.86 ; top5 ->  99.05  and loss:  56.44438601285219
forward train acc: top1 ->  99.8080000024414 ; top5 ->  100.0  and loss:  0.6181217348203063
test acc: top1 ->  91.75 ; top5 ->  99.11  and loss:  57.17165671288967
forward train acc: top1 ->  99.85999997558594 ; top5 ->  100.0  and loss:  0.4585782086942345
test acc: top1 ->  91.84 ; top5 ->  99.14  and loss:  59.35676900297403
forward train acc: top1 ->  99.868 ; top5 ->  99.998  and loss:  0.45774960913695395
test acc: top1 ->  92.03 ; top5 ->  99.14  and loss:  59.40715290606022
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  16 / 512 , inc:  4
---------------- start layer  12  ---------------
adv train loss:  -288.630722284317 , diff:  288.630722284317
adv train loss:  -289.9600167274475 , diff:  1.3292944431304932
adv train loss:  -289.0627374649048 , diff:  0.8972792625427246
adv train loss:  -288.82315707206726 , diff:  0.23958039283752441
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  40
test acc: top1 ->  10.0 ; top5 ->  68.31  and loss:  3987.842914581299
forward train acc: top1 ->  67.48399998291016 ; top5 ->  97.17  and loss:  137.2049830853939
test acc: top1 ->  79.64 ; top5 ->  98.11  and loss:  72.96723121404648
forward train acc: top1 ->  97.76200000976563 ; top5 ->  99.98  and loss:  37.95108526945114
test acc: top1 ->  88.29 ; top5 ->  97.85  and loss:  60.59237611293793
forward train acc: top1 ->  98.63400000244141 ; top5 ->  99.986  and loss:  25.061650276184082
test acc: top1 ->  88.82 ; top5 ->  97.48  and loss:  57.12438681721687
forward train acc: top1 ->  98.90399997802734 ; top5 ->  99.988  and loss:  18.082747161388397
test acc: top1 ->  89.02 ; top5 ->  97.56  and loss:  55.47255399823189
forward train acc: top1 ->  98.99600000488282 ; top5 ->  99.992  and loss:  13.67700196057558
test acc: top1 ->  89.22 ; top5 ->  97.65  and loss:  55.287584364414215
forward train acc: top1 ->  99.13400000488281 ; top5 ->  99.99  and loss:  11.190525844693184
test acc: top1 ->  89.35 ; top5 ->  97.63  and loss:  55.5894425958395
forward train acc: top1 ->  99.14399998046875 ; top5 ->  99.996  and loss:  10.08740995824337
test acc: top1 ->  89.29 ; top5 ->  97.61  and loss:  56.057676538825035
forward train acc: top1 ->  99.16399997558594 ; top5 ->  99.99  and loss:  8.94833543151617
test acc: top1 ->  89.44 ; top5 ->  97.58  and loss:  56.49325770139694
forward train acc: top1 ->  99.31399997558594 ; top5 ->  99.992  and loss:  7.789361737668514
test acc: top1 ->  89.56 ; top5 ->  97.6  and loss:  57.19207476079464
forward train acc: top1 ->  99.30199997558594 ; top5 ->  99.996  and loss:  7.080526657402515
test acc: top1 ->  89.52 ; top5 ->  97.55  and loss:  57.59924182295799
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  41 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -13599.562362670898 , diff:  13599.562362670898
adv train loss:  -21400.945083618164 , diff:  7801.382720947266
adv train loss:  -29193.345672607422 , diff:  7792.400588989258
adv train loss:  -37050.38693237305 , diff:  7857.041259765625
adv train loss:  -44984.384674072266 , diff:  7933.997741699219
adv train loss:  -52741.13314819336 , diff:  7756.748474121094
adv train loss:  -58310.939697265625 , diff:  5569.806549072266
adv train loss:  -59717.06231689453 , diff:  1406.1226196289062
adv train loss:  -60232.92639160156 , diff:  515.8640747070312
adv train loss:  -60423.03259277344 , diff:  190.106201171875
layer  13  adv train finish, try to retain  36
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.484375  ==>  31 / 64 , inc:  1
layer  1  :  0.875  ==>  56 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.41015625  ==>  105 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.140625  ==>  72 / 512 , inc:  2
layer  9  :  0.1875  ==>  96 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.03125  ==>  16 / 512 , inc:  2
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.06640625  ==>  34 / 512 , inc:  2
eps [1.3839609375000002, 0.0022806973457336426, 1.037970703125, 1.037970703125, 1.037970703125, 1.037970703125, 1.037970703125, 0.09730975341796876, 0.0030409297943115236, 0.0022806973457336426, 0.09730975341796876, 1.3839609375000002, 0.0022806973457336426, 14.762250000000002]  wait [2, 2, 4, 4, 4, 4, 4, 4, 0, 2, 4, 2, 2, 0]  inc [1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2]  tol: 4
$$$$$$$$$$$$$ epoch  38  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -2419.822790145874 , diff:  2419.822790145874
adv train loss:  -2492.2005729675293 , diff:  72.37778282165527
adv train loss:  -2487.294246673584 , diff:  4.9063262939453125
adv train loss:  -2525.001064300537 , diff:  37.706817626953125
adv train loss:  -2584.3886890411377 , diff:  59.387624740600586
adv train loss:  -2583.8546142578125 , diff:  0.5340747833251953
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  30
test acc: top1 ->  10.0 ; top5 ->  50.01  and loss:  1121055.650390625
forward train acc: top1 ->  94.41599997558593 ; top5 ->  99.906  and loss:  47.17829251661897
test acc: top1 ->  89.9 ; top5 ->  98.26  and loss:  124.30208134651184
forward train acc: top1 ->  99.36799997558593 ; top5 ->  99.99  and loss:  2.775065843015909
test acc: top1 ->  91.02 ; top5 ->  98.61  and loss:  103.51581344008446
forward train acc: top1 ->  99.47199997802734 ; top5 ->  99.996  and loss:  2.164399929344654
test acc: top1 ->  91.31 ; top5 ->  98.77  and loss:  98.10881540179253
forward train acc: top1 ->  99.616 ; top5 ->  99.998  and loss:  1.4180669066263363
test acc: top1 ->  91.23 ; top5 ->  98.8  and loss:  95.65990409255028
forward train acc: top1 ->  99.6200000024414 ; top5 ->  99.994  and loss:  1.555093340575695
test acc: top1 ->  91.37 ; top5 ->  98.92  and loss:  90.68410703539848
forward train acc: top1 ->  99.66399997558594 ; top5 ->  99.996  and loss:  1.337526856455952
test acc: top1 ->  91.41 ; top5 ->  98.91  and loss:  90.50432293117046
forward train acc: top1 ->  99.65199997558594 ; top5 ->  100.0  and loss:  1.1577591793611646
test acc: top1 ->  91.51 ; top5 ->  98.83  and loss:  88.13619129359722
forward train acc: top1 ->  99.728 ; top5 ->  99.996  and loss:  1.0277314570266753
test acc: top1 ->  91.54 ; top5 ->  98.92  and loss:  87.09048894047737
forward train acc: top1 ->  99.69599997558593 ; top5 ->  99.998  and loss:  1.1331378545146435
test acc: top1 ->  91.49 ; top5 ->  98.95  and loss:  85.786695510149
forward train acc: top1 ->  99.71400000244141 ; top5 ->  100.0  and loss:  1.0388010554015636
test acc: top1 ->  91.59 ; top5 ->  98.92  and loss:  84.90110325813293
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  31 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -0.3870978502673097 , diff:  0.3870978502673097
adv train loss:  -0.3517788715980714 , diff:  0.03531897866923828
adv train loss:  -0.36206753365695477 , diff:  0.010288662058883347
adv train loss:  -0.319980799453333 , diff:  0.042086734203621745
adv train loss:  -0.44032726250588894 , diff:  0.12034646305255592
adv train loss:  -0.3741669727023691 , diff:  0.06616028980351985
adv train loss:  -0.4551029036520049 , diff:  0.0809359309496358
adv train loss:  -0.3767775068990886 , diff:  0.07832539675291628
adv train loss:  -0.3882953096181154 , diff:  0.011517802719026804
adv train loss:  -0.33720018714666367 , diff:  0.05109512247145176
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
adv train loss:  -13.604946497827768 , diff:  13.604946497827768
adv train loss:  -13.58766034990549 , diff:  0.01728614792227745
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  70
test acc: top1 ->  19.83 ; top5 ->  79.3  and loss:  875.4151544570923
forward train acc: top1 ->  99.78199997558593 ; top5 ->  100.0  and loss:  0.692754466785118
test acc: top1 ->  91.83 ; top5 ->  98.97  and loss:  79.72001817822456
forward train acc: top1 ->  99.852 ; top5 ->  99.998  and loss:  0.43454238559206715
test acc: top1 ->  92.11 ; top5 ->  99.12  and loss:  79.29241216927767
==> this epoch:  70 / 512
---------------- start layer  9  ---------------
adv train loss:  -2.1453065648674965 , diff:  2.1453065648674965
adv train loss:  -2.015849861316383 , diff:  0.1294567035511136
adv train loss:  -2.0150847462937236 , diff:  0.0007651150226593018
layer  9  adv train finish, try to retain  459
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
### skip layer  10 wait:  4  ###
---------------- start layer  11  ---------------
adv train loss:  -2233.6377544403076 , diff:  2233.6377544403076
adv train loss:  -2411.482629776001 , diff:  177.84487533569336
adv train loss:  -2426.614803314209 , diff:  15.132173538208008
adv train loss:  -2396.7784938812256 , diff:  29.8363094329834
adv train loss:  -2392.334882736206 , diff:  4.443611145019531
adv train loss:  -2446.059829711914 , diff:  53.72494697570801
adv train loss:  -2464.4959716796875 , diff:  18.436141967773438
adv train loss:  -2465.1194248199463 , diff:  0.6234531402587891
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  14
test acc: top1 ->  10.0 ; top5 ->  61.86  and loss:  43105.7063293457
forward train acc: top1 ->  98.51799997558594 ; top5 ->  99.982  and loss:  6.39818402659148
test acc: top1 ->  91.21 ; top5 ->  98.36  and loss:  68.88096815347672
forward train acc: top1 ->  99.856 ; top5 ->  100.0  and loss:  0.4995625470764935
test acc: top1 ->  91.98 ; top5 ->  98.55  and loss:  67.88423713296652
forward train acc: top1 ->  99.85799997558594 ; top5 ->  100.0  and loss:  0.3708702917210758
test acc: top1 ->  91.93 ; top5 ->  98.63  and loss:  68.14591711014509
forward train acc: top1 ->  99.904 ; top5 ->  99.998  and loss:  0.31176073709502816
test acc: top1 ->  92.05 ; top5 ->  98.6  and loss:  69.24772902205586
forward train acc: top1 ->  99.874 ; top5 ->  100.0  and loss:  0.35084843315416947
test acc: top1 ->  92.2 ; top5 ->  98.56  and loss:  68.65299569815397
==> this epoch:  14 / 512
---------------- start layer  12  ---------------
adv train loss:  -75.27756509184837 , diff:  75.27756509184837
adv train loss:  -74.82090097665787 , diff:  0.456664115190506
adv train loss:  -74.30888530611992 , diff:  0.5120156705379486
adv train loss:  -74.99748802185059 , diff:  0.6886027157306671
adv train loss:  -75.5235338807106 , diff:  0.5260458588600159
adv train loss:  -74.67816182971 , diff:  0.8453720510005951
adv train loss:  -75.46938526630402 , diff:  0.7912234365940094
adv train loss:  -74.94127121567726 , diff:  0.5281140506267548
adv train loss:  -74.7514623105526 , diff:  0.1898089051246643
adv train loss:  -74.7421809732914 , diff:  0.009281337261199951
layer  12  adv train finish, try to retain  501
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -13136.594612121582 , diff:  13136.594612121582
adv train loss:  -21201.511169433594 , diff:  8064.916557312012
adv train loss:  -29197.472778320312 , diff:  7995.961608886719
adv train loss:  -37224.14303588867 , diff:  8026.670257568359
adv train loss:  -45293.956451416016 , diff:  8069.813415527344
adv train loss:  -53386.06851196289 , diff:  8092.112060546875
adv train loss:  -61494.06280517578 , diff:  8107.994293212891
adv train loss:  -69635.34851074219 , diff:  8141.285705566406
adv train loss:  -77767.63403320312 , diff:  8132.2855224609375
adv train loss:  -85901.5386352539 , diff:  8133.904602050781
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  32
test acc: top1 ->  19.64 ; top5 ->  88.66  and loss:  596.4505438804626
forward train acc: top1 ->  95.736 ; top5 ->  99.818  and loss:  18.051562851062045
test acc: top1 ->  91.74 ; top5 ->  98.76  and loss:  61.39421784877777
forward train acc: top1 ->  99.886 ; top5 ->  100.0  and loss:  0.4631407600827515
test acc: top1 ->  91.73 ; top5 ->  98.76  and loss:  61.946908447891474
forward train acc: top1 ->  99.902 ; top5 ->  100.0  and loss:  0.36544624040834606
test acc: top1 ->  91.86 ; top5 ->  98.83  and loss:  62.63498395308852
forward train acc: top1 ->  99.91599997558593 ; top5 ->  100.0  and loss:  0.3203953364863992
test acc: top1 ->  91.8 ; top5 ->  98.87  and loss:  63.044708639383316
forward train acc: top1 ->  99.924 ; top5 ->  99.998  and loss:  0.289743083238136
test acc: top1 ->  91.96 ; top5 ->  98.88  and loss:  62.841471649706364
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.28379193553701043
test acc: top1 ->  91.95 ; top5 ->  98.89  and loss:  62.83060812205076
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.2297357653733343
test acc: top1 ->  91.99 ; top5 ->  98.87  and loss:  63.04187476634979
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.21283840673277155
test acc: top1 ->  91.93 ; top5 ->  98.88  and loss:  63.6774789839983
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.24954048596555367
test acc: top1 ->  91.88 ; top5 ->  98.87  and loss:  64.28617607802153
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.21326049912022427
test acc: top1 ->  91.96 ; top5 ->  98.87  and loss:  64.35716390609741
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  34 / 512 , inc:  2
layer  0  :  0.484375  ==>  31 / 64 , inc:  1
layer  1  :  0.875  ==>  56 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.41015625  ==>  105 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.13671875  ==>  70 / 512 , inc:  4
layer  9  :  0.1875  ==>  96 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.02734375  ==>  14 / 512 , inc:  4
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.06640625  ==>  34 / 512 , inc:  1
eps [1.037970703125, 0.004561394691467285, 1.037970703125, 1.037970703125, 1.037970703125, 1.037970703125, 1.037970703125, 0.09730975341796876, 0.0030409297943115236, 0.004561394691467285, 0.09730975341796876, 1.3839609375000002, 0.004561394691467285, 11.071687500000001]  wait [4, 2, 3, 3, 3, 3, 3, 3, 0, 2, 3, 0, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 4, 1, 1]  tol: 4
$$$$$$$$$$$$$ epoch  39  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
adv train loss:  -51.81685635447502 , diff:  51.81685635447502
adv train loss:  -51.46928787231445 , diff:  0.34756848216056824
adv train loss:  -51.20729401707649 , diff:  0.2619938552379608
adv train loss:  -51.6651032269001 , diff:  0.4578092098236084
adv train loss:  -52.06731626391411 , diff:  0.40221303701400757
adv train loss:  -51.584351032972336 , diff:  0.48296523094177246
adv train loss:  -50.894578635692596 , diff:  0.6897723972797394
adv train loss:  -51.32157975435257 , diff:  0.42700111865997314
adv train loss:  -51.80926561355591 , diff:  0.4876858592033386
adv train loss:  -50.959034979343414 , diff:  0.8502306342124939
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  3  ###
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
adv train loss:  -109.44327169656754 , diff:  109.44327169656754
adv train loss:  -108.40642720460892 , diff:  1.0368444919586182
adv train loss:  -110.2046217918396 , diff:  1.7981945872306824
adv train loss:  -108.98667246103287 , diff:  1.2179493308067322
adv train loss:  -109.63414460420609 , diff:  0.6474721431732178
adv train loss:  -109.27965354919434 , diff:  0.35449105501174927
adv train loss:  -109.61484014987946 , diff:  0.33518660068511963
adv train loss:  -109.6422181725502 , diff:  0.02737802267074585
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  66
test acc: top1 ->  35.21 ; top5 ->  62.28  and loss:  889.7419490814209
forward train acc: top1 ->  99.456 ; top5 ->  99.998  and loss:  2.157741882256232
test acc: top1 ->  91.99 ; top5 ->  98.73  and loss:  79.1814454048872
forward train acc: top1 ->  99.928 ; top5 ->  100.0  and loss:  0.23378969586337917
test acc: top1 ->  92.21 ; top5 ->  98.82  and loss:  78.77340476214886
==> this epoch:  66 / 512
---------------- start layer  9  ---------------
adv train loss:  -0.48249368369579315 , diff:  0.48249368369579315
adv train loss:  -0.3779950032476336 , diff:  0.10449868044815958
adv train loss:  -0.42962846159935 , diff:  0.0516334583517164
adv train loss:  -0.38176412670873106 , diff:  0.04786433489061892
adv train loss:  -0.3677196763455868 , diff:  0.014044450363144279
adv train loss:  -0.4012725332286209 , diff:  0.03355285688303411
adv train loss:  -0.43586152720672544 , diff:  0.034588993978104554
adv train loss:  -0.40869776369072497 , diff:  0.027163763516000472
adv train loss:  -0.4857670906931162 , diff:  0.07706932700239122
adv train loss:  -0.5350241884589195 , diff:  0.04925709776580334
layer  9  adv train finish, try to retain  442
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
### skip layer  10 wait:  3  ###
---------------- start layer  11  ---------------
adv train loss:  -4202.096607208252 , diff:  4202.096607208252
adv train loss:  -4356.082347869873 , diff:  153.9857406616211
adv train loss:  -4373.04386138916 , diff:  16.96151351928711
adv train loss:  -4373.762157440186 , diff:  0.7182960510253906
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  10
test acc: top1 ->  10.0 ; top5 ->  59.31  and loss:  41136.90591430664
forward train acc: top1 ->  99.894 ; top5 ->  100.0  and loss:  0.4422360037569888
test acc: top1 ->  91.83 ; top5 ->  98.61  and loss:  78.74971841275692
forward train acc: top1 ->  99.90999997558593 ; top5 ->  100.0  and loss:  0.20517423073761165
test acc: top1 ->  91.94 ; top5 ->  98.68  and loss:  82.26436150074005
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.16525985655607656
test acc: top1 ->  91.86 ; top5 ->  98.74  and loss:  82.69594965875149
forward train acc: top1 ->  99.92799997558593 ; top5 ->  100.0  and loss:  0.2214189968071878
test acc: top1 ->  91.24 ; top5 ->  98.48  and loss:  90.91091223061085
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.1957848873862531
test acc: top1 ->  92.04 ; top5 ->  98.8  and loss:  83.92983402311802
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.17738275726151187
test acc: top1 ->  92.02 ; top5 ->  98.81  and loss:  82.48817613720894
forward train acc: top1 ->  99.95399997558594 ; top5 ->  100.0  and loss:  0.13407204346731305
test acc: top1 ->  91.97 ; top5 ->  98.95  and loss:  82.72085799276829
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.19319978635758162
test acc: top1 ->  92.11 ; top5 ->  98.92  and loss:  81.45440404117107
==> this epoch:  10 / 512
---------------- start layer  12  ---------------
adv train loss:  -16.020394131541252 , diff:  16.020394131541252
adv train loss:  -16.097364492714405 , diff:  0.07697036117315292
adv train loss:  -15.33374971151352 , diff:  0.7636147812008858
adv train loss:  -15.77846060693264 , diff:  0.4447108954191208
adv train loss:  -15.474211268126965 , diff:  0.3042493388056755
adv train loss:  -15.8294061049819 , diff:  0.3551948368549347
adv train loss:  -16.226893961429596 , diff:  0.3974878564476967
adv train loss:  -16.12866111844778 , diff:  0.09823284298181534
adv train loss:  -15.730048514902592 , diff:  0.3986126035451889
adv train loss:  -15.798947624862194 , diff:  0.06889910995960236
layer  12  adv train finish, try to retain  496
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -18867.4588470459 , diff:  18867.4588470459
adv train loss:  -31715.295654296875 , diff:  12847.836807250977
adv train loss:  -43562.938568115234 , diff:  11847.64291381836
adv train loss:  -55130.533630371094 , diff:  11567.59506225586
adv train loss:  -66563.92944335938 , diff:  11433.395812988281
adv train loss:  -77920.77423095703 , diff:  11356.844787597656
adv train loss:  -89209.12768554688 , diff:  11288.353454589844
adv train loss:  -100506.578125 , diff:  11297.450439453125
adv train loss:  -111646.87341308594 , diff:  11140.295288085938
adv train loss:  -121266.83154296875 , diff:  9619.958129882812
layer  13  adv train finish, try to retain  24
test acc: top1 ->  33.22 ; top5 ->  78.39  and loss:  1108.9622764587402
forward train acc: top1 ->  89.986 ; top5 ->  99.378  and loss:  93.97704634233378
test acc: top1 ->  91.59 ; top5 ->  98.71  and loss:  78.0430315732956
forward train acc: top1 ->  99.86599997558594 ; top5 ->  99.998  and loss:  0.6183073841966689
test acc: top1 ->  91.84 ; top5 ->  98.74  and loss:  75.88257040083408
forward train acc: top1 ->  99.92799997558593 ; top5 ->  99.996  and loss:  0.2929114205762744
test acc: top1 ->  91.96 ; top5 ->  98.79  and loss:  72.91503116488457
forward train acc: top1 ->  99.94 ; top5 ->  100.0  and loss:  0.26724194665439427
test acc: top1 ->  91.99 ; top5 ->  98.77  and loss:  72.54441110789776
forward train acc: top1 ->  99.94199997558594 ; top5 ->  100.0  and loss:  0.20620944956317544
test acc: top1 ->  92.06 ; top5 ->  98.78  and loss:  71.57890872657299
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.1754948639572831
test acc: top1 ->  92.12 ; top5 ->  98.76  and loss:  71.0335564315319
==> this epoch:  24 / 512
layer  0  :  0.484375  ==>  31 / 64 , inc:  1
layer  1  :  0.875  ==>  56 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.41015625  ==>  105 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.12890625  ==>  66 / 512 , inc:  8
layer  9  :  0.1875  ==>  96 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.01953125  ==>  10 / 512 , inc:  5
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.046875  ==>  24 / 512 , inc:  2
eps [1.037970703125, 0.00912278938293457, 1.037970703125, 1.037970703125, 1.037970703125, 1.037970703125, 1.037970703125, 0.09730975341796876, 0.0030409297943115236, 0.00912278938293457, 0.09730975341796876, 1.3839609375000002, 0.00912278938293457, 11.071687500000001]  wait [3, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0]  inc [1, 1, 1, 1, 1, 1, 1, 1, 8, 1, 1, 5, 1, 2]  tol: 4
$$$$$$$$$$$$$ epoch  40  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
adv train loss:  -0.20883505092933774 , diff:  0.20883505092933774
adv train loss:  -0.22957406169734895 , diff:  0.020739010768011212
adv train loss:  -0.23744356393581256 , diff:  0.00786950223846361
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -1262.0512713752687 , diff:  1262.0512713752687
adv train loss:  -1984.790283203125 , diff:  722.7390118278563
adv train loss:  -1988.8607769012451 , diff:  4.070493698120117
adv train loss:  -1983.5123462677002 , diff:  5.348430633544922
adv train loss:  -1994.993034362793 , diff:  11.480688095092773
adv train loss:  -1997.6763725280762 , diff:  2.683338165283203
adv train loss:  -1992.9997806549072 , diff:  4.676591873168945
adv train loss:  -1992.5227317810059 , diff:  0.4770488739013672
adv train loss:  -1988.8284912109375 , diff:  3.6942405700683594
adv train loss:  -1998.7277297973633 , diff:  9.899238586425781
layer  2  adv train finish, try to retain  119
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -1261.130048993975 , diff:  1261.130048993975
adv train loss:  -2224.3222274780273 , diff:  963.1921784840524
adv train loss:  -2227.544967651367 , diff:  3.2227401733398438
adv train loss:  -2236.4858112335205 , diff:  8.94084358215332
adv train loss:  -2240.860502243042 , diff:  4.374691009521484
adv train loss:  -2231.7495517730713 , diff:  9.110950469970703
adv train loss:  -2233.1567821502686 , diff:  1.4072303771972656
adv train loss:  -2261.297296524048 , diff:  28.140514373779297
adv train loss:  -2301.9810638427734 , diff:  40.683767318725586
adv train loss:  -2300.6058349609375 , diff:  1.3752288818359375
layer  3  adv train finish, try to retain  117
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -382.216960484162 , diff:  382.216960484162
adv train loss:  -2026.8719701766968 , diff:  1644.6550096925348
adv train loss:  -2608.7942543029785 , diff:  581.9222841262817
adv train loss:  -2697.8771572113037 , diff:  89.0829029083252
adv train loss:  -2729.783332824707 , diff:  31.90617561340332
adv train loss:  -2778.1286029815674 , diff:  48.34527015686035
adv train loss:  -2871.1884803771973 , diff:  93.05987739562988
adv train loss:  -2962.797281265259 , diff:  91.60880088806152
adv train loss:  -2965.536329269409 , diff:  2.7390480041503906
adv train loss:  -2972.530637741089 , diff:  6.9943084716796875
layer  4  adv train finish, try to retain  206
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -975.6482023969293 , diff:  975.6482023969293
adv train loss:  -2982.3328132629395 , diff:  2006.6846108660102
adv train loss:  -3079.774950027466 , diff:  97.44213676452637
adv train loss:  -3130.886127471924 , diff:  51.11117744445801
adv train loss:  -3160.724145889282 , diff:  29.8380184173584
adv train loss:  -3172.908773422241 , diff:  12.184627532958984
adv train loss:  -3166.232988357544 , diff:  6.675785064697266
adv train loss:  -3211.6902389526367 , diff:  45.45725059509277
adv train loss:  -3221.766269683838 , diff:  10.076030731201172
adv train loss:  -3250.3798999786377 , diff:  28.613630294799805
layer  5  adv train finish, try to retain  213
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -904.5873457361013 , diff:  904.5873457361013
adv train loss:  -2982.5966453552246 , diff:  2078.0092996191233
adv train loss:  -3135.4122772216797 , diff:  152.81563186645508
adv train loss:  -3245.8063945770264 , diff:  110.39411735534668
adv train loss:  -3229.221622467041 , diff:  16.58477210998535
adv train loss:  -3225.628797531128 , diff:  3.592824935913086
adv train loss:  -3238.539218902588 , diff:  12.910421371459961
adv train loss:  -3232.0609951019287 , diff:  6.47822380065918
adv train loss:  -3236.521966934204 , diff:  4.460971832275391
adv train loss:  -3237.769105911255 , diff:  1.2471389770507812
layer  6  adv train finish, try to retain  226
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -0.33203967416193336 , diff:  0.33203967416193336
adv train loss:  -0.3281248306739144 , diff:  0.00391484348801896
layer  7  adv train finish, try to retain  408
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -13.663893476128578 , diff:  13.663893476128578
adv train loss:  -13.40369701385498 , diff:  0.2601964622735977
adv train loss:  -13.604981664568186 , diff:  0.20128465071320534
adv train loss:  -13.94468366727233 , diff:  0.3397020027041435
adv train loss:  -13.196106921881437 , diff:  0.748576745390892
adv train loss:  -13.994165152311325 , diff:  0.7980582304298878
adv train loss:  -13.949562937021255 , diff:  0.04460221529006958
adv train loss:  -13.626454543322325 , diff:  0.32310839369893074
adv train loss:  -13.703705668449402 , diff:  0.0772511251270771
adv train loss:  -13.244974810630083 , diff:  0.45873085781931877
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  58
test acc: top1 ->  19.46 ; top5 ->  68.1  and loss:  795.8439087867737
forward train acc: top1 ->  99.93 ; top5 ->  99.998  and loss:  0.2885990805370966
test acc: top1 ->  91.86 ; top5 ->  98.53  and loss:  78.55911564826965
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.17982392092744703
test acc: top1 ->  92.03 ; top5 ->  98.84  and loss:  79.45733053982258
forward train acc: top1 ->  99.94 ; top5 ->  100.0  and loss:  0.16558447178977076
test acc: top1 ->  92.14 ; top5 ->  98.81  and loss:  82.7740885913372
==> this epoch:  58 / 512
---------------- start layer  9  ---------------
adv train loss:  -0.21285884508688468 , diff:  0.21285884508688468
adv train loss:  -0.22171114978846163 , diff:  0.00885230470157694
layer  9  adv train finish, try to retain  420
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -49.60804796218872 , diff:  49.60804796218872
adv train loss:  -49.826863810420036 , diff:  0.2188158482313156
adv train loss:  -49.686368241906166 , diff:  0.14049556851387024
adv train loss:  -50.0142752379179 , diff:  0.327906996011734
adv train loss:  -49.86769261956215 , diff:  0.14658261835575104
adv train loss:  -49.82053188979626 , diff:  0.04716072976589203
layer  10  adv train finish, try to retain  463
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -3816.4452934265137 , diff:  3816.4452934265137
adv train loss:  -4295.881237030029 , diff:  479.4359436035156
adv train loss:  -4383.900997161865 , diff:  88.01976013183594
adv train loss:  -4387.925022125244 , diff:  4.024024963378906
adv train loss:  -4390.251445770264 , diff:  2.3264236450195312
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  5
test acc: top1 ->  10.05 ; top5 ->  50.0  and loss:  18484.944625854492
forward train acc: top1 ->  97.536 ; top5 ->  99.6  and loss:  11.464051579590887
test acc: top1 ->  91.33 ; top5 ->  98.86  and loss:  51.66812199354172
forward train acc: top1 ->  99.896 ; top5 ->  100.0  and loss:  0.42390589276328683
test acc: top1 ->  91.93 ; top5 ->  98.92  and loss:  52.19165671616793
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.29526203812565655
test acc: top1 ->  91.98 ; top5 ->  98.97  and loss:  54.09335769712925
forward train acc: top1 ->  99.93799997558594 ; top5 ->  100.0  and loss:  0.2593648638576269
test acc: top1 ->  92.13 ; top5 ->  98.97  and loss:  54.917709171772
==> this epoch:  5 / 512
---------------- start layer  12  ---------------
adv train loss:  -231.37630379199982 , diff:  231.37630379199982
adv train loss:  -232.97417616844177 , diff:  1.5978723764419556
adv train loss:  -232.02577471733093 , diff:  0.9484014511108398
adv train loss:  -232.4742832183838 , diff:  0.44850850105285645
layer  12  adv train finish, try to retain  493
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -22499.199737548828 , diff:  22499.199737548828
adv train loss:  -38179.35107421875 , diff:  15680.151336669922
adv train loss:  -53618.1819152832 , diff:  15438.830841064453
adv train loss:  -68852.34014892578 , diff:  15234.158233642578
adv train loss:  -83919.20782470703 , diff:  15066.86767578125
adv train loss:  -98874.71728515625 , diff:  14955.509460449219
adv train loss:  -113779.76379394531 , diff:  14905.046508789062
adv train loss:  -128618.55822753906 , diff:  14838.79443359375
adv train loss:  -143119.8634033203 , diff:  14501.30517578125
adv train loss:  -154470.68981933594 , diff:  11350.826416015625
layer  13  adv train finish, try to retain  32
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.484375  ==>  31 / 64 , inc:  1
layer  1  :  0.875  ==>  56 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.41015625  ==>  105 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.11328125  ==>  58 / 512 , inc:  16
layer  9  :  0.1875  ==>  96 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.009765625  ==>  5 / 512 , inc:  2
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.046875  ==>  24 / 512 , inc:  2
eps [1.037970703125, 0.01824557876586914, 2.07594140625, 2.07594140625, 2.07594140625, 2.07594140625, 2.07594140625, 0.1946195068359375, 0.0030409297943115236, 0.01824557876586914, 0.1946195068359375, 1.3839609375000002, 0.01824557876586914, 22.143375000000002]  wait [2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0]  inc [1, 1, 1, 1, 1, 1, 1, 1, 16, 1, 1, 2, 1, 2]  tol: 4
$$$$$$$$$$$$$ epoch  41  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -826.4240748882294 , diff:  826.4240748882294
adv train loss:  -882.1213045120239 , diff:  55.697229623794556
adv train loss:  -886.3022122383118 , diff:  4.180907726287842
adv train loss:  -887.2439303398132 , diff:  0.9417181015014648
adv train loss:  -887.088716506958 , diff:  0.1552138328552246
adv train loss:  -882.6824426651001 , diff:  4.40627384185791
adv train loss:  -883.877833366394 , diff:  1.1953907012939453
adv train loss:  -885.012951374054 , diff:  1.135118007659912
adv train loss:  -880.2377939224243 , diff:  4.775157451629639
adv train loss:  -882.8157157897949 , diff:  2.5779218673706055
layer  0  adv train finish, try to retain  54
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -0.21959889202844352 , diff:  0.21959889202844352
adv train loss:  -0.17722682724706829 , diff:  0.04237206478137523
adv train loss:  -0.21585580392275006 , diff:  0.03862897667568177
adv train loss:  -0.2405602764338255 , diff:  0.024704472511075437
adv train loss:  -0.19850298576056957 , diff:  0.04205729067325592
adv train loss:  -0.20830121089238673 , diff:  0.009798225131817162
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  55
test acc: top1 ->  29.79 ; top5 ->  67.13  and loss:  748.7498788833618
forward train acc: top1 ->  99.60399997558594 ; top5 ->  99.998  and loss:  1.2649440728127956
test acc: top1 ->  91.19 ; top5 ->  98.81  and loss:  62.61833856254816
forward train acc: top1 ->  99.696 ; top5 ->  99.996  and loss:  1.0184367516776547
test acc: top1 ->  91.38 ; top5 ->  98.97  and loss:  58.21420281380415
forward train acc: top1 ->  99.7960000024414 ; top5 ->  100.0  and loss:  0.6205425709486008
test acc: top1 ->  91.46 ; top5 ->  99.02  and loss:  59.8558634519577
forward train acc: top1 ->  99.77999997558594 ; top5 ->  100.0  and loss:  0.6602215617895126
test acc: top1 ->  91.45 ; top5 ->  98.87  and loss:  62.237177670001984
forward train acc: top1 ->  99.742 ; top5 ->  99.996  and loss:  0.693943876773119
test acc: top1 ->  91.59 ; top5 ->  98.86  and loss:  60.921342208981514
forward train acc: top1 ->  99.846 ; top5 ->  99.998  and loss:  0.507744072645437
test acc: top1 ->  91.57 ; top5 ->  99.02  and loss:  60.936833806335926
forward train acc: top1 ->  99.852 ; top5 ->  100.0  and loss:  0.48187054647132754
test acc: top1 ->  91.5 ; top5 ->  99.06  and loss:  61.59311235696077
forward train acc: top1 ->  99.862 ; top5 ->  100.0  and loss:  0.40953237016219646
test acc: top1 ->  91.67 ; top5 ->  98.96  and loss:  62.09961551427841
forward train acc: top1 ->  99.85399997558594 ; top5 ->  100.0  and loss:  0.41145002376288176
test acc: top1 ->  91.5 ; top5 ->  98.98  and loss:  62.44341865181923
forward train acc: top1 ->  99.84199997802735 ; top5 ->  99.998  and loss:  0.3994244374334812
test acc: top1 ->  91.58 ; top5 ->  99.1  and loss:  61.843685895204544
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  56 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -864.8811144009233 , diff:  864.8811144009233
adv train loss:  -1403.8266258239746 , diff:  538.9455114230514
adv train loss:  -1417.804033279419 , diff:  13.977407455444336
adv train loss:  -1422.2297916412354 , diff:  4.425758361816406
adv train loss:  -1423.3545722961426 , diff:  1.1247806549072266
adv train loss:  -1426.4308109283447 , diff:  3.0762386322021484
adv train loss:  -1422.8730430603027 , diff:  3.557767868041992
adv train loss:  -1421.2801055908203 , diff:  1.5929374694824219
adv train loss:  -1417.1832075119019 , diff:  4.096898078918457
adv train loss:  -1425.9105033874512 , diff:  8.727295875549316
layer  2  adv train finish, try to retain  36
test acc: top1 ->  10.0 ; top5 ->  50.04  and loss:  2065.129404067993
forward train acc: top1 ->  82.258 ; top5 ->  98.04400000244141  and loss:  73.44896936416626
test acc: top1 ->  78.97 ; top5 ->  97.06  and loss:  73.63393631577492
forward train acc: top1 ->  84.9999999951172 ; top5 ->  98.59800000244141  and loss:  48.01097047328949
test acc: top1 ->  81.11 ; top5 ->  97.75  and loss:  65.25305485725403
forward train acc: top1 ->  86.93199998291016 ; top5 ->  98.96599997802734  and loss:  41.088459491729736
test acc: top1 ->  82.28 ; top5 ->  98.16  and loss:  60.28445693850517
forward train acc: top1 ->  88.43999999755859 ; top5 ->  99.2240000024414  and loss:  36.071592301130295
test acc: top1 ->  83.11 ; top5 ->  98.11  and loss:  57.10226747393608
forward train acc: top1 ->  89.46999998779297 ; top5 ->  99.29599997802734  and loss:  32.47495883703232
test acc: top1 ->  83.99 ; top5 ->  98.43  and loss:  54.33119547367096
forward train acc: top1 ->  90.20799999511719 ; top5 ->  99.436  and loss:  30.09625083208084
test acc: top1 ->  84.38 ; top5 ->  98.53  and loss:  53.964575082063675
forward train acc: top1 ->  90.71199998291016 ; top5 ->  99.47199997802734  and loss:  28.44211934506893
test acc: top1 ->  84.51 ; top5 ->  98.53  and loss:  52.5885069668293
forward train acc: top1 ->  91.05399997802735 ; top5 ->  99.5100000024414  and loss:  27.39025440812111
test acc: top1 ->  84.76 ; top5 ->  98.68  and loss:  51.76954133808613
forward train acc: top1 ->  91.42000000488281 ; top5 ->  99.52799997558594  and loss:  26.64118930697441
test acc: top1 ->  85.07 ; top5 ->  98.65  and loss:  51.18565398454666
forward train acc: top1 ->  91.53199999023437 ; top5 ->  99.52399997558594  and loss:  26.209838211536407
test acc: top1 ->  85.24 ; top5 ->  98.68  and loss:  49.752247884869576
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -420.79439666867256 , diff:  420.79439666867256
adv train loss:  -689.9263501167297 , diff:  269.1319534480572
adv train loss:  -691.1037712097168 , diff:  1.1774210929870605
adv train loss:  -690.0841627120972 , diff:  1.019608497619629
adv train loss:  -695.0529565811157 , diff:  4.968793869018555
adv train loss:  -692.6894011497498 , diff:  2.363555431365967
adv train loss:  -692.538544178009 , diff:  0.15085697174072266
adv train loss:  -693.1207976341248 , diff:  0.5822534561157227
adv train loss:  -694.2432203292847 , diff:  1.122422695159912
adv train loss:  -692.8989849090576 , diff:  1.3442354202270508
layer  3  adv train finish, try to retain  23
test acc: top1 ->  10.46 ; top5 ->  50.0  and loss:  644.8689012527466
forward train acc: top1 ->  69.45399999023438 ; top5 ->  95.87999998535156  and loss:  93.23800700902939
test acc: top1 ->  70.29 ; top5 ->  96.78  and loss:  89.31352984905243
forward train acc: top1 ->  74.75199997070312 ; top5 ->  97.62000000976562  and loss:  73.9949301481247
test acc: top1 ->  73.87 ; top5 ->  97.65  and loss:  79.37705326080322
forward train acc: top1 ->  78.01400000732421 ; top5 ->  98.15800000488281  and loss:  65.18033009767532
test acc: top1 ->  75.76 ; top5 ->  97.89  and loss:  74.9552454650402
forward train acc: top1 ->  79.65000000732422 ; top5 ->  98.49799998046875  and loss:  60.02412575483322
test acc: top1 ->  76.59 ; top5 ->  98.0  and loss:  72.47667515277863
forward train acc: top1 ->  80.69599999755859 ; top5 ->  98.56000000976563  and loss:  56.99333420395851
test acc: top1 ->  77.7 ; top5 ->  98.02  and loss:  68.60694918036461
forward train acc: top1 ->  81.64600000976563 ; top5 ->  98.79800000244141  and loss:  53.829743683338165
test acc: top1 ->  78.26 ; top5 ->  98.18  and loss:  66.70309022068977
forward train acc: top1 ->  81.90800000244141 ; top5 ->  98.71400000732422  and loss:  53.13415244221687
test acc: top1 ->  78.52 ; top5 ->  98.22  and loss:  65.72527003288269
forward train acc: top1 ->  82.57199997802735 ; top5 ->  98.83000000976563  and loss:  51.21273970603943
test acc: top1 ->  78.68 ; top5 ->  98.13  and loss:  65.77145820856094
forward train acc: top1 ->  82.70799999755859 ; top5 ->  98.86200000488282  and loss:  50.50599077343941
test acc: top1 ->  79.19 ; top5 ->  98.3  and loss:  64.40373134613037
forward train acc: top1 ->  83.42999999511719 ; top5 ->  98.95200000488282  and loss:  49.04144859313965
test acc: top1 ->  79.41 ; top5 ->  98.28  and loss:  63.46312427520752
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -108.0162915289402 , diff:  108.0162915289402
adv train loss:  -523.7118191719055 , diff:  415.6955276429653
adv train loss:  -561.9616751670837 , diff:  38.24985599517822
adv train loss:  -617.6710729598999 , diff:  55.70939779281616
adv train loss:  -637.2619743347168 , diff:  19.590901374816895
adv train loss:  -650.5300436019897 , diff:  13.26806926727295
adv train loss:  -660.7643623352051 , diff:  10.234318733215332
adv train loss:  -670.5018243789673 , diff:  9.737462043762207
adv train loss:  -671.9337587356567 , diff:  1.4319343566894531
adv train loss:  -671.0583920478821 , diff:  0.8753666877746582
layer  4  adv train finish, try to retain  18
test acc: top1 ->  9.48 ; top5 ->  50.08  and loss:  953.1865191459656
forward train acc: top1 ->  58.011999995117186 ; top5 ->  92.8600000048828  and loss:  124.71220028400421
test acc: top1 ->  61.53 ; top5 ->  94.25  and loss:  114.58648765087128
forward train acc: top1 ->  66.34599998779296 ; top5 ->  95.78199997802734  and loss:  96.99751192331314
test acc: top1 ->  66.9 ; top5 ->  95.83  and loss:  98.7376207113266
forward train acc: top1 ->  70.27600001464843 ; top5 ->  96.72800000732421  and loss:  85.59441763162613
test acc: top1 ->  69.9 ; top5 ->  96.49  and loss:  90.7923994064331
forward train acc: top1 ->  72.64800000732421 ; top5 ->  97.11200001220703  and loss:  78.65298628807068
test acc: top1 ->  72.47 ; top5 ->  97.16  and loss:  83.54595738649368
forward train acc: top1 ->  74.42399998046875 ; top5 ->  97.52800000488281  and loss:  73.50434684753418
test acc: top1 ->  73.74 ; top5 ->  97.45  and loss:  79.35113310813904
forward train acc: top1 ->  75.68399999267578 ; top5 ->  97.81600000976563  and loss:  69.76303642988205
test acc: top1 ->  74.26 ; top5 ->  97.52  and loss:  77.51690417528152
forward train acc: top1 ->  76.32000001220703 ; top5 ->  97.83399998046875  and loss:  68.1175365447998
test acc: top1 ->  74.46 ; top5 ->  97.39  and loss:  77.66073602437973
forward train acc: top1 ->  77.19399999755859 ; top5 ->  98.09800000976563  and loss:  65.89487320184708
test acc: top1 ->  75.19 ; top5 ->  97.67  and loss:  75.19152241945267
forward train acc: top1 ->  77.55400000976563 ; top5 ->  98.07799997558594  and loss:  64.42808902263641
test acc: top1 ->  75.31 ; top5 ->  97.65  and loss:  74.8820571899414
forward train acc: top1 ->  77.80199997070312 ; top5 ->  98.12200000732422  and loss:  63.71181130409241
test acc: top1 ->  76.1 ; top5 ->  97.83  and loss:  72.28490948677063
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  156 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -172.70035475492477 , diff:  172.70035475492477
adv train loss:  -592.6158685684204 , diff:  419.91551381349564
adv train loss:  -684.3863563537598 , diff:  91.77048778533936
adv train loss:  -723.7815580368042 , diff:  39.395201683044434
adv train loss:  -742.0473675727844 , diff:  18.265809535980225
adv train loss:  -752.2887077331543 , diff:  10.241340160369873
adv train loss:  -753.569091796875 , diff:  1.2803840637207031
adv train loss:  -755.2012987136841 , diff:  1.632206916809082
adv train loss:  -758.8722696304321 , diff:  3.670970916748047
adv train loss:  -757.1877179145813 , diff:  1.68455171585083
layer  5  adv train finish, try to retain  17
test acc: top1 ->  14.84 ; top5 ->  57.52  and loss:  410.0499324798584
forward train acc: top1 ->  72.60799999511718 ; top5 ->  97.98399997802734  and loss:  77.35240364074707
test acc: top1 ->  73.22 ; top5 ->  98.06  and loss:  79.1326892375946
forward train acc: top1 ->  79.09399997558593 ; top5 ->  98.8460000024414  and loss:  59.345456659793854
test acc: top1 ->  76.74 ; top5 ->  98.45  and loss:  70.02439877390862
forward train acc: top1 ->  81.46999998291015 ; top5 ->  99.14199997558593  and loss:  52.38511145114899
test acc: top1 ->  78.44 ; top5 ->  98.65  and loss:  65.99850350618362
forward train acc: top1 ->  83.08599999511719 ; top5 ->  99.24599997558593  and loss:  47.98931735754013
test acc: top1 ->  79.7 ; top5 ->  98.79  and loss:  62.26411950588226
forward train acc: top1 ->  84.10400001464843 ; top5 ->  99.33000000488282  and loss:  45.273974388837814
test acc: top1 ->  80.78 ; top5 ->  98.84  and loss:  58.749752938747406
forward train acc: top1 ->  84.87600000732422 ; top5 ->  99.37399997802734  and loss:  42.87131395936012
test acc: top1 ->  80.99 ; top5 ->  98.83  and loss:  58.45262721180916
forward train acc: top1 ->  85.51200001464844 ; top5 ->  99.39999997802734  and loss:  41.274862825870514
test acc: top1 ->  81.53 ; top5 ->  98.93  and loss:  57.78230473399162
forward train acc: top1 ->  85.62600001220703 ; top5 ->  99.43800000244141  and loss:  40.912517726421356
test acc: top1 ->  81.76 ; top5 ->  98.92  and loss:  56.85216075181961
forward train acc: top1 ->  86.13200000732422 ; top5 ->  99.4420000024414  and loss:  39.702223896980286
test acc: top1 ->  82.19 ; top5 ->  98.97  and loss:  56.30155295133591
forward train acc: top1 ->  86.18199998535157 ; top5 ->  99.43  and loss:  39.0923573076725
test acc: top1 ->  82.59 ; top5 ->  98.97  and loss:  54.44599145650864
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -135.9718517512083 , diff:  135.9718517512083
adv train loss:  -840.2827773094177 , diff:  704.3109255582094
adv train loss:  -934.8838710784912 , diff:  94.60109376907349
adv train loss:  -944.6458473205566 , diff:  9.76197624206543
adv train loss:  -958.5908517837524 , diff:  13.9450044631958
adv train loss:  -969.8314085006714 , diff:  11.240556716918945
adv train loss:  -971.2433338165283 , diff:  1.4119253158569336
adv train loss:  -977.5181331634521 , diff:  6.274799346923828
adv train loss:  -993.5244274139404 , diff:  16.00629425048828
adv train loss:  -992.8837833404541 , diff:  0.6406440734863281
layer  6  adv train finish, try to retain  9
test acc: top1 ->  12.46 ; top5 ->  53.96  and loss:  563.9946112632751
forward train acc: top1 ->  81.17600000244141 ; top5 ->  99.03799997802734  and loss:  54.39002838730812
test acc: top1 ->  80.38 ; top5 ->  98.53  and loss:  64.64324599504471
forward train acc: top1 ->  86.58000000488282 ; top5 ->  99.53199997558593  and loss:  38.79741150140762
test acc: top1 ->  82.35 ; top5 ->  98.74  and loss:  59.09170514345169
forward train acc: top1 ->  88.19399997070313 ; top5 ->  99.62399997558593  and loss:  33.67704880237579
test acc: top1 ->  83.14 ; top5 ->  98.77  and loss:  57.5314237177372
forward train acc: top1 ->  89.83199998291016 ; top5 ->  99.686  and loss:  29.70459946990013
test acc: top1 ->  84.3 ; top5 ->  98.83  and loss:  53.914536744356155
forward train acc: top1 ->  90.66399997802735 ; top5 ->  99.72399997558594  and loss:  26.954611614346504
test acc: top1 ->  84.31 ; top5 ->  98.97  and loss:  55.281995356082916
forward train acc: top1 ->  91.27000001220703 ; top5 ->  99.776  and loss:  25.085743322968483
test acc: top1 ->  85.33 ; top5 ->  98.94  and loss:  50.78356698155403
forward train acc: top1 ->  91.47799998291016 ; top5 ->  99.774  and loss:  24.380863085389137
test acc: top1 ->  85.59 ; top5 ->  98.93  and loss:  50.89484503865242
forward train acc: top1 ->  92.0100000024414 ; top5 ->  99.802  and loss:  23.07948412001133
test acc: top1 ->  85.87 ; top5 ->  99.01  and loss:  50.41214922070503
forward train acc: top1 ->  92.29999998046875 ; top5 ->  99.818  and loss:  21.970998093485832
test acc: top1 ->  86.29 ; top5 ->  99.05  and loss:  49.20557424426079
forward train acc: top1 ->  92.48800000976563 ; top5 ->  99.83799997558594  and loss:  21.42976452410221
test acc: top1 ->  86.45 ; top5 ->  99.05  and loss:  49.50487044453621
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  105 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -7.584679659456015 , diff:  7.584679659456015
adv train loss:  -7.8719796650111675 , diff:  0.2873000055551529
adv train loss:  -7.598463878035545 , diff:  0.2735157869756222
adv train loss:  -7.65934718772769 , diff:  0.060883309692144394
adv train loss:  -7.64186155423522 , diff:  0.017485633492469788
adv train loss:  -7.610009104013443 , diff:  0.03185245022177696
adv train loss:  -7.713830374181271 , diff:  0.1038212701678276
adv train loss:  -7.808814935386181 , diff:  0.09498456120491028
adv train loss:  -7.867476537823677 , diff:  0.058661602437496185
adv train loss:  -7.735273122787476 , diff:  0.13220341503620148
layer  7  adv train finish, try to retain  400
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -19.359933450818062 , diff:  19.359933450818062
adv train loss:  -19.33872289955616 , diff:  0.021210551261901855
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  42
test acc: top1 ->  10.29 ; top5 ->  65.6  and loss:  532.7480773925781
forward train acc: top1 ->  98.67200000732421 ; top5 ->  99.994  and loss:  4.449614606797695
test acc: top1 ->  91.62 ; top5 ->  99.5  and loss:  41.759417071938515
forward train acc: top1 ->  99.18799997558594 ; top5 ->  99.996  and loss:  2.259723705239594
test acc: top1 ->  91.63 ; top5 ->  99.46  and loss:  44.15398354828358
forward train acc: top1 ->  99.53000000488281 ; top5 ->  99.99399997558594  and loss:  1.441470555961132
test acc: top1 ->  91.59 ; top5 ->  99.45  and loss:  46.20395575463772
forward train acc: top1 ->  99.61799997558593 ; top5 ->  99.998  and loss:  1.1475777067244053
test acc: top1 ->  91.98 ; top5 ->  99.41  and loss:  49.100484415888786
forward train acc: top1 ->  99.66599997558593 ; top5 ->  99.998  and loss:  0.9809596221894026
test acc: top1 ->  91.9 ; top5 ->  99.45  and loss:  49.4030337780714
forward train acc: top1 ->  99.76199997558594 ; top5 ->  99.998  and loss:  0.7438498195260763
test acc: top1 ->  91.93 ; top5 ->  99.45  and loss:  51.76010923087597
forward train acc: top1 ->  99.71599997558594 ; top5 ->  99.998  and loss:  0.7982919542118907
test acc: top1 ->  92.16 ; top5 ->  99.46  and loss:  50.88709455728531
==> this epoch:  42 / 512
---------------- start layer  9  ---------------
adv train loss:  -0.9947608411312103 , diff:  0.9947608411312103
adv train loss:  -0.9078416214324534 , diff:  0.08691921969875693
adv train loss:  -0.9434323385357857 , diff:  0.03559071710333228
adv train loss:  -0.9217727174982429 , diff:  0.02165962103754282
adv train loss:  -0.9052893347106874 , diff:  0.016483382787555456
adv train loss:  -0.9413130595348775 , diff:  0.03602372482419014
adv train loss:  -0.9201312940567732 , diff:  0.021181765478104353
adv train loss:  -0.8873414732515812 , diff:  0.032789820805191994
adv train loss:  -0.9992872476577759 , diff:  0.11194577440619469
adv train loss:  -0.9320226553827524 , diff:  0.06726459227502346
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  95
test acc: top1 ->  10.0 ; top5 ->  50.53  and loss:  2072.2827339172363
forward train acc: top1 ->  96.0100000024414 ; top5 ->  99.89  and loss:  13.7474366389215
test acc: top1 ->  88.46 ; top5 ->  99.13  and loss:  52.8202600479126
forward train acc: top1 ->  99.40800000488281 ; top5 ->  99.996  and loss:  2.081846959888935
test acc: top1 ->  91.59 ; top5 ->  99.19  and loss:  46.113458693027496
forward train acc: top1 ->  99.59000000488281 ; top5 ->  99.996  and loss:  1.4734030291438103
test acc: top1 ->  91.71 ; top5 ->  99.1  and loss:  50.05044013261795
forward train acc: top1 ->  99.62799997558594 ; top5 ->  99.998  and loss:  1.1511258273385465
test acc: top1 ->  91.78 ; top5 ->  99.24  and loss:  49.16871187835932
forward train acc: top1 ->  99.73200000244141 ; top5 ->  100.0  and loss:  0.8530695410445333
test acc: top1 ->  91.7 ; top5 ->  99.15  and loss:  53.36229505389929
forward train acc: top1 ->  99.75999997558594 ; top5 ->  100.0  and loss:  0.7711751498281956
test acc: top1 ->  91.79 ; top5 ->  99.26  and loss:  52.59308625012636
forward train acc: top1 ->  99.78599997558594 ; top5 ->  99.998  and loss:  0.6673004734329879
test acc: top1 ->  92.04 ; top5 ->  99.19  and loss:  52.429150119423866
forward train acc: top1 ->  99.78399997558594 ; top5 ->  100.0  and loss:  0.6291131265461445
test acc: top1 ->  91.91 ; top5 ->  99.23  and loss:  54.73683179169893
forward train acc: top1 ->  99.81 ; top5 ->  100.0  and loss:  0.5962956426665187
test acc: top1 ->  91.99 ; top5 ->  99.17  and loss:  54.636117070913315
forward train acc: top1 ->  99.8220000024414 ; top5 ->  100.0  and loss:  0.5273002181202173
test acc: top1 ->  91.88 ; top5 ->  99.04  and loss:  55.63048908859491
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  96 / 512 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -272.01567363739014 , diff:  272.01567363739014
adv train loss:  -272.0403485298157 , diff:  0.02467489242553711
layer  10  adv train finish, try to retain  465
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -1959.0440368652344 , diff:  1959.0440368652344
adv train loss:  -2004.1744956970215 , diff:  45.13045883178711
adv train loss:  -2043.1647834777832 , diff:  38.99028778076172
adv train loss:  -2041.8085327148438 , diff:  1.3562507629394531
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  3
test acc: top1 ->  10.0 ; top5 ->  50.36  and loss:  41942.18771362305
forward train acc: top1 ->  98.36199997558593 ; top5 ->  99.988  and loss:  6.096836271230131
test acc: top1 ->  91.39 ; top5 ->  99.01  and loss:  49.15141902863979
forward train acc: top1 ->  99.802 ; top5 ->  100.0  and loss:  0.7540938528254628
test acc: top1 ->  91.86 ; top5 ->  99.12  and loss:  50.29154472053051
forward train acc: top1 ->  99.816 ; top5 ->  100.0  and loss:  0.5925301010720432
test acc: top1 ->  91.77 ; top5 ->  99.12  and loss:  49.696759805083275
forward train acc: top1 ->  99.83 ; top5 ->  100.0  and loss:  0.5605245954357088
test acc: top1 ->  91.91 ; top5 ->  99.14  and loss:  51.74922530353069
forward train acc: top1 ->  99.838 ; top5 ->  100.0  and loss:  0.47906983591383323
test acc: top1 ->  91.99 ; top5 ->  99.19  and loss:  51.226343140006065
forward train acc: top1 ->  99.87 ; top5 ->  100.0  and loss:  0.4115969251142815
test acc: top1 ->  91.98 ; top5 ->  99.14  and loss:  52.82835112512112
forward train acc: top1 ->  99.89599997558594 ; top5 ->  100.0  and loss:  0.3578658897895366
test acc: top1 ->  92.05 ; top5 ->  99.15  and loss:  52.21753406524658
forward train acc: top1 ->  99.8980000024414 ; top5 ->  100.0  and loss:  0.35057831835001707
test acc: top1 ->  92.05 ; top5 ->  99.23  and loss:  52.93766063451767
forward train acc: top1 ->  99.89599997558594 ; top5 ->  100.0  and loss:  0.3222656510770321
test acc: top1 ->  91.99 ; top5 ->  99.28  and loss:  54.41593986749649
forward train acc: top1 ->  99.934 ; top5 ->  99.998  and loss:  0.2916282319929451
test acc: top1 ->  91.97 ; top5 ->  99.26  and loss:  54.18025632202625
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  5 / 512 , inc:  2
---------------- start layer  12  ---------------
adv train loss:  -192.47907090187073 , diff:  192.47907090187073
adv train loss:  -190.61894977092743 , diff:  1.8601211309432983
adv train loss:  -191.65408527851105 , diff:  1.0351355075836182
adv train loss:  -191.78453242778778 , diff:  0.1304471492767334
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  40
test acc: top1 ->  10.0 ; top5 ->  53.26  and loss:  5722.636291503906
forward train acc: top1 ->  89.17799997802734 ; top5 ->  99.396  and loss:  46.02059141173959
test acc: top1 ->  90.46 ; top5 ->  98.35  and loss:  48.69282615184784
forward train acc: top1 ->  99.44799997558594 ; top5 ->  99.996  and loss:  3.608299082145095
test acc: top1 ->  90.69 ; top5 ->  98.51  and loss:  49.10701856017113
forward train acc: top1 ->  99.58999997558594 ; top5 ->  100.0  and loss:  2.5019931476563215
test acc: top1 ->  90.92 ; top5 ->  98.46  and loss:  51.38907143473625
forward train acc: top1 ->  99.6320000024414 ; top5 ->  99.996  and loss:  2.032574752345681
test acc: top1 ->  90.84 ; top5 ->  98.38  and loss:  51.97458624839783
forward train acc: top1 ->  99.71999997558594 ; top5 ->  99.998  and loss:  1.54109029751271
test acc: top1 ->  91.07 ; top5 ->  98.52  and loss:  53.41769815981388
forward train acc: top1 ->  99.698 ; top5 ->  100.0  and loss:  1.4307493888773024
test acc: top1 ->  91.02 ; top5 ->  98.47  and loss:  53.483166202902794
forward train acc: top1 ->  99.736 ; top5 ->  100.0  and loss:  1.3207247420214117
test acc: top1 ->  90.99 ; top5 ->  98.44  and loss:  55.206642508506775
forward train acc: top1 ->  99.73800000244141 ; top5 ->  100.0  and loss:  1.2778101228177547
test acc: top1 ->  91.08 ; top5 ->  98.47  and loss:  54.92296849191189
forward train acc: top1 ->  99.77399997558594 ; top5 ->  100.0  and loss:  1.0957466806285083
test acc: top1 ->  91.1 ; top5 ->  98.48  and loss:  55.21391965448856
forward train acc: top1 ->  99.78000000244141 ; top5 ->  100.0  and loss:  1.1141600725241005
test acc: top1 ->  91.12 ; top5 ->  98.45  and loss:  55.759016916155815
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  41 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -18578.135681152344 , diff:  18578.135681152344
adv train loss:  -30890.036163330078 , diff:  12311.900482177734
adv train loss:  -42655.5791015625 , diff:  11765.542938232422
adv train loss:  -54240.20962524414 , diff:  11584.63052368164
adv train loss:  -65783.19067382812 , diff:  11542.981048583984
adv train loss:  -77273.96368408203 , diff:  11490.773010253906
adv train loss:  -88736.18475341797 , diff:  11462.221069335938
adv train loss:  -100233.86224365234 , diff:  11497.677490234375
adv train loss:  -111729.05798339844 , diff:  11495.195739746094
adv train loss:  -123271.91955566406 , diff:  11542.861572265625
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  22
test acc: top1 ->  48.81 ; top5 ->  98.06  and loss:  508.4975893497467
forward train acc: top1 ->  96.44199997558594 ; top5 ->  99.996  and loss:  17.89447496458888
test acc: top1 ->  91.49 ; top5 ->  98.88  and loss:  68.44374388456345
forward train acc: top1 ->  99.84000000488281 ; top5 ->  99.998  and loss:  0.5720044951885939
test acc: top1 ->  91.66 ; top5 ->  98.94  and loss:  68.52536737918854
forward train acc: top1 ->  99.876 ; top5 ->  100.0  and loss:  0.4475232610711828
test acc: top1 ->  91.81 ; top5 ->  99.01  and loss:  68.30896370112896
forward train acc: top1 ->  99.906 ; top5 ->  100.0  and loss:  0.31697082333266735
test acc: top1 ->  91.73 ; top5 ->  99.03  and loss:  69.42863629758358
forward train acc: top1 ->  99.9 ; top5 ->  100.0  and loss:  0.36123373662121594
test acc: top1 ->  91.93 ; top5 ->  99.0  and loss:  68.52842469513416
forward train acc: top1 ->  99.914 ; top5 ->  100.0  and loss:  0.28142259654123336
test acc: top1 ->  91.87 ; top5 ->  99.03  and loss:  69.24001194536686
forward train acc: top1 ->  99.896 ; top5 ->  100.0  and loss:  0.3228852312313393
test acc: top1 ->  91.88 ; top5 ->  99.06  and loss:  69.93629330396652
forward train acc: top1 ->  99.904 ; top5 ->  100.0  and loss:  0.2948782825260423
test acc: top1 ->  91.95 ; top5 ->  99.01  and loss:  69.81616726517677
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.1978774459566921
test acc: top1 ->  91.93 ; top5 ->  99.01  and loss:  69.61180581152439
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.27374254376627505
test acc: top1 ->  92.03 ; top5 ->  99.0  and loss:  68.61026863753796
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  24 / 512 , inc:  2
layer  0  :  0.484375  ==>  31 / 64 , inc:  1
layer  1  :  0.875  ==>  56 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.41015625  ==>  105 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.08203125  ==>  42 / 512 , inc:  21
layer  9  :  0.1875  ==>  96 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.009765625  ==>  5 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.046875  ==>  24 / 512 , inc:  1
eps [2.07594140625, 0.013684184074401855, 1.5569560546875, 1.5569560546875, 1.5569560546875, 1.5569560546875, 1.5569560546875, 0.389239013671875, 0.0030409297943115236, 0.013684184074401855, 0.389239013671875, 1.037970703125, 0.013684184074401855, 16.60753125]  wait [2, 4, 4, 4, 4, 4, 4, 2, 0, 4, 2, 2, 4, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 21, 1, 1, 1, 1, 1]  tol: 4
$$$$$$$$$$$$$ epoch  42  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1833.4530096054077 , diff:  1833.4530096054077
adv train loss:  -1698.684679031372 , diff:  134.76833057403564
adv train loss:  -1709.1726083755493 , diff:  10.487929344177246
adv train loss:  -1715.1867151260376 , diff:  6.014106750488281
adv train loss:  -1690.9325532913208 , diff:  24.254161834716797
adv train loss:  -1616.4406108856201 , diff:  74.49194240570068
adv train loss:  -1617.2045307159424 , diff:  0.7639198303222656
adv train loss:  -1615.3738231658936 , diff:  1.8307075500488281
adv train loss:  -1618.3957138061523 , diff:  3.021890640258789
adv train loss:  -1619.243140220642 , diff:  0.8474264144897461
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  30
test acc: top1 ->  11.67 ; top5 ->  50.0  and loss:  1266862.677734375
forward train acc: top1 ->  97.98599998291016 ; top5 ->  99.97  and loss:  8.750144412741065
test acc: top1 ->  89.97 ; top5 ->  98.92  and loss:  67.05092689394951
forward train acc: top1 ->  98.6980000024414 ; top5 ->  99.988  and loss:  4.420261541381478
test acc: top1 ->  90.66 ; top5 ->  99.18  and loss:  52.97547274827957
forward train acc: top1 ->  98.87999998046875 ; top5 ->  99.992  and loss:  3.513094138354063
test acc: top1 ->  91.07 ; top5 ->  99.14  and loss:  47.87378400564194
forward train acc: top1 ->  99.03999997802734 ; top5 ->  99.988  and loss:  2.963845042511821
test acc: top1 ->  91.05 ; top5 ->  99.15  and loss:  48.10969850420952
forward train acc: top1 ->  99.22199997802734 ; top5 ->  99.996  and loss:  2.4574149679392576
test acc: top1 ->  91.23 ; top5 ->  99.15  and loss:  48.912324756383896
forward train acc: top1 ->  99.32800000488281 ; top5 ->  99.992  and loss:  1.9906119517982006
test acc: top1 ->  91.28 ; top5 ->  99.11  and loss:  49.339886114001274
forward train acc: top1 ->  99.36799997558593 ; top5 ->  99.994  and loss:  1.8420601319521666
test acc: top1 ->  91.48 ; top5 ->  99.21  and loss:  49.1118870228529
forward train acc: top1 ->  99.40200000488281 ; top5 ->  99.992  and loss:  1.8123166002333164
test acc: top1 ->  91.58 ; top5 ->  99.13  and loss:  49.764813378453255
forward train acc: top1 ->  99.47000000244141 ; top5 ->  99.996  and loss:  1.6561248451471329
test acc: top1 ->  91.54 ; top5 ->  99.21  and loss:  49.54509358108044
forward train acc: top1 ->  99.4980000024414 ; top5 ->  99.998  and loss:  1.6194616816937923
test acc: top1 ->  91.43 ; top5 ->  99.08  and loss:  50.317999839782715
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  31 / 64 , inc:  1
---------------- start layer  1  ---------------
### skip layer  1 wait:  4  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
adv train loss:  -0.7457272983156145 , diff:  0.7457272983156145
adv train loss:  -0.8491836139000952 , diff:  0.10345631558448076
adv train loss:  -0.8161119180731475 , diff:  0.03307169582694769
adv train loss:  -0.8025868469849229 , diff:  0.01352507108822465
adv train loss:  -0.8615592636633664 , diff:  0.05897241667844355
adv train loss:  -0.7517390754073858 , diff:  0.10982018825598061
adv train loss:  -0.7943515963852406 , diff:  0.04261252097785473
adv train loss:  -0.8049820214509964 , diff:  0.010630425065755844
adv train loss:  -0.7815349010052159 , diff:  0.023447120445780456
adv train loss:  -0.8016376267187297 , diff:  0.02010272571351379
layer  7  adv train finish, try to retain  406
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -31.963900163769722 , diff:  31.963900163769722
adv train loss:  -32.309393748641014 , diff:  0.3454935848712921
adv train loss:  -31.8723316937685 , diff:  0.4370620548725128
adv train loss:  -31.839060470461845 , diff:  0.033271223306655884
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  21
test acc: top1 ->  20.1 ; top5 ->  50.76  and loss:  994.18461561203
forward train acc: top1 ->  99.40799997558594 ; top5 ->  100.0  and loss:  2.140324999578297
test acc: top1 ->  92.16 ; top5 ->  99.26  and loss:  48.793052069842815
==> this epoch:  21 / 512
---------------- start layer  9  ---------------
### skip layer  9 wait:  4  ###
---------------- start layer  10  ---------------
adv train loss:  -61.42541620135307 , diff:  61.42541620135307
adv train loss:  -61.87576377391815 , diff:  0.45034757256507874
adv train loss:  -61.7909739613533 , diff:  0.08478981256484985
layer  10  adv train finish, try to retain  460
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -2484.2666149139404 , diff:  2484.2666149139404
adv train loss:  -2653.3608322143555 , diff:  169.09421730041504
adv train loss:  -2658.3954486846924 , diff:  5.034616470336914
adv train loss:  -2655.6811866760254 , diff:  2.714262008666992
layer  11  adv train finish, try to retain  511
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
adv train loss:  -18766.05722808838 , diff:  18766.05722808838
adv train loss:  -32059.494094848633 , diff:  13293.436866760254
adv train loss:  -44524.82305908203 , diff:  12465.328964233398
adv train loss:  -56714.47283935547 , diff:  12189.649780273438
adv train loss:  -68774.39672851562 , diff:  12059.923889160156
adv train loss:  -80772.98468017578 , diff:  11998.587951660156
adv train loss:  -92730.419921875 , diff:  11957.435241699219
adv train loss:  -104678.00677490234 , diff:  11947.586853027344
adv train loss:  -116609.95422363281 , diff:  11931.947448730469
adv train loss:  -128482.33117675781 , diff:  11872.376953125
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  23
test acc: top1 ->  74.57 ; top5 ->  98.65  and loss:  103.94617342948914
forward train acc: top1 ->  99.41599997558593 ; top5 ->  100.0  and loss:  1.568204717244953
test acc: top1 ->  91.92 ; top5 ->  99.09  and loss:  52.4791818857193
forward train acc: top1 ->  99.924 ; top5 ->  100.0  and loss:  0.29686258384026587
test acc: top1 ->  92.04 ; top5 ->  99.11  and loss:  55.58662110567093
forward train acc: top1 ->  99.894 ; top5 ->  100.0  and loss:  0.3128338293900015
test acc: top1 ->  92.0 ; top5 ->  99.2  and loss:  55.7220296561718
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.24029287323355675
test acc: top1 ->  92.03 ; top5 ->  99.15  and loss:  58.51172507554293
forward train acc: top1 ->  99.94 ; top5 ->  100.0  and loss:  0.22974257497116923
test acc: top1 ->  92.12 ; top5 ->  99.2  and loss:  58.62781789153814
==> this epoch:  23 / 512
layer  0  :  0.484375  ==>  31 / 64 , inc:  1
layer  1  :  0.875  ==>  56 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.41015625  ==>  105 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.041015625  ==>  21 / 512 , inc:  10
layer  9  :  0.1875  ==>  96 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.009765625  ==>  5 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.044921875  ==>  23 / 512 , inc:  2
eps [1.5569560546875, 0.013684184074401855, 1.5569560546875, 1.5569560546875, 1.5569560546875, 1.5569560546875, 1.5569560546875, 0.77847802734375, 0.0030409297943115236, 0.013684184074401855, 0.77847802734375, 2.07594140625, 0.013684184074401855, 16.60753125]  wait [4, 3, 3, 3, 3, 3, 3, 2, 0, 3, 2, 2, 3, 0]  inc [1, 1, 1, 1, 1, 1, 1, 1, 10, 1, 1, 1, 1, 2]  tol: 4
$$$$$$$$$$$$$ epoch  43  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
### skip layer  1 wait:  3  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  3  ###
---------------- start layer  7  ---------------
adv train loss:  -0.3658707309514284 , diff:  0.3658707309514284
adv train loss:  -0.406338797416538 , diff:  0.04046806646510959
adv train loss:  -0.3265674230642617 , diff:  0.07977137435227633
adv train loss:  -0.4121951326960698 , diff:  0.0856277096318081
adv train loss:  -0.4152867191005498 , diff:  0.00309158640448004
layer  7  adv train finish, try to retain  408
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -11.254073970019817 , diff:  11.254073970019817
adv train loss:  -10.749103046953678 , diff:  0.5049709230661392
adv train loss:  -10.607307888567448 , diff:  0.14179515838623047
adv train loss:  -10.840002287179232 , diff:  0.23269439861178398
adv train loss:  -11.275616969913244 , diff:  0.4356146827340126
adv train loss:  -10.687925688922405 , diff:  0.587691280990839
adv train loss:  -10.763900138437748 , diff:  0.07597444951534271
adv train loss:  -10.953716199845076 , diff:  0.18981606140732765
adv train loss:  -11.009232971817255 , diff:  0.05551677197217941
adv train loss:  -10.642506416887045 , diff:  0.3667265549302101
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  11
test acc: top1 ->  13.22 ; top5 ->  50.0  and loss:  1344.3650207519531
forward train acc: top1 ->  91.03599997802735 ; top5 ->  99.974  and loss:  28.172794427722692
test acc: top1 ->  89.97 ; top5 ->  98.59  and loss:  60.497826769948006
forward train acc: top1 ->  99.11600000244141 ; top5 ->  100.0  and loss:  3.31923651881516
test acc: top1 ->  90.91 ; top5 ->  98.68  and loss:  62.360039204359055
forward train acc: top1 ->  99.47000000244141 ; top5 ->  100.0  and loss:  1.8736362159252167
test acc: top1 ->  91.1 ; top5 ->  98.62  and loss:  62.4416606426239
forward train acc: top1 ->  99.5820000024414 ; top5 ->  99.998  and loss:  1.4270913377404213
test acc: top1 ->  91.11 ; top5 ->  98.62  and loss:  63.93663631379604
forward train acc: top1 ->  99.722 ; top5 ->  100.0  and loss:  1.0240061040967703
test acc: top1 ->  91.17 ; top5 ->  98.75  and loss:  64.59006215631962
forward train acc: top1 ->  99.7280000024414 ; top5 ->  100.0  and loss:  0.951646450906992
test acc: top1 ->  91.29 ; top5 ->  98.75  and loss:  65.23377029597759
forward train acc: top1 ->  99.74 ; top5 ->  99.998  and loss:  0.8625783150782809
test acc: top1 ->  91.44 ; top5 ->  98.69  and loss:  67.15165887773037
forward train acc: top1 ->  99.73200000244141 ; top5 ->  100.0  and loss:  0.8404102474451065
test acc: top1 ->  91.17 ; top5 ->  98.74  and loss:  67.10464182496071
forward train acc: top1 ->  99.766 ; top5 ->  100.0  and loss:  0.7633021201472729
test acc: top1 ->  91.35 ; top5 ->  98.76  and loss:  67.54082095623016
forward train acc: top1 ->  99.78 ; top5 ->  100.0  and loss:  0.7352577492129058
test acc: top1 ->  91.38 ; top5 ->  98.83  and loss:  67.84657782316208
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  21 / 512 , inc:  10
---------------- start layer  9  ---------------
### skip layer  9 wait:  3  ###
---------------- start layer  10  ---------------
adv train loss:  -317.8620264530182 , diff:  317.8620264530182
adv train loss:  -316.5044581890106 , diff:  1.3575682640075684
adv train loss:  -316.97809076309204 , diff:  0.4736325740814209
layer  10  adv train finish, try to retain  480
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -1525.7785110473633 , diff:  1525.7785110473633
adv train loss:  -1526.8064403533936 , diff:  1.0279293060302734
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  4
test acc: top1 ->  28.66 ; top5 ->  53.61  and loss:  17838.26301574707
forward train acc: top1 ->  98.52 ; top5 ->  99.996  and loss:  5.6101972663309425
test acc: top1 ->  91.22 ; top5 ->  99.03  and loss:  53.46722328662872
forward train acc: top1 ->  99.872 ; top5 ->  99.998  and loss:  0.5665886355563998
test acc: top1 ->  91.84 ; top5 ->  98.98  and loss:  52.0571129322052
forward train acc: top1 ->  99.878 ; top5 ->  100.0  and loss:  0.43754906160756946
test acc: top1 ->  91.87 ; top5 ->  99.01  and loss:  52.98036654293537
forward train acc: top1 ->  99.90399997558593 ; top5 ->  99.99799997558594  and loss:  0.3309211554005742
test acc: top1 ->  91.92 ; top5 ->  98.96  and loss:  54.468234583735466
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.30064115673303604
test acc: top1 ->  91.97 ; top5 ->  99.06  and loss:  55.04837390780449
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.2547247288748622
test acc: top1 ->  91.9 ; top5 ->  99.08  and loss:  56.023842334747314
forward train acc: top1 ->  99.92600000244141 ; top5 ->  100.0  and loss:  0.2543927263468504
test acc: top1 ->  91.86 ; top5 ->  99.03  and loss:  56.873158141970634
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  5 / 512 , inc:  1
---------------- start layer  12  ---------------
### skip layer  12 wait:  3  ###
---------------- start layer  13  ---------------
adv train loss:  -18896.433532714844 , diff:  18896.433532714844
adv train loss:  -31247.415130615234 , diff:  12350.98159790039
adv train loss:  -43148.79739379883 , diff:  11901.382263183594
adv train loss:  -54905.03579711914 , diff:  11756.238403320312
adv train loss:  -66583.99005126953 , diff:  11678.95425415039
adv train loss:  -78176.92199707031 , diff:  11592.931945800781
adv train loss:  -89783.3031616211 , diff:  11606.381164550781
adv train loss:  -101368.17218017578 , diff:  11584.869018554688
adv train loss:  -112944.17602539062 , diff:  11576.003845214844
adv train loss:  -124501.47033691406 , diff:  11557.294311523438
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  21
test acc: top1 ->  36.99 ; top5 ->  84.76  and loss:  748.0525069236755
forward train acc: top1 ->  90.07199997558594 ; top5 ->  98.912  and loss:  70.98093855939806
test acc: top1 ->  91.65 ; top5 ->  99.19  and loss:  40.847653329372406
forward train acc: top1 ->  99.858 ; top5 ->  100.0  and loss:  1.0427696239203215
test acc: top1 ->  91.59 ; top5 ->  99.22  and loss:  40.98702026903629
forward train acc: top1 ->  99.90199997558594 ; top5 ->  100.0  and loss:  0.6952023087069392
test acc: top1 ->  91.67 ; top5 ->  99.21  and loss:  42.63919224590063
forward train acc: top1 ->  99.936 ; top5 ->  100.0  and loss:  0.4670342872850597
test acc: top1 ->  91.99 ; top5 ->  99.22  and loss:  43.510424353182316
forward train acc: top1 ->  99.93799997558594 ; top5 ->  100.0  and loss:  0.4012125157751143
test acc: top1 ->  91.88 ; top5 ->  99.3  and loss:  44.41558563709259
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.3548504945356399
test acc: top1 ->  91.88 ; top5 ->  99.31  and loss:  45.03101132810116
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.3001341922208667
test acc: top1 ->  91.9 ; top5 ->  99.19  and loss:  45.826080314815044
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.2740580481477082
test acc: top1 ->  91.86 ; top5 ->  99.28  and loss:  46.13974246382713
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.2537278092931956
test acc: top1 ->  92.09 ; top5 ->  99.26  and loss:  46.44151100516319
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.2496434932691045
test acc: top1 ->  92.02 ; top5 ->  99.25  and loss:  46.81763684749603
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  23 / 512 , inc:  2
layer  0  :  0.484375  ==>  31 / 64 , inc:  1
layer  1  :  0.875  ==>  56 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.41015625  ==>  105 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.041015625  ==>  21 / 512 , inc:  5
layer  9  :  0.1875  ==>  96 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.009765625  ==>  5 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.044921875  ==>  23 / 512 , inc:  1
eps [1.5569560546875, 0.013684184074401855, 1.5569560546875, 1.5569560546875, 1.5569560546875, 1.5569560546875, 1.5569560546875, 1.5569560546875, 0.0022806973457336426, 0.013684184074401855, 1.5569560546875, 1.5569560546875, 0.013684184074401855, 12.4556484375]  wait [3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1]  tol: 4
$$$$$$$$$$$$$ epoch  44  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1184.5538492202759 , diff:  1184.5538492202759
adv train loss:  -1223.4093494415283 , diff:  38.85550022125244
adv train loss:  -1198.5702409744263 , diff:  24.83910846710205
adv train loss:  -1218.9973649978638 , diff:  20.4271240234375
adv train loss:  -1214.2254629135132 , diff:  4.771902084350586
adv train loss:  -1205.4433851242065 , diff:  8.78207778930664
adv train loss:  -1217.0894765853882 , diff:  11.64609146118164
adv train loss:  -1207.211820602417 , diff:  9.877655982971191
adv train loss:  -1212.719970703125 , diff:  5.508150100708008
adv train loss:  -1210.5604314804077 , diff:  2.159539222717285
layer  0  adv train finish, try to retain  5
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  5802.740196228027
forward train acc: top1 ->  75.85199998046875 ; top5 ->  95.43799998291016  and loss:  148.21916443109512
test acc: top1 ->  48.27 ; top5 ->  84.24  and loss:  189.9295892715454
forward train acc: top1 ->  79.84199998291015 ; top5 ->  97.37600001220703  and loss:  64.9857865869999
test acc: top1 ->  78.48 ; top5 ->  97.26  and loss:  71.54195049405098
forward train acc: top1 ->  82.26999998779297 ; top5 ->  97.98000000976562  and loss:  55.79979169368744
test acc: top1 ->  80.1 ; top5 ->  97.66  and loss:  65.12860682606697
forward train acc: top1 ->  83.96399998779297 ; top5 ->  98.43799998291016  and loss:  49.833714812994
test acc: top1 ->  81.21 ; top5 ->  98.02  and loss:  62.51203626394272
forward train acc: top1 ->  85.26000000488281 ; top5 ->  98.75199998291015  and loss:  45.07742032408714
test acc: top1 ->  82.41 ; top5 ->  98.16  and loss:  58.59631896018982
forward train acc: top1 ->  86.44400001220703 ; top5 ->  98.90399998046875  and loss:  41.61397072672844
test acc: top1 ->  82.73 ; top5 ->  98.33  and loss:  57.53010576963425
forward train acc: top1 ->  86.87000001220703 ; top5 ->  99.0160000024414  and loss:  40.38853162527084
test acc: top1 ->  83.02 ; top5 ->  98.28  and loss:  56.784551709890366
forward train acc: top1 ->  87.43799999023437 ; top5 ->  99.05799997802734  and loss:  38.091796576976776
test acc: top1 ->  83.52 ; top5 ->  98.37  and loss:  55.97598069906235
forward train acc: top1 ->  88.00199999511719 ; top5 ->  99.1240000024414  and loss:  36.9755722284317
test acc: top1 ->  83.93 ; top5 ->  98.4  and loss:  55.052444100379944
forward train acc: top1 ->  88.20800001220704 ; top5 ->  99.166  and loss:  35.8005633354187
test acc: top1 ->  84.03 ; top5 ->  98.47  and loss:  54.56412371993065
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  31 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -5.445185516029596 , diff:  5.445185516029596
adv train loss:  -5.582065299153328 , diff:  0.1368797831237316
adv train loss:  -5.510211318731308 , diff:  0.07185398042201996
adv train loss:  -5.459370944648981 , diff:  0.05084037408232689
adv train loss:  -5.55032941699028 , diff:  0.09095847234129906
adv train loss:  -5.5375053361058235 , diff:  0.012824080884456635
adv train loss:  -5.48471399769187 , diff:  0.05279133841395378
adv train loss:  -5.5097616240382195 , diff:  0.025047626346349716
adv train loss:  -5.560603301972151 , diff:  0.05084167793393135
adv train loss:  -5.542712651193142 , diff:  0.017890650779008865
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -361.62180402874947 , diff:  361.62180402874947
adv train loss:  -561.2292737960815 , diff:  199.60746976733208
adv train loss:  -558.5883493423462 , diff:  2.6409244537353516
adv train loss:  -561.4710540771484 , diff:  2.882704734802246
adv train loss:  -564.3612685203552 , diff:  2.890214443206787
adv train loss:  -557.6532788276672 , diff:  6.707989692687988
adv train loss:  -567.0016860961914 , diff:  9.34840726852417
adv train loss:  -564.3868274688721 , diff:  2.614858627319336
adv train loss:  -564.7007455825806 , diff:  0.3139181137084961
adv train loss:  -565.0619716644287 , diff:  0.36122608184814453
layer  2  adv train finish, try to retain  57
test acc: top1 ->  13.51 ; top5 ->  54.38  and loss:  464.05744647979736
forward train acc: top1 ->  95.01199998779298 ; top5 ->  99.844  and loss:  14.791043601930141
test acc: top1 ->  87.86 ; top5 ->  99.1  and loss:  45.99944940209389
forward train acc: top1 ->  96.27999998779296 ; top5 ->  99.916  and loss:  11.142325267195702
test acc: top1 ->  88.46 ; top5 ->  99.15  and loss:  44.77974288165569
forward train acc: top1 ->  96.96200001464844 ; top5 ->  99.958  and loss:  8.827564604580402
test acc: top1 ->  88.55 ; top5 ->  99.07  and loss:  46.531236723065376
forward train acc: top1 ->  97.32399998535156 ; top5 ->  99.95399997558594  and loss:  7.9408056400716305
test acc: top1 ->  89.13 ; top5 ->  99.25  and loss:  45.645017355680466
forward train acc: top1 ->  97.48799998535156 ; top5 ->  99.978  and loss:  7.257546007633209
test acc: top1 ->  89.17 ; top5 ->  99.27  and loss:  45.68099622428417
forward train acc: top1 ->  97.79200000732422 ; top5 ->  99.976  and loss:  6.46145848557353
test acc: top1 ->  89.25 ; top5 ->  99.28  and loss:  46.147364877164364
forward train acc: top1 ->  97.91800000976562 ; top5 ->  99.972  and loss:  6.1378269381821156
test acc: top1 ->  89.41 ; top5 ->  99.02  and loss:  45.916319131851196
forward train acc: top1 ->  97.84399998291016 ; top5 ->  99.982  and loss:  6.286435078829527
test acc: top1 ->  89.43 ; top5 ->  99.19  and loss:  45.2296581864357
forward train acc: top1 ->  98.05400000976563 ; top5 ->  99.978  and loss:  5.802342228591442
test acc: top1 ->  89.58 ; top5 ->  99.32  and loss:  44.901470854878426
forward train acc: top1 ->  98.17399997558594 ; top5 ->  99.982  and loss:  5.571536831557751
test acc: top1 ->  89.49 ; top5 ->  99.25  and loss:  46.07119332253933
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -621.8792703747749 , diff:  621.8792703747749
adv train loss:  -1055.339786529541 , diff:  433.4605161547661
adv train loss:  -1084.8077430725098 , diff:  29.46795654296875
adv train loss:  -1088.1533632278442 , diff:  3.3456201553344727
adv train loss:  -1088.5152769088745 , diff:  0.36191368103027344
adv train loss:  -1099.9986991882324 , diff:  11.48342227935791
adv train loss:  -1124.6717252731323 , diff:  24.673026084899902
adv train loss:  -1116.444577217102 , diff:  8.227148056030273
adv train loss:  -1123.0556831359863 , diff:  6.611105918884277
adv train loss:  -1123.6601657867432 , diff:  0.6044826507568359
layer  3  adv train finish, try to retain  42
test acc: top1 ->  14.73 ; top5 ->  62.15  and loss:  931.5893359184265
forward train acc: top1 ->  90.84000000976563 ; top5 ->  99.5460000024414  and loss:  28.4956034719944
test acc: top1 ->  85.55 ; top5 ->  98.7  and loss:  51.189916640520096
forward train acc: top1 ->  92.36999999023438 ; top5 ->  99.678  and loss:  22.796719193458557
test acc: top1 ->  86.35 ; top5 ->  98.9  and loss:  47.226885080337524
forward train acc: top1 ->  93.18800000244141 ; top5 ->  99.782  and loss:  19.681407868862152
test acc: top1 ->  86.61 ; top5 ->  98.9  and loss:  47.15431226789951
forward train acc: top1 ->  93.74799997558594 ; top5 ->  99.7960000024414  and loss:  18.73267973214388
test acc: top1 ->  86.97 ; top5 ->  98.99  and loss:  46.590375900268555
forward train acc: top1 ->  94.22599997314452 ; top5 ->  99.836  and loss:  16.882432356476784
test acc: top1 ->  87.16 ; top5 ->  99.0  and loss:  45.38869498670101
forward train acc: top1 ->  94.74 ; top5 ->  99.86599997558594  and loss:  15.546822026371956
test acc: top1 ->  87.33 ; top5 ->  99.08  and loss:  45.49982941150665
forward train acc: top1 ->  94.85199998291016 ; top5 ->  99.87799997558594  and loss:  15.27408403903246
test acc: top1 ->  87.34 ; top5 ->  99.01  and loss:  45.37879003584385
forward train acc: top1 ->  94.98199999511719 ; top5 ->  99.848  and loss:  14.781036205589771
test acc: top1 ->  87.58 ; top5 ->  99.05  and loss:  45.18311522901058
forward train acc: top1 ->  95.03399999511718 ; top5 ->  99.902  and loss:  14.505566455423832
test acc: top1 ->  87.6 ; top5 ->  99.08  and loss:  45.9372835457325
forward train acc: top1 ->  95.07199997558594 ; top5 ->  99.898  and loss:  14.188209146261215
test acc: top1 ->  87.8 ; top5 ->  99.06  and loss:  45.07433494925499
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -142.56435325741768 , diff:  142.56435325741768
adv train loss:  -864.6861009597778 , diff:  722.1217477023602
adv train loss:  -1042.864164352417 , diff:  178.17806339263916
adv train loss:  -1054.872787475586 , diff:  12.008623123168945
adv train loss:  -1072.394938468933 , diff:  17.522150993347168
adv train loss:  -1083.7530364990234 , diff:  11.358098030090332
adv train loss:  -1085.083291053772 , diff:  1.3302545547485352
adv train loss:  -1087.2108545303345 , diff:  2.1275634765625
adv train loss:  -1083.6789321899414 , diff:  3.5319223403930664
adv train loss:  -1085.6529150009155 , diff:  1.973982810974121
layer  4  adv train finish, try to retain  38
test acc: top1 ->  11.79 ; top5 ->  56.18  and loss:  1172.1767406463623
forward train acc: top1 ->  75.89399999511718 ; top5 ->  97.94000000488282  and loss:  70.2455426454544
test acc: top1 ->  75.12 ; top5 ->  97.51  and loss:  75.66718193888664
forward train acc: top1 ->  80.56599997558594 ; top5 ->  98.64399997802734  and loss:  55.297461062669754
test acc: top1 ->  77.98 ; top5 ->  98.11  and loss:  67.64170175790787
forward train acc: top1 ->  83.044 ; top5 ->  98.95200000732422  and loss:  48.362284719944
test acc: top1 ->  78.84 ; top5 ->  97.84  and loss:  66.28237384557724
forward train acc: top1 ->  84.54399998291015 ; top5 ->  99.08999997802735  and loss:  43.89514410495758
test acc: top1 ->  80.5 ; top5 ->  98.38  and loss:  60.667208045721054
forward train acc: top1 ->  85.72799998291016 ; top5 ->  99.28999997558594  and loss:  40.932831943035126
test acc: top1 ->  81.06 ; top5 ->  98.43  and loss:  59.06084477901459
forward train acc: top1 ->  86.61399999267579 ; top5 ->  99.31800000244141  and loss:  38.87935331463814
test acc: top1 ->  81.75 ; top5 ->  98.52  and loss:  56.83395093679428
forward train acc: top1 ->  86.79999999511719 ; top5 ->  99.3560000024414  and loss:  37.76880261301994
test acc: top1 ->  81.93 ; top5 ->  98.63  and loss:  56.31511327624321
forward train acc: top1 ->  86.86999997314453 ; top5 ->  99.34000000488281  and loss:  37.25222656130791
test acc: top1 ->  82.12 ; top5 ->  98.53  and loss:  56.04353708028793
forward train acc: top1 ->  87.27799997802734 ; top5 ->  99.41799998046875  and loss:  36.20373556017876
test acc: top1 ->  82.39 ; top5 ->  98.57  and loss:  55.605612486600876
forward train acc: top1 ->  87.48800001464843 ; top5 ->  99.42600000244141  and loss:  35.50523614883423
test acc: top1 ->  82.28 ; top5 ->  98.54  and loss:  55.52025246620178
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  156 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -176.2798290885985 , diff:  176.2798290885985
adv train loss:  -747.3486156463623 , diff:  571.0687865577638
adv train loss:  -933.0645380020142 , diff:  185.71592235565186
adv train loss:  -968.7624435424805 , diff:  35.69790554046631
adv train loss:  -973.7371530532837 , diff:  4.974709510803223
adv train loss:  -984.9904079437256 , diff:  11.253254890441895
adv train loss:  -992.0344934463501 , diff:  7.044085502624512
adv train loss:  -996.438515663147 , diff:  4.404022216796875
adv train loss:  -1003.0434322357178 , diff:  6.604916572570801
adv train loss:  -1001.9739112854004 , diff:  1.0695209503173828
layer  5  adv train finish, try to retain  49
test acc: top1 ->  23.5 ; top5 ->  62.52  and loss:  858.2545509338379
forward train acc: top1 ->  91.00200000976562 ; top5 ->  99.642  and loss:  26.809514358639717
test acc: top1 ->  85.59 ; top5 ->  99.01  and loss:  48.5988913923502
forward train acc: top1 ->  93.41999997802735 ; top5 ->  99.81  and loss:  19.138913452625275
test acc: top1 ->  87.14 ; top5 ->  99.1  and loss:  46.31216421723366
forward train acc: top1 ->  94.36799997558593 ; top5 ->  99.856  and loss:  16.393743827939034
test acc: top1 ->  87.53 ; top5 ->  99.14  and loss:  44.741161569952965
forward train acc: top1 ->  94.97199997070312 ; top5 ->  99.88999997558594  and loss:  14.438812345266342
test acc: top1 ->  87.78 ; top5 ->  99.18  and loss:  45.12010645866394
forward train acc: top1 ->  95.352 ; top5 ->  99.904  and loss:  13.534750416874886
test acc: top1 ->  88.38 ; top5 ->  99.16  and loss:  43.77647215127945
forward train acc: top1 ->  95.78200001220704 ; top5 ->  99.906  and loss:  12.395865887403488
test acc: top1 ->  88.44 ; top5 ->  99.16  and loss:  43.6348045617342
forward train acc: top1 ->  95.84000001953125 ; top5 ->  99.90199997558594  and loss:  12.112822823226452
test acc: top1 ->  88.48 ; top5 ->  99.2  and loss:  44.003207713365555
forward train acc: top1 ->  96.00599999023437 ; top5 ->  99.93  and loss:  11.642657965421677
test acc: top1 ->  88.66 ; top5 ->  99.22  and loss:  43.99595046043396
forward train acc: top1 ->  96.03399997314453 ; top5 ->  99.93199997558594  and loss:  11.364683769643307
test acc: top1 ->  88.6 ; top5 ->  99.19  and loss:  43.96765402704477
forward train acc: top1 ->  96.04400001953125 ; top5 ->  99.93  and loss:  11.423378832638264
test acc: top1 ->  88.67 ; top5 ->  99.15  and loss:  43.738874189555645
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -195.2860679514706 , diff:  195.2860679514706
adv train loss:  -1188.083969116211 , diff:  992.7979011647403
adv train loss:  -1404.7165985107422 , diff:  216.63262939453125
adv train loss:  -1451.533073425293 , diff:  46.81647491455078
adv train loss:  -1450.8977680206299 , diff:  0.6353054046630859
adv train loss:  -1454.392339706421 , diff:  3.4945716857910156
adv train loss:  -1462.5868759155273 , diff:  8.194536209106445
adv train loss:  -1483.4634256362915 , diff:  20.87654972076416
adv train loss:  -1521.0084943771362 , diff:  37.54506874084473
adv train loss:  -1522.5283298492432 , diff:  1.5198354721069336
layer  6  adv train finish, try to retain  17
test acc: top1 ->  26.66 ; top5 ->  74.11  and loss:  682.0459809303284
forward train acc: top1 ->  88.678 ; top5 ->  99.55599997558593  and loss:  34.13515156507492
test acc: top1 ->  84.36 ; top5 ->  98.76  and loss:  56.62731221318245
forward train acc: top1 ->  92.81200001464843 ; top5 ->  99.822  and loss:  21.254763215780258
test acc: top1 ->  86.03 ; top5 ->  98.96  and loss:  51.876151382923126
forward train acc: top1 ->  93.9600000024414 ; top5 ->  99.852  and loss:  17.53140390664339
test acc: top1 ->  87.02 ; top5 ->  99.07  and loss:  49.640094459056854
forward train acc: top1 ->  94.79 ; top5 ->  99.91199997558594  and loss:  14.820911794900894
test acc: top1 ->  87.52 ; top5 ->  99.13  and loss:  47.85741828382015
forward train acc: top1 ->  95.29199998779296 ; top5 ->  99.93  and loss:  13.543980598449707
test acc: top1 ->  88.05 ; top5 ->  99.23  and loss:  46.84064540266991
forward train acc: top1 ->  95.64999998779297 ; top5 ->  99.932  and loss:  12.520109049975872
test acc: top1 ->  88.15 ; top5 ->  99.28  and loss:  47.19343586266041
forward train acc: top1 ->  95.86399999511718 ; top5 ->  99.938  and loss:  11.90947736799717
test acc: top1 ->  88.26 ; top5 ->  99.19  and loss:  47.433810502290726
forward train acc: top1 ->  96.01399999267578 ; top5 ->  99.936  and loss:  11.39811009913683
test acc: top1 ->  88.34 ; top5 ->  99.26  and loss:  46.89053937792778
forward train acc: top1 ->  96.21999998779297 ; top5 ->  99.946  and loss:  10.734776146709919
test acc: top1 ->  88.45 ; top5 ->  99.25  and loss:  46.91840256750584
forward train acc: top1 ->  96.18000001953125 ; top5 ->  99.936  and loss:  10.974939428269863
test acc: top1 ->  88.64 ; top5 ->  99.23  and loss:  46.16898311674595
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  105 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -2.7442874163389206 , diff:  2.7442874163389206
adv train loss:  -2.7046634070575237 , diff:  0.039624009281396866
adv train loss:  -2.8919271156191826 , diff:  0.18726370856165886
adv train loss:  -2.767562795430422 , diff:  0.12436432018876076
adv train loss:  -2.853657651692629 , diff:  0.08609485626220703
adv train loss:  -2.818358764052391 , diff:  0.03529888764023781
adv train loss:  -2.899171408265829 , diff:  0.08081264421343803
adv train loss:  -2.835488198325038 , diff:  0.06368320994079113
adv train loss:  -2.874461555853486 , diff:  0.038973357528448105
adv train loss:  -2.885271217674017 , diff:  0.010809661820530891
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  103
test acc: top1 ->  19.1 ; top5 ->  48.79  and loss:  2124858.7158203125
forward train acc: top1 ->  83.78800001708984 ; top5 ->  99.02999997558594  and loss:  53.90707791596651
test acc: top1 ->  87.35 ; top5 ->  99.12  and loss:  46.786279648542404
forward train acc: top1 ->  96.63399998535156 ; top5 ->  99.972  and loss:  10.256298050284386
test acc: top1 ->  89.39 ; top5 ->  99.18  and loss:  44.308397471904755
forward train acc: top1 ->  97.96799998046875 ; top5 ->  99.974  and loss:  6.109691880643368
test acc: top1 ->  90.19 ; top5 ->  99.2  and loss:  44.72615945339203
forward train acc: top1 ->  98.55199997802734 ; top5 ->  99.99  and loss:  4.152493849396706
test acc: top1 ->  90.53 ; top5 ->  99.29  and loss:  46.77607003599405
forward train acc: top1 ->  98.89400000488281 ; top5 ->  99.992  and loss:  3.327148487791419
test acc: top1 ->  90.61 ; top5 ->  99.22  and loss:  47.07454491406679
forward train acc: top1 ->  99.10600000732421 ; top5 ->  99.99  and loss:  2.7510721553117037
test acc: top1 ->  90.57 ; top5 ->  99.23  and loss:  48.627705462276936
forward train acc: top1 ->  99.15799997558594 ; top5 ->  99.992  and loss:  2.6241072891280055
test acc: top1 ->  90.82 ; top5 ->  99.25  and loss:  47.470118038356304
forward train acc: top1 ->  99.17399997558594 ; top5 ->  99.994  and loss:  2.4980684872716665
test acc: top1 ->  90.89 ; top5 ->  99.22  and loss:  48.25699707120657
forward train acc: top1 ->  99.17200000488282 ; top5 ->  99.992  and loss:  2.408009434118867
test acc: top1 ->  90.83 ; top5 ->  99.23  and loss:  49.36537566035986
forward train acc: top1 ->  99.31200000488282 ; top5 ->  99.994  and loss:  2.125372404232621
test acc: top1 ->  90.91 ; top5 ->  99.34  and loss:  49.18030206114054
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  104 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -41.3150250017643 , diff:  41.3150250017643
adv train loss:  -41.31642870604992 , diff:  0.001403704285621643
layer  8  adv train finish, try to retain  411
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -8.853829773142934 , diff:  8.853829773142934
adv train loss:  -8.700730681419373 , diff:  0.1530990917235613
adv train loss:  -9.03252499178052 , diff:  0.3317943103611469
adv train loss:  -8.815676737576723 , diff:  0.2168482542037964
adv train loss:  -8.927775848656893 , diff:  0.11209911108016968
adv train loss:  -8.792377665638924 , diff:  0.13539818301796913
adv train loss:  -9.024147398769855 , diff:  0.23176973313093185
adv train loss:  -9.06920301914215 , diff:  0.04505562037229538
adv train loss:  -8.951669998466969 , diff:  0.11753302067518234
adv train loss:  -9.046729929745197 , diff:  0.09505993127822876
layer  9  adv train finish, try to retain  448
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -484.7503294944763 , diff:  484.7503294944763
adv train loss:  -485.3054533004761 , diff:  0.5551238059997559
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  21
test acc: top1 ->  10.0 ; top5 ->  50.02  and loss:  36849.18328857422
forward train acc: top1 ->  78.36399998291016 ; top5 ->  98.53  and loss:  88.97507205605507
test acc: top1 ->  84.95 ; top5 ->  98.59  and loss:  59.445272386074066
forward train acc: top1 ->  98.81799997558593 ; top5 ->  99.99  and loss:  7.001731418073177
test acc: top1 ->  90.47 ; top5 ->  98.96  and loss:  42.40934108197689
forward train acc: top1 ->  99.47000000732422 ; top5 ->  100.0  and loss:  2.8075438551604748
test acc: top1 ->  90.93 ; top5 ->  99.02  and loss:  43.87499848008156
forward train acc: top1 ->  99.636 ; top5 ->  99.996  and loss:  1.6638241410255432
test acc: top1 ->  91.39 ; top5 ->  99.06  and loss:  45.63051526248455
forward train acc: top1 ->  99.762 ; top5 ->  100.0  and loss:  1.0963161690160632
test acc: top1 ->  91.31 ; top5 ->  99.1  and loss:  47.64888623356819
forward train acc: top1 ->  99.798 ; top5 ->  100.0  and loss:  0.8708291319198906
test acc: top1 ->  91.45 ; top5 ->  99.1  and loss:  48.53986284136772
forward train acc: top1 ->  99.7900000024414 ; top5 ->  99.998  and loss:  0.8333279397338629
test acc: top1 ->  91.53 ; top5 ->  99.04  and loss:  48.593481719493866
forward train acc: top1 ->  99.81799997558593 ; top5 ->  99.998  and loss:  0.7107952646911144
test acc: top1 ->  91.56 ; top5 ->  99.09  and loss:  48.81554975360632
forward train acc: top1 ->  99.84 ; top5 ->  100.0  and loss:  0.6586942565627396
test acc: top1 ->  91.59 ; top5 ->  99.06  and loss:  49.793294712901115
forward train acc: top1 ->  99.86 ; top5 ->  99.998  and loss:  0.5542083866894245
test acc: top1 ->  91.66 ; top5 ->  99.02  and loss:  51.105540692806244
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  22 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -3418.4176330566406 , diff:  3418.4176330566406
adv train loss:  -3546.384078979492 , diff:  127.96644592285156
adv train loss:  -3546.4311027526855 , diff:  0.047023773193359375
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  4
test acc: top1 ->  0.78 ; top5 ->  55.42  and loss:  34597.632720947266
forward train acc: top1 ->  95.842 ; top5 ->  99.794  and loss:  20.320439320523292
test acc: top1 ->  90.65 ; top5 ->  99.01  and loss:  43.25176205486059
forward train acc: top1 ->  99.826 ; top5 ->  100.0  and loss:  1.0578544717282057
test acc: top1 ->  91.88 ; top5 ->  99.14  and loss:  39.9966342151165
forward train acc: top1 ->  99.87799997558594 ; top5 ->  100.0  and loss:  0.6776403356343508
test acc: top1 ->  92.01 ; top5 ->  99.18  and loss:  42.34243227541447
forward train acc: top1 ->  99.904 ; top5 ->  100.0  and loss:  0.46310803876258433
test acc: top1 ->  91.97 ; top5 ->  99.14  and loss:  44.96148758381605
forward train acc: top1 ->  99.91 ; top5 ->  100.0  and loss:  0.3813018291257322
test acc: top1 ->  92.01 ; top5 ->  99.17  and loss:  46.1088053137064
forward train acc: top1 ->  99.914 ; top5 ->  100.0  and loss:  0.33772494830191135
test acc: top1 ->  92.01 ; top5 ->  99.18  and loss:  47.20285168290138
forward train acc: top1 ->  99.918 ; top5 ->  100.0  and loss:  0.3063804459525272
test acc: top1 ->  91.97 ; top5 ->  99.18  and loss:  47.14872108399868
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.26974293659441173
test acc: top1 ->  92.07 ; top5 ->  99.18  and loss:  48.082798793911934
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.250014464429114
test acc: top1 ->  92.09 ; top5 ->  99.13  and loss:  48.92143125832081
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.2280692628119141
test acc: top1 ->  92.07 ; top5 ->  99.17  and loss:  49.52756878733635
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  5 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -396.86759209632874 , diff:  396.86759209632874
adv train loss:  -396.841472864151 , diff:  0.026119232177734375
layer  12  adv train finish, try to retain  493
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -10921.352005004883 , diff:  10921.352005004883
adv train loss:  -17763.274948120117 , diff:  6841.922943115234
adv train loss:  -24378.939895629883 , diff:  6615.664947509766
adv train loss:  -30945.470092773438 , diff:  6566.530197143555
adv train loss:  -37530.503814697266 , diff:  6585.033721923828
adv train loss:  -44157.681884765625 , diff:  6627.178070068359
adv train loss:  -50799.73211669922 , diff:  6642.050231933594
adv train loss:  -57433.663818359375 , diff:  6633.931701660156
adv train loss:  -64013.63494873047 , diff:  6579.971130371094
adv train loss:  -70589.29370117188 , diff:  6575.658752441406
layer  13  adv train finish, try to retain  11
test acc: top1 ->  10.0 ; top5 ->  79.33  and loss:  1087.8392896652222
forward train acc: top1 ->  69.0640000024414 ; top5 ->  96.146  and loss:  188.6572544053197
test acc: top1 ->  90.46 ; top5 ->  98.34  and loss:  38.93247751891613
forward train acc: top1 ->  99.68199997558594 ; top5 ->  100.0  and loss:  4.577424086630344
test acc: top1 ->  91.3 ; top5 ->  98.38  and loss:  36.707536444067955
forward train acc: top1 ->  99.818 ; top5 ->  100.0  and loss:  2.496510237455368
test acc: top1 ->  91.57 ; top5 ->  98.37  and loss:  37.28581689298153
forward train acc: top1 ->  99.8440000024414 ; top5 ->  100.0  and loss:  1.7445414513349533
test acc: top1 ->  91.64 ; top5 ->  98.36  and loss:  38.630954913794994
forward train acc: top1 ->  99.866 ; top5 ->  99.998  and loss:  1.3095184676349163
test acc: top1 ->  91.75 ; top5 ->  98.48  and loss:  39.24128547310829
forward train acc: top1 ->  99.912 ; top5 ->  100.0  and loss:  1.0743916369974613
test acc: top1 ->  91.77 ; top5 ->  98.43  and loss:  39.85079460591078
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.9414638606831431
test acc: top1 ->  91.69 ; top5 ->  98.47  and loss:  40.425251603126526
forward train acc: top1 ->  99.89799997558593 ; top5 ->  100.0  and loss:  0.8791920999065042
test acc: top1 ->  91.94 ; top5 ->  98.42  and loss:  40.789450883865356
forward train acc: top1 ->  99.9 ; top5 ->  100.0  and loss:  0.8196926973760128
test acc: top1 ->  91.9 ; top5 ->  98.41  and loss:  41.456846706569195
forward train acc: top1 ->  99.912 ; top5 ->  100.0  and loss:  0.7460056324489415
test acc: top1 ->  91.82 ; top5 ->  98.44  and loss:  41.87260867655277
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  23 / 512 , inc:  1
layer  0  :  0.484375  ==>  31 / 64 , inc:  1
layer  1  :  0.875  ==>  56 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.41015625  ==>  105 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.041015625  ==>  21 / 512 , inc:  5
layer  9  :  0.1875  ==>  96 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.009765625  ==>  5 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.044921875  ==>  23 / 512 , inc:  1
eps [1.167717041015625, 0.02736836814880371, 1.167717041015625, 1.167717041015625, 1.167717041015625, 1.167717041015625, 1.167717041015625, 1.167717041015625, 0.004561394691467285, 0.02736836814880371, 1.167717041015625, 1.167717041015625, 0.02736836814880371, 9.341736328125]  wait [3, 0, 2, 2, 2, 2, 2, 2, 0, 0, 2, 4, 0, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1]  tol: 5
$$$$$$$$$$$$$ epoch  45  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
adv train loss:  -18.102720964699984 , diff:  18.102720964699984
adv train loss:  -17.95689047127962 , diff:  0.14583049342036247
adv train loss:  -18.28697796165943 , diff:  0.33008749037981033
adv train loss:  -17.71046857535839 , diff:  0.5765093863010406
adv train loss:  -18.115428544580936 , diff:  0.4049599692225456
adv train loss:  -18.052062302827835 , diff:  0.06336624175310135
adv train loss:  -17.84690911695361 , diff:  0.2051531858742237
adv train loss:  -17.80697312951088 , diff:  0.03993598744273186
adv train loss:  -18.2873547822237 , diff:  0.48038165271282196
adv train loss:  -17.556712687015533 , diff:  0.730642095208168
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -1235.9352806061506 , diff:  1235.9352806061506
adv train loss:  -1758.09699344635 , diff:  522.1617128401995
adv train loss:  -1769.571870803833 , diff:  11.47487735748291
adv train loss:  -1785.694725036621 , diff:  16.122854232788086
adv train loss:  -1786.4965419769287 , diff:  0.8018169403076172
adv train loss:  -1785.9613399505615 , diff:  0.5352020263671875
adv train loss:  -1783.2515354156494 , diff:  2.7098045349121094
adv train loss:  -1784.7469272613525 , diff:  1.495391845703125
adv train loss:  -1792.5338115692139 , diff:  7.786884307861328
adv train loss:  -1786.7532424926758 , diff:  5.780569076538086
************ all values are small in this layer **********
layer  2  adv train finish, try to retain  99
test acc: top1 ->  19.54 ; top5 ->  68.18  and loss:  1443.045433998108
forward train acc: top1 ->  99.16999997558594 ; top5 ->  99.984  and loss:  3.017684394493699
test acc: top1 ->  91.42 ; top5 ->  99.12  and loss:  69.25911681354046
forward train acc: top1 ->  99.62199997558594 ; top5 ->  99.996  and loss:  1.3061440363526344
test acc: top1 ->  91.15 ; top5 ->  99.12  and loss:  66.11058759689331
forward train acc: top1 ->  99.688 ; top5 ->  99.998  and loss:  1.051718084141612
test acc: top1 ->  91.53 ; top5 ->  99.14  and loss:  64.14806749671698
forward train acc: top1 ->  99.65399997802734 ; top5 ->  100.0  and loss:  1.0098171485587955
test acc: top1 ->  91.2 ; top5 ->  99.09  and loss:  65.69678062200546
forward train acc: top1 ->  99.734 ; top5 ->  100.0  and loss:  0.7938595919404179
test acc: top1 ->  91.47 ; top5 ->  99.15  and loss:  62.70912294834852
forward train acc: top1 ->  99.75600000244141 ; top5 ->  100.0  and loss:  0.760976068675518
test acc: top1 ->  91.69 ; top5 ->  99.09  and loss:  63.72947646677494
forward train acc: top1 ->  99.74599997558593 ; top5 ->  99.998  and loss:  0.67332598939538
test acc: top1 ->  91.7 ; top5 ->  99.17  and loss:  61.90870068222284
forward train acc: top1 ->  99.75399997558594 ; top5 ->  100.0  and loss:  0.7015439081005752
test acc: top1 ->  91.54 ; top5 ->  99.14  and loss:  61.78561960160732
forward train acc: top1 ->  99.76 ; top5 ->  100.0  and loss:  0.6763892318122089
test acc: top1 ->  91.63 ; top5 ->  99.15  and loss:  62.35280792415142
forward train acc: top1 ->  99.814 ; top5 ->  100.0  and loss:  0.5779045124072582
test acc: top1 ->  91.68 ; top5 ->  99.13  and loss:  63.28698045015335
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -1019.121716722846 , diff:  1019.121716722846
adv train loss:  -1675.9410972595215 , diff:  656.8193805366755
adv train loss:  -1738.6626415252686 , diff:  62.72154426574707
adv train loss:  -1738.9623985290527 , diff:  0.2997570037841797
adv train loss:  -1746.0554828643799 , diff:  7.093084335327148
adv train loss:  -1765.7794647216797 , diff:  19.723981857299805
adv train loss:  -1768.1566772460938 , diff:  2.3772125244140625
adv train loss:  -1770.920488357544 , diff:  2.7638111114501953
adv train loss:  -1766.5418663024902 , diff:  4.378622055053711
adv train loss:  -1771.8656196594238 , diff:  5.323753356933594
************ all values are small in this layer **********
layer  3  adv train finish, try to retain  86
test acc: top1 ->  11.28 ; top5 ->  52.87  and loss:  8289.067344665527
forward train acc: top1 ->  99.7840000024414 ; top5 ->  100.0  and loss:  0.7052022386342287
test acc: top1 ->  91.29 ; top5 ->  99.03  and loss:  65.03601488471031
forward train acc: top1 ->  99.7580000024414 ; top5 ->  100.0  and loss:  0.6963793421164155
test acc: top1 ->  91.43 ; top5 ->  99.19  and loss:  61.784950107336044
forward train acc: top1 ->  99.76599997558594 ; top5 ->  100.0  and loss:  0.6569852991960943
test acc: top1 ->  91.59 ; top5 ->  99.2  and loss:  61.59434697031975
forward train acc: top1 ->  99.8320000024414 ; top5 ->  100.0  and loss:  0.5096996519714594
test acc: top1 ->  91.3 ; top5 ->  99.23  and loss:  62.454302445054054
forward train acc: top1 ->  99.84199997558594 ; top5 ->  100.0  and loss:  0.5403227978385985
test acc: top1 ->  91.3 ; top5 ->  99.27  and loss:  63.762724578380585
forward train acc: top1 ->  99.88199997558594 ; top5 ->  100.0  and loss:  0.32453055772930384
test acc: top1 ->  91.59 ; top5 ->  99.22  and loss:  61.697639018297195
forward train acc: top1 ->  99.8620000024414 ; top5 ->  100.0  and loss:  0.41208003740757704
test acc: top1 ->  91.48 ; top5 ->  99.27  and loss:  62.95985206961632
forward train acc: top1 ->  99.864 ; top5 ->  100.0  and loss:  0.3651552972733043
test acc: top1 ->  91.51 ; top5 ->  99.2  and loss:  62.40747492015362
forward train acc: top1 ->  99.858 ; top5 ->  100.0  and loss:  0.41712277662009
test acc: top1 ->  91.52 ; top5 ->  99.25  and loss:  62.52659237384796
forward train acc: top1 ->  99.89 ; top5 ->  100.0  and loss:  0.31241262902040035
test acc: top1 ->  91.44 ; top5 ->  99.15  and loss:  63.02859078347683
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -311.4834910184145 , diff:  311.4834910184145
adv train loss:  -1766.7793550491333 , diff:  1455.2958640307188
adv train loss:  -1948.3550472259521 , diff:  181.57569217681885
adv train loss:  -1960.3232536315918 , diff:  11.968206405639648
adv train loss:  -1996.7285709381104 , diff:  36.405317306518555
adv train loss:  -2079.9889526367188 , diff:  83.2603816986084
adv train loss:  -2176.670488357544 , diff:  96.6815357208252
adv train loss:  -2187.467056274414 , diff:  10.796567916870117
adv train loss:  -2193.8755855560303 , diff:  6.408529281616211
adv train loss:  -2201.9852085113525 , diff:  8.109622955322266
************ all values are small in this layer **********
layer  4  adv train finish, try to retain  155
test acc: top1 ->  14.66 ; top5 ->  54.59  and loss:  11569.713180541992
forward train acc: top1 ->  98.64600000732422 ; top5 ->  99.992  and loss:  4.435062250122428
test acc: top1 ->  90.39 ; top5 ->  99.16  and loss:  52.839355409145355
forward train acc: top1 ->  98.98599997558594 ; top5 ->  99.996  and loss:  2.964990303851664
test acc: top1 ->  90.67 ; top5 ->  99.23  and loss:  48.975004360079765
forward train acc: top1 ->  99.12799997802735 ; top5 ->  99.996  and loss:  2.581646729260683
test acc: top1 ->  90.76 ; top5 ->  99.22  and loss:  49.07750740647316
forward train acc: top1 ->  99.1480000024414 ; top5 ->  99.998  and loss:  2.4666688228026032
test acc: top1 ->  90.93 ; top5 ->  99.21  and loss:  49.92425684630871
forward train acc: top1 ->  99.22599998046876 ; top5 ->  99.99599997558593  and loss:  2.2477262280881405
test acc: top1 ->  90.92 ; top5 ->  99.16  and loss:  49.98770909011364
forward train acc: top1 ->  99.2580000024414 ; top5 ->  99.992  and loss:  2.0081588299944997
test acc: top1 ->  91.07 ; top5 ->  99.2  and loss:  49.77926605939865
forward train acc: top1 ->  99.35199997802735 ; top5 ->  99.994  and loss:  1.8765063397586346
test acc: top1 ->  91.04 ; top5 ->  99.17  and loss:  49.635765731334686
forward train acc: top1 ->  99.436 ; top5 ->  99.998  and loss:  1.6710601663216949
test acc: top1 ->  91.01 ; top5 ->  99.24  and loss:  49.70525008440018
forward train acc: top1 ->  99.37599997802734 ; top5 ->  99.998  and loss:  1.8449972979724407
test acc: top1 ->  90.73 ; top5 ->  99.29  and loss:  50.927953734993935
forward train acc: top1 ->  99.35199998046875 ; top5 ->  99.99799997558594  and loss:  1.8214756324887276
test acc: top1 ->  90.94 ; top5 ->  99.26  and loss:  50.813194662332535
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  156 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -533.6934080682695 , diff:  533.6934080682695
adv train loss:  -1924.5756130218506 , diff:  1390.882204953581
adv train loss:  -1960.6956787109375 , diff:  36.120065689086914
adv train loss:  -1966.4155178070068 , diff:  5.719839096069336
adv train loss:  -1967.418264389038 , diff:  1.00274658203125
adv train loss:  -1968.4719371795654 , diff:  1.0536727905273438
adv train loss:  -1965.8813724517822 , diff:  2.590564727783203
adv train loss:  -1969.7138767242432 , diff:  3.8325042724609375
adv train loss:  -1976.8741912841797 , diff:  7.160314559936523
adv train loss:  -1984.7459964752197 , diff:  7.871805191040039
************ all values are small in this layer **********
layer  5  adv train finish, try to retain  148
test acc: top1 ->  38.11 ; top5 ->  77.42  and loss:  1461.043553352356
forward train acc: top1 ->  99.4540000024414 ; top5 ->  100.0  and loss:  1.7079370096325874
test acc: top1 ->  91.23 ; top5 ->  99.27  and loss:  52.597365237772465
forward train acc: top1 ->  99.57799997802735 ; top5 ->  100.0  and loss:  1.1780584994703531
test acc: top1 ->  91.31 ; top5 ->  99.32  and loss:  54.55166124552488
forward train acc: top1 ->  99.69399997558594 ; top5 ->  99.998  and loss:  0.897418805398047
test acc: top1 ->  91.38 ; top5 ->  99.29  and loss:  54.300692193210125
forward train acc: top1 ->  99.7 ; top5 ->  100.0  and loss:  0.8992695232154801
test acc: top1 ->  91.51 ; top5 ->  99.29  and loss:  54.91615467518568
forward train acc: top1 ->  99.7220000024414 ; top5 ->  100.0  and loss:  0.7993089817464352
test acc: top1 ->  91.36 ; top5 ->  99.29  and loss:  55.54871932417154
forward train acc: top1 ->  99.76199997558594 ; top5 ->  100.0  and loss:  0.6909920545294881
test acc: top1 ->  91.62 ; top5 ->  99.34  and loss:  54.8607539460063
forward train acc: top1 ->  99.80599997558593 ; top5 ->  99.998  and loss:  0.5773022160865366
test acc: top1 ->  91.64 ; top5 ->  99.32  and loss:  55.738253369927406
forward train acc: top1 ->  99.80399997558594 ; top5 ->  100.0  and loss:  0.5750130750238895
test acc: top1 ->  91.47 ; top5 ->  99.26  and loss:  56.79606053978205
forward train acc: top1 ->  99.818 ; top5 ->  100.0  and loss:  0.5220441919518635
test acc: top1 ->  91.52 ; top5 ->  99.35  and loss:  58.13675194233656
forward train acc: top1 ->  99.786 ; top5 ->  100.0  and loss:  0.6087132142856717
test acc: top1 ->  91.54 ; top5 ->  99.31  and loss:  58.395879447460175
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -648.5592789724469 , diff:  648.5592789724469
adv train loss:  -2078.1813793182373 , diff:  1429.6221003457904
adv train loss:  -2105.828042984009 , diff:  27.646663665771484
adv train loss:  -2129.83571434021 , diff:  24.007671356201172
adv train loss:  -2167.1109294891357 , diff:  37.27521514892578
adv train loss:  -2190.049793243408 , diff:  22.93886375427246
adv train loss:  -2197.486597061157 , diff:  7.436803817749023
adv train loss:  -2195.1130695343018 , diff:  2.3735275268554688
adv train loss:  -2219.2364921569824 , diff:  24.123422622680664
adv train loss:  -2217.7238006591797 , diff:  1.5126914978027344
************ all values are small in this layer **********
layer  6  adv train finish, try to retain  104
test acc: top1 ->  10.0 ; top5 ->  50.48  and loss:  2604362.5703125
forward train acc: top1 ->  99.8500000024414 ; top5 ->  100.0  and loss:  0.4355221660807729
test acc: top1 ->  92.19 ; top5 ->  99.38  and loss:  56.61727361381054
==> this epoch:  104 / 256
---------------- start layer  7  ---------------
adv train loss:  -0.32236531539820135 , diff:  0.32236531539820135
adv train loss:  -0.3284504199400544 , diff:  0.00608510454185307
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  103
test acc: top1 ->  15.66 ; top5 ->  53.61  and loss:  2169805.8779296875
forward train acc: top1 ->  96.49199998291016 ; top5 ->  99.916  and loss:  11.890256572514772
test acc: top1 ->  89.45 ; top5 ->  99.04  and loss:  53.53234409540892
forward train acc: top1 ->  98.75399997802734 ; top5 ->  99.998  and loss:  3.804040100425482
test acc: top1 ->  90.0 ; top5 ->  99.28  and loss:  52.83220969885588
forward train acc: top1 ->  99.17399997802734 ; top5 ->  99.996  and loss:  2.538432601839304
test acc: top1 ->  90.83 ; top5 ->  99.19  and loss:  52.116073451936245
forward train acc: top1 ->  99.31599997802735 ; top5 ->  99.996  and loss:  2.1057883258908987
test acc: top1 ->  90.28 ; top5 ->  99.24  and loss:  55.33469807356596
forward train acc: top1 ->  99.36400000488281 ; top5 ->  99.996  and loss:  1.8548093643039465
test acc: top1 ->  90.86 ; top5 ->  99.3  and loss:  54.165087044239044
forward train acc: top1 ->  99.44799998046875 ; top5 ->  99.998  and loss:  1.547847831621766
test acc: top1 ->  90.96 ; top5 ->  99.32  and loss:  54.44603933393955
forward train acc: top1 ->  99.53600000244141 ; top5 ->  99.998  and loss:  1.297103188931942
test acc: top1 ->  90.99 ; top5 ->  99.35  and loss:  55.169440031051636
forward train acc: top1 ->  99.51199997558594 ; top5 ->  100.0  and loss:  1.3983570509590209
test acc: top1 ->  91.02 ; top5 ->  99.24  and loss:  56.11199876666069
forward train acc: top1 ->  99.51800000244141 ; top5 ->  99.998  and loss:  1.3796092299744487
test acc: top1 ->  91.16 ; top5 ->  99.39  and loss:  55.472955755889416
forward train acc: top1 ->  99.58999997802735 ; top5 ->  99.998  and loss:  1.2274655904620886
test acc: top1 ->  91.2 ; top5 ->  99.33  and loss:  56.55715775489807
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  104 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -77.67544400691986 , diff:  77.67544400691986
adv train loss:  -76.9228765964508 , diff:  0.7525674104690552
adv train loss:  -77.21452307701111 , diff:  0.29164648056030273
adv train loss:  -77.98341143131256 , diff:  0.7688883543014526
adv train loss:  -77.34441912174225 , diff:  0.6389923095703125
adv train loss:  -76.4582976102829 , diff:  0.8861215114593506
adv train loss:  -76.93953639268875 , diff:  0.48123878240585327
adv train loss:  -77.62286460399628 , diff:  0.6833282113075256
adv train loss:  -77.79633045196533 , diff:  0.17346584796905518
adv train loss:  -78.1909772157669 , diff:  0.3946467638015747
layer  8  adv train finish, try to retain  426
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -5.5155973471701145 , diff:  5.5155973471701145
adv train loss:  -5.880658354610205 , diff:  0.3650610074400902
adv train loss:  -5.809473626315594 , diff:  0.07118472829461098
adv train loss:  -5.6269691325724125 , diff:  0.18250449374318123
adv train loss:  -5.281107012182474 , diff:  0.34586212038993835
adv train loss:  -5.808421107009053 , diff:  0.5273140948265791
adv train loss:  -6.010542130097747 , diff:  0.20212102308869362
adv train loss:  -5.5472457036376 , diff:  0.4632964264601469
adv train loss:  -5.53832832723856 , diff:  0.008917376399040222
layer  9  adv train finish, try to retain  429
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -348.6713910102844 , diff:  348.6713910102844
adv train loss:  -350.48754239082336 , diff:  1.8161513805389404
adv train loss:  -348.60595202445984 , diff:  1.8815903663635254
adv train loss:  -349.91639471054077 , diff:  1.3104426860809326
adv train loss:  -348.7324917316437 , diff:  1.1839029788970947
adv train loss:  -348.7189495563507 , diff:  0.01354217529296875
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  21
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  67489.5972290039
forward train acc: top1 ->  95.54599997558594 ; top5 ->  99.892  and loss:  24.425796052441
test acc: top1 ->  90.33 ; top5 ->  99.07  and loss:  48.882352128624916
forward train acc: top1 ->  99.82 ; top5 ->  100.0  and loss:  0.9817858918104321
test acc: top1 ->  91.43 ; top5 ->  99.16  and loss:  45.36656966805458
forward train acc: top1 ->  99.842 ; top5 ->  100.0  and loss:  0.6632952028885484
test acc: top1 ->  91.62 ; top5 ->  99.03  and loss:  47.011747889220715
forward train acc: top1 ->  99.894 ; top5 ->  100.0  and loss:  0.49058567418251187
test acc: top1 ->  91.66 ; top5 ->  99.14  and loss:  48.62969160825014
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.36519282194785774
test acc: top1 ->  91.78 ; top5 ->  99.18  and loss:  48.61851164698601
forward train acc: top1 ->  99.912 ; top5 ->  100.0  and loss:  0.33998900884762406
test acc: top1 ->  91.74 ; top5 ->  99.03  and loss:  50.16056939959526
forward train acc: top1 ->  99.924 ; top5 ->  100.0  and loss:  0.31000262906309217
test acc: top1 ->  91.72 ; top5 ->  99.1  and loss:  51.42144515365362
forward train acc: top1 ->  99.918 ; top5 ->  100.0  and loss:  0.3230085269315168
test acc: top1 ->  91.92 ; top5 ->  99.14  and loss:  50.694695591926575
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.28978184796869755
test acc: top1 ->  91.89 ; top5 ->  99.09  and loss:  51.86834283173084
forward train acc: top1 ->  99.91799997558594 ; top5 ->  100.0  and loss:  0.2897958876565099
test acc: top1 ->  91.74 ; top5 ->  99.08  and loss:  53.711853928864
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  22 / 512 , inc:  1
---------------- start layer  11  ---------------
### skip layer  11 wait:  4  ###
---------------- start layer  12  ---------------
adv train loss:  -295.56952476501465 , diff:  295.56952476501465
adv train loss:  -293.23382556438446 , diff:  2.335699200630188
adv train loss:  -293.9386897087097 , diff:  0.7048641443252563
adv train loss:  -294.8309073448181 , diff:  0.8922176361083984
adv train loss:  -293.84595346450806 , diff:  0.9849538803100586
adv train loss:  -294.19232654571533 , diff:  0.3463730812072754
layer  12  adv train finish, try to retain  492
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -19122.201194763184 , diff:  19122.201194763184
adv train loss:  -37512.2575378418 , diff:  18390.056343078613
adv train loss:  -54078.3420715332 , diff:  16566.084533691406
adv train loss:  -69585.86962890625 , diff:  15507.527557373047
adv train loss:  -84654.26647949219 , diff:  15068.396850585938
adv train loss:  -99510.24487304688 , diff:  14855.978393554688
adv train loss:  -113102.763671875 , diff:  13592.518798828125
adv train loss:  -121417.58312988281 , diff:  8314.819458007812
adv train loss:  -125890.56970214844 , diff:  4472.986572265625
adv train loss:  -128891.88159179688 , diff:  3001.3118896484375
layer  13  adv train finish, try to retain  31
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.484375  ==>  31 / 64 , inc:  1
layer  1  :  0.875  ==>  56 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.40625  ==>  104 / 256 , inc:  2
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.041015625  ==>  21 / 512 , inc:  5
layer  9  :  0.1875  ==>  96 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.009765625  ==>  5 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.044921875  ==>  23 / 512 , inc:  1
eps [1.167717041015625, 0.05473673629760742, 0.8757877807617187, 0.8757877807617187, 0.8757877807617187, 0.8757877807617187, 1.167717041015625, 0.8757877807617187, 0.00912278938293457, 0.05473673629760742, 0.8757877807617187, 1.167717041015625, 0.05473673629760742, 18.68347265625]  wait [2, 0, 4, 4, 4, 4, 0, 4, 0, 0, 4, 3, 0, 2]  inc [1, 1, 1, 1, 1, 1, 2, 1, 5, 1, 1, 1, 1, 1]  tol: 5
$$$$$$$$$$$$$ epoch  46  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1881.302890777588 , diff:  1881.302890777588
adv train loss:  -1876.7725410461426 , diff:  4.5303497314453125
adv train loss:  -1864.6309051513672 , diff:  12.14163589477539
adv train loss:  -1867.3837623596191 , diff:  2.752857208251953
adv train loss:  -1863.9343509674072 , diff:  3.449411392211914
adv train loss:  -1867.8364810943604 , diff:  3.902130126953125
adv train loss:  -1860.357521057129 , diff:  7.478960037231445
adv train loss:  -1862.964735031128 , diff:  2.6072139739990234
adv train loss:  -1856.5847549438477 , diff:  6.379980087280273
adv train loss:  -1883.7308025360107 , diff:  27.146047592163086
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  30
test acc: top1 ->  10.0 ; top5 ->  50.11  and loss:  3887604.03125
forward train acc: top1 ->  94.02200001464844 ; top5 ->  99.75199997558593  and loss:  29.42845518887043
test acc: top1 ->  87.66 ; top5 ->  98.82  and loss:  69.75822427868843
forward train acc: top1 ->  96.31200001220704 ; top5 ->  99.918  and loss:  11.765481822192669
test acc: top1 ->  88.6 ; top5 ->  98.92  and loss:  51.53803297877312
forward train acc: top1 ->  96.97000001220704 ; top5 ->  99.94199997558594  and loss:  9.043726272881031
test acc: top1 ->  89.2 ; top5 ->  99.02  and loss:  46.608052998781204
forward train acc: top1 ->  97.55999998779296 ; top5 ->  99.96399997558593  and loss:  7.393932547420263
test acc: top1 ->  89.47 ; top5 ->  99.09  and loss:  44.425970539450645
forward train acc: top1 ->  97.97200000732421 ; top5 ->  99.968  and loss:  6.049126412719488
test acc: top1 ->  89.73 ; top5 ->  99.13  and loss:  44.98508436977863
forward train acc: top1 ->  98.17000000732422 ; top5 ->  99.984  and loss:  5.435657110065222
test acc: top1 ->  90.08 ; top5 ->  99.11  and loss:  43.88379666209221
forward train acc: top1 ->  98.27999998046874 ; top5 ->  99.978  and loss:  4.904294274747372
test acc: top1 ->  90.24 ; top5 ->  99.1  and loss:  44.36757220327854
forward train acc: top1 ->  98.33800000732423 ; top5 ->  99.986  and loss:  4.660645507276058
test acc: top1 ->  90.33 ; top5 ->  99.12  and loss:  44.17211113870144
forward train acc: top1 ->  98.55000000244141 ; top5 ->  99.984  and loss:  4.291294487193227
test acc: top1 ->  90.43 ; top5 ->  99.15  and loss:  44.292200207710266
forward train acc: top1 ->  98.61400000732422 ; top5 ->  99.98199997558594  and loss:  4.117613576352596
test acc: top1 ->  90.53 ; top5 ->  99.22  and loss:  44.145111963152885
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  31 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -0.755344464443624 , diff:  0.755344464443624
adv train loss:  -0.7532766070216894 , diff:  0.0020678574219346046
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  55
test acc: top1 ->  14.69 ; top5 ->  61.33  and loss:  771.2481021881104
forward train acc: top1 ->  99.45199997558593 ; top5 ->  100.0  and loss:  1.6741353273391724
test acc: top1 ->  91.41 ; top5 ->  99.26  and loss:  48.704453244805336
forward train acc: top1 ->  99.60400000244141 ; top5 ->  100.0  and loss:  1.125034287571907
test acc: top1 ->  91.38 ; top5 ->  99.27  and loss:  52.474486101418734
forward train acc: top1 ->  99.68 ; top5 ->  100.0  and loss:  0.9324814779683948
test acc: top1 ->  91.59 ; top5 ->  99.26  and loss:  53.908152528107166
forward train acc: top1 ->  99.77999997558594 ; top5 ->  99.998  and loss:  0.7076732376590371
test acc: top1 ->  91.64 ; top5 ->  99.26  and loss:  57.5456809848547
forward train acc: top1 ->  99.756 ; top5 ->  100.0  and loss:  0.706392755266279
test acc: top1 ->  91.58 ; top5 ->  99.28  and loss:  57.97831556946039
forward train acc: top1 ->  99.806 ; top5 ->  100.0  and loss:  0.5713297014590353
test acc: top1 ->  91.64 ; top5 ->  99.21  and loss:  55.79797638952732
forward train acc: top1 ->  99.816 ; top5 ->  100.0  and loss:  0.5886033608112484
test acc: top1 ->  91.7 ; top5 ->  99.2  and loss:  56.93800852447748
forward train acc: top1 ->  99.834 ; top5 ->  100.0  and loss:  0.482827112195082
test acc: top1 ->  91.68 ; top5 ->  99.3  and loss:  58.01782853901386
forward train acc: top1 ->  99.83000000244141 ; top5 ->  100.0  and loss:  0.48990906681865454
test acc: top1 ->  91.82 ; top5 ->  99.26  and loss:  58.60747128725052
forward train acc: top1 ->  99.83999997558594 ; top5 ->  100.0  and loss:  0.459001325070858
test acc: top1 ->  91.71 ; top5 ->  99.28  and loss:  57.85459330677986
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  56 / 64 , inc:  1
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
adv train loss:  -684.8724253755063 , diff:  684.8724253755063
adv train loss:  -2173.3650512695312 , diff:  1488.492625894025
adv train loss:  -2215.0376625061035 , diff:  41.672611236572266
adv train loss:  -2276.9408779144287 , diff:  61.903215408325195
adv train loss:  -2276.549476623535 , diff:  0.3914012908935547
adv train loss:  -2278.7737426757812 , diff:  2.2242660522460938
adv train loss:  -2279.610372543335 , diff:  0.8366298675537109
adv train loss:  -2276.612651824951 , diff:  2.997720718383789
adv train loss:  -2281.56561088562 , diff:  4.952959060668945
adv train loss:  -2275.4466342926025 , diff:  6.118976593017578
************ all values are small in this layer **********
layer  6  adv train finish, try to retain  102
test acc: top1 ->  16.58 ; top5 ->  60.17  and loss:  543796.1875
forward train acc: top1 ->  99.85399997558594 ; top5 ->  100.0  and loss:  0.4562375177629292
test acc: top1 ->  91.66 ; top5 ->  99.32  and loss:  61.56285126507282
forward train acc: top1 ->  99.88799997558594 ; top5 ->  100.0  and loss:  0.32946184929460287
test acc: top1 ->  91.98 ; top5 ->  99.31  and loss:  62.19804249703884
forward train acc: top1 ->  99.924 ; top5 ->  100.0  and loss:  0.20008577601402067
test acc: top1 ->  92.0 ; top5 ->  99.34  and loss:  62.27214029431343
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.21805355255492032
test acc: top1 ->  91.88 ; top5 ->  99.27  and loss:  63.377585768699646
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.17860625439789146
test acc: top1 ->  91.93 ; top5 ->  99.28  and loss:  66.35758110880852
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.16039863764308393
test acc: top1 ->  92.25 ; top5 ->  99.32  and loss:  64.55481597781181
==> this epoch:  102 / 256
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
adv train loss:  -5.964299097657204 , diff:  5.964299097657204
adv train loss:  -5.910905046388507 , diff:  0.053394051268696785
adv train loss:  -6.059707572683692 , diff:  0.1488025262951851
adv train loss:  -6.0479140263050795 , diff:  0.011793546378612518
adv train loss:  -5.782536005601287 , diff:  0.26537802070379257
adv train loss:  -5.429475070908666 , diff:  0.35306093469262123
adv train loss:  -6.078948221169412 , diff:  0.6494731502607465
adv train loss:  -6.12166385166347 , diff:  0.04271563049405813
adv train loss:  -6.1425741892308 , diff:  0.020910337567329407
adv train loss:  -5.785270527005196 , diff:  0.35730366222560406
layer  8  adv train finish, try to retain  415
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -0.3083667878527194 , diff:  0.3083667878527194
adv train loss:  -0.3303864930057898 , diff:  0.02201970515307039
adv train loss:  -0.391315420740284 , diff:  0.06092892773449421
adv train loss:  -0.37641561147756875 , diff:  0.01489980926271528
adv train loss:  -0.2824822966940701 , diff:  0.09393331478349864
adv train loss:  -0.3767054257914424 , diff:  0.0942231290973723
adv train loss:  -0.31277398974634707 , diff:  0.06393143604509532
adv train loss:  -0.3161284702946432 , diff:  0.0033544805482961237
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  95
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  4539.719326019287
forward train acc: top1 ->  81.92800001220704 ; top5 ->  98.714  and loss:  68.11161328852177
test acc: top1 ->  63.07 ; top5 ->  97.57  and loss:  121.94836962223053
forward train acc: top1 ->  97.32799998535157 ; top5 ->  99.978  and loss:  11.491904258728027
test acc: top1 ->  87.7 ; top5 ->  98.17  and loss:  62.49619525671005
forward train acc: top1 ->  98.42399997558594 ; top5 ->  99.988  and loss:  6.21356537938118
test acc: top1 ->  87.76 ; top5 ->  98.25  and loss:  64.95359110832214
forward train acc: top1 ->  98.946 ; top5 ->  99.98  and loss:  3.997324973344803
test acc: top1 ->  89.5 ; top5 ->  98.54  and loss:  62.01792913675308
forward train acc: top1 ->  99.2480000024414 ; top5 ->  99.996  and loss:  2.6551903560757637
test acc: top1 ->  89.76 ; top5 ->  98.62  and loss:  65.0904700756073
forward train acc: top1 ->  99.41400000488281 ; top5 ->  99.998  and loss:  2.0811954252421856
test acc: top1 ->  90.03 ; top5 ->  98.7  and loss:  64.16170074045658
forward train acc: top1 ->  99.53 ; top5 ->  99.998  and loss:  1.6353547452017665
test acc: top1 ->  90.29 ; top5 ->  98.8  and loss:  62.92646588385105
forward train acc: top1 ->  99.5760000024414 ; top5 ->  99.996  and loss:  1.4147899597883224
test acc: top1 ->  90.39 ; top5 ->  98.84  and loss:  64.95939642190933
forward train acc: top1 ->  99.62800000244141 ; top5 ->  99.996  and loss:  1.2031743852421641
test acc: top1 ->  90.81 ; top5 ->  98.87  and loss:  64.83020648360252
forward train acc: top1 ->  99.71399997558593 ; top5 ->  100.0  and loss:  1.0203064968809485
test acc: top1 ->  90.93 ; top5 ->  98.97  and loss:  63.067952647805214
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  96 / 512 , inc:  1
---------------- start layer  10  ---------------
### skip layer  10 wait:  4  ###
---------------- start layer  11  ---------------
### skip layer  11 wait:  3  ###
---------------- start layer  12  ---------------
adv train loss:  -246.68140482902527 , diff:  246.68140482902527
adv train loss:  -246.43751657009125 , diff:  0.243888258934021
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  40
test acc: top1 ->  10.0 ; top5 ->  69.09  and loss:  3694.442451477051
forward train acc: top1 ->  97.34599997558594 ; top5 ->  99.76  and loss:  12.082245234400034
test acc: top1 ->  90.42 ; top5 ->  98.55  and loss:  54.91832911968231
forward train acc: top1 ->  99.6800000024414 ; top5 ->  100.0  and loss:  1.6623354274779558
test acc: top1 ->  90.63 ; top5 ->  98.48  and loss:  56.732141345739365
forward train acc: top1 ->  99.788 ; top5 ->  100.0  and loss:  1.1011020075529814
test acc: top1 ->  90.71 ; top5 ->  98.44  and loss:  58.072010055184364
forward train acc: top1 ->  99.85199997802735 ; top5 ->  100.0  and loss:  0.798515759408474
test acc: top1 ->  90.81 ; top5 ->  98.42  and loss:  60.71259665489197
forward train acc: top1 ->  99.8400000024414 ; top5 ->  100.0  and loss:  0.707988764392212
test acc: top1 ->  90.86 ; top5 ->  98.34  and loss:  62.57556463778019
forward train acc: top1 ->  99.822 ; top5 ->  99.994  and loss:  0.758692008908838
test acc: top1 ->  90.95 ; top5 ->  98.33  and loss:  62.5420952886343
forward train acc: top1 ->  99.87399997558593 ; top5 ->  100.0  and loss:  0.6115820066770539
test acc: top1 ->  90.91 ; top5 ->  98.31  and loss:  63.108188301324844
forward train acc: top1 ->  99.862 ; top5 ->  100.0  and loss:  0.6147797516314313
test acc: top1 ->  90.97 ; top5 ->  98.28  and loss:  63.472511902451515
forward train acc: top1 ->  99.85999997558594 ; top5 ->  99.998  and loss:  0.6093708007829264
test acc: top1 ->  91.05 ; top5 ->  98.33  and loss:  64.52388443052769
forward train acc: top1 ->  99.868 ; top5 ->  100.0  and loss:  0.5515868676593527
test acc: top1 ->  91.03 ; top5 ->  98.28  and loss:  64.53212703764439
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  41 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -15006.01155090332 , diff:  15006.01155090332
adv train loss:  -24516.511932373047 , diff:  9510.500381469727
adv train loss:  -33695.8317565918 , diff:  9179.31982421875
adv train loss:  -42796.18081665039 , diff:  9100.349060058594
adv train loss:  -51847.76647949219 , diff:  9051.585662841797
adv train loss:  -60892.51306152344 , diff:  9044.74658203125
adv train loss:  -69946.20129394531 , diff:  9053.688232421875
adv train loss:  -79080.89392089844 , diff:  9134.692626953125
adv train loss:  -88291.95684814453 , diff:  9211.062927246094
adv train loss:  -97434.79705810547 , diff:  9142.840209960938
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  22
test acc: top1 ->  64.98 ; top5 ->  91.01  and loss:  333.39507031440735
forward train acc: top1 ->  98.49 ; top5 ->  99.712  and loss:  7.712859321851283
test acc: top1 ->  91.62 ; top5 ->  98.43  and loss:  82.81606687605381
forward train acc: top1 ->  99.928 ; top5 ->  100.0  and loss:  0.265938920492772
test acc: top1 ->  91.73 ; top5 ->  98.55  and loss:  81.1921218931675
forward train acc: top1 ->  99.944 ; top5 ->  99.998  and loss:  0.2247239596908912
test acc: top1 ->  91.77 ; top5 ->  98.62  and loss:  81.19545312225819
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.1997145168716088
test acc: top1 ->  91.94 ; top5 ->  98.64  and loss:  81.49667482823133
forward train acc: top1 ->  99.93399997558593 ; top5 ->  100.0  and loss:  0.1905368072912097
test acc: top1 ->  91.83 ; top5 ->  98.58  and loss:  82.04521889984608
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.21588907163823023
test acc: top1 ->  91.97 ; top5 ->  98.73  and loss:  81.13396994024515
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.13309999974444509
test acc: top1 ->  91.85 ; top5 ->  98.65  and loss:  81.46562677621841
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.14032144495286047
test acc: top1 ->  91.86 ; top5 ->  98.68  and loss:  81.56120780110359
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.14747957768850029
test acc: top1 ->  91.87 ; top5 ->  98.73  and loss:  81.27608577162027
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.1278700929833576
test acc: top1 ->  91.85 ; top5 ->  98.62  and loss:  81.99801556020975
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  23 / 512 , inc:  1
layer  0  :  0.484375  ==>  31 / 64 , inc:  1
layer  1  :  0.875  ==>  56 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.3984375  ==>  102 / 256 , inc:  4
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.041015625  ==>  21 / 512 , inc:  5
layer  9  :  0.1875  ==>  96 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.009765625  ==>  5 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.044921875  ==>  23 / 512 , inc:  1
eps [0.8757877807617187, 0.041052552223205564, 0.8757877807617187, 0.8757877807617187, 0.8757877807617187, 0.8757877807617187, 1.167717041015625, 0.8757877807617187, 0.01824557876586914, 0.041052552223205564, 0.8757877807617187, 1.167717041015625, 0.041052552223205564, 14.0126044921875]  wait [4, 2, 3, 3, 3, 3, 0, 3, 0, 2, 3, 2, 2, 4]  inc [1, 1, 1, 1, 1, 1, 4, 1, 5, 1, 1, 1, 1, 1]  tol: 5
$$$$$$$$$$$$$ epoch  47  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
adv train loss:  -12.500717729330063 , diff:  12.500717729330063
adv train loss:  -12.453850727528334 , diff:  0.0468670018017292
adv train loss:  -11.92650854960084 , diff:  0.527342177927494
adv train loss:  -12.325539492070675 , diff:  0.3990309424698353
adv train loss:  -13.056436473503709 , diff:  0.7308969814330339
adv train loss:  -12.49176074936986 , diff:  0.5646757241338491
adv train loss:  -12.98788121342659 , diff:  0.49612046405673027
adv train loss:  -12.091872043907642 , diff:  0.8960091695189476
adv train loss:  -12.125109266489744 , diff:  0.03323722258210182
adv train loss:  -12.576462656259537 , diff:  0.45135338976979256
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
adv train loss:  -858.1846093125641 , diff:  858.1846093125641
adv train loss:  -3470.629684448242 , diff:  2612.445075135678
adv train loss:  -3768.6234169006348 , diff:  297.9937324523926
adv train loss:  -3840.6467208862305 , diff:  72.0233039855957
adv train loss:  -3877.8298721313477 , diff:  37.18315124511719
adv train loss:  -3874.8661613464355 , diff:  2.9637107849121094
adv train loss:  -3875.095241546631 , diff:  0.2290802001953125
adv train loss:  -3875.992343902588 , diff:  0.8971023559570312
adv train loss:  -3921.661121368408 , diff:  45.66877746582031
adv train loss:  -3920.3031158447266 , diff:  1.3580055236816406
************ all values are small in this layer **********
layer  6  adv train finish, try to retain  98
test acc: top1 ->  13.47 ; top5 ->  50.1  and loss:  1409978.53515625
forward train acc: top1 ->  99.684 ; top5 ->  99.998  and loss:  1.1980437700403854
test acc: top1 ->  91.92 ; top5 ->  99.1  and loss:  90.12589193880558
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.15697509972960688
test acc: top1 ->  91.85 ; top5 ->  99.23  and loss:  88.64662677049637
forward train acc: top1 ->  99.89999997558594 ; top5 ->  100.0  and loss:  0.24695572443306446
test acc: top1 ->  91.9 ; top5 ->  99.21  and loss:  85.85988815873861
forward train acc: top1 ->  99.94799997558594 ; top5 ->  100.0  and loss:  0.15800022333860397
test acc: top1 ->  91.92 ; top5 ->  99.33  and loss:  84.75352425128222
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.22161142621189356
test acc: top1 ->  91.98 ; top5 ->  99.25  and loss:  86.99543365836143
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.1631740242082742
test acc: top1 ->  92.0 ; top5 ->  99.26  and loss:  84.91000283509493
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.2283607154568017
test acc: top1 ->  92.0 ; top5 ->  99.26  and loss:  83.44150231033564
forward train acc: top1 ->  99.942 ; top5 ->  100.0  and loss:  0.18684269070945447
test acc: top1 ->  92.05 ; top5 ->  99.27  and loss:  81.98632916808128
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.17361032048938796
test acc: top1 ->  92.13 ; top5 ->  99.29  and loss:  82.04878254234791
==> this epoch:  98 / 256
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
adv train loss:  -11.367738917469978 , diff:  11.367738917469978
adv train loss:  -11.551868194714189 , diff:  0.18412927724421024
adv train loss:  -11.391906788572669 , diff:  0.15996140614151955
adv train loss:  -10.62241099216044 , diff:  0.7694957964122295
adv train loss:  -11.119733016937971 , diff:  0.4973220247775316
adv train loss:  -11.252776768058538 , diff:  0.13304375112056732
adv train loss:  -11.139423467218876 , diff:  0.11335330083966255
adv train loss:  -10.93691873922944 , diff:  0.2025047279894352
adv train loss:  -11.382424721494317 , diff:  0.44550598226487637
adv train loss:  -11.218230180442333 , diff:  0.16419454105198383
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  16
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  1921.7136840820312
forward train acc: top1 ->  98.3400000024414 ; top5 ->  99.996  and loss:  6.842360025271773
test acc: top1 ->  90.91 ; top5 ->  99.03  and loss:  71.54730975627899
forward train acc: top1 ->  99.64399997558594 ; top5 ->  100.0  and loss:  1.0654415891040117
test acc: top1 ->  91.16 ; top5 ->  99.08  and loss:  71.79665765166283
forward train acc: top1 ->  99.72999997558594 ; top5 ->  100.0  and loss:  0.7559175146743655
test acc: top1 ->  91.28 ; top5 ->  99.08  and loss:  70.69968190789223
forward train acc: top1 ->  99.806 ; top5 ->  100.0  and loss:  0.5957257063710131
test acc: top1 ->  91.36 ; top5 ->  99.11  and loss:  71.46223451197147
forward train acc: top1 ->  99.836 ; top5 ->  100.0  and loss:  0.5154936404142063
test acc: top1 ->  91.26 ; top5 ->  99.1  and loss:  72.03451803326607
forward train acc: top1 ->  99.836 ; top5 ->  100.0  and loss:  0.5456652134889737
test acc: top1 ->  91.39 ; top5 ->  99.09  and loss:  72.80743296444416
forward train acc: top1 ->  99.79799997558594 ; top5 ->  100.0  and loss:  0.5820562094449997
test acc: top1 ->  91.39 ; top5 ->  99.12  and loss:  72.25474969297647
forward train acc: top1 ->  99.836 ; top5 ->  100.0  and loss:  0.4682919030310586
test acc: top1 ->  91.44 ; top5 ->  99.14  and loss:  72.5111350864172
forward train acc: top1 ->  99.85 ; top5 ->  100.0  and loss:  0.3913321194704622
test acc: top1 ->  91.54 ; top5 ->  99.18  and loss:  71.63641886413097
forward train acc: top1 ->  99.82399997558593 ; top5 ->  100.0  and loss:  0.4925888436846435
test acc: top1 ->  91.66 ; top5 ->  99.15  and loss:  70.65045629441738
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  21 / 512 , inc:  5
---------------- start layer  9  ---------------
adv train loss:  -5.423844428732991 , diff:  5.423844428732991
adv train loss:  -5.370019912719727 , diff:  0.053824516013264656
adv train loss:  -5.463937856256962 , diff:  0.09391794353723526
adv train loss:  -5.4793222695589066 , diff:  0.015384413301944733
adv train loss:  -5.174433656036854 , diff:  0.30488861352205276
adv train loss:  -5.200730804353952 , diff:  0.026297148317098618
adv train loss:  -5.51950289029628 , diff:  0.318772085942328
adv train loss:  -5.101215776056051 , diff:  0.41828711424022913
adv train loss:  -5.539920814335346 , diff:  0.43870503827929497
adv train loss:  -5.25364778842777 , diff:  0.2862730259075761
layer  9  adv train finish, try to retain  433
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
### skip layer  10 wait:  3  ###
---------------- start layer  11  ---------------
adv train loss:  -2599.430181503296 , diff:  2599.430181503296
adv train loss:  -2740.0324745178223 , diff:  140.60229301452637
adv train loss:  -3078.6639251708984 , diff:  338.6314506530762
adv train loss:  -3238.0931186676025 , diff:  159.4291934967041
adv train loss:  -3238.660318374634 , diff:  0.56719970703125
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  4
test acc: top1 ->  10.0 ; top5 ->  58.94  and loss:  20814.387237548828
forward train acc: top1 ->  97.818 ; top5 ->  99.992  and loss:  9.239015284460038
test acc: top1 ->  91.5 ; top5 ->  99.04  and loss:  69.89991964399815
forward train acc: top1 ->  99.92000000244141 ; top5 ->  100.0  and loss:  0.3731791079044342
test acc: top1 ->  91.98 ; top5 ->  99.1  and loss:  66.6614299044013
forward train acc: top1 ->  99.886 ; top5 ->  100.0  and loss:  0.36895089322933927
test acc: top1 ->  92.0 ; top5 ->  99.08  and loss:  66.45063057541847
forward train acc: top1 ->  99.942 ; top5 ->  100.0  and loss:  0.23984338351874612
test acc: top1 ->  92.2 ; top5 ->  99.11  and loss:  66.51811845600605
==> this epoch:  4 / 512
---------------- start layer  12  ---------------
adv train loss:  -122.13306105136871 , diff:  122.13306105136871
adv train loss:  -120.7992063164711 , diff:  1.3338547348976135
adv train loss:  -122.5645762681961 , diff:  1.765369951725006
adv train loss:  -121.80939942598343 , diff:  0.755176842212677
adv train loss:  -121.8486270904541 , diff:  0.03922766447067261
layer  12  adv train finish, try to retain  489
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
### skip layer  13 wait:  4  ###
layer  0  :  0.484375  ==>  31 / 64 , inc:  1
layer  1  :  0.875  ==>  56 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.3828125  ==>  98 / 256 , inc:  8
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.041015625  ==>  21 / 512 , inc:  2
layer  9  :  0.1875  ==>  96 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.0078125  ==>  4 / 512 , inc:  2
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.044921875  ==>  23 / 512 , inc:  1
eps [0.8757877807617187, 0.08210510444641113, 0.8757877807617187, 0.8757877807617187, 0.8757877807617187, 0.8757877807617187, 1.167717041015625, 0.8757877807617187, 0.013684184074401855, 0.08210510444641113, 0.8757877807617187, 1.167717041015625, 0.08210510444641113, 14.0126044921875]  wait [3, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 3]  inc [1, 1, 1, 1, 1, 1, 8, 1, 2, 1, 1, 2, 1, 1]  tol: 5
$$$$$$$$$$$$$ epoch  48  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
adv train loss:  -0.193672897759825 , diff:  0.193672897759825
adv train loss:  -0.20827468379866332 , diff:  0.014601786038838327
adv train loss:  -0.20449156046379358 , diff:  0.0037831233348697424
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -0.1807685623934958 , diff:  0.1807685623934958
adv train loss:  -0.18213998872670345 , diff:  0.001371426333207637
layer  2  adv train finish, try to retain  125
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -0.21346554346382618 , diff:  0.21346554346382618
adv train loss:  -0.24355784262297675 , diff:  0.03009229915915057
adv train loss:  -0.1646400522440672 , diff:  0.07891779037890956
adv train loss:  -0.2272671782411635 , diff:  0.0626271259970963
adv train loss:  -0.22261922946199775 , diff:  0.004647948779165745
layer  3  adv train finish, try to retain  127
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -0.1736658294685185 , diff:  0.1736658294685185
adv train loss:  -0.20422102441079915 , diff:  0.03055519494228065
adv train loss:  -0.2265286308247596 , diff:  0.022307606413960457
adv train loss:  -0.19071121071465313 , diff:  0.03581742011010647
adv train loss:  -0.24936475325375795 , diff:  0.05865354253910482
adv train loss:  -0.19697250897297636 , diff:  0.0523922442807816
adv train loss:  -0.20327863888815045 , diff:  0.006306129915174097
layer  4  adv train finish, try to retain  243
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -0.21493642835412174 , diff:  0.21493642835412174
adv train loss:  -0.1956139812245965 , diff:  0.019322447129525244
adv train loss:  -0.1855794982984662 , diff:  0.010034482926130295
adv train loss:  -0.22126082074828446 , diff:  0.035681322449818254
adv train loss:  -0.27967581897974014 , diff:  0.058414998231455684
adv train loss:  -0.20475692808395252 , diff:  0.07491889089578763
adv train loss:  -0.2290246170014143 , diff:  0.024267688917461783
adv train loss:  -0.2038901501800865 , diff:  0.025134466821327806
adv train loss:  -0.18288847734220326 , diff:  0.021001672837883234
adv train loss:  -0.24248078552773222 , diff:  0.059592308185528964
layer  5  adv train finish, try to retain  255
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -879.2777426671237 , diff:  879.2777426671237
adv train loss:  -2871.27059173584 , diff:  1991.9928490687162
adv train loss:  -2910.0483074188232 , diff:  38.7777156829834
adv train loss:  -2951.4666843414307 , diff:  41.41837692260742
adv train loss:  -2991.130464553833 , diff:  39.663780212402344
adv train loss:  -2990.360273361206 , diff:  0.7701911926269531
adv train loss:  -2991.3399238586426 , diff:  0.9796504974365234
adv train loss:  -2989.484386444092 , diff:  1.8555374145507812
adv train loss:  -2989.84051322937 , diff:  0.3561267852783203
adv train loss:  -2989.5819816589355 , diff:  0.2585315704345703
************ all values are small in this layer **********
layer  6  adv train finish, try to retain  90
test acc: top1 ->  10.0 ; top5 ->  42.96  and loss:  1090152.142578125
forward train acc: top1 ->  99.84800000244141 ; top5 ->  100.0  and loss:  0.41735005751252174
test acc: top1 ->  91.41 ; top5 ->  99.02  and loss:  72.7730700224638
forward train acc: top1 ->  99.91 ; top5 ->  100.0  and loss:  0.2744295530428644
test acc: top1 ->  91.76 ; top5 ->  99.11  and loss:  73.47821691632271
forward train acc: top1 ->  99.88599997558593 ; top5 ->  100.0  and loss:  0.31381127797067165
test acc: top1 ->  91.89 ; top5 ->  99.15  and loss:  74.58398862183094
forward train acc: top1 ->  99.902 ; top5 ->  100.0  and loss:  0.294586097355932
test acc: top1 ->  91.99 ; top5 ->  99.2  and loss:  71.56514409184456
forward train acc: top1 ->  99.898 ; top5 ->  100.0  and loss:  0.318392696302908
test acc: top1 ->  91.91 ; top5 ->  99.07  and loss:  72.86474321782589
forward train acc: top1 ->  99.924 ; top5 ->  100.0  and loss:  0.2340529792709276
test acc: top1 ->  91.92 ; top5 ->  99.14  and loss:  70.25701439380646
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.18201148395019118
test acc: top1 ->  91.98 ; top5 ->  99.19  and loss:  69.57601126283407
forward train acc: top1 ->  99.928 ; top5 ->  100.0  and loss:  0.2488066399237141
test acc: top1 ->  92.01 ; top5 ->  99.19  and loss:  70.39304801821709
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.23938979918602854
test acc: top1 ->  91.87 ; top5 ->  99.27  and loss:  71.24398116767406
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.17764567816630006
test acc: top1 ->  91.89 ; top5 ->  99.21  and loss:  72.56754802167416
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  98 / 256 , inc:  8
---------------- start layer  7  ---------------
adv train loss:  -0.8710763426497579 , diff:  0.8710763426497579
adv train loss:  -0.8335519182728603 , diff:  0.037524424376897514
adv train loss:  -0.8651415934509714 , diff:  0.031589675178111065
adv train loss:  -0.9329483327455819 , diff:  0.06780673929461045
adv train loss:  -0.733382125152275 , diff:  0.1995662075933069
adv train loss:  -0.8911805865354836 , diff:  0.15779846138320863
adv train loss:  -0.8204574552364647 , diff:  0.07072313129901886
adv train loss:  -0.8241593725979328 , diff:  0.0037019173614680767
layer  7  adv train finish, try to retain  405
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -5.85874991491437 , diff:  5.85874991491437
adv train loss:  -5.801673410460353 , diff:  0.057076504454016685
adv train loss:  -6.047792971134186 , diff:  0.2461195606738329
adv train loss:  -6.06569348461926 , diff:  0.017900513485074043
adv train loss:  -5.980263931676745 , diff:  0.08542955294251442
adv train loss:  -6.0805992893874645 , diff:  0.10033535771071911
adv train loss:  -6.049208441749215 , diff:  0.0313908476382494
adv train loss:  -5.786023557186127 , diff:  0.2631848845630884
adv train loss:  -6.031670296564698 , diff:  0.2456467393785715
adv train loss:  -5.919467715546489 , diff:  0.11220258101820946
layer  8  adv train finish, try to retain  402
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -2.957374468445778 , diff:  2.957374468445778
adv train loss:  -3.0805297940969467 , diff:  0.12315532565116882
adv train loss:  -2.8892769664525986 , diff:  0.19125282764434814
adv train loss:  -2.799609853886068 , diff:  0.0896671125665307
adv train loss:  -2.95910607278347 , diff:  0.1594962188974023
adv train loss:  -2.7744079418480396 , diff:  0.18469813093543053
adv train loss:  -2.6318284813314676 , diff:  0.142579460516572
adv train loss:  -2.8847211562097073 , diff:  0.25289267487823963
adv train loss:  -2.907850766554475 , diff:  0.02312961034476757
adv train loss:  -2.8982865903526545 , diff:  0.009564176201820374
layer  9  adv train finish, try to retain  421
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -109.92416471242905 , diff:  109.92416471242905
adv train loss:  -110.0377527475357 , diff:  0.11358803510665894
layer  10  adv train finish, try to retain  475
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -3288.193338394165 , diff:  3288.193338394165
adv train loss:  -3806.3516006469727 , diff:  518.1582622528076
adv train loss:  -4189.5490798950195 , diff:  383.1974792480469
adv train loss:  -4192.156322479248 , diff:  2.6072425842285156
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  2
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  10402.760238647461
forward train acc: top1 ->  86.32800000488281 ; top5 ->  99.658  and loss:  59.3672570027411
test acc: top1 ->  76.65 ; top5 ->  96.82  and loss:  96.87711268663406
forward train acc: top1 ->  99.37799997558594 ; top5 ->  99.99  and loss:  3.8771805465221405
test acc: top1 ->  90.45 ; top5 ->  97.6  and loss:  60.73333516716957
forward train acc: top1 ->  99.6480000024414 ; top5 ->  99.994  and loss:  1.9164125183597207
test acc: top1 ->  90.89 ; top5 ->  97.72  and loss:  60.832719549536705
forward train acc: top1 ->  99.76 ; top5 ->  100.0  and loss:  1.1678371601738036
test acc: top1 ->  91.08 ; top5 ->  97.74  and loss:  61.59431204199791
forward train acc: top1 ->  99.81999997558594 ; top5 ->  100.0  and loss:  0.7425582576543093
test acc: top1 ->  91.4 ; top5 ->  97.9  and loss:  62.916884154081345
forward train acc: top1 ->  99.868 ; top5 ->  99.998  and loss:  0.6518896846100688
test acc: top1 ->  91.4 ; top5 ->  97.94  and loss:  62.89580588787794
forward train acc: top1 ->  99.8740000024414 ; top5 ->  100.0  and loss:  0.49997131433337927
test acc: top1 ->  91.45 ; top5 ->  97.98  and loss:  63.58532045036554
forward train acc: top1 ->  99.90399997558593 ; top5 ->  100.0  and loss:  0.4399977186694741
test acc: top1 ->  91.51 ; top5 ->  97.99  and loss:  64.09721219539642
forward train acc: top1 ->  99.88199997558594 ; top5 ->  99.998  and loss:  0.48626597691327333
test acc: top1 ->  91.5 ; top5 ->  97.98  and loss:  64.03227289766073
forward train acc: top1 ->  99.92000000244141 ; top5 ->  100.0  and loss:  0.34476782381534576
test acc: top1 ->  91.61 ; top5 ->  98.09  and loss:  65.18593361973763
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  4 / 512 , inc:  2
---------------- start layer  12  ---------------
adv train loss:  -317.11969113349915 , diff:  317.11969113349915
adv train loss:  -317.4562666416168 , diff:  0.3365755081176758
layer  12  adv train finish, try to retain  493
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
### skip layer  13 wait:  3  ###
layer  0  :  0.484375  ==>  31 / 64 , inc:  1
layer  1  :  0.875  ==>  56 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.3828125  ==>  98 / 256 , inc:  4
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.041015625  ==>  21 / 512 , inc:  2
layer  9  :  0.1875  ==>  96 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.0078125  ==>  4 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.044921875  ==>  23 / 512 , inc:  1
eps [0.8757877807617187, 0.16421020889282226, 1.7515755615234374, 1.7515755615234374, 1.7515755615234374, 1.7515755615234374, 0.8757877807617187, 1.7515755615234374, 0.02736836814880371, 0.16421020889282226, 1.7515755615234374, 0.8757877807617187, 0.16421020889282226, 14.0126044921875]  wait [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]  inc [1, 1, 1, 1, 1, 1, 4, 1, 2, 1, 1, 1, 1, 1]  tol: 5
$$$$$$$$$$$$$ epoch  49  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -32.52233423292637 , diff:  32.52233423292637
adv train loss:  -32.320210710167885 , diff:  0.2021235227584839
adv train loss:  -32.754528760910034 , diff:  0.43431805074214935
adv train loss:  -31.272227823734283 , diff:  1.4823009371757507
adv train loss:  -32.64114336669445 , diff:  1.368915542960167
adv train loss:  -33.19365668296814 , diff:  0.5525133162736893
adv train loss:  -32.914323672652245 , diff:  0.2793330103158951
adv train loss:  -32.668748915195465 , diff:  0.24557475745677948
adv train loss:  -32.845016211271286 , diff:  0.17626729607582092
adv train loss:  -32.27486561238766 , diff:  0.5701505988836288
layer  0  adv train finish, try to retain  57
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -20.012727364897728 , diff:  20.012727364897728
adv train loss:  -20.594918616116047 , diff:  0.5821912512183189
adv train loss:  -20.217304199934006 , diff:  0.37761441618204117
adv train loss:  -20.211361691355705 , diff:  0.005942508578300476
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  55
test acc: top1 ->  18.24 ; top5 ->  57.95  and loss:  1225.3745212554932
forward train acc: top1 ->  99.468 ; top5 ->  99.996  and loss:  2.08386994455941
test acc: top1 ->  91.46 ; top5 ->  98.97  and loss:  76.56188115477562
forward train acc: top1 ->  99.72399997558594 ; top5 ->  100.0  and loss:  0.8027088223025203
test acc: top1 ->  91.46 ; top5 ->  99.11  and loss:  72.88728232681751
forward train acc: top1 ->  99.76199997558594 ; top5 ->  99.998  and loss:  0.7411027140915394
test acc: top1 ->  91.64 ; top5 ->  99.07  and loss:  73.46318306773901
forward train acc: top1 ->  99.778 ; top5 ->  100.0  and loss:  0.6879959284560755
test acc: top1 ->  91.5 ; top5 ->  99.16  and loss:  70.94284717738628
forward train acc: top1 ->  99.792 ; top5 ->  99.998  and loss:  0.5983529696241021
test acc: top1 ->  91.68 ; top5 ->  99.11  and loss:  69.35793013125658
forward train acc: top1 ->  99.83399997558594 ; top5 ->  100.0  and loss:  0.4690671842545271
test acc: top1 ->  91.74 ; top5 ->  99.07  and loss:  68.86990350484848
forward train acc: top1 ->  99.82599997558594 ; top5 ->  99.998  and loss:  0.48672779090702534
test acc: top1 ->  91.64 ; top5 ->  99.16  and loss:  69.30182581394911
forward train acc: top1 ->  99.87 ; top5 ->  100.0  and loss:  0.4706502457847819
test acc: top1 ->  91.72 ; top5 ->  99.14  and loss:  68.75181423872709
forward train acc: top1 ->  99.8280000024414 ; top5 ->  100.0  and loss:  0.4781839232891798
test acc: top1 ->  91.69 ; top5 ->  99.07  and loss:  67.96758339554071
forward train acc: top1 ->  99.85 ; top5 ->  100.0  and loss:  0.4126182681939099
test acc: top1 ->  91.75 ; top5 ->  99.18  and loss:  67.42301707714796
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  56 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -962.9248089343309 , diff:  962.9248089343309
adv train loss:  -1599.6067991256714 , diff:  636.6819901913404
adv train loss:  -1605.8891296386719 , diff:  6.282330513000488
adv train loss:  -1604.7703008651733 , diff:  1.1188287734985352
adv train loss:  -1606.8142080307007 , diff:  2.0439071655273438
adv train loss:  -1604.0420503616333 , diff:  2.772157669067383
adv train loss:  -1597.6440496444702 , diff:  6.398000717163086
adv train loss:  -1596.5431108474731 , diff:  1.1009387969970703
adv train loss:  -1606.4776372909546 , diff:  9.934526443481445
adv train loss:  -1596.352068901062 , diff:  10.125568389892578
layer  2  adv train finish, try to retain  46
test acc: top1 ->  12.81 ; top5 ->  49.31  and loss:  1491.971420288086
forward train acc: top1 ->  89.87999998291015 ; top5 ->  99.35599997558593  and loss:  39.783223539590836
test acc: top1 ->  83.93 ; top5 ->  97.93  and loss:  60.84506966173649
forward train acc: top1 ->  91.79800000732422 ; top5 ->  99.59799998046876  and loss:  25.010462924838066
test acc: top1 ->  85.4 ; top5 ->  98.46  and loss:  53.73800025880337
forward train acc: top1 ->  92.916 ; top5 ->  99.69199997558594  and loss:  21.598455131053925
test acc: top1 ->  86.12 ; top5 ->  98.8  and loss:  50.59906721115112
forward train acc: top1 ->  93.77200001953125 ; top5 ->  99.76599997558594  and loss:  18.835404694080353
test acc: top1 ->  86.74 ; top5 ->  98.76  and loss:  49.73731428384781
forward train acc: top1 ->  94.30999998779296 ; top5 ->  99.802  and loss:  16.987081356346607
test acc: top1 ->  87.32 ; top5 ->  98.81  and loss:  48.22732426226139
forward train acc: top1 ->  94.65199999755859 ; top5 ->  99.80400000244141  and loss:  15.741682156920433
test acc: top1 ->  87.6 ; top5 ->  98.87  and loss:  47.5068815946579
forward train acc: top1 ->  95.00799998779297 ; top5 ->  99.84  and loss:  14.91883685439825
test acc: top1 ->  87.85 ; top5 ->  98.86  and loss:  46.59852050244808
forward train acc: top1 ->  95.13599998779297 ; top5 ->  99.884  and loss:  14.473882600665092
test acc: top1 ->  87.95 ; top5 ->  98.94  and loss:  46.85056284070015
forward train acc: top1 ->  95.22999998779297 ; top5 ->  99.86  and loss:  14.063615471124649
test acc: top1 ->  87.99 ; top5 ->  99.0  and loss:  45.800536543130875
forward train acc: top1 ->  95.63800000976562 ; top5 ->  99.892  and loss:  12.98649599403143
test acc: top1 ->  88.21 ; top5 ->  98.93  and loss:  46.33561000227928
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -425.3812921009958 , diff:  425.3812921009958
adv train loss:  -760.36700963974 , diff:  334.9857175387442
adv train loss:  -783.2789988517761 , diff:  22.911989212036133
adv train loss:  -780.9694361686707 , diff:  2.3095626831054688
adv train loss:  -785.3738393783569 , diff:  4.404403209686279
adv train loss:  -791.3025469779968 , diff:  5.928707599639893
adv train loss:  -795.2141003608704 , diff:  3.911553382873535
adv train loss:  -801.3267102241516 , diff:  6.11260986328125
adv train loss:  -809.2517728805542 , diff:  7.925062656402588
adv train loss:  -807.3800234794617 , diff:  1.8717494010925293
layer  3  adv train finish, try to retain  30
test acc: top1 ->  10.0 ; top5 ->  53.72  and loss:  530.7797899246216
forward train acc: top1 ->  80.70200001464843 ; top5 ->  98.24399998291015  and loss:  59.726975440979004
test acc: top1 ->  78.56 ; top5 ->  97.94  and loss:  67.39929047226906
forward train acc: top1 ->  84.44600001464843 ; top5 ->  98.91800000488281  and loss:  46.289396077394485
test acc: top1 ->  80.59 ; top5 ->  98.37  and loss:  61.30421742796898
forward train acc: top1 ->  86.18599997558594 ; top5 ->  99.17399997558594  and loss:  40.58299779891968
test acc: top1 ->  81.52 ; top5 ->  98.5  and loss:  58.73206585645676
forward train acc: top1 ->  87.34399999511719 ; top5 ->  99.3500000024414  and loss:  37.23872232437134
test acc: top1 ->  82.48 ; top5 ->  98.61  and loss:  55.81549170613289
forward train acc: top1 ->  88.37999999023438 ; top5 ->  99.39999997802734  and loss:  34.09681844711304
test acc: top1 ->  82.98 ; top5 ->  98.71  and loss:  54.399254739284515
forward train acc: top1 ->  89.00600001464844 ; top5 ->  99.4300000024414  and loss:  32.51164701581001
test acc: top1 ->  83.45 ; top5 ->  98.64  and loss:  54.12101247906685
forward train acc: top1 ->  89.058 ; top5 ->  99.5220000024414  and loss:  31.80350813269615
test acc: top1 ->  83.42 ; top5 ->  98.67  and loss:  53.16013726592064
forward train acc: top1 ->  89.35599997802734 ; top5 ->  99.48599997558594  and loss:  31.339964792132378
test acc: top1 ->  83.73 ; top5 ->  98.77  and loss:  52.333368211984634
forward train acc: top1 ->  89.49000001464844 ; top5 ->  99.55599997802734  and loss:  30.529566287994385
test acc: top1 ->  83.84 ; top5 ->  98.84  and loss:  52.20293912291527
forward train acc: top1 ->  89.85399998535156 ; top5 ->  99.56399997802734  and loss:  29.427178129553795
test acc: top1 ->  84.13 ; top5 ->  98.79  and loss:  52.02021682262421
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -113.51014483720064 , diff:  113.51014483720064
adv train loss:  -633.1694540977478 , diff:  519.6593092605472
adv train loss:  -717.0275707244873 , diff:  83.8581166267395
adv train loss:  -723.8143210411072 , diff:  6.786750316619873
adv train loss:  -720.2142128944397 , diff:  3.6001081466674805
adv train loss:  -756.7270865440369 , diff:  36.51287364959717
adv train loss:  -780.3381190299988 , diff:  23.611032485961914
adv train loss:  -784.2770128250122 , diff:  3.9388937950134277
adv train loss:  -785.8149228096008 , diff:  1.537909984588623
adv train loss:  -790.1818342208862 , diff:  4.3669114112854
layer  4  adv train finish, try to retain  31
test acc: top1 ->  16.02 ; top5 ->  55.76  and loss:  806.2357482910156
forward train acc: top1 ->  75.28599997314453 ; top5 ->  97.52000000488282  and loss:  72.41749709844589
test acc: top1 ->  74.52 ; top5 ->  97.5  and loss:  78.3575687110424
forward train acc: top1 ->  79.45599997802735 ; top5 ->  98.28800000732421  and loss:  59.719278216362
test acc: top1 ->  77.74 ; top5 ->  98.0  and loss:  69.20703440904617
forward train acc: top1 ->  81.64200001953125 ; top5 ->  98.65600000488281  and loss:  53.17851656675339
test acc: top1 ->  79.55 ; top5 ->  98.09  and loss:  65.23412188887596
forward train acc: top1 ->  83.23999997802734 ; top5 ->  98.8740000024414  and loss:  49.11040171980858
test acc: top1 ->  80.15 ; top5 ->  98.14  and loss:  63.09055584669113
forward train acc: top1 ->  84.04200000488281 ; top5 ->  98.88000000488282  and loss:  46.5220961868763
test acc: top1 ->  81.23 ; top5 ->  98.37  and loss:  59.62006849050522
forward train acc: top1 ->  84.70600001220703 ; top5 ->  99.11799997558593  and loss:  44.218028753995895
test acc: top1 ->  81.57 ; top5 ->  98.3  and loss:  58.81451591849327
forward train acc: top1 ->  85.26399999511719 ; top5 ->  99.14000000488281  and loss:  42.733487248420715
test acc: top1 ->  81.93 ; top5 ->  98.34  and loss:  59.04258006811142
forward train acc: top1 ->  85.52799999511718 ; top5 ->  99.18800000244141  and loss:  42.27731767296791
test acc: top1 ->  82.08 ; top5 ->  98.37  and loss:  57.55063036084175
forward train acc: top1 ->  85.84399998535156 ; top5 ->  99.17399997802734  and loss:  41.208077639341354
test acc: top1 ->  82.34 ; top5 ->  98.44  and loss:  56.848934292793274
forward train acc: top1 ->  85.88000001220703 ; top5 ->  99.23999997558593  and loss:  40.71712672710419
test acc: top1 ->  82.42 ; top5 ->  98.52  and loss:  56.31790870428085
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  156 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -242.62842667102814 , diff:  242.62842667102814
adv train loss:  -804.4772911071777 , diff:  561.8488644361496
adv train loss:  -894.2413892745972 , diff:  89.76409816741943
adv train loss:  -925.0777444839478 , diff:  30.836355209350586
adv train loss:  -930.2166366577148 , diff:  5.13889217376709
adv train loss:  -939.3377380371094 , diff:  9.121101379394531
adv train loss:  -946.4108638763428 , diff:  7.073125839233398
adv train loss:  -949.1728849411011 , diff:  2.762021064758301
adv train loss:  -948.551685333252 , diff:  0.6211996078491211
adv train loss:  -950.6559438705444 , diff:  2.1042585372924805
layer  5  adv train finish, try to retain  26
test acc: top1 ->  17.54 ; top5 ->  66.3  and loss:  390.3486397266388
forward train acc: top1 ->  82.19599998779297 ; top5 ->  99.12800000244141  and loss:  50.44249787926674
test acc: top1 ->  78.85 ; top5 ->  98.43  and loss:  64.79482677578926
forward train acc: top1 ->  85.74400001953126 ; top5 ->  99.42800000244141  and loss:  40.414446234703064
test acc: top1 ->  81.71 ; top5 ->  98.65  and loss:  57.1338207423687
forward train acc: top1 ->  87.44799999023438 ; top5 ->  99.59199997558594  and loss:  34.96857789158821
test acc: top1 ->  82.73 ; top5 ->  98.86  and loss:  54.841576635837555
forward train acc: top1 ->  88.50599999755859 ; top5 ->  99.62200000244141  and loss:  32.623392671346664
test acc: top1 ->  83.9 ; top5 ->  98.88  and loss:  51.446148574352264
forward train acc: top1 ->  89.18999998779297 ; top5 ->  99.6440000024414  and loss:  30.515771746635437
test acc: top1 ->  84.18 ; top5 ->  98.95  and loss:  50.188938558101654
forward train acc: top1 ->  89.90599998779297 ; top5 ->  99.67999997558594  and loss:  28.934537202119827
test acc: top1 ->  84.78 ; top5 ->  98.92  and loss:  49.594452023506165
forward train acc: top1 ->  90.32599997314453 ; top5 ->  99.7380000048828  and loss:  27.54621821641922
test acc: top1 ->  84.79 ; top5 ->  99.01  and loss:  49.98957099020481
forward train acc: top1 ->  90.30000000732421 ; top5 ->  99.7300000024414  and loss:  27.391474813222885
test acc: top1 ->  85.1 ; top5 ->  98.99  and loss:  48.8108938485384
forward train acc: top1 ->  90.76200000732422 ; top5 ->  99.748  and loss:  26.421725258231163
test acc: top1 ->  85.2 ; top5 ->  99.02  and loss:  47.75018593668938
forward train acc: top1 ->  90.75999998046875 ; top5 ->  99.742  and loss:  26.26870346069336
test acc: top1 ->  85.63 ; top5 ->  98.99  and loss:  47.450966358184814
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -3.984850972890854 , diff:  3.984850972890854
adv train loss:  -3.937391269952059 , diff:  0.04745970293879509
adv train loss:  -3.900363016873598 , diff:  0.03702825307846069
adv train loss:  -4.073352605104446 , diff:  0.1729895882308483
adv train loss:  -3.963471580296755 , diff:  0.10988102480769157
adv train loss:  -3.976774141192436 , diff:  0.013302560895681381
adv train loss:  -3.913354229182005 , diff:  0.06341991201043129
adv train loss:  -3.940179480239749 , diff:  0.026825251057744026
adv train loss:  -4.006974071264267 , diff:  0.06679459102451801
adv train loss:  -4.007064029574394 , diff:  8.99583101272583e-05
layer  6  adv train finish, try to retain  245
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -4.752522144466639 , diff:  4.752522144466639
adv train loss:  -4.740677051246166 , diff:  0.011845093220472336
adv train loss:  -4.798038601875305 , diff:  0.05736155062913895
adv train loss:  -4.739149510860443 , diff:  0.05888909101486206
adv train loss:  -4.806586414575577 , diff:  0.06743690371513367
adv train loss:  -4.735799588263035 , diff:  0.07078682631254196
adv train loss:  -4.706972409039736 , diff:  0.028827179223299026
adv train loss:  -4.821806758642197 , diff:  0.11483434960246086
adv train loss:  -4.7027259059250355 , diff:  0.11908085271716118
adv train loss:  -4.694389417767525 , diff:  0.008336488157510757
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  103
test acc: top1 ->  19.02 ; top5 ->  57.22  and loss:  1079214.94921875
forward train acc: top1 ->  97.90000000488281 ; top5 ->  99.98  and loss:  7.1142488270998
test acc: top1 ->  90.48 ; top5 ->  99.44  and loss:  42.13566901534796
forward train acc: top1 ->  98.87399998046875 ; top5 ->  99.992  and loss:  3.3330308571457863
test acc: top1 ->  90.92 ; top5 ->  99.41  and loss:  46.13097417727113
forward train acc: top1 ->  99.16800000488281 ; top5 ->  100.0  and loss:  2.4897292256355286
test acc: top1 ->  91.07 ; top5 ->  99.42  and loss:  48.745309207588434
forward train acc: top1 ->  99.32399997558593 ; top5 ->  100.0  and loss:  1.9509594924747944
test acc: top1 ->  91.38 ; top5 ->  99.39  and loss:  49.5533950664103
forward train acc: top1 ->  99.414 ; top5 ->  100.0  and loss:  1.6759952763095498
test acc: top1 ->  91.45 ; top5 ->  99.37  and loss:  52.218398328870535
forward train acc: top1 ->  99.54999997802734 ; top5 ->  100.0  and loss:  1.4315516911447048
test acc: top1 ->  91.3 ; top5 ->  99.35  and loss:  52.91424322873354
forward train acc: top1 ->  99.51399997558593 ; top5 ->  99.998  and loss:  1.4593495465815067
test acc: top1 ->  91.48 ; top5 ->  99.36  and loss:  52.9435048289597
forward train acc: top1 ->  99.51399997558593 ; top5 ->  100.0  and loss:  1.3836584417149425
test acc: top1 ->  91.46 ; top5 ->  99.38  and loss:  52.08254378847778
forward train acc: top1 ->  99.52800000732422 ; top5 ->  99.998  and loss:  1.2913571037352085
test acc: top1 ->  91.64 ; top5 ->  99.29  and loss:  53.762202482670546
forward train acc: top1 ->  99.674 ; top5 ->  100.0  and loss:  1.0427735541015863
test acc: top1 ->  91.51 ; top5 ->  99.32  and loss:  55.18039431795478
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  104 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -10.396080259233713 , diff:  10.396080259233713
adv train loss:  -10.681629292666912 , diff:  0.28554903343319893
adv train loss:  -10.272417537868023 , diff:  0.40921175479888916
adv train loss:  -10.660963587462902 , diff:  0.38854604959487915
adv train loss:  -10.621391512453556 , diff:  0.03957207500934601
adv train loss:  -10.564150925725698 , diff:  0.05724058672785759
adv train loss:  -10.410689126700163 , diff:  0.15346179902553558
adv train loss:  -10.367130748927593 , diff:  0.043558377772569656
adv train loss:  -10.663650430738926 , diff:  0.2965196818113327
adv train loss:  -10.513969041407108 , diff:  0.14968138933181763
layer  8  adv train finish, try to retain  382
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -1.8694523572921753 , diff:  1.8694523572921753
adv train loss:  -1.8885696670040488 , diff:  0.01911730971187353
adv train loss:  -2.0882711801677942 , diff:  0.1997015131637454
adv train loss:  -1.8819125108420849 , diff:  0.20635866932570934
adv train loss:  -1.9746993575245142 , diff:  0.09278684668242931
adv train loss:  -1.879763827426359 , diff:  0.09493553009815514
adv train loss:  -1.9817564822733402 , diff:  0.10199265484698117
adv train loss:  -2.0098099689930677 , diff:  0.028053486719727516
adv train loss:  -1.914537774398923 , diff:  0.09527219459414482
adv train loss:  -2.04441869398579 , diff:  0.1298809195868671
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  95
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  4767.68879699707
forward train acc: top1 ->  93.22600000244141 ; top5 ->  99.424  and loss:  26.933455862104893
test acc: top1 ->  80.37 ; top5 ->  98.48  and loss:  82.58184966444969
forward train acc: top1 ->  98.79800000244141 ; top5 ->  99.994  and loss:  4.3979358077049255
test acc: top1 ->  90.37 ; top5 ->  98.75  and loss:  51.00506054610014
forward train acc: top1 ->  99.14599997558594 ; top5 ->  99.996  and loss:  2.9019477292895317
test acc: top1 ->  90.91 ; top5 ->  98.78  and loss:  51.200374498963356
forward train acc: top1 ->  99.48999997558593 ; top5 ->  99.998  and loss:  1.7685446729883552
test acc: top1 ->  91.07 ; top5 ->  98.82  and loss:  52.66124925017357
forward train acc: top1 ->  99.55600000488282 ; top5 ->  100.0  and loss:  1.4462474435567856
test acc: top1 ->  91.27 ; top5 ->  98.86  and loss:  52.765959814190865
forward train acc: top1 ->  99.63599997802734 ; top5 ->  99.998  and loss:  1.1995233707129955
test acc: top1 ->  91.56 ; top5 ->  98.84  and loss:  52.6075519323349
forward train acc: top1 ->  99.71 ; top5 ->  99.998  and loss:  1.057557363063097
test acc: top1 ->  91.57 ; top5 ->  98.96  and loss:  53.21218975633383
forward train acc: top1 ->  99.73799997558594 ; top5 ->  100.0  and loss:  0.8925269478932023
test acc: top1 ->  91.43 ; top5 ->  99.01  and loss:  54.65600907057524
forward train acc: top1 ->  99.756 ; top5 ->  100.0  and loss:  0.8092955721076578
test acc: top1 ->  91.74 ; top5 ->  99.04  and loss:  54.500721991062164
forward train acc: top1 ->  99.75399997558594 ; top5 ->  100.0  and loss:  0.789128677919507
test acc: top1 ->  91.64 ; top5 ->  99.1  and loss:  55.14649472385645
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  96 / 512 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -163.50148952007294 , diff:  163.50148952007294
adv train loss:  -162.37083435058594 , diff:  1.1306551694869995
adv train loss:  -164.01370358467102 , diff:  1.642869234085083
adv train loss:  -163.1628806591034 , diff:  0.850822925567627
adv train loss:  -162.15255224704742 , diff:  1.0103284120559692
adv train loss:  -163.96730315685272 , diff:  1.8147509098052979
adv train loss:  -164.03809463977814 , diff:  0.07079148292541504
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  21
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  145690.36694335938
forward train acc: top1 ->  86.52199997802734 ; top5 ->  97.42  and loss:  90.6626119427383
test acc: top1 ->  89.32 ; top5 ->  99.13  and loss:  47.818856827914715
forward train acc: top1 ->  99.21599998046875 ; top5 ->  99.998  and loss:  3.584864091128111
test acc: top1 ->  90.74 ; top5 ->  99.12  and loss:  44.9716622531414
forward train acc: top1 ->  99.50000000244141 ; top5 ->  99.998  and loss:  2.120199726894498
test acc: top1 ->  91.27 ; top5 ->  99.15  and loss:  46.46594159305096
forward train acc: top1 ->  99.63 ; top5 ->  99.998  and loss:  1.4598082415759563
test acc: top1 ->  91.48 ; top5 ->  99.18  and loss:  47.97525276243687
forward train acc: top1 ->  99.726 ; top5 ->  100.0  and loss:  1.0406978961545974
test acc: top1 ->  91.5 ; top5 ->  99.18  and loss:  49.9438675865531
forward train acc: top1 ->  99.76399997558593 ; top5 ->  99.998  and loss:  0.9160860385745764
test acc: top1 ->  91.53 ; top5 ->  99.14  and loss:  51.05231115221977
forward train acc: top1 ->  99.77799997558594 ; top5 ->  99.998  and loss:  0.8278460232540965
test acc: top1 ->  91.63 ; top5 ->  99.21  and loss:  51.36069531738758
forward train acc: top1 ->  99.80799997558594 ; top5 ->  100.0  and loss:  0.7352563897147775
test acc: top1 ->  91.7 ; top5 ->  99.21  and loss:  52.34281478822231
forward train acc: top1 ->  99.78799997558593 ; top5 ->  100.0  and loss:  0.7554342835210264
test acc: top1 ->  91.53 ; top5 ->  99.17  and loss:  52.718693152070045
forward train acc: top1 ->  99.832 ; top5 ->  100.0  and loss:  0.6515785795636475
test acc: top1 ->  91.75 ; top5 ->  99.24  and loss:  53.63366275280714
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  22 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -129.24397993087769 , diff:  129.24397993087769
adv train loss:  -129.26092886924744 , diff:  0.016948938369750977
layer  11  adv train finish, try to retain  435
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -153.19271111488342 , diff:  153.19271111488342
adv train loss:  -154.28508245944977 , diff:  1.0923713445663452
adv train loss:  -153.4419668316841 , diff:  0.8431156277656555
adv train loss:  -153.44553172588348 , diff:  0.003564894199371338
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  40
test acc: top1 ->  10.84 ; top5 ->  65.42  and loss:  3816.8674144744873
forward train acc: top1 ->  93.964 ; top5 ->  99.894  and loss:  19.138764278963208
test acc: top1 ->  90.47 ; top5 ->  98.63  and loss:  57.838110491633415
forward train acc: top1 ->  99.578 ; top5 ->  100.0  and loss:  1.7851572181098163
test acc: top1 ->  90.7 ; top5 ->  98.64  and loss:  58.63827456533909
forward train acc: top1 ->  99.70999997802734 ; top5 ->  99.996  and loss:  1.2867801254615188
test acc: top1 ->  90.86 ; top5 ->  98.54  and loss:  59.73847937583923
forward train acc: top1 ->  99.71399997802735 ; top5 ->  100.0  and loss:  1.0680627811234444
test acc: top1 ->  90.99 ; top5 ->  98.54  and loss:  60.00002872943878
forward train acc: top1 ->  99.7780000024414 ; top5 ->  100.0  and loss:  0.9849034885410219
test acc: top1 ->  91.1 ; top5 ->  98.49  and loss:  61.135691836476326
forward train acc: top1 ->  99.8080000024414 ; top5 ->  100.0  and loss:  0.7817694947589189
test acc: top1 ->  91.09 ; top5 ->  98.43  and loss:  61.30342656373978
forward train acc: top1 ->  99.83199997558594 ; top5 ->  99.998  and loss:  0.7027845538686961
test acc: top1 ->  91.04 ; top5 ->  98.53  and loss:  61.24292180687189
forward train acc: top1 ->  99.82 ; top5 ->  100.0  and loss:  0.7156795305199921
test acc: top1 ->  91.12 ; top5 ->  98.5  and loss:  61.252503484487534
forward train acc: top1 ->  99.836 ; top5 ->  100.0  and loss:  0.6731590360868722
test acc: top1 ->  91.18 ; top5 ->  98.48  and loss:  61.71409843862057
forward train acc: top1 ->  99.81799997558593 ; top5 ->  100.0  and loss:  0.7290882244706154
test acc: top1 ->  91.16 ; top5 ->  98.49  and loss:  62.01411138474941
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  41 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -17674.295867919922 , diff:  17674.295867919922
adv train loss:  -29137.746322631836 , diff:  11463.450454711914
adv train loss:  -40324.58908081055 , diff:  11186.842758178711
adv train loss:  -51425.40026855469 , diff:  11100.81118774414
adv train loss:  -62483.56378173828 , diff:  11058.163513183594
adv train loss:  -73539.60729980469 , diff:  11056.043518066406
adv train loss:  -84564.50085449219 , diff:  11024.8935546875
adv train loss:  -95589.93048095703 , diff:  11025.429626464844
adv train loss:  -106592.41876220703 , diff:  11002.48828125
adv train loss:  -117611.94201660156 , diff:  11019.523254394531
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  22
test acc: top1 ->  72.86 ; top5 ->  98.38  and loss:  296.02453804016113
forward train acc: top1 ->  98.58999997558594 ; top5 ->  99.996  and loss:  7.919721618294716
test acc: top1 ->  91.86 ; top5 ->  98.85  and loss:  69.79615461081266
forward train acc: top1 ->  99.882 ; top5 ->  100.0  and loss:  0.3815199276432395
test acc: top1 ->  92.04 ; top5 ->  98.85  and loss:  70.34492839127779
forward train acc: top1 ->  99.91999997558594 ; top5 ->  99.998  and loss:  0.2652978333644569
test acc: top1 ->  92.01 ; top5 ->  98.84  and loss:  71.39991600811481
forward train acc: top1 ->  99.92799997558593 ; top5 ->  100.0  and loss:  0.276295417919755
test acc: top1 ->  92.13 ; top5 ->  98.91  and loss:  70.83770854771137
==> this epoch:  22 / 512
layer  0  :  0.484375  ==>  31 / 64 , inc:  1
layer  1  :  0.875  ==>  56 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.3828125  ==>  98 / 256 , inc:  4
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.041015625  ==>  21 / 512 , inc:  2
layer  9  :  0.1875  ==>  96 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.0078125  ==>  4 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.04296875  ==>  22 / 512 , inc:  2
eps [1.7515755615234374, 0.1231576566696167, 1.313681671142578, 1.313681671142578, 1.313681671142578, 1.313681671142578, 1.7515755615234374, 1.313681671142578, 0.05473673629760742, 0.1231576566696167, 1.313681671142578, 1.7515755615234374, 0.1231576566696167, 14.0126044921875]  wait [0, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 0, 2, 0]  inc [1, 1, 1, 1, 1, 1, 4, 1, 2, 1, 1, 1, 1, 2]  tol: 6
$$$$$$$$$$$$$ epoch  50  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1527.6891286373138 , diff:  1527.6891286373138
adv train loss:  -1610.0315132141113 , diff:  82.34238457679749
adv train loss:  -1612.4805355072021 , diff:  2.4490222930908203
adv train loss:  -1611.2366590499878 , diff:  1.2438764572143555
adv train loss:  -1622.9521503448486 , diff:  11.71549129486084
adv train loss:  -1630.855951309204 , diff:  7.903800964355469
adv train loss:  -1630.621069908142 , diff:  0.23488140106201172
adv train loss:  -1633.099895477295 , diff:  2.478825569152832
adv train loss:  -1622.4222507476807 , diff:  10.677644729614258
adv train loss:  -1626.4539976119995 , diff:  4.031746864318848
layer  0  adv train finish, try to retain  2
test acc: top1 ->  10.0 ; top5 ->  48.16  and loss:  1248.8524026870728
forward train acc: top1 ->  18.41599999572754 ; top5 ->  63.16199998779297  and loss:  444.3877511024475
test acc: top1 ->  10.26 ; top5 ->  52.95  and loss:  441.41643357276917
forward train acc: top1 ->  20.73200000427246 ; top5 ->  67.03999997802734  and loss:  215.32563757896423
test acc: top1 ->  22.28 ; top5 ->  67.98  and loss:  216.02792048454285
forward train acc: top1 ->  23.162000001220704 ; top5 ->  70.72200000732421  and loss:  208.29808521270752
test acc: top1 ->  24.84 ; top5 ->  71.2  and loss:  210.53577053546906
forward train acc: top1 ->  26.615999998779298 ; top5 ->  73.75999999511718  and loss:  201.48863172531128
test acc: top1 ->  26.06 ; top5 ->  71.91  and loss:  207.15102529525757
forward train acc: top1 ->  29.069999997558593 ; top5 ->  76.60199998291016  and loss:  195.04314994812012
test acc: top1 ->  30.2 ; top5 ->  75.98  and loss:  197.95804595947266
forward train acc: top1 ->  31.02399999206543 ; top5 ->  78.54400000244141  and loss:  190.48147666454315
test acc: top1 ->  31.89 ; top5 ->  79.22  and loss:  191.6345649957657
forward train acc: top1 ->  32.134000001220706 ; top5 ->  79.70200000976563  and loss:  187.20299673080444
test acc: top1 ->  33.24 ; top5 ->  79.91  and loss:  188.51040291786194
forward train acc: top1 ->  33.17400000610352 ; top5 ->  80.36799997802734  and loss:  184.327778339386
test acc: top1 ->  34.12 ; top5 ->  80.84  and loss:  185.35090839862823
forward train acc: top1 ->  34.69999999267578 ; top5 ->  81.66200000488281  and loss:  180.6269646883011
test acc: top1 ->  35.72 ; top5 ->  82.16  and loss:  182.0527344942093
forward train acc: top1 ->  36.0080000012207 ; top5 ->  82.68199999023437  and loss:  176.94571363925934
test acc: top1 ->  36.61 ; top5 ->  83.08  and loss:  178.33713257312775
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  31 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -62.86451333761215 , diff:  62.86451333761215
adv train loss:  -62.84882402420044 , diff:  0.015689313411712646
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -221.52513080835342 , diff:  221.52513080835342
adv train loss:  -299.0803565979004 , diff:  77.55522578954697
adv train loss:  -301.6936595439911 , diff:  2.6133029460906982
adv train loss:  -301.8264763355255 , diff:  0.13281679153442383
adv train loss:  -303.23728013038635 , diff:  1.4108037948608398
adv train loss:  -305.4743733406067 , diff:  2.237093210220337
adv train loss:  -306.33557081222534 , diff:  0.8611974716186523
adv train loss:  -306.129695892334 , diff:  0.20587491989135742
adv train loss:  -306.83085799217224 , diff:  0.7011620998382568
adv train loss:  -305.8610260486603 , diff:  0.9698319435119629
************ all values are small in this layer **********
layer  2  adv train finish, try to retain  99
test acc: top1 ->  25.03 ; top5 ->  67.28  and loss:  251.69184350967407
forward train acc: top1 ->  99.1440000024414 ; top5 ->  99.99  and loss:  9.38771616667509
test acc: top1 ->  91.56 ; top5 ->  99.27  and loss:  38.296171583235264
forward train acc: top1 ->  99.71999997558594 ; top5 ->  100.0  and loss:  1.033988687209785
test acc: top1 ->  91.6 ; top5 ->  99.24  and loss:  43.27929034084082
forward train acc: top1 ->  99.78799997558593 ; top5 ->  100.0  and loss:  0.7568614175543189
test acc: top1 ->  91.66 ; top5 ->  99.31  and loss:  46.57779924571514
forward train acc: top1 ->  99.792 ; top5 ->  100.0  and loss:  0.6171383946202695
test acc: top1 ->  91.72 ; top5 ->  99.25  and loss:  49.3920129686594
forward train acc: top1 ->  99.864 ; top5 ->  100.0  and loss:  0.4604801961686462
test acc: top1 ->  91.67 ; top5 ->  99.26  and loss:  51.524341493844986
forward train acc: top1 ->  99.864 ; top5 ->  100.0  and loss:  0.48824152146698907
test acc: top1 ->  91.82 ; top5 ->  99.26  and loss:  51.712222553789616
forward train acc: top1 ->  99.866 ; top5 ->  100.0  and loss:  0.4711483714636415
test acc: top1 ->  91.84 ; top5 ->  99.26  and loss:  53.048787109553814
forward train acc: top1 ->  99.86 ; top5 ->  100.0  and loss:  0.4089890052273404
test acc: top1 ->  91.87 ; top5 ->  99.21  and loss:  53.98547771573067
forward train acc: top1 ->  99.84799997558594 ; top5 ->  100.0  and loss:  0.4822299024090171
test acc: top1 ->  91.82 ; top5 ->  99.23  and loss:  54.328716821968555
forward train acc: top1 ->  99.85999997558594 ; top5 ->  100.0  and loss:  0.4194283904507756
test acc: top1 ->  91.79 ; top5 ->  99.26  and loss:  54.23311913013458
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -775.8061447516084 , diff:  775.8061447516084
adv train loss:  -1397.2987775802612 , diff:  621.4926328286529
adv train loss:  -1415.9281015396118 , diff:  18.629323959350586
adv train loss:  -1425.5350522994995 , diff:  9.606950759887695
adv train loss:  -1425.1374759674072 , diff:  0.39757633209228516
adv train loss:  -1424.2414684295654 , diff:  0.8960075378417969
adv train loss:  -1422.099347114563 , diff:  2.1421213150024414
adv train loss:  -1427.350811958313 , diff:  5.25146484375
adv train loss:  -1430.942141532898 , diff:  3.591329574584961
adv train loss:  -1429.744252204895 , diff:  1.1978893280029297
************ all values are small in this layer **********
layer  3  adv train finish, try to retain  86
test acc: top1 ->  10.05 ; top5 ->  50.15  and loss:  14347.2216796875
forward train acc: top1 ->  99.61799997558593 ; top5 ->  99.998  and loss:  1.1278091333806515
test acc: top1 ->  91.7 ; top5 ->  99.35  and loss:  54.355391308665276
forward train acc: top1 ->  99.68199997558594 ; top5 ->  100.0  and loss:  0.9731395575217903
test acc: top1 ->  91.76 ; top5 ->  99.28  and loss:  52.29756164550781
forward train acc: top1 ->  99.724 ; top5 ->  100.0  and loss:  0.8312836030963808
test acc: top1 ->  91.6 ; top5 ->  99.23  and loss:  53.68229162693024
forward train acc: top1 ->  99.772 ; top5 ->  100.0  and loss:  0.7027519897092134
test acc: top1 ->  91.76 ; top5 ->  99.39  and loss:  53.916526444256306
forward train acc: top1 ->  99.788 ; top5 ->  100.0  and loss:  0.6350583860184997
test acc: top1 ->  91.88 ; top5 ->  99.22  and loss:  55.34932565689087
forward train acc: top1 ->  99.80999997558594 ; top5 ->  99.998  and loss:  0.5507673742249608
test acc: top1 ->  91.81 ; top5 ->  99.3  and loss:  54.80855029821396
forward train acc: top1 ->  99.84199997558594 ; top5 ->  100.0  and loss:  0.4726509787142277
test acc: top1 ->  91.89 ; top5 ->  99.32  and loss:  56.713200122117996
forward train acc: top1 ->  99.836 ; top5 ->  100.0  and loss:  0.48976145102642477
test acc: top1 ->  91.83 ; top5 ->  99.26  and loss:  56.94012504070997
forward train acc: top1 ->  99.8140000024414 ; top5 ->  100.0  and loss:  0.5270962445065379
test acc: top1 ->  91.75 ; top5 ->  99.28  and loss:  57.0008529946208
forward train acc: top1 ->  99.832 ; top5 ->  99.996  and loss:  0.48107398132560775
test acc: top1 ->  91.82 ; top5 ->  99.38  and loss:  57.816774897277355
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -378.5736599173397 , diff:  378.5736599173397
adv train loss:  -1619.784083366394 , diff:  1241.2104234490544
adv train loss:  -1734.6781482696533 , diff:  114.89406490325928
adv train loss:  -1888.8568496704102 , diff:  154.17870140075684
adv train loss:  -1907.6361541748047 , diff:  18.77930450439453
adv train loss:  -1933.3765621185303 , diff:  25.740407943725586
adv train loss:  -1942.4396648406982 , diff:  9.063102722167969
adv train loss:  -1951.4185810089111 , diff:  8.97891616821289
adv train loss:  -1948.0370788574219 , diff:  3.381502151489258
adv train loss:  -1936.8951320648193 , diff:  11.141946792602539
************ all values are small in this layer **********
layer  4  adv train finish, try to retain  155
test acc: top1 ->  17.11 ; top5 ->  56.06  and loss:  3593.746524810791
forward train acc: top1 ->  98.16799997802734 ; top5 ->  99.976  and loss:  5.837142009288073
test acc: top1 ->  90.17 ; top5 ->  99.23  and loss:  50.478743359446526
forward train acc: top1 ->  98.63800000488281 ; top5 ->  99.982  and loss:  4.22600531578064
test acc: top1 ->  90.46 ; top5 ->  99.32  and loss:  46.74660636484623
forward train acc: top1 ->  98.73999998046875 ; top5 ->  99.988  and loss:  3.5574051328003407
test acc: top1 ->  90.24 ; top5 ->  99.27  and loss:  49.554640382528305
forward train acc: top1 ->  98.91800000732422 ; top5 ->  99.99  and loss:  3.18268021941185
test acc: top1 ->  90.11 ; top5 ->  99.29  and loss:  48.150796726346016
forward train acc: top1 ->  99.01200000244141 ; top5 ->  99.986  and loss:  2.994844138622284
test acc: top1 ->  90.27 ; top5 ->  99.26  and loss:  48.61130836606026
forward train acc: top1 ->  99.05599997802734 ; top5 ->  99.99199997558594  and loss:  2.707668773829937
test acc: top1 ->  90.57 ; top5 ->  99.28  and loss:  46.850993037223816
forward train acc: top1 ->  99.074 ; top5 ->  99.998  and loss:  2.552006567828357
test acc: top1 ->  90.61 ; top5 ->  99.32  and loss:  47.57777693122625
forward train acc: top1 ->  99.11199997802734 ; top5 ->  99.996  and loss:  2.5039739310741425
test acc: top1 ->  90.55 ; top5 ->  99.31  and loss:  47.99062779545784
forward train acc: top1 ->  99.19999997558594 ; top5 ->  99.998  and loss:  2.2852911334484816
test acc: top1 ->  90.63 ; top5 ->  99.28  and loss:  49.180214419960976
forward train acc: top1 ->  99.21599997802734 ; top5 ->  99.998  and loss:  2.205190708860755
test acc: top1 ->  90.6 ; top5 ->  99.32  and loss:  48.69792252033949
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  156 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -425.90380120649934 , diff:  425.90380120649934
adv train loss:  -1481.2197723388672 , diff:  1055.3159711323678
adv train loss:  -1534.759030342102 , diff:  53.53925800323486
adv train loss:  -1537.733229637146 , diff:  2.9741992950439453
adv train loss:  -1543.753438949585 , diff:  6.020209312438965
adv train loss:  -1568.0348386764526 , diff:  24.281399726867676
adv train loss:  -1567.4523363113403 , diff:  0.5825023651123047
adv train loss:  -1590.9510822296143 , diff:  23.498745918273926
adv train loss:  -1594.3791809082031 , diff:  3.428098678588867
adv train loss:  -1628.371012687683 , diff:  33.99183177947998
************ all values are small in this layer **********
layer  5  adv train finish, try to retain  148
test acc: top1 ->  31.74 ; top5 ->  78.37  and loss:  1437.220227241516
forward train acc: top1 ->  99.38199997802734 ; top5 ->  99.998  and loss:  1.8558715730905533
test acc: top1 ->  91.35 ; top5 ->  99.45  and loss:  48.53930498659611
forward train acc: top1 ->  99.53 ; top5 ->  100.0  and loss:  1.335588906193152
test acc: top1 ->  91.46 ; top5 ->  99.38  and loss:  48.75628135353327
forward train acc: top1 ->  99.68 ; top5 ->  100.0  and loss:  0.9947487951721996
test acc: top1 ->  91.69 ; top5 ->  99.41  and loss:  50.410791143774986
forward train acc: top1 ->  99.71799997802735 ; top5 ->  100.0  and loss:  0.8553873635828495
test acc: top1 ->  91.57 ; top5 ->  99.48  and loss:  53.590582355856895
forward train acc: top1 ->  99.70999997558594 ; top5 ->  100.0  and loss:  0.8285516025498509
test acc: top1 ->  91.58 ; top5 ->  99.46  and loss:  52.345493987202644
forward train acc: top1 ->  99.762 ; top5 ->  100.0  and loss:  0.6943388390354812
test acc: top1 ->  91.7 ; top5 ->  99.43  and loss:  53.29582443088293
forward train acc: top1 ->  99.774 ; top5 ->  100.0  and loss:  0.6985192152205855
test acc: top1 ->  91.69 ; top5 ->  99.43  and loss:  52.85563915967941
forward train acc: top1 ->  99.79599997558594 ; top5 ->  100.0  and loss:  0.6247992552816868
test acc: top1 ->  91.83 ; top5 ->  99.48  and loss:  54.1007886081934
forward train acc: top1 ->  99.788 ; top5 ->  100.0  and loss:  0.5730296338442713
test acc: top1 ->  91.84 ; top5 ->  99.43  and loss:  55.29065175354481
forward train acc: top1 ->  99.826 ; top5 ->  100.0  and loss:  0.5433414620347321
test acc: top1 ->  91.67 ; top5 ->  99.39  and loss:  55.980148285627365
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -633.2914212718606 , diff:  633.2914212718606
adv train loss:  -2100.0797576904297 , diff:  1466.788336418569
adv train loss:  -2125.9959754943848 , diff:  25.916217803955078
adv train loss:  -2122.7357540130615 , diff:  3.260221481323242
adv train loss:  -2125.749542236328 , diff:  3.0137882232666016
adv train loss:  -2127.9844341278076 , diff:  2.234891891479492
adv train loss:  -2160.9115505218506 , diff:  32.92711639404297
adv train loss:  -2172.828868865967 , diff:  11.917318344116211
adv train loss:  -2166.5423545837402 , diff:  6.2865142822265625
adv train loss:  -2167.4068489074707 , diff:  0.8644943237304688
layer  6  adv train finish, try to retain  2
test acc: top1 ->  10.1 ; top5 ->  50.03  and loss:  1819.4673509597778
forward train acc: top1 ->  39.3120000012207 ; top5 ->  85.55400000244141  and loss:  191.81328213214874
test acc: top1 ->  43.03 ; top5 ->  89.53  and loss:  151.70152187347412
forward train acc: top1 ->  46.95200000488281 ; top5 ->  91.75200000488282  and loss:  136.03407275676727
test acc: top1 ->  47.31 ; top5 ->  92.46  and loss:  140.1062295436859
forward train acc: top1 ->  52.46200000732422 ; top5 ->  93.9040000024414  and loss:  122.62084352970123
test acc: top1 ->  53.31 ; top5 ->  94.34  and loss:  126.83311998844147
forward train acc: top1 ->  56.242000003662106 ; top5 ->  95.42600001708985  and loss:  113.39846050739288
test acc: top1 ->  55.13 ; top5 ->  95.03  and loss:  124.36855566501617
forward train acc: top1 ->  59.11599999023438 ; top5 ->  96.34799999511719  and loss:  106.39222455024719
test acc: top1 ->  58.87 ; top5 ->  95.94  and loss:  115.19338166713715
forward train acc: top1 ->  61.94999999389648 ; top5 ->  96.97599998779297  and loss:  100.3633159995079
test acc: top1 ->  60.5 ; top5 ->  96.26  and loss:  111.55574810504913
forward train acc: top1 ->  63.65599998413086 ; top5 ->  97.32000001220703  and loss:  96.0189773440361
test acc: top1 ->  61.15 ; top5 ->  96.52  and loss:  110.50858986377716
forward train acc: top1 ->  64.8340000012207 ; top5 ->  97.51799999023437  and loss:  93.22872787714005
test acc: top1 ->  63.43 ; top5 ->  96.66  and loss:  104.46281468868256
forward train acc: top1 ->  66.32599997314453 ; top5 ->  97.83000000488282  and loss:  89.57882022857666
test acc: top1 ->  64.55 ; top5 ->  96.92  and loss:  102.84898799657822
forward train acc: top1 ->  67.53800000976563 ; top5 ->  97.86400000732422  and loss:  87.03593391180038
test acc: top1 ->  65.77 ; top5 ->  97.18  and loss:  99.2331713438034
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  98 / 256 , inc:  4
---------------- start layer  7  ---------------
adv train loss:  -10.50006601959467 , diff:  10.50006601959467
adv train loss:  -10.692222073674202 , diff:  0.19215605407953262
adv train loss:  -10.595740973949432 , diff:  0.09648109972476959
adv train loss:  -10.728140287101269 , diff:  0.1323993131518364
adv train loss:  -10.696305267512798 , diff:  0.03183501958847046
adv train loss:  -10.722766198217869 , diff:  0.026460930705070496
adv train loss:  -10.540214568376541 , diff:  0.18255162984132767
adv train loss:  -11.709797441959381 , diff:  1.16958287358284
adv train loss:  -18.354301929473877 , diff:  6.644504487514496
adv train loss:  -18.51788017898798 , diff:  0.16357824951410294
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  103
test acc: top1 ->  10.05 ; top5 ->  65.49  and loss:  2346140.75390625
forward train acc: top1 ->  97.10400000976563 ; top5 ->  99.964  and loss:  9.293216079473495
test acc: top1 ->  89.68 ; top5 ->  99.34  and loss:  49.14458367228508
forward train acc: top1 ->  98.96000000488282 ; top5 ->  99.996  and loss:  3.180122459307313
test acc: top1 ->  90.68 ; top5 ->  99.4  and loss:  50.78978896141052
forward train acc: top1 ->  99.33399997558594 ; top5 ->  99.99  and loss:  2.176173957064748
test acc: top1 ->  90.85 ; top5 ->  99.44  and loss:  51.86843606829643
forward train acc: top1 ->  99.44399997802735 ; top5 ->  99.998  and loss:  1.5739692281931639
test acc: top1 ->  91.08 ; top5 ->  99.39  and loss:  54.229091078042984
forward train acc: top1 ->  99.52599997558593 ; top5 ->  100.0  and loss:  1.5031911935657263
test acc: top1 ->  91.08 ; top5 ->  99.43  and loss:  54.85631863772869
forward train acc: top1 ->  99.59399997558593 ; top5 ->  100.0  and loss:  1.206131343729794
test acc: top1 ->  91.27 ; top5 ->  99.49  and loss:  53.73368342220783
forward train acc: top1 ->  99.614 ; top5 ->  100.0  and loss:  1.1442298917099833
test acc: top1 ->  91.32 ; top5 ->  99.45  and loss:  54.61065413802862
forward train acc: top1 ->  99.66799997558594 ; top5 ->  100.0  and loss:  1.000759948976338
test acc: top1 ->  91.27 ; top5 ->  99.43  and loss:  54.74867931008339
forward train acc: top1 ->  99.6620000024414 ; top5 ->  100.0  and loss:  1.0097395814955235
test acc: top1 ->  91.22 ; top5 ->  99.36  and loss:  56.33279424160719
forward train acc: top1 ->  99.67200000244141 ; top5 ->  100.0  and loss:  0.9793991064652801
test acc: top1 ->  91.25 ; top5 ->  99.4  and loss:  57.47647588700056
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  104 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -35.876097939908504 , diff:  35.876097939908504
adv train loss:  -36.8132341504097 , diff:  0.937136210501194
adv train loss:  -36.74481636285782 , diff:  0.06841778755187988
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  19
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  1793.7799730300903
forward train acc: top1 ->  88.55600000488282 ; top5 ->  99.766  and loss:  40.81838580965996
test acc: top1 ->  89.41 ; top5 ->  98.95  and loss:  50.30276717245579
forward train acc: top1 ->  98.46600000732421 ; top5 ->  99.996  and loss:  5.517209481447935
test acc: top1 ->  89.77 ; top5 ->  99.08  and loss:  51.19509842991829
forward train acc: top1 ->  99.00999997558594 ; top5 ->  100.0  and loss:  3.3457890432327986
test acc: top1 ->  90.41 ; top5 ->  98.97  and loss:  52.299278028309345
forward train acc: top1 ->  99.29200000244141 ; top5 ->  100.0  and loss:  2.502191411331296
test acc: top1 ->  90.75 ; top5 ->  99.02  and loss:  52.59055709838867
forward train acc: top1 ->  99.32999997802735 ; top5 ->  99.998  and loss:  2.1850259248167276
test acc: top1 ->  90.77 ; top5 ->  99.05  and loss:  53.510450564324856
forward train acc: top1 ->  99.46199997558594 ; top5 ->  99.996  and loss:  1.729061116464436
test acc: top1 ->  91.09 ; top5 ->  99.02  and loss:  53.09837315231562
forward train acc: top1 ->  99.5100000024414 ; top5 ->  100.0  and loss:  1.67463593557477
test acc: top1 ->  91.06 ; top5 ->  99.02  and loss:  54.00868748873472
forward train acc: top1 ->  99.4740000024414 ; top5 ->  99.998  and loss:  1.6807623617351055
test acc: top1 ->  90.99 ; top5 ->  99.04  and loss:  54.75027696043253
forward train acc: top1 ->  99.5460000024414 ; top5 ->  99.998  and loss:  1.566067524254322
test acc: top1 ->  90.99 ; top5 ->  98.96  and loss:  55.31493306159973
forward train acc: top1 ->  99.60599998046875 ; top5 ->  100.0  and loss:  1.3416316956281662
test acc: top1 ->  91.15 ; top5 ->  98.97  and loss:  55.02429936081171
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  21 / 512 , inc:  2
---------------- start layer  9  ---------------
adv train loss:  -15.698165159672499 , diff:  15.698165159672499
adv train loss:  -15.631862461566925 , diff:  0.06630269810557365
adv train loss:  -15.68227318674326 , diff:  0.05041072517633438
adv train loss:  -15.861489403992891 , diff:  0.17921621724963188
adv train loss:  -15.508438054472208 , diff:  0.3530513495206833
adv train loss:  -15.66937243938446 , diff:  0.16093438491225243
adv train loss:  -15.617226429283619 , diff:  0.05214601010084152
adv train loss:  -15.679709531366825 , diff:  0.06248310208320618
adv train loss:  -15.347064390778542 , diff:  0.33264514058828354
adv train loss:  -15.635931205004454 , diff:  0.2888668142259121
layer  9  adv train finish, try to retain  429
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -80.29209464788437 , diff:  80.29209464788437
adv train loss:  -80.16572284698486 , diff:  0.12637180089950562
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  21
test acc: top1 ->  11.28 ; top5 ->  50.0  and loss:  195282.3134765625
forward train acc: top1 ->  85.89399998046875 ; top5 ->  99.122  and loss:  47.682627718895674
test acc: top1 ->  85.01 ; top5 ->  98.15  and loss:  68.94687843322754
forward train acc: top1 ->  99.24200000488281 ; top5 ->  99.99799997558594  and loss:  3.539621740579605
test acc: top1 ->  90.68 ; top5 ->  98.94  and loss:  47.35879119485617
forward train acc: top1 ->  99.6240000024414 ; top5 ->  99.996  and loss:  1.582606092095375
test acc: top1 ->  91.21 ; top5 ->  98.94  and loss:  48.735270753502846
forward train acc: top1 ->  99.7840000024414 ; top5 ->  100.0  and loss:  0.8777674678713083
test acc: top1 ->  91.26 ; top5 ->  98.95  and loss:  52.138707026839256
forward train acc: top1 ->  99.82199997558594 ; top5 ->  100.0  and loss:  0.6805835254490376
test acc: top1 ->  91.52 ; top5 ->  99.02  and loss:  53.1195310279727
forward train acc: top1 ->  99.858 ; top5 ->  100.0  and loss:  0.5596818402409554
test acc: top1 ->  91.53 ; top5 ->  99.0  and loss:  52.23944916576147
forward train acc: top1 ->  99.896 ; top5 ->  100.0  and loss:  0.4316801163367927
test acc: top1 ->  91.67 ; top5 ->  98.99  and loss:  53.24275994300842
forward train acc: top1 ->  99.882 ; top5 ->  100.0  and loss:  0.4198525582905859
test acc: top1 ->  91.7 ; top5 ->  99.07  and loss:  53.30850774049759
forward train acc: top1 ->  99.9 ; top5 ->  100.0  and loss:  0.3946457044221461
test acc: top1 ->  91.87 ; top5 ->  99.03  and loss:  53.96365054696798
forward train acc: top1 ->  99.896 ; top5 ->  100.0  and loss:  0.39293948048725724
test acc: top1 ->  91.74 ; top5 ->  99.11  and loss:  55.095652505755424
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  22 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -1157.556240081787 , diff:  1157.556240081787
adv train loss:  -1318.1611471176147 , diff:  160.60490703582764
adv train loss:  -1542.5409317016602 , diff:  224.3797845840454
adv train loss:  -1557.1197681427002 , diff:  14.578836441040039
adv train loss:  -1558.2844257354736 , diff:  1.1646575927734375
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  3
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  26208.56022644043
forward train acc: top1 ->  99.00399997558594 ; top5 ->  100.0  and loss:  3.3422909886576235
test acc: top1 ->  91.88 ; top5 ->  98.99  and loss:  73.77134250104427
forward train acc: top1 ->  99.90799997558594 ; top5 ->  100.0  and loss:  0.29500147281214595
test acc: top1 ->  91.9 ; top5 ->  99.0  and loss:  71.87358082830906
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.20780268887756392
test acc: top1 ->  92.09 ; top5 ->  98.97  and loss:  72.08817607164383
forward train acc: top1 ->  99.942 ; top5 ->  100.0  and loss:  0.16853152960538864
test acc: top1 ->  92.07 ; top5 ->  98.98  and loss:  71.1779940277338
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.17302973274490796
test acc: top1 ->  92.35 ; top5 ->  99.03  and loss:  73.49651440232992
==> this epoch:  3 / 512
---------------- start layer  12  ---------------
adv train loss:  -75.79515391588211 , diff:  75.79515391588211
adv train loss:  -75.84162789583206 , diff:  0.04647397994995117
layer  12  adv train finish, try to retain  487
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -15494.470573425293 , diff:  15494.470573425293
adv train loss:  -25611.359985351562 , diff:  10116.88941192627
adv train loss:  -35419.673400878906 , diff:  9808.313415527344
adv train loss:  -45091.8005065918 , diff:  9672.12710571289
adv train loss:  -54696.693786621094 , diff:  9604.893280029297
adv train loss:  -64266.17205810547 , diff:  9569.478271484375
adv train loss:  -73837.2925415039 , diff:  9571.120483398438
adv train loss:  -83396.83618164062 , diff:  9559.543640136719
adv train loss:  -92941.87854003906 , diff:  9545.042358398438
adv train loss:  -102482.28308105469 , diff:  9540.404541015625
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  20
test acc: top1 ->  46.55 ; top5 ->  95.17  and loss:  426.76641845703125
forward train acc: top1 ->  96.37999997558593 ; top5 ->  99.938  and loss:  16.198000294156373
test acc: top1 ->  91.6 ; top5 ->  98.84  and loss:  55.628042593598366
forward train acc: top1 ->  99.912 ; top5 ->  99.996  and loss:  0.4481480602407828
test acc: top1 ->  91.73 ; top5 ->  98.89  and loss:  56.524716049432755
forward train acc: top1 ->  99.91799997558594 ; top5 ->  100.0  and loss:  0.34314944688230753
test acc: top1 ->  92.08 ; top5 ->  98.93  and loss:  56.10371634364128
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.22490323474630713
test acc: top1 ->  92.06 ; top5 ->  98.95  and loss:  56.12907960265875
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.2079836719785817
test acc: top1 ->  92.1 ; top5 ->  98.99  and loss:  56.79680775105953
forward train acc: top1 ->  99.95999997558594 ; top5 ->  100.0  and loss:  0.18031053198501468
test acc: top1 ->  92.13 ; top5 ->  99.02  and loss:  58.09792423248291
==> this epoch:  20 / 512
layer  0  :  0.484375  ==>  31 / 64 , inc:  1
layer  1  :  0.875  ==>  56 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.3828125  ==>  98 / 256 , inc:  2
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.041015625  ==>  21 / 512 , inc:  1
layer  9  :  0.1875  ==>  96 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.005859375  ==>  3 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.0390625  ==>  20 / 512 , inc:  4
eps [1.313681671142578, 0.2463153133392334, 0.9852612533569336, 0.9852612533569336, 0.9852612533569336, 0.9852612533569336, 1.313681671142578, 0.9852612533569336, 0.041052552223205564, 0.2463153133392334, 0.9852612533569336, 1.7515755615234374, 0.2463153133392334, 14.0126044921875]  wait [2, 2, 4, 4, 4, 4, 2, 4, 2, 2, 4, 0, 2, 0]  inc [1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 4]  tol: 6
$$$$$$$$$$$$$ epoch  51  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -754.3982625603676 , diff:  754.3982625603676
adv train loss:  -783.5590872764587 , diff:  29.160824716091156
adv train loss:  -789.7737970352173 , diff:  6.214709758758545
adv train loss:  -804.065381526947 , diff:  14.291584491729736
adv train loss:  -807.4418797492981 , diff:  3.376498222351074
adv train loss:  -807.5765314102173 , diff:  0.13465166091918945
adv train loss:  -805.469973564148 , diff:  2.106557846069336
adv train loss:  -807.176064491272 , diff:  1.7060909271240234
adv train loss:  -803.352566242218 , diff:  3.823498249053955
adv train loss:  -805.8027052879333 , diff:  2.450139045715332
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  30
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  5304874.87890625
forward train acc: top1 ->  99.798 ; top5 ->  99.996  and loss:  0.6563255074433982
test acc: top1 ->  90.59 ; top5 ->  98.69  and loss:  75.52467574179173
forward train acc: top1 ->  99.874 ; top5 ->  99.992  and loss:  0.4366325222654268
test acc: top1 ->  91.77 ; top5 ->  99.1  and loss:  63.34724624454975
forward train acc: top1 ->  99.878 ; top5 ->  99.998  and loss:  0.3398689908790402
test acc: top1 ->  91.7 ; top5 ->  99.1  and loss:  66.25732476264238
forward train acc: top1 ->  99.89 ; top5 ->  99.998  and loss:  0.34459939145017415
test acc: top1 ->  91.8 ; top5 ->  99.08  and loss:  68.83145318180323
forward train acc: top1 ->  99.888 ; top5 ->  100.0  and loss:  0.37114807838224806
test acc: top1 ->  91.58 ; top5 ->  99.09  and loss:  67.18540120124817
forward train acc: top1 ->  99.904 ; top5 ->  100.0  and loss:  0.2609453350305557
test acc: top1 ->  91.92 ; top5 ->  99.18  and loss:  65.42712718248367
forward train acc: top1 ->  99.908 ; top5 ->  100.0  and loss:  0.25230083242058754
test acc: top1 ->  91.86 ; top5 ->  99.14  and loss:  66.60111086815596
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.19434319145511836
test acc: top1 ->  91.94 ; top5 ->  99.23  and loss:  65.14431339502335
forward train acc: top1 ->  99.91 ; top5 ->  100.0  and loss:  0.24862190568819642
test acc: top1 ->  92.07 ; top5 ->  99.13  and loss:  65.05797870457172
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.17128181119915098
test acc: top1 ->  92.11 ; top5 ->  99.22  and loss:  65.80030235648155
==> this epoch:  30 / 64
---------------- start layer  1  ---------------
adv train loss:  -0.1367310148998513 , diff:  0.1367310148998513
adv train loss:  -0.18386355449911207 , diff:  0.04713253959926078
adv train loss:  -0.15921947080641985 , diff:  0.02464408369269222
adv train loss:  -0.21253787619934883 , diff:  0.05331840539292898
adv train loss:  -0.18544547888450325 , diff:  0.027092397314845584
adv train loss:  -0.16721665300428867 , diff:  0.018228825880214572
adv train loss:  -0.17241324298083782 , diff:  0.0051965899765491486
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
adv train loss:  -709.6244844254106 , diff:  709.6244844254106
adv train loss:  -2221.2032165527344 , diff:  1511.5787321273237
adv train loss:  -2299.950294494629 , diff:  78.74707794189453
adv train loss:  -2306.7958488464355 , diff:  6.845554351806641
adv train loss:  -2330.8440990448 , diff:  24.048250198364258
adv train loss:  -2357.782934188843 , diff:  26.93883514404297
adv train loss:  -2359.0979080200195 , diff:  1.3149738311767578
adv train loss:  -2368.5253143310547 , diff:  9.427406311035156
adv train loss:  -2393.0374279022217 , diff:  24.512113571166992
adv train loss:  -2396.321756362915 , diff:  3.2843284606933594
************ all values are small in this layer **********
layer  6  adv train finish, try to retain  96
test acc: top1 ->  10.03 ; top5 ->  50.0  and loss:  990664.2036132812
forward train acc: top1 ->  99.86999997558594 ; top5 ->  100.0  and loss:  0.3952436987310648
test acc: top1 ->  91.82 ; top5 ->  99.17  and loss:  70.19425235688686
forward train acc: top1 ->  99.87799997558594 ; top5 ->  100.0  and loss:  0.37481254152953625
test acc: top1 ->  91.96 ; top5 ->  99.19  and loss:  67.53141193464398
forward train acc: top1 ->  99.9 ; top5 ->  100.0  and loss:  0.2500771936029196
test acc: top1 ->  91.78 ; top5 ->  99.26  and loss:  69.2825144007802
forward train acc: top1 ->  99.89599997558594 ; top5 ->  100.0  and loss:  0.2917727157473564
test acc: top1 ->  91.8 ; top5 ->  99.18  and loss:  68.5463195592165
forward train acc: top1 ->  99.9 ; top5 ->  99.998  and loss:  0.3348255007294938
test acc: top1 ->  91.8 ; top5 ->  99.14  and loss:  67.14997704327106
forward train acc: top1 ->  99.91400000488281 ; top5 ->  100.0  and loss:  0.3051565308123827
test acc: top1 ->  91.99 ; top5 ->  99.25  and loss:  66.2935294508934
forward train acc: top1 ->  99.902 ; top5 ->  100.0  and loss:  0.3077163682319224
test acc: top1 ->  91.82 ; top5 ->  99.27  and loss:  66.48390567302704
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.2341419323347509
test acc: top1 ->  91.89 ; top5 ->  99.29  and loss:  66.9659866169095
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.17995811643777415
test acc: top1 ->  91.61 ; top5 ->  99.25  and loss:  68.82932814955711
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.2154618905224197
test acc: top1 ->  92.04 ; top5 ->  99.18  and loss:  67.59343295544386
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  98 / 256 , inc:  2
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
adv train loss:  -5.420386776328087 , diff:  5.420386776328087
adv train loss:  -6.233100535348058 , diff:  0.8127137590199709
adv train loss:  -5.773999642580748 , diff:  0.45910089276731014
adv train loss:  -5.906987316906452 , diff:  0.13298767432570457
adv train loss:  -6.210418941453099 , diff:  0.30343162454664707
adv train loss:  -5.938866453245282 , diff:  0.2715524882078171
adv train loss:  -5.88049544673413 , diff:  0.05837100651115179
adv train loss:  -5.90409991145134 , diff:  0.02360446471720934
adv train loss:  -5.90651760622859 , diff:  0.00241769477725029
layer  8  adv train finish, try to retain  399
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -0.5403582798317075 , diff:  0.5403582798317075
adv train loss:  -0.5312523916363716 , diff:  0.009105888195335865
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  95
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  5648.66565322876
forward train acc: top1 ->  85.73199998779297 ; top5 ->  97.606  and loss:  53.35568854212761
test acc: top1 ->  28.54 ; top5 ->  83.64  and loss:  430.20962595939636
forward train acc: top1 ->  99.01399997802734 ; top5 ->  99.99  and loss:  5.561223283410072
test acc: top1 ->  88.87 ; top5 ->  97.74  and loss:  58.48990571498871
forward train acc: top1 ->  99.37599997802734 ; top5 ->  99.996  and loss:  2.8202947545796633
test acc: top1 ->  90.43 ; top5 ->  98.2  and loss:  53.10850267112255
forward train acc: top1 ->  99.602 ; top5 ->  99.998  and loss:  1.7668399270623922
test acc: top1 ->  90.35 ; top5 ->  98.33  and loss:  58.04612182080746
forward train acc: top1 ->  99.728 ; top5 ->  99.998  and loss:  1.2215550038963556
test acc: top1 ->  90.68 ; top5 ->  98.36  and loss:  57.127014964818954
forward train acc: top1 ->  99.74999997558594 ; top5 ->  100.0  and loss:  1.055119687691331
test acc: top1 ->  91.17 ; top5 ->  98.42  and loss:  56.176287427544594
forward train acc: top1 ->  99.7780000024414 ; top5 ->  99.998  and loss:  0.8696147091686726
test acc: top1 ->  91.22 ; top5 ->  98.42  and loss:  57.81272278726101
forward train acc: top1 ->  99.79999997558593 ; top5 ->  100.0  and loss:  0.7556984573602676
test acc: top1 ->  91.16 ; top5 ->  98.56  and loss:  57.56514063477516
forward train acc: top1 ->  99.81399997558594 ; top5 ->  100.0  and loss:  0.7127074357122183
test acc: top1 ->  91.37 ; top5 ->  98.47  and loss:  57.55421355366707
forward train acc: top1 ->  99.83399997558594 ; top5 ->  100.0  and loss:  0.6486848909407854
test acc: top1 ->  91.24 ; top5 ->  98.53  and loss:  60.040198251605034
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  96 / 512 , inc:  1
---------------- start layer  10  ---------------
### skip layer  10 wait:  4  ###
---------------- start layer  11  ---------------
adv train loss:  -1789.1267766952515 , diff:  1789.1267766952515
adv train loss:  -1972.8118705749512 , diff:  183.6850938796997
adv train loss:  -1972.4300651550293 , diff:  0.381805419921875
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  2
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  51868.338928222656
forward train acc: top1 ->  97.60999997558594 ; top5 ->  99.988  and loss:  10.947656558826566
test acc: top1 ->  90.29 ; top5 ->  97.79  and loss:  76.03240783512592
forward train acc: top1 ->  99.872 ; top5 ->  99.998  and loss:  0.6365655513945967
test acc: top1 ->  91.6 ; top5 ->  98.39  and loss:  63.032008558511734
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.3535429956391454
test acc: top1 ->  91.74 ; top5 ->  98.46  and loss:  62.44092842191458
forward train acc: top1 ->  99.898 ; top5 ->  100.0  and loss:  0.3817027760669589
test acc: top1 ->  91.86 ; top5 ->  98.35  and loss:  64.27331003546715
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.2896988966385834
test acc: top1 ->  91.9 ; top5 ->  98.43  and loss:  64.53496003895998
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.22863656550180167
test acc: top1 ->  91.98 ; top5 ->  98.5  and loss:  64.01242777705193
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.1762666183640249
test acc: top1 ->  92.14 ; top5 ->  98.52  and loss:  64.95007798075676
==> this epoch:  2 / 512
---------------- start layer  12  ---------------
adv train loss:  -191.238165974617 , diff:  191.238165974617
adv train loss:  -191.0907006263733 , diff:  0.14746534824371338
layer  12  adv train finish, try to retain  489
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -15415.683708190918 , diff:  15415.683708190918
adv train loss:  -25832.88673400879 , diff:  10417.203025817871
adv train loss:  -35993.20785522461 , diff:  10160.32112121582
adv train loss:  -46046.069427490234 , diff:  10052.861572265625
adv train loss:  -56016.81198120117 , diff:  9970.742553710938
adv train loss:  -65978.43975830078 , diff:  9961.62777709961
adv train loss:  -75890.12274169922 , diff:  9911.682983398438
adv train loss:  -85780.41662597656 , diff:  9890.293884277344
adv train loss:  -95649.49505615234 , diff:  9869.078430175781
adv train loss:  -105518.23760986328 , diff:  9868.742553710938
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  16
test acc: top1 ->  33.76 ; top5 ->  86.28  and loss:  619.1634674072266
forward train acc: top1 ->  92.504 ; top5 ->  99.66  and loss:  38.45187336113304
test acc: top1 ->  91.25 ; top5 ->  98.77  and loss:  47.7839091271162
forward train acc: top1 ->  99.86 ; top5 ->  100.0  and loss:  0.8846390414983034
test acc: top1 ->  91.41 ; top5 ->  98.81  and loss:  47.629786133766174
forward train acc: top1 ->  99.892 ; top5 ->  100.0  and loss:  0.5666736885905266
test acc: top1 ->  91.62 ; top5 ->  98.87  and loss:  48.6163479462266
forward train acc: top1 ->  99.93199997558594 ; top5 ->  100.0  and loss:  0.4095623204484582
test acc: top1 ->  91.88 ; top5 ->  98.78  and loss:  48.86762808263302
forward train acc: top1 ->  99.942 ; top5 ->  100.0  and loss:  0.33650095062330365
test acc: top1 ->  91.88 ; top5 ->  98.81  and loss:  49.11622419953346
forward train acc: top1 ->  99.936 ; top5 ->  100.0  and loss:  0.3365270538488403
test acc: top1 ->  91.89 ; top5 ->  98.84  and loss:  49.90383867919445
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  20 / 512 , inc:  4
layer  0  :  0.46875  ==>  30 / 64 , inc:  2
layer  1  :  0.875  ==>  56 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.3828125  ==>  98 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.041015625  ==>  21 / 512 , inc:  1
layer  9  :  0.1875  ==>  96 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.0390625  ==>  20 / 512 , inc:  2
eps [1.313681671142578, 0.4926306266784668, 0.9852612533569336, 0.9852612533569336, 0.9852612533569336, 0.9852612533569336, 0.9852612533569336, 0.9852612533569336, 0.08210510444641113, 0.18473648500442505, 0.9852612533569336, 1.7515755615234374, 0.4926306266784668, 10.509453369140624]  wait [0, 2, 3, 3, 3, 3, 4, 3, 2, 4, 3, 0, 2, 2]  inc [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]  tol: 6
$$$$$$$$$$$$$ epoch  52  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1481.2882318496704 , diff:  1481.2882318496704
adv train loss:  -1500.9009399414062 , diff:  19.61270809173584
adv train loss:  -1500.1794710159302 , diff:  0.7214689254760742
adv train loss:  -1507.5066499710083 , diff:  7.327178955078125
adv train loss:  -1533.6032180786133 , diff:  26.09656810760498
adv train loss:  -1532.1870031356812 , diff:  1.416214942932129
adv train loss:  -1531.777564048767 , diff:  0.4094390869140625
adv train loss:  -1530.1616525650024 , diff:  1.6159114837646484
adv train loss:  -1529.1214141845703 , diff:  1.040238380432129
adv train loss:  -1467.5893325805664 , diff:  61.532081604003906
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  28
test acc: top1 ->  10.09 ; top5 ->  50.0  and loss:  4854.721267700195
forward train acc: top1 ->  97.50799997558593 ; top5 ->  99.956  and loss:  9.664189482107759
test acc: top1 ->  89.68 ; top5 ->  98.78  and loss:  78.88340485095978
forward train acc: top1 ->  99.3660000048828 ; top5 ->  99.996  and loss:  2.2521008998155594
test acc: top1 ->  90.57 ; top5 ->  99.09  and loss:  66.929990619421
forward train acc: top1 ->  99.59599997558594 ; top5 ->  99.998  and loss:  1.3789915349334478
test acc: top1 ->  90.99 ; top5 ->  99.06  and loss:  64.51464505493641
forward train acc: top1 ->  99.65199997558594 ; top5 ->  100.0  and loss:  1.154594472143799
test acc: top1 ->  91.07 ; top5 ->  99.03  and loss:  62.1369815915823
forward train acc: top1 ->  99.682 ; top5 ->  100.0  and loss:  1.0339540047571063
test acc: top1 ->  91.22 ; top5 ->  99.17  and loss:  60.54041178524494
forward train acc: top1 ->  99.74999997558594 ; top5 ->  99.998  and loss:  0.7320918678306043
test acc: top1 ->  91.26 ; top5 ->  99.2  and loss:  61.35791489481926
forward train acc: top1 ->  99.782 ; top5 ->  99.998  and loss:  0.6925846581580117
test acc: top1 ->  91.3 ; top5 ->  99.22  and loss:  61.51262970268726
forward train acc: top1 ->  99.78399997558594 ; top5 ->  100.0  and loss:  0.6601035166531801
test acc: top1 ->  91.4 ; top5 ->  99.2  and loss:  60.90837052464485
forward train acc: top1 ->  99.788 ; top5 ->  99.998  and loss:  0.6417894957121462
test acc: top1 ->  91.44 ; top5 ->  99.2  and loss:  61.425896033644676
forward train acc: top1 ->  99.812 ; top5 ->  100.0  and loss:  0.6068917738739401
test acc: top1 ->  91.45 ; top5 ->  99.14  and loss:  61.23621906340122
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  30 / 64 , inc:  2
---------------- start layer  1  ---------------
adv train loss:  -0.2801485247910023 , diff:  0.2801485247910023
adv train loss:  -0.3277437749202363 , diff:  0.047595250129234046
adv train loss:  -0.32436307321768254 , diff:  0.003380701702553779
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  55
test acc: top1 ->  9.59 ; top5 ->  52.21  and loss:  1361.0126638412476
forward train acc: top1 ->  99.426 ; top5 ->  99.998  and loss:  1.8540013174060732
test acc: top1 ->  91.09 ; top5 ->  99.04  and loss:  60.57828130573034
forward train acc: top1 ->  99.62599997558594 ; top5 ->  99.996  and loss:  1.134544630534947
test acc: top1 ->  91.17 ; top5 ->  99.03  and loss:  60.02331652492285
forward train acc: top1 ->  99.73999997558593 ; top5 ->  99.998  and loss:  0.8027616068720818
test acc: top1 ->  91.23 ; top5 ->  99.12  and loss:  58.53381360322237
forward train acc: top1 ->  99.8020000024414 ; top5 ->  100.0  and loss:  0.5970757603645325
test acc: top1 ->  91.28 ; top5 ->  99.13  and loss:  60.63795583695173
forward train acc: top1 ->  99.798 ; top5 ->  100.0  and loss:  0.6007079400587827
test acc: top1 ->  91.33 ; top5 ->  99.11  and loss:  60.671864442527294
forward train acc: top1 ->  99.8220000024414 ; top5 ->  100.0  and loss:  0.5804458260536194
test acc: top1 ->  91.28 ; top5 ->  99.24  and loss:  59.29011032730341
forward train acc: top1 ->  99.77999997558594 ; top5 ->  100.0  and loss:  0.6058887150138617
test acc: top1 ->  91.49 ; top5 ->  99.24  and loss:  59.733193673193455
forward train acc: top1 ->  99.8380000024414 ; top5 ->  100.0  and loss:  0.4369660587981343
test acc: top1 ->  91.47 ; top5 ->  99.12  and loss:  60.71045042574406
forward train acc: top1 ->  99.82 ; top5 ->  100.0  and loss:  0.4936352908844128
test acc: top1 ->  91.51 ; top5 ->  99.19  and loss:  58.95029832422733
forward train acc: top1 ->  99.86399997558594 ; top5 ->  100.0  and loss:  0.40752729889936745
test acc: top1 ->  91.54 ; top5 ->  99.18  and loss:  60.22697265446186
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  56 / 64 , inc:  1
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
adv train loss:  -43.63114070892334 , diff:  43.63114070892334
adv train loss:  -43.6887429356575 , diff:  0.05760222673416138
layer  8  adv train finish, try to retain  396
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
### skip layer  9 wait:  4  ###
---------------- start layer  10  ---------------
### skip layer  10 wait:  3  ###
---------------- start layer  11  ---------------
adv train loss:  -1524.7176580429077 , diff:  1524.7176580429077
adv train loss:  -1686.2350206375122 , diff:  161.5173625946045
adv train loss:  -1743.7757759094238 , diff:  57.54075527191162
adv train loss:  -1745.4316387176514 , diff:  1.655862808227539
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  1
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  29830.274658203125
forward train acc: top1 ->  96.11399997802734 ; top5 ->  99.946  and loss:  17.58407024294138
test acc: top1 ->  76.7 ; top5 ->  97.79  and loss:  92.23010164499283
forward train acc: top1 ->  99.3560000024414 ; top5 ->  99.998  and loss:  3.6321765892207623
test acc: top1 ->  89.84 ; top5 ->  97.99  and loss:  59.93010403215885
forward train acc: top1 ->  99.574 ; top5 ->  99.998  and loss:  2.1083556283265352
test acc: top1 ->  90.22 ; top5 ->  97.88  and loss:  63.57069183886051
forward train acc: top1 ->  99.61799997558593 ; top5 ->  99.996  and loss:  1.6749427933245897
test acc: top1 ->  90.23 ; top5 ->  98.04  and loss:  66.27355943620205
forward train acc: top1 ->  99.704 ; top5 ->  100.0  and loss:  1.2894085785374045
test acc: top1 ->  90.13 ; top5 ->  98.01  and loss:  68.91138628125191
forward train acc: top1 ->  99.726 ; top5 ->  99.998  and loss:  1.0832769968546927
test acc: top1 ->  90.5 ; top5 ->  97.95  and loss:  66.43780295550823
forward train acc: top1 ->  99.732 ; top5 ->  99.994  and loss:  1.0672572685871273
test acc: top1 ->  90.49 ; top5 ->  97.99  and loss:  67.1795072555542
forward train acc: top1 ->  99.782 ; top5 ->  100.0  and loss:  0.9442012263461947
test acc: top1 ->  90.52 ; top5 ->  97.9  and loss:  67.46472550928593
forward train acc: top1 ->  99.752 ; top5 ->  99.998  and loss:  0.958983103511855
test acc: top1 ->  90.47 ; top5 ->  97.97  and loss:  69.27094876766205
forward train acc: top1 ->  99.82 ; top5 ->  100.0  and loss:  0.733232144266367
test acc: top1 ->  90.52 ; top5 ->  98.0  and loss:  69.11006373167038
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  2 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -393.3005187511444 , diff:  393.3005187511444
adv train loss:  -394.5715265274048 , diff:  1.271007776260376
adv train loss:  -394.09722995758057 , diff:  0.47429656982421875
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  40
test acc: top1 ->  10.0 ; top5 ->  63.03  and loss:  4604.7830810546875
forward train acc: top1 ->  35.1419999987793 ; top5 ->  89.73600000732422  and loss:  329.6095906496048
test acc: top1 ->  40.44 ; top5 ->  96.33  and loss:  148.29741895198822
forward train acc: top1 ->  53.1399999975586 ; top5 ->  98.02  and loss:  106.69175148010254
test acc: top1 ->  57.78 ; top5 ->  97.62  and loss:  118.49010586738586
forward train acc: top1 ->  63.72799998779297 ; top5 ->  99.97  and loss:  95.94929522275925
test acc: top1 ->  59.37 ; top5 ->  97.52  and loss:  115.20470380783081
forward train acc: top1 ->  66.86000000976563 ; top5 ->  99.978  and loss:  91.32081574201584
test acc: top1 ->  62.16 ; top5 ->  97.55  and loss:  113.11258345842361
forward train acc: top1 ->  69.87600000976562 ; top5 ->  99.97599997558594  and loss:  87.45514541864395
test acc: top1 ->  63.89 ; top5 ->  97.64  and loss:  112.09197467565536
forward train acc: top1 ->  74.35400001708985 ; top5 ->  99.976  and loss:  83.74033915996552
test acc: top1 ->  66.09 ; top5 ->  97.59  and loss:  108.79480773210526
forward train acc: top1 ->  76.58199998779297 ; top5 ->  99.972  and loss:  80.31236153841019
test acc: top1 ->  67.84 ; top5 ->  97.56  and loss:  107.03151953220367
forward train acc: top1 ->  78.09999998779297 ; top5 ->  99.986  and loss:  77.68797034025192
test acc: top1 ->  70.85 ; top5 ->  97.62  and loss:  105.39686489105225
forward train acc: top1 ->  79.19999997558594 ; top5 ->  99.984  and loss:  74.92634928226471
test acc: top1 ->  69.06 ; top5 ->  97.64  and loss:  104.15161114931107
forward train acc: top1 ->  80.37599997558594 ; top5 ->  99.986  and loss:  72.50947088003159
test acc: top1 ->  71.63 ; top5 ->  97.59  and loss:  103.59752023220062
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  41 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -24011.410736083984 , diff:  24011.410736083984
adv train loss:  -40653.48956298828 , diff:  16642.078826904297
adv train loss:  -56668.6985168457 , diff:  16015.208953857422
adv train loss:  -72432.25274658203 , diff:  15763.554229736328
adv train loss:  -88048.76727294922 , diff:  15616.514526367188
adv train loss:  -103611.84899902344 , diff:  15563.081726074219
adv train loss:  -119095.29223632812 , diff:  15483.443237304688
adv train loss:  -134535.30114746094 , diff:  15440.008911132812
adv train loss:  -148963.8759765625 , diff:  14428.574829101562
adv train loss:  -158436.8494873047 , diff:  9472.973510742188
layer  13  adv train finish, try to retain  33
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.46875  ==>  30 / 64 , inc:  1
layer  1  :  0.875  ==>  56 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.3828125  ==>  98 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.041015625  ==>  21 / 512 , inc:  1
layer  9  :  0.1875  ==>  96 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.0390625  ==>  20 / 512 , inc:  2
eps [0.9852612533569336, 0.3694729700088501, 0.9852612533569336, 0.9852612533569336, 0.9852612533569336, 0.9852612533569336, 0.9852612533569336, 0.9852612533569336, 0.16421020889282226, 0.18473648500442505, 0.9852612533569336, 1.313681671142578, 0.3694729700088501, 21.01890673828125]  wait [2, 4, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 4, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]  tol: 6
$$$$$$$$$$$$$ epoch  53  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -694.5990881919861 , diff:  694.5990881919861
adv train loss:  -694.2985415458679 , diff:  0.30054664611816406
layer  0  adv train finish, try to retain  59
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -631.3018398284912 , diff:  631.3018398284912
adv train loss:  -628.8982343673706 , diff:  2.4036054611206055
adv train loss:  -631.5469517707825 , diff:  2.6487174034118652
adv train loss:  -631.23548412323 , diff:  0.31146764755249023
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -630.0389699935913 , diff:  630.0389699935913
adv train loss:  -630.5734496116638 , diff:  0.5344796180725098
layer  2  adv train finish, try to retain  122
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -630.5858945846558 , diff:  630.5858945846558
adv train loss:  -629.0027289390564 , diff:  1.5831656455993652
adv train loss:  -630.555422782898 , diff:  1.5526938438415527
adv train loss:  -630.0665616989136 , diff:  0.488861083984375
layer  3  adv train finish, try to retain  124
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -631.2804765701294 , diff:  631.2804765701294
adv train loss:  -630.2339925765991 , diff:  1.0464839935302734
layer  4  adv train finish, try to retain  222
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -630.2431259155273 , diff:  630.2431259155273
adv train loss:  -630.3470678329468 , diff:  0.1039419174194336
layer  5  adv train finish, try to retain  229
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -614.5583939552307 , diff:  614.5583939552307
adv train loss:  -613.2820315361023 , diff:  1.276362419128418
adv train loss:  -614.0695734024048 , diff:  0.7875418663024902
layer  6  adv train finish, try to retain  231
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -630.4584498405457 , diff:  630.4584498405457
adv train loss:  -631.4856867790222 , diff:  1.0272369384765625
layer  7  adv train finish, try to retain  375
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -1173.8508491516113 , diff:  1173.8508491516113
adv train loss:  -1173.270962715149 , diff:  0.5798864364624023
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  20
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  3007.337055206299
forward train acc: top1 ->  87.37399998291015 ; top5 ->  99.73799997558594  and loss:  79.25467336550355
test acc: top1 ->  89.13 ; top5 ->  98.7  and loss:  68.36366827785969
forward train acc: top1 ->  98.91000000244141 ; top5 ->  99.996  and loss:  4.412479639053345
test acc: top1 ->  89.97 ; top5 ->  98.73  and loss:  66.455960765481
forward train acc: top1 ->  99.30199997558594 ; top5 ->  99.998  and loss:  2.5684149470180273
test acc: top1 ->  90.28 ; top5 ->  98.81  and loss:  67.49819880723953
forward train acc: top1 ->  99.47600000244141 ; top5 ->  99.998  and loss:  1.8506553899496794
test acc: top1 ->  90.36 ; top5 ->  98.77  and loss:  67.0301760584116
forward train acc: top1 ->  99.496 ; top5 ->  100.0  and loss:  1.7229412733577192
test acc: top1 ->  90.33 ; top5 ->  98.74  and loss:  68.93310305476189
forward train acc: top1 ->  99.62600000488281 ; top5 ->  100.0  and loss:  1.3468383885920048
test acc: top1 ->  90.32 ; top5 ->  98.78  and loss:  68.5313019156456
forward train acc: top1 ->  99.68599997558594 ; top5 ->  100.0  and loss:  1.1497535714879632
test acc: top1 ->  90.51 ; top5 ->  98.76  and loss:  69.1203336417675
forward train acc: top1 ->  99.65800000244141 ; top5 ->  100.0  and loss:  1.1276594121009111
test acc: top1 ->  90.49 ; top5 ->  98.78  and loss:  69.98041813075542
forward train acc: top1 ->  99.674 ; top5 ->  100.0  and loss:  1.0288250986486673
test acc: top1 ->  90.51 ; top5 ->  98.77  and loss:  70.80678699910641
forward train acc: top1 ->  99.708 ; top5 ->  100.0  and loss:  1.0227823639288545
test acc: top1 ->  90.57 ; top5 ->  98.73  and loss:  70.75865191221237
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  21 / 512 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -45.566914945840836 , diff:  45.566914945840836
adv train loss:  -45.59123092889786 , diff:  0.024315983057022095
layer  9  adv train finish, try to retain  411
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -278.0835602283478 , diff:  278.0835602283478
adv train loss:  -278.5840759277344 , diff:  0.5005156993865967
layer  10  adv train finish, try to retain  475
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -2353.2335815429688 , diff:  2353.2335815429688
adv train loss:  -2543.5785751342773 , diff:  190.3449935913086
adv train loss:  -2542.257272720337 , diff:  1.3213024139404297
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  1
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  24458.570419311523
forward train acc: top1 ->  95.092 ; top5 ->  99.888  and loss:  21.696368099190295
test acc: top1 ->  53.34 ; top5 ->  95.16  and loss:  311.42104041576385
forward train acc: top1 ->  99.576 ; top5 ->  99.994  and loss:  1.4777400884777308
test acc: top1 ->  90.16 ; top5 ->  97.95  and loss:  94.3971375823021
forward train acc: top1 ->  99.68 ; top5 ->  99.998  and loss:  1.0941686915466562
test acc: top1 ->  90.19 ; top5 ->  98.03  and loss:  94.28281661868095
forward train acc: top1 ->  99.724 ; top5 ->  99.996  and loss:  1.022883491590619
test acc: top1 ->  90.63 ; top5 ->  98.01  and loss:  91.25144945085049
forward train acc: top1 ->  99.7300000024414 ; top5 ->  99.998  and loss:  0.9943580999970436
test acc: top1 ->  90.61 ; top5 ->  98.0  and loss:  90.04377157986164
forward train acc: top1 ->  99.808 ; top5 ->  99.998  and loss:  0.6977662130957469
test acc: top1 ->  90.73 ; top5 ->  98.05  and loss:  89.94317464530468
forward train acc: top1 ->  99.8 ; top5 ->  99.996  and loss:  0.7038596883649006
test acc: top1 ->  90.8 ; top5 ->  98.01  and loss:  89.69908218085766
forward train acc: top1 ->  99.8280000024414 ; top5 ->  99.998  and loss:  0.5722367782145739
test acc: top1 ->  90.84 ; top5 ->  98.02  and loss:  90.28928536176682
forward train acc: top1 ->  99.812 ; top5 ->  99.994  and loss:  0.707919959211722
test acc: top1 ->  90.9 ; top5 ->  98.09  and loss:  88.79652318358421
forward train acc: top1 ->  99.828 ; top5 ->  100.0  and loss:  0.5993654942139983
test acc: top1 ->  90.91 ; top5 ->  98.06  and loss:  88.64510110020638
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  2 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -536.045820236206 , diff:  536.045820236206
adv train loss:  -536.461501121521 , diff:  0.4156808853149414
layer  12  adv train finish, try to retain  489
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -21474.81886291504 , diff:  21474.81886291504
adv train loss:  -35587.478576660156 , diff:  14112.659713745117
adv train loss:  -49222.8971862793 , diff:  13635.41860961914
adv train loss:  -62694.144104003906 , diff:  13471.24691772461
adv train loss:  -76088.78875732422 , diff:  13394.644653320312
adv train loss:  -89436.60748291016 , diff:  13347.818725585938
adv train loss:  -102779.74822998047 , diff:  13343.140747070312
adv train loss:  -116069.38452148438 , diff:  13289.636291503906
adv train loss:  -129369.16723632812 , diff:  13299.78271484375
adv train loss:  -142659.35986328125 , diff:  13290.192626953125
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  18
test acc: top1 ->  27.17 ; top5 ->  87.55  and loss:  936.0791382789612
forward train acc: top1 ->  89.092 ; top5 ->  99.564  and loss:  76.96305386163294
test acc: top1 ->  90.52 ; top5 ->  98.59  and loss:  60.821281746029854
forward train acc: top1 ->  99.78800000244141 ; top5 ->  99.998  and loss:  1.5206347834318876
test acc: top1 ->  90.77 ; top5 ->  98.54  and loss:  61.2765326872468
forward train acc: top1 ->  99.824 ; top5 ->  100.0  and loss:  1.0693653537891805
test acc: top1 ->  90.92 ; top5 ->  98.56  and loss:  61.25119675695896
forward train acc: top1 ->  99.904 ; top5 ->  100.0  and loss:  0.6593640025239438
test acc: top1 ->  91.02 ; top5 ->  98.55  and loss:  62.30011035501957
forward train acc: top1 ->  99.90199997558594 ; top5 ->  100.0  and loss:  0.5603034421801567
test acc: top1 ->  91.31 ; top5 ->  98.61  and loss:  62.71340271830559
forward train acc: top1 ->  99.894 ; top5 ->  100.0  and loss:  0.5176466973498464
test acc: top1 ->  91.32 ; top5 ->  98.55  and loss:  63.26851850748062
forward train acc: top1 ->  99.91399997558594 ; top5 ->  100.0  and loss:  0.45926143880933523
test acc: top1 ->  91.45 ; top5 ->  98.59  and loss:  63.676544308662415
forward train acc: top1 ->  99.918 ; top5 ->  100.0  and loss:  0.4326527356170118
test acc: top1 ->  91.43 ; top5 ->  98.59  and loss:  63.89899002015591
forward train acc: top1 ->  99.94 ; top5 ->  100.0  and loss:  0.3112148134969175
test acc: top1 ->  91.62 ; top5 ->  98.6  and loss:  64.0642921924591
forward train acc: top1 ->  99.94 ; top5 ->  100.0  and loss:  0.3160136032383889
test acc: top1 ->  91.55 ; top5 ->  98.56  and loss:  64.9980051368475
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  20 / 512 , inc:  2
layer  0  :  0.46875  ==>  30 / 64 , inc:  1
layer  1  :  0.875  ==>  56 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.3828125  ==>  98 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.041015625  ==>  21 / 512 , inc:  1
layer  9  :  0.1875  ==>  96 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.0390625  ==>  20 / 512 , inc:  1
eps [1.9705225067138672, 0.7389459400177002, 1.9705225067138672, 1.9705225067138672, 1.9705225067138672, 1.9705225067138672, 1.9705225067138672, 1.9705225067138672, 0.1231576566696167, 0.3694729700088501, 1.9705225067138672, 0.9852612533569336, 0.7389459400177002, 15.764180053710938]  wait [0, 2, 0, 0, 0, 0, 1, 0, 2, 1, 0, 2, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 7
$$$$$$$$$$$$$ epoch  54  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1436.2985639572144 , diff:  1436.2985639572144
adv train loss:  -1435.7302284240723 , diff:  0.5683355331420898
adv train loss:  -1446.538667678833 , diff:  10.808439254760742
adv train loss:  -1438.6582374572754 , diff:  7.880430221557617
adv train loss:  -1447.0630512237549 , diff:  8.404813766479492
adv train loss:  -1434.6888847351074 , diff:  12.374166488647461
adv train loss:  -1437.5328426361084 , diff:  2.8439579010009766
adv train loss:  -1442.2568712234497 , diff:  4.724028587341309
adv train loss:  -1441.5524396896362 , diff:  0.7044315338134766
adv train loss:  -1453.3395986557007 , diff:  11.787158966064453
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  29
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  449298.0773925781
forward train acc: top1 ->  98.43200000488281 ; top5 ->  99.922  and loss:  8.154173742979765
test acc: top1 ->  88.88 ; top5 ->  98.43  and loss:  102.38724881410599
forward train acc: top1 ->  99.10399997558594 ; top5 ->  99.982  and loss:  3.706943167373538
test acc: top1 ->  90.66 ; top5 ->  98.95  and loss:  73.65319581329823
forward train acc: top1 ->  99.31599997802735 ; top5 ->  99.996  and loss:  2.403310537338257
test acc: top1 ->  90.9 ; top5 ->  99.02  and loss:  68.52865213155746
forward train acc: top1 ->  99.49599997558593 ; top5 ->  99.998  and loss:  1.569825567305088
test acc: top1 ->  90.96 ; top5 ->  99.03  and loss:  66.44724096357822
forward train acc: top1 ->  99.5720000024414 ; top5 ->  99.998  and loss:  1.425748116336763
test acc: top1 ->  91.04 ; top5 ->  99.14  and loss:  65.91162087023258
forward train acc: top1 ->  99.64000000244141 ; top5 ->  99.996  and loss:  1.1904184892773628
test acc: top1 ->  91.17 ; top5 ->  99.13  and loss:  64.9965718537569
forward train acc: top1 ->  99.612 ; top5 ->  99.998  and loss:  1.175107405113522
test acc: top1 ->  91.08 ; top5 ->  99.2  and loss:  63.030622974038124
forward train acc: top1 ->  99.678 ; top5 ->  100.0  and loss:  1.0116986848879606
test acc: top1 ->  91.01 ; top5 ->  99.13  and loss:  63.37738284468651
forward train acc: top1 ->  99.7340000024414 ; top5 ->  100.0  and loss:  0.8734980616718531
test acc: top1 ->  91.06 ; top5 ->  99.15  and loss:  63.56703210622072
forward train acc: top1 ->  99.73999997558593 ; top5 ->  100.0  and loss:  0.78544832020998
test acc: top1 ->  91.11 ; top5 ->  99.19  and loss:  61.654830783605576
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  30 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -0.27751070098020136 , diff:  0.27751070098020136
adv train loss:  -0.25926501862704754 , diff:  0.018245682353153825
adv train loss:  -0.2799786932300776 , diff:  0.020713674603030086
adv train loss:  -0.3214009397197515 , diff:  0.04142224648967385
adv train loss:  -0.2693425283941906 , diff:  0.05205841132556088
adv train loss:  -0.3037770303490106 , diff:  0.03443450195482001
adv train loss:  -0.33954531932249665 , diff:  0.03576828897348605
adv train loss:  -0.2546834107488394 , diff:  0.08486190857365727
adv train loss:  -0.3097737626521848 , diff:  0.055090351903345436
adv train loss:  -0.2910070290090516 , diff:  0.018766733643133193
layer  1  adv train finish, try to retain  64
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -878.5252390913665 , diff:  878.5252390913665
adv train loss:  -1443.5804567337036 , diff:  565.0552176423371
adv train loss:  -1458.8003244400024 , diff:  15.219867706298828
adv train loss:  -1463.5036725997925 , diff:  4.703348159790039
adv train loss:  -1461.955750465393 , diff:  1.547922134399414
adv train loss:  -1458.860345840454 , diff:  3.095404624938965
adv train loss:  -1472.3852109909058 , diff:  13.52486515045166
adv train loss:  -1477.1984996795654 , diff:  4.813288688659668
adv train loss:  -1476.1761865615845 , diff:  1.022313117980957
adv train loss:  -1482.0959005355835 , diff:  5.919713973999023
layer  2  adv train finish, try to retain  44
test acc: top1 ->  12.06 ; top5 ->  52.35  and loss:  1258.399582862854
forward train acc: top1 ->  89.14799998046875 ; top5 ->  98.97999998046875  and loss:  41.31259140372276
test acc: top1 ->  83.34 ; top5 ->  97.86  and loss:  62.900579676032066
forward train acc: top1 ->  91.29400000732421 ; top5 ->  99.43799997802735  and loss:  27.475981906056404
test acc: top1 ->  85.01 ; top5 ->  98.35  and loss:  53.77420927584171
forward train acc: top1 ->  92.64199997802734 ; top5 ->  99.63199997558594  and loss:  22.549437075853348
test acc: top1 ->  85.91 ; top5 ->  98.54  and loss:  52.337828546762466
forward train acc: top1 ->  93.64200001953125 ; top5 ->  99.7280000024414  and loss:  19.20356035232544
test acc: top1 ->  86.15 ; top5 ->  98.67  and loss:  49.49888452887535
forward train acc: top1 ->  94.18999997070313 ; top5 ->  99.76  and loss:  17.70663784444332
test acc: top1 ->  86.69 ; top5 ->  98.66  and loss:  49.46347486972809
forward train acc: top1 ->  94.7299999975586 ; top5 ->  99.80199997802734  and loss:  16.097103379666805
test acc: top1 ->  87.07 ; top5 ->  98.8  and loss:  46.97227267920971
forward train acc: top1 ->  94.96799999511718 ; top5 ->  99.80999997558594  and loss:  15.232691213488579
test acc: top1 ->  87.13 ; top5 ->  98.8  and loss:  46.987679079174995
forward train acc: top1 ->  95.09000001953125 ; top5 ->  99.82  and loss:  14.79377867281437
test acc: top1 ->  87.32 ; top5 ->  98.76  and loss:  46.772696793079376
forward train acc: top1 ->  95.26199999267578 ; top5 ->  99.83599997558593  and loss:  14.068438731133938
test acc: top1 ->  87.55 ; top5 ->  98.88  and loss:  47.29983472824097
forward train acc: top1 ->  95.38799999267579 ; top5 ->  99.86600000244141  and loss:  13.840874217450619
test acc: top1 ->  87.56 ; top5 ->  98.89  and loss:  45.43976305425167
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -441.2337579391897 , diff:  441.2337579391897
adv train loss:  -805.7740812301636 , diff:  364.5403232909739
adv train loss:  -815.800229549408 , diff:  10.026148319244385
adv train loss:  -823.1809978485107 , diff:  7.380768299102783
adv train loss:  -824.2585916519165 , diff:  1.0775938034057617
adv train loss:  -831.8855395317078 , diff:  7.62694787979126
adv train loss:  -829.411684513092 , diff:  2.4738550186157227
adv train loss:  -830.1670913696289 , diff:  0.7554068565368652
adv train loss:  -829.7644939422607 , diff:  0.40259742736816406
adv train loss:  -830.4222354888916 , diff:  0.6577415466308594
layer  3  adv train finish, try to retain  25
test acc: top1 ->  11.47 ; top5 ->  50.32  and loss:  464.9076154232025
forward train acc: top1 ->  73.57599999023438 ; top5 ->  96.61399998535157  and loss:  81.88414829969406
test acc: top1 ->  73.9 ; top5 ->  97.09  and loss:  79.63273561000824
forward train acc: top1 ->  78.37999998535156 ; top5 ->  97.85000000976562  and loss:  64.401830971241
test acc: top1 ->  76.96 ; top5 ->  97.52  and loss:  72.79646497964859
forward train acc: top1 ->  81.01800000488281 ; top5 ->  98.38199997802734  and loss:  56.665153324604034
test acc: top1 ->  78.62 ; top5 ->  97.78  and loss:  67.37429913878441
forward train acc: top1 ->  82.33200001220703 ; top5 ->  98.61800000488282  and loss:  52.183496594429016
test acc: top1 ->  79.45 ; top5 ->  98.01  and loss:  65.32459765672684
forward train acc: top1 ->  83.64000000976563 ; top5 ->  98.78399998046875  and loss:  48.520579159259796
test acc: top1 ->  80.51 ; top5 ->  97.95  and loss:  63.033724427223206
forward train acc: top1 ->  84.44000001220704 ; top5 ->  98.9000000024414  and loss:  46.07000553607941
test acc: top1 ->  80.94 ; top5 ->  98.15  and loss:  61.03701615333557
forward train acc: top1 ->  84.58000000976563 ; top5 ->  98.89000000488281  and loss:  45.1497808098793
test acc: top1 ->  81.29 ; top5 ->  98.26  and loss:  60.00919136404991
forward train acc: top1 ->  85.0 ; top5 ->  98.96599997558594  and loss:  43.90116110444069
test acc: top1 ->  81.58 ; top5 ->  98.29  and loss:  58.780619859695435
forward train acc: top1 ->  85.3559999975586 ; top5 ->  99.07599997558594  and loss:  42.90233442187309
test acc: top1 ->  81.9 ; top5 ->  98.31  and loss:  58.25913053750992
forward train acc: top1 ->  85.88999998046874 ; top5 ->  99.10399997558594  and loss:  41.660560578107834
test acc: top1 ->  82.11 ; top5 ->  98.37  and loss:  57.83678925037384
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -81.27848271653056 , diff:  81.27848271653056
adv train loss:  -562.1845486164093 , diff:  480.90606589987874
adv train loss:  -684.1482973098755 , diff:  121.96374869346619
adv train loss:  -720.7589297294617 , diff:  36.61063241958618
adv train loss:  -730.1049165725708 , diff:  9.34598684310913
adv train loss:  -731.5717749595642 , diff:  1.4668583869934082
adv train loss:  -731.7324857711792 , diff:  0.16071081161499023
adv train loss:  -733.3300309181213 , diff:  1.5975451469421387
adv train loss:  -732.634307384491 , diff:  0.6957235336303711
adv train loss:  -732.9347429275513 , diff:  0.30043554306030273
layer  4  adv train finish, try to retain  17
test acc: top1 ->  10.0 ; top5 ->  51.57  and loss:  866.3283519744873
forward train acc: top1 ->  61.4279999987793 ; top5 ->  94.53800001708984  and loss:  111.75159353017807
test acc: top1 ->  65.61 ; top5 ->  96.01  and loss:  100.54797160625458
forward train acc: top1 ->  69.07000000488281 ; top5 ->  96.73799998779297  and loss:  88.00442808866501
test acc: top1 ->  70.15 ; top5 ->  96.83  and loss:  89.35340130329132
forward train acc: top1 ->  72.27399998779296 ; top5 ->  97.42799998046875  and loss:  78.68636125326157
test acc: top1 ->  72.38 ; top5 ->  97.26  and loss:  82.56951802968979
forward train acc: top1 ->  74.21399999755859 ; top5 ->  97.83199998046875  and loss:  72.99864226579666
test acc: top1 ->  74.19 ; top5 ->  97.43  and loss:  78.07399368286133
forward train acc: top1 ->  75.94599997070313 ; top5 ->  98.03599998535157  and loss:  68.75569158792496
test acc: top1 ->  74.95 ; top5 ->  97.79  and loss:  74.83647167682648
forward train acc: top1 ->  76.81999998046875 ; top5 ->  98.21599998291016  and loss:  65.69539922475815
test acc: top1 ->  75.61 ; top5 ->  97.83  and loss:  73.18802431225777
forward train acc: top1 ->  77.22799999267578 ; top5 ->  98.18999997802734  and loss:  64.6697244644165
test acc: top1 ->  75.78 ; top5 ->  97.8  and loss:  72.53373342752457
forward train acc: top1 ->  78.08200000488281 ; top5 ->  98.3540000048828  and loss:  62.908893048763275
test acc: top1 ->  76.23 ; top5 ->  97.95  and loss:  71.59763273596764
forward train acc: top1 ->  78.27000001708984 ; top5 ->  98.43200000732422  and loss:  61.945176899433136
test acc: top1 ->  76.46 ; top5 ->  97.96  and loss:  70.25889959931374
forward train acc: top1 ->  78.62200000976563 ; top5 ->  98.49800000976562  and loss:  60.63208842277527
test acc: top1 ->  77.07 ; top5 ->  98.07  and loss:  69.13180375099182
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  156 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -120.00151324272156 , diff:  120.00151324272156
adv train loss:  -591.9669301509857 , diff:  471.96541690826416
adv train loss:  -756.4673910140991 , diff:  164.5004608631134
adv train loss:  -759.5955963134766 , diff:  3.1282052993774414
adv train loss:  -762.6156511306763 , diff:  3.020054817199707
adv train loss:  -772.9394474029541 , diff:  10.323796272277832
adv train loss:  -772.9022855758667 , diff:  0.037161827087402344
adv train loss:  -774.5588455200195 , diff:  1.656559944152832
adv train loss:  -776.0063700675964 , diff:  1.4475245475769043
adv train loss:  -775.9677877426147 , diff:  0.03858232498168945
layer  5  adv train finish, try to retain  19
test acc: top1 ->  10.17 ; top5 ->  50.29  and loss:  563.1498656272888
forward train acc: top1 ->  75.16000000732421 ; top5 ->  98.03799997802734  and loss:  71.69476771354675
test acc: top1 ->  75.22 ; top5 ->  97.77  and loss:  75.99403294920921
forward train acc: top1 ->  81.11399997314453 ; top5 ->  98.89200000488282  and loss:  53.965948432683945
test acc: top1 ->  77.7 ; top5 ->  98.2  and loss:  67.40598428249359
forward train acc: top1 ->  83.29799998535157 ; top5 ->  99.15799998046874  and loss:  47.53717538714409
test acc: top1 ->  79.92 ; top5 ->  98.36  and loss:  61.65444138646126
forward train acc: top1 ->  84.58800000732423 ; top5 ->  99.30999997558594  and loss:  43.63624945282936
test acc: top1 ->  80.88 ; top5 ->  98.51  and loss:  59.03127056360245
forward train acc: top1 ->  85.58199999511719 ; top5 ->  99.36000000244141  and loss:  40.74021050333977
test acc: top1 ->  80.04 ; top5 ->  98.4  and loss:  63.92897033691406
forward train acc: top1 ->  86.38800001464844 ; top5 ->  99.40200000976563  and loss:  38.97985756397247
test acc: top1 ->  81.94 ; top5 ->  98.61  and loss:  57.45123064517975
forward train acc: top1 ->  86.652 ; top5 ->  99.44000000244141  and loss:  38.08794194459915
test acc: top1 ->  82.24 ; top5 ->  98.7  and loss:  55.72846442461014
forward train acc: top1 ->  87.1360000024414 ; top5 ->  99.48599997558594  and loss:  36.91583847999573
test acc: top1 ->  82.33 ; top5 ->  98.74  and loss:  55.03814426064491
forward train acc: top1 ->  87.3800000024414 ; top5 ->  99.51600000488281  and loss:  35.67217367887497
test acc: top1 ->  82.83 ; top5 ->  98.8  and loss:  54.21538779139519
forward train acc: top1 ->  87.65399998046875 ; top5 ->  99.51800000244141  and loss:  35.206589460372925
test acc: top1 ->  82.87 ; top5 ->  98.78  and loss:  54.25469180941582
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -175.6615571603179 , diff:  175.6615571603179
adv train loss:  -1048.1970024108887 , diff:  872.5354452505708
adv train loss:  -1093.3565006256104 , diff:  45.15949821472168
adv train loss:  -1115.0338411331177 , diff:  21.677340507507324
adv train loss:  -1119.0531702041626 , diff:  4.019329071044922
adv train loss:  -1115.212963104248 , diff:  3.840207099914551
adv train loss:  -1114.084243774414 , diff:  1.1287193298339844
adv train loss:  -1116.9338960647583 , diff:  2.8496522903442383
adv train loss:  -1114.023057937622 , diff:  2.9108381271362305
adv train loss:  -1116.3260278701782 , diff:  2.3029699325561523
layer  6  adv train finish, try to retain  6
test acc: top1 ->  10.01 ; top5 ->  54.82  and loss:  908.8107228279114
forward train acc: top1 ->  72.65000001220703 ; top5 ->  98.55199997802734  and loss:  75.66944378614426
test acc: top1 ->  74.11 ; top5 ->  98.26  and loss:  77.38217902183533
forward train acc: top1 ->  80.92000001464844 ; top5 ->  99.2700000024414  and loss:  53.15651160478592
test acc: top1 ->  78.43 ; top5 ->  98.49  and loss:  68.44723549485207
forward train acc: top1 ->  84.12800000976563 ; top5 ->  99.46999997802735  and loss:  44.54581943154335
test acc: top1 ->  80.1 ; top5 ->  98.62  and loss:  63.813995122909546
forward train acc: top1 ->  85.89400000488281 ; top5 ->  99.5780000024414  and loss:  39.54315236210823
test acc: top1 ->  81.73 ; top5 ->  98.73  and loss:  59.472603380680084
forward train acc: top1 ->  87.42599997558594 ; top5 ->  99.60599997558593  and loss:  35.40231090784073
test acc: top1 ->  82.55 ; top5 ->  98.78  and loss:  57.93276500701904
forward train acc: top1 ->  88.45800001953126 ; top5 ->  99.67200000244141  and loss:  32.80936020612717
test acc: top1 ->  83.16 ; top5 ->  98.86  and loss:  56.00743389129639
forward train acc: top1 ->  88.98999997558593 ; top5 ->  99.7120000024414  and loss:  31.30909363925457
test acc: top1 ->  83.45 ; top5 ->  98.88  and loss:  55.367885410785675
forward train acc: top1 ->  89.24599997558593 ; top5 ->  99.68200000244141  and loss:  30.614098757505417
test acc: top1 ->  83.67 ; top5 ->  98.9  and loss:  54.43552851676941
forward train acc: top1 ->  89.58799998779297 ; top5 ->  99.70599997558594  and loss:  29.16212736070156
test acc: top1 ->  84.05 ; top5 ->  98.94  and loss:  53.236437767744064
forward train acc: top1 ->  89.90799998779296 ; top5 ->  99.75399997558594  and loss:  28.27347917854786
test acc: top1 ->  84.4 ; top5 ->  98.95  and loss:  53.14526331424713
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  98 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -8.808045737445354 , diff:  8.808045737445354
adv train loss:  -8.494647771120071 , diff:  0.31339796632528305
adv train loss:  -8.516671255230904 , diff:  0.022023484110832214
adv train loss:  -8.900941971689463 , diff:  0.38427071645855904
adv train loss:  -8.395629595965147 , diff:  0.5053123757243156
adv train loss:  -8.35166697949171 , diff:  0.043962616473436356
adv train loss:  -8.447983205318451 , diff:  0.09631622582674026
adv train loss:  -8.736833453178406 , diff:  0.28885024785995483
adv train loss:  -8.659900460392237 , diff:  0.07693299278616905
adv train loss:  -8.432414311915636 , diff:  0.22748614847660065
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  103
test acc: top1 ->  11.56 ; top5 ->  50.0  and loss:  2737566.423828125
forward train acc: top1 ->  97.8940000024414 ; top5 ->  99.98  and loss:  6.932627137750387
test acc: top1 ->  90.67 ; top5 ->  99.22  and loss:  42.7289464995265
forward train acc: top1 ->  98.90999997802734 ; top5 ->  99.992  and loss:  3.165425954386592
test acc: top1 ->  90.93 ; top5 ->  99.39  and loss:  46.12204346060753
forward train acc: top1 ->  99.27400000244141 ; top5 ->  99.996  and loss:  2.1993821635842323
test acc: top1 ->  91.03 ; top5 ->  99.39  and loss:  48.774907283484936
forward train acc: top1 ->  99.3620000024414 ; top5 ->  100.0  and loss:  1.8414739314466715
test acc: top1 ->  91.05 ; top5 ->  99.27  and loss:  50.38132081180811
forward train acc: top1 ->  99.43799997558594 ; top5 ->  99.996  and loss:  1.6476804772391915
test acc: top1 ->  91.07 ; top5 ->  99.41  and loss:  50.707664296031
forward train acc: top1 ->  99.574 ; top5 ->  99.998  and loss:  1.2502574920654297
test acc: top1 ->  91.32 ; top5 ->  99.42  and loss:  51.65553177893162
forward train acc: top1 ->  99.58399997558594 ; top5 ->  99.998  and loss:  1.2490158760920167
test acc: top1 ->  91.39 ; top5 ->  99.39  and loss:  51.517519012093544
forward train acc: top1 ->  99.606 ; top5 ->  100.0  and loss:  1.2127937988843769
test acc: top1 ->  91.18 ; top5 ->  99.43  and loss:  52.1449193879962
forward train acc: top1 ->  99.62799997558594 ; top5 ->  99.998  and loss:  1.0955310435965657
test acc: top1 ->  91.2 ; top5 ->  99.41  and loss:  53.01290277391672
forward train acc: top1 ->  99.63399997558594 ; top5 ->  100.0  and loss:  1.0642688078805804
test acc: top1 ->  91.42 ; top5 ->  99.36  and loss:  53.9655327424407
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  104 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -16.43514110147953 , diff:  16.43514110147953
adv train loss:  -16.630898036062717 , diff:  0.1957569345831871
adv train loss:  -16.894240468740463 , diff:  0.2633424326777458
adv train loss:  -16.636417903006077 , diff:  0.25782256573438644
adv train loss:  -16.56647615879774 , diff:  0.06994174420833588
adv train loss:  -17.015472251921892 , diff:  0.44899609312415123
adv train loss:  -16.884435459971428 , diff:  0.13103679195046425
adv train loss:  -16.846950002014637 , diff:  0.037485457956790924
adv train loss:  -16.915922939777374 , diff:  0.06897293776273727
adv train loss:  -16.937909200787544 , diff:  0.021986261010169983
layer  8  adv train finish, try to retain  410
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -5.1079957485198975 , diff:  5.1079957485198975
adv train loss:  -4.926650680601597 , diff:  0.18134506791830063
adv train loss:  -4.953652787953615 , diff:  0.027002107352018356
adv train loss:  -4.902149695903063 , diff:  0.05150309205055237
adv train loss:  -4.810651361942291 , diff:  0.09149833396077156
adv train loss:  -4.954521421343088 , diff:  0.1438700594007969
adv train loss:  -4.8061541728675365 , diff:  0.1483672484755516
adv train loss:  -4.887253321707249 , diff:  0.08109914883971214
adv train loss:  -4.992027286440134 , diff:  0.10477396473288536
adv train loss:  -4.9090253338217735 , diff:  0.08300195261836052
layer  9  adv train finish, try to retain  413
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -123.71638631820679 , diff:  123.71638631820679
adv train loss:  -124.03253769874573 , diff:  0.31615138053894043
adv train loss:  -124.28654646873474 , diff:  0.25400876998901367
adv train loss:  -123.53972631692886 , diff:  0.7468201518058777
adv train loss:  -124.19278109073639 , diff:  0.6530547738075256
adv train loss:  -123.540971159935 , diff:  0.6518099308013916
adv train loss:  -124.33780813217163 , diff:  0.7968369722366333
adv train loss:  -124.0675538778305 , diff:  0.2702542543411255
adv train loss:  -233.55638217926025 , diff:  109.48882830142975
adv train loss:  -257.2319498062134 , diff:  23.675567626953125
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  21
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  132089.77905273438
forward train acc: top1 ->  85.57000000732423 ; top5 ->  98.558  and loss:  71.6321263462305
test acc: top1 ->  90.03 ; top5 ->  98.94  and loss:  41.456163451075554
forward train acc: top1 ->  99.0220000024414 ; top5 ->  99.996  and loss:  5.085305720567703
test acc: top1 ->  91.26 ; top5 ->  99.08  and loss:  37.79142443090677
forward train acc: top1 ->  99.476 ; top5 ->  100.0  and loss:  2.498468428850174
test acc: top1 ->  91.57 ; top5 ->  99.03  and loss:  39.07791379839182
forward train acc: top1 ->  99.64599997558594 ; top5 ->  99.998  and loss:  1.6402495075017214
test acc: top1 ->  91.74 ; top5 ->  99.1  and loss:  40.01667623221874
forward train acc: top1 ->  99.772 ; top5 ->  100.0  and loss:  1.0828464184887707
test acc: top1 ->  92.0 ; top5 ->  99.16  and loss:  41.49010334908962
forward train acc: top1 ->  99.766 ; top5 ->  99.998  and loss:  0.9392376996111125
test acc: top1 ->  92.01 ; top5 ->  99.1  and loss:  42.804241225123405
forward train acc: top1 ->  99.85199997558594 ; top5 ->  100.0  and loss:  0.7198300315067172
test acc: top1 ->  92.02 ; top5 ->  99.14  and loss:  43.086105309426785
forward train acc: top1 ->  99.81 ; top5 ->  100.0  and loss:  0.7382142329588532
test acc: top1 ->  92.0 ; top5 ->  99.21  and loss:  43.862819999456406
forward train acc: top1 ->  99.846 ; top5 ->  100.0  and loss:  0.5968250380828977
test acc: top1 ->  92.09 ; top5 ->  99.24  and loss:  44.67074277997017
forward train acc: top1 ->  99.86799997558593 ; top5 ->  100.0  and loss:  0.5414811857044697
test acc: top1 ->  92.06 ; top5 ->  99.18  and loss:  45.594505310058594
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  22 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -454.0852937698364 , diff:  454.0852937698364
adv train loss:  -454.0908348560333 , diff:  0.005541086196899414
layer  11  adv train finish, try to retain  439
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -363.51226115226746 , diff:  363.51226115226746
adv train loss:  -362.67557859420776 , diff:  0.8366825580596924
adv train loss:  -362.4557538032532 , diff:  0.21982479095458984
layer  12  adv train finish, try to retain  488
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -23533.103240966797 , diff:  23533.103240966797
adv train loss:  -40019.68453979492 , diff:  16486.581298828125
adv train loss:  -55811.79089355469 , diff:  15792.106353759766
adv train loss:  -71332.53356933594 , diff:  15520.74267578125
adv train loss:  -86731.99926757812 , diff:  15399.465698242188
adv train loss:  -102027.6318359375 , diff:  15295.632568359375
adv train loss:  -117300.50329589844 , diff:  15272.871459960938
adv train loss:  -132518.03771972656 , diff:  15217.534423828125
adv train loss:  -147738.46337890625 , diff:  15220.425659179688
adv train loss:  -162920.31616210938 , diff:  15181.852783203125
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  19
test acc: top1 ->  25.61 ; top5 ->  88.34  and loss:  593.3687829971313
forward train acc: top1 ->  93.95599997802735 ; top5 ->  99.564  and loss:  26.651636263355613
test acc: top1 ->  91.7 ; top5 ->  99.14  and loss:  48.7917610257864
forward train acc: top1 ->  99.8220000024414 ; top5 ->  100.0  and loss:  0.8597267167642713
test acc: top1 ->  92.02 ; top5 ->  99.16  and loss:  49.368167132139206
forward train acc: top1 ->  99.84799997558594 ; top5 ->  100.0  and loss:  0.568877977784723
test acc: top1 ->  92.03 ; top5 ->  99.17  and loss:  50.111960239708424
forward train acc: top1 ->  99.882 ; top5 ->  100.0  and loss:  0.4664963202085346
test acc: top1 ->  92.03 ; top5 ->  99.19  and loss:  52.19794877618551
forward train acc: top1 ->  99.8920000024414 ; top5 ->  100.0  and loss:  0.40157997515052557
test acc: top1 ->  92.16 ; top5 ->  99.17  and loss:  52.9915362149477
==> this epoch:  19 / 512
layer  0  :  0.46875  ==>  30 / 64 , inc:  1
layer  1  :  0.875  ==>  56 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.3828125  ==>  98 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.041015625  ==>  21 / 512 , inc:  1
layer  9  :  0.1875  ==>  96 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.037109375  ==>  19 / 512 , inc:  2
eps [1.4778918800354004, 1.4778918800354004, 1.4778918800354004, 1.4778918800354004, 1.4778918800354004, 1.4778918800354004, 1.4778918800354004, 1.4778918800354004, 0.2463153133392334, 0.7389459400177002, 1.4778918800354004, 1.9705225067138672, 1.4778918800354004, 15.764180053710938]  wait [2, 2, 2, 2, 2, 2, 3, 2, 2, 1, 2, 2, 2, 0]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]  tol: 7
$$$$$$$$$$$$$ epoch  55  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1493.9027910232544 , diff:  1493.9027910232544
adv train loss:  -1515.9906778335571 , diff:  22.087886810302734
adv train loss:  -1518.6274166107178 , diff:  2.6367387771606445
adv train loss:  -1519.6302862167358 , diff:  1.0028696060180664
adv train loss:  -1519.7987871170044 , diff:  0.1685009002685547
layer  0  adv train finish, try to retain  1
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  775.281795501709
forward train acc: top1 ->  16.99600000366211 ; top5 ->  63.146000004882815  and loss:  444.1304883956909
test acc: top1 ->  9.17 ; top5 ->  51.24  and loss:  272.2869770526886
forward train acc: top1 ->  19.286000004882812 ; top5 ->  67.75200000732421  and loss:  218.19727563858032
test acc: top1 ->  21.71 ; top5 ->  70.86  and loss:  215.27729511260986
forward train acc: top1 ->  21.217999995117186 ; top5 ->  70.43800000732422  and loss:  211.30659651756287
test acc: top1 ->  24.11 ; top5 ->  73.42  and loss:  210.08223509788513
forward train acc: top1 ->  23.642000001831054 ; top5 ->  73.00400001708984  and loss:  206.4149625301361
test acc: top1 ->  26.44 ; top5 ->  76.08  and loss:  203.74866795539856
forward train acc: top1 ->  26.00999999633789 ; top5 ->  75.53999998779297  and loss:  200.86685645580292
test acc: top1 ->  28.88 ; top5 ->  78.23  and loss:  198.2257937192917
forward train acc: top1 ->  27.064000002441407 ; top5 ->  76.82399999511719  and loss:  197.75977981090546
test acc: top1 ->  29.5 ; top5 ->  79.4  and loss:  195.31423354148865
forward train acc: top1 ->  27.958 ; top5 ->  78.54200001464844  and loss:  194.99744987487793
test acc: top1 ->  30.58 ; top5 ->  80.23  and loss:  192.76801073551178
forward train acc: top1 ->  28.95800000366211 ; top5 ->  79.03800000488282  and loss:  192.8104249238968
test acc: top1 ->  30.87 ; top5 ->  80.97  and loss:  190.83748471736908
forward train acc: top1 ->  30.20600000305176 ; top5 ->  80.1359999975586  and loss:  190.10172772407532
test acc: top1 ->  32.06 ; top5 ->  82.15  and loss:  188.08555793762207
forward train acc: top1 ->  30.669999986572265 ; top5 ->  80.93800001708985  and loss:  188.08335626125336
test acc: top1 ->  32.91 ; top5 ->  82.79  and loss:  185.26401591300964
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  30 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -82.86822706460953 , diff:  82.86822706460953
adv train loss:  -82.8486624956131 , diff:  0.019564568996429443
layer  1  adv train finish, try to retain  56
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -206.51980608701706 , diff:  206.51980608701706
adv train loss:  -284.2924613952637 , diff:  77.77265530824661
adv train loss:  -288.2411735057831 , diff:  3.948712110519409
adv train loss:  -288.9292652606964 , diff:  0.6880917549133301
adv train loss:  -287.7966673374176 , diff:  1.1325979232788086
adv train loss:  -287.31029319763184 , diff:  0.4863741397857666
adv train loss:  -286.8467855453491 , diff:  0.46350765228271484
adv train loss:  -286.28187084198 , diff:  0.5649147033691406
adv train loss:  -285.4652442932129 , diff:  0.8166265487670898
adv train loss:  -286.69394063949585 , diff:  1.228696346282959
layer  2  adv train finish, try to retain  72
test acc: top1 ->  28.03 ; top5 ->  67.08  and loss:  239.091534614563
forward train acc: top1 ->  97.37999998046875 ; top5 ->  99.952  and loss:  17.801738034933805
test acc: top1 ->  90.19 ; top5 ->  99.34  and loss:  39.27307552099228
forward train acc: top1 ->  98.87399997802734 ; top5 ->  99.986  and loss:  3.622687540948391
test acc: top1 ->  90.47 ; top5 ->  99.32  and loss:  42.75931125879288
forward train acc: top1 ->  98.95599997558594 ; top5 ->  99.986  and loss:  3.232635399326682
test acc: top1 ->  90.59 ; top5 ->  99.3  and loss:  43.83376821875572
forward train acc: top1 ->  99.20400000732423 ; top5 ->  99.994  and loss:  2.5199447497725487
test acc: top1 ->  90.4 ; top5 ->  99.3  and loss:  45.04672431945801
forward train acc: top1 ->  99.25999997558594 ; top5 ->  99.998  and loss:  2.3234651563689113
test acc: top1 ->  90.62 ; top5 ->  99.37  and loss:  45.9364705234766
forward train acc: top1 ->  99.31799997558593 ; top5 ->  100.0  and loss:  2.0185232618823647
test acc: top1 ->  90.76 ; top5 ->  99.29  and loss:  46.64230254292488
forward train acc: top1 ->  99.3020000024414 ; top5 ->  99.994  and loss:  2.054441438987851
test acc: top1 ->  90.93 ; top5 ->  99.29  and loss:  46.34659168124199
forward train acc: top1 ->  99.34600000488281 ; top5 ->  99.99199997558594  and loss:  1.883346326649189
test acc: top1 ->  90.91 ; top5 ->  99.33  and loss:  46.589521273970604
forward train acc: top1 ->  99.42800000244141 ; top5 ->  99.992  and loss:  1.6987966150045395
test acc: top1 ->  90.86 ; top5 ->  99.31  and loss:  47.75858400762081
forward train acc: top1 ->  99.416 ; top5 ->  99.992  and loss:  1.6794870062731206
test acc: top1 ->  90.97 ; top5 ->  99.31  and loss:  47.35330507159233
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -690.9427749067545 , diff:  690.9427749067545
adv train loss:  -1164.5057020187378 , diff:  473.5629271119833
adv train loss:  -1178.2858200073242 , diff:  13.780117988586426
adv train loss:  -1176.690583229065 , diff:  1.5952367782592773
adv train loss:  -1184.4753894805908 , diff:  7.784806251525879
adv train loss:  -1179.507884979248 , diff:  4.967504501342773
adv train loss:  -1176.071933746338 , diff:  3.4359512329101562
adv train loss:  -1184.2316217422485 , diff:  8.159687995910645
adv train loss:  -1189.541498184204 , diff:  5.309876441955566
adv train loss:  -1186.464599609375 , diff:  3.0768985748291016
layer  3  adv train finish, try to retain  39
test acc: top1 ->  16.01 ; top5 ->  55.59  and loss:  1182.6642417907715
forward train acc: top1 ->  89.48599998291016 ; top5 ->  99.49999997802735  and loss:  34.14480131864548
test acc: top1 ->  84.3 ; top5 ->  98.83  and loss:  52.043439477682114
forward train acc: top1 ->  91.19999998291016 ; top5 ->  99.65999997558593  and loss:  25.829690873622894
test acc: top1 ->  85.02 ; top5 ->  98.96  and loss:  50.03393392264843
forward train acc: top1 ->  91.96000001708984 ; top5 ->  99.74  and loss:  23.317120641469955
test acc: top1 ->  85.56 ; top5 ->  98.98  and loss:  48.91473829746246
forward train acc: top1 ->  92.55800000732422 ; top5 ->  99.76  and loss:  21.906716287136078
test acc: top1 ->  86.25 ; top5 ->  99.1  and loss:  47.03761288523674
forward train acc: top1 ->  93.14799999511719 ; top5 ->  99.80199997558594  and loss:  19.86993470788002
test acc: top1 ->  86.51 ; top5 ->  99.16  and loss:  46.19085864722729
forward train acc: top1 ->  93.50800000732421 ; top5 ->  99.83  and loss:  19.053199499845505
test acc: top1 ->  86.67 ; top5 ->  99.14  and loss:  45.621450275182724
forward train acc: top1 ->  93.67799998046875 ; top5 ->  99.84  and loss:  18.282112382352352
test acc: top1 ->  86.79 ; top5 ->  99.17  and loss:  45.61537981033325
forward train acc: top1 ->  93.75399997314453 ; top5 ->  99.83  and loss:  18.076633915305138
test acc: top1 ->  86.81 ; top5 ->  99.2  and loss:  45.58522938191891
forward train acc: top1 ->  93.80600001953125 ; top5 ->  99.83  and loss:  17.804861903190613
test acc: top1 ->  86.87 ; top5 ->  99.29  and loss:  45.21571289002895
forward train acc: top1 ->  94.004 ; top5 ->  99.8700000024414  and loss:  17.38607606291771
test acc: top1 ->  87.03 ; top5 ->  99.21  and loss:  44.93277198076248
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -106.3092140853405 , diff:  106.3092140853405
adv train loss:  -556.2543683052063 , diff:  449.9451542198658
adv train loss:  -685.7875409126282 , diff:  129.53317260742188
adv train loss:  -765.2600617408752 , diff:  79.47252082824707
adv train loss:  -810.0013461112976 , diff:  44.74128437042236
adv train loss:  -823.3438401222229 , diff:  13.342494010925293
adv train loss:  -844.0828199386597 , diff:  20.738979816436768
adv train loss:  -909.3556547164917 , diff:  65.27283477783203
adv train loss:  -911.9160957336426 , diff:  2.560441017150879
adv train loss:  -909.7071599960327 , diff:  2.2089357376098633
layer  4  adv train finish, try to retain  50
test acc: top1 ->  30.6 ; top5 ->  74.04  and loss:  703.4912872314453
forward train acc: top1 ->  85.4840000024414 ; top5 ->  99.05399998291016  and loss:  43.03709205985069
test acc: top1 ->  81.96 ; top5 ->  98.45  and loss:  57.691055089235306
forward train acc: top1 ->  88.31999997558594 ; top5 ->  99.45799997558593  and loss:  33.95949709415436
test acc: top1 ->  83.42 ; top5 ->  98.74  and loss:  53.19636133313179
forward train acc: top1 ->  89.48000001708985 ; top5 ->  99.526  and loss:  30.526295751333237
test acc: top1 ->  84.51 ; top5 ->  98.84  and loss:  51.00323534011841
forward train acc: top1 ->  90.34600000976563 ; top5 ->  99.674  and loss:  27.677474841475487
test acc: top1 ->  84.8 ; top5 ->  98.81  and loss:  49.854758501052856
forward train acc: top1 ->  90.97600001464843 ; top5 ->  99.68800000244141  and loss:  26.426295906305313
test acc: top1 ->  84.85 ; top5 ->  99.0  and loss:  49.91640292108059
forward train acc: top1 ->  91.27999998291016 ; top5 ->  99.692  and loss:  25.027216836810112
test acc: top1 ->  85.64 ; top5 ->  99.02  and loss:  47.28389799594879
forward train acc: top1 ->  91.33600001953126 ; top5 ->  99.6980000024414  and loss:  24.758092999458313
test acc: top1 ->  85.99 ; top5 ->  98.98  and loss:  46.805411502718925
forward train acc: top1 ->  91.64799997070313 ; top5 ->  99.73799997802735  and loss:  23.908743783831596
test acc: top1 ->  85.59 ; top5 ->  98.91  and loss:  48.398863673210144
forward train acc: top1 ->  91.95000001220703 ; top5 ->  99.7060000024414  and loss:  23.304610162973404
test acc: top1 ->  86.14 ; top5 ->  99.03  and loss:  46.47365291416645
forward train acc: top1 ->  91.95599997802735 ; top5 ->  99.72999997558594  and loss:  22.965814635157585
test acc: top1 ->  86.18 ; top5 ->  99.0  and loss:  46.534722685813904
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  156 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -207.10262946784496 , diff:  207.10262946784496
adv train loss:  -921.1629085540771 , diff:  714.0602790862322
adv train loss:  -1041.6432666778564 , diff:  120.4803581237793
adv train loss:  -1061.4511404037476 , diff:  19.807873725891113
adv train loss:  -1068.2378854751587 , diff:  6.786745071411133
adv train loss:  -1065.7667770385742 , diff:  2.4711084365844727
adv train loss:  -1074.5973205566406 , diff:  8.830543518066406
adv train loss:  -1089.533688545227 , diff:  14.936367988586426
adv train loss:  -1089.9153776168823 , diff:  0.38168907165527344
adv train loss:  -1100.6396856307983 , diff:  10.724308013916016
layer  5  adv train finish, try to retain  41
test acc: top1 ->  42.93 ; top5 ->  84.51  and loss:  280.1357537508011
forward train acc: top1 ->  90.40800001220703 ; top5 ->  99.7000000024414  and loss:  27.408001005649567
test acc: top1 ->  85.55 ; top5 ->  99.0  and loss:  47.810918033123016
forward train acc: top1 ->  92.31399998779297 ; top5 ->  99.79200000488281  and loss:  21.813432559370995
test acc: top1 ->  86.45 ; top5 ->  98.93  and loss:  47.64553779363632
forward train acc: top1 ->  93.10400001953126 ; top5 ->  99.86  and loss:  19.127619460225105
test acc: top1 ->  86.93 ; top5 ->  98.93  and loss:  46.42544761300087
forward train acc: top1 ->  93.8820000024414 ; top5 ->  99.8760000024414  and loss:  17.477958112955093
test acc: top1 ->  87.2 ; top5 ->  99.02  and loss:  45.67021842300892
forward train acc: top1 ->  94.17999997802734 ; top5 ->  99.88599997558593  and loss:  16.355270117521286
test acc: top1 ->  87.93 ; top5 ->  99.18  and loss:  43.88123106956482
forward train acc: top1 ->  94.49600000488282 ; top5 ->  99.906  and loss:  15.417860768735409
test acc: top1 ->  87.96 ; top5 ->  99.17  and loss:  43.061356738209724
forward train acc: top1 ->  94.67799998046875 ; top5 ->  99.9  and loss:  15.032985359430313
test acc: top1 ->  88.19 ; top5 ->  99.12  and loss:  43.324408546090126
forward train acc: top1 ->  94.932 ; top5 ->  99.9  and loss:  14.534146599471569
test acc: top1 ->  88.17 ; top5 ->  99.18  and loss:  43.14338681101799
forward train acc: top1 ->  94.9079999951172 ; top5 ->  99.936  and loss:  14.450903542339802
test acc: top1 ->  88.04 ; top5 ->  99.11  and loss:  43.69146326184273
forward train acc: top1 ->  95.07200000488281 ; top5 ->  99.886  and loss:  14.040008693933487
test acc: top1 ->  88.29 ; top5 ->  99.21  and loss:  42.99245496094227
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
### skip layer  6 wait:  3  ###
---------------- start layer  7  ---------------
adv train loss:  -2.7652736362069845 , diff:  2.7652736362069845
adv train loss:  -2.6648788657039404 , diff:  0.10039477050304413
adv train loss:  -2.702232237905264 , diff:  0.03735337220132351
adv train loss:  -2.691582325845957 , diff:  0.010649912059307098
adv train loss:  -2.6649930346757174 , diff:  0.02658929117023945
adv train loss:  -2.648992031812668 , diff:  0.016001002863049507
adv train loss:  -2.6834772303700447 , diff:  0.03448519855737686
adv train loss:  -2.5733655458316207 , diff:  0.11011168453842402
adv train loss:  -2.7132116900756955 , diff:  0.13984614424407482
adv train loss:  -2.6614404059946537 , diff:  0.05177128408104181
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  103
test acc: top1 ->  16.76 ; top5 ->  63.12  and loss:  2246636.56640625
forward train acc: top1 ->  99.206 ; top5 ->  99.996  and loss:  3.0109560675919056
test acc: top1 ->  91.03 ; top5 ->  99.49  and loss:  43.748752266168594
forward train acc: top1 ->  99.43999997558593 ; top5 ->  99.998  and loss:  1.6854284051805735
test acc: top1 ->  91.43 ; top5 ->  99.34  and loss:  48.6082423850894
forward train acc: top1 ->  99.562 ; top5 ->  100.0  and loss:  1.3243796427268535
test acc: top1 ->  91.46 ; top5 ->  99.36  and loss:  48.980702824890614
forward train acc: top1 ->  99.572 ; top5 ->  100.0  and loss:  1.2306835441850126
test acc: top1 ->  91.5 ; top5 ->  99.33  and loss:  49.511897191405296
forward train acc: top1 ->  99.694 ; top5 ->  99.998  and loss:  0.9184958329424262
test acc: top1 ->  91.49 ; top5 ->  99.35  and loss:  51.44106571376324
forward train acc: top1 ->  99.67200000244141 ; top5 ->  99.998  and loss:  0.9668320510536432
test acc: top1 ->  91.56 ; top5 ->  99.35  and loss:  51.18969973176718
forward train acc: top1 ->  99.72399997558594 ; top5 ->  100.0  and loss:  0.8493026178330183
test acc: top1 ->  91.64 ; top5 ->  99.39  and loss:  51.547328412532806
forward train acc: top1 ->  99.73399997558593 ; top5 ->  99.998  and loss:  0.8538139276206493
test acc: top1 ->  91.65 ; top5 ->  99.3  and loss:  52.92642068117857
forward train acc: top1 ->  99.78 ; top5 ->  100.0  and loss:  0.7006040858104825
test acc: top1 ->  91.67 ; top5 ->  99.41  and loss:  53.11940249055624
forward train acc: top1 ->  99.766 ; top5 ->  100.0  and loss:  0.7217453643679619
test acc: top1 ->  91.72 ; top5 ->  99.34  and loss:  53.83954083919525
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  104 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -14.464147243648767 , diff:  14.464147243648767
adv train loss:  -13.94054415076971 , diff:  0.5236030928790569
adv train loss:  -14.487233281135559 , diff:  0.5466891303658485
adv train loss:  -14.10541283339262 , diff:  0.381820447742939
adv train loss:  -14.684328965842724 , diff:  0.5789161324501038
adv train loss:  -14.343406416475773 , diff:  0.340922549366951
adv train loss:  -14.598957642912865 , diff:  0.2555512264370918
adv train loss:  -15.066014036536217 , diff:  0.46705639362335205
adv train loss:  -14.623298525810242 , diff:  0.44271551072597504
adv train loss:  -14.209275536239147 , diff:  0.4140229895710945
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  20
test acc: top1 ->  15.89 ; top5 ->  50.01  and loss:  1474.2769784927368
forward train acc: top1 ->  94.57399998535156 ; top5 ->  99.968  and loss:  21.831600572913885
test acc: top1 ->  89.7 ; top5 ->  99.15  and loss:  51.40087006986141
forward train acc: top1 ->  99.12200000732422 ; top5 ->  99.998  and loss:  3.265368942171335
test acc: top1 ->  90.87 ; top5 ->  99.08  and loss:  48.53512258827686
forward train acc: top1 ->  99.33199997558594 ; top5 ->  99.998  and loss:  2.243988949805498
test acc: top1 ->  91.26 ; top5 ->  99.1  and loss:  49.00621125847101
forward train acc: top1 ->  99.51999997802734 ; top5 ->  99.998  and loss:  1.642672510817647
test acc: top1 ->  91.44 ; top5 ->  99.17  and loss:  50.07093784213066
forward train acc: top1 ->  99.554 ; top5 ->  100.0  and loss:  1.457305809482932
test acc: top1 ->  90.94 ; top5 ->  98.91  and loss:  53.68531683087349
forward train acc: top1 ->  99.682 ; top5 ->  100.0  and loss:  1.156637997366488
test acc: top1 ->  91.59 ; top5 ->  99.2  and loss:  50.767806723713875
forward train acc: top1 ->  99.656 ; top5 ->  100.0  and loss:  1.141472602263093
test acc: top1 ->  91.31 ; top5 ->  99.17  and loss:  53.330959029495716
forward train acc: top1 ->  99.69399997802735 ; top5 ->  99.998  and loss:  1.0394640024751425
test acc: top1 ->  91.68 ; top5 ->  99.2  and loss:  51.50776096433401
forward train acc: top1 ->  99.716 ; top5 ->  100.0  and loss:  0.9812953961081803
test acc: top1 ->  91.45 ; top5 ->  99.24  and loss:  53.069392919540405
forward train acc: top1 ->  99.694 ; top5 ->  100.0  and loss:  0.9535468895919621
test acc: top1 ->  91.48 ; top5 ->  99.2  and loss:  53.991705134510994
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  21 / 512 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -2.786628719419241 , diff:  2.786628719419241
adv train loss:  -2.6925220042467117 , diff:  0.09410671517252922
adv train loss:  -2.5963618755340576 , diff:  0.09616012871265411
adv train loss:  -2.647442914545536 , diff:  0.051081039011478424
adv train loss:  -2.7524339854717255 , diff:  0.10499107092618942
adv train loss:  -2.7968741916120052 , diff:  0.04444020614027977
adv train loss:  -2.8167283087968826 , diff:  0.019854117184877396
adv train loss:  -2.632706481963396 , diff:  0.18402182683348656
adv train loss:  -2.947725787758827 , diff:  0.31501930579543114
adv train loss:  -2.762911415658891 , diff:  0.184814372099936
layer  9  adv train finish, try to retain  418
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -39.20983371138573 , diff:  39.20983371138573
adv train loss:  -39.91243928670883 , diff:  0.7026055753231049
adv train loss:  -39.921559780836105 , diff:  0.00912049412727356
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  21
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  865102.484375
forward train acc: top1 ->  91.39399997558594 ; top5 ->  99.296  and loss:  30.172834126278758
test acc: top1 ->  90.02 ; top5 ->  98.88  and loss:  49.44534382224083
forward train acc: top1 ->  99.44199997558594 ; top5 ->  99.998  and loss:  2.4437060300260782
test acc: top1 ->  90.69 ; top5 ->  98.81  and loss:  47.07899558544159
forward train acc: top1 ->  99.65 ; top5 ->  100.0  and loss:  1.5453983983024955
test acc: top1 ->  91.18 ; top5 ->  98.83  and loss:  48.8259918987751
forward train acc: top1 ->  99.70799997558593 ; top5 ->  100.0  and loss:  1.0916314851492643
test acc: top1 ->  91.24 ; top5 ->  98.79  and loss:  50.63334360718727
forward train acc: top1 ->  99.80999997802735 ; top5 ->  100.0  and loss:  0.8653931953012943
test acc: top1 ->  91.38 ; top5 ->  98.88  and loss:  50.3958403468132
forward train acc: top1 ->  99.85199997558594 ; top5 ->  100.0  and loss:  0.6926803262904286
test acc: top1 ->  91.6 ; top5 ->  98.74  and loss:  51.74602630734444
forward train acc: top1 ->  99.86 ; top5 ->  100.0  and loss:  0.5825502341613173
test acc: top1 ->  91.58 ; top5 ->  98.84  and loss:  51.705804616212845
forward train acc: top1 ->  99.8640000024414 ; top5 ->  100.0  and loss:  0.55000907368958
test acc: top1 ->  91.69 ; top5 ->  98.79  and loss:  52.34624908864498
forward train acc: top1 ->  99.85799997558594 ; top5 ->  100.0  and loss:  0.526792265009135
test acc: top1 ->  91.69 ; top5 ->  98.77  and loss:  52.905267268419266
forward train acc: top1 ->  99.862 ; top5 ->  100.0  and loss:  0.5156154816504568
test acc: top1 ->  91.71 ; top5 ->  98.78  and loss:  53.505366101861
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  22 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -1855.012393951416 , diff:  1855.012393951416
adv train loss:  -1852.6017532348633 , diff:  2.4106407165527344
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  1
test acc: top1 ->  10.0 ; top5 ->  57.31  and loss:  11607.090202331543
forward train acc: top1 ->  95.224 ; top5 ->  99.964  and loss:  17.06672022677958
test acc: top1 ->  78.7 ; top5 ->  98.68  and loss:  112.47445726394653
forward train acc: top1 ->  99.732 ; top5 ->  100.0  and loss:  1.1499453803990036
test acc: top1 ->  91.51 ; top5 ->  99.14  and loss:  52.12490002065897
forward train acc: top1 ->  99.76799997558594 ; top5 ->  100.0  and loss:  0.869875917211175
test acc: top1 ->  91.67 ; top5 ->  99.17  and loss:  52.088750675320625
forward train acc: top1 ->  99.8480000048828 ; top5 ->  100.0  and loss:  0.6090837698429823
test acc: top1 ->  91.95 ; top5 ->  99.21  and loss:  52.07318922132254
forward train acc: top1 ->  99.896 ; top5 ->  100.0  and loss:  0.4350354333873838
test acc: top1 ->  91.77 ; top5 ->  99.16  and loss:  53.1138024777174
forward train acc: top1 ->  99.898 ; top5 ->  100.0  and loss:  0.34754907456226647
test acc: top1 ->  92.0 ; top5 ->  99.19  and loss:  52.89536591619253
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.3233755056280643
test acc: top1 ->  92.01 ; top5 ->  99.18  and loss:  53.16619807481766
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.31971348263323307
test acc: top1 ->  91.97 ; top5 ->  99.15  and loss:  53.91795141249895
forward train acc: top1 ->  99.9180000024414 ; top5 ->  100.0  and loss:  0.3120985683053732
test acc: top1 ->  92.03 ; top5 ->  99.2  and loss:  53.97752859443426
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.2656085033668205
test acc: top1 ->  91.97 ; top5 ->  99.21  and loss:  54.59121811389923
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  2 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -612.9240683317184 , diff:  612.9240683317184
adv train loss:  -958.6168203353882 , diff:  345.69275200366974
adv train loss:  -994.9686460494995 , diff:  36.35182571411133
adv train loss:  -992.1674451828003 , diff:  2.8012008666992188
adv train loss:  -992.7122554779053 , diff:  0.5448102951049805
adv train loss:  -992.6330184936523 , diff:  0.07923698425292969
adv train loss:  -992.9878206253052 , diff:  0.35480213165283203
adv train loss:  -992.9695682525635 , diff:  0.01825237274169922
layer  12  adv train finish, try to retain  2
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  19599.002822875977
forward train acc: top1 ->  23.692000009765625 ; top5 ->  65.01199997314453  and loss:  922.4217524528503
test acc: top1 ->  32.72 ; top5 ->  82.22  and loss:  293.6133363246918
forward train acc: top1 ->  56.949999970703125 ; top5 ->  88.99600001464843  and loss:  151.96146965026855
test acc: top1 ->  65.66 ; top5 ->  88.42  and loss:  120.96966791152954
forward train acc: top1 ->  81.49000000732421 ; top5 ->  99.18599997558594  and loss:  67.43544793128967
test acc: top1 ->  80.16 ; top5 ->  98.19  and loss:  81.35075378417969
forward train acc: top1 ->  91.75399997314453 ; top5 ->  99.982  and loss:  48.80217555165291
test acc: top1 ->  85.15 ; top5 ->  98.13  and loss:  74.9938006401062
forward train acc: top1 ->  95.82400001708984 ; top5 ->  99.986  and loss:  40.73029777407646
test acc: top1 ->  86.15 ; top5 ->  98.09  and loss:  70.67213451862335
forward train acc: top1 ->  96.75000001220702 ; top5 ->  99.994  and loss:  35.81897833943367
test acc: top1 ->  86.45 ; top5 ->  98.11  and loss:  69.29762569069862
forward train acc: top1 ->  97.24000000732421 ; top5 ->  99.99  and loss:  32.896253794431686
test acc: top1 ->  86.54 ; top5 ->  98.15  and loss:  68.21931755542755
forward train acc: top1 ->  97.57800000976563 ; top5 ->  99.996  and loss:  30.051155000925064
test acc: top1 ->  86.96 ; top5 ->  98.14  and loss:  66.94290682673454
forward train acc: top1 ->  98.03400000732422 ; top5 ->  99.998  and loss:  27.195683360099792
test acc: top1 ->  87.12 ; top5 ->  98.13  and loss:  66.18218392133713
forward train acc: top1 ->  98.19600001220704 ; top5 ->  99.992  and loss:  24.948000952601433
test acc: top1 ->  87.51 ; top5 ->  98.05  and loss:  65.41007128357887
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  41 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -15746.693061828613 , diff:  15746.693061828613
adv train loss:  -24432.160430908203 , diff:  8685.46736907959
adv train loss:  -33121.019958496094 , diff:  8688.85952758789
adv train loss:  -41878.90789794922 , diff:  8757.887939453125
adv train loss:  -50601.03012084961 , diff:  8722.12222290039
adv train loss:  -59301.248474121094 , diff:  8700.218353271484
adv train loss:  -68013.50146484375 , diff:  8712.252990722656
adv train loss:  -76704.50018310547 , diff:  8690.998718261719
adv train loss:  -85399.59533691406 , diff:  8695.095153808594
adv train loss:  -94091.86041259766 , diff:  8692.265075683594
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  17
test acc: top1 ->  12.84 ; top5 ->  73.25  and loss:  2636.946859359741
forward train acc: top1 ->  69.21199997802735 ; top5 ->  93.494  and loss:  477.3042337298393
test acc: top1 ->  89.55 ; top5 ->  98.43  and loss:  76.80017739534378
forward train acc: top1 ->  99.23399997802734 ; top5 ->  100.0  and loss:  2.89265270344913
test acc: top1 ->  90.95 ; top5 ->  98.58  and loss:  67.0923104211688
forward train acc: top1 ->  99.62399997558593 ; top5 ->  100.0  and loss:  1.4112184415571392
test acc: top1 ->  91.4 ; top5 ->  98.68  and loss:  65.00077302753925
forward train acc: top1 ->  99.75400000488281 ; top5 ->  100.0  and loss:  0.9519390799105167
test acc: top1 ->  91.46 ; top5 ->  98.72  and loss:  63.8436129167676
forward train acc: top1 ->  99.788 ; top5 ->  99.998  and loss:  0.7455372987315059
test acc: top1 ->  91.68 ; top5 ->  98.76  and loss:  62.977495424449444
forward train acc: top1 ->  99.84199997558594 ; top5 ->  100.0  and loss:  0.6333912247791886
test acc: top1 ->  91.63 ; top5 ->  98.83  and loss:  63.65185525268316
forward train acc: top1 ->  99.866 ; top5 ->  100.0  and loss:  0.552100024651736
test acc: top1 ->  91.68 ; top5 ->  98.81  and loss:  63.049963906407356
forward train acc: top1 ->  99.838 ; top5 ->  100.0  and loss:  0.5724002309143543
test acc: top1 ->  91.7 ; top5 ->  98.78  and loss:  63.25185834616423
forward train acc: top1 ->  99.882 ; top5 ->  100.0  and loss:  0.44652060978114605
test acc: top1 ->  91.95 ; top5 ->  98.86  and loss:  62.7239742949605
forward train acc: top1 ->  99.89 ; top5 ->  100.0  and loss:  0.4394038466271013
test acc: top1 ->  91.9 ; top5 ->  98.8  and loss:  62.96088553965092
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  19 / 512 , inc:  2
layer  0  :  0.46875  ==>  30 / 64 , inc:  1
layer  1  :  0.875  ==>  56 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.3828125  ==>  98 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.041015625  ==>  21 / 512 , inc:  1
layer  9  :  0.1875  ==>  96 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.037109375  ==>  19 / 512 , inc:  1
eps [1.1084189100265502, 2.955783760070801, 1.1084189100265502, 1.1084189100265502, 1.1084189100265502, 1.1084189100265502, 1.4778918800354004, 1.1084189100265502, 0.18473648500442505, 1.4778918800354004, 1.1084189100265502, 1.4778918800354004, 1.1084189100265502, 11.823135040283203]  wait [4, 2, 4, 4, 4, 4, 2, 4, 4, 1, 4, 4, 4, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 7
$$$$$$$$$$$$$ epoch  56  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
adv train loss:  -63.5356502532959 , diff:  63.5356502532959
adv train loss:  -64.06106436252594 , diff:  0.5254141092300415
adv train loss:  -63.95320221781731 , diff:  0.10786214470863342
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  55
test acc: top1 ->  9.99 ; top5 ->  50.0  and loss:  2236.592601776123
forward train acc: top1 ->  99.344 ; top5 ->  100.0  and loss:  2.4438641403539805
test acc: top1 ->  92.19 ; top5 ->  99.0  and loss:  85.0348006375134
==> this epoch:  55 / 64
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
adv train loss:  -893.5781395817176 , diff:  893.5781395817176
adv train loss:  -3271.6454696655273 , diff:  2378.06733008381
adv train loss:  -3325.002264022827 , diff:  53.356794357299805
adv train loss:  -3327.0166664123535 , diff:  2.014402389526367
adv train loss:  -3343.227554321289 , diff:  16.210887908935547
adv train loss:  -3364.117359161377 , diff:  20.88980484008789
adv train loss:  -3360.863925933838 , diff:  3.2534332275390625
adv train loss:  -3357.442974090576 , diff:  3.4209518432617188
adv train loss:  -3395.1030349731445 , diff:  37.66006088256836
adv train loss:  -3395.1397285461426 , diff:  0.036693572998046875
layer  6  adv train finish, try to retain  11
test acc: top1 ->  18.54 ; top5 ->  71.62  and loss:  1364.4521369934082
forward train acc: top1 ->  85.85999997070313 ; top5 ->  99.23399997802734  and loss:  66.56149625778198
test acc: top1 ->  80.75 ; top5 ->  98.11  and loss:  96.14481979608536
forward train acc: top1 ->  89.92400000976562 ; top5 ->  99.58799997558594  and loss:  33.53350915014744
test acc: top1 ->  83.25 ; top5 ->  98.34  and loss:  75.89820060133934
forward train acc: top1 ->  91.7380000048828 ; top5 ->  99.716  and loss:  26.166141033172607
test acc: top1 ->  84.42 ; top5 ->  98.58  and loss:  65.66688758134842
forward train acc: top1 ->  92.6979999975586 ; top5 ->  99.75  and loss:  22.14826887845993
test acc: top1 ->  85.11 ; top5 ->  98.59  and loss:  63.34538026154041
forward train acc: top1 ->  93.58800001953125 ; top5 ->  99.752  and loss:  19.902495428919792
test acc: top1 ->  85.86 ; top5 ->  98.66  and loss:  58.57022453844547
forward train acc: top1 ->  93.86399999023438 ; top5 ->  99.812  and loss:  18.62601387500763
test acc: top1 ->  86.45 ; top5 ->  98.61  and loss:  57.513483077287674
forward train acc: top1 ->  94.14799997802734 ; top5 ->  99.872  and loss:  17.367187581956387
test acc: top1 ->  86.32 ; top5 ->  98.73  and loss:  57.30866438150406
forward train acc: top1 ->  94.52000001953125 ; top5 ->  99.846  and loss:  16.623527199029922
test acc: top1 ->  86.56 ; top5 ->  98.82  and loss:  56.93354345858097
forward train acc: top1 ->  94.7099999975586 ; top5 ->  99.862  and loss:  15.772214777767658
test acc: top1 ->  86.8 ; top5 ->  98.79  and loss:  55.54408997297287
forward train acc: top1 ->  94.66199997314453 ; top5 ->  99.8520000024414  and loss:  15.779264196753502
test acc: top1 ->  86.84 ; top5 ->  98.85  and loss:  54.484824135899544
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  98 / 256 , inc:  1
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
### skip layer  8 wait:  4  ###
---------------- start layer  9  ---------------
adv train loss:  -5.449007602408528 , diff:  5.449007602408528
adv train loss:  -5.587329030036926 , diff:  0.13832142762839794
adv train loss:  -5.432332165539265 , diff:  0.1549968644976616
adv train loss:  -8.836208283901215 , diff:  3.40387611836195
adv train loss:  -14.05290038883686 , diff:  5.216692104935646
adv train loss:  -28.426139652729034 , diff:  14.373239263892174
adv train loss:  -83.96984151005745 , diff:  55.543701857328415
adv train loss:  -178.29224586486816 , diff:  94.32240435481071
adv train loss:  -196.56429505348206 , diff:  18.27204918861389
adv train loss:  -203.69065237045288 , diff:  7.126357316970825
layer  9  adv train finish, try to retain  4
test acc: top1 ->  10.0 ; top5 ->  47.83  and loss:  2660.2732124328613
forward train acc: top1 ->  71.65799999755859 ; top5 ->  96.40199997558594  and loss:  114.1926420032978
test acc: top1 ->  81.46 ; top5 ->  98.26  and loss:  62.03202033042908
forward train acc: top1 ->  95.17400001464844 ; top5 ->  99.946  and loss:  17.344114042818546
test acc: top1 ->  88.08 ; top5 ->  98.31  and loss:  51.14468830823898
forward train acc: top1 ->  97.84400000976562 ; top5 ->  99.972  and loss:  8.02401227131486
test acc: top1 ->  89.43 ; top5 ->  98.32  and loss:  48.59562875330448
forward train acc: top1 ->  98.76800000488281 ; top5 ->  99.982  and loss:  4.894856121391058
test acc: top1 ->  90.12 ; top5 ->  98.44  and loss:  48.73592738062143
forward train acc: top1 ->  99.15399997802734 ; top5 ->  99.99  and loss:  3.3729214034974575
test acc: top1 ->  90.62 ; top5 ->  98.52  and loss:  49.88771963864565
forward train acc: top1 ->  99.32400000244141 ; top5 ->  99.994  and loss:  2.7179451286792755
test acc: top1 ->  90.72 ; top5 ->  98.53  and loss:  49.40113140642643
forward train acc: top1 ->  99.438 ; top5 ->  99.992  and loss:  2.368876534514129
test acc: top1 ->  91.06 ; top5 ->  98.53  and loss:  49.40338844805956
forward train acc: top1 ->  99.55799997558594 ; top5 ->  100.0  and loss:  2.039437383413315
test acc: top1 ->  91.04 ; top5 ->  98.61  and loss:  49.89118055999279
forward train acc: top1 ->  99.60600000488282 ; top5 ->  99.998  and loss:  1.694074073806405
test acc: top1 ->  91.12 ; top5 ->  98.61  and loss:  49.9388123601675
forward train acc: top1 ->  99.60199997558594 ; top5 ->  99.996  and loss:  1.70079779997468
test acc: top1 ->  91.21 ; top5 ->  98.59  and loss:  50.5239759683609
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  96 / 512 , inc:  1
---------------- start layer  10  ---------------
### skip layer  10 wait:  4  ###
---------------- start layer  11  ---------------
### skip layer  11 wait:  4  ###
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
adv train loss:  -13626.381805419922 , diff:  13626.381805419922
adv train loss:  -21844.819442749023 , diff:  8218.437637329102
adv train loss:  -29928.21942138672 , diff:  8083.399978637695
adv train loss:  -37972.97082519531 , diff:  8044.751403808594
adv train loss:  -46054.288482666016 , diff:  8081.317657470703
adv train loss:  -54146.398834228516 , diff:  8092.1103515625
adv train loss:  -62220.31750488281 , diff:  8073.918670654297
adv train loss:  -70317.24682617188 , diff:  8096.9293212890625
adv train loss:  -78427.35162353516 , diff:  8110.104797363281
adv train loss:  -86536.96881103516 , diff:  8109.6171875
layer  13  adv train finish, try to retain  17
test acc: top1 ->  56.44 ; top5 ->  96.03  and loss:  375.76913952827454
forward train acc: top1 ->  96.94199997558594 ; top5 ->  99.97  and loss:  15.251139599829912
test acc: top1 ->  91.61 ; top5 ->  99.17  and loss:  57.01613099128008
forward train acc: top1 ->  99.824 ; top5 ->  100.0  and loss:  0.6672150092199445
test acc: top1 ->  91.86 ; top5 ->  99.2  and loss:  57.02590249478817
forward train acc: top1 ->  99.86 ; top5 ->  100.0  and loss:  0.4564427931327373
test acc: top1 ->  92.03 ; top5 ->  99.22  and loss:  58.05745965242386
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.25306008174084127
test acc: top1 ->  92.1 ; top5 ->  99.21  and loss:  58.10270807892084
forward train acc: top1 ->  99.912 ; top5 ->  100.0  and loss:  0.256575932726264
test acc: top1 ->  92.04 ; top5 ->  99.2  and loss:  59.5542049780488
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.22329168929718435
test acc: top1 ->  92.2 ; top5 ->  99.25  and loss:  59.20760637521744
==> this epoch:  17 / 512
layer  0  :  0.46875  ==>  30 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  2
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.3828125  ==>  98 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.041015625  ==>  21 / 512 , inc:  1
layer  9  :  0.1875  ==>  96 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.033203125  ==>  17 / 512 , inc:  2
eps [1.1084189100265502, 2.955783760070801, 1.1084189100265502, 1.1084189100265502, 1.1084189100265502, 1.1084189100265502, 1.1084189100265502, 1.1084189100265502, 0.18473648500442505, 1.1084189100265502, 1.1084189100265502, 1.4778918800354004, 1.1084189100265502, 11.823135040283203]  wait [3, 0, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 0]  inc [1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]  tol: 7
$$$$$$$$$$$$$ epoch  57  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
adv train loss:  -0.2957555060274899 , diff:  0.2957555060274899
adv train loss:  -0.23915164591744542 , diff:  0.05660386011004448
adv train loss:  -0.2606562618166208 , diff:  0.021504615899175406
adv train loss:  -0.22316525969654322 , diff:  0.03749100212007761
adv train loss:  -0.22168114176020026 , diff:  0.0014841179363429546
layer  1  adv train finish, try to retain  55
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
### skip layer  8 wait:  3  ###
---------------- start layer  9  ---------------
### skip layer  9 wait:  3  ###
---------------- start layer  10  ---------------
### skip layer  10 wait:  3  ###
---------------- start layer  11  ---------------
### skip layer  11 wait:  3  ###
---------------- start layer  12  ---------------
### skip layer  12 wait:  3  ###
---------------- start layer  13  ---------------
adv train loss:  -18926.23616027832 , diff:  18926.23616027832
adv train loss:  -34481.730529785156 , diff:  15555.494369506836
adv train loss:  -48717.18356323242 , diff:  14235.453033447266
adv train loss:  -62547.53674316406 , diff:  13830.35317993164
adv train loss:  -76160.57727050781 , diff:  13613.04052734375
adv train loss:  -89613.74829101562 , diff:  13453.171020507812
adv train loss:  -102978.38403320312 , diff:  13364.6357421875
adv train loss:  -116351.11413574219 , diff:  13372.730102539062
adv train loss:  -129645.54406738281 , diff:  13294.429931640625
adv train loss:  -142633.4520263672 , diff:  12987.907958984375
layer  13  adv train finish, try to retain  22
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.46875  ==>  30 / 64 , inc:  1
layer  1  :  0.859375  ==>  55 / 64 , inc:  2
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.3828125  ==>  98 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.041015625  ==>  21 / 512 , inc:  1
layer  9  :  0.1875  ==>  96 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.033203125  ==>  17 / 512 , inc:  2
eps [1.1084189100265502, 5.911567520141602, 1.1084189100265502, 1.1084189100265502, 1.1084189100265502, 1.1084189100265502, 1.1084189100265502, 1.1084189100265502, 0.18473648500442505, 1.1084189100265502, 1.1084189100265502, 1.4778918800354004, 1.1084189100265502, 23.646270080566406]  wait [2, 0, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 0]  inc [1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]  tol: 7
$$$$$$$$$$$$$ epoch  58  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1249.7448225021362 , diff:  1249.7448225021362
adv train loss:  -1233.5298328399658 , diff:  16.21498966217041
adv train loss:  -1229.2265586853027 , diff:  4.303274154663086
adv train loss:  -1227.6079587936401 , diff:  1.6185998916625977
adv train loss:  -1251.0914897918701 , diff:  23.48353099822998
adv train loss:  -1315.176293373108 , diff:  64.0848035812378
adv train loss:  -1265.7100620269775 , diff:  49.46623134613037
adv train loss:  -1081.1556720733643 , diff:  184.55438995361328
adv train loss:  -1082.0117950439453 , diff:  0.8561229705810547
adv train loss:  -1085.2825803756714 , diff:  3.270785331726074
layer  0  adv train finish, try to retain  54
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -0.1879856018931605 , diff:  0.1879856018931605
adv train loss:  -0.19111762239481322 , diff:  0.0031320205016527325
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  53
test acc: top1 ->  10.01 ; top5 ->  48.0  and loss:  1297.1124572753906
forward train acc: top1 ->  99.832 ; top5 ->  99.998  and loss:  0.4957937060389668
test acc: top1 ->  91.88 ; top5 ->  99.25  and loss:  64.16224291920662
forward train acc: top1 ->  99.90399997558593 ; top5 ->  100.0  and loss:  0.3103411989286542
test acc: top1 ->  91.94 ; top5 ->  99.22  and loss:  66.6602840423584
forward train acc: top1 ->  99.87 ; top5 ->  100.0  and loss:  0.38585632931790315
test acc: top1 ->  91.95 ; top5 ->  99.22  and loss:  65.3966734111309
forward train acc: top1 ->  99.90599997558594 ; top5 ->  100.0  and loss:  0.2702475618571043
test acc: top1 ->  91.8 ; top5 ->  99.16  and loss:  67.63505721837282
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.21083150857157307
test acc: top1 ->  91.79 ; top5 ->  99.19  and loss:  67.97730325162411
forward train acc: top1 ->  99.94199997558594 ; top5 ->  100.0  and loss:  0.1619698042050004
test acc: top1 ->  92.09 ; top5 ->  99.08  and loss:  67.02394907921553
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.2194730486080516
test acc: top1 ->  91.84 ; top5 ->  99.17  and loss:  66.86110331863165
forward train acc: top1 ->  99.956 ; top5 ->  100.0  and loss:  0.1500204939802643
test acc: top1 ->  91.97 ; top5 ->  99.19  and loss:  65.01512556523085
forward train acc: top1 ->  99.952 ; top5 ->  99.998  and loss:  0.16473872241476784
test acc: top1 ->  92.13 ; top5 ->  99.25  and loss:  65.20761283114552
==> this epoch:  53 / 64
---------------- start layer  2  ---------------
adv train loss:  -847.2473266217858 , diff:  847.2473266217858
adv train loss:  -1568.793077468872 , diff:  721.5457508470863
adv train loss:  -1602.5379524230957 , diff:  33.74487495422363
adv train loss:  -1602.0930471420288 , diff:  0.44490528106689453
adv train loss:  -1600.7880420684814 , diff:  1.3050050735473633
adv train loss:  -1595.4010152816772 , diff:  5.387026786804199
adv train loss:  -1589.1578207015991 , diff:  6.243194580078125
adv train loss:  -1589.8202447891235 , diff:  0.6624240875244141
adv train loss:  -1595.6576118469238 , diff:  5.837367057800293
adv train loss:  -1599.2198314666748 , diff:  3.5622196197509766
layer  2  adv train finish, try to retain  118
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -759.1016818862408 , diff:  759.1016818862408
adv train loss:  -1565.5382890701294 , diff:  806.4366071838886
adv train loss:  -1612.7096538543701 , diff:  47.17136478424072
adv train loss:  -1603.4608154296875 , diff:  9.248838424682617
adv train loss:  -1618.67711353302 , diff:  15.21629810333252
adv train loss:  -1631.920087814331 , diff:  13.242974281311035
adv train loss:  -1675.0697555541992 , diff:  43.149667739868164
adv train loss:  -1696.4672451019287 , diff:  21.397489547729492
adv train loss:  -1693.0471954345703 , diff:  3.4200496673583984
adv train loss:  -1710.0535678863525 , diff:  17.006372451782227
layer  3  adv train finish, try to retain  120
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -268.36385116167367 , diff:  268.36385116167367
adv train loss:  -1502.406358718872 , diff:  1234.0425075571984
adv train loss:  -1879.9399318695068 , diff:  377.53357315063477
adv train loss:  -1985.7505741119385 , diff:  105.81064224243164
adv train loss:  -1979.6646919250488 , diff:  6.085882186889648
adv train loss:  -1976.3255786895752 , diff:  3.339113235473633
adv train loss:  -1973.244960784912 , diff:  3.080617904663086
adv train loss:  -2010.150354385376 , diff:  36.90539360046387
adv train loss:  -2034.584550857544 , diff:  24.43419647216797
adv train loss:  -2040.402946472168 , diff:  5.818395614624023
layer  4  adv train finish, try to retain  207
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -636.9760756213218 , diff:  636.9760756213218
adv train loss:  -2269.1429080963135 , diff:  1632.1668324749917
adv train loss:  -2339.078224182129 , diff:  69.93531608581543
adv train loss:  -2383.793785095215 , diff:  44.71556091308594
adv train loss:  -2396.6744594573975 , diff:  12.880674362182617
adv train loss:  -2436.3233737945557 , diff:  39.6489143371582
adv train loss:  -2439.3838500976562 , diff:  3.060476303100586
adv train loss:  -2457.8091831207275 , diff:  18.42533302307129
adv train loss:  -2475.4916706085205 , diff:  17.68248748779297
adv train loss:  -2477.5025177001953 , diff:  2.0108470916748047
layer  5  adv train finish, try to retain  233
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
### skip layer  6 wait:  3  ###
---------------- start layer  7  ---------------
adv train loss:  -0.11153245282548596 , diff:  0.11153245282548596
adv train loss:  -0.1316950834770978 , diff:  0.020162630651611835
adv train loss:  -0.14608844940084964 , diff:  0.014393365923751844
adv train loss:  -0.11712530130171217 , diff:  0.02896314809913747
adv train loss:  -0.1653299127938226 , diff:  0.048204611492110416
adv train loss:  -0.19152080197818577 , diff:  0.026190889184363186
adv train loss:  -0.1510623242938891 , diff:  0.04045847768429667
adv train loss:  -0.15853213123045862 , diff:  0.007469806936569512
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  103
test acc: top1 ->  26.89 ; top5 ->  58.24  and loss:  2989100.94921875
forward train acc: top1 ->  99.596 ; top5 ->  99.998  and loss:  1.2372927544638515
test acc: top1 ->  91.16 ; top5 ->  99.27  and loss:  59.413957472890615
forward train acc: top1 ->  99.74799997558594 ; top5 ->  100.0  and loss:  0.823366689030081
test acc: top1 ->  91.13 ; top5 ->  99.32  and loss:  60.17091139778495
forward train acc: top1 ->  99.75199997802734 ; top5 ->  100.0  and loss:  0.7575829103589058
test acc: top1 ->  91.43 ; top5 ->  99.23  and loss:  60.179153334349394
forward train acc: top1 ->  99.80599997802734 ; top5 ->  100.0  and loss:  0.6085322666913271
test acc: top1 ->  91.46 ; top5 ->  99.32  and loss:  59.44183333218098
forward train acc: top1 ->  99.7720000024414 ; top5 ->  100.0  and loss:  0.6458711475133896
test acc: top1 ->  91.37 ; top5 ->  99.31  and loss:  60.57096517831087
forward train acc: top1 ->  99.81799997558593 ; top5 ->  100.0  and loss:  0.5663432385772467
test acc: top1 ->  91.77 ; top5 ->  99.34  and loss:  60.069130193442106
forward train acc: top1 ->  99.81799997558593 ; top5 ->  100.0  and loss:  0.5267839399166405
test acc: top1 ->  91.44 ; top5 ->  99.3  and loss:  61.04117343202233
forward train acc: top1 ->  99.87399997558593 ; top5 ->  100.0  and loss:  0.37729506753385067
test acc: top1 ->  91.58 ; top5 ->  99.32  and loss:  62.54165679216385
forward train acc: top1 ->  99.85199997558594 ; top5 ->  100.0  and loss:  0.4049846073612571
test acc: top1 ->  91.46 ; top5 ->  99.19  and loss:  63.10121273249388
forward train acc: top1 ->  99.878 ; top5 ->  100.0  and loss:  0.3962100391217973
test acc: top1 ->  91.38 ; top5 ->  99.27  and loss:  63.245581390336156
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  104 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -4.838386002928019 , diff:  4.838386002928019
adv train loss:  -4.840019255876541 , diff:  0.0016332529485225677
layer  8  adv train finish, try to retain  464
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -521.5965826511383 , diff:  521.5965826511383
adv train loss:  -1144.4527187347412 , diff:  622.8561360836029
adv train loss:  -1384.827428817749 , diff:  240.3747100830078
adv train loss:  -1404.1743249893188 , diff:  19.346896171569824
adv train loss:  -1402.3159971237183 , diff:  1.858327865600586
adv train loss:  -1792.793080329895 , diff:  390.47708320617676
adv train loss:  -2143.19686126709 , diff:  350.4037809371948
adv train loss:  -2210.365432739258 , diff:  67.16857147216797
adv train loss:  -2252.482690811157 , diff:  42.117258071899414
adv train loss:  -2251.631788253784 , diff:  0.8509025573730469
layer  9  adv train finish, try to retain  436
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -40.70148256421089 , diff:  40.70148256421089
adv train loss:  -40.71600429713726 , diff:  0.014521732926368713
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  21
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  350395.6015625
forward train acc: top1 ->  97.36 ; top5 ->  99.94  and loss:  8.734513838775456
test acc: top1 ->  91.09 ; top5 ->  99.14  and loss:  49.80588372051716
forward train acc: top1 ->  99.886 ; top5 ->  100.0  and loss:  0.540611507371068
test acc: top1 ->  91.53 ; top5 ->  99.17  and loss:  50.43465121090412
forward train acc: top1 ->  99.902 ; top5 ->  100.0  and loss:  0.3557152906432748
test acc: top1 ->  91.59 ; top5 ->  99.18  and loss:  51.67682795971632
forward train acc: top1 ->  99.918 ; top5 ->  100.0  and loss:  0.30347881093621254
test acc: top1 ->  91.78 ; top5 ->  99.14  and loss:  53.772017389535904
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.24832839448936284
test acc: top1 ->  91.82 ; top5 ->  99.15  and loss:  56.06841719895601
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.19093336805235595
test acc: top1 ->  91.81 ; top5 ->  99.19  and loss:  54.785017132759094
forward train acc: top1 ->  99.936 ; top5 ->  100.0  and loss:  0.1941133788204752
test acc: top1 ->  91.86 ; top5 ->  99.16  and loss:  55.72066177427769
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.18539622554089874
test acc: top1 ->  91.83 ; top5 ->  99.14  and loss:  56.52073795720935
forward train acc: top1 ->  99.94599997558593 ; top5 ->  99.998  and loss:  0.1725879441946745
test acc: top1 ->  91.75 ; top5 ->  99.13  and loss:  56.484667241573334
forward train acc: top1 ->  99.946 ; top5 ->  100.0  and loss:  0.18713085283525288
test acc: top1 ->  91.97 ; top5 ->  99.15  and loss:  57.27846638858318
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  22 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -1406.101734161377 , diff:  1406.101734161377
adv train loss:  -1499.5403604507446 , diff:  93.43862628936768
adv train loss:  -1500.3526983261108 , diff:  0.8123378753662109
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  1
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  28707.74822998047
forward train acc: top1 ->  93.89399997558594 ; top5 ->  99.488  and loss:  21.40409741178155
test acc: top1 ->  84.81 ; top5 ->  96.35  and loss:  84.49441036581993
forward train acc: top1 ->  99.52200000488281 ; top5 ->  99.996  and loss:  2.468292623758316
test acc: top1 ->  89.87 ; top5 ->  97.16  and loss:  64.87037074565887
forward train acc: top1 ->  99.60599997802734 ; top5 ->  99.998  and loss:  1.7285282891243696
test acc: top1 ->  89.8 ; top5 ->  97.2  and loss:  68.20583379268646
forward train acc: top1 ->  99.64999997802734 ; top5 ->  100.0  and loss:  1.4185262061655521
test acc: top1 ->  89.99 ; top5 ->  97.18  and loss:  70.39668618142605
forward train acc: top1 ->  99.73799997558594 ; top5 ->  99.994  and loss:  1.1542728813365102
test acc: top1 ->  89.99 ; top5 ->  97.2  and loss:  71.28278440237045
forward train acc: top1 ->  99.75000000244141 ; top5 ->  100.0  and loss:  0.9412397537380457
test acc: top1 ->  90.22 ; top5 ->  97.36  and loss:  70.78412419557571
forward train acc: top1 ->  99.77999997558594 ; top5 ->  100.0  and loss:  0.8920839438214898
test acc: top1 ->  90.15 ; top5 ->  97.4  and loss:  71.86852410435677
forward train acc: top1 ->  99.804 ; top5 ->  100.0  and loss:  0.80442571407184
test acc: top1 ->  90.21 ; top5 ->  97.4  and loss:  73.10818788409233
forward train acc: top1 ->  99.784 ; top5 ->  100.0  and loss:  0.8033126285299659
test acc: top1 ->  90.23 ; top5 ->  97.4  and loss:  74.0167028605938
forward train acc: top1 ->  99.786 ; top5 ->  99.998  and loss:  0.8028196124359965
test acc: top1 ->  90.26 ; top5 ->  97.38  and loss:  74.32723432779312
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  2 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -440.2529767025262 , diff:  440.2529767025262
adv train loss:  -1151.8230438232422 , diff:  711.570067120716
adv train loss:  -1160.8165311813354 , diff:  8.993487358093262
adv train loss:  -1162.2774715423584 , diff:  1.4609403610229492
adv train loss:  -1164.0577116012573 , diff:  1.7802400588989258
adv train loss:  -1163.3890800476074 , diff:  0.6686315536499023
adv train loss:  -1163.518072128296 , diff:  0.12899208068847656
adv train loss:  -1161.7475633621216 , diff:  1.7705087661743164
adv train loss:  -1162.820532798767 , diff:  1.0729694366455078
adv train loss:  -1161.841028213501 , diff:  0.9795045852661133
layer  12  adv train finish, try to retain  483
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -21071.68344116211 , diff:  21071.68344116211
adv train loss:  -35565.61740112305 , diff:  14493.933959960938
adv train loss:  -49450.56164550781 , diff:  13884.944244384766
adv train loss:  -63144.96154785156 , diff:  13694.39990234375
adv train loss:  -76705.09411621094 , diff:  13560.132568359375
adv train loss:  -90215.94317626953 , diff:  13510.849060058594
adv train loss:  -103722.02111816406 , diff:  13506.077941894531
adv train loss:  -117194.16040039062 , diff:  13472.139282226562
adv train loss:  -130725.58154296875 , diff:  13531.421142578125
adv train loss:  -144216.2384033203 , diff:  13490.656860351562
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  15
test acc: top1 ->  70.63 ; top5 ->  98.0  and loss:  373.3255488872528
forward train acc: top1 ->  97.66399997558594 ; top5 ->  100.0  and loss:  16.560405264608562
test acc: top1 ->  91.8 ; top5 ->  98.73  and loss:  82.13952741771936
forward train acc: top1 ->  99.932 ; top5 ->  100.0  and loss:  0.21279237692942843
test acc: top1 ->  91.96 ; top5 ->  98.75  and loss:  82.67256081104279
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.14025693607982248
test acc: top1 ->  92.02 ; top5 ->  98.62  and loss:  82.95250547677279
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.12848250090610236
test acc: top1 ->  91.87 ; top5 ->  98.67  and loss:  83.59823297709227
forward train acc: top1 ->  99.96799997558594 ; top5 ->  100.0  and loss:  0.11474855244159698
test acc: top1 ->  91.97 ; top5 ->  98.71  and loss:  85.39890620112419
forward train acc: top1 ->  99.96399997558593 ; top5 ->  100.0  and loss:  0.10754160536453128
test acc: top1 ->  91.94 ; top5 ->  98.64  and loss:  84.44741463661194
forward train acc: top1 ->  99.95999997558594 ; top5 ->  100.0  and loss:  0.11611921153962612
test acc: top1 ->  92.05 ; top5 ->  98.65  and loss:  86.71278305351734
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.09819211944704875
test acc: top1 ->  92.01 ; top5 ->  98.69  and loss:  85.45395436882973
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.11782669941021595
test acc: top1 ->  92.05 ; top5 ->  98.72  and loss:  84.75494173169136
forward train acc: top1 ->  99.96399997558593 ; top5 ->  100.0  and loss:  0.13105734530836344
test acc: top1 ->  92.09 ; top5 ->  98.75  and loss:  85.53510308265686
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  17 / 512 , inc:  2
layer  0  :  0.46875  ==>  30 / 64 , inc:  1
layer  1  :  0.828125  ==>  53 / 64 , inc:  4
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.3828125  ==>  98 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.041015625  ==>  21 / 512 , inc:  1
layer  9  :  0.1875  ==>  96 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.033203125  ==>  17 / 512 , inc:  1
eps [2.2168378200531005, 5.911567520141602, 2.2168378200531005, 2.2168378200531005, 2.2168378200531005, 2.2168378200531005, 1.1084189100265502, 0.8313141825199126, 0.3694729700088501, 2.2168378200531005, 0.8313141825199126, 1.1084189100265502, 2.2168378200531005, 17.734702560424804]  wait [2, 0, 2, 2, 2, 2, 2, 4, 2, 2, 4, 4, 2, 2]  inc [1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 7
$$$$$$$$$$$$$ epoch  59  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1732.278250694275 , diff:  1732.278250694275
adv train loss:  -1737.0350551605225 , diff:  4.756804466247559
adv train loss:  -1714.9421710968018 , diff:  22.092884063720703
adv train loss:  -1721.296392440796 , diff:  6.354221343994141
adv train loss:  -1723.7147760391235 , diff:  2.4183835983276367
adv train loss:  -1714.2847728729248 , diff:  9.43000316619873
adv train loss:  -1717.801706314087 , diff:  3.5169334411621094
adv train loss:  -1705.8166599273682 , diff:  11.98504638671875
adv train loss:  -1714.556372642517 , diff:  8.739712715148926
adv train loss:  -1710.4016427993774 , diff:  4.154729843139648
layer  0  adv train finish, try to retain  2
test acc: top1 ->  10.98 ; top5 ->  50.0  and loss:  1551.234962463379
forward train acc: top1 ->  18.383999995117186 ; top5 ->  63.92199999511719  and loss:  518.4465539455414
test acc: top1 ->  10.08 ; top5 ->  51.94  and loss:  366.6946918964386
forward train acc: top1 ->  20.985999993286132 ; top5 ->  71.13400001464844  and loss:  213.60776615142822
test acc: top1 ->  23.97 ; top5 ->  72.42  and loss:  212.03571832180023
forward train acc: top1 ->  23.69199999572754 ; top5 ->  75.21199998535157  and loss:  203.44050931930542
test acc: top1 ->  27.48 ; top5 ->  74.96  and loss:  205.43564355373383
forward train acc: top1 ->  26.049999995117187 ; top5 ->  77.48199997558594  and loss:  197.15790271759033
test acc: top1 ->  28.78 ; top5 ->  76.02  and loss:  202.0024505853653
forward train acc: top1 ->  27.864000003051757 ; top5 ->  79.41000001464843  and loss:  192.4121971130371
test acc: top1 ->  30.77 ; top5 ->  80.07  and loss:  192.2241973876953
forward train acc: top1 ->  29.226000002441406 ; top5 ->  80.74199997802734  and loss:  188.5902155637741
test acc: top1 ->  32.49 ; top5 ->  82.77  and loss:  186.40953826904297
forward train acc: top1 ->  29.91999999938965 ; top5 ->  81.28599997314453  and loss:  186.87633776664734
test acc: top1 ->  32.94 ; top5 ->  83.07  and loss:  185.31044578552246
forward train acc: top1 ->  30.599999998779296 ; top5 ->  81.7520000024414  and loss:  185.53109860420227
test acc: top1 ->  33.28 ; top5 ->  82.75  and loss:  184.85954105854034
forward train acc: top1 ->  31.150000009765623 ; top5 ->  82.53800001708984  and loss:  183.54794764518738
test acc: top1 ->  33.84 ; top5 ->  84.51  and loss:  180.77393972873688
forward train acc: top1 ->  31.811999993896485 ; top5 ->  82.8619999975586  and loss:  182.13301301002502
test acc: top1 ->  34.66 ; top5 ->  84.66  and loss:  179.35110127925873
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  30 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -81.53758931159973 , diff:  81.53758931159973
adv train loss:  -81.54825925827026 , diff:  0.010669946670532227
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  49
test acc: top1 ->  10.0 ; top5 ->  50.09  and loss:  312.9052023887634
forward train acc: top1 ->  99.2840000024414 ; top5 ->  99.996  and loss:  10.667944346554577
test acc: top1 ->  91.37 ; top5 ->  99.22  and loss:  41.07863026857376
forward train acc: top1 ->  99.82799997558594 ; top5 ->  100.0  and loss:  0.8301710048690438
test acc: top1 ->  91.53 ; top5 ->  99.16  and loss:  45.773760698735714
forward train acc: top1 ->  99.87599997558594 ; top5 ->  100.0  and loss:  0.5128999315202236
test acc: top1 ->  91.65 ; top5 ->  99.18  and loss:  49.814076878130436
forward train acc: top1 ->  99.888 ; top5 ->  100.0  and loss:  0.4539636857807636
test acc: top1 ->  91.58 ; top5 ->  99.13  and loss:  50.9929421171546
forward train acc: top1 ->  99.85799997558594 ; top5 ->  100.0  and loss:  0.43726719357073307
test acc: top1 ->  91.71 ; top5 ->  99.13  and loss:  54.071050092577934
forward train acc: top1 ->  99.916 ; top5 ->  100.0  and loss:  0.3122635902836919
test acc: top1 ->  91.75 ; top5 ->  99.07  and loss:  56.838850647211075
forward train acc: top1 ->  99.902 ; top5 ->  100.0  and loss:  0.34099567541852593
test acc: top1 ->  91.85 ; top5 ->  99.18  and loss:  56.049809619784355
forward train acc: top1 ->  99.906 ; top5 ->  100.0  and loss:  0.29862634639721364
test acc: top1 ->  91.94 ; top5 ->  99.13  and loss:  55.898738503456116
forward train acc: top1 ->  99.952 ; top5 ->  100.0  and loss:  0.23099351325072348
test acc: top1 ->  91.95 ; top5 ->  99.11  and loss:  57.84703974425793
forward train acc: top1 ->  99.916 ; top5 ->  100.0  and loss:  0.2931805530679412
test acc: top1 ->  91.92 ; top5 ->  99.08  and loss:  57.22717659920454
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  53 / 64 , inc:  4
---------------- start layer  2  ---------------
adv train loss:  -734.2778211999685 , diff:  734.2778211999685
adv train loss:  -1284.6927099227905 , diff:  550.4148887228221
adv train loss:  -1297.1461572647095 , diff:  12.453447341918945
adv train loss:  -1302.0069007873535 , diff:  4.860743522644043
adv train loss:  -1305.0721731185913 , diff:  3.065272331237793
adv train loss:  -1299.7604656219482 , diff:  5.311707496643066
adv train loss:  -1301.2786493301392 , diff:  1.518183708190918
adv train loss:  -1295.9557094573975 , diff:  5.322939872741699
adv train loss:  -1304.8134307861328 , diff:  8.857721328735352
adv train loss:  -1298.3257160186768 , diff:  6.487714767456055
layer  2  adv train finish, try to retain  40
test acc: top1 ->  10.0 ; top5 ->  49.44  and loss:  728.2465777397156
forward train acc: top1 ->  85.77199997314453 ; top5 ->  98.73199997802735  and loss:  50.81778034567833
test acc: top1 ->  81.75 ; top5 ->  97.81  and loss:  62.15022239089012
forward train acc: top1 ->  88.91199997070312 ; top5 ->  99.23999997558593  and loss:  34.446003913879395
test acc: top1 ->  83.45 ; top5 ->  98.27  and loss:  55.771976709365845
forward train acc: top1 ->  90.30599998779297 ; top5 ->  99.42399997802734  and loss:  29.627858474850655
test acc: top1 ->  84.7 ; top5 ->  98.52  and loss:  52.62458437681198
forward train acc: top1 ->  91.35599999267578 ; top5 ->  99.554  and loss:  26.340407967567444
test acc: top1 ->  85.35 ; top5 ->  98.74  and loss:  50.848704785108566
forward train acc: top1 ->  92.03200000732421 ; top5 ->  99.632  and loss:  23.75773310661316
test acc: top1 ->  85.79 ; top5 ->  98.87  and loss:  48.00300291180611
forward train acc: top1 ->  92.77999997558594 ; top5 ->  99.6740000024414  and loss:  21.980275869369507
test acc: top1 ->  86.06 ; top5 ->  98.92  and loss:  47.68669831752777
forward train acc: top1 ->  92.84199998535156 ; top5 ->  99.70399997802734  and loss:  21.342644587159157
test acc: top1 ->  86.41 ; top5 ->  98.97  and loss:  47.0012826025486
forward train acc: top1 ->  93.15799997802735 ; top5 ->  99.71  and loss:  20.605620071291924
test acc: top1 ->  86.64 ; top5 ->  98.94  and loss:  46.49774222075939
forward train acc: top1 ->  93.33399999755859 ; top5 ->  99.77799997558594  and loss:  19.661044105887413
test acc: top1 ->  86.98 ; top5 ->  99.0  and loss:  46.069643542170525
forward train acc: top1 ->  93.64399997802734 ; top5 ->  99.77400000244141  and loss:  18.826044023036957
test acc: top1 ->  87.0 ; top5 ->  98.95  and loss:  46.35635828971863
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -405.6187784150243 , diff:  405.6187784150243
adv train loss:  -732.0315165519714 , diff:  326.41273813694715
adv train loss:  -752.3934321403503 , diff:  20.361915588378906
adv train loss:  -758.6633024215698 , diff:  6.269870281219482
adv train loss:  -759.763557434082 , diff:  1.100255012512207
adv train loss:  -760.3304347991943 , diff:  0.5668773651123047
adv train loss:  -757.7930088043213 , diff:  2.537425994873047
adv train loss:  -758.6748580932617 , diff:  0.8818492889404297
adv train loss:  -758.9419178962708 , diff:  0.2670598030090332
adv train loss:  -762.1902685165405 , diff:  3.2483506202697754
layer  3  adv train finish, try to retain  20
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  511.6428542137146
forward train acc: top1 ->  72.44599999511719 ; top5 ->  96.85599999023438  and loss:  84.60449087619781
test acc: top1 ->  71.97 ; top5 ->  96.68  and loss:  87.52423858642578
forward train acc: top1 ->  76.21600001464844 ; top5 ->  97.74200000976562  and loss:  70.46934252977371
test acc: top1 ->  74.6 ; top5 ->  97.11  and loss:  79.77141273021698
forward train acc: top1 ->  78.69800001220703 ; top5 ->  98.10799998291016  and loss:  63.27019762992859
test acc: top1 ->  76.03 ; top5 ->  97.57  and loss:  75.0713859796524
forward train acc: top1 ->  80.01799997314453 ; top5 ->  98.35000000732421  and loss:  59.132805079221725
test acc: top1 ->  77.19 ; top5 ->  97.78  and loss:  70.71211168169975
forward train acc: top1 ->  80.96599997802734 ; top5 ->  98.53400000488281  and loss:  55.98763248324394
test acc: top1 ->  77.88 ; top5 ->  97.52  and loss:  69.39605724811554
forward train acc: top1 ->  81.78199999511719 ; top5 ->  98.60999997558594  and loss:  53.69215685129166
test acc: top1 ->  78.47 ; top5 ->  97.98  and loss:  66.99222275614738
forward train acc: top1 ->  82.18800000488281 ; top5 ->  98.69600000488282  and loss:  52.33913308382034
test acc: top1 ->  78.81 ; top5 ->  98.05  and loss:  65.84521690011024
forward train acc: top1 ->  82.7219999975586 ; top5 ->  98.79000000732422  and loss:  50.92270639538765
test acc: top1 ->  78.91 ; top5 ->  98.03  and loss:  65.3730517923832
forward train acc: top1 ->  82.82600000488281 ; top5 ->  98.80999998046875  and loss:  50.15844377875328
test acc: top1 ->  79.35 ; top5 ->  97.98  and loss:  64.3425685763359
forward train acc: top1 ->  83.06199997558593 ; top5 ->  98.90399997802734  and loss:  49.02878162264824
test acc: top1 ->  79.78 ; top5 ->  98.01  and loss:  63.384310364723206
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -86.57326120883226 , diff:  86.57326120883226
adv train loss:  -427.3947403430939 , diff:  340.8214791342616
adv train loss:  -509.78602504730225 , diff:  82.39128470420837
adv train loss:  -533.1017422676086 , diff:  23.315717220306396
adv train loss:  -556.1325144767761 , diff:  23.03077220916748
adv train loss:  -554.8769488334656 , diff:  1.2555656433105469
adv train loss:  -553.8004803657532 , diff:  1.0764684677124023
adv train loss:  -584.387026309967 , diff:  30.586545944213867
adv train loss:  -586.7207236289978 , diff:  2.3336973190307617
adv train loss:  -606.4331073760986 , diff:  19.71238374710083
layer  4  adv train finish, try to retain  13
test acc: top1 ->  9.99 ; top5 ->  49.97  and loss:  780.882538318634
forward train acc: top1 ->  56.32600000854492 ; top5 ->  93.76999999267578  and loss:  122.61461126804352
test acc: top1 ->  60.32 ; top5 ->  95.26  and loss:  112.91195434331894
forward train acc: top1 ->  63.220000001220704 ; top5 ->  95.85399997558594  and loss:  100.51868295669556
test acc: top1 ->  63.93 ; top5 ->  96.15  and loss:  104.50072473287582
forward train acc: top1 ->  67.20599997802735 ; top5 ->  96.87400001464843  and loss:  89.76663196086884
test acc: top1 ->  67.17 ; top5 ->  97.1  and loss:  94.42654347419739
forward train acc: top1 ->  69.6599999975586 ; top5 ->  97.38799998046875  and loss:  83.60348796844482
test acc: top1 ->  69.75 ; top5 ->  97.37  and loss:  88.17877399921417
forward train acc: top1 ->  71.50399999023438 ; top5 ->  97.72200000976562  and loss:  78.45712119340897
test acc: top1 ->  70.73 ; top5 ->  97.58  and loss:  85.67190963029861
forward train acc: top1 ->  72.8800000024414 ; top5 ->  98.02799998291016  and loss:  75.00972902774811
test acc: top1 ->  71.44 ; top5 ->  97.53  and loss:  83.46700447797775
forward train acc: top1 ->  73.61799997314453 ; top5 ->  98.02400000488281  and loss:  73.29896873235703
test acc: top1 ->  72.4 ; top5 ->  97.7  and loss:  81.43484538793564
forward train acc: top1 ->  74.28799999023437 ; top5 ->  98.09799998535156  and loss:  71.98803615570068
test acc: top1 ->  72.55 ; top5 ->  97.72  and loss:  80.39541536569595
forward train acc: top1 ->  74.54600001220703 ; top5 ->  98.18599998046875  and loss:  71.01840990781784
test acc: top1 ->  73.11 ; top5 ->  97.62  and loss:  79.61571609973907
forward train acc: top1 ->  75.04799997070313 ; top5 ->  98.22800000976562  and loss:  69.67851430177689
test acc: top1 ->  73.47 ; top5 ->  97.81  and loss:  78.27038538455963
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  156 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -106.98622986674309 , diff:  106.98622986674309
adv train loss:  -469.81015133857727 , diff:  362.8239214718342
adv train loss:  -621.085147857666 , diff:  151.27499651908875
adv train loss:  -695.7737674713135 , diff:  74.68861961364746
adv train loss:  -704.0497097969055 , diff:  8.275942325592041
adv train loss:  -706.0170655250549 , diff:  1.967355728149414
adv train loss:  -710.6886630058289 , diff:  4.671597480773926
adv train loss:  -722.0028467178345 , diff:  11.314183712005615
adv train loss:  -735.6540937423706 , diff:  13.651247024536133
adv train loss:  -743.870792388916 , diff:  8.21669864654541
layer  5  adv train finish, try to retain  14
test acc: top1 ->  12.34 ; top5 ->  50.66  and loss:  442.13635444641113
forward train acc: top1 ->  67.64800000732421 ; top5 ->  96.96200000732422  and loss:  90.42023175954819
test acc: top1 ->  70.8 ; top5 ->  97.54  and loss:  86.65666401386261
forward train acc: top1 ->  75.99799998046875 ; top5 ->  98.38600000976562  and loss:  67.83140975236893
test acc: top1 ->  74.58 ; top5 ->  97.98  and loss:  76.69761198759079
forward train acc: top1 ->  79.19600000244141 ; top5 ->  98.73400001220703  and loss:  59.233298897743225
test acc: top1 ->  76.7 ; top5 ->  98.27  and loss:  70.55330500006676
forward train acc: top1 ->  81.1440000024414 ; top5 ->  98.97600000732422  and loss:  53.67231461405754
test acc: top1 ->  78.16 ; top5 ->  98.48  and loss:  65.7889831662178
forward train acc: top1 ->  82.35399998046876 ; top5 ->  99.084  and loss:  50.06555289030075
test acc: top1 ->  79.16 ; top5 ->  98.43  and loss:  63.31165423989296
forward train acc: top1 ->  83.51199998779298 ; top5 ->  99.17799997558593  and loss:  47.051119804382324
test acc: top1 ->  79.63 ; top5 ->  98.49  and loss:  62.43726500868797
forward train acc: top1 ->  83.92199998291015 ; top5 ->  99.17800000244141  and loss:  45.944664508104324
test acc: top1 ->  80.09 ; top5 ->  98.5  and loss:  61.7022223174572
forward train acc: top1 ->  84.33000000976563 ; top5 ->  99.28000000244141  and loss:  44.40033274888992
test acc: top1 ->  80.87 ; top5 ->  98.53  and loss:  59.249857515096664
forward train acc: top1 ->  84.76199998535157 ; top5 ->  99.25600000488281  and loss:  43.63266956806183
test acc: top1 ->  81.13 ; top5 ->  98.63  and loss:  58.3992081284523
forward train acc: top1 ->  85.16400001464844 ; top5 ->  99.2960000024414  and loss:  42.46715712547302
test acc: top1 ->  81.54 ; top5 ->  98.64  and loss:  58.351726442575455
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -170.6591727808118 , diff:  170.6591727808118
adv train loss:  -930.1386432647705 , diff:  759.4794704839587
adv train loss:  -1064.4340286254883 , diff:  134.29538536071777
adv train loss:  -1086.6149034500122 , diff:  22.180874824523926
adv train loss:  -1086.932656288147 , diff:  0.3177528381347656
adv train loss:  -1091.3126735687256 , diff:  4.380017280578613
adv train loss:  -1111.3737831115723 , diff:  20.06110954284668
adv train loss:  -1110.3654861450195 , diff:  1.0082969665527344
adv train loss:  -1110.357213973999 , diff:  0.008272171020507812
layer  6  adv train finish, try to retain  233
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
adv train loss:  -15.458182714879513 , diff:  15.458182714879513
adv train loss:  -15.810048893094063 , diff:  0.35186617821455
adv train loss:  -15.672925099730492 , diff:  0.13712379336357117
adv train loss:  -15.558197662234306 , diff:  0.1147274374961853
adv train loss:  -15.756212778389454 , diff:  0.19801511615514755
adv train loss:  -15.653419181704521 , diff:  0.10279359668493271
adv train loss:  -15.83919395506382 , diff:  0.1857747733592987
adv train loss:  -15.839770868420601 , diff:  0.0005769133567810059
layer  8  adv train finish, try to retain  457
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -59.66951808333397 , diff:  59.66951808333397
adv train loss:  -152.33387398719788 , diff:  92.6643559038639
adv train loss:  -193.6844573020935 , diff:  41.35058331489563
adv train loss:  -215.0002636909485 , diff:  21.31580638885498
adv train loss:  -217.02322506904602 , diff:  2.022961378097534
adv train loss:  -231.3285937309265 , diff:  14.305368661880493
adv train loss:  -242.663902759552 , diff:  11.335309028625488
adv train loss:  -247.57919192314148 , diff:  4.9152891635894775
adv train loss:  -286.86310601234436 , diff:  39.28391408920288
adv train loss:  -310.0617125034332 , diff:  23.198606491088867
layer  9  adv train finish, try to retain  1
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  1443.290286064148
forward train acc: top1 ->  57.42600001708984 ; top5 ->  95.82399997558593  and loss:  121.77646470069885
test acc: top1 ->  45.54 ; top5 ->  96.52  and loss:  179.220951795578
forward train acc: top1 ->  88.39200000732421 ; top5 ->  99.6360000024414  and loss:  47.63662511110306
test acc: top1 ->  84.19 ; top5 ->  97.08  and loss:  71.31944870948792
forward train acc: top1 ->  94.37399997070312 ; top5 ->  99.798  and loss:  26.033270731568336
test acc: top1 ->  86.14 ; top5 ->  97.14  and loss:  67.97043478488922
forward train acc: top1 ->  96.09599999267579 ; top5 ->  99.88999997558594  and loss:  17.449615344405174
test acc: top1 ->  87.42 ; top5 ->  97.3  and loss:  66.26257380843163
forward train acc: top1 ->  96.91999999023437 ; top5 ->  99.938  and loss:  13.064139991998672
test acc: top1 ->  87.37 ; top5 ->  97.21  and loss:  70.84062354266644
forward train acc: top1 ->  97.54800000488281 ; top5 ->  99.938  and loss:  10.541594296693802
test acc: top1 ->  87.93 ; top5 ->  97.25  and loss:  68.93720418214798
forward train acc: top1 ->  97.69800000976562 ; top5 ->  99.948  and loss:  9.551507957279682
test acc: top1 ->  87.87 ; top5 ->  97.17  and loss:  71.0682914108038
forward train acc: top1 ->  97.91000001464843 ; top5 ->  99.946  and loss:  8.626410000026226
test acc: top1 ->  88.08 ; top5 ->  97.15  and loss:  71.3166073858738
forward train acc: top1 ->  98.21399998291015 ; top5 ->  99.972  and loss:  7.438298109918833
test acc: top1 ->  88.39 ; top5 ->  97.18  and loss:  71.97750835120678
forward train acc: top1 ->  98.23400000976562 ; top5 ->  99.978  and loss:  7.13677254319191
test acc: top1 ->  88.35 ; top5 ->  97.15  and loss:  73.83100998401642
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  96 / 512 , inc:  1
---------------- start layer  10  ---------------
### skip layer  10 wait:  4  ###
---------------- start layer  11  ---------------
### skip layer  11 wait:  4  ###
---------------- start layer  12  ---------------
adv train loss:  -152.93284738063812 , diff:  152.93284738063812
adv train loss:  -239.16605520248413 , diff:  86.23320782184601
adv train loss:  -498.7722156047821 , diff:  259.606160402298
adv train loss:  -593.6767749786377 , diff:  94.90455937385559
adv train loss:  -633.2917861938477 , diff:  39.61501121520996
adv train loss:  -632.8490633964539 , diff:  0.44272279739379883
adv train loss:  -633.4912357330322 , diff:  0.6421723365783691
adv train loss:  -676.997163772583 , diff:  43.50592803955078
adv train loss:  -684.0992131233215 , diff:  7.102049350738525
adv train loss:  -682.8353929519653 , diff:  1.2638201713562012
layer  12  adv train finish, try to retain  1
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  4791.759422302246
forward train acc: top1 ->  39.66199998901367 ; top5 ->  96.04599997558594  and loss:  223.10804915428162
test acc: top1 ->  49.7 ; top5 ->  97.36  and loss:  129.0982961654663
forward train acc: top1 ->  67.21199999267579 ; top5 ->  99.834  and loss:  97.4676393866539
test acc: top1 ->  66.71 ; top5 ->  97.35  and loss:  112.92230272293091
forward train acc: top1 ->  74.99199998046875 ; top5 ->  99.89399997558594  and loss:  89.22761869430542
test acc: top1 ->  69.63 ; top5 ->  97.36  and loss:  109.04813581705093
forward train acc: top1 ->  77.05600001220704 ; top5 ->  99.942  and loss:  84.23744195699692
test acc: top1 ->  69.04 ; top5 ->  97.58  and loss:  106.17085713148117
forward train acc: top1 ->  78.95799999267578 ; top5 ->  99.942  and loss:  79.69513565301895
test acc: top1 ->  74.21 ; top5 ->  97.51  and loss:  103.48402142524719
forward train acc: top1 ->  80.66400000244141 ; top5 ->  99.948  and loss:  76.5413726568222
test acc: top1 ->  73.12 ; top5 ->  97.52  and loss:  102.22311228513718
forward train acc: top1 ->  82.42999998535156 ; top5 ->  99.96599997558594  and loss:  74.12824082374573
test acc: top1 ->  73.05 ; top5 ->  97.57  and loss:  101.676782310009
forward train acc: top1 ->  83.15 ; top5 ->  99.96  and loss:  72.47009235620499
test acc: top1 ->  75.51 ; top5 ->  97.54  and loss:  100.42482966184616
forward train acc: top1 ->  84.762 ; top5 ->  99.972  and loss:  70.4376956820488
test acc: top1 ->  78.43 ; top5 ->  97.53  and loss:  99.41357320547104
forward train acc: top1 ->  86.68200000244141 ; top5 ->  99.964  and loss:  68.20985186100006
test acc: top1 ->  76.93 ; top5 ->  97.6  and loss:  98.82732832431793
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  41 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -21377.309951782227 , diff:  21377.309951782227
adv train loss:  -34905.81689453125 , diff:  13528.506942749023
adv train loss:  -48079.432189941406 , diff:  13173.615295410156
adv train loss:  -61105.03192138672 , diff:  13025.599731445312
adv train loss:  -76917.30444335938 , diff:  15812.272521972656
adv train loss:  -96805.79089355469 , diff:  19888.486450195312
adv train loss:  -114592.05639648438 , diff:  17786.265502929688
adv train loss:  -131492.8748779297 , diff:  16900.818481445312
adv train loss:  -147847.8621826172 , diff:  16354.9873046875
adv train loss:  -163869.76171875 , diff:  16021.899536132812
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  16
test acc: top1 ->  17.84 ; top5 ->  81.81  and loss:  1674.7686576843262
forward train acc: top1 ->  79.89199997558593 ; top5 ->  99.106  and loss:  202.2887085713446
test acc: top1 ->  90.5 ; top5 ->  98.75  and loss:  77.68359149992466
forward train acc: top1 ->  99.35799997558594 ; top5 ->  99.99  and loss:  4.085501603782177
test acc: top1 ->  91.07 ; top5 ->  98.97  and loss:  73.03656449913979
forward train acc: top1 ->  99.65200000244141 ; top5 ->  99.998  and loss:  1.8665767898783088
test acc: top1 ->  91.27 ; top5 ->  99.03  and loss:  71.71426126360893
forward train acc: top1 ->  99.774 ; top5 ->  99.996  and loss:  1.068922090344131
test acc: top1 ->  91.3 ; top5 ->  99.03  and loss:  70.67164591699839
forward train acc: top1 ->  99.7460000024414 ; top5 ->  99.998  and loss:  0.9394077132456005
test acc: top1 ->  91.58 ; top5 ->  99.09  and loss:  71.85700604319572
forward train acc: top1 ->  99.79 ; top5 ->  99.998  and loss:  0.7306946653407067
test acc: top1 ->  91.61 ; top5 ->  99.09  and loss:  71.10471411794424
forward train acc: top1 ->  99.868 ; top5 ->  99.994  and loss:  0.6230174396187067
test acc: top1 ->  91.62 ; top5 ->  99.09  and loss:  71.29225607216358
forward train acc: top1 ->  99.888 ; top5 ->  100.0  and loss:  0.4500373634509742
test acc: top1 ->  91.59 ; top5 ->  99.01  and loss:  72.97534041106701
forward train acc: top1 ->  99.862 ; top5 ->  99.998  and loss:  0.5440203526522964
test acc: top1 ->  91.7 ; top5 ->  99.09  and loss:  71.95448379963636
forward train acc: top1 ->  99.876 ; top5 ->  100.0  and loss:  0.506627588532865
test acc: top1 ->  91.68 ; top5 ->  99.09  and loss:  72.58980897068977
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  17 / 512 , inc:  1
layer  0  :  0.46875  ==>  30 / 64 , inc:  1
layer  1  :  0.828125  ==>  53 / 64 , inc:  2
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.3828125  ==>  98 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.041015625  ==>  21 / 512 , inc:  1
layer  9  :  0.1875  ==>  96 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.033203125  ==>  17 / 512 , inc:  1
eps [1.6626283650398253, 4.433675640106201, 1.6626283650398253, 1.6626283650398253, 1.6626283650398253, 1.6626283650398253, 2.2168378200531005, 0.8313141825199126, 0.7389459400177002, 1.6626283650398253, 0.8313141825199126, 1.1084189100265502, 1.6626283650398253, 13.301026920318602]  wait [4, 2, 4, 4, 4, 4, 2, 3, 2, 4, 3, 3, 4, 4]  inc [1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 7
$$$$$$$$$$$$$ epoch  60  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -2196.860544204712 , diff:  2196.860544204712
adv train loss:  -2189.2315196990967 , diff:  7.629024505615234
adv train loss:  -2199.2052669525146 , diff:  9.973747253417969
adv train loss:  -2184.135034561157 , diff:  15.070232391357422
adv train loss:  -2189.3408794403076 , diff:  5.205844879150391
adv train loss:  -2188.9151573181152 , diff:  0.4257221221923828
layer  0  adv train finish, try to retain  1
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  2162.1020698547363
forward train acc: top1 ->  19.925999994506835 ; top5 ->  65.41999998046875  and loss:  608.3663041591644
test acc: top1 ->  10.37 ; top5 ->  51.83  and loss:  432.04142355918884
forward train acc: top1 ->  24.08600000305176 ; top5 ->  72.74799997558594  and loss:  212.07090187072754
test acc: top1 ->  27.85 ; top5 ->  76.39  and loss:  201.03261351585388
forward train acc: top1 ->  26.449999997558592 ; top5 ->  77.09600000488281  and loss:  199.0315384864807
test acc: top1 ->  30.4 ; top5 ->  80.39  and loss:  192.7583508491516
forward train acc: top1 ->  28.393999994506835 ; top5 ->  79.19399998291016  and loss:  193.34518110752106
test acc: top1 ->  31.89 ; top5 ->  81.88  and loss:  187.91783022880554
forward train acc: top1 ->  29.665999994506837 ; top5 ->  80.62999998046875  and loss:  189.38574647903442
test acc: top1 ->  33.68 ; top5 ->  83.2  and loss:  185.1406341791153
forward train acc: top1 ->  30.53399999206543 ; top5 ->  81.18799997314453  and loss:  187.47416758537292
test acc: top1 ->  33.74 ; top5 ->  83.78  and loss:  182.8655788898468
forward train acc: top1 ->  31.197999993896484 ; top5 ->  81.82800001708985  and loss:  185.6006554365158
test acc: top1 ->  34.51 ; top5 ->  84.79  and loss:  181.12314522266388
forward train acc: top1 ->  31.54399998901367 ; top5 ->  82.2759999975586  and loss:  184.51095950603485
test acc: top1 ->  34.73 ; top5 ->  84.85  and loss:  180.4513646364212
forward train acc: top1 ->  32.25 ; top5 ->  82.44000001953125  and loss:  183.21458446979523
test acc: top1 ->  35.74 ; top5 ->  85.43  and loss:  178.32928431034088
forward train acc: top1 ->  33.21999999633789 ; top5 ->  83.12199998291015  and loss:  181.30056726932526
test acc: top1 ->  35.91 ; top5 ->  85.7  and loss:  177.28494429588318
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  30 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -78.08196306228638 , diff:  78.08196306228638
adv train loss:  -78.12721568346024 , diff:  0.04525262117385864
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  51
test acc: top1 ->  10.0 ; top5 ->  50.01  and loss:  371.450425863266
forward train acc: top1 ->  99.014 ; top5 ->  99.998  and loss:  10.8045888915658
test acc: top1 ->  91.64 ; top5 ->  99.26  and loss:  37.63796494528651
forward train acc: top1 ->  99.77400000244141 ; top5 ->  100.0  and loss:  0.970285183750093
test acc: top1 ->  91.77 ; top5 ->  99.3  and loss:  43.78985571861267
forward train acc: top1 ->  99.812 ; top5 ->  100.0  and loss:  0.6805472336709499
test acc: top1 ->  91.87 ; top5 ->  99.22  and loss:  46.359668739140034
forward train acc: top1 ->  99.856 ; top5 ->  99.998  and loss:  0.5357564687728882
test acc: top1 ->  91.93 ; top5 ->  99.24  and loss:  49.636927515268326
forward train acc: top1 ->  99.89 ; top5 ->  100.0  and loss:  0.4178540613502264
test acc: top1 ->  91.91 ; top5 ->  99.21  and loss:  52.00212533026934
forward train acc: top1 ->  99.904 ; top5 ->  100.0  and loss:  0.3536916591692716
test acc: top1 ->  91.8 ; top5 ->  99.29  and loss:  51.69535221159458
forward train acc: top1 ->  99.91999997558594 ; top5 ->  100.0  and loss:  0.2763281837105751
test acc: top1 ->  91.94 ; top5 ->  99.22  and loss:  52.45966089516878
forward train acc: top1 ->  99.892 ; top5 ->  99.998  and loss:  0.34406653116457164
test acc: top1 ->  91.89 ; top5 ->  99.27  and loss:  53.079527750611305
forward train acc: top1 ->  99.89599997558594 ; top5 ->  100.0  and loss:  0.3405676055699587
test acc: top1 ->  92.05 ; top5 ->  99.3  and loss:  53.661504216492176
forward train acc: top1 ->  99.902 ; top5 ->  100.0  and loss:  0.32113934657536447
test acc: top1 ->  91.99 ; top5 ->  99.29  and loss:  53.65724641829729
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  53 / 64 , inc:  2
---------------- start layer  2  ---------------
adv train loss:  -742.23445844464 , diff:  742.23445844464
adv train loss:  -1325.1311464309692 , diff:  582.8966879863292
adv train loss:  -1334.416856765747 , diff:  9.285710334777832
adv train loss:  -1332.8690099716187 , diff:  1.547846794128418
adv train loss:  -1332.421202659607 , diff:  0.44780731201171875
adv train loss:  -1330.9062805175781 , diff:  1.5149221420288086
adv train loss:  -1336.6521434783936 , diff:  5.74586296081543
adv train loss:  -1325.327675819397 , diff:  11.324467658996582
adv train loss:  -1324.2303142547607 , diff:  1.0973615646362305
adv train loss:  -1325.8647413253784 , diff:  1.6344270706176758
layer  2  adv train finish, try to retain  53
test acc: top1 ->  10.0 ; top5 ->  55.1  and loss:  2075.3507118225098
forward train acc: top1 ->  93.96399997070313 ; top5 ->  99.72399997558594  and loss:  21.065509408712387
test acc: top1 ->  87.3 ; top5 ->  98.9  and loss:  47.47439210116863
forward train acc: top1 ->  95.32000001220703 ; top5 ->  99.852  and loss:  14.490991070866585
test acc: top1 ->  87.84 ; top5 ->  99.05  and loss:  46.80336418747902
forward train acc: top1 ->  95.93799998779296 ; top5 ->  99.894  and loss:  12.307347320020199
test acc: top1 ->  88.36 ; top5 ->  99.11  and loss:  44.03856109082699
forward train acc: top1 ->  96.34399999023438 ; top5 ->  99.932  and loss:  10.875758059322834
test acc: top1 ->  88.66 ; top5 ->  99.17  and loss:  44.12434424459934
forward train acc: top1 ->  96.76000001220703 ; top5 ->  99.93400000244141  and loss:  9.61446300148964
test acc: top1 ->  88.95 ; top5 ->  99.11  and loss:  45.05950692296028
forward train acc: top1 ->  96.92800000976563 ; top5 ->  99.948  and loss:  8.919483553618193
test acc: top1 ->  89.03 ; top5 ->  99.18  and loss:  44.15999974310398
forward train acc: top1 ->  97.09600000732422 ; top5 ->  99.956  and loss:  8.46184604242444
test acc: top1 ->  89.16 ; top5 ->  99.27  and loss:  43.04460373520851
forward train acc: top1 ->  97.20600001464844 ; top5 ->  99.95799997558593  and loss:  8.029592327773571
test acc: top1 ->  89.03 ; top5 ->  99.24  and loss:  43.614108711481094
forward train acc: top1 ->  97.30599998291015 ; top5 ->  99.948  and loss:  7.989518936723471
test acc: top1 ->  89.09 ; top5 ->  99.23  and loss:  43.033074364066124
forward train acc: top1 ->  97.33199998291016 ; top5 ->  99.98  and loss:  7.678148299455643
test acc: top1 ->  89.42 ; top5 ->  99.21  and loss:  43.712293058633804
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -501.3767593726516 , diff:  501.3767593726516
adv train loss:  -857.8733768463135 , diff:  356.4966174736619
adv train loss:  -867.1185312271118 , diff:  9.24515438079834
adv train loss:  -867.6782865524292 , diff:  0.5597553253173828
adv train loss:  -873.0011224746704 , diff:  5.322835922241211
adv train loss:  -867.7385635375977 , diff:  5.262558937072754
adv train loss:  -870.0380458831787 , diff:  2.2994823455810547
adv train loss:  -873.1448984146118 , diff:  3.1068525314331055
adv train loss:  -868.8792843818665 , diff:  4.265614032745361
adv train loss:  -870.7164993286133 , diff:  1.8372149467468262
layer  3  adv train finish, try to retain  31
test acc: top1 ->  10.0 ; top5 ->  51.94  and loss:  909.7717571258545
forward train acc: top1 ->  82.65599998779297 ; top5 ->  98.60200000732422  and loss:  54.01339769363403
test acc: top1 ->  79.87 ; top5 ->  98.11  and loss:  64.49306842684746
forward train acc: top1 ->  85.8500000024414 ; top5 ->  99.24399998046874  and loss:  41.30498152971268
test acc: top1 ->  81.45 ; top5 ->  98.61  and loss:  58.71831318736076
forward train acc: top1 ->  87.37799999267578 ; top5 ->  99.34599997558594  and loss:  36.829643458127975
test acc: top1 ->  82.38 ; top5 ->  98.52  and loss:  57.44953814148903
forward train acc: top1 ->  88.44000001953125 ; top5 ->  99.42599997558594  and loss:  34.06120586395264
test acc: top1 ->  83.35 ; top5 ->  98.65  and loss:  54.79271358251572
forward train acc: top1 ->  89.01399998535156 ; top5 ->  99.4780000024414  and loss:  32.03784731030464
test acc: top1 ->  84.03 ; top5 ->  98.81  and loss:  52.30436010658741
forward train acc: top1 ->  89.44000000244141 ; top5 ->  99.588  and loss:  30.696273267269135
test acc: top1 ->  83.89 ; top5 ->  98.87  and loss:  52.82188904285431
forward train acc: top1 ->  89.86200001464844 ; top5 ->  99.59200000244141  and loss:  29.58615869283676
test acc: top1 ->  84.46 ; top5 ->  98.93  and loss:  51.31310307979584
forward train acc: top1 ->  89.99399998046874 ; top5 ->  99.6000000024414  and loss:  28.986114278435707
test acc: top1 ->  84.64 ; top5 ->  98.92  and loss:  51.06551416218281
forward train acc: top1 ->  90.15200001464844 ; top5 ->  99.63199997558594  and loss:  28.63884800672531
test acc: top1 ->  84.85 ; top5 ->  98.97  and loss:  50.2019839733839
forward train acc: top1 ->  90.39199997070313 ; top5 ->  99.61000000244141  and loss:  28.038831308484077
test acc: top1 ->  84.84 ; top5 ->  98.98  and loss:  49.943027168512344
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -69.26298098266125 , diff:  69.26298098266125
adv train loss:  -401.45359802246094 , diff:  332.1906170397997
adv train loss:  -513.423481464386 , diff:  111.96988344192505
adv train loss:  -605.3169622421265 , diff:  91.89348077774048
adv train loss:  -724.8389348983765 , diff:  119.52197265625
adv train loss:  -767.5590462684631 , diff:  42.72011137008667
adv train loss:  -777.2481198310852 , diff:  9.68907356262207
adv train loss:  -778.5565376281738 , diff:  1.308417797088623
adv train loss:  -788.9363780021667 , diff:  10.37984037399292
adv train loss:  -810.0018124580383 , diff:  21.065434455871582
layer  4  adv train finish, try to retain  34
test acc: top1 ->  26.3 ; top5 ->  68.0  and loss:  503.8282823562622
forward train acc: top1 ->  79.03999999267577 ; top5 ->  98.16000000244141  and loss:  62.4239741563797
test acc: top1 ->  77.23 ; top5 ->  97.8  and loss:  70.46357265114784
forward train acc: top1 ->  82.68599998779297 ; top5 ->  98.81199997802734  and loss:  50.658558785915375
test acc: top1 ->  79.43 ; top5 ->  98.02  and loss:  64.23563274741173
forward train acc: top1 ->  84.5319999975586 ; top5 ->  99.0700000024414  and loss:  44.98662322759628
test acc: top1 ->  80.98 ; top5 ->  98.24  and loss:  60.39828664064407
forward train acc: top1 ->  85.86599997558594 ; top5 ->  99.16800000488281  and loss:  41.730183094739914
test acc: top1 ->  81.57 ; top5 ->  98.34  and loss:  58.222705602645874
forward train acc: top1 ->  86.496 ; top5 ->  99.2340000024414  and loss:  39.443990617990494
test acc: top1 ->  82.19 ; top5 ->  98.41  and loss:  56.85987430810928
forward train acc: top1 ->  87.45600001464844 ; top5 ->  99.35599997558593  and loss:  36.65790104866028
test acc: top1 ->  82.32 ; top5 ->  98.59  and loss:  55.77059155702591
forward train acc: top1 ->  87.44400001220703 ; top5 ->  99.35199997802735  and loss:  36.597612619400024
test acc: top1 ->  82.66 ; top5 ->  98.44  and loss:  55.077347218990326
forward train acc: top1 ->  87.49999999023437 ; top5 ->  99.3920000024414  and loss:  36.197369992733
test acc: top1 ->  83.02 ; top5 ->  98.55  and loss:  53.988352328538895
forward train acc: top1 ->  87.94400001953125 ; top5 ->  99.374  and loss:  35.45721033215523
test acc: top1 ->  83.23 ; top5 ->  98.56  and loss:  54.01960551738739
forward train acc: top1 ->  88.024 ; top5 ->  99.40200000488281  and loss:  34.76595941185951
test acc: top1 ->  82.97 ; top5 ->  98.52  and loss:  55.02733901143074
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  156 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -174.7201483771205 , diff:  174.7201483771205
adv train loss:  -709.4529037475586 , diff:  534.7327553704381
adv train loss:  -859.2048120498657 , diff:  149.75190830230713
adv train loss:  -912.0460186004639 , diff:  52.841206550598145
adv train loss:  -929.4973268508911 , diff:  17.451308250427246
adv train loss:  -938.0016851425171 , diff:  8.504358291625977
adv train loss:  -956.9964094161987 , diff:  18.99472427368164
adv train loss:  -962.9072694778442 , diff:  5.910860061645508
adv train loss:  -967.2196683883667 , diff:  4.312398910522461
adv train loss:  -969.9544200897217 , diff:  2.7347517013549805
layer  5  adv train finish, try to retain  37
test acc: top1 ->  19.05 ; top5 ->  64.86  and loss:  729.3561296463013
forward train acc: top1 ->  87.23800000732422 ; top5 ->  99.53599997802735  and loss:  35.51686830818653
test acc: top1 ->  83.61 ; top5 ->  98.94  and loss:  52.63720881938934
forward train acc: top1 ->  90.6579999951172 ; top5 ->  99.73799997558594  and loss:  26.368114665150642
test acc: top1 ->  85.11 ; top5 ->  99.07  and loss:  49.14291861653328
forward train acc: top1 ->  91.80999997070313 ; top5 ->  99.78  and loss:  23.044452607631683
test acc: top1 ->  85.65 ; top5 ->  99.12  and loss:  47.7860012203455
forward train acc: top1 ->  92.54599998046875 ; top5 ->  99.808  and loss:  21.374198004603386
test acc: top1 ->  86.42 ; top5 ->  99.14  and loss:  46.389304369688034
forward train acc: top1 ->  93.16000000488282 ; top5 ->  99.826  and loss:  19.557236298918724
test acc: top1 ->  86.74 ; top5 ->  99.23  and loss:  45.32013227045536
forward train acc: top1 ->  93.45599999267579 ; top5 ->  99.824  and loss:  18.712656438350677
test acc: top1 ->  86.97 ; top5 ->  99.18  and loss:  44.98411253094673
forward train acc: top1 ->  93.75399997314453 ; top5 ->  99.888  and loss:  17.606555208563805
test acc: top1 ->  86.9 ; top5 ->  99.2  and loss:  45.610782504081726
forward train acc: top1 ->  93.9720000024414 ; top5 ->  99.854  and loss:  17.13770417869091
test acc: top1 ->  87.06 ; top5 ->  99.23  and loss:  45.05306328833103
forward train acc: top1 ->  93.87999997070312 ; top5 ->  99.8700000024414  and loss:  17.407996341586113
test acc: top1 ->  87.2 ; top5 ->  99.2  and loss:  45.044733479619026
forward train acc: top1 ->  94.05400000488281 ; top5 ->  99.88400000244141  and loss:  16.58013502508402
test acc: top1 ->  87.52 ; top5 ->  99.22  and loss:  44.204130202531815
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -254.39696404337883 , diff:  254.39696404337883
adv train loss:  -1137.5167741775513 , diff:  883.1198101341724
adv train loss:  -1309.9200973510742 , diff:  172.40332317352295
adv train loss:  -1362.5413341522217 , diff:  52.62123680114746
adv train loss:  -1370.4693326950073 , diff:  7.9279985427856445
adv train loss:  -1370.7082958221436 , diff:  0.23896312713623047
adv train loss:  -1372.0393953323364 , diff:  1.331099510192871
adv train loss:  -1369.8606805801392 , diff:  2.1787147521972656
adv train loss:  -1379.2652797698975 , diff:  9.4045991897583
adv train loss:  -1378.3073844909668 , diff:  0.9578952789306641
layer  6  adv train finish, try to retain  6
test acc: top1 ->  13.4 ; top5 ->  58.94  and loss:  548.2419157028198
forward train acc: top1 ->  67.31399999267578 ; top5 ->  98.01999998291015  and loss:  85.88993161916733
test acc: top1 ->  69.73 ; top5 ->  97.71  and loss:  88.86127585172653
forward train acc: top1 ->  76.08799998046875 ; top5 ->  99.02599997802734  and loss:  64.2022390961647
test acc: top1 ->  74.7 ; top5 ->  98.15  and loss:  78.59690520167351
forward train acc: top1 ->  79.82800000732422 ; top5 ->  99.3080000024414  and loss:  55.46309953927994
test acc: top1 ->  77.22 ; top5 ->  98.28  and loss:  73.46546944975853
forward train acc: top1 ->  82.26999997314454 ; top5 ->  99.43199997802735  and loss:  49.07981777191162
test acc: top1 ->  77.49 ; top5 ->  98.27  and loss:  72.5306365787983
forward train acc: top1 ->  84.04000000732422 ; top5 ->  99.538  and loss:  44.211873561143875
test acc: top1 ->  80.13 ; top5 ->  98.37  and loss:  65.04990422725677
forward train acc: top1 ->  85.18400001220704 ; top5 ->  99.51999997558593  and loss:  41.2331428527832
test acc: top1 ->  80.39 ; top5 ->  98.47  and loss:  64.78505498170853
forward train acc: top1 ->  85.92599998046875 ; top5 ->  99.56  and loss:  39.681303292512894
test acc: top1 ->  80.78 ; top5 ->  98.48  and loss:  64.18933323025703
forward train acc: top1 ->  86.19599998291015 ; top5 ->  99.574  and loss:  38.524437606334686
test acc: top1 ->  81.08 ; top5 ->  98.53  and loss:  61.40921926498413
forward train acc: top1 ->  86.88600000488282 ; top5 ->  99.61600000244141  and loss:  37.11387196183205
test acc: top1 ->  81.65 ; top5 ->  98.65  and loss:  60.554058492183685
forward train acc: top1 ->  87.1180000024414 ; top5 ->  99.61199997558593  and loss:  36.26852849125862
test acc: top1 ->  81.96 ; top5 ->  98.71  and loss:  59.485214203596115
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  98 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -7.8786393366754055 , diff:  7.8786393366754055
adv train loss:  -7.838240552693605 , diff:  0.04039878398180008
adv train loss:  -7.913868483155966 , diff:  0.07562793046236038
adv train loss:  -7.853487264364958 , diff:  0.060381218791007996
adv train loss:  -7.907868459820747 , diff:  0.054381195455789566
adv train loss:  -7.847633466124535 , diff:  0.06023499369621277
adv train loss:  -8.0083122625947 , diff:  0.16067879647016525
adv train loss:  -7.8774176351726055 , diff:  0.13089462742209435
adv train loss:  -8.006894867867231 , diff:  0.12947723269462585
adv train loss:  -7.959349829703569 , diff:  0.04754503816366196
layer  7  adv train finish, try to retain  402
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -20.917892202734947 , diff:  20.917892202734947
adv train loss:  -21.296478763222694 , diff:  0.3785865604877472
adv train loss:  -20.992450453341007 , diff:  0.30402830988168716
adv train loss:  -20.969995737075806 , diff:  0.02245471626520157
layer  8  adv train finish, try to retain  470
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -262.932386636734 , diff:  262.932386636734
adv train loss:  -350.8816795349121 , diff:  87.9492928981781
adv train loss:  -434.4613401889801 , diff:  83.579660654068
adv train loss:  -448.10652589797974 , diff:  13.645185708999634
adv train loss:  -486.45802211761475 , diff:  38.35149621963501
adv train loss:  -522.905387878418 , diff:  36.44736576080322
adv train loss:  -530.1447539329529 , diff:  7.239366054534912
adv train loss:  -539.0169906616211 , diff:  8.872236728668213
adv train loss:  -540.7933402061462 , diff:  1.7763495445251465
adv train loss:  -557.4834241867065 , diff:  16.690083980560303
layer  9  adv train finish, try to retain  1
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  3132.563501358032
forward train acc: top1 ->  41.012000002441404 ; top5 ->  86.67799997558593  and loss:  185.99748706817627
test acc: top1 ->  28.11 ; top5 ->  86.54  and loss:  239.91184759140015
forward train acc: top1 ->  79.26599997314453 ; top5 ->  99.60599997558593  and loss:  72.30972495675087
test acc: top1 ->  73.36 ; top5 ->  97.8  and loss:  93.6702618598938
forward train acc: top1 ->  92.29599997070312 ; top5 ->  99.896  and loss:  29.35787345468998
test acc: top1 ->  83.87 ; top5 ->  98.5  and loss:  64.72165617346764
forward train acc: top1 ->  95.90799999267578 ; top5 ->  99.954  and loss:  16.554084695875645
test acc: top1 ->  87.52 ; top5 ->  98.7  and loss:  55.71523867547512
forward train acc: top1 ->  97.36399999267579 ; top5 ->  99.974  and loss:  10.61113080009818
test acc: top1 ->  89.11 ; top5 ->  98.78  and loss:  50.957039192318916
forward train acc: top1 ->  98.12999997802734 ; top5 ->  99.986  and loss:  7.428631536662579
test acc: top1 ->  89.13 ; top5 ->  98.82  and loss:  52.065579786896706
forward train acc: top1 ->  98.35399998046876 ; top5 ->  99.992  and loss:  6.398659288883209
test acc: top1 ->  89.73 ; top5 ->  98.78  and loss:  50.64826075732708
forward train acc: top1 ->  98.61000000976563 ; top5 ->  99.982  and loss:  5.518342230468988
test acc: top1 ->  90.02 ; top5 ->  98.81  and loss:  50.675699293613434
forward train acc: top1 ->  98.72599997802735 ; top5 ->  99.992  and loss:  4.697845496237278
test acc: top1 ->  90.25 ; top5 ->  98.87  and loss:  51.579832158982754
forward train acc: top1 ->  98.98000000488281 ; top5 ->  99.994  and loss:  4.056256011128426
test acc: top1 ->  90.08 ; top5 ->  98.87  and loss:  52.7567555308342
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  96 / 512 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -180.5457682609558 , diff:  180.5457682609558
adv train loss:  -179.56781339645386 , diff:  0.9779548645019531
adv train loss:  -180.06392538547516 , diff:  0.49611198902130127
adv train loss:  -180.96366357803345 , diff:  0.8997381925582886
adv train loss:  -179.54278981685638 , diff:  1.420873761177063
adv train loss:  -180.1308935880661 , diff:  0.5881037712097168
adv train loss:  -179.78719854354858 , diff:  0.3436950445175171
layer  10  adv train finish, try to retain  478
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -877.5525951385498 , diff:  877.5525951385498
adv train loss:  -1184.421181678772 , diff:  306.86858654022217
adv train loss:  -1378.0938930511475 , diff:  193.6727113723755
adv train loss:  -1378.7923374176025 , diff:  0.6984443664550781
layer  11  adv train finish, try to retain  498
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -1607.2526140213013 , diff:  1607.2526140213013
adv train loss:  -1858.1977787017822 , diff:  250.94516468048096
adv train loss:  -1857.739740371704 , diff:  0.458038330078125
layer  12  adv train finish, try to retain  1
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  7911.403305053711
forward train acc: top1 ->  13.526000001220703 ; top5 ->  59.71199998901367  and loss:  495.21048498153687
test acc: top1 ->  22.61 ; top5 ->  60.51  and loss:  260.4490873813629
forward train acc: top1 ->  37.29599999267578 ; top5 ->  91.61200000732421  and loss:  160.90062987804413
test acc: top1 ->  41.44 ; top5 ->  97.52  and loss:  148.2672609090805
forward train acc: top1 ->  45.73 ; top5 ->  99.694  and loss:  131.53137922286987
test acc: top1 ->  43.7 ; top5 ->  97.52  and loss:  141.42115247249603
forward train acc: top1 ->  49.62600000244141 ; top5 ->  99.77999997558594  and loss:  124.0959677696228
test acc: top1 ->  48.83 ; top5 ->  97.67  and loss:  137.55539453029633
forward train acc: top1 ->  52.465999985351566 ; top5 ->  99.8460000024414  and loss:  118.34160447120667
test acc: top1 ->  48.6 ; top5 ->  97.73  and loss:  134.07070207595825
forward train acc: top1 ->  55.231999985351564 ; top5 ->  99.854  and loss:  114.58785843849182
test acc: top1 ->  47.21 ; top5 ->  97.69  and loss:  132.9901386499405
forward train acc: top1 ->  55.8239999987793 ; top5 ->  99.872  and loss:  112.07325553894043
test acc: top1 ->  51.47 ; top5 ->  97.68  and loss:  132.3459599018097
forward train acc: top1 ->  56.66799998657226 ; top5 ->  99.882  and loss:  109.98907625675201
test acc: top1 ->  51.51 ; top5 ->  97.84  and loss:  131.36559903621674
forward train acc: top1 ->  56.894 ; top5 ->  99.93399997558593  and loss:  107.42569434642792
test acc: top1 ->  55.51 ; top5 ->  97.78  and loss:  130.09459555149078
forward train acc: top1 ->  57.51800000610351 ; top5 ->  99.914  and loss:  105.56221437454224
test acc: top1 ->  56.16 ; top5 ->  97.8  and loss:  129.28355979919434
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  41 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -18134.24119567871 , diff:  18134.24119567871
adv train loss:  -28372.692932128906 , diff:  10238.451736450195
adv train loss:  -38750.187408447266 , diff:  10377.49447631836
adv train loss:  -49183.055267333984 , diff:  10432.867858886719
adv train loss:  -59645.72088623047 , diff:  10462.665618896484
adv train loss:  -70192.50665283203 , diff:  10546.785766601562
adv train loss:  -82560.56610107422 , diff:  12368.059448242188
adv train loss:  -102004.5160522461 , diff:  19443.949951171875
adv train loss:  -122251.40454101562 , diff:  20246.88848876953
adv train loss:  -139906.37524414062 , diff:  17654.970703125
layer  13  adv train finish, try to retain  10
test acc: top1 ->  19.7 ; top5 ->  50.81  and loss:  3183.6614818573
forward train acc: top1 ->  40.56200000610352 ; top5 ->  72.55400000732422  and loss:  1234.5790437459946
test acc: top1 ->  52.8 ; top5 ->  87.51  and loss:  189.8371309041977
forward train acc: top1 ->  92.01199998291015 ; top5 ->  99.18  and loss:  31.001067526638508
test acc: top1 ->  90.01 ; top5 ->  98.09  and loss:  53.147266522049904
forward train acc: top1 ->  99.20999997558594 ; top5 ->  99.974  and loss:  7.692430976778269
test acc: top1 ->  90.63 ; top5 ->  98.07  and loss:  49.646599881350994
forward train acc: top1 ->  99.44199997558594 ; top5 ->  99.982  and loss:  4.567263383418322
test acc: top1 ->  90.89 ; top5 ->  98.1  and loss:  48.624748691916466
forward train acc: top1 ->  99.58 ; top5 ->  99.984  and loss:  3.2713194899260998
test acc: top1 ->  91.12 ; top5 ->  98.19  and loss:  47.97984917461872
forward train acc: top1 ->  99.66000000244141 ; top5 ->  99.974  and loss:  2.627308949828148
test acc: top1 ->  91.2 ; top5 ->  98.25  and loss:  48.73452787846327
forward train acc: top1 ->  99.664 ; top5 ->  99.992  and loss:  2.2788659315556288
test acc: top1 ->  91.3 ; top5 ->  98.19  and loss:  49.015309669077396
forward train acc: top1 ->  99.6440000024414 ; top5 ->  99.988  and loss:  2.135154142975807
test acc: top1 ->  91.41 ; top5 ->  98.24  and loss:  49.86561544984579
forward train acc: top1 ->  99.7340000024414 ; top5 ->  99.992  and loss:  1.8093803264200687
test acc: top1 ->  91.41 ; top5 ->  98.29  and loss:  48.918090514838696
forward train acc: top1 ->  99.72999997558594 ; top5 ->  99.994  and loss:  1.6143128089606762
test acc: top1 ->  91.38 ; top5 ->  98.27  and loss:  50.00323656946421
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  17 / 512 , inc:  1
layer  0  :  0.46875  ==>  30 / 64 , inc:  1
layer  1  :  0.828125  ==>  53 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.3828125  ==>  98 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.041015625  ==>  21 / 512 , inc:  1
layer  9  :  0.1875  ==>  96 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.033203125  ==>  17 / 512 , inc:  1
eps [1.2469712737798688, 3.3252567300796505, 1.2469712737798688, 1.2469712737798688, 1.2469712737798688, 1.2469712737798688, 1.6626283650398253, 1.6626283650398253, 1.4778918800354004, 1.2469712737798688, 1.6626283650398253, 2.2168378200531005, 1.2469712737798688, 9.97577019023895]  wait [4, 2, 4, 4, 4, 4, 2, 1, 0, 4, 1, 1, 4, 4]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 8
$$$$$$$$$$$$$ epoch  61  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
adv train loss:  -1281.0150241851807 , diff:  1281.0150241851807
adv train loss:  -1280.9385147094727 , diff:  0.07650947570800781
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  52
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  3967.097888946533
forward train acc: top1 ->  91.76199997558594 ; top5 ->  98.682  and loss:  106.15248769475147
test acc: top1 ->  90.93 ; top5 ->  98.58  and loss:  80.3280207067728
forward train acc: top1 ->  99.62200000732422 ; top5 ->  99.998  and loss:  1.319746260298416
test acc: top1 ->  91.39 ; top5 ->  98.68  and loss:  76.1402325630188
forward train acc: top1 ->  99.72000000244141 ; top5 ->  100.0  and loss:  0.895768333110027
test acc: top1 ->  91.41 ; top5 ->  98.7  and loss:  75.81022509932518
forward train acc: top1 ->  99.79400000244141 ; top5 ->  100.0  and loss:  0.729669704567641
test acc: top1 ->  91.48 ; top5 ->  98.66  and loss:  76.54175771772861
forward train acc: top1 ->  99.85 ; top5 ->  99.998  and loss:  0.5581294202711433
test acc: top1 ->  91.59 ; top5 ->  98.73  and loss:  75.78399742394686
forward train acc: top1 ->  99.81 ; top5 ->  100.0  and loss:  0.6355503807135392
test acc: top1 ->  91.48 ; top5 ->  98.64  and loss:  76.9868977740407
forward train acc: top1 ->  99.8400000024414 ; top5 ->  100.0  and loss:  0.592540473502595
test acc: top1 ->  91.6 ; top5 ->  98.73  and loss:  76.32221373170614
forward train acc: top1 ->  99.88199997558594 ; top5 ->  100.0  and loss:  0.35864372240030207
test acc: top1 ->  91.7 ; top5 ->  98.78  and loss:  77.06192893534899
forward train acc: top1 ->  99.84799997558594 ; top5 ->  100.0  and loss:  0.5088053937943187
test acc: top1 ->  91.76 ; top5 ->  98.82  and loss:  76.69394617527723
forward train acc: top1 ->  99.86 ; top5 ->  100.0  and loss:  0.4999578911229037
test acc: top1 ->  91.83 ; top5 ->  98.83  and loss:  76.62695240229368
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  53 / 64 , inc:  1
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
adv train loss:  -990.7659857183171 , diff:  990.7659857183171
adv train loss:  -2988.594799041748 , diff:  1997.828813323431
adv train loss:  -3074.208692550659 , diff:  85.61389350891113
adv train loss:  -3117.061237335205 , diff:  42.8525447845459
adv train loss:  -3114.3720989227295 , diff:  2.689138412475586
adv train loss:  -3107.9888248443604 , diff:  6.383274078369141
adv train loss:  -3147.3287410736084 , diff:  39.33991622924805
adv train loss:  -3187.3545627593994 , diff:  40.025821685791016
adv train loss:  -3175.453001022339 , diff:  11.901561737060547
adv train loss:  -3184.10658454895 , diff:  8.653583526611328
layer  6  adv train finish, try to retain  3
test acc: top1 ->  10.03 ; top5 ->  56.88  and loss:  927.9711565971375
forward train acc: top1 ->  50.40199998779297 ; top5 ->  95.33000001708984  and loss:  213.46708464622498
test acc: top1 ->  51.14 ; top5 ->  95.99  and loss:  159.9490077495575
forward train acc: top1 ->  57.73999999145508 ; top5 ->  97.58000000732422  and loss:  114.90203887224197
test acc: top1 ->  57.63 ; top5 ->  96.7  and loss:  124.95769256353378
forward train acc: top1 ->  61.01399998413086 ; top5 ->  98.10599998046875  and loss:  100.83590739965439
test acc: top1 ->  61.05 ; top5 ->  97.11  and loss:  113.21344244480133
forward train acc: top1 ->  64.60199997802735 ; top5 ->  98.45600000732422  and loss:  92.07488238811493
test acc: top1 ->  63.27 ; top5 ->  97.44  and loss:  107.2077819108963
forward train acc: top1 ->  68.04000001220703 ; top5 ->  98.74599997802734  and loss:  83.57164627313614
test acc: top1 ->  66.98 ; top5 ->  97.82  and loss:  98.55793541669846
forward train acc: top1 ->  70.47400000732422 ; top5 ->  98.89399998046875  and loss:  78.12724274396896
test acc: top1 ->  67.94 ; top5 ->  98.05  and loss:  95.61949294805527
forward train acc: top1 ->  72.11199998779297 ; top5 ->  98.93999997558593  and loss:  74.80323672294617
test acc: top1 ->  69.47 ; top5 ->  98.01  and loss:  92.07883548736572
forward train acc: top1 ->  73.20999999511719 ; top5 ->  99.03000000732422  and loss:  72.19532060623169
test acc: top1 ->  70.27 ; top5 ->  98.12  and loss:  89.56458938121796
forward train acc: top1 ->  74.32199998291016 ; top5 ->  99.11799997802734  and loss:  69.5018555521965
test acc: top1 ->  71.08 ; top5 ->  98.15  and loss:  88.40575134754181
forward train acc: top1 ->  75.35799997070312 ; top5 ->  99.09800000244141  and loss:  67.24469143152237
test acc: top1 ->  72.17 ; top5 ->  98.26  and loss:  85.87933319807053
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  98 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -14.56718710809946 , diff:  14.56718710809946
adv train loss:  -14.410098262131214 , diff:  0.15708884596824646
adv train loss:  -14.514763236045837 , diff:  0.10466497391462326
adv train loss:  -14.500789798796177 , diff:  0.013973437249660492
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  103
test acc: top1 ->  17.83 ; top5 ->  65.46  and loss:  7922202.09765625
forward train acc: top1 ->  96.65799997558594 ; top5 ->  99.954  and loss:  10.516993405297399
test acc: top1 ->  90.7 ; top5 ->  99.2  and loss:  42.44606989622116
forward train acc: top1 ->  99.2580000024414 ; top5 ->  99.998  and loss:  2.4748141383752227
test acc: top1 ->  90.98 ; top5 ->  99.29  and loss:  45.78363960236311
forward train acc: top1 ->  99.44799997558594 ; top5 ->  99.998  and loss:  1.7134077623486519
test acc: top1 ->  91.23 ; top5 ->  99.15  and loss:  48.77760395407677
forward train acc: top1 ->  99.53600000488281 ; top5 ->  100.0  and loss:  1.4486336912959814
test acc: top1 ->  91.3 ; top5 ->  99.07  and loss:  51.77318228781223
forward train acc: top1 ->  99.58 ; top5 ->  100.0  and loss:  1.3157845339737833
test acc: top1 ->  91.36 ; top5 ->  99.23  and loss:  51.831046275794506
forward train acc: top1 ->  99.64199997558593 ; top5 ->  99.998  and loss:  1.0843931315466762
test acc: top1 ->  91.36 ; top5 ->  99.3  and loss:  52.19149765372276
forward train acc: top1 ->  99.68399997802734 ; top5 ->  100.0  and loss:  0.9394916733726859
test acc: top1 ->  91.29 ; top5 ->  99.18  and loss:  53.412002846598625
forward train acc: top1 ->  99.704 ; top5 ->  100.0  and loss:  0.9068683085497469
test acc: top1 ->  91.45 ; top5 ->  99.3  and loss:  54.585310243070126
forward train acc: top1 ->  99.68799997558594 ; top5 ->  100.0  and loss:  0.9283740604296327
test acc: top1 ->  91.42 ; top5 ->  99.37  and loss:  55.713131196796894
forward train acc: top1 ->  99.726 ; top5 ->  99.998  and loss:  0.8457514089532197
test acc: top1 ->  91.35 ; top5 ->  99.23  and loss:  55.92702906578779
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  104 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -1380.1515398025513 , diff:  1380.1515398025513
adv train loss:  -1733.1914901733398 , diff:  353.0399503707886
adv train loss:  -1801.7793521881104 , diff:  68.58786201477051
adv train loss:  -2001.3748416900635 , diff:  199.59548950195312
adv train loss:  -2033.8097343444824 , diff:  32.434892654418945
adv train loss:  -2036.271677017212 , diff:  2.461942672729492
adv train loss:  -2141.45556640625 , diff:  105.18388938903809
adv train loss:  -2198.2692127227783 , diff:  56.81364631652832
adv train loss:  -2268.5415649414062 , diff:  70.27235221862793
adv train loss:  -2265.5746746063232 , diff:  2.966890335083008
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  20
test acc: top1 ->  12.07 ; top5 ->  55.44  and loss:  47517.63247680664
forward train acc: top1 ->  98.92399997558594 ; top5 ->  99.996  and loss:  3.26196815026924
test acc: top1 ->  91.58 ; top5 ->  99.25  and loss:  54.148388646543026
forward train acc: top1 ->  99.736 ; top5 ->  99.998  and loss:  0.8223261365201324
test acc: top1 ->  91.77 ; top5 ->  99.28  and loss:  54.94960556179285
forward train acc: top1 ->  99.79400000244141 ; top5 ->  100.0  and loss:  0.6676556691527367
test acc: top1 ->  91.91 ; top5 ->  99.33  and loss:  55.449221067130566
forward train acc: top1 ->  99.84199997558594 ; top5 ->  100.0  and loss:  0.5235138144344091
test acc: top1 ->  91.86 ; top5 ->  99.29  and loss:  57.445329427719116
forward train acc: top1 ->  99.84199997558594 ; top5 ->  100.0  and loss:  0.48602660465985537
test acc: top1 ->  91.96 ; top5 ->  99.31  and loss:  58.200697995722294
forward train acc: top1 ->  99.842 ; top5 ->  100.0  and loss:  0.4809585619950667
test acc: top1 ->  92.03 ; top5 ->  99.27  and loss:  57.892298854887486
forward train acc: top1 ->  99.87799997558594 ; top5 ->  100.0  and loss:  0.41407723585143685
test acc: top1 ->  91.95 ; top5 ->  99.28  and loss:  58.98101764917374
forward train acc: top1 ->  99.914 ; top5 ->  100.0  and loss:  0.31396683515049517
test acc: top1 ->  92.01 ; top5 ->  99.34  and loss:  58.834203630685806
forward train acc: top1 ->  99.878 ; top5 ->  100.0  and loss:  0.4022143315523863
test acc: top1 ->  92.07 ; top5 ->  99.32  and loss:  59.27077806740999
forward train acc: top1 ->  99.8740000024414 ; top5 ->  100.0  and loss:  0.3618041779845953
test acc: top1 ->  92.16 ; top5 ->  99.33  and loss:  60.27505300939083
==> this epoch:  20 / 512
---------------- start layer  9  ---------------
### skip layer  9 wait:  4  ###
---------------- start layer  10  ---------------
adv train loss:  -65.80355143547058 , diff:  65.80355143547058
adv train loss:  -69.16431936621666 , diff:  3.3607679307460785
adv train loss:  -69.65880379080772 , diff:  0.49448442459106445
adv train loss:  -69.80925825238228 , diff:  0.15045446157455444
adv train loss:  -69.76648509502411 , diff:  0.042773157358169556
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  21
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  1051160.8999023438
forward train acc: top1 ->  71.20800001220704 ; top5 ->  94.654  and loss:  176.33046911656857
test acc: top1 ->  65.97 ; top5 ->  97.82  and loss:  101.60572737455368
forward train acc: top1 ->  98.00999997802734 ; top5 ->  99.984  and loss:  12.214069992303848
test acc: top1 ->  89.65 ; top5 ->  98.4  and loss:  44.12823607027531
forward train acc: top1 ->  99.10200000488281 ; top5 ->  99.984  and loss:  4.6825548857450485
test acc: top1 ->  90.48 ; top5 ->  98.48  and loss:  45.504856407642365
forward train acc: top1 ->  99.40599997558594 ; top5 ->  99.992  and loss:  2.7723130136728287
test acc: top1 ->  90.9 ; top5 ->  98.52  and loss:  46.76294280588627
forward train acc: top1 ->  99.602 ; top5 ->  99.998  and loss:  1.9311657240614295
test acc: top1 ->  90.97 ; top5 ->  98.7  and loss:  48.511476904153824
forward train acc: top1 ->  99.66999997558594 ; top5 ->  100.0  and loss:  1.574422650039196
test acc: top1 ->  91.15 ; top5 ->  98.73  and loss:  48.52543079853058
forward train acc: top1 ->  99.68800000488281 ; top5 ->  99.996  and loss:  1.3825979381799698
test acc: top1 ->  91.19 ; top5 ->  98.64  and loss:  49.658931612968445
forward train acc: top1 ->  99.75799997558593 ; top5 ->  100.0  and loss:  1.1461005061864853
test acc: top1 ->  91.39 ; top5 ->  98.6  and loss:  50.148135647177696
forward train acc: top1 ->  99.72599997558594 ; top5 ->  100.0  and loss:  1.1112965531647205
test acc: top1 ->  91.36 ; top5 ->  98.65  and loss:  51.160721972584724
forward train acc: top1 ->  99.788 ; top5 ->  100.0  and loss:  0.9796341555193067
test acc: top1 ->  91.45 ; top5 ->  98.71  and loss:  51.633763305842876
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  22 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -2235.026206970215 , diff:  2235.026206970215
adv train loss:  -2330.0919761657715 , diff:  95.06576919555664
adv train loss:  -2332.0946369171143 , diff:  2.0026607513427734
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  1
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  55286.89779663086
forward train acc: top1 ->  89.94800000976562 ; top5 ->  99.628  and loss:  35.92414008080959
test acc: top1 ->  79.44 ; top5 ->  97.02  and loss:  114.67062413692474
forward train acc: top1 ->  97.7619999951172 ; top5 ->  99.984  and loss:  9.478290896862745
test acc: top1 ->  88.16 ; top5 ->  97.48  and loss:  73.19282722473145
forward train acc: top1 ->  98.5960000024414 ; top5 ->  99.988  and loss:  5.89304031804204
test acc: top1 ->  88.8 ; top5 ->  97.43  and loss:  74.27530366182327
forward train acc: top1 ->  98.94400000244141 ; top5 ->  99.992  and loss:  4.3952012825757265
test acc: top1 ->  89.26 ; top5 ->  97.45  and loss:  74.86045557260513
forward train acc: top1 ->  99.18199997802735 ; top5 ->  99.994  and loss:  3.42684418335557
test acc: top1 ->  89.64 ; top5 ->  97.39  and loss:  75.47436937689781
forward train acc: top1 ->  99.35999997558594 ; top5 ->  99.992  and loss:  2.773550132289529
test acc: top1 ->  89.79 ; top5 ->  97.51  and loss:  75.89548893272877
forward train acc: top1 ->  99.40799997558594 ; top5 ->  99.998  and loss:  2.4087022729218006
test acc: top1 ->  89.87 ; top5 ->  97.58  and loss:  75.73913565278053
forward train acc: top1 ->  99.43200000488281 ; top5 ->  99.992  and loss:  2.224347051233053
test acc: top1 ->  90.03 ; top5 ->  97.68  and loss:  76.31622937321663
forward train acc: top1 ->  99.56000000488281 ; top5 ->  99.996  and loss:  1.8901922591030598
test acc: top1 ->  90.29 ; top5 ->  97.69  and loss:  76.86014397442341
forward train acc: top1 ->  99.56800000244141 ; top5 ->  99.996  and loss:  1.766650676727295
test acc: top1 ->  90.3 ; top5 ->  97.67  and loss:  76.37587162852287
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  2 / 512 , inc:  1
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
### skip layer  13 wait:  4  ###
layer  0  :  0.46875  ==>  30 / 64 , inc:  1
layer  1  :  0.828125  ==>  53 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.3828125  ==>  98 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.0390625  ==>  20 / 512 , inc:  2
layer  9  :  0.1875  ==>  96 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.033203125  ==>  17 / 512 , inc:  1
eps [1.2469712737798688, 2.4939425475597377, 1.2469712737798688, 1.2469712737798688, 1.2469712737798688, 1.2469712737798688, 1.2469712737798688, 1.2469712737798688, 1.4778918800354004, 1.2469712737798688, 1.2469712737798688, 1.6626283650398253, 1.2469712737798688, 9.97577019023895]  wait [3, 4, 3, 3, 3, 3, 4, 3, 0, 3, 3, 3, 3, 3]  inc [1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1]  tol: 8
$$$$$$$$$$$$$ epoch  62  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
### skip layer  1 wait:  4  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
adv train loss:  -1692.7929935455322 , diff:  1692.7929935455322
adv train loss:  -2041.8045501708984 , diff:  349.0115566253662
adv train loss:  -2253.126605987549 , diff:  211.3220558166504
adv train loss:  -2628.761775970459 , diff:  375.63516998291016
adv train loss:  -2747.7525882720947 , diff:  118.99081230163574
adv train loss:  -2783.927682876587 , diff:  36.17509460449219
adv train loss:  -2931.8668575286865 , diff:  147.9391746520996
adv train loss:  -2935.7805194854736 , diff:  3.9136619567871094
adv train loss:  -2933.353666305542 , diff:  2.4268531799316406
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  18
test acc: top1 ->  12.66 ; top5 ->  65.18  and loss:  8587.238422393799
forward train acc: top1 ->  99.47600000244141 ; top5 ->  99.998  and loss:  1.839798127533868
test acc: top1 ->  91.99 ; top5 ->  98.96  and loss:  70.58462085574865
forward train acc: top1 ->  99.892 ; top5 ->  100.0  and loss:  0.34360967646352947
test acc: top1 ->  92.11 ; top5 ->  99.06  and loss:  71.47230687737465
==> this epoch:  18 / 512
---------------- start layer  9  ---------------
### skip layer  9 wait:  3  ###
---------------- start layer  10  ---------------
### skip layer  10 wait:  3  ###
---------------- start layer  11  ---------------
### skip layer  11 wait:  3  ###
---------------- start layer  12  ---------------
### skip layer  12 wait:  3  ###
---------------- start layer  13  ---------------
### skip layer  13 wait:  3  ###
layer  0  :  0.46875  ==>  30 / 64 , inc:  1
layer  1  :  0.828125  ==>  53 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.3828125  ==>  98 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.03515625  ==>  18 / 512 , inc:  4
layer  9  :  0.1875  ==>  96 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.033203125  ==>  17 / 512 , inc:  1
eps [1.2469712737798688, 2.4939425475597377, 1.2469712737798688, 1.2469712737798688, 1.2469712737798688, 1.2469712737798688, 1.2469712737798688, 1.2469712737798688, 1.4778918800354004, 1.2469712737798688, 1.2469712737798688, 1.6626283650398253, 1.2469712737798688, 9.97577019023895]  wait [2, 3, 2, 2, 2, 2, 3, 2, 0, 2, 2, 2, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1]  tol: 8
$$$$$$$$$$$$$ epoch  63  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1758.6876010894775 , diff:  1758.6876010894775
adv train loss:  -1787.6011562347412 , diff:  28.913555145263672
adv train loss:  -1818.301721572876 , diff:  30.700565338134766
adv train loss:  -1815.8292350769043 , diff:  2.4724864959716797
adv train loss:  -1692.4426536560059 , diff:  123.38658142089844
adv train loss:  -1682.0456562042236 , diff:  10.396997451782227
adv train loss:  -1683.0467376708984 , diff:  1.0010814666748047
adv train loss:  -1690.1691303253174 , diff:  7.122392654418945
adv train loss:  -1680.258768081665 , diff:  9.910362243652344
adv train loss:  -1686.2917079925537 , diff:  6.032939910888672
layer  0  adv train finish, try to retain  14
test acc: top1 ->  10.0 ; top5 ->  50.02  and loss:  12489.380760192871
forward train acc: top1 ->  87.37000001464844 ; top5 ->  98.66400000488281  and loss:  62.07183599472046
test acc: top1 ->  51.63 ; top5 ->  87.1  and loss:  348.9548330307007
forward train acc: top1 ->  90.37199997314453 ; top5 ->  99.31000000244141  and loss:  31.432839825749397
test acc: top1 ->  85.17 ; top5 ->  98.6  and loss:  55.91383109986782
forward train acc: top1 ->  91.9479999975586 ; top5 ->  99.58599997558593  and loss:  25.24714607000351
test acc: top1 ->  86.37 ; top5 ->  98.85  and loss:  52.218264147639275
forward train acc: top1 ->  92.91199997802734 ; top5 ->  99.676  and loss:  21.46551252901554
test acc: top1 ->  87.13 ; top5 ->  98.9  and loss:  49.14858813583851
forward train acc: top1 ->  94.02399999267578 ; top5 ->  99.72  and loss:  18.677050083875656
test acc: top1 ->  87.37 ; top5 ->  98.97  and loss:  47.504491820931435
forward train acc: top1 ->  94.20200000244141 ; top5 ->  99.7640000024414  and loss:  17.359748482704163
test acc: top1 ->  87.6 ; top5 ->  98.97  and loss:  46.98320038616657
forward train acc: top1 ->  94.73200001953126 ; top5 ->  99.79999997558593  and loss:  16.036097064614296
test acc: top1 ->  87.87 ; top5 ->  99.02  and loss:  46.23903979361057
forward train acc: top1 ->  95.01399997314454 ; top5 ->  99.84399997558593  and loss:  15.01189212501049
test acc: top1 ->  87.91 ; top5 ->  99.11  and loss:  45.95455002784729
forward train acc: top1 ->  95.27399997070313 ; top5 ->  99.88  and loss:  14.28248891979456
test acc: top1 ->  88.14 ; top5 ->  99.02  and loss:  46.06993642449379
forward train acc: top1 ->  95.51999999511719 ; top5 ->  99.866  and loss:  13.335904374718666
test acc: top1 ->  88.23 ; top5 ->  99.08  and loss:  45.40571178495884
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  30 / 64 , inc:  1
---------------- start layer  1  ---------------
### skip layer  1 wait:  3  ###
---------------- start layer  2  ---------------
adv train loss:  -419.7881316654384 , diff:  419.7881316654384
adv train loss:  -761.5147504806519 , diff:  341.72661881521344
adv train loss:  -777.6597990989685 , diff:  16.14504861831665
adv train loss:  -776.589816570282 , diff:  1.0699825286865234
adv train loss:  -779.017279624939 , diff:  2.4274630546569824
adv train loss:  -779.198079586029 , diff:  0.1807999610900879
adv train loss:  -776.1998567581177 , diff:  2.998222827911377
adv train loss:  -779.1776309013367 , diff:  2.977774143218994
adv train loss:  -777.7620334625244 , diff:  1.4155974388122559
adv train loss:  -778.7628808021545 , diff:  1.000847339630127
layer  2  adv train finish, try to retain  88
test acc: top1 ->  9.3 ; top5 ->  53.63  and loss:  4540.0040855407715
forward train acc: top1 ->  99.31200000244141 ; top5 ->  99.998  and loss:  2.2475896552205086
test acc: top1 ->  91.38 ; top5 ->  99.33  and loss:  44.16515935957432
forward train acc: top1 ->  99.53 ; top5 ->  100.0  and loss:  1.3460987363941967
test acc: top1 ->  91.3 ; top5 ->  99.33  and loss:  50.34895168244839
forward train acc: top1 ->  99.672 ; top5 ->  100.0  and loss:  1.0022274642251432
test acc: top1 ->  91.57 ; top5 ->  99.32  and loss:  50.464754804968834
forward train acc: top1 ->  99.70199997558593 ; top5 ->  100.0  and loss:  0.8673585951328278
test acc: top1 ->  91.61 ; top5 ->  99.36  and loss:  50.70655911415815
forward train acc: top1 ->  99.772 ; top5 ->  100.0  and loss:  0.7600768686970696
test acc: top1 ->  91.5 ; top5 ->  99.35  and loss:  54.875536769628525
forward train acc: top1 ->  99.73199997558594 ; top5 ->  100.0  and loss:  0.7949168127961457
test acc: top1 ->  91.67 ; top5 ->  99.35  and loss:  53.82449932396412
forward train acc: top1 ->  99.77 ; top5 ->  100.0  and loss:  0.6959368512034416
test acc: top1 ->  91.96 ; top5 ->  99.37  and loss:  53.767732411623
forward train acc: top1 ->  99.826 ; top5 ->  100.0  and loss:  0.5650158948265016
test acc: top1 ->  91.77 ; top5 ->  99.36  and loss:  54.49911005049944
forward train acc: top1 ->  99.818 ; top5 ->  100.0  and loss:  0.5358163537457585
test acc: top1 ->  91.87 ; top5 ->  99.37  and loss:  54.644556157290936
forward train acc: top1 ->  99.812 ; top5 ->  100.0  and loss:  0.5514004051219672
test acc: top1 ->  91.71 ; top5 ->  99.31  and loss:  55.18920695781708
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -738.5871915910393 , diff:  738.5871915910393
adv train loss:  -1472.6406955718994 , diff:  734.0535039808601
adv train loss:  -1569.3691301345825 , diff:  96.7284345626831
adv train loss:  -1609.294174194336 , diff:  39.92504405975342
adv train loss:  -1606.0680513381958 , diff:  3.2261228561401367
adv train loss:  -1609.0361366271973 , diff:  2.968085289001465
adv train loss:  -1601.7077903747559 , diff:  7.328346252441406
adv train loss:  -1610.1504278182983 , diff:  8.44263744354248
adv train loss:  -1607.6044778823853 , diff:  2.545949935913086
adv train loss:  -1602.6516885757446 , diff:  4.952789306640625
layer  3  adv train finish, try to retain  70
test acc: top1 ->  14.48 ; top5 ->  50.7  and loss:  8223.477882385254
forward train acc: top1 ->  98.806 ; top5 ->  99.988  and loss:  3.7364665865898132
test acc: top1 ->  90.88 ; top5 ->  99.36  and loss:  49.637662544846535
forward train acc: top1 ->  99.06399997558594 ; top5 ->  99.996  and loss:  2.809425253421068
test acc: top1 ->  91.14 ; top5 ->  99.44  and loss:  49.31737335771322
forward train acc: top1 ->  99.12799997802735 ; top5 ->  100.0  and loss:  2.425475012511015
test acc: top1 ->  91.45 ; top5 ->  99.33  and loss:  47.7789064347744
forward train acc: top1 ->  99.33600000244141 ; top5 ->  99.99  and loss:  1.9146152548491955
test acc: top1 ->  91.28 ; top5 ->  99.39  and loss:  50.710884638130665
forward train acc: top1 ->  99.3560000024414 ; top5 ->  99.996  and loss:  1.9047752395272255
test acc: top1 ->  91.22 ; top5 ->  99.33  and loss:  52.512137457728386
forward train acc: top1 ->  99.41599997558593 ; top5 ->  99.998  and loss:  1.6694163037464023
test acc: top1 ->  91.41 ; top5 ->  99.37  and loss:  51.07682193815708
forward train acc: top1 ->  99.48200000244141 ; top5 ->  100.0  and loss:  1.5986262913793325
test acc: top1 ->  91.5 ; top5 ->  99.36  and loss:  50.43282976746559
forward train acc: top1 ->  99.426 ; top5 ->  100.0  and loss:  1.703464388847351
test acc: top1 ->  91.5 ; top5 ->  99.35  and loss:  50.686171501874924
forward train acc: top1 ->  99.50199997558593 ; top5 ->  100.0  and loss:  1.4413842847570777
test acc: top1 ->  91.56 ; top5 ->  99.38  and loss:  50.978354163467884
forward train acc: top1 ->  99.50999997802734 ; top5 ->  99.996  and loss:  1.4999253060668707
test acc: top1 ->  91.59 ; top5 ->  99.42  and loss:  52.26461781561375
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -335.0195113681257 , diff:  335.0195113681257
adv train loss:  -1333.4042673110962 , diff:  998.3847559429705
adv train loss:  -1562.1675329208374 , diff:  228.7632656097412
adv train loss:  -1641.9212894439697 , diff:  79.75375652313232
adv train loss:  -1690.728777885437 , diff:  48.807488441467285
adv train loss:  -1693.179256439209 , diff:  2.4504785537719727
adv train loss:  -1703.6165103912354 , diff:  10.437253952026367
adv train loss:  -1714.1945991516113 , diff:  10.578088760375977
adv train loss:  -1711.9319972991943 , diff:  2.262601852416992
adv train loss:  -1726.1077308654785 , diff:  14.17573356628418
layer  4  adv train finish, try to retain  114
test acc: top1 ->  25.18 ; top5 ->  72.0  and loss:  5461.285890579224
forward train acc: top1 ->  97.88799998779297 ; top5 ->  99.96599997558594  and loss:  6.662193316966295
test acc: top1 ->  90.23 ; top5 ->  99.28  and loss:  47.15900593996048
forward train acc: top1 ->  98.43200000244141 ; top5 ->  99.988  and loss:  4.56150976754725
test acc: top1 ->  90.5 ; top5 ->  99.28  and loss:  45.47709260880947
forward train acc: top1 ->  98.55600001220704 ; top5 ->  99.99  and loss:  4.085570365190506
test acc: top1 ->  90.5 ; top5 ->  99.18  and loss:  46.28791256248951
forward train acc: top1 ->  98.76399998535156 ; top5 ->  99.986  and loss:  3.5753675159066916
test acc: top1 ->  90.79 ; top5 ->  99.23  and loss:  48.98161821067333
forward train acc: top1 ->  98.91199997802734 ; top5 ->  99.994  and loss:  3.278256783261895
test acc: top1 ->  90.76 ; top5 ->  99.26  and loss:  46.67575931549072
forward train acc: top1 ->  99.00799997802734 ; top5 ->  99.992  and loss:  3.021446907892823
test acc: top1 ->  90.78 ; top5 ->  99.26  and loss:  47.56876750290394
forward train acc: top1 ->  99.02999997802735 ; top5 ->  99.996  and loss:  2.7290847040712833
test acc: top1 ->  90.89 ; top5 ->  99.34  and loss:  47.14172485470772
forward train acc: top1 ->  99.03999998291016 ; top5 ->  99.988  and loss:  2.737309116870165
test acc: top1 ->  91.02 ; top5 ->  99.36  and loss:  47.60825543105602
forward train acc: top1 ->  99.03800000732421 ; top5 ->  100.0  and loss:  2.721864987164736
test acc: top1 ->  90.75 ; top5 ->  99.32  and loss:  47.045899987220764
forward train acc: top1 ->  99.19399998535157 ; top5 ->  99.998  and loss:  2.3466880433261395
test acc: top1 ->  90.99 ; top5 ->  99.33  and loss:  49.37582504749298
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  156 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -480.88424635306 , diff:  480.88424635306
adv train loss:  -1668.3024168014526 , diff:  1187.4181704483926
adv train loss:  -1731.056245803833 , diff:  62.75382900238037
adv train loss:  -1739.323184967041 , diff:  8.266939163208008
adv train loss:  -1756.8251857757568 , diff:  17.50200080871582
adv train loss:  -1765.1644973754883 , diff:  8.339311599731445
adv train loss:  -1761.2158374786377 , diff:  3.948659896850586
adv train loss:  -1765.452386856079 , diff:  4.236549377441406
adv train loss:  -1764.867431640625 , diff:  0.5849552154541016
adv train loss:  -1777.3891944885254 , diff:  12.52176284790039
layer  5  adv train finish, try to retain  117
test acc: top1 ->  34.62 ; top5 ->  79.66  and loss:  1852.3860836029053
forward train acc: top1 ->  99.312 ; top5 ->  99.998  and loss:  1.9976466787047684
test acc: top1 ->  91.34 ; top5 ->  99.37  and loss:  49.55841816216707
forward train acc: top1 ->  99.542 ; top5 ->  99.996  and loss:  1.3466435228474438
test acc: top1 ->  91.25 ; top5 ->  99.3  and loss:  51.25554406642914
forward train acc: top1 ->  99.57800000488281 ; top5 ->  100.0  and loss:  1.2262791190296412
test acc: top1 ->  91.65 ; top5 ->  99.39  and loss:  51.65418376028538
forward train acc: top1 ->  99.61200000732421 ; top5 ->  99.998  and loss:  1.1510700341314077
test acc: top1 ->  91.62 ; top5 ->  99.23  and loss:  53.93081145733595
forward train acc: top1 ->  99.67399997802734 ; top5 ->  100.0  and loss:  0.9649951197206974
test acc: top1 ->  91.44 ; top5 ->  99.31  and loss:  53.121088564395905
forward train acc: top1 ->  99.7360000024414 ; top5 ->  100.0  and loss:  0.7390238642692566
test acc: top1 ->  91.66 ; top5 ->  99.35  and loss:  53.55156351625919
forward train acc: top1 ->  99.7240000024414 ; top5 ->  100.0  and loss:  0.7399347024038434
test acc: top1 ->  91.87 ; top5 ->  99.35  and loss:  53.851906448602676
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
### skip layer  6 wait:  3  ###
---------------- start layer  7  ---------------
adv train loss:  -0.2664136062376201 , diff:  0.2664136062376201
adv train loss:  -0.28997169985086657 , diff:  0.023558093613246456
adv train loss:  -0.33536886298679747 , diff:  0.045397163135930896
adv train loss:  -0.2772161776665598 , diff:  0.05815268532023765
adv train loss:  -0.28186745091807097 , diff:  0.0046512732515111566
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  103
test acc: top1 ->  10.0 ; top5 ->  50.02  and loss:  5841835.328125
forward train acc: top1 ->  92.13399998291015 ; top5 ->  99.64  and loss:  27.295312087982893
test acc: top1 ->  89.39 ; top5 ->  98.67  and loss:  49.747454673051834
forward train acc: top1 ->  98.94600000732422 ; top5 ->  99.992  and loss:  3.626570913940668
test acc: top1 ->  90.65 ; top5 ->  99.02  and loss:  47.84693306684494
forward train acc: top1 ->  99.32799997558594 ; top5 ->  100.0  and loss:  2.1336109787225723
test acc: top1 ->  91.07 ; top5 ->  99.11  and loss:  48.93831957876682
forward train acc: top1 ->  99.52399997558594 ; top5 ->  100.0  and loss:  1.5715386476367712
test acc: top1 ->  91.26 ; top5 ->  99.31  and loss:  49.26067325472832
forward train acc: top1 ->  99.588 ; top5 ->  99.998  and loss:  1.3781354050152004
test acc: top1 ->  91.41 ; top5 ->  99.42  and loss:  50.014699809253216
forward train acc: top1 ->  99.68199997558594 ; top5 ->  100.0  and loss:  1.0126403253525496
test acc: top1 ->  91.42 ; top5 ->  99.34  and loss:  50.33334966003895
forward train acc: top1 ->  99.69999997802735 ; top5 ->  100.0  and loss:  0.9717522859573364
test acc: top1 ->  91.5 ; top5 ->  99.35  and loss:  51.55375589430332
forward train acc: top1 ->  99.71599997558594 ; top5 ->  100.0  and loss:  0.8605548962950706
test acc: top1 ->  91.65 ; top5 ->  99.42  and loss:  51.44470413029194
forward train acc: top1 ->  99.73999997558593 ; top5 ->  100.0  and loss:  0.8508800435811281
test acc: top1 ->  91.54 ; top5 ->  99.33  and loss:  52.90283268690109
forward train acc: top1 ->  99.798 ; top5 ->  100.0  and loss:  0.7230333928018808
test acc: top1 ->  91.57 ; top5 ->  99.37  and loss:  53.69926265627146
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  104 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -217.23255223035812 , diff:  217.23255223035812
adv train loss:  -623.0846848487854 , diff:  405.8521326184273
adv train loss:  -874.7742447853088 , diff:  251.68955993652344
adv train loss:  -1337.2105445861816 , diff:  462.4362998008728
adv train loss:  -2101.1739597320557 , diff:  763.963415145874
adv train loss:  -2453.7980041503906 , diff:  352.62404441833496
adv train loss:  -2451.2501792907715 , diff:  2.5478248596191406
adv train loss:  -2451.9257106781006 , diff:  0.6755313873291016
adv train loss:  -2461.6854038238525 , diff:  9.759693145751953
adv train loss:  -2464.444311141968 , diff:  2.7589073181152344
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  14
test acc: top1 ->  38.78 ; top5 ->  88.45  and loss:  1818.4085874557495
forward train acc: top1 ->  99.84 ; top5 ->  100.0  and loss:  0.4848335115239024
test acc: top1 ->  92.2 ; top5 ->  99.34  and loss:  57.82506687939167
==> this epoch:  14 / 512
---------------- start layer  9  ---------------
adv train loss:  -0.9407909121364355 , diff:  0.9407909121364355
adv train loss:  -0.9048588313162327 , diff:  0.03593208082020283
adv train loss:  -0.8771736305207014 , diff:  0.027685200795531273
adv train loss:  -0.8542904015630484 , diff:  0.022883228957653046
adv train loss:  -0.9422838781028986 , diff:  0.08799347653985023
adv train loss:  -0.8941673538647592 , diff:  0.04811652423813939
adv train loss:  -0.873191065620631 , diff:  0.020976288244128227
adv train loss:  -0.8583888430148363 , diff:  0.014802222605794668
adv train loss:  -24.125961586833 , diff:  23.267572743818164
adv train loss:  -1063.6102375984192 , diff:  1039.4842760115862
layer  9  adv train finish, try to retain  20
test acc: top1 ->  10.0 ; top5 ->  56.9  and loss:  2910.152843475342
forward train acc: top1 ->  99.67199997558593 ; top5 ->  99.994  and loss:  1.3666998385451734
test acc: top1 ->  91.32 ; top5 ->  99.07  and loss:  60.31757044047117
forward train acc: top1 ->  99.91799997558594 ; top5 ->  100.0  and loss:  0.30374643206596375
test acc: top1 ->  92.14 ; top5 ->  99.0  and loss:  58.69294025376439
==> this epoch:  20 / 512
---------------- start layer  10  ---------------
adv train loss:  -157.74076068401337 , diff:  157.74076068401337
adv train loss:  -157.80849158763885 , diff:  0.06773090362548828
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  21
test acc: top1 ->  10.01 ; top5 ->  59.48  and loss:  261188.1876220703
forward train acc: top1 ->  73.6519999951172 ; top5 ->  96.408  and loss:  101.81403955817223
test acc: top1 ->  56.35 ; top5 ->  97.47  and loss:  128.46837931871414
forward train acc: top1 ->  97.83599997802735 ; top5 ->  99.992  and loss:  11.751793794333935
test acc: top1 ->  88.76 ; top5 ->  98.95  and loss:  49.137981340289116
forward train acc: top1 ->  99.46599997558594 ; top5 ->  100.0  and loss:  2.9168450590223074
test acc: top1 ->  90.11 ; top5 ->  99.01  and loss:  49.44321823120117
forward train acc: top1 ->  99.70799997558593 ; top5 ->  100.0  and loss:  1.4958125296980143
test acc: top1 ->  91.01 ; top5 ->  99.11  and loss:  49.01062588393688
forward train acc: top1 ->  99.79199997558594 ; top5 ->  100.0  and loss:  1.024978432804346
test acc: top1 ->  91.38 ; top5 ->  99.11  and loss:  49.56414473056793
forward train acc: top1 ->  99.85999997558594 ; top5 ->  100.0  and loss:  0.7549119801260531
test acc: top1 ->  91.52 ; top5 ->  99.11  and loss:  49.726393327116966
forward train acc: top1 ->  99.826 ; top5 ->  100.0  and loss:  0.7671026424504817
test acc: top1 ->  91.58 ; top5 ->  99.15  and loss:  51.01520900428295
forward train acc: top1 ->  99.854 ; top5 ->  99.998  and loss:  0.6660724021494389
test acc: top1 ->  91.61 ; top5 ->  99.16  and loss:  50.11937375366688
forward train acc: top1 ->  99.872 ; top5 ->  100.0  and loss:  0.6052993931807578
test acc: top1 ->  91.62 ; top5 ->  99.13  and loss:  50.478128507733345
forward train acc: top1 ->  99.87 ; top5 ->  100.0  and loss:  0.560411224141717
test acc: top1 ->  91.81 ; top5 ->  99.13  and loss:  50.94832918047905
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  22 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -4165.714710235596 , diff:  4165.714710235596
adv train loss:  -4261.998291015625 , diff:  96.2835807800293
adv train loss:  -4264.031940460205 , diff:  2.033649444580078
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  1
test acc: top1 ->  10.77 ; top5 ->  50.0  and loss:  18452.1388092041
forward train acc: top1 ->  88.74999998535156 ; top5 ->  99.58  and loss:  66.92100623250008
test acc: top1 ->  84.1 ; top5 ->  98.37  and loss:  97.70434510707855
forward train acc: top1 ->  98.62400000732421 ; top5 ->  99.998  and loss:  4.86392542719841
test acc: top1 ->  89.5 ; top5 ->  98.74  and loss:  72.96620897948742
forward train acc: top1 ->  99.1740000024414 ; top5 ->  99.998  and loss:  3.030533247627318
test acc: top1 ->  89.94 ; top5 ->  98.76  and loss:  72.87648591399193
forward train acc: top1 ->  99.4240000024414 ; top5 ->  99.998  and loss:  2.1111905397847295
test acc: top1 ->  90.37 ; top5 ->  98.81  and loss:  72.0075895935297
forward train acc: top1 ->  99.578 ; top5 ->  100.0  and loss:  1.6316040474921465
test acc: top1 ->  90.47 ; top5 ->  98.71  and loss:  72.42571823298931
forward train acc: top1 ->  99.696 ; top5 ->  100.0  and loss:  1.170583673287183
test acc: top1 ->  90.61 ; top5 ->  98.82  and loss:  71.63589645922184
forward train acc: top1 ->  99.75 ; top5 ->  100.0  and loss:  1.0209834636189044
test acc: top1 ->  90.76 ; top5 ->  98.81  and loss:  71.0824393928051
forward train acc: top1 ->  99.77400000244141 ; top5 ->  99.998  and loss:  0.8638691697269678
test acc: top1 ->  90.86 ; top5 ->  98.78  and loss:  71.3075830489397
forward train acc: top1 ->  99.722 ; top5 ->  99.994  and loss:  1.0499510560184717
test acc: top1 ->  90.83 ; top5 ->  98.87  and loss:  70.1526954472065
forward train acc: top1 ->  99.806 ; top5 ->  100.0  and loss:  0.6621724064461887
test acc: top1 ->  90.94 ; top5 ->  98.82  and loss:  70.56672185659409
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  2 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -1586.7224025726318 , diff:  1586.7224025726318
adv train loss:  -1750.3432092666626 , diff:  163.62080669403076
adv train loss:  -1921.222599029541 , diff:  170.87938976287842
adv train loss:  -1899.6626567840576 , diff:  21.5599422454834
adv train loss:  -1898.8479328155518 , diff:  0.8147239685058594
adv train loss:  -1898.2362251281738 , diff:  0.6117076873779297
adv train loss:  -1898.0321311950684 , diff:  0.20409393310546875
layer  12  adv train finish, try to retain  6
test acc: top1 ->  10.0 ; top5 ->  61.55  and loss:  21308.488204956055
forward train acc: top1 ->  80.8800000024414 ; top5 ->  95.588  and loss:  94.29694389551878
test acc: top1 ->  89.75 ; top5 ->  98.44  and loss:  45.343298986554146
forward train acc: top1 ->  99.5100000024414 ; top5 ->  99.988  and loss:  5.04248483851552
test acc: top1 ->  90.82 ; top5 ->  98.59  and loss:  44.6727758795023
forward train acc: top1 ->  99.65399997558593 ; top5 ->  99.996  and loss:  2.857596630230546
test acc: top1 ->  91.1 ; top5 ->  98.64  and loss:  46.10970886051655
forward train acc: top1 ->  99.812 ; top5 ->  99.998  and loss:  1.7696669436991215
test acc: top1 ->  91.24 ; top5 ->  98.67  and loss:  47.565391682088375
forward train acc: top1 ->  99.87999997558593 ; top5 ->  100.0  and loss:  1.1871314272284508
test acc: top1 ->  91.29 ; top5 ->  98.67  and loss:  48.9578013792634
forward train acc: top1 ->  99.87599997558594 ; top5 ->  100.0  and loss:  1.0222980566322803
test acc: top1 ->  91.32 ; top5 ->  98.67  and loss:  49.20035955309868
forward train acc: top1 ->  99.884 ; top5 ->  100.0  and loss:  0.9172021802514791
test acc: top1 ->  91.35 ; top5 ->  98.68  and loss:  50.41130304336548
forward train acc: top1 ->  99.88400000244141 ; top5 ->  100.0  and loss:  0.8502239938825369
test acc: top1 ->  91.53 ; top5 ->  98.67  and loss:  51.12776478379965
forward train acc: top1 ->  99.924 ; top5 ->  100.0  and loss:  0.7060200199484825
test acc: top1 ->  91.49 ; top5 ->  98.67  and loss:  51.87647833675146
forward train acc: top1 ->  99.9 ; top5 ->  100.0  and loss:  0.6547558361198753
test acc: top1 ->  91.59 ; top5 ->  98.63  and loss:  52.555967055261135
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  41 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -18324.641914367676 , diff:  18324.641914367676
adv train loss:  -33253.23556518555 , diff:  14928.593650817871
adv train loss:  -46541.32556152344 , diff:  13288.08999633789
adv train loss:  -59338.35662841797 , diff:  12797.031066894531
adv train loss:  -71948.46875 , diff:  12610.112121582031
adv train loss:  -84416.68145751953 , diff:  12468.212707519531
adv train loss:  -96778.22784423828 , diff:  12361.54638671875
adv train loss:  -108825.99206542969 , diff:  12047.764221191406
adv train loss:  -117970.93664550781 , diff:  9144.944580078125
adv train loss:  -121867.6005859375 , diff:  3896.6639404296875
layer  13  adv train finish, try to retain  32
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.46875  ==>  30 / 64 , inc:  1
layer  1  :  0.828125  ==>  53 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.3828125  ==>  98 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.02734375  ==>  14 / 512 , inc:  7
layer  9  :  0.0390625  ==>  20 / 512 , inc:  2
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.033203125  ==>  17 / 512 , inc:  1
eps [0.9352284553349016, 2.4939425475597377, 0.9352284553349016, 0.9352284553349016, 0.9352284553349016, 0.9352284553349016, 1.2469712737798688, 0.9352284553349016, 1.4778918800354004, 1.2469712737798688, 0.9352284553349016, 1.2469712737798688, 0.9352284553349016, 19.9515403804779]  wait [4, 2, 4, 4, 4, 4, 2, 4, 0, 0, 4, 4, 4, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 7, 2, 1, 1, 1, 1]  tol: 8
$$$$$$$$$$$$$ epoch  64  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
adv train loss:  -9.936139769852161 , diff:  9.936139769852161
adv train loss:  -10.112891979515553 , diff:  0.1767522096633911
adv train loss:  -9.530557710677385 , diff:  0.5823342688381672
adv train loss:  -9.76463920250535 , diff:  0.23408149182796478
adv train loss:  -9.947242479771376 , diff:  0.18260327726602554
adv train loss:  -9.771264545619488 , diff:  0.1759779341518879
adv train loss:  -9.609370477497578 , diff:  0.1618940681219101
adv train loss:  -9.96192080900073 , diff:  0.35255033150315285
adv train loss:  -9.874528035521507 , diff:  0.08739277347922325
adv train loss:  -9.681805774569511 , diff:  0.19272226095199585
layer  1  adv train finish, try to retain  53
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
adv train loss:  -1253.2349180057645 , diff:  1253.2349180057645
adv train loss:  -4002.0181579589844 , diff:  2748.78323995322
adv train loss:  -4107.175453186035 , diff:  105.15729522705078
adv train loss:  -4102.261341094971 , diff:  4.914112091064453
adv train loss:  -4100.117351531982 , diff:  2.1439895629882812
adv train loss:  -4094.2380294799805 , diff:  5.879322052001953
adv train loss:  -4109.369403839111 , diff:  15.13137435913086
adv train loss:  -4128.083953857422 , diff:  18.714550018310547
adv train loss:  -4142.153770446777 , diff:  14.069816589355469
adv train loss:  -4139.855899810791 , diff:  2.297870635986328
layer  6  adv train finish, try to retain  66
test acc: top1 ->  32.17 ; top5 ->  89.94  and loss:  1919.5123596191406
forward train acc: top1 ->  99.606 ; top5 ->  100.0  and loss:  1.4174870228162035
test acc: top1 ->  91.57 ; top5 ->  98.99  and loss:  107.79695075750351
forward train acc: top1 ->  99.82999997558593 ; top5 ->  100.0  and loss:  0.5264372481033206
test acc: top1 ->  91.63 ; top5 ->  99.02  and loss:  105.89981240034103
forward train acc: top1 ->  99.83800000488282 ; top5 ->  100.0  and loss:  0.5963901355862617
test acc: top1 ->  91.63 ; top5 ->  99.07  and loss:  100.26897191256285
forward train acc: top1 ->  99.854 ; top5 ->  100.0  and loss:  0.47922831858159043
test acc: top1 ->  91.76 ; top5 ->  99.13  and loss:  101.04716394841671
forward train acc: top1 ->  99.862 ; top5 ->  100.0  and loss:  0.4667603084817529
test acc: top1 ->  91.56 ; top5 ->  99.09  and loss:  102.39183584600687
forward train acc: top1 ->  99.89199997558593 ; top5 ->  100.0  and loss:  0.3221842865459621
test acc: top1 ->  91.77 ; top5 ->  99.04  and loss:  95.60963040590286
forward train acc: top1 ->  99.908 ; top5 ->  100.0  and loss:  0.29732584580779076
test acc: top1 ->  92.0 ; top5 ->  99.15  and loss:  94.38679477572441
forward train acc: top1 ->  99.90199997558594 ; top5 ->  100.0  and loss:  0.2852054741233587
test acc: top1 ->  91.82 ; top5 ->  99.1  and loss:  94.35617953538895
forward train acc: top1 ->  99.912 ; top5 ->  100.0  and loss:  0.23549262605956756
test acc: top1 ->  91.92 ; top5 ->  99.07  and loss:  94.4736599996686
forward train acc: top1 ->  99.87 ; top5 ->  100.0  and loss:  0.3797807424562052
test acc: top1 ->  91.92 ; top5 ->  99.08  and loss:  93.2721831202507
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  98 / 256 , inc:  1
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
adv train loss:  -1428.556231021881 , diff:  1428.556231021881
adv train loss:  -3603.538028717041 , diff:  2174.98179769516
adv train loss:  -4271.120826721191 , diff:  667.5827980041504
adv train loss:  -4708.68367767334 , diff:  437.56285095214844
adv train loss:  -4903.829967498779 , diff:  195.14628982543945
adv train loss:  -4910.493614196777 , diff:  6.663646697998047
adv train loss:  -4926.106494903564 , diff:  15.61288070678711
adv train loss:  -4945.713436126709 , diff:  19.60694122314453
adv train loss:  -4955.447071075439 , diff:  9.733634948730469
adv train loss:  -4953.647415161133 , diff:  1.7996559143066406
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  7
test acc: top1 ->  45.71 ; top5 ->  93.51  and loss:  1249.2856016159058
forward train acc: top1 ->  98.10999997558594 ; top5 ->  99.94  and loss:  9.129690174944699
test acc: top1 ->  90.94 ; top5 ->  98.27  and loss:  78.42239068821073
forward train acc: top1 ->  99.788 ; top5 ->  100.0  and loss:  0.7140639586141333
test acc: top1 ->  91.18 ; top5 ->  98.5  and loss:  76.37844193540514
forward train acc: top1 ->  99.848 ; top5 ->  100.0  and loss:  0.46238706167787313
test acc: top1 ->  91.42 ; top5 ->  98.55  and loss:  75.44806099310517
forward train acc: top1 ->  99.864 ; top5 ->  100.0  and loss:  0.40159893059171736
test acc: top1 ->  91.41 ; top5 ->  98.59  and loss:  77.0531270839274
forward train acc: top1 ->  99.886 ; top5 ->  100.0  and loss:  0.3216303930967115
test acc: top1 ->  91.57 ; top5 ->  98.69  and loss:  78.74015111103654
forward train acc: top1 ->  99.876 ; top5 ->  100.0  and loss:  0.3958074892871082
test acc: top1 ->  91.53 ; top5 ->  98.72  and loss:  77.74511376023293
forward train acc: top1 ->  99.918 ; top5 ->  100.0  and loss:  0.26539227389730513
test acc: top1 ->  91.52 ; top5 ->  98.68  and loss:  78.12301386147738
forward train acc: top1 ->  99.924 ; top5 ->  100.0  and loss:  0.2610915171680972
test acc: top1 ->  91.57 ; top5 ->  98.74  and loss:  78.86955738440156
forward train acc: top1 ->  99.91799997558594 ; top5 ->  100.0  and loss:  0.22207500552758574
test acc: top1 ->  91.62 ; top5 ->  98.74  and loss:  79.5194341391325
forward train acc: top1 ->  99.92999997558594 ; top5 ->  100.0  and loss:  0.258213946595788
test acc: top1 ->  91.73 ; top5 ->  98.79  and loss:  80.01026345789433
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  14 / 512 , inc:  7
---------------- start layer  9  ---------------
adv train loss:  -839.6196449100971 , diff:  839.6196449100971
adv train loss:  -2334.7719917297363 , diff:  1495.1523468196392
adv train loss:  -2680.5838146209717 , diff:  345.81182289123535
adv train loss:  -2734.4748859405518 , diff:  53.89107131958008
adv train loss:  -3849.6500205993652 , diff:  1115.1751346588135
adv train loss:  -3849.8490676879883 , diff:  0.19904708862304688
adv train loss:  -3879.9821815490723 , diff:  30.133113861083984
adv train loss:  -3884.426242828369 , diff:  4.444061279296875
adv train loss:  -3883.842658996582 , diff:  0.5835838317871094
adv train loss:  -3884.288074493408 , diff:  0.4454154968261719
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  18
test acc: top1 ->  10.0 ; top5 ->  64.35  and loss:  12243.933059692383
forward train acc: top1 ->  99.426 ; top5 ->  100.0  and loss:  2.915161188779166
test acc: top1 ->  91.26 ; top5 ->  99.09  and loss:  85.30015905946493
forward train acc: top1 ->  99.92 ; top5 ->  99.998  and loss:  0.24595083952590358
test acc: top1 ->  92.08 ; top5 ->  99.06  and loss:  77.63026610761881
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.1653954653738765
test acc: top1 ->  92.07 ; top5 ->  99.07  and loss:  77.80005918443203
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.11216447847982636
test acc: top1 ->  92.06 ; top5 ->  99.09  and loss:  78.92992082983255
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.1415039451676421
test acc: top1 ->  92.21 ; top5 ->  99.07  and loss:  78.76329219341278
==> this epoch:  18 / 512
---------------- start layer  10  ---------------
### skip layer  10 wait:  4  ###
---------------- start layer  11  ---------------
### skip layer  11 wait:  4  ###
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
adv train loss:  -19002.340156555176 , diff:  19002.340156555176
adv train loss:  -31589.229293823242 , diff:  12586.889137268066
adv train loss:  -43726.3366394043 , diff:  12137.107345581055
adv train loss:  -55731.138671875 , diff:  12004.802032470703
adv train loss:  -67663.90948486328 , diff:  11932.770812988281
adv train loss:  -79554.47045898438 , diff:  11890.560974121094
adv train loss:  -91460.24816894531 , diff:  11905.777709960938
adv train loss:  -103350.6187133789 , diff:  11890.370544433594
adv train loss:  -115195.58471679688 , diff:  11844.966003417969
adv train loss:  -127065.32800292969 , diff:  11869.743286132812
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  16
test acc: top1 ->  47.36 ; top5 ->  96.13  and loss:  556.9760980606079
forward train acc: top1 ->  96.436 ; top5 ->  99.982  and loss:  21.43480250122957
test acc: top1 ->  91.64 ; top5 ->  98.93  and loss:  82.26906061172485
forward train acc: top1 ->  99.88 ; top5 ->  100.0  and loss:  0.37935312135959975
test acc: top1 ->  91.79 ; top5 ->  98.95  and loss:  82.2667124569416
forward train acc: top1 ->  99.912 ; top5 ->  100.0  and loss:  0.2780211758799851
test acc: top1 ->  91.8 ; top5 ->  99.04  and loss:  82.33087878674269
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.23921066807815805
test acc: top1 ->  91.84 ; top5 ->  98.94  and loss:  81.96855712682009
forward train acc: top1 ->  99.944 ; top5 ->  100.0  and loss:  0.18946556108130608
test acc: top1 ->  92.09 ; top5 ->  98.95  and loss:  82.79987420886755
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.20494921017962042
test acc: top1 ->  91.87 ; top5 ->  98.95  and loss:  82.83015061914921
forward train acc: top1 ->  99.938 ; top5 ->  100.0  and loss:  0.17822947638342157
test acc: top1 ->  92.03 ; top5 ->  98.94  and loss:  82.41928889602423
forward train acc: top1 ->  99.94 ; top5 ->  100.0  and loss:  0.21025410303263925
test acc: top1 ->  91.99 ; top5 ->  98.84  and loss:  83.03709971159697
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.13703563055605628
test acc: top1 ->  92.0 ; top5 ->  99.04  and loss:  82.45539765805006
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.14771598258812446
test acc: top1 ->  91.88 ; top5 ->  99.02  and loss:  82.7229313030839
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  17 / 512 , inc:  1
layer  0  :  0.46875  ==>  30 / 64 , inc:  1
layer  1  :  0.828125  ==>  53 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.3828125  ==>  98 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.02734375  ==>  14 / 512 , inc:  3
layer  9  :  0.03515625  ==>  18 / 512 , inc:  4
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.033203125  ==>  17 / 512 , inc:  1
eps [0.9352284553349016, 4.987885095119475, 0.9352284553349016, 0.9352284553349016, 0.9352284553349016, 0.9352284553349016, 0.9352284553349016, 0.9352284553349016, 1.1084189100265502, 1.2469712737798688, 0.9352284553349016, 1.2469712737798688, 0.9352284553349016, 14.963655285358426]  wait [3, 2, 3, 3, 3, 3, 4, 3, 2, 0, 3, 3, 3, 4]  inc [1, 1, 1, 1, 1, 1, 1, 1, 3, 4, 1, 1, 1, 1]  tol: 8
$$$$$$$$$$$$$ epoch  65  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
adv train loss:  -4.38669141381979 , diff:  4.38669141381979
adv train loss:  -5.023282380774617 , diff:  0.6365909669548273
adv train loss:  -5.238453157246113 , diff:  0.21517077647149563
adv train loss:  -5.055273937061429 , diff:  0.1831792201846838
adv train loss:  -4.723697045817971 , diff:  0.3315768912434578
adv train loss:  -5.013469537720084 , diff:  0.28977249190211296
adv train loss:  -4.628063973039389 , diff:  0.38540556468069553
adv train loss:  -4.909338712692261 , diff:  0.2812747396528721
adv train loss:  -4.390107782557607 , diff:  0.519230930134654
adv train loss:  -4.749345742166042 , diff:  0.35923795960843563
layer  1  adv train finish, try to retain  52
test acc: top1 ->  9.83 ; top5 ->  48.54  and loss:  1249.744776725769
forward train acc: top1 ->  99.86199997558593 ; top5 ->  100.0  and loss:  0.5055434433743358
test acc: top1 ->  92.04 ; top5 ->  98.99  and loss:  98.49597623944283
forward train acc: top1 ->  99.93400000244141 ; top5 ->  100.0  and loss:  0.232201486825943
test acc: top1 ->  92.02 ; top5 ->  99.11  and loss:  94.69681614637375
forward train acc: top1 ->  99.90999997558593 ; top5 ->  100.0  and loss:  0.2912402299698442
test acc: top1 ->  92.14 ; top5 ->  99.18  and loss:  99.90774025022984
==> this epoch:  52 / 64
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
adv train loss:  -342.1874253153801 , diff:  342.1874253153801
adv train loss:  -2840.7370796203613 , diff:  2498.5496543049812
adv train loss:  -3794.990951538086 , diff:  954.2538719177246
adv train loss:  -3786.3784217834473 , diff:  8.612529754638672
adv train loss:  -3902.3328552246094 , diff:  115.95443344116211
adv train loss:  -4138.171085357666 , diff:  235.83823013305664
adv train loss:  -4158.666133880615 , diff:  20.49504852294922
adv train loss:  -4281.7099609375 , diff:  123.04382705688477
adv train loss:  -4500.965995788574 , diff:  219.25603485107422
adv train loss:  -4499.621593475342 , diff:  1.3444023132324219
layer  8  adv train finish, try to retain  495
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -615.8080656304955 , diff:  615.8080656304955
adv train loss:  -4157.270618438721 , diff:  3541.462552808225
adv train loss:  -4842.402961730957 , diff:  685.1323432922363
adv train loss:  -4878.630558013916 , diff:  36.227596282958984
adv train loss:  -4933.894397735596 , diff:  55.26383972167969
adv train loss:  -5049.245208740234 , diff:  115.35081100463867
adv train loss:  -5051.852741241455 , diff:  2.607532501220703
adv train loss:  -5051.473468780518 , diff:  0.3792724609375
adv train loss:  -5103.04708480835 , diff:  51.57361602783203
adv train loss:  -5103.220691680908 , diff:  0.17360687255859375
layer  9  adv train finish, try to retain  26
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
### skip layer  10 wait:  3  ###
---------------- start layer  11  ---------------
### skip layer  11 wait:  3  ###
---------------- start layer  12  ---------------
### skip layer  12 wait:  3  ###
---------------- start layer  13  ---------------
### skip layer  13 wait:  4  ###
layer  0  :  0.46875  ==>  30 / 64 , inc:  1
layer  1  :  0.8125  ==>  52 / 64 , inc:  2
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.3828125  ==>  98 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.02734375  ==>  14 / 512 , inc:  3
layer  9  :  0.03515625  ==>  18 / 512 , inc:  4
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.033203125  ==>  17 / 512 , inc:  1
eps [0.9352284553349016, 4.987885095119475, 0.9352284553349016, 0.9352284553349016, 0.9352284553349016, 0.9352284553349016, 0.9352284553349016, 0.9352284553349016, 2.2168378200531005, 2.4939425475597377, 0.9352284553349016, 1.2469712737798688, 0.9352284553349016, 14.963655285358426]  wait [2, 0, 2, 2, 2, 2, 3, 2, 2, 0, 2, 2, 2, 3]  inc [1, 2, 1, 1, 1, 1, 1, 1, 3, 4, 1, 1, 1, 1]  tol: 8
$$$$$$$$$$$$$ epoch  66  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -177.05126094818115 , diff:  177.05126094818115
adv train loss:  -178.156329870224 , diff:  1.1050689220428467
adv train loss:  -181.232262134552 , diff:  3.075932264328003
adv train loss:  -180.71241748332977 , diff:  0.519844651222229
adv train loss:  -175.36553287506104 , diff:  5.346884608268738
adv train loss:  -179.44243454933167 , diff:  4.07690167427063
adv train loss:  -179.60770344734192 , diff:  0.1652688980102539
layer  0  adv train finish, try to retain  59
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -0.1620903704315424 , diff:  0.1620903704315424
adv train loss:  -0.15634945005876943 , diff:  0.005740920372772962
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  50
test acc: top1 ->  10.37 ; top5 ->  50.0  and loss:  1732.7748165130615
forward train acc: top1 ->  99.78 ; top5 ->  100.0  and loss:  0.8916880716569722
test acc: top1 ->  91.79 ; top5 ->  99.12  and loss:  96.86993159353733
forward train acc: top1 ->  99.83 ; top5 ->  100.0  and loss:  0.7233037879850599
test acc: top1 ->  91.84 ; top5 ->  99.08  and loss:  87.99528923630714
forward train acc: top1 ->  99.864 ; top5 ->  100.0  and loss:  0.45935861038742587
test acc: top1 ->  91.84 ; top5 ->  99.14  and loss:  90.14789436757565
forward train acc: top1 ->  99.85199997558594 ; top5 ->  100.0  and loss:  0.3947365116328001
test acc: top1 ->  91.55 ; top5 ->  99.16  and loss:  94.50454330444336
forward train acc: top1 ->  99.874 ; top5 ->  100.0  and loss:  0.42477489546581637
test acc: top1 ->  91.92 ; top5 ->  99.19  and loss:  91.20829877257347
forward train acc: top1 ->  99.88799997558594 ; top5 ->  100.0  and loss:  0.3466948112472892
test acc: top1 ->  91.87 ; top5 ->  99.15  and loss:  90.94929661601782
forward train acc: top1 ->  99.908 ; top5 ->  100.0  and loss:  0.29973571907612495
test acc: top1 ->  91.9 ; top5 ->  99.08  and loss:  86.0044417232275
forward train acc: top1 ->  99.91 ; top5 ->  100.0  and loss:  0.2652997854165733
test acc: top1 ->  92.0 ; top5 ->  99.07  and loss:  86.91881413757801
forward train acc: top1 ->  99.914 ; top5 ->  100.0  and loss:  0.2853244429570623
test acc: top1 ->  92.12 ; top5 ->  99.21  and loss:  86.33313467353582
==> this epoch:  50 / 64
---------------- start layer  2  ---------------
adv train loss:  -0.2587038576602936 , diff:  0.2587038576602936
adv train loss:  -0.28148343271459453 , diff:  0.02277957505430095
adv train loss:  -0.27814232744276524 , diff:  0.003341105271829292
layer  2  adv train finish, try to retain  119
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -0.16529861686285585 , diff:  0.16529861686285585
adv train loss:  -0.2297539929204504 , diff:  0.06445537605759455
adv train loss:  -0.2453165298247768 , diff:  0.015562536904326407
adv train loss:  -0.21170215024903882 , diff:  0.033614379575737985
adv train loss:  -0.22978707394213416 , diff:  0.018084923693095334
adv train loss:  -0.2850932434666902 , diff:  0.055306169524556026
adv train loss:  -0.3502495032735169 , diff:  0.06515625980682671
adv train loss:  -0.21894714706286322 , diff:  0.13130235621065367
adv train loss:  -0.248293825017754 , diff:  0.029346677954890765
adv train loss:  -0.2595842182636261 , diff:  0.01129039324587211
layer  3  adv train finish, try to retain  126
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -0.2960222848923877 , diff:  0.2960222848923877
adv train loss:  -0.25786574743688107 , diff:  0.03815653745550662
adv train loss:  -0.22832884080708027 , diff:  0.029536906629800797
adv train loss:  -0.2617710311897099 , diff:  0.03344219038262963
adv train loss:  -0.2350336231647816 , diff:  0.0267374080249283
adv train loss:  -0.19993287555553252 , diff:  0.03510074760924908
adv train loss:  -0.32436854764819145 , diff:  0.12443567209265893
adv train loss:  -0.26816154413245386 , diff:  0.0562070035157376
adv train loss:  -0.22956544253975153 , diff:  0.038596101592702325
adv train loss:  -0.2952178241685033 , diff:  0.06565238162875175
layer  4  adv train finish, try to retain  245
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -0.2977668673156586 , diff:  0.2977668673156586
adv train loss:  -0.22085794666782022 , diff:  0.07690892064783839
adv train loss:  -0.20734849013388157 , diff:  0.013509456533938646
adv train loss:  -0.2052650689438451 , diff:  0.0020834211900364608
layer  5  adv train finish, try to retain  252
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
### skip layer  6 wait:  3  ###
---------------- start layer  7  ---------------
adv train loss:  -0.38197099353419617 , diff:  0.38197099353419617
adv train loss:  -0.4438137480756268 , diff:  0.06184275454143062
adv train loss:  -0.401568082626909 , diff:  0.04224566544871777
adv train loss:  -0.5436173342750408 , diff:  0.14204925164813176
adv train loss:  -0.574307887814939 , diff:  0.030690553539898247
adv train loss:  -0.458844477776438 , diff:  0.11546341003850102
adv train loss:  -0.4748980381991714 , diff:  0.016053560422733426
adv train loss:  -0.48262873850762844 , diff:  0.007730700308457017
layer  7  adv train finish, try to retain  400
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -105.17400455474854 , diff:  105.17400455474854
adv train loss:  -387.65006363391876 , diff:  282.4760590791702
adv train loss:  -1901.901354789734 , diff:  1514.2512911558151
adv train loss:  -2865.442741394043 , diff:  963.5413866043091
adv train loss:  -3325.2839279174805 , diff:  459.8411865234375
adv train loss:  -3324.648769378662 , diff:  0.6351585388183594
adv train loss:  -3413.50980758667 , diff:  88.86103820800781
adv train loss:  -3808.8294944763184 , diff:  395.31968688964844
adv train loss:  -3811.745143890381 , diff:  2.9156494140625
adv train loss:  -3813.0794677734375 , diff:  1.3343238830566406
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  11
test acc: top1 ->  46.05 ; top5 ->  92.86  and loss:  1036.45609664917
forward train acc: top1 ->  99.824 ; top5 ->  100.0  and loss:  0.5358049924070656
test acc: top1 ->  91.91 ; top5 ->  99.2  and loss:  83.79775760322809
forward train acc: top1 ->  99.87199997558594 ; top5 ->  100.0  and loss:  0.37973389867693186
test acc: top1 ->  92.05 ; top5 ->  99.04  and loss:  79.72462467104197
forward train acc: top1 ->  99.862 ; top5 ->  100.0  and loss:  0.44542555174120935
test acc: top1 ->  92.03 ; top5 ->  99.18  and loss:  80.33364183455706
forward train acc: top1 ->  99.902 ; top5 ->  100.0  and loss:  0.31433368782745674
test acc: top1 ->  92.04 ; top5 ->  99.2  and loss:  80.89154909551144
forward train acc: top1 ->  99.88999997558594 ; top5 ->  100.0  and loss:  0.2968784971162677
test acc: top1 ->  91.87 ; top5 ->  99.21  and loss:  84.5240498483181
forward train acc: top1 ->  99.9180000024414 ; top5 ->  100.0  and loss:  0.2698199339210987
test acc: top1 ->  91.99 ; top5 ->  99.19  and loss:  81.47461853921413
forward train acc: top1 ->  99.93599997558594 ; top5 ->  100.0  and loss:  0.17848758096806705
test acc: top1 ->  92.11 ; top5 ->  99.21  and loss:  81.69334541261196
==> this epoch:  11 / 512
---------------- start layer  9  ---------------
adv train loss:  -3822.5131225585938 , diff:  3822.5131225585938
adv train loss:  -3936.1622581481934 , diff:  113.64913558959961
adv train loss:  -3948.5701942443848 , diff:  12.407936096191406
adv train loss:  -3953.7840003967285 , diff:  5.21380615234375
adv train loss:  -4041.7937622070312 , diff:  88.00976181030273
adv train loss:  -3908.959384918213 , diff:  132.83437728881836
adv train loss:  -4106.425987243652 , diff:  197.46660232543945
adv train loss:  -4234.318477630615 , diff:  127.89249038696289
adv train loss:  -4233.16597366333 , diff:  1.1525039672851562
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  14
test acc: top1 ->  17.09 ; top5 ->  60.63  and loss:  7729.611057281494
forward train acc: top1 ->  99.68199997558594 ; top5 ->  99.998  and loss:  0.9288577497936785
test acc: top1 ->  91.06 ; top5 ->  98.75  and loss:  85.17523813992739
forward train acc: top1 ->  99.886 ; top5 ->  100.0  and loss:  0.3649097286397591
test acc: top1 ->  91.95 ; top5 ->  98.99  and loss:  72.55748170614243
forward train acc: top1 ->  99.89 ; top5 ->  100.0  and loss:  0.35558629501610994
test acc: top1 ->  92.0 ; top5 ->  99.11  and loss:  79.1556014046073
forward train acc: top1 ->  99.922 ; top5 ->  100.0  and loss:  0.20046287082368508
test acc: top1 ->  92.13 ; top5 ->  98.92  and loss:  77.4564998075366
==> this epoch:  14 / 512
---------------- start layer  10  ---------------
adv train loss:  -31.23105989396572 , diff:  31.23105989396572
adv train loss:  -31.170316711068153 , diff:  0.06074318289756775
adv train loss:  -31.180559068918228 , diff:  0.010242357850074768
layer  10  adv train finish, try to retain  471
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -3048.610397338867 , diff:  3048.610397338867
adv train loss:  -3339.4721717834473 , diff:  290.8617744445801
adv train loss:  -3364.5133094787598 , diff:  25.0411376953125
adv train loss:  -3363.245288848877 , diff:  1.2680206298828125
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  1
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  22306.22508239746
forward train acc: top1 ->  64.58799997314453 ; top5 ->  94.608  and loss:  142.95271041989326
test acc: top1 ->  41.89 ; top5 ->  78.49  and loss:  471.17211532592773
forward train acc: top1 ->  89.91199999267577 ; top5 ->  99.982  and loss:  33.06594969332218
test acc: top1 ->  84.54 ; top5 ->  98.53  and loss:  63.2855988740921
forward train acc: top1 ->  97.59200000976563 ; top5 ->  99.964  and loss:  12.274816334247589
test acc: top1 ->  89.31 ; top5 ->  98.55  and loss:  51.17054797708988
forward train acc: top1 ->  99.02199997802734 ; top5 ->  99.994  and loss:  5.1810097098350525
test acc: top1 ->  89.47 ; top5 ->  98.55  and loss:  53.162035301327705
forward train acc: top1 ->  99.16399997558594 ; top5 ->  99.986  and loss:  3.9565953128039837
test acc: top1 ->  89.69 ; top5 ->  98.59  and loss:  53.902888759970665
forward train acc: top1 ->  99.30400000488281 ; top5 ->  99.994  and loss:  3.1817574240267277
test acc: top1 ->  89.92 ; top5 ->  98.61  and loss:  54.793946608901024
forward train acc: top1 ->  99.33000000244141 ; top5 ->  99.99  and loss:  3.0786057636141777
test acc: top1 ->  89.96 ; top5 ->  98.58  and loss:  55.70717699825764
forward train acc: top1 ->  99.39199997558593 ; top5 ->  99.992  and loss:  2.7207490541040897
test acc: top1 ->  89.88 ; top5 ->  98.63  and loss:  56.50620397925377
forward train acc: top1 ->  99.472 ; top5 ->  99.996  and loss:  2.363207192160189
test acc: top1 ->  90.02 ; top5 ->  98.6  and loss:  57.319627568125725
forward train acc: top1 ->  99.474 ; top5 ->  99.994  and loss:  2.3285061875358224
test acc: top1 ->  90.09 ; top5 ->  98.63  and loss:  56.803150683641434
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  2 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -37.92140105366707 , diff:  37.92140105366707
adv train loss:  -38.78030976653099 , diff:  0.8589087128639221
adv train loss:  -38.60046876966953 , diff:  0.17984099686145782
adv train loss:  -37.93632897734642 , diff:  0.6641397923231125
adv train loss:  -38.23358917236328 , diff:  0.29726019501686096
adv train loss:  -38.19600106775761 , diff:  0.037588104605674744
layer  12  adv train finish, try to retain  486
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
### skip layer  13 wait:  3  ###
layer  0  :  0.46875  ==>  30 / 64 , inc:  1
layer  1  :  0.78125  ==>  50 / 64 , inc:  4
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.3828125  ==>  98 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.021484375  ==>  11 / 512 , inc:  5
layer  9  :  0.02734375  ==>  14 / 512 , inc:  7
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.033203125  ==>  17 / 512 , inc:  1
eps [1.8704569106698032, 4.987885095119475, 1.8704569106698032, 1.8704569106698032, 1.8704569106698032, 1.8704569106698032, 0.9352284553349016, 1.8704569106698032, 2.2168378200531005, 2.4939425475597377, 1.8704569106698032, 0.9352284553349016, 1.8704569106698032, 14.963655285358426]  wait [2, 0, 2, 2, 2, 2, 2, 2, 0, 0, 2, 4, 2, 2]  inc [1, 4, 1, 1, 1, 1, 1, 1, 5, 7, 1, 1, 1, 1]  tol: 8
$$$$$$$$$$$$$ epoch  67  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -2008.472743988037 , diff:  2008.472743988037
adv train loss:  -2016.2741050720215 , diff:  7.801361083984375
adv train loss:  -2010.577527999878 , diff:  5.696577072143555
adv train loss:  -2017.5555419921875 , diff:  6.97801399230957
adv train loss:  -2014.3914279937744 , diff:  3.164113998413086
adv train loss:  -2037.300703048706 , diff:  22.90927505493164
adv train loss:  -2056.0612449645996 , diff:  18.760541915893555
adv train loss:  -2055.8974628448486 , diff:  0.16378211975097656
layer  0  adv train finish, try to retain  2
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  2424.0221633911133
forward train acc: top1 ->  19.425999993896486 ; top5 ->  68.838  and loss:  485.1956341266632
test acc: top1 ->  10.3 ; top5 ->  49.8  and loss:  284.29659271240234
forward train acc: top1 ->  24.006000001220702 ; top5 ->  76.77400000244141  and loss:  203.95595347881317
test acc: top1 ->  27.46 ; top5 ->  80.56  and loss:  198.00679802894592
forward train acc: top1 ->  26.15399999206543 ; top5 ->  79.42400000488281  and loss:  196.26267910003662
test acc: top1 ->  30.16 ; top5 ->  82.5  and loss:  191.61439502239227
forward train acc: top1 ->  28.65799999572754 ; top5 ->  81.38600000488282  and loss:  189.96102499961853
test acc: top1 ->  31.45 ; top5 ->  84.08  and loss:  186.8982914686203
forward train acc: top1 ->  30.443999993896483 ; top5 ->  83.19400001464844  and loss:  185.1714962720871
test acc: top1 ->  32.68 ; top5 ->  85.67  and loss:  181.84972274303436
forward train acc: top1 ->  31.709999985351562 ; top5 ->  83.91200000488281  and loss:  181.7441804409027
test acc: top1 ->  35.23 ; top5 ->  86.24  and loss:  177.56362402439117
forward train acc: top1 ->  33.01999998779297 ; top5 ->  84.66599998535156  and loss:  179.38979482650757
test acc: top1 ->  35.95 ; top5 ->  86.95  and loss:  175.16338324546814
forward train acc: top1 ->  33.82800000366211 ; top5 ->  85.24399998535156  and loss:  177.1441365480423
test acc: top1 ->  36.3 ; top5 ->  87.66  and loss:  172.8921686410904
forward train acc: top1 ->  34.67199999572754 ; top5 ->  85.77000000732421  and loss:  175.0472490787506
test acc: top1 ->  38.35 ; top5 ->  88.11  and loss:  169.9649477005005
forward train acc: top1 ->  35.28200000244141 ; top5 ->  85.95799999023437  and loss:  173.7548954486847
test acc: top1 ->  38.02 ; top5 ->  88.3  and loss:  168.89723527431488
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  30 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -85.75338751077652 , diff:  85.75338751077652
adv train loss:  -85.75595766305923 , diff:  0.0025701522827148438
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  46
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  354.7725455760956
forward train acc: top1 ->  98.22799997802734 ; top5 ->  99.99  and loss:  13.277543231844902
test acc: top1 ->  91.3 ; top5 ->  99.11  and loss:  43.07039491087198
forward train acc: top1 ->  99.72599997558594 ; top5 ->  100.0  and loss:  0.9783819681033492
test acc: top1 ->  91.41 ; top5 ->  99.16  and loss:  47.4251434803009
forward train acc: top1 ->  99.7160000024414 ; top5 ->  99.996  and loss:  0.9458138588815928
test acc: top1 ->  91.58 ; top5 ->  99.11  and loss:  50.7195695489645
forward train acc: top1 ->  99.796 ; top5 ->  100.0  and loss:  0.640791526879184
test acc: top1 ->  91.77 ; top5 ->  99.05  and loss:  52.17437241971493
forward train acc: top1 ->  99.856 ; top5 ->  99.998  and loss:  0.48486319149378687
test acc: top1 ->  91.65 ; top5 ->  99.15  and loss:  54.739371702075005
forward train acc: top1 ->  99.836 ; top5 ->  100.0  and loss:  0.4996016719378531
test acc: top1 ->  91.69 ; top5 ->  99.2  and loss:  55.095265097916126
forward train acc: top1 ->  99.81999997558594 ; top5 ->  99.998  and loss:  0.523517498280853
test acc: top1 ->  91.63 ; top5 ->  99.13  and loss:  56.223551727831364
forward train acc: top1 ->  99.856 ; top5 ->  100.0  and loss:  0.4726423295214772
test acc: top1 ->  91.58 ; top5 ->  99.1  and loss:  55.895041577517986
forward train acc: top1 ->  99.862 ; top5 ->  100.0  and loss:  0.44302216125652194
test acc: top1 ->  91.74 ; top5 ->  99.14  and loss:  56.796240761876106
forward train acc: top1 ->  99.832 ; top5 ->  100.0  and loss:  0.463930458878167
test acc: top1 ->  91.66 ; top5 ->  99.0  and loss:  56.93792241066694
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  50 / 64 , inc:  4
---------------- start layer  2  ---------------
adv train loss:  -445.43387389555573 , diff:  445.43387389555573
adv train loss:  -1001.1183776855469 , diff:  555.6845037899911
adv train loss:  -1103.859790802002 , diff:  102.74141311645508
adv train loss:  -1135.1667261123657 , diff:  31.30693531036377
adv train loss:  -1135.2461462020874 , diff:  0.07942008972167969
adv train loss:  -1131.8636598587036 , diff:  3.382486343383789
adv train loss:  -1148.020318031311 , diff:  16.156658172607422
adv train loss:  -1163.4030332565308 , diff:  15.382715225219727
adv train loss:  -1163.7381210327148 , diff:  0.33508777618408203
adv train loss:  -1166.7422103881836 , diff:  3.00408935546875
layer  2  adv train finish, try to retain  59
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  4586.283241271973
forward train acc: top1 ->  94.77599999023437 ; top5 ->  99.75799997558593  and loss:  18.013008318841457
test acc: top1 ->  87.76 ; top5 ->  98.75  and loss:  49.55225323140621
forward train acc: top1 ->  95.91800001953125 ; top5 ->  99.89199997558593  and loss:  12.403614185750484
test acc: top1 ->  88.47 ; top5 ->  98.92  and loss:  46.859648779034615
forward train acc: top1 ->  96.51800001220703 ; top5 ->  99.92399997558594  and loss:  10.47675734013319
test acc: top1 ->  88.87 ; top5 ->  98.97  and loss:  47.04380311071873
forward train acc: top1 ->  96.89599997070313 ; top5 ->  99.944  and loss:  9.26766785979271
test acc: top1 ->  89.12 ; top5 ->  99.03  and loss:  46.126268699765205
forward train acc: top1 ->  97.29800001953124 ; top5 ->  99.95  and loss:  8.001620080322027
test acc: top1 ->  89.13 ; top5 ->  99.02  and loss:  47.802524745464325
forward train acc: top1 ->  97.56199998291015 ; top5 ->  99.964  and loss:  7.161072820425034
test acc: top1 ->  89.29 ; top5 ->  99.19  and loss:  46.80309998989105
forward train acc: top1 ->  97.51200001220703 ; top5 ->  99.96  and loss:  7.153571173548698
test acc: top1 ->  89.41 ; top5 ->  99.08  and loss:  46.521027728915215
forward train acc: top1 ->  97.59199998291015 ; top5 ->  99.962  and loss:  6.942044112831354
test acc: top1 ->  89.49 ; top5 ->  99.19  and loss:  46.11447237432003
forward train acc: top1 ->  97.74799998535157 ; top5 ->  99.958  and loss:  6.654130417853594
test acc: top1 ->  89.69 ; top5 ->  99.14  and loss:  46.31829135119915
forward train acc: top1 ->  97.79599998291016 ; top5 ->  99.978  and loss:  6.462221227586269
test acc: top1 ->  89.43 ; top5 ->  99.18  and loss:  47.34930941462517
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -619.5230522118509 , diff:  619.5230522118509
adv train loss:  -1004.897931098938 , diff:  385.3748788870871
adv train loss:  -1020.0378684997559 , diff:  15.139937400817871
adv train loss:  -1036.1935806274414 , diff:  16.155712127685547
adv train loss:  -1044.1434144973755 , diff:  7.949833869934082
adv train loss:  -1043.0093955993652 , diff:  1.134018898010254
adv train loss:  -1043.2905340194702 , diff:  0.28113842010498047
adv train loss:  -1040.8683547973633 , diff:  2.4221792221069336
adv train loss:  -1041.8436584472656 , diff:  0.9753036499023438
adv train loss:  -1040.0486450195312 , diff:  1.795013427734375
layer  3  adv train finish, try to retain  27
test acc: top1 ->  10.0 ; top5 ->  53.23  and loss:  971.677731513977
forward train acc: top1 ->  80.29000001708984 ; top5 ->  98.25400000488281  and loss:  62.651432663202286
test acc: top1 ->  77.99 ; top5 ->  97.63  and loss:  72.12588003277779
forward train acc: top1 ->  83.27399999755859 ; top5 ->  98.88399997802735  and loss:  49.59350594878197
test acc: top1 ->  79.29 ; top5 ->  98.07  and loss:  68.46368309855461
forward train acc: top1 ->  84.76999998291015 ; top5 ->  99.01799997802735  and loss:  45.49416849017143
test acc: top1 ->  80.27 ; top5 ->  98.24  and loss:  63.102455854415894
forward train acc: top1 ->  85.83000000732422 ; top5 ->  99.15599997802734  and loss:  42.42607635259628
test acc: top1 ->  81.02 ; top5 ->  98.28  and loss:  61.081245958805084
forward train acc: top1 ->  86.63999998046874 ; top5 ->  99.21999997802735  and loss:  39.83705484867096
test acc: top1 ->  81.9 ; top5 ->  98.38  and loss:  59.14412096142769
forward train acc: top1 ->  87.31600000244141 ; top5 ->  99.31200000488282  and loss:  37.3131265938282
test acc: top1 ->  82.23 ; top5 ->  98.37  and loss:  58.213844776153564
forward train acc: top1 ->  87.59800001708984 ; top5 ->  99.31999997558594  and loss:  36.66168129444122
test acc: top1 ->  82.51 ; top5 ->  98.43  and loss:  57.554833233356476
forward train acc: top1 ->  87.79199999267578 ; top5 ->  99.39399997558594  and loss:  36.03990337252617
test acc: top1 ->  82.8 ; top5 ->  98.39  and loss:  57.25707679986954
forward train acc: top1 ->  87.76600000976562 ; top5 ->  99.35999998046876  and loss:  35.749494045972824
test acc: top1 ->  82.78 ; top5 ->  98.36  and loss:  57.193262070417404
forward train acc: top1 ->  88.29200001464844 ; top5 ->  99.39599997802735  and loss:  34.28247782588005
test acc: top1 ->  82.88 ; top5 ->  98.53  and loss:  55.78875410556793
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -102.91731005907059 , diff:  102.91731005907059
adv train loss:  -539.6046919822693 , diff:  436.6873819231987
adv train loss:  -655.6904211044312 , diff:  116.08572912216187
adv train loss:  -734.2254772186279 , diff:  78.53505611419678
adv train loss:  -743.1653904914856 , diff:  8.939913272857666
adv train loss:  -752.7866168022156 , diff:  9.62122631072998
adv train loss:  -761.0444922447205 , diff:  8.257875442504883
adv train loss:  -769.0057678222656 , diff:  7.961275577545166
adv train loss:  -770.5599093437195 , diff:  1.5541415214538574
adv train loss:  -772.4197020530701 , diff:  1.859792709350586
layer  4  adv train finish, try to retain  21
test acc: top1 ->  10.15 ; top5 ->  49.99  and loss:  882.9381885528564
forward train acc: top1 ->  69.45000001464844 ; top5 ->  95.90400001464843  and loss:  90.5217934846878
test acc: top1 ->  71.62 ; top5 ->  96.57  and loss:  86.37990319728851
forward train acc: top1 ->  75.12599999511718 ; top5 ->  97.48599998535157  and loss:  72.21120119094849
test acc: top1 ->  74.67 ; top5 ->  97.28  and loss:  77.37160050868988
forward train acc: top1 ->  77.58399999511718 ; top5 ->  97.92999998535156  and loss:  65.0684432387352
test acc: top1 ->  76.02 ; top5 ->  97.68  and loss:  73.2682318687439
forward train acc: top1 ->  79.21199997558594 ; top5 ->  98.24600000732421  and loss:  60.276673674583435
test acc: top1 ->  77.21 ; top5 ->  97.75  and loss:  69.48187386989594
forward train acc: top1 ->  80.33400000732422 ; top5 ->  98.47400000732422  and loss:  57.21276879310608
test acc: top1 ->  77.94 ; top5 ->  97.91  and loss:  68.16084051132202
forward train acc: top1 ->  81.20200001464843 ; top5 ->  98.66400000732422  and loss:  54.27649047970772
test acc: top1 ->  78.15 ; top5 ->  97.95  and loss:  67.10187509655952
forward train acc: top1 ->  81.06599999755859 ; top5 ->  98.65200000244141  and loss:  53.83309492468834
test acc: top1 ->  78.7 ; top5 ->  97.94  and loss:  65.68569722771645
forward train acc: top1 ->  81.63799999511718 ; top5 ->  98.75800000488282  and loss:  52.46016988158226
test acc: top1 ->  79.14 ; top5 ->  98.14  and loss:  64.52317342162132
forward train acc: top1 ->  81.90200000488281 ; top5 ->  98.73199998046876  and loss:  51.92577901482582
test acc: top1 ->  79.48 ; top5 ->  98.25  and loss:  62.90562656521797
forward train acc: top1 ->  82.30399999511718 ; top5 ->  98.78399997802734  and loss:  50.9340078830719
test acc: top1 ->  79.66 ; top5 ->  98.12  and loss:  63.14643010497093
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  156 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -163.21531634777784 , diff:  163.21531634777784
adv train loss:  -760.201229095459 , diff:  596.9859127476811
adv train loss:  -805.0466628074646 , diff:  44.845433712005615
adv train loss:  -826.9250693321228 , diff:  21.878406524658203
adv train loss:  -831.7263031005859 , diff:  4.801233768463135
adv train loss:  -835.2103967666626 , diff:  3.48409366607666
adv train loss:  -839.4863748550415 , diff:  4.275978088378906
adv train loss:  -847.7857313156128 , diff:  8.299356460571289
adv train loss:  -854.6716737747192 , diff:  6.885942459106445
adv train loss:  -856.7283535003662 , diff:  2.0566797256469727
layer  5  adv train finish, try to retain  21
test acc: top1 ->  15.81 ; top5 ->  61.4  and loss:  512.2904262542725
forward train acc: top1 ->  75.98799999511719 ; top5 ->  98.05999998046875  and loss:  69.13024413585663
test acc: top1 ->  76.82 ; top5 ->  97.99  and loss:  72.58155977725983
forward train acc: top1 ->  81.33999999267579 ; top5 ->  98.95200000488282  and loss:  53.44957834482193
test acc: top1 ->  79.05 ; top5 ->  98.24  and loss:  65.7638635635376
forward train acc: top1 ->  83.53599999755859 ; top5 ->  99.19200000488281  and loss:  47.24588477611542
test acc: top1 ->  80.67 ; top5 ->  98.41  and loss:  60.71627601981163
forward train acc: top1 ->  84.85799999023438 ; top5 ->  99.27799997802734  and loss:  43.62594836950302
test acc: top1 ->  81.0 ; top5 ->  98.46  and loss:  60.81693375110626
forward train acc: top1 ->  85.54800001220703 ; top5 ->  99.3800000024414  and loss:  41.234519958496094
test acc: top1 ->  82.27 ; top5 ->  98.64  and loss:  55.60266810655594
forward train acc: top1 ->  86.42599999023437 ; top5 ->  99.43199997558594  and loss:  38.833148807287216
test acc: top1 ->  82.58 ; top5 ->  98.68  and loss:  55.319360479712486
forward train acc: top1 ->  87.02 ; top5 ->  99.44000000488282  and loss:  37.87432834506035
test acc: top1 ->  82.64 ; top5 ->  98.69  and loss:  54.73064514994621
forward train acc: top1 ->  87.28999999511718 ; top5 ->  99.45399997558594  and loss:  36.718348532915115
test acc: top1 ->  83.04 ; top5 ->  98.79  and loss:  53.923301696777344
forward train acc: top1 ->  87.58199997070312 ; top5 ->  99.5140000024414  and loss:  35.74008584022522
test acc: top1 ->  83.24 ; top5 ->  98.82  and loss:  53.32085810601711
forward train acc: top1 ->  87.73799998779297 ; top5 ->  99.56  and loss:  34.77624207735062
test acc: top1 ->  83.23 ; top5 ->  98.82  and loss:  52.700159430503845
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -7.873176611959934 , diff:  7.873176611959934
adv train loss:  -7.962854363024235 , diff:  0.08967775106430054
adv train loss:  -7.858265407383442 , diff:  0.10458895564079285
adv train loss:  -7.895753853023052 , diff:  0.03748844563961029
adv train loss:  -7.70629309117794 , diff:  0.18946076184511185
adv train loss:  -7.888650447130203 , diff:  0.18235735595226288
adv train loss:  -7.8708168640732765 , diff:  0.017833583056926727
adv train loss:  -7.845201648771763 , diff:  0.025615215301513672
adv train loss:  -7.876957677304745 , diff:  0.03175602853298187
adv train loss:  -7.870168603956699 , diff:  0.006789073348045349
layer  6  adv train finish, try to retain  244
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -9.032897308468819 , diff:  9.032897308468819
adv train loss:  -9.056507535278797 , diff:  0.023610226809978485
adv train loss:  -8.910476848483086 , diff:  0.14603068679571152
adv train loss:  -9.130575604736805 , diff:  0.22009875625371933
adv train loss:  -8.944162003695965 , diff:  0.18641360104084015
adv train loss:  -8.98400966823101 , diff:  0.039847664535045624
adv train loss:  -8.95187772065401 , diff:  0.032131947576999664
adv train loss:  -9.043432638049126 , diff:  0.0915549173951149
adv train loss:  -8.860760644078255 , diff:  0.18267199397087097
adv train loss:  -8.937333405017853 , diff:  0.07657276093959808
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  103
test acc: top1 ->  5.53 ; top5 ->  58.42  and loss:  1289708.7724609375
forward train acc: top1 ->  88.01800000976563 ; top5 ->  99.182  and loss:  39.53665991127491
test acc: top1 ->  90.07 ; top5 ->  99.07  and loss:  40.72934474050999
forward train acc: top1 ->  98.39999997802734 ; top5 ->  99.982  and loss:  5.160245871171355
test acc: top1 ->  91.1 ; top5 ->  99.08  and loss:  42.70697324723005
forward train acc: top1 ->  99.00999997802734 ; top5 ->  99.998  and loss:  3.0365388486534357
test acc: top1 ->  91.44 ; top5 ->  99.29  and loss:  43.91087203472853
forward train acc: top1 ->  99.28799997802734 ; top5 ->  100.0  and loss:  2.141454577445984
test acc: top1 ->  91.36 ; top5 ->  99.29  and loss:  46.2600262761116
forward train acc: top1 ->  99.48599997558594 ; top5 ->  100.0  and loss:  1.6051716366782784
test acc: top1 ->  91.29 ; top5 ->  99.34  and loss:  48.780746430158615
forward train acc: top1 ->  99.54000000488281 ; top5 ->  100.0  and loss:  1.397989822551608
test acc: top1 ->  91.48 ; top5 ->  99.23  and loss:  48.44361966103315
forward train acc: top1 ->  99.594 ; top5 ->  99.998  and loss:  1.2499801334924996
test acc: top1 ->  91.51 ; top5 ->  99.38  and loss:  48.51242035627365
forward train acc: top1 ->  99.6500000024414 ; top5 ->  100.0  and loss:  1.15094805508852
test acc: top1 ->  91.49 ; top5 ->  99.35  and loss:  48.69651897251606
forward train acc: top1 ->  99.65999997558593 ; top5 ->  99.998  and loss:  1.0919382777065039
test acc: top1 ->  91.52 ; top5 ->  99.36  and loss:  49.68255274742842
forward train acc: top1 ->  99.64999997558594 ; top5 ->  100.0  and loss:  1.0444167032837868
test acc: top1 ->  91.56 ; top5 ->  99.34  and loss:  50.893982134759426
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  104 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -1226.2523851394653 , diff:  1226.2523851394653
adv train loss:  -2176.4748401641846 , diff:  950.2224550247192
adv train loss:  -2964.474349975586 , diff:  787.9995098114014
adv train loss:  -3074.539255142212 , diff:  110.06490516662598
adv train loss:  -3070.296630859375 , diff:  4.242624282836914
adv train loss:  -3059.7029361724854 , diff:  10.593694686889648
adv train loss:  -3100.4913082122803 , diff:  40.78837203979492
adv train loss:  -3138.7209091186523 , diff:  38.22960090637207
adv train loss:  -3140.588689804077 , diff:  1.8677806854248047
adv train loss:  -3138.0882472991943 , diff:  2.5004425048828125
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  6
test acc: top1 ->  31.94 ; top5 ->  85.51  and loss:  1262.9891805648804
forward train acc: top1 ->  99.22599997558594 ; top5 ->  99.998  and loss:  2.3488964922726154
test acc: top1 ->  91.91 ; top5 ->  99.31  and loss:  48.787106689065695
forward train acc: top1 ->  99.75199997802734 ; top5 ->  100.0  and loss:  0.804312301799655
test acc: top1 ->  92.09 ; top5 ->  99.28  and loss:  50.3879469037056
forward train acc: top1 ->  99.80600000244141 ; top5 ->  100.0  and loss:  0.6613545995205641
test acc: top1 ->  91.88 ; top5 ->  99.27  and loss:  54.50638299807906
forward train acc: top1 ->  99.832 ; top5 ->  100.0  and loss:  0.5662515888689086
test acc: top1 ->  91.88 ; top5 ->  99.29  and loss:  55.24397660419345
forward train acc: top1 ->  99.862 ; top5 ->  99.998  and loss:  0.3963145134039223
test acc: top1 ->  91.92 ; top5 ->  99.27  and loss:  57.164412282407284
forward train acc: top1 ->  99.87 ; top5 ->  100.0  and loss:  0.3858373552793637
test acc: top1 ->  91.99 ; top5 ->  99.37  and loss:  56.045290399342775
forward train acc: top1 ->  99.89399997558594 ; top5 ->  100.0  and loss:  0.33092672377824783
test acc: top1 ->  92.03 ; top5 ->  99.35  and loss:  57.06291974708438
forward train acc: top1 ->  99.90799997558594 ; top5 ->  100.0  and loss:  0.3129085348919034
test acc: top1 ->  92.02 ; top5 ->  99.18  and loss:  59.05008074268699
forward train acc: top1 ->  99.87999997558593 ; top5 ->  100.0  and loss:  0.3376809561159462
test acc: top1 ->  92.15 ; top5 ->  99.15  and loss:  59.282966658473015
==> this epoch:  6 / 512
---------------- start layer  9  ---------------
adv train loss:  -1782.6148509979248 , diff:  1782.6148509979248
adv train loss:  -2185.9001960754395 , diff:  403.28534507751465
adv train loss:  -2234.958938598633 , diff:  49.05874252319336
adv train loss:  -2382.0655403137207 , diff:  147.1066017150879
adv train loss:  -2488.846933364868 , diff:  106.78139305114746
adv train loss:  -2550.7085132598877 , diff:  61.86157989501953
adv train loss:  -2568.2908115386963 , diff:  17.582298278808594
adv train loss:  -2567.2942028045654 , diff:  0.9966087341308594
adv train loss:  -2568.685802459717 , diff:  1.3915996551513672
adv train loss:  -2567.3107566833496 , diff:  1.3750457763671875
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  7
test acc: top1 ->  10.0 ; top5 ->  43.7  and loss:  3164.8372230529785
forward train acc: top1 ->  96.014 ; top5 ->  99.976  and loss:  17.012283966876566
test acc: top1 ->  41.08 ; top5 ->  88.41  and loss:  587.8514657020569
forward train acc: top1 ->  99.73799997802735 ; top5 ->  99.99799997558594  and loss:  1.535172738134861
test acc: top1 ->  91.57 ; top5 ->  99.19  and loss:  41.121209517121315
forward train acc: top1 ->  99.74399997558594 ; top5 ->  100.0  and loss:  1.0951545936986804
test acc: top1 ->  91.76 ; top5 ->  99.15  and loss:  43.5996104106307
forward train acc: top1 ->  99.824 ; top5 ->  100.0  and loss:  0.7462219027802348
test acc: top1 ->  91.74 ; top5 ->  99.15  and loss:  46.48364927619696
forward train acc: top1 ->  99.844 ; top5 ->  100.0  and loss:  0.6217791722156107
test acc: top1 ->  91.94 ; top5 ->  99.39  and loss:  47.52612125873566
forward train acc: top1 ->  99.832 ; top5 ->  100.0  and loss:  0.5680231780279428
test acc: top1 ->  91.96 ; top5 ->  99.3  and loss:  49.091268479824066
forward train acc: top1 ->  99.89799997558593 ; top5 ->  100.0  and loss:  0.4015538804233074
test acc: top1 ->  92.07 ; top5 ->  99.35  and loss:  49.74137520045042
forward train acc: top1 ->  99.87799997558594 ; top5 ->  100.0  and loss:  0.4487052052281797
test acc: top1 ->  91.83 ; top5 ->  99.34  and loss:  51.860017977654934
forward train acc: top1 ->  99.878 ; top5 ->  100.0  and loss:  0.4272032347507775
test acc: top1 ->  91.97 ; top5 ->  99.36  and loss:  51.461641021072865
forward train acc: top1 ->  99.87199997558594 ; top5 ->  100.0  and loss:  0.4429671885445714
test acc: top1 ->  92.06 ; top5 ->  99.33  and loss:  53.24804851412773
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  14 / 512 , inc:  7
---------------- start layer  10  ---------------
adv train loss:  -481.27051162719727 , diff:  481.27051162719727
adv train loss:  -482.554003238678 , diff:  1.283491611480713
adv train loss:  -481.9270520210266 , diff:  0.6269512176513672
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  21
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  417373.7736816406
forward train acc: top1 ->  78.82999997558593 ; top5 ->  94.692  and loss:  124.25026235729456
test acc: top1 ->  67.02 ; top5 ->  98.14  and loss:  102.47780656814575
forward train acc: top1 ->  99.29399997802734 ; top5 ->  99.99599997558593  and loss:  5.549086287617683
test acc: top1 ->  90.83 ; top5 ->  99.24  and loss:  39.95325382053852
forward train acc: top1 ->  99.63199997558594 ; top5 ->  100.0  and loss:  2.3244664147496223
test acc: top1 ->  91.19 ; top5 ->  99.2  and loss:  41.96745163947344
forward train acc: top1 ->  99.742 ; top5 ->  100.0  and loss:  1.3750075660645962
test acc: top1 ->  91.4 ; top5 ->  99.28  and loss:  43.42386317998171
forward train acc: top1 ->  99.78799997802734 ; top5 ->  100.0  and loss:  1.0168668441474438
test acc: top1 ->  91.54 ; top5 ->  99.3  and loss:  45.36506254225969
forward train acc: top1 ->  99.85999997558594 ; top5 ->  100.0  and loss:  0.7113054487854242
test acc: top1 ->  91.55 ; top5 ->  99.33  and loss:  46.724652387201786
forward train acc: top1 ->  99.836 ; top5 ->  100.0  and loss:  0.6664656940847635
test acc: top1 ->  91.59 ; top5 ->  99.35  and loss:  46.97630241513252
forward train acc: top1 ->  99.82399997558593 ; top5 ->  100.0  and loss:  0.6946618654765189
test acc: top1 ->  91.65 ; top5 ->  99.31  and loss:  48.09760946780443
forward train acc: top1 ->  99.8680000024414 ; top5 ->  100.0  and loss:  0.5501732192933559
test acc: top1 ->  91.58 ; top5 ->  99.3  and loss:  48.91437382251024
forward train acc: top1 ->  99.85799997558594 ; top5 ->  100.0  and loss:  0.583699612878263
test acc: top1 ->  91.62 ; top5 ->  99.32  and loss:  49.82107439637184
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  22 / 512 , inc:  1
---------------- start layer  11  ---------------
### skip layer  11 wait:  4  ###
---------------- start layer  12  ---------------
adv train loss:  -1835.7306451797485 , diff:  1835.7306451797485
adv train loss:  -2019.9003562927246 , diff:  184.16971111297607
adv train loss:  -2204.9907093048096 , diff:  185.09035301208496
adv train loss:  -2235.5116004943848 , diff:  30.520891189575195
adv train loss:  -2234.1367797851562 , diff:  1.3748207092285156
adv train loss:  -2232.78049659729 , diff:  1.356283187866211
adv train loss:  -2233.4153366088867 , diff:  0.6348400115966797
layer  12  adv train finish, try to retain  3
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  12801.669403076172
forward train acc: top1 ->  42.50999998535156 ; top5 ->  85.68399997558593  and loss:  373.94554102420807
test acc: top1 ->  62.48 ; top5 ->  96.24  and loss:  114.88595753908157
forward train acc: top1 ->  86.6379999975586 ; top5 ->  99.874  and loss:  61.73397180438042
test acc: top1 ->  84.83 ; top5 ->  96.79  and loss:  84.59940296411514
forward train acc: top1 ->  96.46000000976562 ; top5 ->  99.976  and loss:  38.787785083055496
test acc: top1 ->  88.19 ; top5 ->  97.36  and loss:  66.58877348899841
forward train acc: top1 ->  98.30000000244141 ; top5 ->  99.978  and loss:  23.210425093770027
test acc: top1 ->  89.25 ; top5 ->  97.63  and loss:  57.114832013845444
forward train acc: top1 ->  98.93799997558594 ; top5 ->  99.97399997558594  and loss:  14.608205318450928
test acc: top1 ->  89.67 ; top5 ->  97.8  and loss:  52.576942294836044
forward train acc: top1 ->  99.07800000488281 ; top5 ->  99.98599997558594  and loss:  10.676837384700775
test acc: top1 ->  89.97 ; top5 ->  97.83  and loss:  51.429549142718315
forward train acc: top1 ->  99.19199998535156 ; top5 ->  99.98800000244141  and loss:  8.782378673553467
test acc: top1 ->  89.87 ; top5 ->  97.77  and loss:  51.932137712836266
forward train acc: top1 ->  99.2240000024414 ; top5 ->  99.986  and loss:  7.591342903673649
test acc: top1 ->  90.1 ; top5 ->  97.86  and loss:  51.039707347750664
forward train acc: top1 ->  99.29799998046875 ; top5 ->  99.98  and loss:  6.427453953772783
test acc: top1 ->  90.23 ; top5 ->  97.89  and loss:  51.31253603100777
forward train acc: top1 ->  99.41799997802734 ; top5 ->  99.982  and loss:  5.4575577192008495
test acc: top1 ->  90.27 ; top5 ->  97.96  and loss:  51.70692564547062
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  41 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -21261.723342895508 , diff:  21261.723342895508
adv train loss:  -33829.6217956543 , diff:  12567.898452758789
adv train loss:  -46138.289123535156 , diff:  12308.66732788086
adv train loss:  -58404.77844238281 , diff:  12266.489318847656
adv train loss:  -70671.22064208984 , diff:  12266.442199707031
adv train loss:  -82939.20837402344 , diff:  12267.987731933594
adv train loss:  -95219.74615478516 , diff:  12280.537780761719
adv train loss:  -107513.06164550781 , diff:  12293.315490722656
adv train loss:  -119839.99096679688 , diff:  12326.929321289062
adv train loss:  -132162.0841064453 , diff:  12322.093139648438
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  16
test acc: top1 ->  28.31 ; top5 ->  77.28  and loss:  2532.461301803589
forward train acc: top1 ->  79.832 ; top5 ->  95.56  and loss:  272.07808106578887
test acc: top1 ->  90.14 ; top5 ->  98.78  and loss:  55.31085550785065
forward train acc: top1 ->  99.5320000024414 ; top5 ->  99.998  and loss:  2.2372688818722963
test acc: top1 ->  91.03 ; top5 ->  98.91  and loss:  50.109637662768364
forward train acc: top1 ->  99.68999997802734 ; top5 ->  100.0  and loss:  1.3208923302590847
test acc: top1 ->  91.4 ; top5 ->  98.95  and loss:  48.69451180100441
forward train acc: top1 ->  99.804 ; top5 ->  100.0  and loss:  0.9642540970817208
test acc: top1 ->  91.58 ; top5 ->  98.95  and loss:  48.78171268105507
forward train acc: top1 ->  99.832 ; top5 ->  100.0  and loss:  0.8047016635537148
test acc: top1 ->  91.7 ; top5 ->  99.04  and loss:  49.386725790798664
forward train acc: top1 ->  99.832 ; top5 ->  100.0  and loss:  0.7237768284976482
test acc: top1 ->  91.78 ; top5 ->  99.03  and loss:  48.501557633280754
forward train acc: top1 ->  99.892 ; top5 ->  99.998  and loss:  0.5627708323299885
test acc: top1 ->  91.71 ; top5 ->  99.05  and loss:  49.149897649884224
forward train acc: top1 ->  99.872 ; top5 ->  100.0  and loss:  0.5693087296094745
test acc: top1 ->  91.77 ; top5 ->  99.06  and loss:  49.26533577591181
forward train acc: top1 ->  99.894 ; top5 ->  100.0  and loss:  0.5070067259948701
test acc: top1 ->  91.78 ; top5 ->  99.08  and loss:  50.2169608399272
forward train acc: top1 ->  99.89 ; top5 ->  100.0  and loss:  0.4780699051916599
test acc: top1 ->  91.77 ; top5 ->  99.12  and loss:  48.9538349211216
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  17 / 512 , inc:  1
layer  0  :  0.46875  ==>  30 / 64 , inc:  1
layer  1  :  0.78125  ==>  50 / 64 , inc:  2
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.3828125  ==>  98 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.01171875  ==>  6 / 512 , inc:  3
layer  9  :  0.02734375  ==>  14 / 512 , inc:  3
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.033203125  ==>  17 / 512 , inc:  1
eps [1.4028426830023524, 3.7409138213396065, 1.4028426830023524, 1.4028426830023524, 1.4028426830023524, 1.4028426830023524, 1.8704569106698032, 1.4028426830023524, 2.2168378200531005, 1.8704569106698032, 1.4028426830023524, 0.9352284553349016, 1.4028426830023524, 11.222741464018819]  wait [4, 2, 4, 4, 4, 4, 2, 4, 0, 2, 4, 3, 4, 4]  inc [1, 2, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1]  tol: 8
$$$$$$$$$$$$$ epoch  68  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
adv train loss:  -74.55684888362885 , diff:  74.55684888362885
adv train loss:  -75.14958721399307 , diff:  0.5927383303642273
adv train loss:  -74.69809871912003 , diff:  0.4514884948730469
adv train loss:  -74.76197585463524 , diff:  0.06387713551521301
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  48
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  2346.187641143799
forward train acc: top1 ->  99.29199997558594 ; top5 ->  100.0  and loss:  2.4481251426041126
test acc: top1 ->  91.61 ; top5 ->  99.05  and loss:  72.35131438076496
forward train acc: top1 ->  99.864 ; top5 ->  99.998  and loss:  0.4306085587013513
test acc: top1 ->  91.85 ; top5 ->  99.13  and loss:  68.71114066243172
forward train acc: top1 ->  99.878 ; top5 ->  100.0  and loss:  0.3502570573473349
test acc: top1 ->  91.81 ; top5 ->  99.18  and loss:  69.92020340263844
forward train acc: top1 ->  99.892 ; top5 ->  100.0  and loss:  0.37596991227474064
test acc: top1 ->  91.84 ; top5 ->  99.21  and loss:  70.58248089253902
forward train acc: top1 ->  99.872 ; top5 ->  99.998  and loss:  0.3706735244777519
test acc: top1 ->  92.01 ; top5 ->  99.2  and loss:  69.94703440368176
forward train acc: top1 ->  99.916 ; top5 ->  100.0  and loss:  0.28405972896143794
test acc: top1 ->  92.16 ; top5 ->  99.19  and loss:  67.91487030684948
==> this epoch:  48 / 64
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
adv train loss:  -834.702398288995 , diff:  834.702398288995
adv train loss:  -2761.0122299194336 , diff:  1926.3098316304386
adv train loss:  -2848.263023376465 , diff:  87.25079345703125
adv train loss:  -2857.8251609802246 , diff:  9.562137603759766
adv train loss:  -2862.5357990264893 , diff:  4.710638046264648
adv train loss:  -2865.761541366577 , diff:  3.2257423400878906
adv train loss:  -2870.4720287323 , diff:  4.710487365722656
adv train loss:  -2856.525827407837 , diff:  13.94620132446289
adv train loss:  -2868.5921478271484 , diff:  12.066320419311523
adv train loss:  -2865.5413970947266 , diff:  3.050750732421875
layer  6  adv train finish, try to retain  5
test acc: top1 ->  10.84 ; top5 ->  49.91  and loss:  2297.805170059204
forward train acc: top1 ->  54.90599999877929 ; top5 ->  91.71199998779296  and loss:  195.95039242506027
test acc: top1 ->  56.7 ; top5 ->  93.98  and loss:  134.0904619693756
forward train acc: top1 ->  64.87200000244141 ; top5 ->  97.51400000976562  and loss:  95.91280925273895
test acc: top1 ->  62.66 ; top5 ->  95.64  and loss:  113.32893604040146
forward train acc: top1 ->  68.7679999951172 ; top5 ->  98.31399998046875  and loss:  84.311210334301
test acc: top1 ->  65.57 ; top5 ->  96.5  and loss:  104.24789291620255
forward train acc: top1 ->  72.04799997314453 ; top5 ->  98.72200000488282  and loss:  75.39262694120407
test acc: top1 ->  68.73 ; top5 ->  97.17  and loss:  95.78682321310043
forward train acc: top1 ->  75.19000001708984 ; top5 ->  98.87599997802734  and loss:  67.99762415885925
test acc: top1 ->  72.1 ; top5 ->  97.43  and loss:  87.68571346998215
forward train acc: top1 ->  77.85199999511718 ; top5 ->  99.05399997802735  and loss:  60.81444954872131
test acc: top1 ->  73.14 ; top5 ->  97.5  and loss:  85.333744764328
forward train acc: top1 ->  79.15599999023438 ; top5 ->  99.12999997558593  and loss:  57.444886296987534
test acc: top1 ->  74.02 ; top5 ->  97.65  and loss:  83.03273445367813
forward train acc: top1 ->  80.59399997558593 ; top5 ->  99.2540000024414  and loss:  54.01188814640045
test acc: top1 ->  75.17 ; top5 ->  97.79  and loss:  79.39391988515854
forward train acc: top1 ->  81.66199997314453 ; top5 ->  99.3660000048828  and loss:  51.00776997208595
test acc: top1 ->  75.19 ; top5 ->  97.9  and loss:  79.5097051858902
forward train acc: top1 ->  82.592 ; top5 ->  99.43599997802734  and loss:  48.58144721388817
test acc: top1 ->  76.6 ; top5 ->  98.17  and loss:  75.7469168305397
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  98 / 256 , inc:  1
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
adv train loss:  -1113.337779045105 , diff:  1113.337779045105
adv train loss:  -1443.0998678207397 , diff:  329.76208877563477
adv train loss:  -1811.2677173614502 , diff:  368.16784954071045
adv train loss:  -2093.225751876831 , diff:  281.95803451538086
adv train loss:  -2092.206859588623 , diff:  1.0188922882080078
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  3
test acc: top1 ->  35.61 ; top5 ->  68.83  and loss:  2254.3108711242676
forward train acc: top1 ->  93.55399998046875 ; top5 ->  99.906  and loss:  20.426482781767845
test acc: top1 ->  89.23 ; top5 ->  98.52  and loss:  55.80766263604164
forward train acc: top1 ->  99.2280000024414 ; top5 ->  100.0  and loss:  2.7583501804620028
test acc: top1 ->  90.57 ; top5 ->  98.62  and loss:  57.0378145724535
forward train acc: top1 ->  99.43999998046876 ; top5 ->  100.0  and loss:  1.8388076610863209
test acc: top1 ->  90.9 ; top5 ->  98.72  and loss:  59.59406401216984
forward train acc: top1 ->  99.576 ; top5 ->  100.0  and loss:  1.3430679254233837
test acc: top1 ->  90.92 ; top5 ->  98.72  and loss:  61.71695502102375
forward train acc: top1 ->  99.69999997802735 ; top5 ->  99.998  and loss:  0.9941456168889999
test acc: top1 ->  91.18 ; top5 ->  98.79  and loss:  63.27834962308407
forward train acc: top1 ->  99.764 ; top5 ->  100.0  and loss:  0.8281609225086868
test acc: top1 ->  91.2 ; top5 ->  98.71  and loss:  64.65989524126053
forward train acc: top1 ->  99.778 ; top5 ->  100.0  and loss:  0.7689122050069273
test acc: top1 ->  91.27 ; top5 ->  98.73  and loss:  65.25965802371502
forward train acc: top1 ->  99.7540000024414 ; top5 ->  100.0  and loss:  0.7551508396863937
test acc: top1 ->  91.34 ; top5 ->  98.67  and loss:  65.53414613008499
forward train acc: top1 ->  99.79199997558594 ; top5 ->  100.0  and loss:  0.6441030586138368
test acc: top1 ->  91.33 ; top5 ->  98.85  and loss:  66.21592216193676
forward train acc: top1 ->  99.802 ; top5 ->  100.0  and loss:  0.6084460746496916
test acc: top1 ->  91.59 ; top5 ->  98.75  and loss:  67.09874011576176
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  6 / 512 , inc:  3
---------------- start layer  9  ---------------
adv train loss:  -1907.7350912094116 , diff:  1907.7350912094116
adv train loss:  -2058.015754699707 , diff:  150.2806634902954
adv train loss:  -2148.5054302215576 , diff:  90.48967552185059
adv train loss:  -2201.8667850494385 , diff:  53.36135482788086
adv train loss:  -2204.124397277832 , diff:  2.2576122283935547
adv train loss:  -2225.820339202881 , diff:  21.695941925048828
adv train loss:  -2226.6514892578125 , diff:  0.8311500549316406
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  11
test acc: top1 ->  10.96 ; top5 ->  57.47  and loss:  12640.441017150879
forward train acc: top1 ->  98.032 ; top5 ->  99.994  and loss:  6.607303448021412
test acc: top1 ->  84.09 ; top5 ->  98.72  and loss:  104.56524509191513
forward train acc: top1 ->  99.79399997558593 ; top5 ->  100.0  and loss:  0.6964359353296459
test acc: top1 ->  91.62 ; top5 ->  99.21  and loss:  58.13014218211174
forward train acc: top1 ->  99.81 ; top5 ->  100.0  and loss:  0.5762845481513068
test acc: top1 ->  91.71 ; top5 ->  99.21  and loss:  58.66468124091625
forward train acc: top1 ->  99.86399997558594 ; top5 ->  100.0  and loss:  0.46670236997306347
test acc: top1 ->  91.79 ; top5 ->  99.19  and loss:  59.691791757941246
forward train acc: top1 ->  99.872 ; top5 ->  100.0  and loss:  0.43284746655263007
test acc: top1 ->  91.89 ; top5 ->  99.17  and loss:  60.12185078859329
forward train acc: top1 ->  99.9000000024414 ; top5 ->  100.0  and loss:  0.3380445223301649
test acc: top1 ->  91.86 ; top5 ->  99.24  and loss:  59.40436564385891
forward train acc: top1 ->  99.892 ; top5 ->  100.0  and loss:  0.33971662330441177
test acc: top1 ->  91.9 ; top5 ->  99.27  and loss:  59.65986390411854
forward train acc: top1 ->  99.87199998046874 ; top5 ->  100.0  and loss:  0.3751629777252674
test acc: top1 ->  92.0 ; top5 ->  99.24  and loss:  60.2465034276247
forward train acc: top1 ->  99.89 ; top5 ->  100.0  and loss:  0.34498102765064687
test acc: top1 ->  91.96 ; top5 ->  99.26  and loss:  60.68841788172722
forward train acc: top1 ->  99.912 ; top5 ->  100.0  and loss:  0.24682449887041003
test acc: top1 ->  91.94 ; top5 ->  99.29  and loss:  61.34511208534241
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  14 / 512 , inc:  3
---------------- start layer  10  ---------------
### skip layer  10 wait:  4  ###
---------------- start layer  11  ---------------
### skip layer  11 wait:  3  ###
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
### skip layer  13 wait:  4  ###
layer  0  :  0.46875  ==>  30 / 64 , inc:  1
layer  1  :  0.75  ==>  48 / 64 , inc:  4
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.3828125  ==>  98 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.01171875  ==>  6 / 512 , inc:  1
layer  9  :  0.02734375  ==>  14 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.033203125  ==>  17 / 512 , inc:  1
eps [1.4028426830023524, 3.7409138213396065, 1.4028426830023524, 1.4028426830023524, 1.4028426830023524, 1.4028426830023524, 1.4028426830023524, 1.4028426830023524, 1.6626283650398253, 1.4028426830023524, 1.4028426830023524, 0.9352284553349016, 1.4028426830023524, 11.222741464018819]  wait [3, 0, 3, 3, 3, 3, 4, 3, 2, 4, 3, 2, 3, 3]  inc [1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 8
$$$$$$$$$$$$$ epoch  69  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
adv train loss:  -5.177095137536526 , diff:  5.177095137536526
adv train loss:  -5.057431742548943 , diff:  0.11966339498758316
adv train loss:  -5.0244042463600636 , diff:  0.03302749618887901
adv train loss:  -5.551765192300081 , diff:  0.5273609459400177
adv train loss:  -5.032367132604122 , diff:  0.5193980596959591
adv train loss:  -5.1200630236417055 , diff:  0.08769589103758335
adv train loss:  -5.437327496707439 , diff:  0.3172644730657339
adv train loss:  -5.142399072647095 , diff:  0.2949284240603447
adv train loss:  -4.909599751234055 , diff:  0.23279932141304016
adv train loss:  -5.482923563569784 , diff:  0.5733238123357296
layer  1  adv train finish, try to retain  48
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
adv train loss:  -1781.6346530914307 , diff:  1781.6346530914307
adv train loss:  -2457.4873237609863 , diff:  675.8526706695557
adv train loss:  -2810.208475112915 , diff:  352.7211513519287
adv train loss:  -3226.876735687256 , diff:  416.6682605743408
adv train loss:  -3644.9979934692383 , diff:  418.1212577819824
adv train loss:  -3753.955421447754 , diff:  108.95742797851562
adv train loss:  -3752.5279502868652 , diff:  1.4274711608886719
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  5
test acc: top1 ->  11.5 ; top5 ->  68.55  and loss:  3908.628604888916
forward train acc: top1 ->  99.132 ; top5 ->  99.998  and loss:  3.2555927224457264
test acc: top1 ->  91.92 ; top5 ->  99.29  and loss:  62.40018726885319
forward train acc: top1 ->  99.84399997558593 ; top5 ->  100.0  and loss:  0.4682590523734689
test acc: top1 ->  91.85 ; top5 ->  99.3  and loss:  64.28686885535717
forward train acc: top1 ->  99.88799997558594 ; top5 ->  100.0  and loss:  0.4037090549245477
test acc: top1 ->  92.04 ; top5 ->  99.27  and loss:  65.41797025501728
forward train acc: top1 ->  99.916 ; top5 ->  100.0  and loss:  0.3130533220246434
test acc: top1 ->  91.75 ; top5 ->  99.3  and loss:  65.5549097508192
forward train acc: top1 ->  99.892 ; top5 ->  100.0  and loss:  0.2939455035375431
test acc: top1 ->  92.1 ; top5 ->  99.22  and loss:  64.75101687014103
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.21487766690552235
test acc: top1 ->  92.23 ; top5 ->  99.29  and loss:  66.01903441548347
==> this epoch:  5 / 512
---------------- start layer  9  ---------------
### skip layer  9 wait:  4  ###
---------------- start layer  10  ---------------
### skip layer  10 wait:  3  ###
---------------- start layer  11  ---------------
adv train loss:  -1053.81441116333 , diff:  1053.81441116333
adv train loss:  -1055.8957214355469 , diff:  2.081310272216797
layer  11  adv train finish, try to retain  457
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
### skip layer  12 wait:  3  ###
---------------- start layer  13  ---------------
### skip layer  13 wait:  3  ###
layer  0  :  0.46875  ==>  30 / 64 , inc:  1
layer  1  :  0.75  ==>  48 / 64 , inc:  4
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.3828125  ==>  98 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.009765625  ==>  5 / 512 , inc:  2
layer  9  :  0.02734375  ==>  14 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.033203125  ==>  17 / 512 , inc:  1
eps [1.4028426830023524, 7.481827642679213, 1.4028426830023524, 1.4028426830023524, 1.4028426830023524, 1.4028426830023524, 1.4028426830023524, 1.4028426830023524, 1.6626283650398253, 1.4028426830023524, 1.4028426830023524, 1.8704569106698032, 1.4028426830023524, 11.222741464018819]  wait [2, 0, 2, 2, 2, 2, 3, 2, 0, 3, 2, 2, 2, 2]  inc [1, 4, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1]  tol: 8
$$$$$$$$$$$$$ epoch  70  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -1706.9460401535034 , diff:  1706.9460401535034
adv train loss:  -1708.2504024505615 , diff:  1.3043622970581055
adv train loss:  -1741.4059562683105 , diff:  33.15555381774902
adv train loss:  -1754.1775302886963 , diff:  12.771574020385742
adv train loss:  -1749.7639331817627 , diff:  4.413597106933594
adv train loss:  -1751.1047172546387 , diff:  1.3407840728759766
adv train loss:  -1762.6288681030273 , diff:  11.524150848388672
adv train loss:  -1757.4848556518555 , diff:  5.144012451171875
adv train loss:  -1753.638298034668 , diff:  3.8465576171875
adv train loss:  -1754.1646041870117 , diff:  0.52630615234375
layer  0  adv train finish, try to retain  6
test acc: top1 ->  7.51 ; top5 ->  48.44  and loss:  2640.5220127105713
forward train acc: top1 ->  34.00200000366211 ; top5 ->  80.33199998046875  and loss:  339.0364360809326
test acc: top1 ->  15.74 ; top5 ->  63.2  and loss:  335.5028200149536
forward train acc: top1 ->  39.812000007324215 ; top5 ->  85.362  and loss:  169.4523354768753
test acc: top1 ->  44.27 ; top5 ->  87.47  and loss:  162.4744485616684
forward train acc: top1 ->  42.84600000488281 ; top5 ->  87.31799998779297  and loss:  160.59702801704407
test acc: top1 ->  45.93 ; top5 ->  89.05  and loss:  155.49029052257538
forward train acc: top1 ->  45.8839999975586 ; top5 ->  88.89199999267578  and loss:  152.45765006542206
test acc: top1 ->  49.47 ; top5 ->  90.5  and loss:  146.60888755321503
forward train acc: top1 ->  48.770000009765624 ; top5 ->  90.03999999267577  and loss:  145.21378529071808
test acc: top1 ->  51.17 ; top5 ->  91.29  and loss:  140.43804359436035
forward train acc: top1 ->  50.71399998535156 ; top5 ->  90.8940000024414  and loss:  139.43635046482086
test acc: top1 ->  52.83 ; top5 ->  91.82  and loss:  136.0521125793457
forward train acc: top1 ->  52.0320000012207 ; top5 ->  91.3340000024414  and loss:  135.9685319662094
test acc: top1 ->  54.0 ; top5 ->  92.37  and loss:  132.64946961402893
forward train acc: top1 ->  53.38799998535156 ; top5 ->  92.1240000024414  and loss:  132.07576406002045
test acc: top1 ->  55.25 ; top5 ->  92.96  and loss:  129.0698208808899
forward train acc: top1 ->  54.725999985351564 ; top5 ->  92.35399999755859  and loss:  129.4874050617218
test acc: top1 ->  56.37 ; top5 ->  93.25  and loss:  125.72159057855606
forward train acc: top1 ->  56.0020000012207 ; top5 ->  92.89600000244141  and loss:  125.5516391992569
test acc: top1 ->  57.51 ; top5 ->  93.4  and loss:  123.21958363056183
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  30 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -35.52568671107292 , diff:  35.52568671107292
adv train loss:  -35.48755392432213 , diff:  0.03813278675079346
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  44
test acc: top1 ->  10.02 ; top5 ->  50.0  and loss:  565.3208498954773
forward train acc: top1 ->  98.9660000024414 ; top5 ->  99.992  and loss:  5.66814991645515
test acc: top1 ->  91.42 ; top5 ->  99.36  and loss:  41.7885901927948
forward train acc: top1 ->  99.47000000244141 ; top5 ->  100.0  and loss:  1.7295428840443492
test acc: top1 ->  91.67 ; top5 ->  99.43  and loss:  42.708332538604736
forward train acc: top1 ->  99.61999997558594 ; top5 ->  99.996  and loss:  1.2175673246383667
test acc: top1 ->  91.44 ; top5 ->  99.31  and loss:  48.55968116223812
forward train acc: top1 ->  99.638 ; top5 ->  99.998  and loss:  1.041093965061009
test acc: top1 ->  91.64 ; top5 ->  99.28  and loss:  50.861263275146484
forward train acc: top1 ->  99.74199997558594 ; top5 ->  100.0  and loss:  0.8306821389123797
test acc: top1 ->  91.62 ; top5 ->  99.35  and loss:  52.50057569146156
forward train acc: top1 ->  99.73199997558594 ; top5 ->  100.0  and loss:  0.7674550693482161
test acc: top1 ->  91.73 ; top5 ->  99.34  and loss:  51.52991546690464
forward train acc: top1 ->  99.79999997558593 ; top5 ->  100.0  and loss:  0.5854830527678132
test acc: top1 ->  91.7 ; top5 ->  99.34  and loss:  53.65177400410175
forward train acc: top1 ->  99.79999997558593 ; top5 ->  100.0  and loss:  0.5469947878737003
test acc: top1 ->  91.76 ; top5 ->  99.35  and loss:  54.454062685370445
forward train acc: top1 ->  99.78 ; top5 ->  100.0  and loss:  0.6400218701455742
test acc: top1 ->  91.67 ; top5 ->  99.3  and loss:  54.41672393679619
forward train acc: top1 ->  99.81999997558594 ; top5 ->  100.0  and loss:  0.5427927351556718
test acc: top1 ->  91.6 ; top5 ->  99.37  and loss:  55.91818751394749
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  48 / 64 , inc:  4
---------------- start layer  2  ---------------
adv train loss:  -100.24243303015828 , diff:  100.24243303015828
adv train loss:  -775.3132343292236 , diff:  675.0708012990654
adv train loss:  -897.1929807662964 , diff:  121.87974643707275
adv train loss:  -895.5362787246704 , diff:  1.6567020416259766
adv train loss:  -895.008017539978 , diff:  0.5282611846923828
adv train loss:  -899.0923366546631 , diff:  4.084319114685059
adv train loss:  -896.5369758605957 , diff:  2.555360794067383
adv train loss:  -901.2885246276855 , diff:  4.751548767089844
adv train loss:  -938.736102104187 , diff:  37.447577476501465
adv train loss:  -950.0756902694702 , diff:  11.339588165283203
layer  2  adv train finish, try to retain  76
test acc: top1 ->  14.37 ; top5 ->  50.35  and loss:  4852.546585083008
forward train acc: top1 ->  98.38399997558594 ; top5 ->  99.986  and loss:  5.112722102552652
test acc: top1 ->  90.47 ; top5 ->  99.03  and loss:  52.24844126403332
forward train acc: top1 ->  98.94599997802734 ; top5 ->  99.992  and loss:  3.1956597715616226
test acc: top1 ->  90.78 ; top5 ->  99.33  and loss:  49.137152284383774
forward train acc: top1 ->  99.09799997558594 ; top5 ->  99.994  and loss:  2.703616177663207
test acc: top1 ->  90.86 ; top5 ->  99.32  and loss:  46.28356657922268
forward train acc: top1 ->  99.17399997802734 ; top5 ->  99.99  and loss:  2.5203586453571916
test acc: top1 ->  90.4 ; top5 ->  99.25  and loss:  52.35131274163723
forward train acc: top1 ->  99.31800000244141 ; top5 ->  100.0  and loss:  2.1144625321030617
test acc: top1 ->  90.78 ; top5 ->  99.19  and loss:  51.860038578510284
forward train acc: top1 ->  99.37799997802735 ; top5 ->  99.998  and loss:  1.9963435344398022
test acc: top1 ->  91.19 ; top5 ->  99.37  and loss:  50.90229259431362
forward train acc: top1 ->  99.4240000024414 ; top5 ->  99.998  and loss:  1.7421805439516902
test acc: top1 ->  91.07 ; top5 ->  99.35  and loss:  50.43412110209465
forward train acc: top1 ->  99.41399997558594 ; top5 ->  99.996  and loss:  1.757183932699263
test acc: top1 ->  91.24 ; top5 ->  99.37  and loss:  49.50216272473335
forward train acc: top1 ->  99.45199997558593 ; top5 ->  99.994  and loss:  1.676965480670333
test acc: top1 ->  91.13 ; top5 ->  99.4  and loss:  49.862543269991875
forward train acc: top1 ->  99.49599997558593 ; top5 ->  99.996  and loss:  1.4397545084357262
test acc: top1 ->  91.24 ; top5 ->  99.39  and loss:  50.35976408421993
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -631.4032331742346 , diff:  631.4032331742346
adv train loss:  -1229.3839254379272 , diff:  597.9806922636926
adv train loss:  -1277.8232650756836 , diff:  48.43933963775635
adv train loss:  -1280.6199293136597 , diff:  2.796664237976074
adv train loss:  -1277.6705207824707 , diff:  2.949408531188965
adv train loss:  -1280.6770162582397 , diff:  3.006495475769043
adv train loss:  -1281.5721969604492 , diff:  0.8951807022094727
adv train loss:  -1281.438678741455 , diff:  0.13351821899414062
adv train loss:  -1274.3637762069702 , diff:  7.074902534484863
adv train loss:  -1283.3251781463623 , diff:  8.96140193939209
layer  3  adv train finish, try to retain  48
test acc: top1 ->  17.57 ; top5 ->  59.89  and loss:  1716.334056854248
forward train acc: top1 ->  92.80799998046875 ; top5 ->  99.73199997802735  and loss:  23.331212624907494
test acc: top1 ->  85.78 ; top5 ->  98.43  and loss:  56.56318540871143
forward train acc: top1 ->  94.3620000024414 ; top5 ->  99.81800000244141  and loss:  16.954506158828735
test acc: top1 ->  87.19 ; top5 ->  98.91  and loss:  49.75470519065857
forward train acc: top1 ->  94.98599999267579 ; top5 ->  99.9  and loss:  14.779127404093742
test acc: top1 ->  87.37 ; top5 ->  99.09  and loss:  49.03550639748573
forward train acc: top1 ->  95.30599997314454 ; top5 ->  99.884  and loss:  13.851888507604599
test acc: top1 ->  87.88 ; top5 ->  99.03  and loss:  46.272267922759056
forward train acc: top1 ->  95.71000001464844 ; top5 ->  99.914  and loss:  12.482439294457436
test acc: top1 ->  88.02 ; top5 ->  99.11  and loss:  46.67611785233021
forward train acc: top1 ->  96.09000001953125 ; top5 ->  99.91399997558594  and loss:  11.538108713924885
test acc: top1 ->  88.18 ; top5 ->  99.14  and loss:  45.981791228055954
forward train acc: top1 ->  96.1239999975586 ; top5 ->  99.942  and loss:  11.002173334360123
test acc: top1 ->  88.24 ; top5 ->  99.13  and loss:  46.700071066617966
forward train acc: top1 ->  96.00200001708984 ; top5 ->  99.9180000024414  and loss:  11.471028842031956
test acc: top1 ->  88.19 ; top5 ->  99.06  and loss:  45.73901176452637
forward train acc: top1 ->  96.33199999511719 ; top5 ->  99.942  and loss:  10.812756955623627
test acc: top1 ->  88.36 ; top5 ->  99.17  and loss:  46.355091750621796
forward train acc: top1 ->  96.27799998535156 ; top5 ->  99.948  and loss:  10.650422289967537
test acc: top1 ->  88.42 ; top5 ->  99.16  and loss:  45.508233308792114
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -167.2597532644868 , diff:  167.2597532644868
adv train loss:  -828.2650165557861 , diff:  661.0052632912993
adv train loss:  -962.895435333252 , diff:  134.63041877746582
adv train loss:  -1043.767255783081 , diff:  80.8718204498291
adv train loss:  -1077.964879989624 , diff:  34.19762420654297
adv train loss:  -1092.7035579681396 , diff:  14.738677978515625
adv train loss:  -1088.8503723144531 , diff:  3.8531856536865234
adv train loss:  -1092.4167547225952 , diff:  3.56638240814209
adv train loss:  -1106.5896863937378 , diff:  14.172931671142578
adv train loss:  -1108.1002597808838 , diff:  1.510573387145996
layer  4  adv train finish, try to retain  55
test acc: top1 ->  30.31 ; top5 ->  69.05  and loss:  1290.1388244628906
forward train acc: top1 ->  86.78199999023437 ; top5 ->  99.2460000024414  and loss:  39.284073293209076
test acc: top1 ->  82.93 ; top5 ->  98.36  and loss:  55.71806317567825
forward train acc: top1 ->  89.19999999267579 ; top5 ->  99.484  and loss:  31.52350941300392
test acc: top1 ->  83.86 ; top5 ->  98.62  and loss:  53.04809047281742
forward train acc: top1 ->  90.29599998535156 ; top5 ->  99.58  and loss:  28.034863367676735
test acc: top1 ->  85.16 ; top5 ->  98.82  and loss:  50.57972928881645
forward train acc: top1 ->  91.12400000976562 ; top5 ->  99.65399997558593  and loss:  25.990043610334396
test acc: top1 ->  85.4 ; top5 ->  98.84  and loss:  49.55368633568287
forward train acc: top1 ->  91.43999998046876 ; top5 ->  99.7120000024414  and loss:  24.609282955527306
test acc: top1 ->  85.15 ; top5 ->  98.66  and loss:  53.21926248073578
forward train acc: top1 ->  91.87999998046875 ; top5 ->  99.71199997802735  and loss:  23.554605320096016
test acc: top1 ->  86.31 ; top5 ->  98.87  and loss:  47.45168659090996
forward train acc: top1 ->  92.1539999975586 ; top5 ->  99.73  and loss:  22.558024287223816
test acc: top1 ->  86.41 ; top5 ->  98.92  and loss:  46.3791571110487
forward train acc: top1 ->  92.402 ; top5 ->  99.734  and loss:  21.87225890159607
test acc: top1 ->  86.54 ; top5 ->  99.01  and loss:  46.46873289346695
forward train acc: top1 ->  92.75600000488281 ; top5 ->  99.73400000488282  and loss:  21.227086529135704
test acc: top1 ->  86.66 ; top5 ->  98.93  and loss:  47.0729153752327
forward train acc: top1 ->  92.71800001220703 ; top5 ->  99.74799997558594  and loss:  21.221720799803734
test acc: top1 ->  86.76 ; top5 ->  99.04  and loss:  46.11925229430199
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  156 / 256 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -172.31525572389364 , diff:  172.31525572389364
adv train loss:  -937.871741771698 , diff:  765.5564860478044
adv train loss:  -1041.6730890274048 , diff:  103.80134725570679
adv train loss:  -1066.742175102234 , diff:  25.0690860748291
adv train loss:  -1081.9634428024292 , diff:  15.221267700195312
adv train loss:  -1090.8182792663574 , diff:  8.854836463928223
adv train loss:  -1103.8845491409302 , diff:  13.066269874572754
adv train loss:  -1104.0145807266235 , diff:  0.13003158569335938
adv train loss:  -1105.5152397155762 , diff:  1.5006589889526367
adv train loss:  -1106.32004737854 , diff:  0.8048076629638672
layer  5  adv train finish, try to retain  69
test acc: top1 ->  49.63 ; top5 ->  87.04  and loss:  370.5570750236511
forward train acc: top1 ->  95.08599998779297 ; top5 ->  99.88  and loss:  14.576109245419502
test acc: top1 ->  88.04 ; top5 ->  99.16  and loss:  47.48630003631115
forward train acc: top1 ->  96.48800001220702 ; top5 ->  99.952  and loss:  10.324735254049301
test acc: top1 ->  88.41 ; top5 ->  99.18  and loss:  47.17234371602535
forward train acc: top1 ->  96.82399999023437 ; top5 ->  99.96  and loss:  9.35976181551814
test acc: top1 ->  89.33 ; top5 ->  99.19  and loss:  46.03796820342541
forward train acc: top1 ->  97.22799998291016 ; top5 ->  99.968  and loss:  8.078589204698801
test acc: top1 ->  89.48 ; top5 ->  99.14  and loss:  46.99128386378288
forward train acc: top1 ->  97.49400001220702 ; top5 ->  99.97  and loss:  7.094527866691351
test acc: top1 ->  89.02 ; top5 ->  99.28  and loss:  47.43930512666702
forward train acc: top1 ->  97.56800000976563 ; top5 ->  99.98  and loss:  6.8750004805624485
test acc: top1 ->  89.67 ; top5 ->  99.25  and loss:  45.50209383666515
forward train acc: top1 ->  97.67800000976563 ; top5 ->  99.984  and loss:  6.54690907523036
test acc: top1 ->  89.87 ; top5 ->  99.3  and loss:  46.14463958889246
forward train acc: top1 ->  97.80600001220704 ; top5 ->  99.974  and loss:  6.32248380035162
test acc: top1 ->  89.73 ; top5 ->  99.29  and loss:  46.613369815051556
forward train acc: top1 ->  97.77799997802734 ; top5 ->  99.976  and loss:  6.26843473687768
test acc: top1 ->  89.84 ; top5 ->  99.33  and loss:  46.3974916189909
forward train acc: top1 ->  97.82399998779297 ; top5 ->  99.976  and loss:  6.139429647475481
test acc: top1 ->  89.82 ; top5 ->  99.35  and loss:  46.429094322025776
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  149 / 256 , inc:  1
---------------- start layer  6  ---------------
### skip layer  6 wait:  3  ###
---------------- start layer  7  ---------------
adv train loss:  -1.4044493064284325 , diff:  1.4044493064284325
adv train loss:  -1.4496014546602964 , diff:  0.045152148231863976
adv train loss:  -1.476734827272594 , diff:  0.027133372612297535
adv train loss:  -1.490455772727728 , diff:  0.013720945455133915
adv train loss:  -1.5437767636030912 , diff:  0.05332099087536335
adv train loss:  -1.5231207199394703 , diff:  0.02065604366362095
adv train loss:  -1.5253535769879818 , diff:  0.002232857048511505
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  103
test acc: top1 ->  17.07 ; top5 ->  56.59  and loss:  3068491.1953125
forward train acc: top1 ->  90.24399998046874 ; top5 ->  99.314  and loss:  35.31186757609248
test acc: top1 ->  89.23 ; top5 ->  99.0  and loss:  42.33987797051668
forward train acc: top1 ->  98.47200000732421 ; top5 ->  99.994  and loss:  5.333181358873844
test acc: top1 ->  90.05 ; top5 ->  98.99  and loss:  44.84785044193268
forward train acc: top1 ->  98.9620000024414 ; top5 ->  99.992  and loss:  3.3883690536022186
test acc: top1 ->  90.27 ; top5 ->  99.05  and loss:  46.81843760609627
forward train acc: top1 ->  99.23999997558593 ; top5 ->  99.99  and loss:  2.518304906785488
test acc: top1 ->  90.36 ; top5 ->  99.18  and loss:  49.70909096300602
forward train acc: top1 ->  99.34200000244141 ; top5 ->  99.994  and loss:  2.2235146425664425
test acc: top1 ->  90.58 ; top5 ->  99.23  and loss:  49.205385863780975
forward train acc: top1 ->  99.4120000024414 ; top5 ->  100.0  and loss:  1.8794309012591839
test acc: top1 ->  90.88 ; top5 ->  99.21  and loss:  48.514671728014946
forward train acc: top1 ->  99.498 ; top5 ->  100.0  and loss:  1.6637196792289615
test acc: top1 ->  91.0 ; top5 ->  99.24  and loss:  49.428320817649364
forward train acc: top1 ->  99.47800000488282 ; top5 ->  100.0  and loss:  1.5885313861072063
test acc: top1 ->  91.06 ; top5 ->  99.17  and loss:  49.916773930191994
forward train acc: top1 ->  99.60199997802735 ; top5 ->  99.996  and loss:  1.3609045948833227
test acc: top1 ->  91.01 ; top5 ->  99.24  and loss:  50.26733823120594
forward train acc: top1 ->  99.54800000244141 ; top5 ->  100.0  and loss:  1.389815155416727
test acc: top1 ->  91.04 ; top5 ->  99.25  and loss:  52.93671201169491
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  104 / 512 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -1883.222864151001 , diff:  1883.222864151001
adv train loss:  -2631.740249633789 , diff:  748.5173854827881
adv train loss:  -2956.4241771698 , diff:  324.68392753601074
adv train loss:  -3120.3451824188232 , diff:  163.92100524902344
adv train loss:  -3280.7456493377686 , diff:  160.4004669189453
adv train loss:  -3414.8285331726074 , diff:  134.08288383483887
adv train loss:  -3437.8263549804688 , diff:  22.997821807861328
adv train loss:  -3438.3868408203125 , diff:  0.56048583984375
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  3
test acc: top1 ->  21.46 ; top5 ->  83.22  and loss:  1880.7079639434814
forward train acc: top1 ->  97.42399998046875 ; top5 ->  99.974  and loss:  8.088835773989558
test acc: top1 ->  90.93 ; top5 ->  98.79  and loss:  49.78269764780998
forward train acc: top1 ->  99.56799997558593 ; top5 ->  99.996  and loss:  1.5370300728827715
test acc: top1 ->  91.18 ; top5 ->  98.93  and loss:  52.50920017063618
forward train acc: top1 ->  99.70799997558593 ; top5 ->  100.0  and loss:  1.0064421142451465
test acc: top1 ->  91.42 ; top5 ->  98.94  and loss:  53.64134921133518
forward train acc: top1 ->  99.77799997558594 ; top5 ->  100.0  and loss:  0.7951652742922306
test acc: top1 ->  91.31 ; top5 ->  98.88  and loss:  58.436488941311836
forward train acc: top1 ->  99.80999997558594 ; top5 ->  100.0  and loss:  0.5985043263062835
test acc: top1 ->  91.59 ; top5 ->  98.96  and loss:  56.13743720948696
forward train acc: top1 ->  99.868 ; top5 ->  100.0  and loss:  0.48980702622793615
test acc: top1 ->  91.61 ; top5 ->  99.04  and loss:  56.76773704588413
forward train acc: top1 ->  99.848 ; top5 ->  99.996  and loss:  0.48793625086545944
test acc: top1 ->  91.63 ; top5 ->  99.07  and loss:  55.86601039022207
forward train acc: top1 ->  99.852 ; top5 ->  100.0  and loss:  0.48774137487635016
test acc: top1 ->  91.56 ; top5 ->  98.95  and loss:  61.33391046524048
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  5 / 512 , inc:  2
---------------- start layer  9  ---------------
### skip layer  9 wait:  3  ###
---------------- start layer  10  ---------------
adv train loss:  -42.28222642838955 , diff:  42.28222642838955
adv train loss:  -42.42545402050018 , diff:  0.14322759211063385
adv train loss:  -42.07486715912819 , diff:  0.350586861371994
adv train loss:  -42.57156157493591 , diff:  0.496694415807724
adv train loss:  -43.009357273578644 , diff:  0.4377956986427307
adv train loss:  -42.54448655247688 , diff:  0.46487072110176086
adv train loss:  -126.72152304649353 , diff:  84.17703649401665
adv train loss:  -163.4000869989395 , diff:  36.678563952445984
adv train loss:  -163.00704073905945 , diff:  0.3930462598800659
adv train loss:  -162.99738585948944 , diff:  0.009654879570007324
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  21
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  263050.4094238281
forward train acc: top1 ->  88.41399997802735 ; top5 ->  97.84  and loss:  54.327204782515764
test acc: top1 ->  71.5 ; top5 ->  98.23  and loss:  90.64602154493332
forward train acc: top1 ->  99.4180000024414 ; top5 ->  99.996  and loss:  3.3629989586770535
test acc: top1 ->  90.75 ; top5 ->  99.08  and loss:  40.25243158638477
forward train acc: top1 ->  99.63400000244141 ; top5 ->  100.0  and loss:  1.724078955128789
test acc: top1 ->  90.95 ; top5 ->  99.25  and loss:  42.11909759789705
forward train acc: top1 ->  99.768 ; top5 ->  100.0  and loss:  1.01015970017761
test acc: top1 ->  91.52 ; top5 ->  99.27  and loss:  43.37280924618244
forward train acc: top1 ->  99.82399997558593 ; top5 ->  100.0  and loss:  0.7506904890760779
test acc: top1 ->  91.63 ; top5 ->  99.27  and loss:  45.13809098303318
forward train acc: top1 ->  99.82799997558594 ; top5 ->  100.0  and loss:  0.6398721770383418
test acc: top1 ->  91.46 ; top5 ->  99.3  and loss:  47.19196544587612
forward train acc: top1 ->  99.864 ; top5 ->  100.0  and loss:  0.5104916451964527
test acc: top1 ->  91.62 ; top5 ->  99.35  and loss:  47.33082699775696
forward train acc: top1 ->  99.90599997558594 ; top5 ->  100.0  and loss:  0.4291923991404474
test acc: top1 ->  91.77 ; top5 ->  99.34  and loss:  47.228060357272625
forward train acc: top1 ->  99.87199997558594 ; top5 ->  100.0  and loss:  0.46553725143894553
test acc: top1 ->  91.64 ; top5 ->  99.29  and loss:  48.10666051506996
forward train acc: top1 ->  99.8800000024414 ; top5 ->  100.0  and loss:  0.415020571090281
test acc: top1 ->  91.76 ; top5 ->  99.36  and loss:  48.35404680669308
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  22 / 512 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -2844.4531631469727 , diff:  2844.4531631469727
adv train loss:  -3188.027229309082 , diff:  343.5740661621094
adv train loss:  -3185.305316925049 , diff:  2.721912384033203
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  1
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  71999.1177368164
forward train acc: top1 ->  89.20799997802735 ; top5 ->  99.168  and loss:  47.809776198118925
test acc: top1 ->  50.19 ; top5 ->  95.25  and loss:  272.8192468881607
forward train acc: top1 ->  99.0160000024414 ; top5 ->  99.996  and loss:  4.638377029448748
test acc: top1 ->  89.41 ; top5 ->  97.67  and loss:  66.16471725702286
forward train acc: top1 ->  99.32400000244141 ; top5 ->  100.0  and loss:  3.0298130530864
test acc: top1 ->  89.89 ; top5 ->  97.7  and loss:  66.35609988868237
forward train acc: top1 ->  99.46999997558594 ; top5 ->  99.998  and loss:  2.2878146693110466
test acc: top1 ->  90.08 ; top5 ->  97.67  and loss:  66.79783755540848
forward train acc: top1 ->  99.5080000024414 ; top5 ->  99.998  and loss:  1.9877114221453667
test acc: top1 ->  90.21 ; top5 ->  97.68  and loss:  67.00383245944977
forward train acc: top1 ->  99.56200000488282 ; top5 ->  100.0  and loss:  1.7650406453758478
test acc: top1 ->  90.23 ; top5 ->  97.88  and loss:  67.51422445476055
forward train acc: top1 ->  99.658 ; top5 ->  100.0  and loss:  1.4861557921394706
test acc: top1 ->  90.29 ; top5 ->  97.68  and loss:  67.87467247247696
forward train acc: top1 ->  99.6180000024414 ; top5 ->  100.0  and loss:  1.5032677929848433
test acc: top1 ->  90.32 ; top5 ->  97.72  and loss:  67.96678067743778
forward train acc: top1 ->  99.64600000244141 ; top5 ->  99.998  and loss:  1.3654610710218549
test acc: top1 ->  90.36 ; top5 ->  97.7  and loss:  68.35733668506145
forward train acc: top1 ->  99.6620000024414 ; top5 ->  100.0  and loss:  1.3476712256669998
test acc: top1 ->  90.31 ; top5 ->  97.78  and loss:  68.14298690855503
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  2 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -255.4391561690718 , diff:  255.4391561690718
adv train loss:  -797.701901435852 , diff:  542.2627452667803
adv train loss:  -1201.807463645935 , diff:  404.105562210083
adv train loss:  -1258.2454566955566 , diff:  56.43799304962158
adv train loss:  -1570.7622652053833 , diff:  312.51680850982666
adv train loss:  -1640.7787809371948 , diff:  70.01651573181152
adv train loss:  -1639.049331665039 , diff:  1.7294492721557617
adv train loss:  -1640.3395261764526 , diff:  1.2901945114135742
adv train loss:  -1638.179485321045 , diff:  2.160040855407715
adv train loss:  -1642.1447076797485 , diff:  3.9652223587036133
layer  12  adv train finish, try to retain  5
test acc: top1 ->  10.88 ; top5 ->  52.93  and loss:  15061.109115600586
forward train acc: top1 ->  66.02400000244141 ; top5 ->  93.618  and loss:  254.57159574329853
test acc: top1 ->  88.33 ; top5 ->  98.62  and loss:  54.19024205207825
forward train acc: top1 ->  98.88399998291015 ; top5 ->  99.98799997558594  and loss:  13.698493361473083
test acc: top1 ->  89.97 ; top5 ->  98.83  and loss:  42.63736869394779
forward train acc: top1 ->  99.408 ; top5 ->  99.996  and loss:  7.178181018680334
test acc: top1 ->  90.77 ; top5 ->  98.82  and loss:  40.98027081787586
forward train acc: top1 ->  99.63199997558594 ; top5 ->  100.0  and loss:  4.2923817951232195
test acc: top1 ->  91.2 ; top5 ->  98.8  and loss:  40.494373470544815
forward train acc: top1 ->  99.75 ; top5 ->  100.0  and loss:  2.7914437912404537
test acc: top1 ->  91.26 ; top5 ->  98.77  and loss:  41.162489116191864
forward train acc: top1 ->  99.79 ; top5 ->  99.998  and loss:  2.1839172579348087
test acc: top1 ->  91.45 ; top5 ->  98.81  and loss:  41.604477211833
forward train acc: top1 ->  99.8400000024414 ; top5 ->  100.0  and loss:  1.7782777668908238
test acc: top1 ->  91.52 ; top5 ->  98.8  and loss:  42.13770120590925
forward train acc: top1 ->  99.824 ; top5 ->  100.0  and loss:  1.6339618479833007
test acc: top1 ->  91.42 ; top5 ->  98.78  and loss:  42.35565346479416
forward train acc: top1 ->  99.8520000024414 ; top5 ->  100.0  and loss:  1.4652322167530656
test acc: top1 ->  91.52 ; top5 ->  98.85  and loss:  43.26781367510557
forward train acc: top1 ->  99.86199997558593 ; top5 ->  100.0  and loss:  1.314361178316176
test acc: top1 ->  91.66 ; top5 ->  98.8  and loss:  43.35362756997347
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  41 / 512 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -19528.66000366211 , diff:  19528.66000366211
adv train loss:  -31213.208572387695 , diff:  11684.548568725586
adv train loss:  -46794.14431762695 , diff:  15580.935745239258
adv train loss:  -65096.55535888672 , diff:  18302.411041259766
adv train loss:  -81568.64141845703 , diff:  16472.086059570312
adv train loss:  -97320.47998046875 , diff:  15751.838562011719
adv train loss:  -112718.32019042969 , diff:  15397.840209960938
adv train loss:  -127699.41333007812 , diff:  14981.093139648438
adv train loss:  -141283.8104248047 , diff:  13584.397094726562
adv train loss:  -153265.04528808594 , diff:  11981.23486328125
layer  13  adv train finish, try to retain  20
>>>>>>> reverse layer  13  since no improvement >>>>>>>
layer  0  :  0.46875  ==>  30 / 64 , inc:  1
layer  1  :  0.75  ==>  48 / 64 , inc:  2
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.3828125  ==>  98 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.009765625  ==>  5 / 512 , inc:  1
layer  9  :  0.02734375  ==>  14 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.033203125  ==>  17 / 512 , inc:  1
eps [1.0521320122517643, 5.6113707320094095, 1.0521320122517643, 1.0521320122517643, 1.0521320122517643, 1.0521320122517643, 1.4028426830023524, 1.0521320122517643, 1.2469712737798688, 1.4028426830023524, 1.0521320122517643, 1.4028426830023524, 1.0521320122517643, 22.445482928037638]  wait [4, 2, 4, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 2]  inc [1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 8
$$$$$$$$$$$$$ epoch  71  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -3239.6818256378174 , diff:  3239.6818256378174
adv train loss:  -3248.52303314209 , diff:  8.841207504272461
adv train loss:  -3110.3443660736084 , diff:  138.17866706848145
adv train loss:  -3089.9923095703125 , diff:  20.3520565032959
adv train loss:  -3111.895071029663 , diff:  21.902761459350586
adv train loss:  -3106.2535495758057 , diff:  5.641521453857422
adv train loss:  -3100.6718978881836 , diff:  5.58165168762207
adv train loss:  -3107.135829925537 , diff:  6.463932037353516
adv train loss:  -3108.610189437866 , diff:  1.4743595123291016
layer  0  adv train finish, try to retain  54
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -1190.7754201889038 , diff:  1190.7754201889038
adv train loss:  -1190.3493204116821 , diff:  0.4260997772216797
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  46
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  3385.8900394439697
forward train acc: top1 ->  92.85999997802735 ; top5 ->  99.724  and loss:  64.13483246974647
test acc: top1 ->  91.27 ; top5 ->  99.11  and loss:  88.41601625084877
forward train acc: top1 ->  99.74599997558593 ; top5 ->  100.0  and loss:  0.9018125429283828
test acc: top1 ->  91.65 ; top5 ->  99.16  and loss:  84.83636048436165
forward train acc: top1 ->  99.79800000244141 ; top5 ->  99.998  and loss:  0.6951728295534849
test acc: top1 ->  91.78 ; top5 ->  99.21  and loss:  83.92889653146267
forward train acc: top1 ->  99.8500000024414 ; top5 ->  100.0  and loss:  0.4690138753503561
test acc: top1 ->  91.95 ; top5 ->  99.13  and loss:  83.1614460349083
forward train acc: top1 ->  99.862 ; top5 ->  100.0  and loss:  0.4552850322215818
test acc: top1 ->  91.85 ; top5 ->  99.14  and loss:  82.38763375580311
forward train acc: top1 ->  99.88599997558593 ; top5 ->  99.998  and loss:  0.4178269952535629
test acc: top1 ->  91.89 ; top5 ->  99.18  and loss:  81.45716813206673
forward train acc: top1 ->  99.884 ; top5 ->  100.0  and loss:  0.33791282630409114
test acc: top1 ->  91.78 ; top5 ->  99.22  and loss:  82.38927344977856
forward train acc: top1 ->  99.89 ; top5 ->  100.0  and loss:  0.3454441099893302
test acc: top1 ->  91.88 ; top5 ->  99.23  and loss:  81.49285803735256
forward train acc: top1 ->  99.894 ; top5 ->  100.0  and loss:  0.3403533636883367
test acc: top1 ->  91.96 ; top5 ->  99.17  and loss:  81.3811863064766
forward train acc: top1 ->  99.858 ; top5 ->  100.0  and loss:  0.4528704209951684
test acc: top1 ->  91.7 ; top5 ->  99.25  and loss:  80.29483807086945
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  48 / 64 , inc:  2
---------------- start layer  2  ---------------
adv train loss:  -150.02442025393248 , diff:  150.02442025393248
adv train loss:  -775.3165102005005 , diff:  625.292089946568
adv train loss:  -916.034245967865 , diff:  140.7177357673645
adv train loss:  -982.0234060287476 , diff:  65.98916006088257
adv train loss:  -982.9609718322754 , diff:  0.937565803527832
adv train loss:  -977.7939147949219 , diff:  5.167057037353516
adv train loss:  -990.9697046279907 , diff:  13.175789833068848
adv train loss:  -981.3287515640259 , diff:  9.640953063964844
adv train loss:  -990.3065309524536 , diff:  8.977779388427734
adv train loss:  -982.6357097625732 , diff:  7.670821189880371
layer  2  adv train finish, try to retain  123
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -1324.9189455211163 , diff:  1324.9189455211163
adv train loss:  -2412.4817085266113 , diff:  1087.562763005495
adv train loss:  -2451.5992336273193 , diff:  39.11752510070801
adv train loss:  -2467.1474800109863 , diff:  15.548246383666992
adv train loss:  -2463.1576652526855 , diff:  3.9898147583007812
adv train loss:  -2472.1312103271484 , diff:  8.97354507446289
adv train loss:  -2506.690361022949 , diff:  34.55915069580078
adv train loss:  -2509.5722103118896 , diff:  2.8818492889404297
adv train loss:  -2514.201629638672 , diff:  4.629419326782227
adv train loss:  -2512.296817779541 , diff:  1.9048118591308594
layer  3  adv train finish, try to retain  118
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -567.7395543884486 , diff:  567.7395543884486
adv train loss:  -2529.708875656128 , diff:  1961.9693212676793
adv train loss:  -2883.4945335388184 , diff:  353.78565788269043
adv train loss:  -3061.2678756713867 , diff:  177.77334213256836
adv train loss:  -3108.9722480773926 , diff:  47.70437240600586
adv train loss:  -3190.0613498687744 , diff:  81.08910179138184
adv train loss:  -3219.5480995178223 , diff:  29.48674964904785
adv train loss:  -3271.935218811035 , diff:  52.38711929321289
adv train loss:  -3291.5928497314453 , diff:  19.657630920410156
adv train loss:  -3293.2864685058594 , diff:  1.6936187744140625
layer  4  adv train finish, try to retain  204
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -1041.5410345103592 , diff:  1041.5410345103592
adv train loss:  -3296.964515686035 , diff:  2255.423481175676
adv train loss:  -3343.5421237945557 , diff:  46.57760810852051
adv train loss:  -3409.4030952453613 , diff:  65.86097145080566
adv train loss:  -3504.502758026123 , diff:  95.09966278076172
adv train loss:  -3587.142921447754 , diff:  82.64016342163086
adv train loss:  -3647.8797645568848 , diff:  60.73684310913086
adv train loss:  -3707.5411338806152 , diff:  59.66136932373047
adv train loss:  -3723.1091232299805 , diff:  15.567989349365234
adv train loss:  -3714.561050415039 , diff:  8.548072814941406
layer  5  adv train finish, try to retain  217
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -1098.8786551412195 , diff:  1098.8786551412195
adv train loss:  -3395.6656761169434 , diff:  2296.787020975724
adv train loss:  -3423.102882385254 , diff:  27.437206268310547
adv train loss:  -3431.597421646118 , diff:  8.494539260864258
adv train loss:  -3466.7919425964355 , diff:  35.19452095031738
adv train loss:  -3498.966167449951 , diff:  32.174224853515625
adv train loss:  -3606.285701751709 , diff:  107.31953430175781
adv train loss:  -3606.868824005127 , diff:  0.5831222534179688
adv train loss:  -3612.4715003967285 , diff:  5.6026763916015625
adv train loss:  -3598.937099456787 , diff:  13.534400939941406
layer  6  adv train finish, try to retain  12
test acc: top1 ->  45.32 ; top5 ->  81.06  and loss:  1358.456350326538
forward train acc: top1 ->  86.24599997314454 ; top5 ->  99.422  and loss:  58.20959988236427
test acc: top1 ->  81.87 ; top5 ->  98.66  and loss:  72.56968057155609
forward train acc: top1 ->  90.20000001708985 ; top5 ->  99.744  and loss:  28.36689965426922
test acc: top1 ->  84.22 ; top5 ->  98.89  and loss:  61.548483699560165
forward train acc: top1 ->  91.6539999975586 ; top5 ->  99.834  and loss:  23.753395304083824
test acc: top1 ->  84.34 ; top5 ->  99.02  and loss:  60.037538439035416
forward train acc: top1 ->  92.64600000976563 ; top5 ->  99.878  and loss:  20.916619390249252
test acc: top1 ->  85.2 ; top5 ->  99.05  and loss:  57.5634123980999
forward train acc: top1 ->  93.59999997802734 ; top5 ->  99.88999997558594  and loss:  18.31355446577072
test acc: top1 ->  85.9 ; top5 ->  99.12  and loss:  55.02233311533928
forward train acc: top1 ->  93.862 ; top5 ->  99.904  and loss:  17.49061605334282
test acc: top1 ->  86.16 ; top5 ->  99.22  and loss:  53.813012808561325
forward train acc: top1 ->  94.04600000732422 ; top5 ->  99.89199997558593  and loss:  16.893998578190804
test acc: top1 ->  86.4 ; top5 ->  99.21  and loss:  53.12274643778801
forward train acc: top1 ->  94.50600001953126 ; top5 ->  99.92  and loss:  15.654181085526943
test acc: top1 ->  86.93 ; top5 ->  99.2  and loss:  51.33333870768547
forward train acc: top1 ->  94.75200001708984 ; top5 ->  99.922  and loss:  15.067150421440601
test acc: top1 ->  86.76 ; top5 ->  99.25  and loss:  52.5291893184185
forward train acc: top1 ->  94.88000000488282 ; top5 ->  99.918  and loss:  14.603598050773144
test acc: top1 ->  86.87 ; top5 ->  99.23  and loss:  52.33484151959419
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  98 / 256 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -2.224977068603039 , diff:  2.224977068603039
adv train loss:  -2.110503900796175 , diff:  0.11447316780686378
adv train loss:  -2.0537562165409327 , diff:  0.05674768425524235
adv train loss:  -2.1821685191243887 , diff:  0.12841230258345604
adv train loss:  -2.1228630170226097 , diff:  0.059305502101778984
adv train loss:  -2.2093998454511166 , diff:  0.08653682842850685
adv train loss:  -2.0992896892130375 , diff:  0.11011015623807907
adv train loss:  -2.1289937775582075 , diff:  0.02970408834517002
adv train loss:  -2.323607875034213 , diff:  0.19461409747600555
adv train loss:  -2.0689823906868696 , diff:  0.25462548434734344
layer  7  adv train finish, try to retain  402
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -1078.3515014648438 , diff:  1078.3515014648438
adv train loss:  -1077.5229120254517 , diff:  0.8285894393920898
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  4
test acc: top1 ->  24.31 ; top5 ->  79.74  and loss:  3828.5673484802246
forward train acc: top1 ->  94.82000000488281 ; top5 ->  99.922  and loss:  19.113965567201376
test acc: top1 ->  90.66 ; top5 ->  98.52  and loss:  48.272460505366325
forward train acc: top1 ->  99.47399997558594 ; top5 ->  100.0  and loss:  1.8967681853100657
test acc: top1 ->  91.19 ; top5 ->  98.63  and loss:  51.98510883003473
forward train acc: top1 ->  99.686 ; top5 ->  99.998  and loss:  1.1501390347257257
test acc: top1 ->  91.38 ; top5 ->  98.76  and loss:  54.14015328884125
forward train acc: top1 ->  99.81 ; top5 ->  100.0  and loss:  0.7346422206610441
test acc: top1 ->  91.57 ; top5 ->  98.79  and loss:  57.26183918863535
forward train acc: top1 ->  99.814 ; top5 ->  100.0  and loss:  0.6800411487929523
test acc: top1 ->  91.53 ; top5 ->  98.88  and loss:  59.377974309027195
forward train acc: top1 ->  99.852 ; top5 ->  100.0  and loss:  0.5288853906095028
test acc: top1 ->  91.6 ; top5 ->  98.87  and loss:  59.73451428115368
forward train acc: top1 ->  99.854 ; top5 ->  100.0  and loss:  0.4801348045002669
test acc: top1 ->  91.74 ; top5 ->  98.83  and loss:  60.098804853856564
forward train acc: top1 ->  99.842 ; top5 ->  100.0  and loss:  0.4588267389917746
test acc: top1 ->  91.64 ; top5 ->  98.84  and loss:  60.310084000229836
forward train acc: top1 ->  99.87 ; top5 ->  100.0  and loss:  0.44801660894881934
test acc: top1 ->  91.7 ; top5 ->  98.92  and loss:  61.52402423322201
forward train acc: top1 ->  99.86799997558593 ; top5 ->  100.0  and loss:  0.394947312772274
test acc: top1 ->  91.69 ; top5 ->  98.83  and loss:  62.71969736367464
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  5 / 512 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -1142.5551385879517 , diff:  1142.5551385879517
adv train loss:  -1838.6270771026611 , diff:  696.0719385147095
adv train loss:  -2264.694761276245 , diff:  426.067684173584
adv train loss:  -2390.955520629883 , diff:  126.2607593536377
adv train loss:  -2410.823989868164 , diff:  19.86846923828125
adv train loss:  -2408.8651390075684 , diff:  1.9588508605957031
adv train loss:  -2408.918134689331 , diff:  0.05299568176269531
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  13
test acc: top1 ->  10.0 ; top5 ->  59.1  and loss:  20687.003509521484
forward train acc: top1 ->  98.0040000024414 ; top5 ->  99.996  and loss:  7.481384190730751
test acc: top1 ->  75.39 ; top5 ->  98.65  and loss:  150.6429488658905
forward train acc: top1 ->  99.804 ; top5 ->  100.0  and loss:  0.6442892495542765
test acc: top1 ->  91.49 ; top5 ->  99.25  and loss:  57.10124696046114
forward train acc: top1 ->  99.848 ; top5 ->  100.0  and loss:  0.5104504628106952
test acc: top1 ->  91.5 ; top5 ->  99.21  and loss:  57.98091485723853
forward train acc: top1 ->  99.92 ; top5 ->  100.0  and loss:  0.36043672636151314
test acc: top1 ->  91.7 ; top5 ->  99.27  and loss:  58.45688886195421
forward train acc: top1 ->  99.88 ; top5 ->  100.0  and loss:  0.3708376588183455
test acc: top1 ->  91.78 ; top5 ->  99.26  and loss:  59.87181682139635
forward train acc: top1 ->  99.90999997558593 ; top5 ->  100.0  and loss:  0.3061465681530535
test acc: top1 ->  91.77 ; top5 ->  99.25  and loss:  59.70716383308172
forward train acc: top1 ->  99.912 ; top5 ->  100.0  and loss:  0.30610324116423726
test acc: top1 ->  91.73 ; top5 ->  99.29  and loss:  60.54719044268131
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  14 / 512 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -81.1201511323452 , diff:  81.1201511323452
adv train loss:  -80.4340868294239 , diff:  0.6860643029212952
adv train loss:  -80.09962257742882 , diff:  0.33446425199508667
adv train loss:  -81.36318442225456 , diff:  1.2635618448257446
adv train loss:  -80.91072353720665 , diff:  0.4524608850479126
adv train loss:  -80.69989341497421 , diff:  0.21083012223243713
adv train loss:  -79.44555217027664 , diff:  1.2543412446975708
adv train loss:  -310.1901145577431 , diff:  230.74456238746643
adv train loss:  -336.2523834705353 , diff:  26.062268912792206
adv train loss:  -369.9557707309723 , diff:  33.70338726043701
layer  10  adv train finish, try to retain  482
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -1840.544038772583 , diff:  1840.544038772583
adv train loss:  -2399.163736343384 , diff:  558.6196975708008
adv train loss:  -2811.579486846924 , diff:  412.41575050354004
adv train loss:  -2812.3158798217773 , diff:  0.7363929748535156
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  1
test acc: top1 ->  10.0 ; top5 ->  50.0  and loss:  23942.67820739746
forward train acc: top1 ->  87.92600000488281 ; top5 ->  99.064  and loss:  49.34738317877054
test acc: top1 ->  51.81 ; top5 ->  97.19  and loss:  162.31905269622803
forward train acc: top1 ->  99.356 ; top5 ->  100.0  and loss:  5.3326337318867445
test acc: top1 ->  90.52 ; top5 ->  99.32  and loss:  40.77113078534603
forward train acc: top1 ->  99.67599997558594 ; top5 ->  100.0  and loss:  2.2329276464879513
test acc: top1 ->  90.89 ; top5 ->  99.34  and loss:  44.797701209783554
forward train acc: top1 ->  99.76199997558594 ; top5 ->  100.0  and loss:  1.415747955441475
test acc: top1 ->  90.96 ; top5 ->  99.26  and loss:  47.46976985037327
forward train acc: top1 ->  99.766 ; top5 ->  100.0  and loss:  1.0117490612901747
test acc: top1 ->  91.07 ; top5 ->  99.25  and loss:  49.004496708512306
forward train acc: top1 ->  99.79199997558594 ; top5 ->  100.0  and loss:  0.9226825907826424
test acc: top1 ->  91.07 ; top5 ->  99.25  and loss:  49.84674556553364
forward train acc: top1 ->  99.78999997558594 ; top5 ->  99.998  and loss:  0.9104673312976956
test acc: top1 ->  91.26 ; top5 ->  99.22  and loss:  50.41896712034941
forward train acc: top1 ->  99.824 ; top5 ->  100.0  and loss:  0.7593866302631795
test acc: top1 ->  91.32 ; top5 ->  99.23  and loss:  50.899792924523354
forward train acc: top1 ->  99.836 ; top5 ->  100.0  and loss:  0.6887095782440156
test acc: top1 ->  91.31 ; top5 ->  99.16  and loss:  52.568185701966286
forward train acc: top1 ->  99.86 ; top5 ->  100.0  and loss:  0.626394216902554
test acc: top1 ->  91.41 ; top5 ->  99.19  and loss:  51.08644989132881
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  2 / 512 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -1383.162636756897 , diff:  1383.162636756897
adv train loss:  -1694.555302619934 , diff:  311.3926658630371
adv train loss:  -1922.2024211883545 , diff:  227.6471185684204
adv train loss:  -2263.0630111694336 , diff:  340.8605899810791
adv train loss:  -2312.2871074676514 , diff:  49.22409629821777
adv train loss:  -2334.3529262542725 , diff:  22.065818786621094
adv train loss:  -2448.136148452759 , diff:  113.78322219848633
adv train loss:  -2453.7658252716064 , diff:  5.629676818847656
adv train loss:  -2455.8648071289062 , diff:  2.0989818572998047
adv train loss:  -2457.040599822998 , diff:  1.1757926940917969
layer  12  adv train finish, try to retain  484
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -16460.878578186035 , diff:  16460.878578186035
adv train loss:  -28160.0565032959 , diff:  11699.177925109863
adv train loss:  -40394.299743652344 , diff:  12234.243240356445
adv train loss:  -58120.131408691406 , diff:  17725.831665039062
adv train loss:  -74843.56896972656 , diff:  16723.437561035156
adv train loss:  -90263.49700927734 , diff:  15419.928039550781
adv train loss:  -105116.12438964844 , diff:  14852.627380371094
adv train loss:  -119603.69482421875 , diff:  14487.570434570312
adv train loss:  -133871.34899902344 , diff:  14267.654174804688
adv train loss:  -147933.58557128906 , diff:  14062.236572265625
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  16
test acc: top1 ->  29.65 ; top5 ->  78.37  and loss:  1835.782202720642
forward train acc: top1 ->  84.66400000244141 ; top5 ->  98.644  and loss:  163.87120310775936
test acc: top1 ->  90.48 ; top5 ->  98.49  and loss:  67.72149240970612
forward train acc: top1 ->  99.64399997558594 ; top5 ->  99.998  and loss:  1.7162917563691735
test acc: top1 ->  90.91 ; top5 ->  98.55  and loss:  66.2045343965292
forward train acc: top1 ->  99.73399997558593 ; top5 ->  100.0  and loss:  1.1513033881783485
test acc: top1 ->  91.13 ; top5 ->  98.69  and loss:  65.6295617967844
forward train acc: top1 ->  99.83999997558594 ; top5 ->  100.0  and loss:  0.775180047377944
test acc: top1 ->  91.35 ; top5 ->  98.67  and loss:  64.76164026558399
forward train acc: top1 ->  99.814 ; top5 ->  99.998  and loss:  0.7296352055855095
test acc: top1 ->  91.4 ; top5 ->  98.72  and loss:  64.92428151518106
forward train acc: top1 ->  99.86199997558593 ; top5 ->  100.0  and loss:  0.5386685375124216
test acc: top1 ->  91.46 ; top5 ->  98.75  and loss:  64.55312000960112
forward train acc: top1 ->  99.878 ; top5 ->  99.998  and loss:  0.497410821961239
test acc: top1 ->  91.53 ; top5 ->  98.73  and loss:  64.4237646535039
forward train acc: top1 ->  99.908 ; top5 ->  99.998  and loss:  0.43588206451386213
test acc: top1 ->  91.59 ; top5 ->  98.73  and loss:  64.69678095728159
forward train acc: top1 ->  99.88599997558593 ; top5 ->  99.998  and loss:  0.48821557965129614
test acc: top1 ->  91.57 ; top5 ->  98.73  and loss:  64.61747044324875
forward train acc: top1 ->  99.91 ; top5 ->  100.0  and loss:  0.3979180606547743
test acc: top1 ->  91.55 ; top5 ->  98.69  and loss:  64.79563396424055
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  17 / 512 , inc:  1
layer  0  :  0.46875  ==>  30 / 64 , inc:  1
layer  1  :  0.75  ==>  48 / 64 , inc:  1
layer  2  :  0.78125  ==>  100 / 128 , inc:  1
layer  3  :  0.6796875  ==>  87 / 128 , inc:  1
layer  4  :  0.609375  ==>  156 / 256 , inc:  1
layer  5  :  0.58203125  ==>  149 / 256 , inc:  1
layer  6  :  0.3828125  ==>  98 / 256 , inc:  1
layer  7  :  0.203125  ==>  104 / 512 , inc:  1
layer  8  :  0.009765625  ==>  5 / 512 , inc:  1
layer  9  :  0.02734375  ==>  14 / 512 , inc:  1
layer  10  :  0.04296875  ==>  22 / 512 , inc:  1
layer  11  :  0.00390625  ==>  2 / 512 , inc:  1
layer  12  :  0.080078125  ==>  41 / 512 , inc:  1
layer  13  :  0.033203125  ==>  17 / 512 , inc:  1
eps [2.1042640245035287, 4.208528049007057, 2.1042640245035287, 2.1042640245035287, 2.1042640245035287, 2.1042640245035287, 1.0521320122517643, 2.1042640245035287, 0.9352284553349016, 1.0521320122517643, 2.1042640245035287, 1.0521320122517643, 2.1042640245035287, 16.83411219602823]  wait [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 9
$$$$$$$$$$$$$ epoch  72  $$$$$$$$$$$$
-------------- post train ------------
forward train acc: top1 ->  96.148 ; top5 ->  99.978  and loss:  25.185500502586365
test acc: top1 ->  90.97 ; top5 ->  99.26  and loss:  90.65561458468437
forward train acc: top1 ->  99.752 ; top5 ->  100.0  and loss:  0.8118940708809532
test acc: top1 ->  91.52 ; top5 ->  99.28  and loss:  83.34441593289375
forward train acc: top1 ->  99.87399997558593 ; top5 ->  100.0  and loss:  0.4340443667024374
test acc: top1 ->  91.71 ; top5 ->  99.32  and loss:  81.53836132586002
forward train acc: top1 ->  99.91599997558593 ; top5 ->  100.0  and loss:  0.2662281822413206
test acc: top1 ->  91.98 ; top5 ->  99.27  and loss:  78.94328248500824
forward train acc: top1 ->  99.91399997558594 ; top5 ->  100.0  and loss:  0.26372165279462934
test acc: top1 ->  92.0 ; top5 ->  99.28  and loss:  78.959018856287
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.1941402822267264
test acc: top1 ->  91.98 ; top5 ->  99.26  and loss:  78.67481707036495
forward train acc: top1 ->  99.934 ; top5 ->  100.0  and loss:  0.17011660621210467
test acc: top1 ->  92.04 ; top5 ->  99.24  and loss:  78.98751485347748
forward train acc: top1 ->  99.926 ; top5 ->  100.0  and loss:  0.20785047211393248
test acc: top1 ->  92.22 ; top5 ->  99.26  and loss:  78.35089790821075
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.1682166888494976
test acc: top1 ->  92.13 ; top5 ->  99.25  and loss:  78.54003097116947
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.12776549812406301
test acc: top1 ->  92.14 ; top5 ->  99.32  and loss:  78.33878608047962
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.16358835161372554
test acc: top1 ->  92.12 ; top5 ->  99.28  and loss:  78.46124795079231
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.1103230735170655
test acc: top1 ->  92.26 ; top5 ->  99.32  and loss:  77.99178721010685
forward train acc: top1 ->  99.958 ; top5 ->  100.0  and loss:  0.15600302549137268
test acc: top1 ->  92.16 ; top5 ->  99.32  and loss:  78.38202333450317
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.16778301199337875
test acc: top1 ->  92.25 ; top5 ->  99.28  and loss:  77.95043434202671
forward train acc: top1 ->  99.94 ; top5 ->  100.0  and loss:  0.18555431196000427
test acc: top1 ->  92.24 ; top5 ->  99.33  and loss:  77.77657869458199
forward train acc: top1 ->  99.95 ; top5 ->  100.0  and loss:  0.12108721238109865
test acc: top1 ->  92.36 ; top5 ->  99.26  and loss:  78.01301181316376
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.09670039461525448
test acc: top1 ->  92.19 ; top5 ->  99.28  and loss:  77.93255741894245
forward train acc: top1 ->  99.95799997558593 ; top5 ->  100.0  and loss:  0.12560620810836554
test acc: top1 ->  92.3 ; top5 ->  99.32  and loss:  77.93933056294918
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.09632913768291473
test acc: top1 ->  92.28 ; top5 ->  99.32  and loss:  78.15932765603065
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.13448978553060442
test acc: top1 ->  92.29 ; top5 ->  99.3  and loss:  78.57315874099731
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.10664314619498327
test acc: top1 ->  92.21 ; top5 ->  99.31  and loss:  77.60906721651554
forward train acc: top1 ->  99.96799997558594 ; top5 ->  100.0  and loss:  0.09915583953261375
test acc: top1 ->  92.34 ; top5 ->  99.33  and loss:  77.98214915394783
forward train acc: top1 ->  99.974 ; top5 ->  100.0  and loss:  0.07918677502311766
test acc: top1 ->  92.2 ; top5 ->  99.32  and loss:  78.7496450394392
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.09295183891663328
test acc: top1 ->  92.31 ; top5 ->  99.27  and loss:  78.36979606747627
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.06648295017657802
test acc: top1 ->  92.31 ; top5 ->  99.31  and loss:  78.17929865419865
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.10705011630489025
test acc: top1 ->  92.23 ; top5 ->  99.33  and loss:  78.2968775331974
forward train acc: top1 ->  99.95999997558594 ; top5 ->  100.0  and loss:  0.14960462460294366
test acc: top1 ->  92.25 ; top5 ->  99.37  and loss:  78.54281198978424
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.13769169674924342
test acc: top1 ->  92.33 ; top5 ->  99.33  and loss:  77.98353165388107
forward train acc: top1 ->  99.93 ; top5 ->  100.0  and loss:  0.17537423648900585
test acc: top1 ->  92.29 ; top5 ->  99.36  and loss:  78.02508443593979
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.08025444005033933
test acc: top1 ->  92.35 ; top5 ->  99.36  and loss:  78.1469351798296
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.09066979947965592
test acc: top1 ->  92.39 ; top5 ->  99.29  and loss:  77.97512613236904
forward train acc: top1 ->  99.97799997558593 ; top5 ->  100.0  and loss:  0.10695442976430058
test acc: top1 ->  92.35 ; top5 ->  99.33  and loss:  77.93949404358864
forward train acc: top1 ->  99.96 ; top5 ->  100.0  and loss:  0.12123459001304582
test acc: top1 ->  92.26 ; top5 ->  99.33  and loss:  78.44519051909447
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.14030241768341511
test acc: top1 ->  92.33 ; top5 ->  99.35  and loss:  78.26181007921696
forward train acc: top1 ->  99.962 ; top5 ->  100.0  and loss:  0.114138370990986
test acc: top1 ->  92.34 ; top5 ->  99.35  and loss:  77.32842062413692
forward train acc: top1 ->  99.976 ; top5 ->  100.0  and loss:  0.05944113877194468
test acc: top1 ->  92.28 ; top5 ->  99.36  and loss:  77.8570183813572
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.09287625054457749
test acc: top1 ->  92.26 ; top5 ->  99.39  and loss:  77.8625732511282
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.10941589907451998
test acc: top1 ->  92.33 ; top5 ->  99.32  and loss:  78.13859662413597
forward train acc: top1 ->  99.96599997558594 ; top5 ->  100.0  and loss:  0.10227404464967549
test acc: top1 ->  92.19 ; top5 ->  99.34  and loss:  78.04247823357582
forward train acc: top1 ->  99.948 ; top5 ->  100.0  and loss:  0.15264026791555807
test acc: top1 ->  92.26 ; top5 ->  99.36  and loss:  77.8939508497715
forward train acc: top1 ->  99.968 ; top5 ->  100.0  and loss:  0.10056343098403886
test acc: top1 ->  92.19 ; top5 ->  99.37  and loss:  78.0709014236927
forward train acc: top1 ->  99.954 ; top5 ->  100.0  and loss:  0.12196294358000159
test acc: top1 ->  92.27 ; top5 ->  99.34  and loss:  77.64725610613823
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.09366200433578342
test acc: top1 ->  92.34 ; top5 ->  99.36  and loss:  77.48435926437378
forward train acc: top1 ->  99.966 ; top5 ->  100.0  and loss:  0.09886211808770895
test acc: top1 ->  92.39 ; top5 ->  99.38  and loss:  78.01593807339668
forward train acc: top1 ->  99.972 ; top5 ->  100.0  and loss:  0.0814803154207766
test acc: top1 ->  92.28 ; top5 ->  99.35  and loss:  78.01735183596611
forward train acc: top1 ->  99.97 ; top5 ->  100.0  and loss:  0.11507983540650457
test acc: top1 ->  92.29 ; top5 ->  99.33  and loss:  78.30524192750454
forward train acc: top1 ->  99.95399997558594 ; top5 ->  100.0  and loss:  0.10586858028545976
test acc: top1 ->  92.25 ; top5 ->  99.37  and loss:  78.30146719515324
forward train acc: top1 ->  99.96599997558594 ; top5 ->  100.0  and loss:  0.10863890242762864
test acc: top1 ->  92.41 ; top5 ->  99.34  and loss:  78.00765912234783
forward train acc: top1 ->  99.982 ; top5 ->  100.0  and loss:  0.06225092265231069
test acc: top1 ->  92.22 ; top5 ->  99.32  and loss:  77.9800737798214
forward train acc: top1 ->  99.964 ; top5 ->  100.0  and loss:  0.10170273686526343
test acc: top1 ->  92.37 ; top5 ->  99.35  and loss:  77.69103942811489
------------- sparsity -----------
layer  0  :  0.46875  ==>  30 / 64
layer  1  :  0.75  ==>  48 / 64
layer  2  :  0.78125  ==>  100 / 128
layer  3  :  0.6796875  ==>  87 / 128
layer  4  :  0.609375  ==>  156 / 256
layer  5  :  0.58203125  ==>  149 / 256
layer  6  :  0.3828125  ==>  98 / 256
layer  7  :  0.203125  ==>  104 / 512
layer  8  :  0.009765625  ==>  5 / 512
layer  9  :  0.02734375  ==>  14 / 512
layer  10  :  0.04296875  ==>  22 / 512
layer  11  :  0.00390625  ==>  2 / 512
layer  12  :  0.080078125  ==>  41 / 512
layer  13  :  0.033203125  ==>  17 / 512
