DataParallel(
  (module): RESNET_BASIC_Mask(
    (net): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer3): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (4): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (5): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer4): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): BasicBlock(
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)
      (fc): Linear(in_features=512, out_features=1000, bias=True)
    )
    (mask): ParameterList(
        (0): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (1): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (2): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (3): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (4): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (5): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (6): Parameter containing: [torch.cuda.FloatTensor of size 128 (GPU 0)]
        (7): Parameter containing: [torch.cuda.FloatTensor of size 128 (GPU 0)]
        (8): Parameter containing: [torch.cuda.FloatTensor of size 128 (GPU 0)]
        (9): Parameter containing: [torch.cuda.FloatTensor of size 128 (GPU 0)]
        (10): Parameter containing: [torch.cuda.FloatTensor of size 128 (GPU 0)]
        (11): Parameter containing: [torch.cuda.FloatTensor of size 128 (GPU 0)]
        (12): Parameter containing: [torch.cuda.FloatTensor of size 128 (GPU 0)]
        (13): Parameter containing: [torch.cuda.FloatTensor of size 128 (GPU 0)]
        (14): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
        (15): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
        (16): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
        (17): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
        (18): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
        (19): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
        (20): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
        (21): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
        (22): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
        (23): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
        (24): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
        (25): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
        (26): Parameter containing: [torch.cuda.FloatTensor of size 512 (GPU 0)]
        (27): Parameter containing: [torch.cuda.FloatTensor of size 512 (GPU 0)]
        (28): Parameter containing: [torch.cuda.FloatTensor of size 512 (GPU 0)]
        (29): Parameter containing: [torch.cuda.FloatTensor of size 512 (GPU 0)]
        (30): Parameter containing: [torch.cuda.FloatTensor of size 512 (GPU 0)]
        (31): Parameter containing: [torch.cuda.FloatTensor of size 512 (GPU 0)]
      
    )
  )
)
eps:  [0.00015625, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 3.90625e-05, 3.90625e-05, 3.90625e-05, 3.90625e-05, 3.90625e-05, 3.90625e-05, 3.90625e-05, 3.90625e-05, 3.90625e-05, 3.90625e-05, 3.90625e-05, 3.90625e-05, 1.953125e-05, 1.953125e-05, 1.953125e-05, 1.953125e-05, 1.953125e-05, 1.953125e-05]
$$$$$$$$$$$$$ epoch  0  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -55.39239639043808 , diff:  55.39239639043808
adv train loss:  -56.37132662534714 , diff:  0.9789302349090576
layer  0  adv train finish, try to retain  62
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -54.91570687294006 , diff:  54.91570687294006
adv train loss:  -59.94603204727173 , diff:  5.030325174331665
adv train loss:  -55.48278683423996 , diff:  4.463245213031769
adv train loss:  -62.4895002245903 , diff:  7.006713390350342
adv train loss:  -59.55671525001526 , diff:  2.9327849745750427
adv train loss:  -56.95224380493164 , diff:  2.604471445083618
adv train loss:  -60.68692708015442 , diff:  3.7346832752227783
adv train loss:  -61.89064484834671 , diff:  1.2037177681922913
adv train loss:  -59.40227645635605 , diff:  2.4883683919906616
adv train loss:  -59.47057098150253 , diff:  0.06829452514648438
layer  1  adv train finish, try to retain  61
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -55.56353163719177 , diff:  55.56353163719177
adv train loss:  -53.92695188522339 , diff:  1.6365797519683838
adv train loss:  -61.21566045284271 , diff:  7.288708567619324
adv train loss:  -54.62671321630478 , diff:  6.588947236537933
adv train loss:  -58.44793111085892 , diff:  3.821217894554138
adv train loss:  -57.4768807888031 , diff:  0.9710503220558167
layer  2  adv train finish, try to retain  63
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -54.61105352640152 , diff:  54.61105352640152
adv train loss:  -61.57609272003174 , diff:  6.9650391936302185
adv train loss:  -59.012168765068054 , diff:  2.563923954963684
adv train loss:  -58.147232830524445 , diff:  0.8649359345436096
layer  3  adv train finish, try to retain  59
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -58.14644932746887 , diff:  58.14644932746887
adv train loss:  -55.08847272396088 , diff:  3.0579766035079956
adv train loss:  -61.68987292051315 , diff:  6.601400196552277
adv train loss:  -56.61085331439972 , diff:  5.079019606113434
adv train loss:  -59.02498823404312 , diff:  2.414134919643402
adv train loss:  -55.77072262763977 , diff:  3.254265606403351
adv train loss:  -53.12570810317993 , diff:  2.645014524459839
adv train loss:  -57.01701468229294 , diff:  3.8913065791130066
adv train loss:  -55.789267897605896 , diff:  1.2277467846870422
adv train loss:  -56.88550090789795 , diff:  1.0962330102920532
layer  4  adv train finish, try to retain  62
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -58.50549274682999 , diff:  58.50549274682999
adv train loss:  -56.110601007938385 , diff:  2.3948917388916016
adv train loss:  -58.32957345247269 , diff:  2.2189724445343018
adv train loss:  -57.97788739204407 , diff:  0.3516860604286194
layer  5  adv train finish, try to retain  62
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -57.83114320039749 , diff:  57.83114320039749
adv train loss:  -55.59946149587631 , diff:  2.231681704521179
adv train loss:  -57.25415372848511 , diff:  1.6546922326087952
adv train loss:  -54.84229236841202 , diff:  2.4118613600730896
adv train loss:  -59.12996280193329 , diff:  4.287670433521271
adv train loss:  -58.29648005962372 , diff:  0.8334827423095703
layer  6  adv train finish, try to retain  126
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -57.84961950778961 , diff:  57.84961950778961
adv train loss:  -56.45868915319443 , diff:  1.3909303545951843
adv train loss:  -54.30479449033737 , diff:  2.1538946628570557
adv train loss:  -54.16413205862045 , diff:  0.14066243171691895
layer  7  adv train finish, try to retain  127
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -56.499591648578644 , diff:  56.499591648578644
adv train loss:  -57.21745789051056 , diff:  0.7178662419319153
layer  8  adv train finish, try to retain  121
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -58.25912833213806 , diff:  58.25912833213806
adv train loss:  -58.63250112533569 , diff:  0.37337279319763184
layer  9  adv train finish, try to retain  126
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -54.00145447254181 , diff:  54.00145447254181
adv train loss:  -54.64757168292999 , diff:  0.6461172103881836
layer  10  adv train finish, try to retain  124
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -56.225603342056274 , diff:  56.225603342056274
adv train loss:  -55.34057068824768 , diff:  0.8850326538085938
layer  11  adv train finish, try to retain  125
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -55.75359404087067 , diff:  55.75359404087067
adv train loss:  -57.01578050851822 , diff:  1.2621864676475525
adv train loss:  -57.04275768995285 , diff:  0.026977181434631348
layer  12  adv train finish, try to retain  126
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -57.65291118621826 , diff:  57.65291118621826
adv train loss:  -57.19179058074951 , diff:  0.46112060546875
layer  13  adv train finish, try to retain  124
>>>>>>> reverse layer  13  since no improvement >>>>>>>
---------------- start layer  14  ---------------
adv train loss:  -59.115875601768494 , diff:  59.115875601768494
adv train loss:  -51.07121068239212 , diff:  8.044664919376373
adv train loss:  -57.616381883621216 , diff:  6.5451712012290955
adv train loss:  -56.540098905563354 , diff:  1.0762829780578613
adv train loss:  -56.52002036571503 , diff:  0.020078539848327637
layer  14  adv train finish, try to retain  254
>>>>>>> reverse layer  14  since no improvement >>>>>>>
---------------- start layer  15  ---------------
adv train loss:  -56.30834895372391 , diff:  56.30834895372391
adv train loss:  -57.52053141593933 , diff:  1.2121824622154236
adv train loss:  -59.13096612691879 , diff:  1.6104347109794617
adv train loss:  -56.321542620658875 , diff:  2.809423506259918
adv train loss:  -57.63500618934631 , diff:  1.313463568687439
adv train loss:  -57.98324304819107 , diff:  0.3482368588447571
layer  15  adv train finish, try to retain  253
>>>>>>> reverse layer  15  since no improvement >>>>>>>
---------------- start layer  16  ---------------
adv train loss:  -56.28772711753845 , diff:  56.28772711753845
adv train loss:  -56.62603795528412 , diff:  0.3383108377456665
layer  16  adv train finish, try to retain  254
>>>>>>> reverse layer  16  since no improvement >>>>>>>
---------------- start layer  17  ---------------
adv train loss:  -55.67164367437363 , diff:  55.67164367437363
adv train loss:  -57.02217125892639 , diff:  1.350527584552765
adv train loss:  -56.10089999437332 , diff:  0.9212712645530701
layer  17  adv train finish, try to retain  254
>>>>>>> reverse layer  17  since no improvement >>>>>>>
---------------- start layer  18  ---------------
adv train loss:  -57.626462519168854 , diff:  57.626462519168854
adv train loss:  -57.72936761379242 , diff:  0.10290509462356567
layer  18  adv train finish, try to retain  252
>>>>>>> reverse layer  18  since no improvement >>>>>>>
---------------- start layer  19  ---------------
adv train loss:  -56.237702548503876 , diff:  56.237702548503876
adv train loss:  -56.88889491558075 , diff:  0.6511923670768738
layer  19  adv train finish, try to retain  250
>>>>>>> reverse layer  19  since no improvement >>>>>>>
---------------- start layer  20  ---------------
adv train loss:  -56.709644079208374 , diff:  56.709644079208374
adv train loss:  -55.88824290037155 , diff:  0.8214011788368225
layer  20  adv train finish, try to retain  255
>>>>>>> reverse layer  20  since no improvement >>>>>>>
---------------- start layer  21  ---------------
adv train loss:  -58.36652100086212 , diff:  58.36652100086212
adv train loss:  -54.85079491138458 , diff:  3.515726089477539
adv train loss:  -55.42993479967117 , diff:  0.5791398882865906
layer  21  adv train finish, try to retain  252
>>>>>>> reverse layer  21  since no improvement >>>>>>>
---------------- start layer  22  ---------------
adv train loss:  -52.79360246658325 , diff:  52.79360246658325
adv train loss:  -57.45190900564194 , diff:  4.658306539058685
adv train loss:  -55.89492619037628 , diff:  1.5569828152656555
adv train loss:  -56.874221205711365 , diff:  0.979295015335083
layer  22  adv train finish, try to retain  253
>>>>>>> reverse layer  22  since no improvement >>>>>>>
---------------- start layer  23  ---------------
adv train loss:  -53.61718571186066 , diff:  53.61718571186066
adv train loss:  -56.33358955383301 , diff:  2.716403841972351
adv train loss:  -55.7396445274353 , diff:  0.5939450263977051
layer  23  adv train finish, try to retain  253
>>>>>>> reverse layer  23  since no improvement >>>>>>>
---------------- start layer  24  ---------------
adv train loss:  -58.677834272384644 , diff:  58.677834272384644
adv train loss:  -53.47889858484268 , diff:  5.198935687541962
adv train loss:  -55.802109122276306 , diff:  2.3232105374336243
adv train loss:  -56.85532879829407 , diff:  1.0532196760177612
adv train loss:  -56.8764990568161 , diff:  0.02117025852203369
layer  24  adv train finish, try to retain  252
>>>>>>> reverse layer  24  since no improvement >>>>>>>
---------------- start layer  25  ---------------
adv train loss:  -59.120326817035675 , diff:  59.120326817035675
adv train loss:  -57.42033541202545 , diff:  1.6999914050102234
adv train loss:  -54.6550327539444 , diff:  2.7653026580810547
adv train loss:  -56.67834186553955 , diff:  2.023309111595154
adv train loss:  -54.63345670700073 , diff:  2.0448851585388184
adv train loss:  -55.5139741897583 , diff:  0.8805174827575684
layer  25  adv train finish, try to retain  253
>>>>>>> reverse layer  25  since no improvement >>>>>>>
---------------- start layer  26  ---------------
adv train loss:  -57.440055429935455 , diff:  57.440055429935455
adv train loss:  -57.570476949214935 , diff:  0.13042151927947998
layer  26  adv train finish, try to retain  511
>>>>>>> reverse layer  26  since no improvement >>>>>>>
---------------- start layer  27  ---------------
adv train loss:  -57.47058683633804 , diff:  57.47058683633804
adv train loss:  -54.61334776878357 , diff:  2.857239067554474
adv train loss:  -56.61911183595657 , diff:  2.005764067173004
adv train loss:  -55.96356701850891 , diff:  0.6555448174476624
layer  27  adv train finish, try to retain  508
>>>>>>> reverse layer  27  since no improvement >>>>>>>
---------------- start layer  28  ---------------
adv train loss:  -59.275092363357544 , diff:  59.275092363357544
adv train loss:  -54.83765774965286 , diff:  4.437434613704681
adv train loss:  -55.13232880830765 , diff:  0.29467105865478516
layer  28  adv train finish, try to retain  504
>>>>>>> reverse layer  28  since no improvement >>>>>>>
---------------- start layer  29  ---------------
adv train loss:  -57.67659515142441 , diff:  57.67659515142441
adv train loss:  -56.241608023643494 , diff:  1.4349871277809143
adv train loss:  -58.43198889493942 , diff:  2.190380871295929
adv train loss:  -55.47181707620621 , diff:  2.9601718187332153
adv train loss:  -59.46472978591919 , diff:  3.992912709712982
adv train loss:  -55.67259734869003 , diff:  3.7921324372291565
adv train loss:  -55.34031483530998 , diff:  0.33228251338005066
layer  29  adv train finish, try to retain  510
>>>>>>> reverse layer  29  since no improvement >>>>>>>
---------------- start layer  30  ---------------
adv train loss:  -53.438566505908966 , diff:  53.438566505908966
adv train loss:  -57.70594131946564 , diff:  4.267374813556671
adv train loss:  -55.87119668722153 , diff:  1.83474463224411
adv train loss:  -57.303047478199005 , diff:  1.431850790977478
adv train loss:  -55.86299151182175 , diff:  1.4400559663772583
adv train loss:  -56.343457102775574 , diff:  0.4804655909538269
layer  30  adv train finish, try to retain  510
>>>>>>> reverse layer  30  since no improvement >>>>>>>
---------------- start layer  31  ---------------
adv train loss:  -60.40824484825134 , diff:  60.40824484825134
adv train loss:  -57.85887545347214 , diff:  2.5493693947792053
adv train loss:  -55.47411060333252 , diff:  2.384764850139618
adv train loss:  -53.38608002662659 , diff:  2.0880305767059326
adv train loss:  -57.8082919716835 , diff:  4.422211945056915
adv train loss:  -56.88477635383606 , diff:  0.9235156178474426
layer  31  adv train finish, try to retain  511
>>>>>>> reverse layer  31  since no improvement >>>>>>>
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.0003125, 0.0003125, 0.0003125, 0.0003125, 0.0003125, 0.0003125, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 3.90625e-05, 3.90625e-05, 3.90625e-05, 3.90625e-05, 3.90625e-05, 3.90625e-05]  wait [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 0
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  1  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -55.908193469047546 , diff:  55.908193469047546
adv train loss:  -59.9003489613533 , diff:  3.9921554923057556
adv train loss:  -54.029589891433716 , diff:  5.870759069919586
adv train loss:  -55.19197756052017 , diff:  1.1623876690864563
adv train loss:  -58.262942254543304 , diff:  3.0709646940231323
adv train loss:  -54.35963273048401 , diff:  3.9033095240592957
adv train loss:  -57.90824216604233 , diff:  3.548609435558319
adv train loss:  -58.41148442029953 , diff:  0.5032422542572021
layer  0  adv train finish, try to retain  61
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -59.52905476093292 , diff:  59.52905476093292
adv train loss:  -61.41796541213989 , diff:  1.8889106512069702
adv train loss:  -60.276036620140076 , diff:  1.141928791999817
adv train loss:  -59.234053552150726 , diff:  1.0419830679893494
adv train loss:  -58.79129147529602 , diff:  0.4427620768547058
layer  1  adv train finish, try to retain  60
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -56.36715513467789 , diff:  56.36715513467789
adv train loss:  -58.02487200498581 , diff:  1.6577168703079224
adv train loss:  -56.4096018075943 , diff:  1.61527019739151
adv train loss:  -58.14370721578598 , diff:  1.734105408191681
adv train loss:  -56.489464581012726 , diff:  1.6542426347732544
adv train loss:  -54.853055477142334 , diff:  1.6364091038703918
adv train loss:  -55.82470524311066 , diff:  0.9716497659683228
layer  2  adv train finish, try to retain  59
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -55.28259468078613 , diff:  55.28259468078613
adv train loss:  -59.3366916179657 , diff:  4.054096937179565
adv train loss:  -56.67014300823212 , diff:  2.6665486097335815
adv train loss:  -59.560898303985596 , diff:  2.890755295753479
adv train loss:  -55.618770360946655 , diff:  3.9421279430389404
adv train loss:  -58.35165798664093 , diff:  2.732887625694275
adv train loss:  -60.52078694105148 , diff:  2.169128954410553
adv train loss:  -55.31030058860779 , diff:  5.210486352443695
adv train loss:  -56.77202099561691 , diff:  1.4617204070091248
adv train loss:  -58.13773649930954 , diff:  1.365715503692627
layer  3  adv train finish, try to retain  62
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -56.63702541589737 , diff:  56.63702541589737
adv train loss:  -56.10680681467056 , diff:  0.5302186012268066
layer  4  adv train finish, try to retain  58
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -57.580150961875916 , diff:  57.580150961875916
adv train loss:  -58.558673620224 , diff:  0.9785226583480835
layer  5  adv train finish, try to retain  57
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -58.444579780101776 , diff:  58.444579780101776
adv train loss:  -59.988713562488556 , diff:  1.5441337823867798
adv train loss:  -56.11333191394806 , diff:  3.875381648540497
adv train loss:  -57.70308417081833 , diff:  1.5897522568702698
adv train loss:  -58.14576929807663 , diff:  0.4426851272583008
layer  6  adv train finish, try to retain  123
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -54.30037981271744 , diff:  54.30037981271744
adv train loss:  -55.06729704141617 , diff:  0.7669172286987305
layer  7  adv train finish, try to retain  122
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -51.702800035476685 , diff:  51.702800035476685
adv train loss:  -57.30020081996918 , diff:  5.597400784492493
adv train loss:  -56.78592574596405 , diff:  0.514275074005127
layer  8  adv train finish, try to retain  122
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -57.356953740119934 , diff:  57.356953740119934
adv train loss:  -57.456893503665924 , diff:  0.09993976354598999
layer  9  adv train finish, try to retain  119
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -58.459784150123596 , diff:  58.459784150123596
adv train loss:  -55.187850534915924 , diff:  3.271933615207672
adv train loss:  -58.38611972332001 , diff:  3.1982691884040833
adv train loss:  -57.965287268161774 , diff:  0.42083245515823364
layer  10  adv train finish, try to retain  124
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -57.149836242198944 , diff:  57.149836242198944
adv train loss:  -58.39622128009796 , diff:  1.2463850378990173
adv train loss:  -56.61985719203949 , diff:  1.7763640880584717
adv train loss:  -59.06459277868271 , diff:  2.444735586643219
adv train loss:  -55.979383647441864 , diff:  3.0852091312408447
adv train loss:  -57.124712347984314 , diff:  1.14532870054245
adv train loss:  -56.16004306077957 , diff:  0.9646692872047424
layer  11  adv train finish, try to retain  122
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -59.965201020240784 , diff:  59.965201020240784
adv train loss:  -59.1264186501503 , diff:  0.8387823700904846
layer  12  adv train finish, try to retain  122
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -59.86376219987869 , diff:  59.86376219987869
adv train loss:  -56.41302877664566 , diff:  3.4507334232330322
adv train loss:  -59.503069281578064 , diff:  3.0900405049324036
adv train loss:  -56.6211211681366 , diff:  2.8819481134414673
adv train loss:  -58.93311029672623 , diff:  2.31198912858963
adv train loss:  -58.326250433921814 , diff:  0.6068598628044128
layer  13  adv train finish, try to retain  116
>>>>>>> reverse layer  13  since no improvement >>>>>>>
---------------- start layer  14  ---------------
adv train loss:  -56.13605958223343 , diff:  56.13605958223343
adv train loss:  -55.48293101787567 , diff:  0.6531285643577576
layer  14  adv train finish, try to retain  253
>>>>>>> reverse layer  14  since no improvement >>>>>>>
---------------- start layer  15  ---------------
adv train loss:  -56.97745814919472 , diff:  56.97745814919472
adv train loss:  -57.238462924957275 , diff:  0.261004775762558
layer  15  adv train finish, try to retain  251
>>>>>>> reverse layer  15  since no improvement >>>>>>>
---------------- start layer  16  ---------------
adv train loss:  -53.22406750917435 , diff:  53.22406750917435
adv train loss:  -56.10668361186981 , diff:  2.882616102695465
adv train loss:  -57.32647830247879 , diff:  1.2197946906089783
adv train loss:  -56.41625189781189 , diff:  0.9102264046669006
layer  16  adv train finish, try to retain  252
>>>>>>> reverse layer  16  since no improvement >>>>>>>
---------------- start layer  17  ---------------
adv train loss:  -58.47650396823883 , diff:  58.47650396823883
adv train loss:  -56.19672328233719 , diff:  2.279780685901642
adv train loss:  -54.297374069690704 , diff:  1.8993492126464844
adv train loss:  -55.9091659784317 , diff:  1.6117919087409973
adv train loss:  -55.70516800880432 , diff:  0.20399796962738037
layer  17  adv train finish, try to retain  253
>>>>>>> reverse layer  17  since no improvement >>>>>>>
---------------- start layer  18  ---------------
adv train loss:  -55.92577916383743 , diff:  55.92577916383743
adv train loss:  -56.050100922584534 , diff:  0.12432175874710083
layer  18  adv train finish, try to retain  249
>>>>>>> reverse layer  18  since no improvement >>>>>>>
---------------- start layer  19  ---------------
adv train loss:  -56.73467242717743 , diff:  56.73467242717743
adv train loss:  -55.64986735582352 , diff:  1.0848050713539124
adv train loss:  -58.67959177494049 , diff:  3.029724419116974
adv train loss:  -56.837465703487396 , diff:  1.8421260714530945
adv train loss:  -55.75613111257553 , diff:  1.0813345909118652
adv train loss:  -57.082356095314026 , diff:  1.3262249827384949
adv train loss:  -55.44417369365692 , diff:  1.6381824016571045
adv train loss:  -55.99357634782791 , diff:  0.54940265417099
layer  19  adv train finish, try to retain  251
>>>>>>> reverse layer  19  since no improvement >>>>>>>
---------------- start layer  20  ---------------
adv train loss:  -54.57364785671234 , diff:  54.57364785671234
adv train loss:  -56.05934554338455 , diff:  1.4856976866722107
adv train loss:  -56.7659707069397 , diff:  0.7066251635551453
layer  20  adv train finish, try to retain  251
>>>>>>> reverse layer  20  since no improvement >>>>>>>
---------------- start layer  21  ---------------
adv train loss:  -56.65176224708557 , diff:  56.65176224708557
adv train loss:  -56.538227915763855 , diff:  0.11353433132171631
layer  21  adv train finish, try to retain  250
>>>>>>> reverse layer  21  since no improvement >>>>>>>
---------------- start layer  22  ---------------
adv train loss:  -56.72478413581848 , diff:  56.72478413581848
adv train loss:  -54.54986023902893 , diff:  2.174923896789551
adv train loss:  -57.79240524768829 , diff:  3.242545008659363
adv train loss:  -55.583270728588104 , diff:  2.209134519100189
adv train loss:  -55.3105862736702 , diff:  0.2726844549179077
layer  22  adv train finish, try to retain  246
>>>>>>> reverse layer  22  since no improvement >>>>>>>
---------------- start layer  23  ---------------
adv train loss:  -55.48406898975372 , diff:  55.48406898975372
adv train loss:  -57.23418712615967 , diff:  1.7501181364059448
adv train loss:  -55.87099599838257 , diff:  1.3631911277770996
adv train loss:  -54.938129246234894 , diff:  0.9328667521476746
layer  23  adv train finish, try to retain  251
>>>>>>> reverse layer  23  since no improvement >>>>>>>
---------------- start layer  24  ---------------
adv train loss:  -57.714473247528076 , diff:  57.714473247528076
adv train loss:  -54.33901780843735 , diff:  3.3754554390907288
adv train loss:  -54.79705157876015 , diff:  0.4580337703227997
layer  24  adv train finish, try to retain  252
>>>>>>> reverse layer  24  since no improvement >>>>>>>
---------------- start layer  25  ---------------
adv train loss:  -56.699517488479614 , diff:  56.699517488479614
adv train loss:  -57.759239196777344 , diff:  1.0597217082977295
adv train loss:  -53.68915146589279 , diff:  4.070087730884552
adv train loss:  -56.515768587589264 , diff:  2.826617121696472
adv train loss:  -56.029898285865784 , diff:  0.4858703017234802
layer  25  adv train finish, try to retain  251
>>>>>>> reverse layer  25  since no improvement >>>>>>>
---------------- start layer  26  ---------------
adv train loss:  -56.51322418451309 , diff:  56.51322418451309
adv train loss:  -55.683221876621246 , diff:  0.8300023078918457
layer  26  adv train finish, try to retain  500
test acc: top1 ->  71.69999780807495 ; top5 ->  90.35999741210938  and loss:  970.3859063610435
forward train acc: top1 ->  29.53333250999451 ; top5 ->  54.408331928253176  and loss:  697.566913664341
test acc: top1 ->  26.74199922814369 ; top5 ->  52.925998562288285  and loss:  3137.549966275692
forward train acc: top1 ->  30.92499918937683 ; top5 ->  57.11666515350342  and loss:  655.1428472995758
test acc: top1 ->  31.897999112462998 ; top5 ->  58.4919983253479  and loss:  2684.528675377369
forward train acc: top1 ->  32.14999922752381 ; top5 ->  58.16666528701782  and loss:  646.3946480751038
test acc: top1 ->  34.77999906497001 ; top5 ->  62.22799813041687  and loss:  2518.7634764015675
forward train acc: top1 ->  39.949998693466185 ; top5 ->  66.41666467666626  and loss:  545.3282233476639
test acc: top1 ->  47.1599987575531 ; top5 ->  74.04399780731201  and loss:  1919.2245320677757
forward train acc: top1 ->  43.6749986743927 ; top5 ->  69.2999977684021  and loss:  506.9421249628067
test acc: top1 ->  48.44999859189987 ; top5 ->  75.105997756958  and loss:  1866.8199194520712
forward train acc: top1 ->  43.42499885559082 ; top5 ->  68.78333124160767  and loss:  514.9962015151978
test acc: top1 ->  48.479998631954196 ; top5 ->  74.67399772720337  and loss:  1882.089828044176
forward train acc: top1 ->  48.42499878883362 ; top5 ->  72.79166416168214  and loss:  462.01155030727386
test acc: top1 ->  55.45199841527939 ; top5 ->  79.98799765625  and loss:  1590.9761338829994
forward train acc: top1 ->  50.241665534973144 ; top5 ->  74.66666427612304  and loss:  438.5532182455063
test acc: top1 ->  56.06199842834473 ; top5 ->  80.66599766540527  and loss:  1559.7050060853362
forward train acc: top1 ->  51.99999862670899 ; top5 ->  76.14999752044677  and loss:  423.36535263061523
test acc: top1 ->  56.917998415088654 ; top5 ->  81.04199766464234  and loss:  1523.0599418878555
forward train acc: top1 ->  53.6249987411499 ; top5 ->  77.0666640663147  and loss:  407.2034478187561
test acc: top1 ->  59.44199838447571 ; top5 ->  82.96199762878418  and loss:  1420.9959842860699
>>>>>>> reverse layer  26  since performance drop >>>>>>>
==> this epoch:  502 / 512 , inc:  1
---------------- start layer  27  ---------------
adv train loss:  -102.15110194683075 , diff:  102.15110194683075
adv train loss:  -102.09496605396271 , diff:  0.05613589286804199
layer  27  adv train finish, try to retain  508
>>>>>>> reverse layer  27  since no improvement >>>>>>>
---------------- start layer  28  ---------------
adv train loss:  -102.75048863887787 , diff:  102.75048863887787
adv train loss:  -102.90694093704224 , diff:  0.15645229816436768
layer  28  adv train finish, try to retain  508
>>>>>>> reverse layer  28  since no improvement >>>>>>>
---------------- start layer  29  ---------------
adv train loss:  -102.204225897789 , diff:  102.204225897789
adv train loss:  -103.5192996263504 , diff:  1.3150737285614014
adv train loss:  -98.17866814136505 , diff:  5.340631484985352
adv train loss:  -100.09982132911682 , diff:  1.92115318775177
adv train loss:  -95.97875726222992 , diff:  4.121064066886902
adv train loss:  -97.29446518421173 , diff:  1.3157079219818115
adv train loss:  -101.42249190807343 , diff:  4.128026723861694
adv train loss:  -100.17900037765503 , diff:  1.243491530418396
adv train loss:  -98.30541718006134 , diff:  1.873583197593689
adv train loss:  -103.43152785301208 , diff:  5.126110672950745
layer  29  adv train finish, try to retain  509
>>>>>>> reverse layer  29  since no improvement >>>>>>>
---------------- start layer  30  ---------------
adv train loss:  -100.36704742908478 , diff:  100.36704742908478
adv train loss:  -98.40578937530518 , diff:  1.961258053779602
adv train loss:  -103.56488108634949 , diff:  5.1590917110443115
adv train loss:  -96.13506627082825 , diff:  7.42981481552124
adv train loss:  -97.40916514396667 , diff:  1.2740988731384277
adv train loss:  -104.45091664791107 , diff:  7.041751503944397
adv train loss:  -98.91171860694885 , diff:  5.539198040962219
adv train loss:  -101.2994190454483 , diff:  2.3877004384994507
adv train loss:  -95.93515300750732 , diff:  5.364266037940979
adv train loss:  -100.40444457530975 , diff:  4.469291567802429
layer  30  adv train finish, try to retain  511
>>>>>>> reverse layer  30  since no improvement >>>>>>>
---------------- start layer  31  ---------------
adv train loss:  -100.36086666584015 , diff:  100.36086666584015
adv train loss:  -101.53979778289795 , diff:  1.1789311170578003
adv train loss:  -101.45989084243774 , diff:  0.07990694046020508
layer  31  adv train finish, try to retain  507
>>>>>>> reverse layer  31  since no improvement >>>>>>>
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.000625, 0.000625, 0.000625, 0.000625, 0.000625, 0.000625, 0.0003125, 0.0003125, 0.0003125, 0.0003125, 0.0003125, 0.0003125, 0.0003125, 0.0003125, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 2.9296875e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05]  wait [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 0
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  2  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -100.48849976062775 , diff:  100.48849976062775
adv train loss:  -102.94153273105621 , diff:  2.453032970428467
adv train loss:  -98.95749747753143 , diff:  3.9840352535247803
adv train loss:  -103.2579185962677 , diff:  4.300421118736267
adv train loss:  -100.61475169658661 , diff:  2.6431668996810913
adv train loss:  -101.53833281993866 , diff:  0.9235811233520508
layer  0  adv train finish, try to retain  53
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -99.78731417655945 , diff:  99.78731417655945
adv train loss:  -100.70244991779327 , diff:  0.9151357412338257
layer  1  adv train finish, try to retain  55
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -101.03339052200317 , diff:  101.03339052200317
adv train loss:  -105.5081877708435 , diff:  4.474797248840332
adv train loss:  -101.4459285736084 , diff:  4.062259197235107
adv train loss:  -105.55292546749115 , diff:  4.1069968938827515
adv train loss:  -99.02638649940491 , diff:  6.526538968086243
adv train loss:  -100.42665123939514 , diff:  1.4002647399902344
adv train loss:  -103.53854537010193 , diff:  3.111894130706787
adv train loss:  -99.99444878101349 , diff:  3.54409658908844
adv train loss:  -106.21815383434296 , diff:  6.223705053329468
adv train loss:  -98.92976021766663 , diff:  7.288393616676331
layer  2  adv train finish, try to retain  57
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -101.5660572052002 , diff:  101.5660572052002
adv train loss:  -97.20533263683319 , diff:  4.360724568367004
adv train loss:  -103.45860934257507 , diff:  6.253276705741882
adv train loss:  -100.41830623149872 , diff:  3.040303111076355
adv train loss:  -100.57023549079895 , diff:  0.15192925930023193
layer  3  adv train finish, try to retain  58
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -101.48051738739014 , diff:  101.48051738739014
adv train loss:  -100.87101364135742 , diff:  0.6095037460327148
layer  4  adv train finish, try to retain  62
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -100.02904224395752 , diff:  100.02904224395752
adv train loss:  -98.31089663505554 , diff:  1.7181456089019775
adv train loss:  -101.24987483024597 , diff:  2.9389781951904297
adv train loss:  -99.88204538822174 , diff:  1.367829442024231
adv train loss:  -99.97618556022644 , diff:  0.09414017200469971
layer  5  adv train finish, try to retain  59
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -98.23309111595154 , diff:  98.23309111595154
adv train loss:  -106.97154474258423 , diff:  8.73845362663269
adv train loss:  -100.71217155456543 , diff:  6.259373188018799
adv train loss:  -101.7270976305008 , diff:  1.0149260759353638
adv train loss:  -97.60509121417999 , diff:  4.122006416320801
adv train loss:  -102.83075273036957 , diff:  5.225661516189575
adv train loss:  -99.10136413574219 , diff:  3.7293885946273804
adv train loss:  -99.4406806230545 , diff:  0.3393164873123169
layer  6  adv train finish, try to retain  118
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -101.73627376556396 , diff:  101.73627376556396
adv train loss:  -100.30029261112213 , diff:  1.4359811544418335
adv train loss:  -95.9309012889862 , diff:  4.369391322135925
adv train loss:  -100.66478383541107 , diff:  4.733882546424866
adv train loss:  -98.55984020233154 , diff:  2.104943633079529
adv train loss:  -104.2741711139679 , diff:  5.7143309116363525
adv train loss:  -101.32517302036285 , diff:  2.9489980936050415
adv train loss:  -101.65184283256531 , diff:  0.3266698122024536
layer  7  adv train finish, try to retain  119
test acc: top1 ->  58.46999826469421 ; top5 ->  82.19999755249023  and loss:  1462.9442593306303
forward train acc: top1 ->  33.11666565895081 ; top5 ->  59.549998626708984  and loss:  633.0837763547897
test acc: top1 ->  35.649999041461946 ; top5 ->  63.29199820175171  and loss:  2524.7444751262665
forward train acc: top1 ->  33.69999917030334 ; top5 ->  60.23333187103272  and loss:  624.3943560123444
test acc: top1 ->  37.43399895777702 ; top5 ->  65.0999981262207  and loss:  2366.217190384865
forward train acc: top1 ->  33.791665787696836 ; top5 ->  60.3083317565918  and loss:  618.5849595069885
test acc: top1 ->  37.425998985099795 ; top5 ->  65.01599799804687  and loss:  2383.399430900812
forward train acc: top1 ->  41.69999886512756 ; top5 ->  66.70833116531372  and loss:  540.4257619380951
test acc: top1 ->  48.75999867658615 ; top5 ->  74.87999778671265  and loss:  1866.651976943016
forward train acc: top1 ->  43.533332118988035 ; top5 ->  69.0166643333435  and loss:  514.0361545085907
test acc: top1 ->  49.01799860458374 ; top5 ->  75.42199788665772  and loss:  1851.2447481155396
forward train acc: top1 ->  43.94999878883362 ; top5 ->  69.57499778747558  and loss:  505.4043833017349
test acc: top1 ->  50.223998574066165 ; top5 ->  76.13199774475098  and loss:  1796.3144871294498
forward train acc: top1 ->  47.55833207130432 ; top5 ->  72.68333080291748  and loss:  468.58734953403473
test acc: top1 ->  55.249998418712615 ; top5 ->  79.90399757919312  and loss:  1588.0024889111519
forward train acc: top1 ->  50.04999886512756 ; top5 ->  73.97499746322632  and loss:  449.1272021532059
test acc: top1 ->  55.79599842910767 ; top5 ->  80.40999741439819  and loss:  1559.849967122078
forward train acc: top1 ->  50.949998712539674 ; top5 ->  74.54999757766724  and loss:  434.8260873556137
test acc: top1 ->  56.813998357009886 ; top5 ->  81.12199756011962  and loss:  1530.9212599545717
forward train acc: top1 ->  51.67499873161316 ; top5 ->  75.4583306312561  and loss:  427.9707285165787
test acc: top1 ->  58.64399833221436 ; top5 ->  82.34799756011962  and loss:  1457.3689607679844
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  122 / 128 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -104.44017589092255 , diff:  104.44017589092255
adv train loss:  -106.16833293437958 , diff:  1.7281570434570312
adv train loss:  -103.62896132469177 , diff:  2.539371609687805
adv train loss:  -105.09005844593048 , diff:  1.4610971212387085
adv train loss:  -104.6603399515152 , diff:  0.4297184944152832
layer  8  adv train finish, try to retain  125
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -105.23525595664978 , diff:  105.23525595664978
adv train loss:  -106.87557244300842 , diff:  1.6403164863586426
adv train loss:  -102.68969249725342 , diff:  4.185879945755005
adv train loss:  -105.21591866016388 , diff:  2.5262261629104614
adv train loss:  -103.84123229980469 , diff:  1.374686360359192
adv train loss:  -105.73216664791107 , diff:  1.8909343481063843
adv train loss:  -106.09024631977081 , diff:  0.3580796718597412
layer  9  adv train finish, try to retain  116
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -107.71803867816925 , diff:  107.71803867816925
adv train loss:  -106.98166513442993 , diff:  0.7363735437393188
layer  10  adv train finish, try to retain  118
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -104.46747922897339 , diff:  104.46747922897339
adv train loss:  -104.44949495792389 , diff:  0.01798427104949951
layer  11  adv train finish, try to retain  116
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -102.8010379076004 , diff:  102.8010379076004
adv train loss:  -106.76715242862701 , diff:  3.9661145210266113
adv train loss:  -101.72161173820496 , diff:  5.045540690422058
adv train loss:  -102.6841778755188 , diff:  0.9625661373138428
layer  12  adv train finish, try to retain  115
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -106.93785762786865 , diff:  106.93785762786865
adv train loss:  -104.98954951763153 , diff:  1.9483081102371216
adv train loss:  -104.69548606872559 , diff:  0.2940634489059448
layer  13  adv train finish, try to retain  122
>>>>>>> reverse layer  13  since no improvement >>>>>>>
---------------- start layer  14  ---------------
adv train loss:  -105.05616390705109 , diff:  105.05616390705109
adv train loss:  -107.55599653720856 , diff:  2.4998326301574707
adv train loss:  -103.50193965435028 , diff:  4.054056882858276
adv train loss:  -104.17462146282196 , diff:  0.6726818084716797
layer  14  adv train finish, try to retain  245
test acc: top1 ->  57.9639983669281 ; top5 ->  81.7059976928711  and loss:  1486.660749733448
forward train acc: top1 ->  36.69166563034057 ; top5 ->  63.16666488647461  and loss:  590.5589938163757
test acc: top1 ->  39.25399890284538 ; top5 ->  67.04799813995362  and loss:  2301.255177229643
forward train acc: top1 ->  35.68333236694336 ; top5 ->  62.65833164215088  and loss:  592.4921875
test acc: top1 ->  40.04399888458252 ; top5 ->  67.73999802627563  and loss:  2269.583056360483
forward train acc: top1 ->  37.32499877929688 ; top5 ->  62.524998321533204  and loss:  585.390700340271
test acc: top1 ->  41.093998786640164 ; top5 ->  68.4919978981018  and loss:  2239.543836683035
forward train acc: top1 ->  42.374998788833615 ; top5 ->  67.89166486740112  and loss:  524.7023991346359
test acc: top1 ->  49.835998632717136 ; top5 ->  75.74199765701294  and loss:  1818.0279496014118
forward train acc: top1 ->  44.64166553497314 ; top5 ->  69.95833101272584  and loss:  498.5658348798752
test acc: top1 ->  50.755998657035825 ; top5 ->  76.82199777069091  and loss:  1777.2211924642324
forward train acc: top1 ->  45.508332109451295 ; top5 ->  70.79166440963745  and loss:  490.4518916606903
test acc: top1 ->  50.94799853801727 ; top5 ->  76.86999777984619  and loss:  1763.7594053894281
forward train acc: top1 ->  48.52499888420105 ; top5 ->  73.09166437149048  and loss:  462.03914272785187
test acc: top1 ->  55.27599842243195 ; top5 ->  79.87799765396119  and loss:  1598.0195903033018
forward train acc: top1 ->  49.908332090377804 ; top5 ->  74.33333127975465  and loss:  446.44928777217865
test acc: top1 ->  55.86599831008911 ; top5 ->  80.5099975616455  and loss:  1555.3059239685535
forward train acc: top1 ->  50.64166543960571 ; top5 ->  75.58333103179932  and loss:  433.69540548324585
test acc: top1 ->  56.293998378562925 ; top5 ->  80.82199750976562  and loss:  1543.5084446966648
forward train acc: top1 ->  51.76666526794433 ; top5 ->  75.41666408538818  and loss:  430.19341576099396
test acc: top1 ->  58.3599983133316 ; top5 ->  82.1039976486206  and loss:  1462.0460810512304
>>>>>>> reverse layer  14  since performance drop >>>>>>>
==> this epoch:  250 / 256 , inc:  1
---------------- start layer  15  ---------------
adv train loss:  -106.00992000102997 , diff:  106.00992000102997
adv train loss:  -102.02298855781555 , diff:  3.9869314432144165
adv train loss:  -106.92439937591553 , diff:  4.901410818099976
adv train loss:  -107.63553428649902 , diff:  0.7111349105834961
layer  15  adv train finish, try to retain  245
test acc: top1 ->  57.30799830627441 ; top5 ->  81.46399744110107  and loss:  1498.831992045045
forward train acc: top1 ->  38.46666543960571 ; top5 ->  64.30833152770997  and loss:  573.064262509346
test acc: top1 ->  42.699998842763904 ; top5 ->  69.33399794311524  and loss:  2148.825905740261
forward train acc: top1 ->  38.774998874664306 ; top5 ->  64.8249983215332  and loss:  566.2925288677216
test acc: top1 ->  42.56399884467125 ; top5 ->  69.45199795074463  and loss:  2177.6043512821198
forward train acc: top1 ->  39.733332290649415 ; top5 ->  65.23333152770996  and loss:  559.4383369684219
test acc: top1 ->  42.76199876303673 ; top5 ->  69.89599797668457  and loss:  2128.53058385849
forward train acc: top1 ->  44.19166540145874 ; top5 ->  68.54166484832764  and loss:  511.90867924690247
test acc: top1 ->  50.49199860987663 ; top5 ->  76.28399764099122  and loss:  1792.5299028903246
forward train acc: top1 ->  45.24166536331177 ; top5 ->  70.24999771118163  and loss:  499.01066052913666
test acc: top1 ->  51.48399857058525 ; top5 ->  77.39799763031006  and loss:  1734.6515082269907
forward train acc: top1 ->  46.61666533470154 ; top5 ->  71.34999792098999  and loss:  487.95861864089966
test acc: top1 ->  52.07199853124619 ; top5 ->  77.80999778137208  and loss:  1709.749173283577
forward train acc: top1 ->  48.283332109451294 ; top5 ->  73.09999759674072  and loss:  462.2264668941498
test acc: top1 ->  56.28599848194122 ; top5 ->  80.45999765319824  and loss:  1546.6727015972137
forward train acc: top1 ->  50.26666537284851 ; top5 ->  74.5249976348877  and loss:  442.67754352092743
test acc: top1 ->  56.853998427581786 ; top5 ->  80.93399761657714  and loss:  1529.6258293539286
forward train acc: top1 ->  50.43333201408386 ; top5 ->  74.28333106994629  and loss:  440.13660287857056
test acc: top1 ->  56.91199836845398 ; top5 ->  81.02799764099122  and loss:  1522.1145484298468
forward train acc: top1 ->  52.63333198547363 ; top5 ->  76.07499757766723  and loss:  421.0452969074249
test acc: top1 ->  58.717998304367065 ; top5 ->  82.32199762115478  and loss:  1448.8887452930212
>>>>>>> reverse layer  15  since performance drop >>>>>>>
==> this epoch:  248 / 256 , inc:  1
---------------- start layer  16  ---------------
adv train loss:  -104.39309120178223 , diff:  104.39309120178223
adv train loss:  -107.58404552936554 , diff:  3.190954327583313
adv train loss:  -107.07479465007782 , diff:  0.5092508792877197
layer  16  adv train finish, try to retain  250
>>>>>>> reverse layer  16  since no improvement >>>>>>>
---------------- start layer  17  ---------------
adv train loss:  -104.56632149219513 , diff:  104.56632149219513
adv train loss:  -107.23238945007324 , diff:  2.666067957878113
adv train loss:  -104.11837935447693 , diff:  3.1140100955963135
adv train loss:  -108.26823353767395 , diff:  4.1498541831970215
adv train loss:  -105.23946905136108 , diff:  3.028764486312866
adv train loss:  -103.94975101947784 , diff:  1.2897180318832397
adv train loss:  -102.45599389076233 , diff:  1.4937571287155151
adv train loss:  -105.82191777229309 , diff:  3.3659238815307617
adv train loss:  -105.8765720129013 , diff:  0.05465424060821533
layer  17  adv train finish, try to retain  251
>>>>>>> reverse layer  17  since no improvement >>>>>>>
---------------- start layer  18  ---------------
adv train loss:  -107.00965535640717 , diff:  107.00965535640717
adv train loss:  -106.35120928287506 , diff:  0.6584460735321045
layer  18  adv train finish, try to retain  243
>>>>>>> reverse layer  18  since no improvement >>>>>>>
---------------- start layer  19  ---------------
adv train loss:  -103.96232402324677 , diff:  103.96232402324677
adv train loss:  -104.60376501083374 , diff:  0.6414409875869751
layer  19  adv train finish, try to retain  249
>>>>>>> reverse layer  19  since no improvement >>>>>>>
---------------- start layer  20  ---------------
adv train loss:  -103.90543735027313 , diff:  103.90543735027313
adv train loss:  -107.67893350124359 , diff:  3.773496150970459
adv train loss:  -105.07925927639008 , diff:  2.5996742248535156
adv train loss:  -107.04418933391571 , diff:  1.9649300575256348
adv train loss:  -103.3711234331131 , diff:  3.6730659008026123
adv train loss:  -104.43856549263 , diff:  1.0674420595169067
adv train loss:  -105.17577922344208 , diff:  0.7372137308120728
layer  20  adv train finish, try to retain  246
>>>>>>> reverse layer  20  since no improvement >>>>>>>
---------------- start layer  21  ---------------
adv train loss:  -107.24227488040924 , diff:  107.24227488040924
adv train loss:  -106.01636493206024 , diff:  1.225909948348999
adv train loss:  -105.25089859962463 , diff:  0.7654663324356079
layer  21  adv train finish, try to retain  249
>>>>>>> reverse layer  21  since no improvement >>>>>>>
---------------- start layer  22  ---------------
adv train loss:  -106.80631411075592 , diff:  106.80631411075592
adv train loss:  -106.75786900520325 , diff:  0.04844510555267334
layer  22  adv train finish, try to retain  248
>>>>>>> reverse layer  22  since no improvement >>>>>>>
---------------- start layer  23  ---------------
adv train loss:  -106.48863661289215 , diff:  106.48863661289215
adv train loss:  -106.5230165719986 , diff:  0.03437995910644531
layer  23  adv train finish, try to retain  245
>>>>>>> reverse layer  23  since no improvement >>>>>>>
---------------- start layer  24  ---------------
adv train loss:  -106.23014271259308 , diff:  106.23014271259308
adv train loss:  -104.75814163684845 , diff:  1.472001075744629
adv train loss:  -105.32956326007843 , diff:  0.5714216232299805
layer  24  adv train finish, try to retain  245
>>>>>>> reverse layer  24  since no improvement >>>>>>>
---------------- start layer  25  ---------------
adv train loss:  -101.0593912601471 , diff:  101.0593912601471
adv train loss:  -109.57386755943298 , diff:  8.514476299285889
adv train loss:  -105.6803662776947 , diff:  3.8935012817382812
adv train loss:  -105.09582829475403 , diff:  0.5845379829406738
layer  25  adv train finish, try to retain  242
>>>>>>> reverse layer  25  since no improvement >>>>>>>
---------------- start layer  26  ---------------
adv train loss:  -105.85827016830444 , diff:  105.85827016830444
adv train loss:  -106.4728193283081 , diff:  0.6145491600036621
layer  26  adv train finish, try to retain  506
>>>>>>> reverse layer  26  since no improvement >>>>>>>
---------------- start layer  27  ---------------
adv train loss:  -104.47430741786957 , diff:  104.47430741786957
adv train loss:  -108.12988233566284 , diff:  3.655574917793274
adv train loss:  -102.13985991477966 , diff:  5.990022420883179
adv train loss:  -104.66816210746765 , diff:  2.5283021926879883
adv train loss:  -106.2829669713974 , diff:  1.6148048639297485
adv train loss:  -105.31468296051025 , diff:  0.968284010887146
layer  27  adv train finish, try to retain  505
>>>>>>> reverse layer  27  since no improvement >>>>>>>
---------------- start layer  28  ---------------
adv train loss:  -105.10849595069885 , diff:  105.10849595069885
adv train loss:  -105.7232825756073 , diff:  0.6147866249084473
layer  28  adv train finish, try to retain  499
test acc: top1 ->  58.21199835510254 ; top5 ->  81.80599763031006  and loss:  1476.0636155754328
forward train acc: top1 ->  41.683331985473636 ; top5 ->  67.48333124160767  and loss:  539.1743214130402
test acc: top1 ->  45.00799873037338 ; top5 ->  72.3779979347229  and loss:  2056.469900339842
forward train acc: top1 ->  39.058332176208495 ; top5 ->  65.43333145141601  and loss:  558.5939112901688
test acc: top1 ->  43.791998764657976 ; top5 ->  70.80599780731201  and loss:  2085.20029515028
forward train acc: top1 ->  39.68333225250244 ; top5 ->  65.81666481018067  and loss:  557.5607814788818
test acc: top1 ->  45.1819987765789 ; top5 ->  72.50599782409668  and loss:  2010.2835148870945
forward train acc: top1 ->  45.491665534973144 ; top5 ->  70.74999755859375  and loss:  494.90224826335907
test acc: top1 ->  51.76599850645065 ; top5 ->  77.3359976524353  and loss:  1732.4269842207432
forward train acc: top1 ->  47.541665391921995 ; top5 ->  71.9083310508728  and loss:  473.4209967851639
test acc: top1 ->  53.07399850635529 ; top5 ->  78.14199778900146  and loss:  1692.3493222594261
forward train acc: top1 ->  47.51666553497314 ; top5 ->  71.94166410446167  and loss:  475.74962508678436
test acc: top1 ->  53.453998496246335 ; top5 ->  78.59199775161743  and loss:  1668.0735580027103
forward train acc: top1 ->  49.95833208084107 ; top5 ->  73.72499736785889  and loss:  447.07543206214905
test acc: top1 ->  56.74799832191467 ; top5 ->  80.89999758605957  and loss:  1536.9326425641775
forward train acc: top1 ->  51.20833204269409 ; top5 ->  74.89166423797607  and loss:  439.1633975505829
test acc: top1 ->  57.407998434448245 ; top5 ->  81.41799756469726  and loss:  1507.1265918761492
forward train acc: top1 ->  51.58333214759827 ; top5 ->  75.34999780654907  and loss:  427.70383524894714
test acc: top1 ->  57.885998365020754 ; top5 ->  81.77599746551513  and loss:  1484.1307562589645
forward train acc: top1 ->  52.791665439605715 ; top5 ->  76.63333068847656  and loss:  415.7388423681259
test acc: top1 ->  59.09799829864502 ; top5 ->  82.53399745178223  and loss:  1433.223312407732
>>>>>>> reverse layer  28  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  29  ---------------
adv train loss:  -104.38697290420532 , diff:  104.38697290420532
adv train loss:  -103.04752802848816 , diff:  1.339444875717163
adv train loss:  -103.10336172580719 , diff:  0.05583369731903076
layer  29  adv train finish, try to retain  500
>>>>>>> reverse layer  29  since no improvement >>>>>>>
---------------- start layer  30  ---------------
adv train loss:  -101.0128743648529 , diff:  101.0128743648529
adv train loss:  -99.2032665014267 , diff:  1.8096078634262085
adv train loss:  -102.4607253074646 , diff:  3.257458806037903
adv train loss:  -101.4437335729599 , diff:  1.0169917345046997
adv train loss:  -106.9152067899704 , diff:  5.471473217010498
adv train loss:  -101.0472469329834 , diff:  5.8679598569869995
adv train loss:  -101.61141121387482 , diff:  0.5641642808914185
layer  30  adv train finish, try to retain  495
test acc: top1 ->  58.99199828109741 ; top5 ->  82.48799761047363  and loss:  1438.7275919616222
forward train acc: top1 ->  42.19999873161316 ; top5 ->  67.76666460037231  and loss:  528.8527940511703
test acc: top1 ->  46.02399871811867 ; top5 ->  73.11999771652222  and loss:  2005.0121288001537
forward train acc: top1 ->  41.283332176208496 ; top5 ->  67.24166473388672  and loss:  535.7963302135468
test acc: top1 ->  46.13999868798256 ; top5 ->  73.17999778137207  and loss:  1969.2741405069828
forward train acc: top1 ->  40.72499881744385 ; top5 ->  66.79999809265136  and loss:  544.3157408237457
test acc: top1 ->  45.51799873347282 ; top5 ->  72.45199784469604  and loss:  2000.1572013497353
forward train acc: top1 ->  45.466665391922 ; top5 ->  70.73333129882812  and loss:  492.1649411916733
test acc: top1 ->  52.741998471832275 ; top5 ->  78.3099976776123  and loss:  1685.2257531285286
forward train acc: top1 ->  47.52499871253967 ; top5 ->  72.44999784469604  and loss:  469.79747009277344
test acc: top1 ->  53.9879984908104 ; top5 ->  79.02999764404296  and loss:  1635.5021311193705
forward train acc: top1 ->  48.24166541099548 ; top5 ->  72.89166439056396  and loss:  464.09948766231537
test acc: top1 ->  53.69799848089218 ; top5 ->  78.76599759979248  and loss:  1653.1386363059282
forward train acc: top1 ->  49.9749986743927 ; top5 ->  74.19166402816772  and loss:  440.22254621982574
test acc: top1 ->  56.87399834575653 ; top5 ->  81.101997605896  and loss:  1521.0552202016115
forward train acc: top1 ->  51.20833218574524 ; top5 ->  75.14999748229981  and loss:  432.96154832839966
test acc: top1 ->  57.28599845352173 ; top5 ->  81.44999760971069  and loss:  1508.4880767464638
forward train acc: top1 ->  52.008332138061526 ; top5 ->  75.59166429519654  and loss:  427.9994730949402
test acc: top1 ->  57.71799834356308 ; top5 ->  81.75599758453369  and loss:  1487.0601657778025
forward train acc: top1 ->  53.166665401458744 ; top5 ->  76.33333078384399  and loss:  416.11391592025757
test acc: top1 ->  59.12399836540222 ; top5 ->  82.68399761199952  and loss:  1432.9574816972017
>>>>>>> reverse layer  30  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  31  ---------------
adv train loss:  -103.30582666397095 , diff:  103.30582666397095
adv train loss:  -103.69803893566132 , diff:  0.39221227169036865
layer  31  adv train finish, try to retain  499
test acc: top1 ->  58.68799831113815 ; top5 ->  82.22799757537842  and loss:  1455.1835869699717
forward train acc: top1 ->  43.841665515899656 ; top5 ->  69.54166437149048  and loss:  514.5667575597763
test acc: top1 ->  48.223998741531375 ; top5 ->  74.77199766044616  and loss:  1894.933509618044
forward train acc: top1 ->  42.69166541099548 ; top5 ->  68.80833124160766  and loss:  520.7919981479645
test acc: top1 ->  47.03799871501923 ; top5 ->  73.50599773406982  and loss:  1955.3974343389273
forward train acc: top1 ->  42.95833203315735 ; top5 ->  67.61666467666626  and loss:  527.6509439945221
test acc: top1 ->  47.85199867444038 ; top5 ->  74.52799777526856  and loss:  1903.3057938218117
forward train acc: top1 ->  47.208332223892214 ; top5 ->  71.8166640472412  and loss:  480.05058777332306
test acc: top1 ->  53.31599847831726 ; top5 ->  78.61799777908325  and loss:  1667.4475327581167
forward train acc: top1 ->  47.84999868392944 ; top5 ->  72.33333089828491  and loss:  472.882287979126
test acc: top1 ->  54.40399840221405 ; top5 ->  79.40599763412476  and loss:  1635.239095121622
forward train acc: top1 ->  49.12499872207641 ; top5 ->  73.2999974822998  and loss:  461.15469098091125
test acc: top1 ->  54.99399836769104 ; top5 ->  80.0399976737976  and loss:  1600.6118375211954
forward train acc: top1 ->  51.84166534423828 ; top5 ->  75.60833086013794  and loss:  429.5442669391632
test acc: top1 ->  57.655998491477966 ; top5 ->  81.56199742965698  and loss:  1497.2559033185244
forward train acc: top1 ->  52.61666542053223 ; top5 ->  76.12499752044678  and loss:  418.37113106250763
test acc: top1 ->  58.097998427772524 ; top5 ->  82.08399762420655  and loss:  1476.082581013441
forward train acc: top1 ->  52.641665325164794 ; top5 ->  75.84166414260864  and loss:  423.05096566677094
test acc: top1 ->  58.62799838104248 ; top5 ->  82.25799754257203  and loss:  1459.2679217755795
forward train acc: top1 ->  53.14999858856201 ; top5 ->  76.99999753952027  and loss:  408.37606489658356
test acc: top1 ->  59.55799826583862 ; top5 ->  82.98599745178223  and loss:  1415.2908905148506
>>>>>>> reverse layer  31  since performance drop >>>>>>>
==> this epoch:  501 / 512 , inc:  1
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.00125, 0.00125, 0.00125, 0.00125, 0.00125, 0.00125, 0.000625, 0.000234375, 0.000625, 0.000625, 0.000625, 0.000625, 0.000625, 0.000625, 0.0001171875, 0.0001171875, 0.0003125, 0.0003125, 0.0003125, 0.0003125, 0.0003125, 0.0003125, 0.0003125, 0.0003125, 0.0003125, 0.0003125, 5.859375e-05, 0.00015625, 5.859375e-05, 0.00015625, 5.859375e-05, 5.859375e-05]  wait [0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 0
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  3  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -103.20333230495453 , diff:  103.20333230495453
adv train loss:  -106.24481570720673 , diff:  3.0414834022521973
adv train loss:  -101.47627186775208 , diff:  4.768543839454651
adv train loss:  -104.6134797334671 , diff:  3.137207865715027
adv train loss:  -102.4123694896698 , diff:  2.2011102437973022
adv train loss:  -99.27896618843079 , diff:  3.1334033012390137
adv train loss:  -104.09689009189606 , diff:  4.817923903465271
adv train loss:  -101.89240300655365 , diff:  2.2044870853424072
adv train loss:  -105.66801464557648 , diff:  3.775611639022827
adv train loss:  -101.43413150310516 , diff:  4.2338831424713135
layer  0  adv train finish, try to retain  54
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -106.3186184167862 , diff:  106.3186184167862
adv train loss:  -102.65130150318146 , diff:  3.6673169136047363
adv train loss:  -103.08318960666656 , diff:  0.4318881034851074
layer  1  adv train finish, try to retain  60
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -104.27406644821167 , diff:  104.27406644821167
adv train loss:  -103.33894681930542 , diff:  0.93511962890625
layer  2  adv train finish, try to retain  52
test acc: top1 ->  58.48399840431213 ; top5 ->  82.22199750823975  and loss:  1463.785889223218
forward train acc: top1 ->  44.27499876976013 ; top5 ->  69.24999788284302  and loss:  512.9575997591019
test acc: top1 ->  49.099998644065856 ; top5 ->  75.4679977607727  and loss:  1873.2579717040062
forward train acc: top1 ->  43.899998836517334 ; top5 ->  69.7916643333435  and loss:  506.609139919281
test acc: top1 ->  47.80399868659973 ; top5 ->  74.42199772567749  and loss:  1924.2363211512566
forward train acc: top1 ->  43.53333208084106 ; top5 ->  69.16666444778443  and loss:  514.8703447580338
test acc: top1 ->  46.723998707914355 ; top5 ->  73.52999789733887  and loss:  1958.8641407191753
forward train acc: top1 ->  46.9666654586792 ; top5 ->  71.28333114624023  and loss:  479.9823168516159
test acc: top1 ->  54.34799841680527 ; top5 ->  79.27199773330689  and loss:  1628.9410392642021
forward train acc: top1 ->  48.89999877929687 ; top5 ->  73.3499973487854  and loss:  459.95992600917816
test acc: top1 ->  55.28399850111008 ; top5 ->  79.69999771957397  and loss:  1606.821515738964
forward train acc: top1 ->  50.09999866485596 ; top5 ->  74.41666410446167  and loss:  442.8469090461731
test acc: top1 ->  55.25999851989746 ; top5 ->  80.04599768447876  and loss:  1588.698676198721
forward train acc: top1 ->  52.2333321094513 ; top5 ->  76.20833084106445  and loss:  422.80987787246704
test acc: top1 ->  57.9799982635498 ; top5 ->  81.69199753570557  and loss:  1482.9813202023506
forward train acc: top1 ->  52.97499864578247 ; top5 ->  76.35833084106446  and loss:  417.12251937389374
test acc: top1 ->  58.481998334503174 ; top5 ->  82.02799757843017  and loss:  1465.9079401940107
forward train acc: top1 ->  53.83333215713501 ; top5 ->  76.60833074569702  and loss:  409.5220727920532
test acc: top1 ->  58.68599836769104 ; top5 ->  82.32199752502441  and loss:  1455.4938300848007
forward train acc: top1 ->  52.98333206176758 ; top5 ->  76.64166423797607  and loss:  413.5130115747452
test acc: top1 ->  59.85199833202362 ; top5 ->  83.08199752044678  and loss:  1410.945191308856
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  57 / 64 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -102.96192538738251 , diff:  102.96192538738251
adv train loss:  -101.20539367198944 , diff:  1.7565317153930664
adv train loss:  -99.87411499023438 , diff:  1.331278681755066
adv train loss:  -104.60896837711334 , diff:  4.734853386878967
adv train loss:  -100.2017092704773 , diff:  4.407259106636047
adv train loss:  -100.36502838134766 , diff:  0.16331911087036133
layer  3  adv train finish, try to retain  53
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -105.82543516159058 , diff:  105.82543516159058
adv train loss:  -102.44612336158752 , diff:  3.3793118000030518
adv train loss:  -102.71106374263763 , diff:  0.26494038105010986
layer  4  adv train finish, try to retain  56
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -99.01056051254272 , diff:  99.01056051254272
adv train loss:  -104.41749262809753 , diff:  5.40693211555481
adv train loss:  -102.57209527492523 , diff:  1.8453973531723022
adv train loss:  -105.70372247695923 , diff:  3.1316272020339966
adv train loss:  -104.0862044095993 , diff:  1.6175180673599243
adv train loss:  -100.99653232097626 , diff:  3.089672088623047
adv train loss:  -102.00650906562805 , diff:  1.0099767446517944
adv train loss:  -100.43370354175568 , diff:  1.5728055238723755
adv train loss:  -103.35038983821869 , diff:  2.9166862964630127
adv train loss:  -100.3580687046051 , diff:  2.9923211336135864
layer  5  adv train finish, try to retain  56
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -102.26895880699158 , diff:  102.26895880699158
adv train loss:  -100.42755234241486 , diff:  1.8414064645767212
adv train loss:  -99.61353170871735 , diff:  0.8140206336975098
layer  6  adv train finish, try to retain  112
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -103.45434093475342 , diff:  103.45434093475342
adv train loss:  -102.1574695110321 , diff:  1.2968714237213135
adv train loss:  -98.94669950008392 , diff:  3.210770010948181
adv train loss:  -99.27580392360687 , diff:  0.3291044235229492
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  121
test acc: top1 ->  58.67199838943481 ; top5 ->  82.15999761657714  and loss:  1456.540290594101
forward train acc: top1 ->  44.94166536331177 ; top5 ->  70.12499801635742  and loss:  499.52984952926636
test acc: top1 ->  48.675998647880554 ; top5 ->  75.08799780426025  and loss:  1877.294441923499
forward train acc: top1 ->  43.95833211898804 ; top5 ->  69.28333108901978  and loss:  511.60155737400055
test acc: top1 ->  49.861998602485656 ; top5 ->  75.49399768295288  and loss:  1845.5750076770782
forward train acc: top1 ->  45.09999871253967 ; top5 ->  69.36666435241699  and loss:  507.4489721059799
test acc: top1 ->  49.22999862375259 ; top5 ->  75.66799769210816  and loss:  1851.5892091691494
forward train acc: top1 ->  46.54999877929688 ; top5 ->  71.74166425704956  and loss:  481.13932383060455
test acc: top1 ->  55.093998441123965 ; top5 ->  79.6339976272583  and loss:  1604.3425381928682
forward train acc: top1 ->  50.316665496826175 ; top5 ->  73.55833080291748  and loss:  445.99906849861145
test acc: top1 ->  55.53599842219353 ; top5 ->  80.17199750366211  and loss:  1580.9526825100183
forward train acc: top1 ->  50.90833206176758 ; top5 ->  74.874997215271  and loss:  435.6209354400635
test acc: top1 ->  56.00999842338562 ; top5 ->  80.59599764709472  and loss:  1561.2650794684887
forward train acc: top1 ->  51.43333194732666 ; top5 ->  75.57499742507935  and loss:  428.0480123758316
test acc: top1 ->  58.417998315048216 ; top5 ->  81.86999759292603  and loss:  1471.2232499420643
forward train acc: top1 ->  51.974998760223386 ; top5 ->  75.78333099365234  and loss:  420.66141760349274
test acc: top1 ->  58.919998252105714 ; top5 ->  82.42999744567871  and loss:  1446.6753703802824
forward train acc: top1 ->  53.67499864578247 ; top5 ->  76.43333066940308  and loss:  414.4158388376236
test acc: top1 ->  59.289998276138306 ; top5 ->  82.64599753112793  and loss:  1436.166248112917
forward train acc: top1 ->  53.833332023620606 ; top5 ->  77.0999973487854  and loss:  406.90311205387115
test acc: top1 ->  60.07199831085205 ; top5 ->  83.22599771270752  and loss:  1403.0279105007648
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  122 / 128 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -100.95651602745056 , diff:  100.95651602745056
adv train loss:  -103.12684500217438 , diff:  2.170328974723816
adv train loss:  -102.97704243659973 , diff:  0.149802565574646
layer  8  adv train finish, try to retain  106
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -99.08141040802002 , diff:  99.08141040802002
adv train loss:  -101.2755298614502 , diff:  2.194119453430176
adv train loss:  -98.3902199268341 , diff:  2.885309934616089
adv train loss:  -103.73817944526672 , diff:  5.347959518432617
adv train loss:  -101.47425198554993 , diff:  2.263927459716797
adv train loss:  -98.48033809661865 , diff:  2.9939138889312744
adv train loss:  -102.14185917377472 , diff:  3.661521077156067
adv train loss:  -102.65280902385712 , diff:  0.5109498500823975
layer  9  adv train finish, try to retain  115
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -101.04356336593628 , diff:  101.04356336593628
adv train loss:  -98.94866836071014 , diff:  2.0948950052261353
adv train loss:  -100.60870683193207 , diff:  1.6600384712219238
adv train loss:  -98.24031460285187 , diff:  2.3683922290802
adv train loss:  -99.67537748813629 , diff:  1.4350628852844238
adv train loss:  -100.2501425743103 , diff:  0.5747650861740112
layer  10  adv train finish, try to retain  114
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -103.52454793453217 , diff:  103.52454793453217
adv train loss:  -102.6538393497467 , diff:  0.8707085847854614
layer  11  adv train finish, try to retain  112
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -101.4149329662323 , diff:  101.4149329662323
adv train loss:  -103.9233500957489 , diff:  2.5084171295166016
adv train loss:  -98.54149031639099 , diff:  5.38185977935791
adv train loss:  -98.08456516265869 , diff:  0.4569251537322998
layer  12  adv train finish, try to retain  111
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -101.74677050113678 , diff:  101.74677050113678
adv train loss:  -103.34353792667389 , diff:  1.5967674255371094
adv train loss:  -105.46800148487091 , diff:  2.1244635581970215
adv train loss:  -102.48015654087067 , diff:  2.987844944000244
adv train loss:  -104.38593411445618 , diff:  1.9057775735855103
adv train loss:  -101.30970621109009 , diff:  3.076227903366089
adv train loss:  -106.5515102148056 , diff:  5.241804003715515
adv train loss:  -99.11121392250061 , diff:  7.440296292304993
adv train loss:  -102.94773399829865 , diff:  3.8365200757980347
adv train loss:  -104.1288697719574 , diff:  1.1811357736587524
layer  13  adv train finish, try to retain  120
>>>>>>> reverse layer  13  since no improvement >>>>>>>
---------------- start layer  14  ---------------
adv train loss:  -102.48632061481476 , diff:  102.48632061481476
adv train loss:  -101.61726880073547 , diff:  0.8690518140792847
************ all values are small in this layer **********
layer  14  adv train finish, try to retain  249
test acc: top1 ->  59.45399829559326 ; top5 ->  82.79999754333497  and loss:  1427.5579197853804
forward train acc: top1 ->  46.53333203315735 ; top5 ->  71.39166425704956  and loss:  483.60795283317566
test acc: top1 ->  50.42999854135513 ; top5 ->  76.5219976776123  and loss:  1799.2193657159805
forward train acc: top1 ->  44.2499987411499 ; top5 ->  70.3833310508728  and loss:  496.86669754981995
test acc: top1 ->  49.15799864616394 ; top5 ->  75.56799770355225  and loss:  1857.1804992556572
forward train acc: top1 ->  44.31666550636292 ; top5 ->  69.89999771118164  and loss:  504.42175447940826
test acc: top1 ->  50.985998584747314 ; top5 ->  76.73999755096436  and loss:  1777.19800696522
forward train acc: top1 ->  48.32499879837036 ; top5 ->  72.55833082199096  and loss:  462.5830434560776
test acc: top1 ->  55.419998419189454 ; top5 ->  80.14199756317139  and loss:  1582.2232733666897
forward train acc: top1 ->  49.98333192825317 ; top5 ->  74.33333078384399  and loss:  446.4380885362625
test acc: top1 ->  56.105998435401915 ; top5 ->  80.57599747467042  and loss:  1561.9881686270237
forward train acc: top1 ->  50.7416654586792 ; top5 ->  74.84166418075561  and loss:  439.15728986263275
test acc: top1 ->  56.50199833106995 ; top5 ->  80.72199767913818  and loss:  1536.6877062618732
forward train acc: top1 ->  51.85833206176758 ; top5 ->  75.766664352417  and loss:  428.10854256153107
test acc: top1 ->  58.56399836769104 ; top5 ->  82.37999764633179  and loss:  1456.7030028700829
forward train acc: top1 ->  53.85833209037781 ; top5 ->  76.83333055496216  and loss:  409.8290991783142
test acc: top1 ->  59.04199832839966 ; top5 ->  82.58999751739502  and loss:  1440.0135363936424
forward train acc: top1 ->  53.52499872207642 ; top5 ->  76.73333076477051  and loss:  410.6212201118469
test acc: top1 ->  59.83799831733704 ; top5 ->  82.96599753875732  and loss:  1417.7404690682888
forward train acc: top1 ->  54.341665496826174 ; top5 ->  77.97499767303466  and loss:  399.78759372234344
test acc: top1 ->  60.53799831314087 ; top5 ->  83.66399762573242  and loss:  1384.669740781188
>>>>>>> reverse layer  14  since performance drop >>>>>>>
==> this epoch:  250 / 256 , inc:  1
---------------- start layer  15  ---------------
adv train loss:  -100.91644883155823 , diff:  100.91644883155823
adv train loss:  -100.16699886322021 , diff:  0.7494499683380127
************ all values are small in this layer **********
layer  15  adv train finish, try to retain  247
test acc: top1 ->  59.89399828796387 ; top5 ->  83.18599759979249  and loss:  1409.006631910801
forward train acc: top1 ->  47.07499878883362 ; top5 ->  71.44999740600586  and loss:  479.79938519001007
test acc: top1 ->  51.15199859676361 ; top5 ->  77.37999771575927  and loss:  1770.3211387395859
forward train acc: top1 ->  45.67499877929688 ; top5 ->  70.7166644668579  and loss:  488.749475479126
test acc: top1 ->  50.5039985830307 ; top5 ->  76.47399772644043  and loss:  1802.0863262712955
forward train acc: top1 ->  45.74999875068664 ; top5 ->  70.47499786376953  and loss:  491.8132485151291
test acc: top1 ->  50.51199853563309 ; top5 ->  76.90999767074585  and loss:  1780.4667279720306
forward train acc: top1 ->  49.81666533470154 ; top5 ->  73.58333110809326  and loss:  456.1255555152893
test acc: top1 ->  56.01399841079712 ; top5 ->  80.47599763717652  and loss:  1563.6451611965895
forward train acc: top1 ->  50.516665363311766 ; top5 ->  74.73333089828492  and loss:  440.8064122200012
test acc: top1 ->  56.54599846477509 ; top5 ->  80.8799975112915  and loss:  1539.1178516745567
forward train acc: top1 ->  52.00833200454712 ; top5 ->  75.41666412353516  and loss:  432.40916085243225
test acc: top1 ->  56.27399837760925 ; top5 ->  80.87999753875732  and loss:  1540.100484535098
forward train acc: top1 ->  52.21666538238525 ; top5 ->  76.224997215271  and loss:  419.81857669353485
test acc: top1 ->  58.94999837341309 ; top5 ->  82.43599748229981  and loss:  1444.7495353519917
forward train acc: top1 ->  54.19999877929688 ; top5 ->  77.17499769210815  and loss:  404.62493109703064
test acc: top1 ->  59.27599836940765 ; top5 ->  82.83599754257202  and loss:  1428.058244511485
forward train acc: top1 ->  54.36666547775268 ; top5 ->  77.08333101272584  and loss:  404.8585579395294
test acc: top1 ->  59.90399830780029 ; top5 ->  83.12599753265381  and loss:  1399.0033214986324
forward train acc: top1 ->  54.766665344238284 ; top5 ->  77.91666404724121  and loss:  400.10181856155396
test acc: top1 ->  60.61199825706482 ; top5 ->  83.62599750823975  and loss:  1376.3062009364367
>>>>>>> reverse layer  15  since performance drop >>>>>>>
==> this epoch:  248 / 256 , inc:  1
---------------- start layer  16  ---------------
adv train loss:  -98.95385265350342 , diff:  98.95385265350342
adv train loss:  -96.97112429141998 , diff:  1.982728362083435
adv train loss:  -97.97342109680176 , diff:  1.002296805381775
adv train loss:  -99.96532583236694 , diff:  1.9919047355651855
adv train loss:  -101.23091864585876 , diff:  1.2655928134918213
adv train loss:  -100.44099366664886 , diff:  0.7899249792098999
layer  16  adv train finish, try to retain  244
>>>>>>> reverse layer  16  since no improvement >>>>>>>
---------------- start layer  17  ---------------
adv train loss:  -93.94339787960052 , diff:  93.94339787960052
adv train loss:  -97.77172553539276 , diff:  3.8283276557922363
adv train loss:  -99.55296218395233 , diff:  1.7812366485595703
adv train loss:  -98.25697612762451 , diff:  1.2959860563278198
adv train loss:  -101.786447763443 , diff:  3.5294716358184814
adv train loss:  -98.13760232925415 , diff:  3.6488454341888428
adv train loss:  -97.66933798789978 , diff:  0.4682643413543701
layer  17  adv train finish, try to retain  235
>>>>>>> reverse layer  17  since no improvement >>>>>>>
---------------- start layer  18  ---------------
adv train loss:  -100.50512063503265 , diff:  100.50512063503265
adv train loss:  -97.15138375759125 , diff:  3.3537368774414062
adv train loss:  -99.33157551288605 , diff:  2.1801917552948
adv train loss:  -99.45319759845734 , diff:  0.12162208557128906
layer  18  adv train finish, try to retain  236
>>>>>>> reverse layer  18  since no improvement >>>>>>>
---------------- start layer  19  ---------------
adv train loss:  -96.16683769226074 , diff:  96.16683769226074
adv train loss:  -100.72331821918488 , diff:  4.556480526924133
adv train loss:  -97.02201354503632 , diff:  3.7013046741485596
adv train loss:  -96.41835308074951 , diff:  0.6036604642868042
layer  19  adv train finish, try to retain  240
>>>>>>> reverse layer  19  since no improvement >>>>>>>
---------------- start layer  20  ---------------
adv train loss:  -99.91013848781586 , diff:  99.91013848781586
adv train loss:  -103.91229450702667 , diff:  4.002156019210815
adv train loss:  -97.50465106964111 , diff:  6.407643437385559
adv train loss:  -100.90792167186737 , diff:  3.4032706022262573
adv train loss:  -99.03771686553955 , diff:  1.8702048063278198
adv train loss:  -99.47977006435394 , diff:  0.4420531988143921
layer  20  adv train finish, try to retain  239
>>>>>>> reverse layer  20  since no improvement >>>>>>>
---------------- start layer  21  ---------------
adv train loss:  -99.97850382328033 , diff:  99.97850382328033
adv train loss:  -97.9799154996872 , diff:  1.9985883235931396
adv train loss:  -98.16090106964111 , diff:  0.18098556995391846
layer  21  adv train finish, try to retain  238
test acc: top1 ->  59.997998320770265 ; top5 ->  83.28399750823975  and loss:  1392.7757535874844
forward train acc: top1 ->  47.28333208084106 ; top5 ->  71.98333112716675  and loss:  478.1223406791687
test acc: top1 ->  51.54399855289459 ; top5 ->  77.41199776000977  and loss:  1752.8040826320648
forward train acc: top1 ->  47.2999986076355 ; top5 ->  72.16666427612304  and loss:  472.88827764987946
test acc: top1 ->  51.15799860811234 ; top5 ->  76.95599784927369  and loss:  1759.7919206023216
forward train acc: top1 ->  46.24999873161316 ; top5 ->  71.42499753952026  and loss:  483.7095400094986
test acc: top1 ->  50.89999854812622 ; top5 ->  76.7619977294922  and loss:  1771.7735486924648
forward train acc: top1 ->  48.84166538238525 ; top5 ->  73.33333076477051  and loss:  457.24582731723785
test acc: top1 ->  56.22599846839905 ; top5 ->  80.57799756698608  and loss:  1547.5408730208874
forward train acc: top1 ->  51.63333204269409 ; top5 ->  75.47499773025513  and loss:  428.6034016609192
test acc: top1 ->  56.88799842243195 ; top5 ->  81.02999762878417  and loss:  1524.713287010789
forward train acc: top1 ->  51.3833320236206 ; top5 ->  75.5583307647705  and loss:  430.63606667518616
test acc: top1 ->  57.27199834022522 ; top5 ->  81.36399761657715  and loss:  1509.2384622991085
forward train acc: top1 ->  53.566665477752686 ; top5 ->  77.00833074569702  and loss:  408.0401095151901
test acc: top1 ->  59.15799828414917 ; top5 ->  82.47999765167236  and loss:  1436.0046897679567
forward train acc: top1 ->  53.97499868392944 ; top5 ->  77.04999713897705  and loss:  407.98971354961395
test acc: top1 ->  59.653998336410524 ; top5 ->  83.01599743041992  and loss:  1418.4652370363474
forward train acc: top1 ->  54.749998779296874 ; top5 ->  77.39999753952026  and loss:  403.8246548175812
test acc: top1 ->  60.107998314285275 ; top5 ->  83.21399744567871  and loss:  1397.8929096907377
forward train acc: top1 ->  55.00833204269409 ; top5 ->  78.24166395187378  and loss:  395.77450239658356
test acc: top1 ->  60.87199819831848 ; top5 ->  83.64399742889404  and loss:  1370.7434693425894
>>>>>>> reverse layer  21  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  22  ---------------
adv train loss:  -97.31645917892456 , diff:  97.31645917892456
adv train loss:  -97.19368982315063 , diff:  0.12276935577392578
layer  22  adv train finish, try to retain  231
>>>>>>> reverse layer  22  since no improvement >>>>>>>
---------------- start layer  23  ---------------
adv train loss:  -95.70396757125854 , diff:  95.70396757125854
adv train loss:  -101.24032616615295 , diff:  5.536358594894409
adv train loss:  -95.55999076366425 , diff:  5.6803354024887085
adv train loss:  -98.20337855815887 , diff:  2.643387794494629
adv train loss:  -102.57778406143188 , diff:  4.37440550327301
adv train loss:  -94.68213725090027 , diff:  7.895646810531616
adv train loss:  -96.0787125825882 , diff:  1.3965753316879272
adv train loss:  -98.00524628162384 , diff:  1.9265336990356445
adv train loss:  -98.33792793750763 , diff:  0.33268165588378906
layer  23  adv train finish, try to retain  237
>>>>>>> reverse layer  23  since no improvement >>>>>>>
---------------- start layer  24  ---------------
adv train loss:  -99.11600160598755 , diff:  99.11600160598755
adv train loss:  -98.34732806682587 , diff:  0.7686735391616821
layer  24  adv train finish, try to retain  240
test acc: top1 ->  60.743998257446286 ; top5 ->  83.55599754638672  and loss:  1377.761429399252
forward train acc: top1 ->  48.633332061767575 ; top5 ->  73.3499976158142  and loss:  461.5732637643814
test acc: top1 ->  51.871998540878295 ; top5 ->  77.34199766769409  and loss:  1746.291648954153
forward train acc: top1 ->  47.608332042694094 ; top5 ->  72.13333110809326  and loss:  471.69068121910095
test acc: top1 ->  52.96199863348007 ; top5 ->  78.10599759521484  and loss:  1696.3821789175272
forward train acc: top1 ->  46.699998741149905 ; top5 ->  70.82499765396118  and loss:  482.3966989517212
test acc: top1 ->  52.13199851932526 ; top5 ->  77.78399773254395  and loss:  1717.7181330621243
forward train acc: top1 ->  50.04166547775269 ; top5 ->  74.12499750137329  and loss:  445.454540848732
test acc: top1 ->  56.29799847521782 ; top5 ->  80.83199765930176  and loss:  1541.5090534687042
forward train acc: top1 ->  51.37499862670899 ; top5 ->  75.09166412353515  and loss:  431.42676854133606
test acc: top1 ->  57.22999833202362 ; top5 ->  81.38799769592285  and loss:  1508.6481730639935
forward train acc: top1 ->  51.18333200454712 ; top5 ->  75.59166410446167  and loss:  429.88188803195953
test acc: top1 ->  57.49199843521118 ; top5 ->  81.5339975982666  and loss:  1500.027462437749
forward train acc: top1 ->  53.06666551589966 ; top5 ->  76.4499974822998  and loss:  411.3415470123291
test acc: top1 ->  59.43799829483032 ; top5 ->  82.93999767303467  and loss:  1420.9827589392662
forward train acc: top1 ->  54.39166530609131 ; top5 ->  77.64166421890259  and loss:  400.226744890213
test acc: top1 ->  59.823998246574405 ; top5 ->  83.16999755859375  and loss:  1401.2211825102568
forward train acc: top1 ->  54.44166551589966 ; top5 ->  77.30833093643189  and loss:  402.8819909095764
test acc: top1 ->  60.05199825286865 ; top5 ->  83.45599762420655  and loss:  1391.4350247085094
forward train acc: top1 ->  55.78333198547363 ; top5 ->  78.0583309173584  and loss:  393.08508372306824
test acc: top1 ->  61.031998210906984 ; top5 ->  84.02399752044678  and loss:  1358.5043614059687
>>>>>>> reverse layer  24  since performance drop >>>>>>>
==> this epoch:  244 / 256 , inc:  1
---------------- start layer  25  ---------------
adv train loss:  -95.05445337295532 , diff:  95.05445337295532
adv train loss:  -95.72653245925903 , diff:  0.6720790863037109
layer  25  adv train finish, try to retain  244
>>>>>>> reverse layer  25  since no improvement >>>>>>>
---------------- start layer  26  ---------------
adv train loss:  -96.70172238349915 , diff:  96.70172238349915
adv train loss:  -96.22866749763489 , diff:  0.4730548858642578
layer  26  adv train finish, try to retain  506
>>>>>>> reverse layer  26  since no improvement >>>>>>>
---------------- start layer  27  ---------------
adv train loss:  -95.87391924858093 , diff:  95.87391924858093
adv train loss:  -102.21338963508606 , diff:  6.339470386505127
adv train loss:  -101.485999584198 , diff:  0.7273900508880615
layer  27  adv train finish, try to retain  495
test acc: top1 ->  60.695998118209836 ; top5 ->  83.66999746551514  and loss:  1372.4297268688679
forward train acc: top1 ->  48.7749988079071 ; top5 ->  72.39999761581421  and loss:  467.1460438966751
test acc: top1 ->  53.91399853000641 ; top5 ->  79.12399770431519  and loss:  1647.0633537322283
forward train acc: top1 ->  47.08333215713501 ; top5 ->  72.22499771118164  and loss:  475.7517354488373
test acc: top1 ->  52.94999851140976 ; top5 ->  78.45399765014649  and loss:  1684.0711219906807
forward train acc: top1 ->  47.54999877929688 ; top5 ->  72.79999738693238  and loss:  470.0161974430084
test acc: top1 ->  52.549998543071744 ; top5 ->  78.27199769515991  and loss:  1697.8104756176472
forward train acc: top1 ->  50.60833198547363 ; top5 ->  74.69166399002076  and loss:  439.478884100914
test acc: top1 ->  57.059998412132266 ; top5 ->  81.10999756469727  and loss:  1520.3594352602959
forward train acc: top1 ->  51.89166543006897 ; top5 ->  75.64166414260865  and loss:  427.2855712175369
test acc: top1 ->  57.60999838829041 ; top5 ->  81.80199760665893  and loss:  1491.756234973669
forward train acc: top1 ->  51.9083320236206 ; top5 ->  75.71666416168213  and loss:  424.57128620147705
test acc: top1 ->  57.95999828643799 ; top5 ->  81.7279976486206  and loss:  1480.5570011287928
forward train acc: top1 ->  52.99999875068664 ; top5 ->  76.62499731063843  and loss:  415.2916634082794
test acc: top1 ->  59.76999837036133 ; top5 ->  83.13599751281738  and loss:  1408.0236260294914
forward train acc: top1 ->  54.791665363311765 ; top5 ->  77.49999738693238  and loss:  399.5642411708832
test acc: top1 ->  60.567998237609864 ; top5 ->  83.4419974899292  and loss:  1387.2293654978275
forward train acc: top1 ->  54.64166540145874 ; top5 ->  77.78333089828492  and loss:  396.7059758901596
test acc: top1 ->  60.58799841766358 ; top5 ->  83.60999739379884  and loss:  1382.6745036244392
forward train acc: top1 ->  55.38333192825317 ; top5 ->  77.89999725341796  and loss:  395.3912845849991
test acc: top1 ->  61.3899981918335 ; top5 ->  84.05799757995605  and loss:  1351.735502243042
>>>>>>> reverse layer  27  since performance drop >>>>>>>
==> this epoch:  505 / 512 , inc:  1
---------------- start layer  28  ---------------
adv train loss:  -97.62454605102539 , diff:  97.62454605102539
adv train loss:  -99.21916925907135 , diff:  1.5946232080459595
adv train loss:  -97.36449980735779 , diff:  1.854669451713562
adv train loss:  -98.35652351379395 , diff:  0.9920237064361572
layer  28  adv train finish, try to retain  501
test acc: top1 ->  61.341998260116576 ; top5 ->  83.99199764404297  and loss:  1357.4000066071749
forward train acc: top1 ->  48.724998750686645 ; top5 ->  73.1749973678589  and loss:  461.53836953639984
test acc: top1 ->  53.41199856414795 ; top5 ->  78.85999761886596  and loss:  1663.8443395197392
forward train acc: top1 ->  48.30833199501038 ; top5 ->  72.98333082199096  and loss:  462.5676648616791
test acc: top1 ->  52.64599850091934 ; top5 ->  78.20999759674072  and loss:  1701.0447672903538
forward train acc: top1 ->  48.09999869346618 ; top5 ->  72.89999757766724  and loss:  465.8277106285095
test acc: top1 ->  53.70799842634201 ; top5 ->  78.90999764404297  and loss:  1663.5132939517498
forward train acc: top1 ->  50.47499881744385 ; top5 ->  74.89999769210816  and loss:  437.64726412296295
test acc: top1 ->  57.885998358917234 ; top5 ->  81.81399743804931  and loss:  1488.5662100613117
forward train acc: top1 ->  52.6249986076355 ; top5 ->  76.16666397094727  and loss:  420.4728021621704
test acc: top1 ->  58.51399833183289 ; top5 ->  82.17599759216309  and loss:  1472.0522527098656
forward train acc: top1 ->  53.358331966400144 ; top5 ->  76.74166408538818  and loss:  414.0980052947998
test acc: top1 ->  58.20799841842651 ; top5 ->  82.08599744262695  and loss:  1473.804468691349
forward train acc: top1 ->  54.31666532516479 ; top5 ->  77.04999740600586  and loss:  409.5312043428421
test acc: top1 ->  60.08799829864502 ; top5 ->  83.26599754028321  and loss:  1398.7068796902895
forward train acc: top1 ->  55.374998779296874 ; top5 ->  77.97499753952026  and loss:  398.4516805410385
test acc: top1 ->  60.785998225402835 ; top5 ->  83.62799737701415  and loss:  1376.2314241677523
forward train acc: top1 ->  54.474998760223386 ; top5 ->  77.64999759674072  and loss:  401.6581964492798
test acc: top1 ->  61.09999825248718 ; top5 ->  83.86599743652344  and loss:  1363.015004813671
forward train acc: top1 ->  55.524998626708985 ; top5 ->  78.86666389465331  and loss:  386.0332626104355
test acc: top1 ->  61.56999826202392 ; top5 ->  84.18999742126465  and loss:  1345.3034357130527
>>>>>>> reverse layer  28  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  29  ---------------
adv train loss:  -95.52965950965881 , diff:  95.52965950965881
adv train loss:  -95.54335308074951 , diff:  0.013693571090698242
layer  29  adv train finish, try to retain  488
test acc: top1 ->  61.52399823760987 ; top5 ->  84.11999759674072  and loss:  1347.4510217010975
forward train acc: top1 ->  49.79166551589966 ; top5 ->  73.54166410446167  and loss:  456.2759586572647
test acc: top1 ->  54.44799846191406 ; top5 ->  79.68599758605957  and loss:  1632.4996052831411
forward train acc: top1 ->  47.74166542053223 ; top5 ->  72.54166423797608  and loss:  465.4689985513687
test acc: top1 ->  53.60599851465225 ; top5 ->  78.91599764556885  and loss:  1653.97355183959
forward train acc: top1 ->  48.391665363311766 ; top5 ->  73.01666431427002  and loss:  461.64325761795044
test acc: top1 ->  53.09799852766991 ; top5 ->  78.08399763412476  and loss:  1690.1483702361584
forward train acc: top1 ->  50.54166537284851 ; top5 ->  75.0249976348877  and loss:  436.6568557024002
test acc: top1 ->  58.06599836406708 ; top5 ->  81.9539975402832  and loss:  1483.9134730994701
forward train acc: top1 ->  52.191665496826175 ; top5 ->  75.70833101272584  and loss:  424.52830719947815
test acc: top1 ->  58.3959982460022 ; top5 ->  82.26999761047364  and loss:  1464.872300580144
forward train acc: top1 ->  53.666665401458744 ; top5 ->  76.77499759674072  and loss:  409.13211727142334
test acc: top1 ->  58.45599826660156 ; top5 ->  82.18199760131836  and loss:  1465.474591165781
forward train acc: top1 ->  54.45833206176758 ; top5 ->  77.16666402816773  and loss:  402.5660421848297
test acc: top1 ->  60.281998274993896 ; top5 ->  83.31799763793946  and loss:  1393.2483979314566
forward train acc: top1 ->  55.1583320236206 ; top5 ->  78.30833068847656  and loss:  392.3028256893158
test acc: top1 ->  60.86599831609726 ; top5 ->  83.67599753189087  and loss:  1373.6797609776258
forward train acc: top1 ->  54.983331966400144 ; top5 ->  78.00833099365235  and loss:  397.02440762519836
test acc: top1 ->  61.10799820632935 ; top5 ->  83.79599746551514  and loss:  1364.238669037819
forward train acc: top1 ->  56.558331909179685 ; top5 ->  78.1833306312561  and loss:  386.9973088502884
test acc: top1 ->  61.78599825057984 ; top5 ->  84.46799753417969  and loss:  1337.8772258609533
>>>>>>> reverse layer  29  since performance drop >>>>>>>
==> this epoch:  498 / 512 , inc:  1
---------------- start layer  30  ---------------
adv train loss:  -95.83928453922272 , diff:  95.83928453922272
adv train loss:  -96.67362701892853 , diff:  0.8343424797058105
layer  30  adv train finish, try to retain  505
>>>>>>> reverse layer  30  since no improvement >>>>>>>
---------------- start layer  31  ---------------
adv train loss:  -99.93132519721985 , diff:  99.93132519721985
adv train loss:  -93.399853348732 , diff:  6.531471848487854
adv train loss:  -99.56577622890472 , diff:  6.1659228801727295
adv train loss:  -97.04154872894287 , diff:  2.524227499961853
adv train loss:  -95.7042669057846 , diff:  1.3372818231582642
adv train loss:  -96.45128631591797 , diff:  0.7470194101333618
layer  31  adv train finish, try to retain  504
>>>>>>> reverse layer  31  since no improvement >>>>>>>
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.0025, 0.0025, 0.0009375, 0.0025, 0.0025, 0.0025, 0.00125, 0.00017578125, 0.00125, 0.00125, 0.00125, 0.00125, 0.00125, 0.00125, 8.7890625e-05, 8.7890625e-05, 0.000625, 0.000625, 0.000625, 0.000625, 0.000625, 0.000234375, 0.000625, 0.000625, 0.000234375, 0.000625, 0.0001171875, 0.0001171875, 4.39453125e-05, 0.0001171875, 0.0001171875, 0.0001171875]  wait [0, 0, 2, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 4, 4, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 2, 2, 4, 2, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 0
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  4  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -93.85260510444641 , diff:  93.85260510444641
adv train loss:  -96.01725459098816 , diff:  2.164649486541748
adv train loss:  -97.40216934680939 , diff:  1.384914755821228
adv train loss:  -96.73982512950897 , diff:  0.662344217300415
layer  0  adv train finish, try to retain  53
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -94.93574273586273 , diff:  94.93574273586273
adv train loss:  -99.51297199726105 , diff:  4.577229261398315
adv train loss:  -94.51161658763885 , diff:  5.001355409622192
adv train loss:  -95.90605878829956 , diff:  1.3944422006607056
adv train loss:  -99.82444489002228 , diff:  3.9183861017227173
adv train loss:  -96.26440405845642 , diff:  3.560040831565857
adv train loss:  -96.01911115646362 , diff:  0.24529290199279785
layer  1  adv train finish, try to retain  52
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -94.74545180797577 , diff:  94.74545180797577
adv train loss:  -96.48972547054291 , diff:  1.7442736625671387
adv train loss:  -96.02432203292847 , diff:  0.4654034376144409
layer  2  adv train finish, try to retain  55
test acc: top1 ->  61.41199824180603 ; top5 ->  84.03999753265381  and loss:  1354.2879091203213
forward train acc: top1 ->  49.949998712539674 ; top5 ->  73.81666416168213  and loss:  445.54380333423615
test acc: top1 ->  53.84199849433899 ; top5 ->  78.9259976638794  and loss:  1661.5322031974792
forward train acc: top1 ->  49.06666527748108 ; top5 ->  73.22499774932861  and loss:  458.3005483150482
test acc: top1 ->  54.259998507118226 ; top5 ->  79.2039977256775  and loss:  1645.1023973673582
forward train acc: top1 ->  49.56666535377502 ; top5 ->  73.74166435241699  and loss:  454.8914371728897
test acc: top1 ->  54.143998450279234 ; top5 ->  79.0219977508545  and loss:  1652.3218204975128
forward train acc: top1 ->  52.04166528701782 ; top5 ->  75.99999755859375  and loss:  424.3132554292679
test acc: top1 ->  57.6819982925415 ; top5 ->  81.68199751434327  and loss:  1502.170868769288
forward train acc: top1 ->  53.541665439605715 ; top5 ->  76.96666442871094  and loss:  412.453831076622
test acc: top1 ->  58.70599835700989 ; top5 ->  82.35599750213623  and loss:  1464.1657592356205
forward train acc: top1 ->  53.341665210723875 ; top5 ->  75.83333097457886  and loss:  414.73394763469696
test acc: top1 ->  58.56199833908081 ; top5 ->  82.50199756469726  and loss:  1457.0161311477423
forward train acc: top1 ->  54.39999879837036 ; top5 ->  77.82499740600586  and loss:  401.8532508611679
test acc: top1 ->  60.39999826927185 ; top5 ->  83.53599758911133  and loss:  1384.3288723677397
forward train acc: top1 ->  55.29999872207642 ; top5 ->  78.1749975013733  and loss:  392.24953615665436
test acc: top1 ->  60.98199829044342 ; top5 ->  83.92999744415283  and loss:  1365.8724758327007
forward train acc: top1 ->  55.2749987411499 ; top5 ->  78.09999725341797  and loss:  392.0115703344345
test acc: top1 ->  61.32799820632935 ; top5 ->  83.98599754333496  and loss:  1357.9552827328444
forward train acc: top1 ->  56.92499870300293 ; top5 ->  79.44999765396118  and loss:  374.2264802455902
test acc: top1 ->  61.79399829788208 ; top5 ->  84.36799745941163  and loss:  1334.3258904665709
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  57 / 64 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -98.10717129707336 , diff:  98.10717129707336
adv train loss:  -96.03083324432373 , diff:  2.076338052749634
adv train loss:  -100.0641438961029 , diff:  4.033310651779175
adv train loss:  -99.60777580738068 , diff:  0.456368088722229
layer  3  adv train finish, try to retain  52
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -93.27996516227722 , diff:  93.27996516227722
adv train loss:  -95.41517519950867 , diff:  2.1352100372314453
adv train loss:  -93.63932192325592 , diff:  1.7758532762527466
adv train loss:  -94.31300055980682 , diff:  0.6736786365509033
layer  4  adv train finish, try to retain  50
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -97.91613352298737 , diff:  97.91613352298737
adv train loss:  -100.41925287246704 , diff:  2.5031193494796753
adv train loss:  -97.52514326572418 , diff:  2.894109606742859
adv train loss:  -97.77342414855957 , diff:  0.24828088283538818
layer  5  adv train finish, try to retain  57
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -93.707998752594 , diff:  93.707998752594
adv train loss:  -95.28886389732361 , diff:  1.5808651447296143
adv train loss:  -95.2128871679306 , diff:  0.07597672939300537
layer  6  adv train finish, try to retain  107
test acc: top1 ->  58.4299983543396 ; top5 ->  81.86999751739502  and loss:  1469.2181413918734
forward train acc: top1 ->  49.6666654586792 ; top5 ->  74.14999736785889  and loss:  448.82209372520447
test acc: top1 ->  55.25399844551087 ; top5 ->  79.98599763870239  and loss:  1594.3292615115643
forward train acc: top1 ->  50.08333206176758 ; top5 ->  74.56666412353516  and loss:  440.9029166698456
test acc: top1 ->  54.37399850196839 ; top5 ->  79.58799740447998  and loss:  1623.9836681038141
forward train acc: top1 ->  50.00833200454712 ; top5 ->  74.1583309173584  and loss:  449.1092120409012
test acc: top1 ->  54.5899984629631 ; top5 ->  79.65599764709472  and loss:  1620.2775243222713
forward train acc: top1 ->  51.85833192825317 ; top5 ->  75.39166418075561  and loss:  430.0181301832199
test acc: top1 ->  57.909998342323306 ; top5 ->  81.8959976135254  and loss:  1485.22063075006
forward train acc: top1 ->  53.28333194732666 ; top5 ->  76.63333040237427  and loss:  410.6799943447113
test acc: top1 ->  58.29199841194153 ; top5 ->  82.04199755325317  and loss:  1465.1947310715914
forward train acc: top1 ->  53.46666540145874 ; top5 ->  76.94166429519653  and loss:  411.099786400795
test acc: top1 ->  59.08999830656052 ; top5 ->  82.6879975189209  and loss:  1433.660074532032
forward train acc: top1 ->  54.57499877929688 ; top5 ->  77.2416641998291  and loss:  399.1254663467407
test acc: top1 ->  60.75999823608399 ; top5 ->  83.6659975265503  and loss:  1373.984450802207
forward train acc: top1 ->  56.2833320236206 ; top5 ->  78.61666412353516  and loss:  385.201496720314
test acc: top1 ->  61.06599826316833 ; top5 ->  83.81999748840332  and loss:  1366.0434290468693
forward train acc: top1 ->  56.116665306091306 ; top5 ->  78.74166408538818  and loss:  386.39432966709137
test acc: top1 ->  61.06799829292297 ; top5 ->  83.78399744720458  and loss:  1362.6965887993574
forward train acc: top1 ->  56.51666528701782 ; top5 ->  79.17499738693238  and loss:  382.1884231567383
test acc: top1 ->  62.09599825325012 ; top5 ->  84.32599749450684  and loss:  1326.9200303852558
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  112 / 128 , inc:  1
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
adv train loss:  -96.77585458755493 , diff:  96.77585458755493
adv train loss:  -90.30645203590393 , diff:  6.469402551651001
adv train loss:  -95.15103673934937 , diff:  4.844584703445435
adv train loss:  -97.10155129432678 , diff:  1.950514554977417
adv train loss:  -93.03222918510437 , diff:  4.069322109222412
adv train loss:  -96.75403046607971 , diff:  3.721801280975342
adv train loss:  -94.55927670001984 , diff:  2.1947537660598755
adv train loss:  -94.33398079872131 , diff:  0.22529590129852295
layer  8  adv train finish, try to retain  112
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -97.45055997371674 , diff:  97.45055997371674
adv train loss:  -90.54037511348724 , diff:  6.910184860229492
adv train loss:  -91.98223900794983 , diff:  1.4418638944625854
adv train loss:  -93.24366688728333 , diff:  1.261427879333496
adv train loss:  -96.1626695394516 , diff:  2.919002652168274
adv train loss:  -96.92899811267853 , diff:  0.7663285732269287
layer  9  adv train finish, try to retain  113
test acc: top1 ->  60.61599822731018 ; top5 ->  83.44999764709473  and loss:  1382.7267264723778
forward train acc: top1 ->  50.5249987411499 ; top5 ->  74.85833076477051  and loss:  437.3632595539093
test acc: top1 ->  55.04799840297699 ; top5 ->  79.83599753799439  and loss:  1597.3414808362722
forward train acc: top1 ->  49.799998655319214 ; top5 ->  74.06666400909424  and loss:  449.9076761007309
test acc: top1 ->  53.76799845275879 ; top5 ->  79.07599767837524  and loss:  1656.24642431736
forward train acc: top1 ->  50.74999876022339 ; top5 ->  74.36666433334351  and loss:  444.8572566509247
test acc: top1 ->  54.59799849739075 ; top5 ->  79.57799763565063  and loss:  1622.8027395009995
forward train acc: top1 ->  52.199998722076415 ; top5 ->  76.04999757766724  and loss:  425.8538476228714
test acc: top1 ->  58.64999833221435 ; top5 ->  82.1639977027893  and loss:  1463.8174067288637
forward train acc: top1 ->  54.024998655319216 ; top5 ->  77.06666425704957  and loss:  410.5228087902069
test acc: top1 ->  58.94399829788208 ; top5 ->  82.57199756622315  and loss:  1445.388639435172
forward train acc: top1 ->  53.79166526794434 ; top5 ->  77.10833099365234  and loss:  411.1107808351517
test acc: top1 ->  59.313998257446286 ; top5 ->  82.71599737854004  and loss:  1433.6381775289774
forward train acc: top1 ->  55.608332023620605 ; top5 ->  78.56666391372681  and loss:  395.2543925046921
test acc: top1 ->  61.29199819145202 ; top5 ->  83.95399741668702  and loss:  1360.602355092764
forward train acc: top1 ->  56.20833208084107 ; top5 ->  78.49166402816772  and loss:  385.2708201408386
test acc: top1 ->  61.43999811058045 ; top5 ->  84.2619974975586  and loss:  1349.5302198976278
forward train acc: top1 ->  56.258332080841065 ; top5 ->  78.32499786376952  and loss:  385.7576984167099
test acc: top1 ->  61.743998164749144 ; top5 ->  84.32599756317138  and loss:  1340.892419680953
forward train acc: top1 ->  56.591665210723875 ; top5 ->  79.18333082199096  and loss:  378.1676230430603
test acc: top1 ->  62.25399824256897 ; top5 ->  84.58999750823975  and loss:  1318.5003440380096
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  115 / 128 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -93.02439749240875 , diff:  93.02439749240875
adv train loss:  -94.66361165046692 , diff:  1.6392141580581665
adv train loss:  -93.35850417613983 , diff:  1.3051074743270874
adv train loss:  -94.99838471412659 , diff:  1.6398805379867554
adv train loss:  -93.07707023620605 , diff:  1.9213144779205322
adv train loss:  -94.4309870004654 , diff:  1.3539167642593384
adv train loss:  -97.69363570213318 , diff:  3.2626487016677856
adv train loss:  -91.35396540164948 , diff:  6.339670300483704
adv train loss:  -96.79211843013763 , diff:  5.438153028488159
adv train loss:  -96.60401105880737 , diff:  0.18810737133026123
layer  10  adv train finish, try to retain  107
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -93.57573986053467 , diff:  93.57573986053467
adv train loss:  -95.02278649806976 , diff:  1.4470466375350952
adv train loss:  -96.37747883796692 , diff:  1.3546923398971558
adv train loss:  -95.47253465652466 , diff:  0.9049441814422607
layer  11  adv train finish, try to retain  110
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -92.66339719295502 , diff:  92.66339719295502
adv train loss:  -94.27931094169617 , diff:  1.61591374874115
adv train loss:  -95.48226988315582 , diff:  1.2029589414596558
adv train loss:  -96.2080374956131 , diff:  0.7257676124572754
layer  12  adv train finish, try to retain  105
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -94.07329797744751 , diff:  94.07329797744751
adv train loss:  -93.41122007369995 , diff:  0.6620779037475586
layer  13  adv train finish, try to retain  106
>>>>>>> reverse layer  13  since no improvement >>>>>>>
---------------- start layer  14  ---------------
### skip layer  14 wait:  4  ###
---------------- start layer  15  ---------------
### skip layer  15 wait:  4  ###
---------------- start layer  16  ---------------
adv train loss:  -94.31523430347443 , diff:  94.31523430347443
adv train loss:  -95.99424576759338 , diff:  1.6790114641189575
adv train loss:  -93.61386799812317 , diff:  2.380377769470215
adv train loss:  -97.24588906764984 , diff:  3.6320210695266724
adv train loss:  -96.17076504230499 , diff:  1.0751240253448486
adv train loss:  -91.85386681556702 , diff:  4.316898226737976
adv train loss:  -95.31264412403107 , diff:  3.4587773084640503
adv train loss:  -96.84360980987549 , diff:  1.5309656858444214
adv train loss:  -91.83468699455261 , diff:  5.008922815322876
adv train loss:  -94.62632036209106 , diff:  2.791633367538452
layer  16  adv train finish, try to retain  229
test acc: top1 ->  62.08999816036224 ; top5 ->  84.48999750976563  and loss:  1327.9228456318378
forward train acc: top1 ->  50.82499864578247 ; top5 ->  74.28333074569701  and loss:  439.98658215999603
test acc: top1 ->  55.36799834079743 ; top5 ->  80.03399752502442  and loss:  1585.250981092453
forward train acc: top1 ->  49.666665382385254 ; top5 ->  74.2666640472412  and loss:  447.6548867225647
test acc: top1 ->  55.541998467636105 ; top5 ->  80.2819976234436  and loss:  1572.1205240488052
forward train acc: top1 ->  49.98333207130432 ; top5 ->  73.84999759674072  and loss:  449.6180862188339
test acc: top1 ->  55.54199841423035 ; top5 ->  80.18799758453369  and loss:  1580.6329635381699
forward train acc: top1 ->  52.40833208084106 ; top5 ->  75.86666423797607  and loss:  418.54728412628174
test acc: top1 ->  59.45599832763672 ; top5 ->  82.79599752807617  and loss:  1431.3326364159584
forward train acc: top1 ->  53.2249987411499 ; top5 ->  76.1916641998291  and loss:  416.91174137592316
test acc: top1 ->  59.66199830436707 ; top5 ->  82.87999756622314  and loss:  1415.8359169661999
forward train acc: top1 ->  54.1749987411499 ; top5 ->  77.44166423797607  and loss:  402.9206931591034
test acc: top1 ->  59.47199835624695 ; top5 ->  82.86399765090943  and loss:  1421.4804463088512
forward train acc: top1 ->  55.84166542053222 ; top5 ->  78.44999710083007  and loss:  390.02431666851044
test acc: top1 ->  61.12599826698303 ; top5 ->  83.9379975982666  and loss:  1354.5787361562252
forward train acc: top1 ->  55.7583320236206 ; top5 ->  77.8499976158142  and loss:  391.52411460876465
test acc: top1 ->  61.68599814720154 ; top5 ->  84.12799750366212  and loss:  1344.4545703977346
forward train acc: top1 ->  56.29999856948852 ; top5 ->  78.86666399002075  and loss:  380.4394336938858
test acc: top1 ->  61.905998218154906 ; top5 ->  84.37799756317139  and loss:  1331.905771061778
forward train acc: top1 ->  57.04999853134155 ; top5 ->  79.06666402816772  and loss:  380.76130282878876
test acc: top1 ->  62.58799827308655 ; top5 ->  84.82799749450683  and loss:  1306.042412236333
>>>>>>> reverse layer  16  since performance drop >>>>>>>
==> this epoch:  239 / 256 , inc:  1
---------------- start layer  17  ---------------
adv train loss:  -95.58762454986572 , diff:  95.58762454986572
adv train loss:  -97.31335806846619 , diff:  1.7257335186004639
adv train loss:  -92.43666577339172 , diff:  4.876692295074463
adv train loss:  -93.71438002586365 , diff:  1.2777142524719238
adv train loss:  -94.72244071960449 , diff:  1.0080606937408447
adv train loss:  -97.71252024173737 , diff:  2.9900795221328735
adv train loss:  -95.7608209848404 , diff:  1.9516992568969727
adv train loss:  -93.42799210548401 , diff:  2.3328288793563843
adv train loss:  -93.79300546646118 , diff:  0.36501336097717285
layer  17  adv train finish, try to retain  232
>>>>>>> reverse layer  17  since no improvement >>>>>>>
---------------- start layer  18  ---------------
adv train loss:  -89.46076095104218 , diff:  89.46076095104218
adv train loss:  -97.13902056217194 , diff:  7.678259611129761
adv train loss:  -93.00070214271545 , diff:  4.138318419456482
adv train loss:  -92.92084121704102 , diff:  0.07986092567443848
layer  18  adv train finish, try to retain  224
test acc: top1 ->  62.42399812431336 ; top5 ->  84.65999760437012  and loss:  1311.3389408290386
forward train acc: top1 ->  51.61666542053223 ; top5 ->  75.54166442871093  and loss:  433.5382958650589
test acc: top1 ->  56.36599835910797 ; top5 ->  80.87399759140014  and loss:  1553.4743265211582
forward train acc: top1 ->  51.033332118988035 ; top5 ->  74.75833086013795  and loss:  439.32314562797546
test acc: top1 ->  55.8299983891964 ; top5 ->  80.49799758758544  and loss:  1557.0973882079124
forward train acc: top1 ->  51.16666543006897 ; top5 ->  75.09166376113892  and loss:  436.92542481422424
test acc: top1 ->  55.50599843521118 ; top5 ->  80.30799751358032  and loss:  1586.0231531858444
forward train acc: top1 ->  52.18333215713501 ; top5 ->  75.47499759674072  and loss:  427.08226013183594
test acc: top1 ->  59.32199834785462 ; top5 ->  82.66599759674072  and loss:  1436.2665157914162
forward train acc: top1 ->  53.62499870300293 ; top5 ->  76.47499746322632  and loss:  411.4220402240753
test acc: top1 ->  59.81199826431274 ; top5 ->  83.08799757919311  and loss:  1411.8742856830359
forward train acc: top1 ->  54.24166522979736 ; top5 ->  76.19999738693237  and loss:  410.38848769664764
test acc: top1 ->  59.58399820518493 ; top5 ->  82.79999763412475  and loss:  1421.8347121477127
forward train acc: top1 ->  55.85833198547363 ; top5 ->  78.43333087921143  and loss:  390.73651671409607
test acc: top1 ->  61.30199823074341 ; top5 ->  83.99399742736816  and loss:  1351.8244001567364
forward train acc: top1 ->  56.241665554046634 ; top5 ->  78.93333080291748  and loss:  384.5501836538315
test acc: top1 ->  61.553998191452024 ; top5 ->  84.21199751739502  and loss:  1341.7468862980604
forward train acc: top1 ->  56.49166536331177 ; top5 ->  78.91666427612304  and loss:  382.92932307720184
test acc: top1 ->  61.83399821395874 ; top5 ->  84.51999738311767  and loss:  1327.5519589558244
forward train acc: top1 ->  56.94999855041504 ; top5 ->  78.99999755859375  and loss:  375.98611414432526
test acc: top1 ->  62.47399819259643 ; top5 ->  84.8599975692749  and loss:  1304.9201115593314
>>>>>>> reverse layer  18  since performance drop >>>>>>>
==> this epoch:  235 / 256 , inc:  1
---------------- start layer  19  ---------------
adv train loss:  -93.3478171825409 , diff:  93.3478171825409
adv train loss:  -94.64296793937683 , diff:  1.2951507568359375
adv train loss:  -95.07473158836365 , diff:  0.4317636489868164
layer  19  adv train finish, try to retain  224
>>>>>>> reverse layer  19  since no improvement >>>>>>>
---------------- start layer  20  ---------------
adv train loss:  -92.78781163692474 , diff:  92.78781163692474
adv train loss:  -94.12359654903412 , diff:  1.335784912109375
adv train loss:  -93.23209512233734 , diff:  0.8915014266967773
layer  20  adv train finish, try to retain  222
test acc: top1 ->  62.22999812049866 ; top5 ->  84.71999755401612  and loss:  1314.9570882990956
forward train acc: top1 ->  52.39999877929687 ; top5 ->  75.91666427612304  and loss:  422.55794632434845
test acc: top1 ->  56.73599836425781 ; top5 ->  81.03599758453369  and loss:  1536.5447678789496
forward train acc: top1 ->  50.94999883651733 ; top5 ->  74.7666640472412  and loss:  437.4606280326843
test acc: top1 ->  56.2499984375 ; top5 ->  80.537997605896  and loss:  1557.3433946967125
forward train acc: top1 ->  50.61666540145874 ; top5 ->  75.19166412353516  and loss:  433.24230575561523
test acc: top1 ->  55.545998353672026 ; top5 ->  80.28199765930175  and loss:  1576.0146429240704
forward train acc: top1 ->  52.95833206176758 ; top5 ->  76.19999740600586  and loss:  418.43313896656036
test acc: top1 ->  59.32999830131531 ; top5 ->  82.76199759902954  and loss:  1437.7743356227875
forward train acc: top1 ->  53.65833190917969 ; top5 ->  76.9999973487854  and loss:  409.59614050388336
test acc: top1 ->  59.74599827957153 ; top5 ->  83.10599747772217  and loss:  1412.3767317384481
forward train acc: top1 ->  54.29999877929688 ; top5 ->  77.36666427612305  and loss:  405.9109569787979
test acc: top1 ->  60.105998238945006 ; top5 ->  83.3859975479126  and loss:  1399.625645160675
forward train acc: top1 ->  55.44999870300293 ; top5 ->  78.39166423797607  and loss:  391.30096757411957
test acc: top1 ->  61.62599822235107 ; top5 ->  84.37599747772217  and loss:  1340.1314239352942
forward train acc: top1 ->  56.874998588562015 ; top5 ->  79.33333078384399  and loss:  379.2864543199539
test acc: top1 ->  62.05399822540283 ; top5 ->  84.61999754180908  and loss:  1323.4130651801825
forward train acc: top1 ->  56.86666521072388 ; top5 ->  79.14999740600587  and loss:  375.40479838848114
test acc: top1 ->  62.40799811172485 ; top5 ->  84.63599750366211  and loss:  1315.3593026548624
forward train acc: top1 ->  56.89999864578247 ; top5 ->  79.02499748229981  and loss:  382.1203684806824
test acc: top1 ->  62.56999811515808 ; top5 ->  84.95999745635986  and loss:  1298.693943887949
>>>>>>> reverse layer  20  since performance drop >>>>>>>
==> this epoch:  225 / 256 , inc:  1
---------------- start layer  21  ---------------
adv train loss:  -91.42603051662445 , diff:  91.42603051662445
adv train loss:  -91.49530637264252 , diff:  0.0692758560180664
************ all values are small in this layer **********
layer  21  adv train finish, try to retain  241
test acc: top1 ->  62.60999818687439 ; top5 ->  84.86799752807617  and loss:  1300.925072580576
forward train acc: top1 ->  51.99166534423828 ; top5 ->  75.64166389465332  and loss:  425.25303506851196
test acc: top1 ->  56.699998408842085 ; top5 ->  81.08399749526977  and loss:  1545.453679740429
forward train acc: top1 ->  52.058331985473636 ; top5 ->  76.31666416168213  and loss:  423.573389172554
test acc: top1 ->  56.65199830780029 ; top5 ->  81.22599762878419  and loss:  1535.835262477398
forward train acc: top1 ->  50.97499883651734 ; top5 ->  75.024997215271  and loss:  434.43997752666473
test acc: top1 ->  56.07799842071533 ; top5 ->  80.89999756317138  and loss:  1550.0584553778172
forward train acc: top1 ->  53.96666538238525 ; top5 ->  76.2583307647705  and loss:  411.388263463974
test acc: top1 ->  59.72799828109741 ; top5 ->  82.92999744720458  and loss:  1418.3637287467718
forward train acc: top1 ->  54.50833192825317 ; top5 ->  77.9166641998291  and loss:  396.6339199542999
test acc: top1 ->  60.04199827003479 ; top5 ->  83.40999756622314  and loss:  1396.6239846348763
forward train acc: top1 ->  55.9083320236206 ; top5 ->  78.54999767303467  and loss:  389.528954744339
test acc: top1 ->  59.829998273086545 ; top5 ->  83.27199766235351  and loss:  1406.723623752594
forward train acc: top1 ->  55.791665439605715 ; top5 ->  78.18333093643189  and loss:  389.08090460300446
test acc: top1 ->  61.62999818954468 ; top5 ->  84.31599753875733  and loss:  1334.5126756876707
forward train acc: top1 ->  57.31666524887085 ; top5 ->  78.97499755859376  and loss:  378.88983929157257
test acc: top1 ->  61.85799815711975 ; top5 ->  84.43599749908448  and loss:  1330.613253608346
forward train acc: top1 ->  57.18333202362061 ; top5 ->  79.6916639328003  and loss:  372.3352062702179
test acc: top1 ->  62.257998155212405 ; top5 ->  84.6439975769043  and loss:  1319.1409222334623
forward train acc: top1 ->  56.9916654586792 ; top5 ->  78.75833082199097  and loss:  377.47018826007843
test acc: top1 ->  62.791998204040524 ; top5 ->  85.05799746246338  and loss:  1292.4058798104525
>>>>>>> reverse layer  21  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  22  ---------------
adv train loss:  -89.95868813991547 , diff:  89.95868813991547
adv train loss:  -90.20510303974152 , diff:  0.2464148998260498
layer  22  adv train finish, try to retain  218
>>>>>>> reverse layer  22  since no improvement >>>>>>>
---------------- start layer  23  ---------------
adv train loss:  -92.83436584472656 , diff:  92.83436584472656
adv train loss:  -91.34688627719879 , diff:  1.487479567527771
adv train loss:  -91.38461172580719 , diff:  0.03772544860839844
layer  23  adv train finish, try to retain  230
>>>>>>> reverse layer  23  since no improvement >>>>>>>
---------------- start layer  24  ---------------
adv train loss:  -89.34733521938324 , diff:  89.34733521938324
adv train loss:  -89.957648396492 , diff:  0.6103131771087646
************ all values are small in this layer **********
layer  24  adv train finish, try to retain  243
test acc: top1 ->  62.697998119735715 ; top5 ->  85.029997605896  and loss:  1294.5298841148615
forward train acc: top1 ->  52.00833193778992 ; top5 ->  75.8499976348877  and loss:  424.4940687417984
test acc: top1 ->  56.97399851169586 ; top5 ->  81.09399751586913  and loss:  1531.7309882640839
forward train acc: top1 ->  52.17499856948852 ; top5 ->  75.64166414260865  and loss:  425.12570810317993
test acc: top1 ->  56.23199842529297 ; top5 ->  80.80799772186279  and loss:  1558.1999078467488
forward train acc: top1 ->  51.05833206176758 ; top5 ->  74.88333068847656  and loss:  438.48000609874725
test acc: top1 ->  56.505998399353025 ; top5 ->  81.23599750061035  and loss:  1538.8672736883163
forward train acc: top1 ->  53.65833208084106 ; top5 ->  77.03333103179932  and loss:  416.58369851112366
test acc: top1 ->  59.7599983291626 ; top5 ->  83.13799753723144  and loss:  1414.5466128587723
forward train acc: top1 ->  54.824998664855954 ; top5 ->  77.02499736785889  and loss:  402.47065222263336
test acc: top1 ->  60.41599830551147 ; top5 ->  83.43399753875732  and loss:  1391.0034695714712
forward train acc: top1 ->  55.7083318901062 ; top5 ->  78.31666400909424  and loss:  392.6145372390747
test acc: top1 ->  60.50399832458496 ; top5 ->  83.4119975036621  and loss:  1388.7193421423435
forward train acc: top1 ->  56.06666538238525 ; top5 ->  78.33333072662353  and loss:  392.39483392238617
test acc: top1 ->  61.82799829750061 ; top5 ->  84.37799747009278  and loss:  1331.0416224747896
forward train acc: top1 ->  56.97499862670898 ; top5 ->  78.79999757766724  and loss:  374.41774225234985
test acc: top1 ->  61.90199831199646 ; top5 ->  84.42199755706787  and loss:  1327.13995359838
forward train acc: top1 ->  56.883332061767575 ; top5 ->  79.02499748229981  and loss:  379.2582758665085
test acc: top1 ->  62.261998157882694 ; top5 ->  84.60399754943847  and loss:  1315.5250619649887
forward train acc: top1 ->  58.81666530609131 ; top5 ->  80.09166439056396  and loss:  369.9201843738556
test acc: top1 ->  62.92199824829102 ; top5 ->  84.91599763336181  and loss:  1294.6068341732025
>>>>>>> reverse layer  24  since performance drop >>>>>>>
==> this epoch:  244 / 256 , inc:  1
---------------- start layer  25  ---------------
adv train loss:  -92.52316665649414 , diff:  92.52316665649414
adv train loss:  -91.79657971858978 , diff:  0.7265869379043579
layer  25  adv train finish, try to retain  225
test acc: top1 ->  62.15999823226929 ; top5 ->  84.58199756164551  and loss:  1318.72362652421
forward train acc: top1 ->  52.54999881744385 ; top5 ->  75.58333087921143  and loss:  422.757630944252
test acc: top1 ->  57.077998378181455 ; top5 ->  81.32199757537842  and loss:  1519.929178789258
forward train acc: top1 ->  52.14999866485596 ; top5 ->  75.69166408538818  and loss:  428.652835726738
test acc: top1 ->  57.05999835243225 ; top5 ->  81.04199752960206  and loss:  1527.847324758768
forward train acc: top1 ->  50.5999987411499 ; top5 ->  74.89166429519653  and loss:  438.4954125881195
test acc: top1 ->  56.87399838628769 ; top5 ->  81.0679975982666  and loss:  1538.4983099848032
forward train acc: top1 ->  52.983332023620605 ; top5 ->  76.06666408538818  and loss:  419.56895673274994
test acc: top1 ->  59.955998361968994 ; top5 ->  83.15599744873047  and loss:  1406.1219409704208
forward train acc: top1 ->  55.183331985473636 ; top5 ->  77.89999742507935  and loss:  398.7844282388687
test acc: top1 ->  60.66999825744629 ; top5 ->  83.55999761657715  and loss:  1377.974996164441
forward train acc: top1 ->  55.17499856948852 ; top5 ->  78.30833087921143  and loss:  393.8457999229431
test acc: top1 ->  60.677998291015626 ; top5 ->  83.60999747772217  and loss:  1375.3874987661839
forward train acc: top1 ->  56.79166532516479 ; top5 ->  79.19166418075561  and loss:  380.94348537921906
test acc: top1 ->  62.14799818534851 ; top5 ->  84.49799748687744  and loss:  1322.5074135959148
forward train acc: top1 ->  57.38333192825317 ; top5 ->  79.25833072662354  and loss:  376.49525129795074
test acc: top1 ->  62.24999824638367 ; top5 ->  84.57199764251709  and loss:  1312.788643449545
forward train acc: top1 ->  57.15833198547363 ; top5 ->  79.67499742507934  and loss:  372.74910283088684
test acc: top1 ->  62.50199822425842 ; top5 ->  84.9119975112915  and loss:  1302.8268598020077
forward train acc: top1 ->  57.39166524887085 ; top5 ->  79.76666397094726  and loss:  372.970671415329
test acc: top1 ->  63.0299981338501 ; top5 ->  85.07599742736816  and loss:  1286.6578279659152
>>>>>>> reverse layer  25  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  26  ---------------
adv train loss:  -92.18117165565491 , diff:  92.18117165565491
adv train loss:  -92.8557801246643 , diff:  0.6746084690093994
************ all values are small in this layer **********
layer  26  adv train finish, try to retain  501
test acc: top1 ->  62.88399815292358 ; top5 ->  85.03199748535157  and loss:  1291.186252757907
forward train acc: top1 ->  53.341665267944336 ; top5 ->  76.766664352417  and loss:  414.9669567346573
test acc: top1 ->  57.30799842338562 ; top5 ->  81.57199768829345  and loss:  1510.4831655919552
forward train acc: top1 ->  52.14166524887085 ; top5 ->  75.99166437149047  and loss:  426.3515088558197
test acc: top1 ->  56.82399843177795 ; top5 ->  81.07599759674072  and loss:  1546.3742576241493
forward train acc: top1 ->  51.95833204269409 ; top5 ->  75.44999765396118  and loss:  431.74193000793457
test acc: top1 ->  56.31599844894409 ; top5 ->  80.98199754333496  and loss:  1545.1027401387691
forward train acc: top1 ->  53.57499864578247 ; top5 ->  76.90833095550538  and loss:  410.049942612648
test acc: top1 ->  60.10799819259643 ; top5 ->  83.30199743194581  and loss:  1402.5607573986053
forward train acc: top1 ->  54.60833200454712 ; top5 ->  77.57499761581421  and loss:  400.1939263343811
test acc: top1 ->  60.38999829940796 ; top5 ->  83.49599746398926  and loss:  1390.0015153735876
forward train acc: top1 ->  55.48333187103272 ; top5 ->  78.39166402816772  and loss:  389.9648457765579
test acc: top1 ->  60.553998303604125 ; top5 ->  83.61799750061036  and loss:  1380.878182798624
forward train acc: top1 ->  55.849998760223386 ; top5 ->  78.47499746322632  and loss:  388.4816200733185
test acc: top1 ->  61.94799808654785 ; top5 ->  84.51199756317139  and loss:  1326.1436476707458
forward train acc: top1 ->  57.29166519165039 ; top5 ->  79.1416640472412  and loss:  379.50666904449463
test acc: top1 ->  62.30399808044434 ; top5 ->  84.87199755401612  and loss:  1311.6672715991735
forward train acc: top1 ->  57.691665210723876 ; top5 ->  79.74166404724122  and loss:  366.7084254026413
test acc: top1 ->  62.84799822311401 ; top5 ->  85.17799756774902  and loss:  1299.0149514824152
forward train acc: top1 ->  58.81666536331177 ; top5 ->  80.57499740600586  and loss:  362.1905714273453
test acc: top1 ->  63.11799811668396 ; top5 ->  85.28199753417968  and loss:  1289.1588540822268
>>>>>>> reverse layer  26  since performance drop >>>>>>>
==> this epoch:  502 / 512 , inc:  1
---------------- start layer  27  ---------------
adv train loss:  -94.02536308765411 , diff:  94.02536308765411
adv train loss:  -92.4085807800293 , diff:  1.616782307624817
adv train loss:  -95.87560784816742 , diff:  3.4670270681381226
adv train loss:  -92.39251804351807 , diff:  3.483089804649353
adv train loss:  -93.54189229011536 , diff:  1.14937424659729
adv train loss:  -91.52540814876556 , diff:  2.0164841413497925
adv train loss:  -87.03378570079803 , diff:  4.491622447967529
adv train loss:  -94.1096408367157 , diff:  7.075855135917664
adv train loss:  -89.94106483459473 , diff:  4.168576002120972
adv train loss:  -92.79079067707062 , diff:  2.849725842475891
************ all values are small in this layer **********
layer  27  adv train finish, try to retain  504
test acc: top1 ->  63.10999815864563 ; top5 ->  85.27599754180908  and loss:  1291.1434535384178
forward train acc: top1 ->  53.54999870300293 ; top5 ->  76.4166640663147  and loss:  414.6616960763931
test acc: top1 ->  57.49599831180573 ; top5 ->  81.4519975906372  and loss:  1507.2442554235458
forward train acc: top1 ->  51.30833203315735 ; top5 ->  75.09166397094727  and loss:  430.18673491477966
test acc: top1 ->  57.431998319244386 ; top5 ->  81.58999758148194  and loss:  1510.6414930820465
forward train acc: top1 ->  50.65833209991455 ; top5 ->  75.18333103179931  and loss:  437.0300467014313
test acc: top1 ->  57.063998434448244 ; top5 ->  81.4119975769043  and loss:  1519.95395475626
forward train acc: top1 ->  54.35833198547363 ; top5 ->  76.65833070755005  and loss:  409.96436309814453
test acc: top1 ->  60.511998386001586 ; top5 ->  83.63199751434327  and loss:  1381.6548114866018
forward train acc: top1 ->  56.07499870300293 ; top5 ->  78.89999740600587  and loss:  389.11739230155945
test acc: top1 ->  60.71799828414917 ; top5 ->  83.92999750213623  and loss:  1370.4800405353308
forward train acc: top1 ->  55.59999855041504 ; top5 ->  77.84999746322632  and loss:  393.70304560661316
test acc: top1 ->  60.897998226547244 ; top5 ->  83.87399767303467  and loss:  1370.1821451634169
forward train acc: top1 ->  56.66666524887085 ; top5 ->  78.93333068847656  and loss:  381.1815439462662
test acc: top1 ->  62.26999811859131 ; top5 ->  84.63999753417968  and loss:  1316.5271311104298
forward train acc: top1 ->  57.70833192825317 ; top5 ->  80.04166393280029  and loss:  365.69147419929504
test acc: top1 ->  62.57199814758301 ; top5 ->  84.84599753723144  and loss:  1307.9436353296041
forward train acc: top1 ->  58.14999877929687 ; top5 ->  79.8833307647705  and loss:  370.5556261539459
test acc: top1 ->  62.85199821166992 ; top5 ->  84.99199736633301  and loss:  1295.670845553279
forward train acc: top1 ->  57.41666521072388 ; top5 ->  79.9166641998291  and loss:  369.3659003973007
test acc: top1 ->  63.36199817314148 ; top5 ->  85.305997605896  and loss:  1276.9053744226694
>>>>>>> reverse layer  27  since performance drop >>>>>>>
==> this epoch:  505 / 512 , inc:  1
---------------- start layer  28  ---------------
### skip layer  28 wait:  4  ###
---------------- start layer  29  ---------------
adv train loss:  -90.42564845085144 , diff:  90.42564845085144
adv train loss:  -91.2265272140503 , diff:  0.8008787631988525
************ all values are small in this layer **********
layer  29  adv train finish, try to retain  497
test acc: top1 ->  63.07999827003479 ; top5 ->  85.22999752960204  and loss:  1285.2801584601402
forward train acc: top1 ->  52.59166536331177 ; top5 ->  76.36666402816772  and loss:  421.26358938217163
test acc: top1 ->  57.49399838027954 ; top5 ->  82.02799752502442  and loss:  1500.532342299819
forward train acc: top1 ->  52.76666542053223 ; top5 ->  76.44166421890259  and loss:  417.4358596801758
test acc: top1 ->  57.91399841785431 ; top5 ->  81.83199769592285  and loss:  1499.9859130233526
forward train acc: top1 ->  52.93333202362061 ; top5 ->  76.3499974822998  and loss:  414.6817421913147
test acc: top1 ->  57.82199827041626 ; top5 ->  81.63999752502441  and loss:  1502.3415805920959
forward train acc: top1 ->  53.94166528701782 ; top5 ->  76.99166423797607  and loss:  410.02890837192535
test acc: top1 ->  60.7139983581543 ; top5 ->  83.62999757843018  and loss:  1383.0998245328665
forward train acc: top1 ->  55.45833194732666 ; top5 ->  78.13333074569702  and loss:  393.0532660484314
test acc: top1 ->  61.09399833297729 ; top5 ->  83.8139974609375  and loss:  1365.956517353654
forward train acc: top1 ->  56.28333225250244 ; top5 ->  78.44166418075561  and loss:  390.43807303905487
test acc: top1 ->  61.43799827346802 ; top5 ->  84.13399755096435  and loss:  1352.9772064760327
forward train acc: top1 ->  56.04999858856201 ; top5 ->  78.45833084106445  and loss:  386.66299653053284
test acc: top1 ->  62.61799823455811 ; top5 ->  84.75199761199951  and loss:  1308.252104923129
forward train acc: top1 ->  57.00833194732666 ; top5 ->  79.17499725341797  and loss:  375.7243824005127
test acc: top1 ->  62.97999825668335 ; top5 ->  85.0019975479126  and loss:  1296.6546677052975
forward train acc: top1 ->  58.33333190917969 ; top5 ->  80.04166408538818  and loss:  366.0860016942024
test acc: top1 ->  63.16199822998047 ; top5 ->  85.0779975112915  and loss:  1289.7469433695078
forward train acc: top1 ->  58.308331985473636 ; top5 ->  80.4916639328003  and loss:  362.64387583732605
test acc: top1 ->  63.605998164367676 ; top5 ->  85.3879974456787  and loss:  1271.653458699584
>>>>>>> reverse layer  29  since performance drop >>>>>>>
==> this epoch:  498 / 512 , inc:  1
---------------- start layer  30  ---------------
adv train loss:  -88.03477883338928 , diff:  88.03477883338928
adv train loss:  -93.92549431324005 , diff:  5.890715479850769
adv train loss:  -88.98201191425323 , diff:  4.943482398986816
adv train loss:  -90.61338305473328 , diff:  1.6313711404800415
adv train loss:  -89.08171856403351 , diff:  1.531664490699768
adv train loss:  -87.53864324092865 , diff:  1.5430753231048584
adv train loss:  -91.88682913780212 , diff:  4.348185896873474
adv train loss:  -90.45765507221222 , diff:  1.4291740655899048
adv train loss:  -90.1410082578659 , diff:  0.3166468143463135
************ all values are small in this layer **********
layer  30  adv train finish, try to retain  502
test acc: top1 ->  63.413998194122314 ; top5 ->  85.32399754333495  and loss:  1276.9048853069544
forward train acc: top1 ->  53.64999869346619 ; top5 ->  76.9916641998291  and loss:  410.3169642686844
test acc: top1 ->  58.31599820957184 ; top5 ->  82.04799757385254  and loss:  1480.5637644678354
forward train acc: top1 ->  52.31666538238525 ; top5 ->  76.58333097457886  and loss:  417.30485022068024
test acc: top1 ->  58.151998393249514 ; top5 ->  81.8539975402832  and loss:  1482.91506947577
forward train acc: top1 ->  52.88333198547363 ; top5 ->  76.7999973678589  and loss:  412.93552911281586
test acc: top1 ->  57.821998317337034 ; top5 ->  81.68599756622315  and loss:  1504.4102343916893
forward train acc: top1 ->  55.391665344238284 ; top5 ->  77.49166412353516  and loss:  400.4457857608795
test acc: top1 ->  60.45199820327759 ; top5 ->  83.49599759216309  and loss:  1392.2586879432201
forward train acc: top1 ->  56.349998798370365 ; top5 ->  78.54999723434449  and loss:  388.2763372659683
test acc: top1 ->  61.451998253631594 ; top5 ->  83.97999748077393  and loss:  1364.0665773749352
forward train acc: top1 ->  56.3083318901062 ; top5 ->  79.06666408538818  and loss:  382.1934676170349
test acc: top1 ->  61.40999824867249 ; top5 ->  84.2659974685669  and loss:  1355.994402140379
forward train acc: top1 ->  56.54999856948852 ; top5 ->  79.11666391372681  and loss:  378.44370234012604
test acc: top1 ->  62.91599821395874 ; top5 ->  85.07399761505127  and loss:  1304.5567804425955
forward train acc: top1 ->  57.72499862670898 ; top5 ->  80.35833080291748  and loss:  366.09887731075287
test acc: top1 ->  63.05799809875488 ; top5 ->  85.1759975036621  and loss:  1292.3268002718687
forward train acc: top1 ->  57.16666522979736 ; top5 ->  79.14166423797607  and loss:  374.5721124410629
test acc: top1 ->  63.135998180770876 ; top5 ->  85.23799741821288  and loss:  1286.9996955543756
forward train acc: top1 ->  58.89999866485596 ; top5 ->  80.15833108901978  and loss:  365.9553906917572
test acc: top1 ->  63.38399805679321 ; top5 ->  85.44799748687744  and loss:  1272.9332267194986
>>>>>>> reverse layer  30  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  31  ---------------
adv train loss:  -91.97891235351562 , diff:  91.97891235351562
adv train loss:  -91.40790092945099 , diff:  0.5710114240646362
************ all values are small in this layer **********
layer  31  adv train finish, try to retain  500
test acc: top1 ->  62.977998174667356 ; top5 ->  85.24799746704102  and loss:  1286.4831129312515
forward train acc: top1 ->  53.28333204269409 ; top5 ->  76.70833082199097  and loss:  410.6743688583374
test acc: top1 ->  58.12999836235046 ; top5 ->  81.98399758911133  and loss:  1491.206381097436
forward train acc: top1 ->  53.11666540145874 ; top5 ->  76.93333084106445  and loss:  414.4881970882416
test acc: top1 ->  58.03999840278625 ; top5 ->  81.90999759368897  and loss:  1480.8982980847359
forward train acc: top1 ->  52.36666534423828 ; top5 ->  76.19999746322632  and loss:  420.9101754426956
test acc: top1 ->  58.503998445892336 ; top5 ->  81.99399756469727  and loss:  1472.874619141221
forward train acc: top1 ->  54.83333208084107 ; top5 ->  77.74999757766723  and loss:  396.00345849990845
test acc: top1 ->  60.45399833297729 ; top5 ->  83.48199749298095  and loss:  1387.4081898331642
forward train acc: top1 ->  55.274998836517334 ; top5 ->  77.85833084106446  and loss:  396.538653254509
test acc: top1 ->  61.26799829750061 ; top5 ->  83.96599753417969  and loss:  1357.1673312336206
forward train acc: top1 ->  56.2499986076355 ; top5 ->  79.0499973487854  and loss:  383.4586212038994
test acc: top1 ->  61.27599812011719 ; top5 ->  83.92799765472412  and loss:  1356.7548266351223
forward train acc: top1 ->  57.441665420532225 ; top5 ->  79.69166412353516  and loss:  375.34629464149475
test acc: top1 ->  62.51599820251465 ; top5 ->  84.85199758758544  and loss:  1302.2932355552912
forward train acc: top1 ->  57.54999870300293 ; top5 ->  79.96666389465332  and loss:  369.5028954744339
test acc: top1 ->  63.09399817581177 ; top5 ->  84.96399751434326  and loss:  1290.9139281213284
forward train acc: top1 ->  57.8833318901062 ; top5 ->  80.20833084106445  and loss:  365.7019155025482
test acc: top1 ->  62.91599817047119 ; top5 ->  85.20999758911132  and loss:  1282.9920903295279
forward train acc: top1 ->  58.81666528701782 ; top5 ->  80.26666427612305  and loss:  362.2564172744751
test acc: top1 ->  63.50399822769165 ; top5 ->  85.5699975112915  and loss:  1263.7799289226532
>>>>>>> reverse layer  31  since performance drop >>>>>>>
==> this epoch:  501 / 512 , inc:  1
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.005, 0.005, 0.000703125, 0.005, 0.005, 0.005, 0.0009375, 0.00017578125, 0.0025, 0.0009375, 0.0025, 0.0025, 0.0025, 0.0025, 8.7890625e-05, 8.7890625e-05, 0.00046875, 0.00125, 0.00046875, 0.00125, 0.00046875, 0.00017578125, 0.00125, 0.00125, 0.00017578125, 0.00046875, 8.7890625e-05, 8.7890625e-05, 4.39453125e-05, 8.7890625e-05, 8.7890625e-05, 8.7890625e-05]  wait [0, 0, 4, 0, 0, 0, 2, 3, 0, 2, 0, 0, 0, 0, 3, 3, 2, 0, 2, 0, 2, 4, 0, 0, 4, 2, 4, 4, 3, 4, 4, 4]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 0
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  5  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -90.5709468126297 , diff:  90.5709468126297
adv train loss:  -87.17099642753601 , diff:  3.399950385093689
adv train loss:  -85.99231576919556 , diff:  1.178680658340454
adv train loss:  -87.59774959087372 , diff:  1.6054338216781616
adv train loss:  -92.09623324871063 , diff:  4.498483657836914
adv train loss:  -90.2272515296936 , diff:  1.8689817190170288
adv train loss:  -88.95058059692383 , diff:  1.2766709327697754
adv train loss:  -91.01059377193451 , diff:  2.060013175010681
adv train loss:  -89.28442883491516 , diff:  1.7261649370193481
adv train loss:  -87.0847978591919 , diff:  2.1996309757232666
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  36
test acc: top1 ->  56.50999843482971 ; top5 ->  79.96799754104615  and loss:  1566.7608852535486
forward train acc: top1 ->  53.8333318901062 ; top5 ->  76.86666418075562  and loss:  408.3051484823227
test acc: top1 ->  58.769998342514036 ; top5 ->  82.5059974899292  and loss:  1463.3909706175327
forward train acc: top1 ->  52.999998588562015 ; top5 ->  77.07499759674073  and loss:  412.66750395298004
test acc: top1 ->  58.18599833831787 ; top5 ->  81.99399766235352  and loss:  1478.0308833867311
forward train acc: top1 ->  52.991665515899655 ; top5 ->  76.6833307647705  and loss:  414.85118317604065
test acc: top1 ->  57.95399832954407 ; top5 ->  81.90999755249024  and loss:  1479.335393846035
forward train acc: top1 ->  54.92499879837036 ; top5 ->  77.24999717712403  and loss:  402.32739078998566
test acc: top1 ->  60.975998302459715 ; top5 ->  83.90199759216308  and loss:  1359.681955948472
forward train acc: top1 ->  56.433331985473636 ; top5 ->  78.79999753952026  and loss:  383.90383183956146
test acc: top1 ->  61.337998232269285 ; top5 ->  83.94599744720459  and loss:  1350.4740613102913
forward train acc: top1 ->  56.099998722076414 ; top5 ->  78.90833065032959  and loss:  380.0913208723068
test acc: top1 ->  61.61599815940857 ; top5 ->  84.1819974899292  and loss:  1341.6604557037354
forward train acc: top1 ->  56.0499987411499 ; top5 ->  78.7999975013733  and loss:  384.0212308168411
test acc: top1 ->  62.62999817008972 ; top5 ->  84.96799755706787  and loss:  1298.270741686225
forward train acc: top1 ->  57.791665134429934 ; top5 ->  79.6916640472412  and loss:  371.48911142349243
test acc: top1 ->  63.095998230361936 ; top5 ->  85.12599752960205  and loss:  1286.0628925263882
forward train acc: top1 ->  58.1249987411499 ; top5 ->  80.57499744415283  and loss:  365.2161988019943
test acc: top1 ->  63.20999822998047 ; top5 ->  85.27599747161865  and loss:  1277.01453076303
forward train acc: top1 ->  58.14999866485596 ; top5 ->  79.75833086013795  and loss:  367.674178481102
test acc: top1 ->  63.71999822807312 ; top5 ->  85.48599745635987  and loss:  1261.0487933158875
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  37 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -92.40583574771881 , diff:  92.40583574771881
adv train loss:  -93.85268175601959 , diff:  1.4468460083007812
adv train loss:  -88.25161683559418 , diff:  5.601064920425415
adv train loss:  -91.12104630470276 , diff:  2.8694294691085815
adv train loss:  -94.86068093776703 , diff:  3.73963463306427
adv train loss:  -92.34196525812149 , diff:  2.5187156796455383
adv train loss:  -92.31736063957214 , diff:  0.024604618549346924
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  43
test acc: top1 ->  57.93599829711914 ; top5 ->  81.2099974685669  and loss:  1497.2306928783655
forward train acc: top1 ->  54.11666538238526 ; top5 ->  77.63333122253418  and loss:  402.81038665771484
test acc: top1 ->  58.56999825515747 ; top5 ->  82.07199768066407  and loss:  1479.114957690239
forward train acc: top1 ->  53.43333194732666 ; top5 ->  76.5749973487854  and loss:  412.9679536819458
test acc: top1 ->  57.51199837188721 ; top5 ->  81.64199751663207  and loss:  1509.7830203175545
forward train acc: top1 ->  53.56666536331177 ; top5 ->  76.43333080291748  and loss:  413.3679703474045
test acc: top1 ->  57.76999831981659 ; top5 ->  81.90599750213623  and loss:  1498.303657412529
forward train acc: top1 ->  54.09166524887085 ; top5 ->  77.1833306503296  and loss:  401.46391201019287
test acc: top1 ->  61.00799831905365 ; top5 ->  83.8339974609375  and loss:  1369.6854480952024
forward train acc: top1 ->  55.13333198547363 ; top5 ->  78.09166423797608  and loss:  393.5301228761673
test acc: top1 ->  61.33799827613831 ; top5 ->  84.2719975265503  and loss:  1351.8371391743422
forward train acc: top1 ->  55.958332023620606 ; top5 ->  78.84166387557984  and loss:  385.6953535079956
test acc: top1 ->  61.357998262023926 ; top5 ->  84.2659975845337  and loss:  1346.1688099354506
forward train acc: top1 ->  57.899998626708985 ; top5 ->  79.68333061218262  and loss:  369.7119097709656
test acc: top1 ->  62.68599816513061 ; top5 ->  84.96399737701417  and loss:  1298.4154895991087
forward train acc: top1 ->  58.408331871032715 ; top5 ->  80.39166410446167  and loss:  366.81005120277405
test acc: top1 ->  63.12599816055298 ; top5 ->  85.07999743652344  and loss:  1290.0776342451572
forward train acc: top1 ->  58.49166511535645 ; top5 ->  80.62499759674073  and loss:  362.06361520290375
test acc: top1 ->  63.143998085784915 ; top5 ->  85.22199746551513  and loss:  1285.4037277847528
forward train acc: top1 ->  59.25833190917969 ; top5 ->  80.69166425704957  and loss:  361.0439583659172
test acc: top1 ->  63.781998114013675 ; top5 ->  85.47199755249024  and loss:  1264.8107702732086
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  44 / 64 , inc:  1
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
adv train loss:  -95.59222364425659 , diff:  95.59222364425659
adv train loss:  -94.9781973361969 , diff:  0.6140263080596924
************ all values are small in this layer **********
layer  3  adv train finish, try to retain  42
test acc: top1 ->  57.703998295974735 ; top5 ->  81.33199761810303  and loss:  1503.496779680252
forward train acc: top1 ->  54.25833211898804 ; top5 ->  76.79166400909423  and loss:  407.52572202682495
test acc: top1 ->  57.8759982509613 ; top5 ->  81.55999755859375  and loss:  1498.8793254494667
forward train acc: top1 ->  53.39166551589966 ; top5 ->  77.17499774932861  and loss:  409.0757849216461
test acc: top1 ->  57.555998275375366 ; top5 ->  81.82399763641358  and loss:  1497.18381690979
forward train acc: top1 ->  54.3416653251648 ; top5 ->  77.1416640663147  and loss:  405.00891065597534
test acc: top1 ->  58.0619984539032 ; top5 ->  81.97199751052857  and loss:  1479.6175591647625
forward train acc: top1 ->  54.93333194732666 ; top5 ->  78.47499740600585  and loss:  391.84508740901947
test acc: top1 ->  60.9599981962204 ; top5 ->  83.91999743652343  and loss:  1369.252510368824
forward train acc: top1 ->  56.44999870300293 ; top5 ->  79.0833309173584  and loss:  380.59546399116516
test acc: top1 ->  61.085998240661624 ; top5 ->  84.19999762573242  and loss:  1355.9743402451277
forward train acc: top1 ->  56.84999856948853 ; top5 ->  78.84166402816773  and loss:  383.13163709640503
test acc: top1 ->  61.7059982257843 ; top5 ->  84.3499974533081  and loss:  1339.608246549964
forward train acc: top1 ->  57.51666522979736 ; top5 ->  80.00833065032958  and loss:  370.8679666519165
test acc: top1 ->  62.82599810562134 ; top5 ->  85.04599749908448  and loss:  1297.601499632001
forward train acc: top1 ->  58.42499851226807 ; top5 ->  79.65833040237426  and loss:  371.1594114303589
test acc: top1 ->  63.12999820632935 ; top5 ->  85.20399744567871  and loss:  1286.464892938733
forward train acc: top1 ->  58.29166521072388 ; top5 ->  80.51666381835938  and loss:  360.0890657901764
test acc: top1 ->  63.08799822616577 ; top5 ->  85.41999741363526  and loss:  1278.192236110568
forward train acc: top1 ->  58.99999868392944 ; top5 ->  80.8499976348877  and loss:  352.0437549352646
test acc: top1 ->  63.755998225402834 ; top5 ->  85.71399752349853  and loss:  1260.7534831017256
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  43 / 64 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -91.73824679851532 , diff:  91.73824679851532
adv train loss:  -90.01601850986481 , diff:  1.7222282886505127
adv train loss:  -91.84425985813141 , diff:  1.8282413482666016
adv train loss:  -92.79530823230743 , diff:  0.9510483741760254
************ all values are small in this layer **********
layer  4  adv train finish, try to retain  39
test acc: top1 ->  60.91199823074341 ; top5 ->  83.74199745483398  and loss:  1368.7475297898054
forward train acc: top1 ->  54.641665363311766 ; top5 ->  77.65833080291748  and loss:  400.4227184057236
test acc: top1 ->  58.73599824142456 ; top5 ->  82.42199761199952  and loss:  1459.7351988106966
forward train acc: top1 ->  53.816665439605714 ; top5 ->  76.8499977684021  and loss:  411.22206008434296
test acc: top1 ->  58.02999819450378 ; top5 ->  81.7539975036621  and loss:  1485.5671634823084
forward train acc: top1 ->  53.7833320236206 ; top5 ->  77.23333059310913  and loss:  407.0685636997223
test acc: top1 ->  57.955998303604126 ; top5 ->  82.0259975593567  and loss:  1483.6338061094284
forward train acc: top1 ->  55.57499864578247 ; top5 ->  78.07499746322632  and loss:  391.8890998363495
test acc: top1 ->  61.15199823150635 ; top5 ->  84.11599758453369  and loss:  1354.9813033938408
forward train acc: top1 ->  56.874998569488525 ; top5 ->  79.1749974822998  and loss:  379.08309638500214
test acc: top1 ->  61.375998182678224 ; top5 ->  84.21199762573242  and loss:  1353.8363327234983
forward train acc: top1 ->  57.0833318901062 ; top5 ->  79.45833080291749  and loss:  375.8725860118866
test acc: top1 ->  62.04999825782776 ; top5 ->  84.4659975479126  and loss:  1343.0978227704763
forward train acc: top1 ->  58.249998588562015 ; top5 ->  80.43333084106445  and loss:  364.4602121114731
test acc: top1 ->  63.075998135375976 ; top5 ->  85.2539976852417  and loss:  1292.9006924629211
forward train acc: top1 ->  58.54166534423828 ; top5 ->  80.21666412353515  and loss:  364.8224105834961
test acc: top1 ->  63.26999821395874 ; top5 ->  85.413997555542  and loss:  1280.1419637650251
forward train acc: top1 ->  58.39166524887085 ; top5 ->  80.28333106994629  and loss:  365.1020052433014
test acc: top1 ->  63.5459982711792 ; top5 ->  85.58999757232667  and loss:  1271.18263605237
forward train acc: top1 ->  58.29166530609131 ; top5 ->  79.80833099365235  and loss:  364.5266513824463
test acc: top1 ->  64.04599816818238 ; top5 ->  85.76799745330811  and loss:  1253.8960101306438
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  40 / 64 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -92.56948614120483 , diff:  92.56948614120483
adv train loss:  -89.25057399272919 , diff:  3.318912148475647
adv train loss:  -89.81019711494446 , diff:  0.559623122215271
************ all values are small in this layer **********
layer  5  adv train finish, try to retain  28
test acc: top1 ->  59.45399827804565 ; top5 ->  82.34199759674073  and loss:  1446.592822611332
forward train acc: top1 ->  54.31666551589966 ; top5 ->  77.28333070755005  and loss:  408.52118384838104
test acc: top1 ->  58.939998252868655 ; top5 ->  82.53399753417969  and loss:  1455.792634561658
forward train acc: top1 ->  53.43333200454712 ; top5 ->  77.04166412353516  and loss:  410.6655659675598
test acc: top1 ->  58.50399830856323 ; top5 ->  82.01799755401612  and loss:  1467.900065690279
forward train acc: top1 ->  55.091665449142454 ; top5 ->  77.18333086013794  and loss:  400.99519884586334
test acc: top1 ->  58.5139984249115 ; top5 ->  82.3059975479126  and loss:  1460.720127105713
forward train acc: top1 ->  55.59999856948853 ; top5 ->  78.4333307647705  and loss:  389.8391524553299
test acc: top1 ->  61.401998193359375 ; top5 ->  84.08399746856689  and loss:  1352.8093923926353
forward train acc: top1 ->  57.41666532516479 ; top5 ->  79.50833087921143  and loss:  373.60379683971405
test acc: top1 ->  61.56199812927246 ; top5 ->  84.21399753875733  and loss:  1344.887549340725
forward train acc: top1 ->  57.516665096282956 ; top5 ->  79.39999773025512  and loss:  374.1815242767334
test acc: top1 ->  62.073998231124875 ; top5 ->  84.54799751739502  and loss:  1325.5246113538742
forward train acc: top1 ->  58.26666519165039 ; top5 ->  80.19999767303467  and loss:  363.42034900188446
test acc: top1 ->  63.18799819869995 ; top5 ->  85.187997555542  and loss:  1282.4279164671898
forward train acc: top1 ->  58.86666534423828 ; top5 ->  80.04999742507934  and loss:  364.3409163951874
test acc: top1 ->  63.48599820861816 ; top5 ->  85.32399761810302  and loss:  1271.8404141366482
forward train acc: top1 ->  58.84166524887085 ; top5 ->  80.46666416168213  and loss:  361.43497478961945
test acc: top1 ->  63.54199821777344 ; top5 ->  85.46799747314454  and loss:  1269.6348636597395
forward train acc: top1 ->  59.741665325164796 ; top5 ->  80.95833065032959  and loss:  351.99838280677795
test acc: top1 ->  63.959998167419435 ; top5 ->  85.73199751434326  and loss:  1251.3929323852062
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  29 / 64 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -89.57711243629456 , diff:  89.57711243629456
adv train loss:  -86.78165364265442 , diff:  2.7954587936401367
adv train loss:  -90.39393055438995 , diff:  3.6122769117355347
adv train loss:  -90.58680427074432 , diff:  0.19287371635437012
layer  6  adv train finish, try to retain  111
test acc: top1 ->  62.96199822845459 ; top5 ->  84.99399766693115  and loss:  1296.1391661912203
forward train acc: top1 ->  55.34999858856201 ; top5 ->  78.14999740600587  and loss:  394.5717206001282
test acc: top1 ->  59.101998262786864 ; top5 ->  82.38399760894775  and loss:  1462.101878926158
forward train acc: top1 ->  53.8499987411499 ; top5 ->  77.01666427612305  and loss:  407.3657611608505
test acc: top1 ->  58.973998268890384 ; top5 ->  82.46999747467041  and loss:  1447.2264165878296
forward train acc: top1 ->  54.44999881744385 ; top5 ->  77.19166414260864  and loss:  406.8789849281311
test acc: top1 ->  59.00199827919006 ; top5 ->  82.36999754867554  and loss:  1449.735232874751
forward train acc: top1 ->  55.89166540145874 ; top5 ->  78.23333084106446  and loss:  391.93063974380493
test acc: top1 ->  61.39999817771912 ; top5 ->  84.16999743118286  and loss:  1352.711986720562
forward train acc: top1 ->  56.474998760223386 ; top5 ->  78.74999740600586  and loss:  383.48691606521606
test acc: top1 ->  61.53599816188812 ; top5 ->  84.2059974456787  and loss:  1342.069697380066
forward train acc: top1 ->  57.29999876022339 ; top5 ->  79.48333063125611  and loss:  374.5954964160919
test acc: top1 ->  61.787998214149475 ; top5 ->  84.50199757537841  and loss:  1334.712232902646
forward train acc: top1 ->  58.09999881744385 ; top5 ->  80.13333095550537  and loss:  365.9470964670181
test acc: top1 ->  62.98799811592102 ; top5 ->  85.18799764099121  and loss:  1285.3577824234962
forward train acc: top1 ->  59.01666540145874 ; top5 ->  81.00833072662354  and loss:  356.3244470357895
test acc: top1 ->  63.4079981212616 ; top5 ->  85.41799756011963  and loss:  1267.5275720655918
forward train acc: top1 ->  58.999998512268064 ; top5 ->  80.35833084106446  and loss:  359.0533872246742
test acc: top1 ->  63.649998205566405 ; top5 ->  85.68199747314453  and loss:  1260.6711291521788
forward train acc: top1 ->  59.2833317565918 ; top5 ->  80.7416641998291  and loss:  354.079274058342
test acc: top1 ->  63.853998137664796 ; top5 ->  85.69599747924805  and loss:  1251.3913569301367
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  112 / 128 , inc:  1
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
adv train loss:  -89.40197253227234 , diff:  89.40197253227234
adv train loss:  -87.31804049015045 , diff:  2.083932042121887
adv train loss:  -89.47709906101227 , diff:  2.1590585708618164
adv train loss:  -92.96643471717834 , diff:  3.4893356561660767
adv train loss:  -88.61782574653625 , diff:  4.34860897064209
adv train loss:  -91.76680541038513 , diff:  3.148979663848877
adv train loss:  -90.09092450141907 , diff:  1.6758809089660645
adv train loss:  -87.45442807674408 , diff:  2.636496424674988
adv train loss:  -83.90654063224792 , diff:  3.547887444496155
adv train loss:  -87.65169739723206 , diff:  3.745156764984131
layer  8  adv train finish, try to retain  101
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -88.2599429488182 , diff:  88.2599429488182
adv train loss:  -90.10960555076599 , diff:  1.8496626019477844
adv train loss:  -86.36859428882599 , diff:  3.7410112619400024
adv train loss:  -89.86331582069397 , diff:  3.494721531867981
adv train loss:  -88.95922636985779 , diff:  0.9040894508361816
layer  9  adv train finish, try to retain  109
test acc: top1 ->  63.74799810638428 ; top5 ->  85.55599750061035  and loss:  1257.9631961286068
forward train acc: top1 ->  56.04166530609131 ; top5 ->  78.04166423797608  and loss:  390.4786365032196
test acc: top1 ->  59.319998306274414 ; top5 ->  82.88999756469727  and loss:  1436.1734873503447
forward train acc: top1 ->  53.716665287017825 ; top5 ->  77.32499736785888  and loss:  406.6703985929489
test acc: top1 ->  59.15199834060669 ; top5 ->  82.76999756011963  and loss:  1437.3192076683044
forward train acc: top1 ->  54.74999864578247 ; top5 ->  77.54999731063843  and loss:  402.40640449523926
test acc: top1 ->  58.765998254776 ; top5 ->  82.46999756774902  and loss:  1450.4311798363924
forward train acc: top1 ->  56.399998569488524 ; top5 ->  78.63333072662354  and loss:  386.3457924127579
test acc: top1 ->  61.797998209762575 ; top5 ->  84.30199757461548  and loss:  1338.7988941818476
forward train acc: top1 ->  56.72499868392944 ; top5 ->  79.0999976348877  and loss:  379.83714413642883
test acc: top1 ->  62.09199814605713 ; top5 ->  84.58999753112793  and loss:  1315.9175074845552
forward train acc: top1 ->  57.17499870300293 ; top5 ->  79.13333072662354  and loss:  373.8939392566681
test acc: top1 ->  62.15599823074341 ; top5 ->  84.88599758605957  and loss:  1317.6044327616692
forward train acc: top1 ->  57.94166528701782 ; top5 ->  79.70833072662353  and loss:  366.69981038570404
test acc: top1 ->  63.2919981880188 ; top5 ->  85.34599754180908  and loss:  1277.5943370759487
forward train acc: top1 ->  59.38333194732666 ; top5 ->  80.9916641998291  and loss:  354.6010959148407
test acc: top1 ->  63.72199816360474 ; top5 ->  85.67799752807618  and loss:  1262.7670589238405
forward train acc: top1 ->  58.40833194732666 ; top5 ->  80.19166431427001  and loss:  366.58617800474167
test acc: top1 ->  63.86999803581238 ; top5 ->  85.51599748382569  and loss:  1263.4766050875187
forward train acc: top1 ->  59.1333318901062 ; top5 ->  80.47499759674072  and loss:  358.5291516780853
test acc: top1 ->  64.32999808044434 ; top5 ->  85.80599757232666  and loss:  1244.53929951787
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  115 / 128 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -89.32650375366211 , diff:  89.32650375366211
adv train loss:  -86.31300085783005 , diff:  3.0135028958320618
adv train loss:  -85.6824107170105 , diff:  0.6305901408195496
layer  10  adv train finish, try to retain  96
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -85.676274061203 , diff:  85.676274061203
adv train loss:  -86.89980888366699 , diff:  1.2235348224639893
adv train loss:  -89.8940464258194 , diff:  2.994237542152405
adv train loss:  -86.40738320350647 , diff:  3.4866632223129272
adv train loss:  -85.25604748725891 , diff:  1.1513357162475586
adv train loss:  -91.080601811409 , diff:  5.8245543241500854
adv train loss:  -88.75086462497711 , diff:  2.3297371864318848
adv train loss:  -87.97042882442474 , diff:  0.7804358005523682
layer  11  adv train finish, try to retain  104
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -88.99564957618713 , diff:  88.99564957618713
adv train loss:  -87.85435891151428 , diff:  1.1412906646728516
adv train loss:  -87.72027814388275 , diff:  0.13408076763153076
layer  12  adv train finish, try to retain  111
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -90.42214334011078 , diff:  90.42214334011078
adv train loss:  -88.0913075208664 , diff:  2.3308358192443848
adv train loss:  -91.29337847232819 , diff:  3.202070951461792
adv train loss:  -89.7744300365448 , diff:  1.5189484357833862
adv train loss:  -88.95443367958069 , diff:  0.8199963569641113
layer  13  adv train finish, try to retain  103
>>>>>>> reverse layer  13  since no improvement >>>>>>>
---------------- start layer  14  ---------------
### skip layer  14 wait:  3  ###
---------------- start layer  15  ---------------
### skip layer  15 wait:  3  ###
---------------- start layer  16  ---------------
adv train loss:  -89.12338185310364 , diff:  89.12338185310364
adv train loss:  -84.13905096054077 , diff:  4.984330892562866
adv train loss:  -85.58093619346619 , diff:  1.441885232925415
adv train loss:  -87.8346381187439 , diff:  2.25370192527771
adv train loss:  -89.38181710243225 , diff:  1.5471789836883545
adv train loss:  -87.17038142681122 , diff:  2.2114356756210327
adv train loss:  -87.11260950565338 , diff:  0.057771921157836914
************ all values are small in this layer **********
layer  16  adv train finish, try to retain  238
test acc: top1 ->  64.1559980003357 ; top5 ->  85.72399753723144  and loss:  1246.6031312644482
forward train acc: top1 ->  55.324998683929444 ; top5 ->  78.77499748229981  and loss:  387.0969902276993
test acc: top1 ->  59.63599821472168 ; top5 ->  82.65199764862061  and loss:  1435.4734268933535
forward train acc: top1 ->  54.48333207130432 ; top5 ->  77.55833074569702  and loss:  401.5737553834915
test acc: top1 ->  59.08799834766388 ; top5 ->  82.91199755554199  and loss:  1431.4605341255665
forward train acc: top1 ->  53.4833319568634 ; top5 ->  76.93333068847656  and loss:  409.41506123542786
test acc: top1 ->  58.0159982334137 ; top5 ->  81.82199760437011  and loss:  1485.910965204239
forward train acc: top1 ->  56.48333192825317 ; top5 ->  78.79999763488769  and loss:  382.9716136455536
test acc: top1 ->  62.0679982006073 ; top5 ->  84.45599754638671  and loss:  1328.0597572475672
forward train acc: top1 ->  57.67499856948852 ; top5 ->  79.93333057403565  and loss:  369.77480244636536
test acc: top1 ->  62.31199809989929 ; top5 ->  84.65799745941162  and loss:  1318.3446302562952
forward train acc: top1 ->  56.93333194732666 ; top5 ->  79.28333065032959  and loss:  377.3062090873718
test acc: top1 ->  62.38399824638367 ; top5 ->  84.71199739837647  and loss:  1312.2081194221973
forward train acc: top1 ->  58.291665439605715 ; top5 ->  80.24999752044678  and loss:  364.2212303876877
test acc: top1 ->  63.377998180770874 ; top5 ->  85.39999750213623  and loss:  1274.250904634595
forward train acc: top1 ->  58.65833198547363 ; top5 ->  80.57499740600586  and loss:  358.2208186388016
test acc: top1 ->  63.733998229217526 ; top5 ->  85.52999754486083  and loss:  1259.7441137135029
forward train acc: top1 ->  57.933331985473636 ; top5 ->  79.86666423797607  and loss:  366.92804288864136
test acc: top1 ->  64.26199809722901 ; top5 ->  85.60999737701415  and loss:  1249.9883873537183
forward train acc: top1 ->  60.11666511535645 ; top5 ->  81.29166389465333  and loss:  350.0603483915329
test acc: top1 ->  64.39399820480347 ; top5 ->  86.04599750366211  and loss:  1238.9231657981873
>>>>>>> reverse layer  16  since performance drop >>>>>>>
==> this epoch:  239 / 256 , inc:  1
---------------- start layer  17  ---------------
adv train loss:  -83.4780580997467 , diff:  83.4780580997467
adv train loss:  -87.1447536945343 , diff:  3.6666955947875977
adv train loss:  -86.94484281539917 , diff:  0.19991087913513184
layer  17  adv train finish, try to retain  210
test acc: top1 ->  64.02199815826415 ; top5 ->  85.62399748535157  and loss:  1258.3892410844564
forward train acc: top1 ->  55.04166526794434 ; top5 ->  78.19166399002076  and loss:  391.38137996196747
test acc: top1 ->  59.58999832611084 ; top5 ->  83.30199758300782  and loss:  1413.0385333150625
forward train acc: top1 ->  54.549998664855956 ; top5 ->  77.53333124160767  and loss:  399.6983904838562
test acc: top1 ->  59.461998239135745 ; top5 ->  82.87999760131837  and loss:  1429.310026422143
forward train acc: top1 ->  53.9916654586792 ; top5 ->  76.98333089828492  and loss:  402.88944935798645
test acc: top1 ->  59.579998189926144 ; top5 ->  83.1159975982666  and loss:  1413.2199027985334
forward train acc: top1 ->  56.65833206176758 ; top5 ->  79.38333057403564  and loss:  380.82761311531067
test acc: top1 ->  61.75799822540283 ; top5 ->  84.45399748687744  and loss:  1328.5976325273514
forward train acc: top1 ->  56.96666519165039 ; top5 ->  79.99999740600586  and loss:  375.03990626335144
test acc: top1 ->  62.175998164749146 ; top5 ->  84.63599748687744  and loss:  1316.836198657751
forward train acc: top1 ->  57.9666651725769 ; top5 ->  79.90833087921142  and loss:  368.5256155729294
test acc: top1 ->  62.163998280334475 ; top5 ->  84.65799743499755  and loss:  1318.8124017566442
forward train acc: top1 ->  59.133331871032716 ; top5 ->  80.39166439056396  and loss:  356.67612755298615
test acc: top1 ->  63.24799816246033 ; top5 ->  85.47399754333496  and loss:  1272.000871002674
forward train acc: top1 ->  58.141665325164794 ; top5 ->  80.17499744415284  and loss:  365.5191739797592
test acc: top1 ->  63.82399826660156 ; top5 ->  85.62999767608643  and loss:  1261.2049007862806
forward train acc: top1 ->  59.48333168029785 ; top5 ->  81.14166400909424  and loss:  352.6090998649597
test acc: top1 ->  63.993998138427735 ; top5 ->  85.81399752807617  and loss:  1248.677608013153
forward train acc: top1 ->  59.449998722076415 ; top5 ->  80.92499744415284  and loss:  354.86814391613007
test acc: top1 ->  64.34999819869995 ; top5 ->  85.9679974395752  and loss:  1235.6131234765053
>>>>>>> reverse layer  17  since performance drop >>>>>>>
==> this epoch:  230 / 256 , inc:  1
---------------- start layer  18  ---------------
adv train loss:  -85.29298865795135 , diff:  85.29298865795135
adv train loss:  -87.18386816978455 , diff:  1.890879511833191
adv train loss:  -88.25070142745972 , diff:  1.066833257675171
adv train loss:  -86.16887891292572 , diff:  2.0818225145339966
adv train loss:  -86.90307819843292 , diff:  0.7341992855072021
************ all values are small in this layer **********
layer  18  adv train finish, try to retain  234
test acc: top1 ->  64.08599814682007 ; top5 ->  85.91799741363525  and loss:  1241.347440302372
forward train acc: top1 ->  55.30833200454712 ; top5 ->  78.57499740600586  and loss:  390.8936433792114
test acc: top1 ->  59.84999825725556 ; top5 ->  83.14599761047363  and loss:  1415.3371209204197
forward train acc: top1 ->  54.7833320236206 ; top5 ->  78.00833097457885  and loss:  398.4264395236969
test acc: top1 ->  59.73399835395813 ; top5 ->  83.18599752197265  and loss:  1408.219870492816
forward train acc: top1 ->  54.83333213806152 ; top5 ->  78.37499732971192  and loss:  393.33051359653473
test acc: top1 ->  59.509998237609864 ; top5 ->  82.86999747772217  and loss:  1425.2282411456108
forward train acc: top1 ->  56.53333204269409 ; top5 ->  78.90833051681518  and loss:  381.5126539468765
test acc: top1 ->  61.91399825286865 ; top5 ->  84.68599750061036  and loss:  1314.9588713049889
forward train acc: top1 ->  57.33333190917969 ; top5 ->  79.77499767303466  and loss:  372.4861413240433
test acc: top1 ->  62.317998192214965 ; top5 ->  84.83199770965577  and loss:  1307.5187313482165
forward train acc: top1 ->  58.62499847412109 ; top5 ->  80.74166389465331  and loss:  359.5742521286011
test acc: top1 ->  62.4939981552124 ; top5 ->  84.98799763793946  and loss:  1302.13283033669
forward train acc: top1 ->  58.34166515350342 ; top5 ->  80.51666419982911  and loss:  361.58592307567596
test acc: top1 ->  63.63999813423157 ; top5 ->  85.65999748382568  and loss:  1263.19935066998
forward train acc: top1 ->  58.99999849319458 ; top5 ->  80.37499757766723  and loss:  356.4782649874687
test acc: top1 ->  63.845998037338255 ; top5 ->  85.83799748535156  and loss:  1249.9775731414557
forward train acc: top1 ->  58.966665229797364 ; top5 ->  80.51666412353515  and loss:  358.60507440567017
test acc: top1 ->  63.99799807510376 ; top5 ->  85.88199750061035  and loss:  1247.0774895995855
forward train acc: top1 ->  60.099998474121094 ; top5 ->  81.7999973678589  and loss:  341.15429508686066
test acc: top1 ->  64.54799813652039 ; top5 ->  86.0839975112915  and loss:  1229.8485732525587
>>>>>>> reverse layer  18  since performance drop >>>>>>>
==> this epoch:  235 / 256 , inc:  1
---------------- start layer  19  ---------------
adv train loss:  -87.98551189899445 , diff:  87.98551189899445
adv train loss:  -93.76950633525848 , diff:  5.783994436264038
adv train loss:  -92.98819375038147 , diff:  0.7813125848770142
layer  19  adv train finish, try to retain  207
>>>>>>> reverse layer  19  since no improvement >>>>>>>
---------------- start layer  20  ---------------
adv train loss:  -86.92358005046844 , diff:  86.92358005046844
adv train loss:  -88.67149448394775 , diff:  1.747914433479309
adv train loss:  -87.51160073280334 , diff:  1.1598937511444092
adv train loss:  -90.13676702976227 , diff:  2.6251662969589233
adv train loss:  -89.24347496032715 , diff:  0.8932920694351196
************ all values are small in this layer **********
layer  20  adv train finish, try to retain  224
test acc: top1 ->  64.33999808883667 ; top5 ->  86.05999749450683  and loss:  1231.940046198666
forward train acc: top1 ->  56.37499870300293 ; top5 ->  78.91666389465333  and loss:  383.11151123046875
test acc: top1 ->  60.09999837265015 ; top5 ->  83.42999750518798  and loss:  1401.2454229444265
forward train acc: top1 ->  55.8416653251648 ; top5 ->  78.56666395187378  and loss:  387.7673578262329
test acc: top1 ->  59.90199826202392 ; top5 ->  83.22599754943847  and loss:  1410.1500610411167
forward train acc: top1 ->  55.0083320236206 ; top5 ->  78.19166421890259  and loss:  395.2339369058609
test acc: top1 ->  59.24199826183319 ; top5 ->  82.87399758148193  and loss:  1432.1233425289392
forward train acc: top1 ->  56.12499864578247 ; top5 ->  79.09999738693237  and loss:  382.74746215343475
test acc: top1 ->  62.21799818534851 ; top5 ->  84.69399751739502  and loss:  1314.2757218629122
forward train acc: top1 ->  57.90833198547363 ; top5 ->  79.59999732971191  and loss:  372.8581931591034
test acc: top1 ->  62.44799818725586 ; top5 ->  84.92799738616944  and loss:  1312.4307214915752
forward train acc: top1 ->  57.3083318901062 ; top5 ->  79.49166412353516  and loss:  373.97490322589874
test acc: top1 ->  62.05999819335938 ; top5 ->  84.6819976348877  and loss:  1319.5330874323845
forward train acc: top1 ->  58.458331775665286 ; top5 ->  80.40833099365234  and loss:  364.7354391813278
test acc: top1 ->  63.52399822120667 ; top5 ->  85.59799753570556  and loss:  1263.850512906909
forward train acc: top1 ->  58.99166515350342 ; top5 ->  80.92499740600586  and loss:  354.15199315547943
test acc: top1 ->  63.819998125076296 ; top5 ->  85.8319974533081  and loss:  1253.4752081856132
forward train acc: top1 ->  59.91666519165039 ; top5 ->  81.35833087921142  and loss:  347.03001379966736
test acc: top1 ->  64.15599814605713 ; top5 ->  85.8639975479126  and loss:  1247.8136166483164
forward train acc: top1 ->  59.32499856948853 ; top5 ->  81.07499736785888  and loss:  355.42909705638885
test acc: top1 ->  64.5899981098175 ; top5 ->  86.20199745330811  and loss:  1228.9258733987808
>>>>>>> reverse layer  20  since performance drop >>>>>>>
==> this epoch:  225 / 256 , inc:  1
---------------- start layer  21  ---------------
### skip layer  21 wait:  4  ###
---------------- start layer  22  ---------------
adv train loss:  -85.08806478977203 , diff:  85.08806478977203
adv train loss:  -84.76461040973663 , diff:  0.3234543800354004
layer  22  adv train finish, try to retain  194
test acc: top1 ->  64.23599811897277 ; top5 ->  85.99999742584228  and loss:  1241.8812994360924
forward train acc: top1 ->  55.82499856948853 ; top5 ->  78.69166389465332  and loss:  385.13005900382996
test acc: top1 ->  60.635998270988466 ; top5 ->  83.71999741973877  and loss:  1382.2733310833573
forward train acc: top1 ->  56.02499864578247 ; top5 ->  78.33333074569703  and loss:  391.53470408916473
test acc: top1 ->  59.44399826316833 ; top5 ->  82.77999747619629  and loss:  1430.6305889040232
forward train acc: top1 ->  55.51666530609131 ; top5 ->  78.23333080291748  and loss:  392.1291778087616
test acc: top1 ->  60.091998266601564 ; top5 ->  83.44199753417969  and loss:  1402.999491751194
forward train acc: top1 ->  57.733332023620605 ; top5 ->  79.74166408538818  and loss:  367.652955532074
test acc: top1 ->  62.27199818077087 ; top5 ->  84.61799754333497  and loss:  1321.089447543025
forward train acc: top1 ->  57.92499849319458 ; top5 ->  80.14999732971191  and loss:  366.53985118865967
test acc: top1 ->  62.679998189544676 ; top5 ->  84.90799749221802  and loss:  1307.2786712795496
forward train acc: top1 ->  57.791665153503416 ; top5 ->  79.8416642189026  and loss:  371.12894105911255
test acc: top1 ->  62.689998169708254 ; top5 ->  84.81799752044678  and loss:  1307.635351702571
forward train acc: top1 ->  58.03333194732666 ; top5 ->  80.65833076477051  and loss:  362.0691809654236
test acc: top1 ->  63.88199807395935 ; top5 ->  85.53599755859375  and loss:  1263.9586368650198
forward train acc: top1 ->  58.4749986076355 ; top5 ->  80.58333101272584  and loss:  360.8850201368332
test acc: top1 ->  63.823998076629636 ; top5 ->  85.64599745941162  and loss:  1258.1781383007765
forward train acc: top1 ->  59.6333318901062 ; top5 ->  80.85833072662354  and loss:  354.022891998291
test acc: top1 ->  64.02399816894531 ; top5 ->  85.82199750061035  and loss:  1248.2196974754333
forward train acc: top1 ->  60.34999832153321 ; top5 ->  81.49999736785888  and loss:  346.71948021650314
test acc: top1 ->  64.47799817314149 ; top5 ->  86.11999746246337  and loss:  1232.757293894887
>>>>>>> reverse layer  22  since performance drop >>>>>>>
==> this epoch:  208 / 256 , inc:  1
---------------- start layer  23  ---------------
adv train loss:  -85.98280942440033 , diff:  85.98280942440033
adv train loss:  -86.588742852211 , diff:  0.605933427810669
layer  23  adv train finish, try to retain  213
>>>>>>> reverse layer  23  since no improvement >>>>>>>
---------------- start layer  24  ---------------
### skip layer  24 wait:  4  ###
---------------- start layer  25  ---------------
adv train loss:  -88.81702542304993 , diff:  88.81702542304993
adv train loss:  -85.28048861026764 , diff:  3.5365368127822876
adv train loss:  -90.04828572273254 , diff:  4.767797112464905
adv train loss:  -87.48200726509094 , diff:  2.5662784576416016
adv train loss:  -87.73596179485321 , diff:  0.25395452976226807
************ all values are small in this layer **********
layer  25  adv train finish, try to retain  241
test acc: top1 ->  63.961998056411744 ; top5 ->  85.87999743652344  and loss:  1248.8281380981207
forward train acc: top1 ->  55.933331985473636 ; top5 ->  78.51666423797607  and loss:  387.8241082429886
test acc: top1 ->  59.99799828338623 ; top5 ->  83.29799744415283  and loss:  1421.0683694332838
forward train acc: top1 ->  54.19166551589966 ; top5 ->  77.37499740600586  and loss:  404.10039508342743
test acc: top1 ->  59.835998362350466 ; top5 ->  83.09199763336181  and loss:  1412.5477559119463
forward train acc: top1 ->  54.86666538238526 ; top5 ->  78.25833086013795  and loss:  393.83932197093964
test acc: top1 ->  59.411998276519775 ; top5 ->  82.74799755249023  and loss:  1435.2462720125914
forward train acc: top1 ->  57.13333192825317 ; top5 ->  79.19166412353516  and loss:  378.571942448616
test acc: top1 ->  62.38399819526672 ; top5 ->  84.7519975906372  and loss:  1320.3233002573252
forward train acc: top1 ->  57.69999864578247 ; top5 ->  79.54999725341797  and loss:  369.59453773498535
test acc: top1 ->  62.71199823799133 ; top5 ->  84.93599750366211  and loss:  1303.572958856821
forward train acc: top1 ->  57.68333185195923 ; top5 ->  79.98333087921142  and loss:  367.9839984178543
test acc: top1 ->  63.007998155593874 ; top5 ->  85.19399771118164  and loss:  1292.4068175554276
forward train acc: top1 ->  59.34999881744385 ; top5 ->  81.0499974822998  and loss:  356.6787623167038
test acc: top1 ->  64.07199809265137 ; top5 ->  85.67999748687744  and loss:  1257.6569636017084
forward train acc: top1 ->  59.82499855041504 ; top5 ->  81.17499740600586  and loss:  351.7225276231766
test acc: top1 ->  64.29799818572998 ; top5 ->  85.96399746398926  and loss:  1247.8605031520128
forward train acc: top1 ->  60.84166519165039 ; top5 ->  81.7833306312561  and loss:  344.9903860092163
test acc: top1 ->  64.601998122406 ; top5 ->  86.03599742736816  and loss:  1237.2281493097544
forward train acc: top1 ->  59.75833179473877 ; top5 ->  81.2416641998291  and loss:  349.88314163684845
test acc: top1 ->  64.82999816818237 ; top5 ->  86.29599749755859  and loss:  1224.3251121789217
>>>>>>> reverse layer  25  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  26  ---------------
### skip layer  26 wait:  4  ###
---------------- start layer  27  ---------------
### skip layer  27 wait:  4  ###
---------------- start layer  28  ---------------
### skip layer  28 wait:  3  ###
---------------- start layer  29  ---------------
### skip layer  29 wait:  4  ###
---------------- start layer  30  ---------------
### skip layer  30 wait:  4  ###
---------------- start layer  31  ---------------
### skip layer  31 wait:  4  ###
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.00375, 0.00375, 0.000703125, 0.00375, 0.00375, 0.00375, 0.000703125, 0.00017578125, 0.005, 0.000703125, 0.005, 0.005, 0.005, 0.005, 8.7890625e-05, 8.7890625e-05, 0.0003515625, 0.0009375, 0.0003515625, 0.0025, 0.0003515625, 0.00017578125, 0.0009375, 0.0025, 0.00017578125, 0.0003515625, 8.7890625e-05, 8.7890625e-05, 4.39453125e-05, 8.7890625e-05, 8.7890625e-05, 8.7890625e-05]  wait [2, 2, 3, 2, 2, 2, 4, 2, 0, 4, 0, 0, 0, 0, 2, 2, 4, 2, 4, 0, 4, 3, 2, 0, 3, 4, 3, 3, 2, 3, 3, 3]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 0
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  6  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -84.03231298923492 , diff:  84.03231298923492
adv train loss:  -84.07798147201538 , diff:  0.04566848278045654
layer  0  adv train finish, try to retain  49
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -89.88174569606781 , diff:  89.88174569606781
adv train loss:  -92.48554253578186 , diff:  2.6037968397140503
adv train loss:  -88.39107632637024 , diff:  4.094466209411621
adv train loss:  -86.35814619064331 , diff:  2.0329301357269287
adv train loss:  -90.92027521133423 , diff:  4.562129020690918
adv train loss:  -90.47489845752716 , diff:  0.44537675380706787
layer  1  adv train finish, try to retain  50
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
adv train loss:  -86.75870299339294 , diff:  86.75870299339294
adv train loss:  -86.5329555273056 , diff:  0.2257474660873413
layer  3  adv train finish, try to retain  50
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -87.86797595024109 , diff:  87.86797595024109
adv train loss:  -90.29772078990936 , diff:  2.429744839668274
adv train loss:  -91.70325696468353 , diff:  1.40553617477417
adv train loss:  -86.85747969150543 , diff:  4.845777273178101
adv train loss:  -89.18967425823212 , diff:  2.3321945667266846
adv train loss:  -85.28335428237915 , diff:  3.9063199758529663
adv train loss:  -87.71673214435577 , diff:  2.4333778619766235
adv train loss:  -86.24680745601654 , diff:  1.4699246883392334
adv train loss:  -88.74748313426971 , diff:  2.500675678253174
adv train loss:  -86.38182878494263 , diff:  2.3656543493270874
layer  4  adv train finish, try to retain  47
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -84.45767915248871 , diff:  84.45767915248871
adv train loss:  -85.79249477386475 , diff:  1.3348156213760376
adv train loss:  -86.62961149215698 , diff:  0.8371167182922363
layer  5  adv train finish, try to retain  53
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
adv train loss:  -85.56399518251419 , diff:  85.56399518251419
adv train loss:  -83.80875957012177 , diff:  1.7552356123924255
adv train loss:  -86.49045586585999 , diff:  2.68169629573822
adv train loss:  -85.29800248146057 , diff:  1.192453384399414
adv train loss:  -86.6871178150177 , diff:  1.389115333557129
adv train loss:  -89.85441982746124 , diff:  3.1673020124435425
adv train loss:  -86.72794389724731 , diff:  3.1264759302139282
adv train loss:  -86.84149539470673 , diff:  0.11355149745941162
layer  7  adv train finish, try to retain  123
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -84.02540326118469 , diff:  84.02540326118469
adv train loss:  -85.84798347949982 , diff:  1.8225802183151245
adv train loss:  -87.12241554260254 , diff:  1.2744320631027222
adv train loss:  -85.88957142829895 , diff:  1.2328441143035889
adv train loss:  -86.84941911697388 , diff:  0.9598476886749268
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  86
test acc: top1 ->  62.82999813957214 ; top5 ->  84.79799749145508  and loss:  1301.5373086929321
forward train acc: top1 ->  55.92499872207642 ; top5 ->  78.92499771118165  and loss:  383.4446130990982
test acc: top1 ->  59.65399830818176 ; top5 ->  83.09999746551513  and loss:  1422.094821318984
forward train acc: top1 ->  55.5083320236206 ; top5 ->  79.32499773025512  and loss:  383.4386878013611
test acc: top1 ->  59.97399830245972 ; top5 ->  83.56399741668702  and loss:  1400.8100492209196
forward train acc: top1 ->  55.008332080841065 ; top5 ->  77.54166399002075  and loss:  396.86607003211975
test acc: top1 ->  59.57199832878113 ; top5 ->  83.20599751739502  and loss:  1418.3244980424643
forward train acc: top1 ->  56.6333318901062 ; top5 ->  78.74999761581421  and loss:  382.4354249238968
test acc: top1 ->  62.395998190307616 ; top5 ->  84.63799742431641  and loss:  1319.3224080204964
forward train acc: top1 ->  56.84999864578247 ; top5 ->  78.90833040237426  and loss:  377.17001152038574
test acc: top1 ->  62.7019981048584 ; top5 ->  84.86799744873046  and loss:  1302.4998633265495
forward train acc: top1 ->  58.42499851226807 ; top5 ->  80.73333095550537  and loss:  363.6081358194351
test acc: top1 ->  62.735998194885255 ; top5 ->  84.90999754638672  and loss:  1295.2974738255143
forward train acc: top1 ->  59.499998550415036 ; top5 ->  81.19166408538818  and loss:  356.2935701608658
test acc: top1 ->  63.83199802398681 ; top5 ->  85.61999749145508  and loss:  1256.5611200332642
forward train acc: top1 ->  59.50833168029785 ; top5 ->  80.88333045959473  and loss:  354.76195311546326
test acc: top1 ->  64.02799807815552 ; top5 ->  85.81599749908447  and loss:  1248.5574343949556
forward train acc: top1 ->  59.25833196640015 ; top5 ->  81.27499748229981  and loss:  350.56757938861847
test acc: top1 ->  64.11599814682006 ; top5 ->  85.85399747314453  and loss:  1246.662100583315
forward train acc: top1 ->  60.3333317565918 ; top5 ->  81.43333118438721  and loss:  346.59585559368134
test acc: top1 ->  64.55799809570313 ; top5 ->  86.04799756622315  and loss:  1230.0520074516535
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  9  ---------------
### skip layer  9 wait:  4  ###
---------------- start layer  10  ---------------
adv train loss:  -82.79973828792572 , diff:  82.79973828792572
adv train loss:  -84.54899513721466 , diff:  1.7492568492889404
adv train loss:  -87.13202381134033 , diff:  2.5830286741256714
adv train loss:  -82.87229299545288 , diff:  4.259730815887451
adv train loss:  -85.61393582820892 , diff:  2.7416428327560425
adv train loss:  -87.73499429225922 , diff:  2.121058464050293
adv train loss:  -85.14246225357056 , diff:  2.5925320386886597
adv train loss:  -85.78640246391296 , diff:  0.6439402103424072
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  86
test acc: top1 ->  63.74399812011719 ; top5 ->  85.58799751586913  and loss:  1259.6412546485662
forward train acc: top1 ->  56.68333194732666 ; top5 ->  78.95833084106445  and loss:  382.8553453683853
test acc: top1 ->  60.31799826812744 ; top5 ->  83.47199751739502  and loss:  1397.3815619945526
forward train acc: top1 ->  55.24166534423828 ; top5 ->  78.34999757766724  and loss:  392.1321499347687
test acc: top1 ->  60.335998237228395 ; top5 ->  83.28399741973877  and loss:  1403.2784411460161
forward train acc: top1 ->  55.08333194732666 ; top5 ->  78.13333078384399  and loss:  394.1899790763855
test acc: top1 ->  60.3279982711792 ; top5 ->  83.4439975982666  and loss:  1398.191924072802
forward train acc: top1 ->  57.499998550415036 ; top5 ->  79.49999763488769  and loss:  378.05649065971375
test acc: top1 ->  62.691998199462894 ; top5 ->  84.87999752197265  and loss:  1306.7829235196114
forward train acc: top1 ->  57.40833200454712 ; top5 ->  79.29166423797608  and loss:  373.9549252986908
test acc: top1 ->  62.5579981880188 ; top5 ->  85.08999747619629  and loss:  1302.6269309669733
forward train acc: top1 ->  58.066665191650394 ; top5 ->  80.14166397094726  and loss:  366.45932137966156
test acc: top1 ->  63.02199818840027 ; top5 ->  85.1939974822998  and loss:  1287.5455329492688
forward train acc: top1 ->  59.31666511535644 ; top5 ->  80.89999748229981  and loss:  353.62192153930664
test acc: top1 ->  63.9179981388092 ; top5 ->  85.63799738616943  and loss:  1257.9304916411638
forward train acc: top1 ->  59.5499984741211 ; top5 ->  81.46666400909425  and loss:  350.5221583843231
test acc: top1 ->  64.1539981590271 ; top5 ->  85.86999753723144  and loss:  1245.002247929573
forward train acc: top1 ->  60.17499856948852 ; top5 ->  81.39999748229981  and loss:  348.5780019760132
test acc: top1 ->  64.35799814338684 ; top5 ->  86.08399760131836  and loss:  1237.6107153743505
forward train acc: top1 ->  60.20833166122436 ; top5 ->  81.3583303451538  and loss:  347.4087728857994
test acc: top1 ->  64.78799806671142 ; top5 ->  86.3259975769043  and loss:  1222.3613760918379
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -87.01650774478912 , diff:  87.01650774478912
adv train loss:  -86.74885213375092 , diff:  0.267655611038208
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  99
test acc: top1 ->  62.265998134613035 ; top5 ->  84.51599766235351  and loss:  1323.2136783897877
forward train acc: top1 ->  56.25833198547363 ; top5 ->  78.70833068847656  and loss:  389.2824627161026
test acc: top1 ->  60.21999832611084 ; top5 ->  83.30199760742188  and loss:  1414.7595372796059
forward train acc: top1 ->  55.091665287017825 ; top5 ->  77.51666397094726  and loss:  396.8070892095566
test acc: top1 ->  60.583998320961 ; top5 ->  83.58599748687745  and loss:  1381.1784299015999
forward train acc: top1 ->  55.674998874664304 ; top5 ->  78.6499971961975  and loss:  390.6385865211487
test acc: top1 ->  60.231998337554934 ; top5 ->  83.7159974899292  and loss:  1386.4513757377863
forward train acc: top1 ->  57.34166534423828 ; top5 ->  79.1833307647705  and loss:  376.0899603366852
test acc: top1 ->  62.72799824142456 ; top5 ->  84.99399755096435  and loss:  1303.0886591598392
forward train acc: top1 ->  57.408331813812254 ; top5 ->  79.67499719619751  and loss:  372.93491661548615
test acc: top1 ->  63.145998135185245 ; top5 ->  85.31799748840332  and loss:  1286.9029301702976
forward train acc: top1 ->  57.62499868392944 ; top5 ->  79.63333065032958  and loss:  367.46296775341034
test acc: top1 ->  63.25199824523926 ; top5 ->  85.36599735870361  and loss:  1281.0365385264158
forward train acc: top1 ->  59.83333194732666 ; top5 ->  81.00833065032958  and loss:  351.0438721179962
test acc: top1 ->  64.20199819641114 ; top5 ->  85.93199749298095  and loss:  1247.7801993787289
forward train acc: top1 ->  59.66666524887085 ; top5 ->  81.01666423797607  and loss:  351.0006710886955
test acc: top1 ->  64.38599815597534 ; top5 ->  86.04599760284424  and loss:  1239.4819668754935
forward train acc: top1 ->  60.85833166122436 ; top5 ->  82.24166374206543  and loss:  341.61740201711655
test acc: top1 ->  64.56199806137084 ; top5 ->  86.15999751129151  and loss:  1232.0005887970328
forward train acc: top1 ->  59.94166532516479 ; top5 ->  80.68333103179931  and loss:  353.84034579992294
test acc: top1 ->  65.02999794464111 ; top5 ->  86.41199745178223  and loss:  1216.3530245423317
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -85.75209522247314 , diff:  85.75209522247314
adv train loss:  -85.80154848098755 , diff:  0.0494532585144043
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  97
test acc: top1 ->  64.10599822273254 ; top5 ->  85.83599766235352  and loss:  1248.2680436819792
forward train acc: top1 ->  57.12499864578247 ; top5 ->  79.9833309173584  and loss:  372.2964789867401
test acc: top1 ->  60.14799831581116 ; top5 ->  83.44399752502441  and loss:  1402.6631802767515
forward train acc: top1 ->  55.47499870300293 ; top5 ->  78.47499753952026  and loss:  389.01151871681213
test acc: top1 ->  60.55399834327698 ; top5 ->  83.80199738006591  and loss:  1375.0913178175688
forward train acc: top1 ->  55.691665439605714 ; top5 ->  78.4999974822998  and loss:  387.15388667583466
test acc: top1 ->  60.68399825363159 ; top5 ->  83.90999749755859  and loss:  1374.5916704535484
forward train acc: top1 ->  57.08333194732666 ; top5 ->  79.41666408538818  and loss:  374.3599967956543
test acc: top1 ->  62.88399818191528 ; top5 ->  85.01599759521484  and loss:  1298.423063442111
forward train acc: top1 ->  58.54999851226807 ; top5 ->  80.20833065032959  and loss:  361.5881915092468
test acc: top1 ->  63.163998094558714 ; top5 ->  85.17199751739501  and loss:  1288.2908463776112
forward train acc: top1 ->  58.16666526794434 ; top5 ->  80.35833087921142  and loss:  362.7023993730545
test acc: top1 ->  63.037998294448855 ; top5 ->  85.22399752502442  and loss:  1287.6411668658257
forward train acc: top1 ->  60.0499984741211 ; top5 ->  81.51666427612305  and loss:  350.0389454960823
test acc: top1 ->  64.07799811820983 ; top5 ->  85.72799757080078  and loss:  1249.6444145590067
forward train acc: top1 ->  59.34999856948853 ; top5 ->  80.40833068847657  and loss:  355.10196125507355
test acc: top1 ->  64.26399815292359 ; top5 ->  85.94199762878418  and loss:  1236.7126858010888
forward train acc: top1 ->  59.983331775665285 ; top5 ->  81.78333068847657  and loss:  343.5337710380554
test acc: top1 ->  64.31799821891785 ; top5 ->  85.92799745330811  and loss:  1241.426590025425
forward train acc: top1 ->  61.37499828338623 ; top5 ->  81.8583309173584  and loss:  337.889851808548
test acc: top1 ->  64.785998097229 ; top5 ->  86.29799751586914  and loss:  1223.2542773261666
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  98 / 128 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -87.55648100376129 , diff:  87.55648100376129
adv train loss:  -85.19131708145142 , diff:  2.3651639223098755
adv train loss:  -84.81951880455017 , diff:  0.3717982769012451
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  70
test acc: top1 ->  59.989998313140866 ; top5 ->  82.89399747772217  and loss:  1413.170851662755
forward train acc: top1 ->  56.26666526794433 ; top5 ->  78.64166418075561  and loss:  387.9076931476593
test acc: top1 ->  60.15399827690125 ; top5 ->  83.32399749603272  and loss:  1395.5194093734026
forward train acc: top1 ->  55.44999876022339 ; top5 ->  78.37499771118163  and loss:  390.20364797115326
test acc: top1 ->  60.56199817085266 ; top5 ->  83.63199756317138  and loss:  1386.7351197451353
forward train acc: top1 ->  55.7833320236206 ; top5 ->  78.2666640472412  and loss:  392.1040064096451
test acc: top1 ->  59.799998320007326 ; top5 ->  82.99799758834838  and loss:  1421.7665786892176
forward train acc: top1 ->  57.11666540145874 ; top5 ->  79.07499752044677  and loss:  378.82506799697876
test acc: top1 ->  62.58399811973572 ; top5 ->  84.85799748229981  and loss:  1307.0411310493946
forward train acc: top1 ->  58.224998512268066 ; top5 ->  80.63333074569702  and loss:  361.8628690838814
test acc: top1 ->  62.827998175811764 ; top5 ->  85.1039974975586  and loss:  1298.247265547514
forward train acc: top1 ->  58.4999986076355 ; top5 ->  80.7416641998291  and loss:  356.6644846200943
test acc: top1 ->  62.77399810333252 ; top5 ->  85.08999757385254  and loss:  1288.4834214299917
forward train acc: top1 ->  59.98333179473877 ; top5 ->  81.21666402816773  and loss:  350.1525970697403
test acc: top1 ->  64.11599818115235 ; top5 ->  85.82599748535156  and loss:  1247.3716270029545
forward train acc: top1 ->  59.166665401458744 ; top5 ->  80.82499744415283  and loss:  353.5117905139923
test acc: top1 ->  64.22999813232421 ; top5 ->  85.93999751586914  and loss:  1240.0415978729725
forward train acc: top1 ->  60.19166513442993 ; top5 ->  81.36666439056397  and loss:  345.85615706443787
test acc: top1 ->  64.42199809951782 ; top5 ->  86.03399745635987  and loss:  1237.5312415510416
forward train acc: top1 ->  59.26666526794433 ; top5 ->  81.00833084106445  and loss:  352.20564222335815
test acc: top1 ->  64.7959981426239 ; top5 ->  86.34999750976563  and loss:  1217.3960577994585
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  71 / 128 , inc:  1
---------------- start layer  14  ---------------
adv train loss:  -86.80834013223648 , diff:  86.80834013223648
adv train loss:  -85.03989362716675 , diff:  1.7684465050697327
adv train loss:  -92.06418311595917 , diff:  7.024289488792419
adv train loss:  -79.86949098110199 , diff:  12.194692134857178
adv train loss:  -86.71952319145203 , diff:  6.850032210350037
adv train loss:  -88.46310222148895 , diff:  1.7435790300369263
adv train loss:  -86.95637822151184 , diff:  1.5067239999771118
adv train loss:  -89.95702636241913 , diff:  3.0006481409072876
adv train loss:  -84.91411411762238 , diff:  5.042912244796753
adv train loss:  -86.14151525497437 , diff:  1.2274011373519897
layer  14  adv train finish, try to retain  249
test acc: top1 ->  64.51199806404114 ; top5 ->  86.2579974533081  and loss:  1227.037606626749
forward train acc: top1 ->  57.56666509628296 ; top5 ->  80.02499759674072  and loss:  371.3851623535156
test acc: top1 ->  60.31599819641113 ; top5 ->  83.42199755706787  and loss:  1398.4527956396341
forward train acc: top1 ->  56.59999858856201 ; top5 ->  78.84166402816773  and loss:  378.69929134845734
test acc: top1 ->  60.00199830474853 ; top5 ->  83.26999755401611  and loss:  1396.2779521644115
forward train acc: top1 ->  55.92499858856201 ; top5 ->  78.94999740600586  and loss:  381.9083516597748
test acc: top1 ->  60.765998234558104 ; top5 ->  83.8299974746704  and loss:  1381.3437575995922
forward train acc: top1 ->  56.949998722076415 ; top5 ->  79.1249974822998  and loss:  376.7483739852905
test acc: top1 ->  62.76999817123413 ; top5 ->  85.1939974395752  and loss:  1297.5024637281895
forward train acc: top1 ->  57.7416654586792 ; top5 ->  80.05833106994629  and loss:  365.60416531562805
test acc: top1 ->  63.08599820327759 ; top5 ->  85.24199745483398  and loss:  1284.6566885113716
forward train acc: top1 ->  58.90833200454712 ; top5 ->  80.9166641998291  and loss:  355.6771069765091
test acc: top1 ->  63.33999808883667 ; top5 ->  85.48199757843018  and loss:  1272.3553662896156
forward train acc: top1 ->  58.649998607635496 ; top5 ->  81.01666423797607  and loss:  353.99296832084656
test acc: top1 ->  64.11799811553955 ; top5 ->  85.99799750213623  and loss:  1238.0529667139053
forward train acc: top1 ->  60.19999855041504 ; top5 ->  81.15833087921142  and loss:  349.7394725084305
test acc: top1 ->  64.58599819488525 ; top5 ->  86.35399752502441  and loss:  1222.518446996808
forward train acc: top1 ->  60.90833177566528 ; top5 ->  82.35833080291748  and loss:  340.3755567073822
test acc: top1 ->  64.68999804458618 ; top5 ->  86.38399744415283  and loss:  1221.7309257388115
forward train acc: top1 ->  61.374998588562015 ; top5 ->  82.29999755859374  and loss:  334.68329960107803
test acc: top1 ->  64.85999806365967 ; top5 ->  86.52399754486083  and loss:  1212.8717521429062
>>>>>>> reverse layer  14  since performance drop >>>>>>>
==> this epoch:  250 / 256 , inc:  1
---------------- start layer  15  ---------------
adv train loss:  -87.31637907028198 , diff:  87.31637907028198
adv train loss:  -84.95102339982986 , diff:  2.365355670452118
adv train loss:  -85.9167948961258 , diff:  0.965771496295929
layer  15  adv train finish, try to retain  251
>>>>>>> reverse layer  15  since no improvement >>>>>>>
---------------- start layer  16  ---------------
### skip layer  16 wait:  4  ###
---------------- start layer  17  ---------------
adv train loss:  -83.47325909137726 , diff:  83.47325909137726
adv train loss:  -85.16607701778412 , diff:  1.6928179264068604
adv train loss:  -85.8651430606842 , diff:  0.6990660429000854
layer  17  adv train finish, try to retain  218
test acc: top1 ->  64.15999811096191 ; top5 ->  85.90599750823975  and loss:  1243.7094069421291
forward train acc: top1 ->  56.60833194732666 ; top5 ->  79.16666404724121  and loss:  376.8722245693207
test acc: top1 ->  60.13599818725586 ; top5 ->  83.30999756469727  and loss:  1402.9389654397964
forward train acc: top1 ->  57.049998626708984 ; top5 ->  78.99166421890259  and loss:  377.54288214445114
test acc: top1 ->  60.77799821548462 ; top5 ->  83.99799745941162  and loss:  1362.0215011686087
forward train acc: top1 ->  56.974998722076414 ; top5 ->  79.44166379928589  and loss:  377.7195700407028
test acc: top1 ->  60.13199826488495 ; top5 ->  83.45599756469727  and loss:  1396.0089627504349
forward train acc: top1 ->  57.524998569488524 ; top5 ->  79.74166418075562  and loss:  367.9803116321564
test acc: top1 ->  62.883998164176944 ; top5 ->  85.19399750671387  and loss:  1294.7491129934788
forward train acc: top1 ->  58.774998626708985 ; top5 ->  80.46666381835938  and loss:  360.9727921485901
test acc: top1 ->  63.329998102188114 ; top5 ->  85.35799750823975  and loss:  1278.2865866273642
forward train acc: top1 ->  58.55833194732666 ; top5 ->  80.44166412353516  and loss:  360.99324572086334
test acc: top1 ->  63.12999812164307 ; top5 ->  85.3319975479126  and loss:  1274.8069293946028
forward train acc: top1 ->  60.14166513442993 ; top5 ->  81.54166431427002  and loss:  349.1215628385544
test acc: top1 ->  64.25999806747437 ; top5 ->  85.9179974685669  and loss:  1239.1694652587175
forward train acc: top1 ->  60.09166538238525 ; top5 ->  81.68333072662354  and loss:  346.1725444793701
test acc: top1 ->  64.44599814758301 ; top5 ->  86.10399743652344  and loss:  1229.6321950107813
forward train acc: top1 ->  60.08333181381226 ; top5 ->  81.75833087921143  and loss:  343.1602148413658
test acc: top1 ->  64.69599804840088 ; top5 ->  86.32399738922119  and loss:  1225.1069944500923
forward train acc: top1 ->  61.191664905548095 ; top5 ->  82.09166439056396  and loss:  337.57422441244125
test acc: top1 ->  64.9199982559204 ; top5 ->  86.40799733886719  and loss:  1212.8580707907677
>>>>>>> reverse layer  17  since performance drop >>>>>>>
==> this epoch:  230 / 256 , inc:  1
---------------- start layer  18  ---------------
### skip layer  18 wait:  4  ###
---------------- start layer  19  ---------------
adv train loss:  -85.0105642080307 , diff:  85.0105642080307
adv train loss:  -88.26711344718933 , diff:  3.2565492391586304
adv train loss:  -85.19032907485962 , diff:  3.076784372329712
adv train loss:  -84.0315123796463 , diff:  1.1588166952133179
adv train loss:  -86.46866261959076 , diff:  2.437150239944458
adv train loss:  -83.17063534259796 , diff:  3.298027276992798
adv train loss:  -86.74327278137207 , diff:  3.572637438774109
adv train loss:  -88.70046615600586 , diff:  1.957193374633789
adv train loss:  -87.12984299659729 , diff:  1.5706231594085693
adv train loss:  -85.94731783866882 , diff:  1.1825251579284668
layer  19  adv train finish, try to retain  187
>>>>>>> reverse layer  19  since no improvement >>>>>>>
---------------- start layer  20  ---------------
### skip layer  20 wait:  4  ###
---------------- start layer  21  ---------------
### skip layer  21 wait:  3  ###
---------------- start layer  22  ---------------
adv train loss:  -82.12166166305542 , diff:  82.12166166305542
adv train loss:  -89.23518931865692 , diff:  7.1135276556015015
adv train loss:  -83.05049729347229 , diff:  6.184692025184631
adv train loss:  -86.94585907459259 , diff:  3.8953617811203003
adv train loss:  -88.50277876853943 , diff:  1.5569196939468384
adv train loss:  -83.50194931030273 , diff:  5.000829458236694
adv train loss:  -85.40732073783875 , diff:  1.9053714275360107
adv train loss:  -87.13543248176575 , diff:  1.728111743927002
adv train loss:  -83.93327975273132 , diff:  3.202152729034424
adv train loss:  -87.51232504844666 , diff:  3.579045295715332
layer  22  adv train finish, try to retain  224
>>>>>>> reverse layer  22  since no improvement >>>>>>>
---------------- start layer  23  ---------------
adv train loss:  -86.54146361351013 , diff:  86.54146361351013
adv train loss:  -87.61444425582886 , diff:  1.0729806423187256
adv train loss:  -88.34753549098969 , diff:  0.7330912351608276
layer  23  adv train finish, try to retain  197
>>>>>>> reverse layer  23  since no improvement >>>>>>>
---------------- start layer  24  ---------------
### skip layer  24 wait:  3  ###
---------------- start layer  25  ---------------
### skip layer  25 wait:  4  ###
---------------- start layer  26  ---------------
### skip layer  26 wait:  3  ###
---------------- start layer  27  ---------------
### skip layer  27 wait:  3  ###
---------------- start layer  28  ---------------
adv train loss:  -87.44798028469086 , diff:  87.44798028469086
adv train loss:  -84.61158990859985 , diff:  2.8363903760910034
adv train loss:  -83.54145139455795 , diff:  1.0701385140419006
adv train loss:  -85.83528733253479 , diff:  2.293835937976837
adv train loss:  -83.72615247964859 , diff:  2.1091348528862
adv train loss:  -84.43279945850372 , diff:  0.7066469788551331
layer  28  adv train finish, try to retain  504
>>>>>>> reverse layer  28  since no improvement >>>>>>>
---------------- start layer  29  ---------------
### skip layer  29 wait:  3  ###
---------------- start layer  30  ---------------
### skip layer  30 wait:  3  ###
---------------- start layer  31  ---------------
### skip layer  31 wait:  3  ###
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.0075, 0.0075, 0.000703125, 0.0075, 0.0075, 0.0075, 0.000703125, 0.0003515625, 0.00375, 0.000703125, 0.00375, 0.00375, 0.00375, 0.00375, 6.591796875e-05, 0.00017578125, 0.0003515625, 0.000703125, 0.0003515625, 0.005, 0.0003515625, 0.00017578125, 0.001875, 0.005, 0.00017578125, 0.0003515625, 8.7890625e-05, 8.7890625e-05, 8.7890625e-05, 8.7890625e-05, 8.7890625e-05, 8.7890625e-05]  wait [2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 4, 2, 3, 4, 3, 0, 3, 2, 2, 0, 2, 3, 2, 2, 2, 2, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 0
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  7  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -87.04964685440063 , diff:  87.04964685440063
adv train loss:  -87.39328110218048 , diff:  0.3436342477798462
layer  0  adv train finish, try to retain  44
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -85.11671888828278 , diff:  85.11671888828278
adv train loss:  -87.84161818027496 , diff:  2.7248992919921875
adv train loss:  -88.98313355445862 , diff:  1.1415153741836548
adv train loss:  -86.8062391281128 , diff:  2.176894426345825
adv train loss:  -86.32861721515656 , diff:  0.4776219129562378
layer  1  adv train finish, try to retain  49
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -85.4983024597168 , diff:  85.4983024597168
adv train loss:  -84.4083960056305 , diff:  1.0899064540863037
adv train loss:  -89.76316750049591 , diff:  5.3547714948654175
adv train loss:  -83.03979706764221 , diff:  6.723370432853699
adv train loss:  -84.23943996429443 , diff:  1.1996428966522217
adv train loss:  -86.98882901668549 , diff:  2.7493890523910522
adv train loss:  -84.23530435562134 , diff:  2.753524661064148
adv train loss:  -86.70093417167664 , diff:  2.465629816055298
adv train loss:  -85.24164044857025 , diff:  1.4592937231063843
adv train loss:  -84.38670861721039 , diff:  0.8549318313598633
************ all values are small in this layer **********
layer  2  adv train finish, try to retain  56
test acc: top1 ->  64.24999805297851 ; top5 ->  85.85199747619629  and loss:  1243.7261575907469
forward train acc: top1 ->  57.133331832885744 ; top5 ->  79.15833066940307  and loss:  374.47803807258606
test acc: top1 ->  60.77399833106995 ; top5 ->  83.70399749450684  and loss:  1386.4609132111073
forward train acc: top1 ->  56.49166536331177 ; top5 ->  79.05833087921143  and loss:  382.6727167367935
test acc: top1 ->  61.07599836883545 ; top5 ->  83.81799749603272  and loss:  1374.2345712035894
forward train acc: top1 ->  56.04999855041504 ; top5 ->  78.58333076477051  and loss:  386.0362048149109
test acc: top1 ->  60.91199823608398 ; top5 ->  83.8179975769043  and loss:  1376.7451199293137
forward train acc: top1 ->  57.09166534423828 ; top5 ->  79.38333072662354  and loss:  376.27443289756775
test acc: top1 ->  63.035998204422 ; top5 ->  85.01799746398926  and loss:  1295.8571798354387
forward train acc: top1 ->  58.18333181381226 ; top5 ->  80.39166400909424  and loss:  365.0767687559128
test acc: top1 ->  63.423998173522946 ; top5 ->  85.20199753112793  and loss:  1285.0640746504068
forward train acc: top1 ->  58.683331985473636 ; top5 ->  80.90833084106445  and loss:  355.24185943603516
test acc: top1 ->  63.50599812011719 ; top5 ->  85.50599752502441  and loss:  1268.099687114358
forward train acc: top1 ->  60.64166519165039 ; top5 ->  81.40833068847657  and loss:  347.67141032218933
test acc: top1 ->  64.50199817886353 ; top5 ->  86.0819975845337  and loss:  1231.662399366498
forward train acc: top1 ->  60.09166524887085 ; top5 ->  82.0749974822998  and loss:  340.7524186372757
test acc: top1 ->  64.8439981388092 ; top5 ->  86.14599754943848  and loss:  1228.258986197412
forward train acc: top1 ->  60.92499826431274 ; top5 ->  81.60833084106446  and loss:  344.7416812181473
test acc: top1 ->  64.93599816207886 ; top5 ->  86.31599750061035  and loss:  1219.0674214065075
forward train acc: top1 ->  60.81666498184204 ; top5 ->  82.17499744415284  and loss:  340.2487715482712
test acc: top1 ->  65.17799812850951 ; top5 ->  86.53599756317139  and loss:  1205.7504515200853
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  57 / 64 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -84.81879949569702 , diff:  84.81879949569702
adv train loss:  -86.44307315349579 , diff:  1.624273657798767
adv train loss:  -84.13699942827225 , diff:  2.3060737252235413
adv train loss:  -84.5901962518692 , diff:  0.45319682359695435
layer  3  adv train finish, try to retain  45
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -81.58552014827728 , diff:  81.58552014827728
adv train loss:  -87.60243153572083 , diff:  6.0169113874435425
adv train loss:  -85.6070955991745 , diff:  1.9953359365463257
adv train loss:  -83.27500879764557 , diff:  2.3320868015289307
adv train loss:  -85.73721265792847 , diff:  2.462203860282898
adv train loss:  -88.2199136018753 , diff:  2.4827009439468384
adv train loss:  -84.75918811559677 , diff:  3.460725486278534
adv train loss:  -80.1309996843338 , diff:  4.62818843126297
adv train loss:  -88.02126038074493 , diff:  7.890260696411133
adv train loss:  -85.92956268787384 , diff:  2.0916976928710938
layer  4  adv train finish, try to retain  41
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -83.9232931137085 , diff:  83.9232931137085
adv train loss:  -84.54980432987213 , diff:  0.6265112161636353
layer  5  adv train finish, try to retain  49
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
### skip layer  6 wait:  3  ###
---------------- start layer  7  ---------------
adv train loss:  -88.52896785736084 , diff:  88.52896785736084
adv train loss:  -86.37464487552643 , diff:  2.1543229818344116
adv train loss:  -84.26888883113861 , diff:  2.1057560443878174
adv train loss:  -84.26007175445557 , diff:  0.008817076683044434
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  121
test acc: top1 ->  63.831998122406006 ; top5 ->  85.71799748077393  and loss:  1260.1547524183989
forward train acc: top1 ->  57.558331928253175 ; top5 ->  79.57499752044677  and loss:  371.95442950725555
test acc: top1 ->  60.54799831466675 ; top5 ->  83.64999748535156  and loss:  1387.1036034971476
forward train acc: top1 ->  56.80833196640015 ; top5 ->  79.00833078384399  and loss:  378.17034912109375
test acc: top1 ->  60.16199825744629 ; top5 ->  83.42199758148193  and loss:  1405.9087762981653
forward train acc: top1 ->  56.62499866485596 ; top5 ->  79.12499744415283  and loss:  381.0756641626358
test acc: top1 ->  59.949998300552366 ; top5 ->  83.39599744110107  and loss:  1407.095594316721
forward train acc: top1 ->  57.48333187103272 ; top5 ->  78.96666427612304  and loss:  374.61837887763977
test acc: top1 ->  63.06199823150635 ; top5 ->  85.10999755859375  and loss:  1301.3925596028566
forward train acc: top1 ->  58.199998683929444 ; top5 ->  80.18333068847656  and loss:  365.75123822689056
test acc: top1 ->  63.47599819145203 ; top5 ->  85.41999760742188  and loss:  1278.6861954703927
forward train acc: top1 ->  58.65833200454712 ; top5 ->  80.75833091735839  and loss:  357.44280153512955
test acc: top1 ->  63.50199814834595 ; top5 ->  85.37399744873046  and loss:  1271.104852154851
forward train acc: top1 ->  59.466665210723875 ; top5 ->  81.23333095550537  and loss:  349.05347895622253
test acc: top1 ->  64.315998147583 ; top5 ->  86.05599743347167  and loss:  1237.8562179207802
forward train acc: top1 ->  60.599998455047604 ; top5 ->  82.31666435241699  and loss:  337.624842941761
test acc: top1 ->  64.87599820785522 ; top5 ->  86.29399742736817  and loss:  1223.8693259730935
forward train acc: top1 ->  60.0666650390625 ; top5 ->  81.15833072662353  and loss:  349.8368949890137
test acc: top1 ->  65.12199814376831 ; top5 ->  86.35599730529785  and loss:  1218.7128953933716
forward train acc: top1 ->  60.541665172576906 ; top5 ->  81.35833087921142  and loss:  345.5822912454605
test acc: top1 ->  65.42199811401368 ; top5 ->  86.58799750976563  and loss:  1204.1904101371765
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  122 / 128 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -82.40986812114716 , diff:  82.40986812114716
adv train loss:  -85.2914309501648 , diff:  2.881562829017639
adv train loss:  -84.08277726173401 , diff:  1.2086536884307861
adv train loss:  -85.10377728939056 , diff:  1.0210000276565552
adv train loss:  -84.57471120357513 , diff:  0.5290660858154297
layer  8  adv train finish, try to retain  85
test acc: top1 ->  63.76999813079834 ; top5 ->  85.75999745025635  and loss:  1260.9609905481339
forward train acc: top1 ->  57.69999858856201 ; top5 ->  79.01666425704956  and loss:  375.41450238227844
test acc: top1 ->  60.62599829921722 ; top5 ->  83.64799758300781  and loss:  1383.568143159151
forward train acc: top1 ->  56.73333190917969 ; top5 ->  78.89166400909424  and loss:  380.0633339881897
test acc: top1 ->  61.32799818572998 ; top5 ->  83.96199747161866  and loss:  1363.9045816659927
forward train acc: top1 ->  56.37499879837036 ; top5 ->  78.69166418075561  and loss:  381.2156457901001
test acc: top1 ->  60.43799827041626 ; top5 ->  83.4979974456787  and loss:  1388.882972896099
forward train acc: top1 ->  57.2499986076355 ; top5 ->  79.60833076477051  and loss:  372.789547085762
test acc: top1 ->  63.28199814758301 ; top5 ->  85.31599745483399  and loss:  1288.7873860150576
forward train acc: top1 ->  58.408331851959225 ; top5 ->  80.33333072662353  and loss:  359.7219760417938
test acc: top1 ->  63.5739982093811 ; top5 ->  85.38399754180908  and loss:  1276.401154562831
forward train acc: top1 ->  60.18333183288574 ; top5 ->  81.6749974822998  and loss:  346.89764642715454
test acc: top1 ->  63.57999824523926 ; top5 ->  85.41799759979249  and loss:  1271.2716619372368
forward train acc: top1 ->  59.92499855041504 ; top5 ->  81.47499736785889  and loss:  346.86620700359344
test acc: top1 ->  64.60799812850952 ; top5 ->  86.08399757232667  and loss:  1233.1438974440098
forward train acc: top1 ->  60.09999849319458 ; top5 ->  81.61666412353516  and loss:  345.20002365112305
test acc: top1 ->  64.59399810638428 ; top5 ->  86.29999751586914  and loss:  1223.6339889019728
forward train acc: top1 ->  60.30833169937134 ; top5 ->  81.71666431427002  and loss:  342.61832493543625
test acc: top1 ->  64.90599813995361 ; top5 ->  86.34799739379883  and loss:  1220.2290320396423
forward train acc: top1 ->  61.108331756591795 ; top5 ->  81.7416641998291  and loss:  342.37465167045593
test acc: top1 ->  65.12399809265136 ; top5 ->  86.67999749908448  and loss:  1203.360724940896
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  9  ---------------
### skip layer  9 wait:  3  ###
---------------- start layer  10  ---------------
adv train loss:  -86.93354594707489 , diff:  86.93354594707489
adv train loss:  -83.42734456062317 , diff:  3.506201386451721
adv train loss:  -83.15309870243073 , diff:  0.27424585819244385
layer  10  adv train finish, try to retain  85
test acc: top1 ->  64.22599801864624 ; top5 ->  85.93999759674072  and loss:  1246.754663810134
forward train acc: top1 ->  57.00833198547363 ; top5 ->  79.19999729156494  and loss:  378.50047063827515
test acc: top1 ->  60.76799829826355 ; top5 ->  83.78199750976563  and loss:  1375.988225966692
forward train acc: top1 ->  56.683331928253175 ; top5 ->  79.3833307647705  and loss:  377.9289014339447
test acc: top1 ->  61.22799821052551 ; top5 ->  84.02399749908447  and loss:  1356.8608675897121
forward train acc: top1 ->  56.858332023620605 ; top5 ->  78.94999757766723  and loss:  383.63328671455383
test acc: top1 ->  60.90399832611084 ; top5 ->  83.89999752349854  and loss:  1368.1312867552042
forward train acc: top1 ->  57.958331985473635 ; top5 ->  80.16666389465333  and loss:  365.24167025089264
test acc: top1 ->  63.19199816551208 ; top5 ->  85.33999740753174  and loss:  1281.8015455901623
forward train acc: top1 ->  58.933331928253175 ; top5 ->  80.40833072662353  and loss:  359.9168961048126
test acc: top1 ->  63.439998105621335 ; top5 ->  85.42599745788574  and loss:  1271.22764351964
forward train acc: top1 ->  58.54999845504761 ; top5 ->  80.56666397094726  and loss:  358.81803488731384
test acc: top1 ->  64.04399809570313 ; top5 ->  85.76799751434326  and loss:  1255.860099852085
forward train acc: top1 ->  59.45833192825317 ; top5 ->  80.85833103179931  and loss:  353.61916172504425
test acc: top1 ->  64.65199806060791 ; top5 ->  86.17599733276367  and loss:  1228.0400148034096
forward train acc: top1 ->  61.23333162307739 ; top5 ->  82.49999752044678  and loss:  330.5755789279938
test acc: top1 ->  64.8479980682373 ; top5 ->  86.22399747924804  and loss:  1219.9477535933256
forward train acc: top1 ->  60.19999843597412 ; top5 ->  81.39999740600587  and loss:  342.57143092155457
test acc: top1 ->  64.90399801254273 ; top5 ->  86.26999759216308  and loss:  1216.0403997302055
forward train acc: top1 ->  60.39999851226807 ; top5 ->  81.95833061218262  and loss:  339.38842380046844
test acc: top1 ->  65.23199818038941 ; top5 ->  86.46999741973877  and loss:  1205.357675820589
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -85.1917895078659 , diff:  85.1917895078659
adv train loss:  -84.90015840530396 , diff:  0.2916311025619507
layer  11  adv train finish, try to retain  96
test acc: top1 ->  64.19399816741944 ; top5 ->  85.63199755706788  and loss:  1251.2791877985
forward train acc: top1 ->  57.74166522979736 ; top5 ->  79.8833307647705  and loss:  370.0081958770752
test acc: top1 ->  61.59599814872742 ; top5 ->  83.92799752044678  and loss:  1355.552787259221
forward train acc: top1 ->  56.699998683929444 ; top5 ->  79.09166397094727  and loss:  382.15407621860504
test acc: top1 ->  61.37799824733734 ; top5 ->  84.12399751739503  and loss:  1352.0962271988392
forward train acc: top1 ->  56.55833196640015 ; top5 ->  79.31666412353516  and loss:  375.83731400966644
test acc: top1 ->  61.5259981590271 ; top5 ->  84.16199754638671  and loss:  1355.224654391408
forward train acc: top1 ->  58.17499856948852 ; top5 ->  80.63333103179932  and loss:  362.2611343860626
test acc: top1 ->  63.00999815979004 ; top5 ->  85.17399759521484  and loss:  1288.3379607200623
forward train acc: top1 ->  58.96666524887085 ; top5 ->  80.84166439056396  and loss:  357.7963092327118
test acc: top1 ->  63.6859981842041 ; top5 ->  85.44199748077392  and loss:  1273.4793940782547
forward train acc: top1 ->  59.40833177566528 ; top5 ->  80.69999738693237  and loss:  355.846067070961
test acc: top1 ->  63.72199827537537 ; top5 ->  85.62999752807617  and loss:  1262.0097266733646
forward train acc: top1 ->  59.4999986076355 ; top5 ->  81.06666416168213  and loss:  354.5400023460388
test acc: top1 ->  64.64599814376831 ; top5 ->  86.03599742584228  and loss:  1231.3219807744026
forward train acc: top1 ->  60.4666653251648 ; top5 ->  81.29999715805053  and loss:  346.8923556804657
test acc: top1 ->  65.02799815139771 ; top5 ->  86.30399740447999  and loss:  1216.4281168282032
forward train acc: top1 ->  60.75833171844482 ; top5 ->  82.49999740600586  and loss:  338.9042819738388
test acc: top1 ->  65.23399804916382 ; top5 ->  86.50999739379883  and loss:  1206.3006566017866
forward train acc: top1 ->  60.366665267944335 ; top5 ->  82.39999729156494  and loss:  337.76327562332153
test acc: top1 ->  65.48599814834594 ; top5 ->  86.62599752349854  and loss:  1200.3174284100533
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -81.87714672088623 , diff:  81.87714672088623
adv train loss:  -83.03441381454468 , diff:  1.1572670936584473
adv train loss:  -83.42120921611786 , diff:  0.38679540157318115
layer  12  adv train finish, try to retain  95
test acc: top1 ->  64.91399813232422 ; top5 ->  86.12799750976562  and loss:  1230.046947106719
forward train acc: top1 ->  57.0916653251648 ; top5 ->  79.50833080291748  and loss:  375.116662979126
test acc: top1 ->  61.60799822235107 ; top5 ->  84.04799745025635  and loss:  1351.4967881441116
forward train acc: top1 ->  56.8249986076355 ; top5 ->  79.35833082199096  and loss:  375.9951841831207
test acc: top1 ->  61.27799823379517 ; top5 ->  83.94199752044678  and loss:  1364.203993394971
forward train acc: top1 ->  56.19166530609131 ; top5 ->  79.29999752044678  and loss:  379.98732113838196
test acc: top1 ->  61.4719982131958 ; top5 ->  84.19799755249024  and loss:  1349.6765516251326
forward train acc: top1 ->  57.51666547775269 ; top5 ->  79.73333057403565  and loss:  372.33147382736206
test acc: top1 ->  63.549998124313355 ; top5 ->  85.51799751281739  and loss:  1276.435473650694
forward train acc: top1 ->  58.99166515350342 ; top5 ->  80.69999727249146  and loss:  356.0650985240936
test acc: top1 ->  63.96999812011719 ; top5 ->  85.65999756622314  and loss:  1257.3947363495827
forward train acc: top1 ->  59.84166507720947 ; top5 ->  80.91666404724121  and loss:  356.05367797613144
test acc: top1 ->  64.0399981880188 ; top5 ->  85.82399740753173  and loss:  1258.0576271414757
forward train acc: top1 ->  59.79166521072388 ; top5 ->  81.5666641998291  and loss:  345.0287810564041
test acc: top1 ->  64.9939980583191 ; top5 ->  86.27799733581543  and loss:  1221.8375279605389
forward train acc: top1 ->  60.74166534423828 ; top5 ->  81.47499729156495  and loss:  342.44856238365173
test acc: top1 ->  65.16199815826415 ; top5 ->  86.41199747619629  and loss:  1213.608078211546
forward train acc: top1 ->  60.45833164215088 ; top5 ->  81.64166408538819  and loss:  339.08031022548676
test acc: top1 ->  65.20799805870057 ; top5 ->  86.49399750671387  and loss:  1207.3536947965622
forward train acc: top1 ->  60.41666522979736 ; top5 ->  82.0833309173584  and loss:  337.60574877262115
test acc: top1 ->  65.72199806556702 ; top5 ->  86.85599748840332  and loss:  1192.4228230714798
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  98 / 128 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -81.58549535274506 , diff:  81.58549535274506
adv train loss:  -85.61536955833435 , diff:  4.029874205589294
adv train loss:  -86.45147240161896 , diff:  0.8361028432846069
layer  13  adv train finish, try to retain  94
>>>>>>> reverse layer  13  since no improvement >>>>>>>
---------------- start layer  14  ---------------
### skip layer  14 wait:  4  ###
---------------- start layer  15  ---------------
adv train loss:  -85.37658488750458 , diff:  85.37658488750458
adv train loss:  -85.16697955131531 , diff:  0.20960533618927002
layer  15  adv train finish, try to retain  243
test acc: top1 ->  64.86399806823731 ; top5 ->  86.14999744110108  and loss:  1231.0030801296234
forward train acc: top1 ->  57.41666526794434 ; top5 ->  79.68333084106445  and loss:  373.0555840730667
test acc: top1 ->  61.725998218536375 ; top5 ->  84.4379975402832  and loss:  1341.1000279858708
forward train acc: top1 ->  56.65833177566528 ; top5 ->  79.70833072662353  and loss:  374.2961286306381
test acc: top1 ->  61.32999814987183 ; top5 ->  83.85599761123657  and loss:  1363.3712967038155
forward train acc: top1 ->  57.39999849319458 ; top5 ->  79.41666414260864  and loss:  374.1384197473526
test acc: top1 ->  61.30999818649292 ; top5 ->  83.89799742126465  and loss:  1367.597641557455
forward train acc: top1 ->  57.5833318901062 ; top5 ->  79.76666416168213  and loss:  369.12986624240875
test acc: top1 ->  63.50799814414978 ; top5 ->  85.37399756622314  and loss:  1278.977582335472
forward train acc: top1 ->  58.63333200454712 ; top5 ->  80.11666423797607  and loss:  361.386039018631
test acc: top1 ->  63.80599816207886 ; top5 ->  85.49199766540528  and loss:  1270.49640879035
forward train acc: top1 ->  59.9416650390625 ; top5 ->  81.34166404724121  and loss:  348.0857719182968
test acc: top1 ->  63.92599820480347 ; top5 ->  85.6199975189209  and loss:  1258.22836060822
forward train acc: top1 ->  60.008331756591794 ; top5 ->  81.70833099365234  and loss:  344.15959548950195
test acc: top1 ->  64.92199813690185 ; top5 ->  86.24399760284423  and loss:  1227.3829963356256
forward train acc: top1 ->  61.008331832885744 ; top5 ->  81.73333103179931  and loss:  342.36936461925507
test acc: top1 ->  65.20399807739258 ; top5 ->  86.34799759216308  and loss:  1215.5172310322523
forward train acc: top1 ->  59.89166521072388 ; top5 ->  81.63333084106445  and loss:  341.5101680755615
test acc: top1 ->  65.29799815368652 ; top5 ->  86.46999756469727  and loss:  1207.9885148853064
forward train acc: top1 ->  61.24999847412109 ; top5 ->  82.24999713897705  and loss:  336.275042951107
test acc: top1 ->  65.56799813308716 ; top5 ->  86.66599767761231  and loss:  1199.159159913659
>>>>>>> reverse layer  15  since performance drop >>>>>>>
==> this epoch:  248 / 256 , inc:  1
---------------- start layer  16  ---------------
### skip layer  16 wait:  3  ###
---------------- start layer  17  ---------------
### skip layer  17 wait:  4  ###
---------------- start layer  18  ---------------
### skip layer  18 wait:  3  ###
---------------- start layer  19  ---------------
adv train loss:  -81.51914465427399 , diff:  81.51914465427399
adv train loss:  -79.02768659591675 , diff:  2.4914580583572388
adv train loss:  -85.33624267578125 , diff:  6.308556079864502
adv train loss:  -83.43598651885986 , diff:  1.9002561569213867
adv train loss:  -86.85681354999542 , diff:  3.420827031135559
adv train loss:  -84.01469075679779 , diff:  2.842122793197632
adv train loss:  -85.8235330581665 , diff:  1.8088423013687134
adv train loss:  -85.1330133676529 , diff:  0.6905196905136108
************ all values are small in this layer **********
layer  19  adv train finish, try to retain  178
test acc: top1 ->  64.82199807739258 ; top5 ->  86.07399750823974  and loss:  1235.3185195177794
forward train acc: top1 ->  57.374998512268064 ; top5 ->  79.67499744415284  and loss:  369.47096371650696
test acc: top1 ->  61.01199828186035 ; top5 ->  84.07999732818604  and loss:  1367.878586024046
forward train acc: top1 ->  57.24999870300293 ; top5 ->  79.92499767303467  and loss:  368.8917461633682
test acc: top1 ->  61.233998316955564 ; top5 ->  84.10999746398926  and loss:  1360.9792806655169
forward train acc: top1 ->  57.358332023620605 ; top5 ->  79.41666393280029  and loss:  374.90825295448303
test acc: top1 ->  61.30799825210571 ; top5 ->  84.07199757843017  and loss:  1354.4129057377577
forward train acc: top1 ->  58.874998512268064 ; top5 ->  81.35833072662354  and loss:  355.0479074716568
test acc: top1 ->  63.32999818687439 ; top5 ->  85.48199761657715  and loss:  1275.3780833482742
forward train acc: top1 ->  59.383331756591794 ; top5 ->  81.14999702453613  and loss:  349.57952904701233
test acc: top1 ->  64.00399819946288 ; top5 ->  85.73399766387939  and loss:  1258.5615041702986
forward train acc: top1 ->  59.56666530609131 ; top5 ->  81.34166416168213  and loss:  349.50902688503265
test acc: top1 ->  64.02799813079834 ; top5 ->  85.7719974609375  and loss:  1254.49052593112
forward train acc: top1 ->  60.11666511535645 ; top5 ->  81.9333304977417  and loss:  342.509410738945
test acc: top1 ->  64.94599811782837 ; top5 ->  86.28199744415284  and loss:  1221.3263229131699
forward train acc: top1 ->  60.63333190917969 ; top5 ->  81.76666439056396  and loss:  344.89475321769714
test acc: top1 ->  64.8619981842041 ; top5 ->  86.30399745178222  and loss:  1219.2196984887123
forward train acc: top1 ->  60.641665077209474 ; top5 ->  81.516664352417  and loss:  341.09495997428894
test acc: top1 ->  65.19599802703857 ; top5 ->  86.45399754638672  and loss:  1210.4657927900553
forward train acc: top1 ->  62.341664943695065 ; top5 ->  82.64166408538819  and loss:  329.10113990306854
test acc: top1 ->  65.48399808654786 ; top5 ->  86.59599744110108  and loss:  1197.9901260733604
>>>>>>> reverse layer  19  since performance drop >>>>>>>
==> this epoch:  179 / 256 , inc:  1
---------------- start layer  20  ---------------
### skip layer  20 wait:  3  ###
---------------- start layer  21  ---------------
adv train loss:  -88.12194788455963 , diff:  88.12194788455963
adv train loss:  -84.27581560611725 , diff:  3.846132278442383
adv train loss:  -82.89888024330139 , diff:  1.376935362815857
adv train loss:  -83.89508855342865 , diff:  0.9962083101272583
layer  21  adv train finish, try to retain  247
>>>>>>> reverse layer  21  since no improvement >>>>>>>
---------------- start layer  22  ---------------
adv train loss:  -81.70712637901306 , diff:  81.70712637901306
adv train loss:  -85.21204614639282 , diff:  3.5049197673797607
adv train loss:  -87.01815128326416 , diff:  1.806105136871338
adv train loss:  -85.80973970890045 , diff:  1.2084115743637085
adv train loss:  -84.18934202194214 , diff:  1.620397686958313
adv train loss:  -82.29160726070404 , diff:  1.8977347612380981
adv train loss:  -85.5399044752121 , diff:  3.2482972145080566
adv train loss:  -84.45169842243195 , diff:  1.0882060527801514
adv train loss:  -80.52029466629028 , diff:  3.9314037561416626
adv train loss:  -85.281325340271 , diff:  4.761030673980713
layer  22  adv train finish, try to retain  214
>>>>>>> reverse layer  22  since no improvement >>>>>>>
---------------- start layer  23  ---------------
adv train loss:  -84.77145290374756 , diff:  84.77145290374756
adv train loss:  -84.28219664096832 , diff:  0.48925626277923584
************ all values are small in this layer **********
layer  23  adv train finish, try to retain  186
test acc: top1 ->  65.01799813156128 ; top5 ->  86.3239974395752  and loss:  1217.8168554008007
forward train acc: top1 ->  58.21666513442993 ; top5 ->  80.74166400909424  and loss:  362.20467042922974
test acc: top1 ->  61.92199808959961 ; top5 ->  84.37599754028321  and loss:  1342.5346478819847
forward train acc: top1 ->  57.991665306091306 ; top5 ->  80.02499744415283  and loss:  368.76542365550995
test acc: top1 ->  61.77799823284149 ; top5 ->  84.35799755249023  and loss:  1341.695015490055
forward train acc: top1 ->  57.4999987411499 ; top5 ->  80.13333086013795  and loss:  367.5788984298706
test acc: top1 ->  61.713998157119754 ; top5 ->  84.41999751586914  and loss:  1348.5902148932219
forward train acc: top1 ->  58.80833177566528 ; top5 ->  80.49166414260864  and loss:  361.70433700084686
test acc: top1 ->  63.819998170471194 ; top5 ->  85.64599742279053  and loss:  1265.1132496148348
forward train acc: top1 ->  59.44166513442993 ; top5 ->  80.60833080291748  and loss:  357.38633465766907
test acc: top1 ->  63.92199811325073 ; top5 ->  85.72799737091064  and loss:  1258.6990802288055
forward train acc: top1 ->  59.74999849319458 ; top5 ->  81.31666416168213  and loss:  351.89678514003754
test acc: top1 ->  63.89399815979004 ; top5 ->  85.70599739990234  and loss:  1257.1378450542688
forward train acc: top1 ->  60.708331718444825 ; top5 ->  82.0249976348877  and loss:  342.4552617073059
test acc: top1 ->  64.7839980545044 ; top5 ->  86.44599746551513  and loss:  1223.2734041810036
forward train acc: top1 ->  61.016665096282956 ; top5 ->  82.02499752044677  and loss:  338.78213226795197
test acc: top1 ->  65.1879981163025 ; top5 ->  86.48399750823975  and loss:  1210.4169039428234
forward train acc: top1 ->  60.616665267944335 ; top5 ->  81.63333099365235  and loss:  342.19614934921265
test acc: top1 ->  65.2259981639862 ; top5 ->  86.69799744720459  and loss:  1205.577520251274
forward train acc: top1 ->  61.19166511535644 ; top5 ->  82.3499976348877  and loss:  333.06586641073227
test acc: top1 ->  65.50999816360473 ; top5 ->  86.82799742736816  and loss:  1199.7260338664055
>>>>>>> reverse layer  23  since performance drop >>>>>>>
==> this epoch:  187 / 256 , inc:  1
---------------- start layer  24  ---------------
adv train loss:  -86.54705512523651 , diff:  86.54705512523651
adv train loss:  -86.42271864414215 , diff:  0.12433648109436035
layer  24  adv train finish, try to retain  248
>>>>>>> reverse layer  24  since no improvement >>>>>>>
---------------- start layer  25  ---------------
### skip layer  25 wait:  3  ###
---------------- start layer  26  ---------------
adv train loss:  -83.96367311477661 , diff:  83.96367311477661
adv train loss:  -81.49635189771652 , diff:  2.467321217060089
adv train loss:  -83.96326184272766 , diff:  2.466909945011139
adv train loss:  -81.65636801719666 , diff:  2.306893825531006
adv train loss:  -82.94972765445709 , diff:  1.293359637260437
adv train loss:  -81.08779287338257 , diff:  1.861934781074524
adv train loss:  -82.9999452829361 , diff:  1.9121524095535278
adv train loss:  -85.119304895401 , diff:  2.119359612464905
adv train loss:  -82.69005680084229 , diff:  2.429248094558716
adv train loss:  -85.19577705860138 , diff:  2.5057202577590942
layer  26  adv train finish, try to retain  494
test acc: top1 ->  65.19399805679322 ; top5 ->  86.70999743652344  and loss:  1201.1611182689667
forward train acc: top1 ->  58.0833317565918 ; top5 ->  79.37499744415283  and loss:  370.12323343753815
test acc: top1 ->  61.66399832496643 ; top5 ->  84.34799751129151  and loss:  1341.6740777045488
forward train acc: top1 ->  57.374998550415036 ; top5 ->  79.59166404724121  and loss:  370.624006152153
test acc: top1 ->  61.555998272705075 ; top5 ->  84.0159975265503  and loss:  1363.350082680583
forward train acc: top1 ->  56.19999855041504 ; top5 ->  78.84166431427002  and loss:  380.6673867702484
test acc: top1 ->  62.0799983089447 ; top5 ->  84.44199766540527  and loss:  1338.414444833994
forward train acc: top1 ->  58.374998512268064 ; top5 ->  80.29166408538818  and loss:  363.2874425649643
test acc: top1 ->  64.17799824333191 ; top5 ->  85.76199746246338  and loss:  1255.8396209031343
forward train acc: top1 ->  59.516665115356446 ; top5 ->  81.10833082199096  and loss:  351.4064166545868
test acc: top1 ->  64.19799817733765 ; top5 ->  85.74399739837646  and loss:  1253.2301102131605
forward train acc: top1 ->  59.84166515350342 ; top5 ->  81.54166416168214  and loss:  350.0321444272995
test acc: top1 ->  64.25599808502197 ; top5 ->  85.72199753112793  and loss:  1249.5096701830626
forward train acc: top1 ->  60.166665153503416 ; top5 ->  81.36666431427003  and loss:  345.5369197130203
test acc: top1 ->  65.00399804763794 ; top5 ->  86.35199736328126  and loss:  1214.4096511229873
forward train acc: top1 ->  61.09166507720947 ; top5 ->  82.18333080291748  and loss:  332.5756406188011
test acc: top1 ->  65.3499980140686 ; top5 ->  86.59199738922119  and loss:  1207.5661696493626
forward train acc: top1 ->  61.30833181381226 ; top5 ->  81.99166397094727  and loss:  337.2729212641716
test acc: top1 ->  65.37399802322388 ; top5 ->  86.54999742736817  and loss:  1202.6121132075787
forward train acc: top1 ->  61.69999843597412 ; top5 ->  82.44999740600586  and loss:  333.346798658371
test acc: top1 ->  65.75399804534912 ; top5 ->  86.84399744567871  and loss:  1189.3002749681473
>>>>>>> reverse layer  26  since performance drop >>>>>>>
==> this epoch:  502 / 512 , inc:  1
---------------- start layer  27  ---------------
adv train loss:  -84.03241968154907 , diff:  84.03241968154907
adv train loss:  -83.39299416542053 , diff:  0.63942551612854
layer  27  adv train finish, try to retain  497
test acc: top1 ->  65.34399806861877 ; top5 ->  86.64799739532471  and loss:  1204.6730800569057
forward train acc: top1 ->  57.90833194732666 ; top5 ->  79.90833122253417  and loss:  367.0042769908905
test acc: top1 ->  62.12799826316834 ; top5 ->  84.5819973701477  and loss:  1333.6304283887148
forward train acc: top1 ->  57.44999862670898 ; top5 ->  80.14999736785889  and loss:  371.27626514434814
test acc: top1 ->  61.773998263931276 ; top5 ->  84.36999745788575  and loss:  1343.1673761308193
forward train acc: top1 ->  58.35833185195923 ; top5 ->  80.05833068847656  and loss:  366.7898978590965
test acc: top1 ->  61.981998302459715 ; top5 ->  84.36399749450683  and loss:  1342.66765139997
forward train acc: top1 ->  58.98333185195923 ; top5 ->  80.45833072662353  and loss:  360.28206717967987
test acc: top1 ->  63.957997989654544 ; top5 ->  85.54199761123657  and loss:  1265.8702439665794
forward train acc: top1 ->  59.81666526794434 ; top5 ->  80.94166381835937  and loss:  350.3105167746544
test acc: top1 ->  64.1899980167389 ; top5 ->  85.82199753265381  and loss:  1248.1496846526861
forward train acc: top1 ->  60.58333181381226 ; top5 ->  81.41666400909423  and loss:  345.80974197387695
test acc: top1 ->  64.48399808959961 ; top5 ->  85.90799746551514  and loss:  1241.633013933897
forward train acc: top1 ->  59.59999841690063 ; top5 ->  81.20833068847656  and loss:  346.5281751155853
test acc: top1 ->  65.31399807891846 ; top5 ->  86.48999761199951  and loss:  1211.8015120774508
forward train acc: top1 ->  60.516665058135985 ; top5 ->  82.09166416168213  and loss:  340.97516691684723
test acc: top1 ->  65.46599805755615 ; top5 ->  86.5779974822998  and loss:  1205.0964793413877
forward train acc: top1 ->  61.18333181381226 ; top5 ->  82.47499771118164  and loss:  336.59423273801804
test acc: top1 ->  65.46399799575806 ; top5 ->  86.73599765625  and loss:  1200.5788723528385
forward train acc: top1 ->  61.71666511535645 ; top5 ->  82.54999740600586  and loss:  331.04987198114395
test acc: top1 ->  65.87799805526734 ; top5 ->  86.95399742126465  and loss:  1187.7335819751024
>>>>>>> reverse layer  27  since performance drop >>>>>>>
==> this epoch:  505 / 512 , inc:  1
---------------- start layer  28  ---------------
adv train loss:  -86.23272955417633 , diff:  86.23272955417633
adv train loss:  -83.64006632566452 , diff:  2.5926632285118103
adv train loss:  -85.25474715232849 , diff:  1.614680826663971
adv train loss:  -87.00036811828613 , diff:  1.7456209659576416
adv train loss:  -84.53305327892303 , diff:  2.467314839363098
adv train loss:  -84.54590404033661 , diff:  0.012850761413574219
layer  28  adv train finish, try to retain  497
test acc: top1 ->  65.5419980934143 ; top5 ->  86.75599753570556  and loss:  1202.3857014030218
forward train acc: top1 ->  57.733331966400144 ; top5 ->  80.28333068847657  and loss:  368.8028476238251
test acc: top1 ->  62.27599817276001 ; top5 ->  84.78199762878418  and loss:  1318.2411299049854
forward train acc: top1 ->  57.69166534423828 ; top5 ->  80.41666416168214  and loss:  364.35483372211456
test acc: top1 ->  62.07799818687439 ; top5 ->  84.56599759521484  and loss:  1329.083833321929
forward train acc: top1 ->  57.96666534423828 ; top5 ->  80.19166423797607  and loss:  365.9011026620865
test acc: top1 ->  62.25999814682007 ; top5 ->  84.65999756317139  and loss:  1321.4815660864115
forward train acc: top1 ->  59.166665363311765 ; top5 ->  80.62499769210815  and loss:  358.8060930967331
test acc: top1 ->  63.925998071289065 ; top5 ->  85.66799742889404  and loss:  1256.43968385458
forward train acc: top1 ->  59.14999858856201 ; top5 ->  81.09166408538819  and loss:  351.8930875658989
test acc: top1 ->  64.05599815063476 ; top5 ->  85.89599745178222  and loss:  1250.2592551857233
forward train acc: top1 ->  59.674998416900635 ; top5 ->  81.20833059310912  and loss:  348.7826511859894
test acc: top1 ->  64.32599811401367 ; top5 ->  85.95399753875732  and loss:  1243.4591518193483
forward train acc: top1 ->  60.724998455047604 ; top5 ->  82.15833095550538  and loss:  338.7040801048279
test acc: top1 ->  65.15799807968139 ; top5 ->  86.46999763488769  and loss:  1212.2436749190092
forward train acc: top1 ->  61.016665077209474 ; top5 ->  82.03333087921142  and loss:  341.79733300209045
test acc: top1 ->  65.50399801940918 ; top5 ->  86.76399754333497  and loss:  1195.248715609312
forward train acc: top1 ->  61.249998302459716 ; top5 ->  82.31666408538818  and loss:  334.09761077165604
test acc: top1 ->  65.37199809722901 ; top5 ->  86.65999750518799  and loss:  1195.1464816331863
forward train acc: top1 ->  61.224998474121094 ; top5 ->  81.96666397094727  and loss:  335.5199651122093
test acc: top1 ->  65.88199812927246 ; top5 ->  86.8439975982666  and loss:  1185.6018529385328
>>>>>>> reverse layer  28  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  29  ---------------
adv train loss:  -84.10915195941925 , diff:  84.10915195941925
adv train loss:  -80.26829302310944 , diff:  3.8408589363098145
adv train loss:  -81.28972113132477 , diff:  1.021428108215332
adv train loss:  -79.12170314788818 , diff:  2.1680179834365845
adv train loss:  -83.6809309720993 , diff:  4.559227824211121
adv train loss:  -82.51564717292786 , diff:  1.1652837991714478
adv train loss:  -84.08367258310318 , diff:  1.5680254101753235
adv train loss:  -81.8891248703003 , diff:  2.194547712802887
adv train loss:  -82.17636382579803 , diff:  0.2872389554977417
layer  29  adv train finish, try to retain  499
>>>>>>> reverse layer  29  since no improvement >>>>>>>
---------------- start layer  30  ---------------
adv train loss:  -85.17907571792603 , diff:  85.17907571792603
adv train loss:  -83.5478663444519 , diff:  1.631209373474121
adv train loss:  -81.28054428100586 , diff:  2.267322063446045
adv train loss:  -83.9126056432724 , diff:  2.6320613622665405
adv train loss:  -84.47699177265167 , diff:  0.5643861293792725
layer  30  adv train finish, try to retain  493
test acc: top1 ->  65.80999803085327 ; top5 ->  86.76999747772217  and loss:  1188.580792427063
forward train acc: top1 ->  58.26666543960571 ; top5 ->  80.58333080291749  and loss:  362.0801998972893
test acc: top1 ->  61.883998106765745 ; top5 ->  84.35399753875733  and loss:  1345.9841434061527
forward train acc: top1 ->  57.83333206176758 ; top5 ->  80.56666400909424  and loss:  365.5166400671005
test acc: top1 ->  62.02399822616577 ; top5 ->  84.67399742889404  and loss:  1334.177218258381
forward train acc: top1 ->  57.94999855041504 ; top5 ->  80.24999752044678  and loss:  366.2517721056938
test acc: top1 ->  61.783998237991334 ; top5 ->  84.23999755859376  and loss:  1339.540367975831
forward train acc: top1 ->  58.741665325164796 ; top5 ->  80.52499732971191  and loss:  359.4541161060333
test acc: top1 ->  64.14999826278687 ; top5 ->  85.83199750213623  and loss:  1247.8473688960075
forward train acc: top1 ->  59.69166511535644 ; top5 ->  80.90833065032959  and loss:  351.45703542232513
test acc: top1 ->  64.12999810943603 ; top5 ->  85.79799748687743  and loss:  1248.9927365779877
forward train acc: top1 ->  59.71666509628296 ; top5 ->  80.87499759674073  and loss:  352.03310775756836
test acc: top1 ->  64.61399815216065 ; top5 ->  86.13199739379883  and loss:  1233.193542972207
forward train acc: top1 ->  60.80833179473877 ; top5 ->  81.91666404724121  and loss:  340.0766673088074
test acc: top1 ->  65.21599798278808 ; top5 ->  86.54799744415283  and loss:  1209.5247085392475
forward train acc: top1 ->  60.80833183288574 ; top5 ->  81.86666400909424  and loss:  339.81197917461395
test acc: top1 ->  65.50599810180664 ; top5 ->  86.60799752655029  and loss:  1201.638904184103
forward train acc: top1 ->  61.54999853134155 ; top5 ->  82.5499973487854  and loss:  333.1819242835045
test acc: top1 ->  65.65799799499511 ; top5 ->  86.65599747619629  and loss:  1193.021980598569
forward train acc: top1 ->  61.34999837875366 ; top5 ->  82.82499740600586  and loss:  329.80132895708084
test acc: top1 ->  65.99999813690185 ; top5 ->  86.85599737548829  and loss:  1183.8842448741198
>>>>>>> reverse layer  30  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  31  ---------------
adv train loss:  -85.46112501621246 , diff:  85.46112501621246
adv train loss:  -82.05722308158875 , diff:  3.4039019346237183
adv train loss:  -81.1690503358841 , diff:  0.8881727457046509
layer  31  adv train finish, try to retain  499
test acc: top1 ->  65.34799813156128 ; top5 ->  86.57799747161866  and loss:  1206.535635754466
forward train acc: top1 ->  57.85833173751831 ; top5 ->  80.1666640663147  and loss:  365.18123519420624
test acc: top1 ->  61.885998197555544 ; top5 ->  84.39799745178223  and loss:  1339.0797741562128
forward train acc: top1 ->  58.35833179473877 ; top5 ->  80.54166416168214  and loss:  361.5169245004654
test acc: top1 ->  62.39799819374085 ; top5 ->  84.7859975769043  and loss:  1312.5382065996528
forward train acc: top1 ->  58.73333179473877 ; top5 ->  80.76666427612305  and loss:  360.1782031059265
test acc: top1 ->  62.13199821891784 ; top5 ->  84.6399975402832  and loss:  1332.8512968346477
forward train acc: top1 ->  59.16666530609131 ; top5 ->  80.48333080291748  and loss:  357.4973759651184
test acc: top1 ->  64.347997971344 ; top5 ->  85.7879975982666  and loss:  1246.1438746005297
forward train acc: top1 ->  60.991665077209475 ; top5 ->  81.91666381835938  and loss:  346.92140555381775
test acc: top1 ->  64.32599804992675 ; top5 ->  86.14999753265381  and loss:  1237.0018023252487
forward train acc: top1 ->  59.758331871032716 ; top5 ->  80.79166399002075  and loss:  355.3407281637192
test acc: top1 ->  64.52399813766479 ; top5 ->  86.13399754638672  and loss:  1228.915640681982
forward train acc: top1 ->  60.28333171844483 ; top5 ->  81.98333076477051  and loss:  344.114538192749
test acc: top1 ->  65.23599814376831 ; top5 ->  86.4779975112915  and loss:  1208.6848920583725
forward train acc: top1 ->  61.01666496276855 ; top5 ->  81.71666416168213  and loss:  340.0819367170334
test acc: top1 ->  65.53799820747375 ; top5 ->  86.73399745025635  and loss:  1195.528352677822
forward train acc: top1 ->  61.17499851226807 ; top5 ->  82.09999732971191  and loss:  336.3566858768463
test acc: top1 ->  65.62999810028076 ; top5 ->  86.87399745635986  and loss:  1191.4276248514652
forward train acc: top1 ->  61.45833173751831 ; top5 ->  82.75833061218262  and loss:  329.83226239681244
test acc: top1 ->  65.94999806060791 ; top5 ->  87.02199730834961  and loss:  1177.79954482615
>>>>>>> reverse layer  31  since performance drop >>>>>>>
==> this epoch:  501 / 512 , inc:  1
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.015, 0.015, 0.00052734375, 0.015, 0.015, 0.015, 0.000703125, 0.000263671875, 0.0028125, 0.000703125, 0.0028125, 0.0028125, 0.0028125, 0.0075, 6.591796875e-05, 0.0001318359375, 0.0003515625, 0.000703125, 0.0003515625, 0.00375, 0.0003515625, 0.0003515625, 0.00375, 0.00375, 0.0003515625, 0.0003515625, 6.591796875e-05, 6.591796875e-05, 6.591796875e-05, 0.00017578125, 6.591796875e-05, 6.591796875e-05]  wait [2, 2, 4, 2, 2, 2, 2, 4, 4, 2, 4, 4, 4, 2, 3, 4, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 2, 4, 4]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 0
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  8  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -81.82816141843796 , diff:  81.82816141843796
adv train loss:  -83.52608919143677 , diff:  1.6979277729988098
adv train loss:  -80.06100535392761 , diff:  3.4650838375091553
adv train loss:  -85.05167710781097 , diff:  4.990671753883362
adv train loss:  -86.65877401828766 , diff:  1.6070969104766846
adv train loss:  -86.31266176700592 , diff:  0.3461122512817383
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  36
test acc: top1 ->  58.159998333358764 ; top5 ->  81.06399763946533  and loss:  1504.8371516764164
forward train acc: top1 ->  57.774998626708985 ; top5 ->  79.90833084106445  and loss:  371.1038155555725
test acc: top1 ->  62.00399826202393 ; top5 ->  84.45599749298096  and loss:  1332.5135285109282
forward train acc: top1 ->  57.141665325164794 ; top5 ->  79.36666423797607  and loss:  376.1527135372162
test acc: top1 ->  61.64199824752808 ; top5 ->  84.28599744262695  and loss:  1340.047909423709
forward train acc: top1 ->  57.641665325164794 ; top5 ->  79.95833076477051  and loss:  367.3205268383026
test acc: top1 ->  62.399998182678225 ; top5 ->  84.80599759674072  and loss:  1313.938487932086
forward train acc: top1 ->  58.999998531341554 ; top5 ->  80.79999752044678  and loss:  356.4768860936165
test acc: top1 ->  63.957998127365116 ; top5 ->  85.73599754638671  and loss:  1250.3644858449697
forward train acc: top1 ->  59.183331928253175 ; top5 ->  81.24166404724122  and loss:  353.84123480319977
test acc: top1 ->  64.35599813728332 ; top5 ->  85.90199743499755  and loss:  1241.6031487584114
forward train acc: top1 ->  59.38333179473877 ; top5 ->  81.10833065032959  and loss:  352.0834319591522
test acc: top1 ->  64.34799815216064 ; top5 ->  86.00199756011963  and loss:  1237.8436724245548
forward train acc: top1 ->  60.91666534423828 ; top5 ->  81.77499752044677  and loss:  341.63855624198914
test acc: top1 ->  65.23799814605712 ; top5 ->  86.49999749603272  and loss:  1207.4770073220134
forward train acc: top1 ->  61.283331928253176 ; top5 ->  81.81666423797607  and loss:  339.07258439064026
test acc: top1 ->  65.51799801406861 ; top5 ->  86.79999739837646  and loss:  1192.0725319385529
forward train acc: top1 ->  61.1416650390625 ; top5 ->  82.68333068847656  and loss:  331.85778868198395
test acc: top1 ->  65.80799807357788 ; top5 ->  86.78799745483398  and loss:  1189.5981778651476
forward train acc: top1 ->  60.874998531341554 ; top5 ->  81.92499740600586  and loss:  336.05526089668274
test acc: top1 ->  66.01799809799195 ; top5 ->  86.93599746704102  and loss:  1177.3062150031328
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  37 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -87.9311705827713 , diff:  87.9311705827713
adv train loss:  -87.83110761642456 , diff:  0.10006296634674072
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  43
test acc: top1 ->  62.39999822006226 ; top5 ->  84.60399736938477  and loss:  1312.7150102853775
forward train acc: top1 ->  57.741665306091306 ; top5 ->  79.88333068847656  and loss:  368.2959086894989
test acc: top1 ->  62.583998210144046 ; top5 ->  84.80599749298096  and loss:  1308.9505566209555
forward train acc: top1 ->  57.683331928253175 ; top5 ->  80.29166379928589  and loss:  361.7509980201721
test acc: top1 ->  62.25199819564819 ; top5 ->  84.73799769744873  and loss:  1332.3047576993704
forward train acc: top1 ->  57.616665287017824 ; top5 ->  79.54999755859374  and loss:  374.074387550354
test acc: top1 ->  62.20399825248718 ; top5 ->  84.70999753570557  and loss:  1322.45584076643
forward train acc: top1 ->  59.29166507720947 ; top5 ->  80.6583309173584  and loss:  355.7739037871361
test acc: top1 ->  64.08599813766479 ; top5 ->  85.67199749603272  and loss:  1255.3565322235227
forward train acc: top1 ->  59.908331851959225 ; top5 ->  81.8166640472412  and loss:  344.4893701672554
test acc: top1 ->  64.5659980682373 ; top5 ->  86.13799738464355  and loss:  1233.6444150060415
forward train acc: top1 ->  60.308331680297854 ; top5 ->  81.6916641998291  and loss:  346.00779151916504
test acc: top1 ->  64.62799816703796 ; top5 ->  86.18999745635986  and loss:  1231.6574475318193
forward train acc: top1 ->  60.78333179473877 ; top5 ->  82.12499763488769  and loss:  340.408891916275
test acc: top1 ->  65.29199810180664 ; top5 ->  86.49399752502441  and loss:  1210.1157404780388
forward train acc: top1 ->  61.26666524887085 ; top5 ->  82.44999767303467  and loss:  333.0085824728012
test acc: top1 ->  65.58399806594849 ; top5 ->  86.70599731903076  and loss:  1196.7298147529364
forward train acc: top1 ->  62.07499839782715 ; top5 ->  82.78333065032959  and loss:  329.02196395397186
test acc: top1 ->  65.76199808197022 ; top5 ->  86.65599747314454  and loss:  1189.7333489358425
forward train acc: top1 ->  61.1583317565918 ; top5 ->  82.02499748229981  and loss:  335.500394821167
test acc: top1 ->  66.19799813308715 ; top5 ->  86.97799750366211  and loss:  1177.8899366110563
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  44 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -83.41373836994171 , diff:  83.41373836994171
adv train loss:  -87.33396124839783 , diff:  3.9202228784561157
adv train loss:  -81.9533622264862 , diff:  5.380599021911621
adv train loss:  -80.13326275348663 , diff:  1.8200994729995728
adv train loss:  -83.06488800048828 , diff:  2.931625247001648
adv train loss:  -79.86717975139618 , diff:  3.197708249092102
adv train loss:  -84.52839589118958 , diff:  4.661216139793396
adv train loss:  -82.3196929693222 , diff:  2.2087029218673706
adv train loss:  -85.877720952034 , diff:  3.558027982711792
adv train loss:  -86.46900629997253 , diff:  0.5912853479385376
layer  2  adv train finish, try to retain  56
test acc: top1 ->  64.63599801483154 ; top5 ->  85.88599751281738  and loss:  1230.0397479236126
forward train acc: top1 ->  58.2999984741211 ; top5 ->  80.74999732971192  and loss:  359.32528960704803
test acc: top1 ->  62.53999823608398 ; top5 ->  84.8619975265503  and loss:  1313.3232711404562
forward train acc: top1 ->  58.14999866485596 ; top5 ->  80.54166427612304  and loss:  362.11331444978714
test acc: top1 ->  62.091998363494874 ; top5 ->  84.64399745025635  and loss:  1328.232688523829
forward train acc: top1 ->  58.68333202362061 ; top5 ->  80.19166408538818  and loss:  365.48466658592224
test acc: top1 ->  62.15199824295044 ; top5 ->  84.69999762268067  and loss:  1320.7051414102316
forward train acc: top1 ->  60.21666505813599 ; top5 ->  80.78333057403565  and loss:  354.84987580776215
test acc: top1 ->  63.80999827079773 ; top5 ->  85.70599766235351  and loss:  1264.4585940390825
forward train acc: top1 ->  60.158331813812254 ; top5 ->  81.75833084106445  and loss:  345.0618636608124
test acc: top1 ->  64.29199807395935 ; top5 ->  86.12399736480712  and loss:  1239.9966328293085
forward train acc: top1 ->  60.61666515350342 ; top5 ->  81.87499746322632  and loss:  341.3466806411743
test acc: top1 ->  64.44999806098939 ; top5 ->  86.03199742584229  and loss:  1237.84970125556
forward train acc: top1 ->  60.091665210723875 ; top5 ->  81.6249973487854  and loss:  341.4954599738121
test acc: top1 ->  65.3359979927063 ; top5 ->  86.72599751129151  and loss:  1199.7729507014155
forward train acc: top1 ->  61.74166511535645 ; top5 ->  82.8916640472412  and loss:  328.7122039794922
test acc: top1 ->  65.55799810218811 ; top5 ->  86.73199748382568  and loss:  1193.5638434514403
forward train acc: top1 ->  62.28333173751831 ; top5 ->  82.52499759674072  and loss:  330.7234953045845
test acc: top1 ->  65.8699981414795 ; top5 ->  86.9999974822998  and loss:  1185.1627256572247
forward train acc: top1 ->  60.93333169937134 ; top5 ->  82.02499740600587  and loss:  339.32257890701294
test acc: top1 ->  66.19999807434083 ; top5 ->  87.04599764099122  and loss:  1172.6439198628068
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  57 / 64 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -83.8695936203003 , diff:  83.8695936203003
adv train loss:  -81.49299490451813 , diff:  2.3765987157821655
adv train loss:  -81.0865523815155 , diff:  0.4064425230026245
************ all values are small in this layer **********
layer  3  adv train finish, try to retain  42
test acc: top1 ->  60.397998233413695 ; top5 ->  83.04999752502441  and loss:  1401.5636731237173
forward train acc: top1 ->  58.13333204269409 ; top5 ->  80.05833087921143  and loss:  364.46959376335144
test acc: top1 ->  61.923998222351074 ; top5 ->  84.55599747924805  and loss:  1330.2534595504403
forward train acc: top1 ->  57.61666536331177 ; top5 ->  80.23333089828492  and loss:  367.68590664863586
test acc: top1 ->  61.9399981798172 ; top5 ->  84.73999741973877  and loss:  1325.6747055351734
forward train acc: top1 ->  57.78333200454712 ; top5 ->  79.60833084106446  and loss:  371.2226130962372
test acc: top1 ->  61.51799821166992 ; top5 ->  84.37199756011962  and loss:  1343.8493545651436
forward train acc: top1 ->  58.791665172576906 ; top5 ->  80.28333057403565  and loss:  362.0667301416397
test acc: top1 ->  64.0939981803894 ; top5 ->  85.75799753570557  and loss:  1255.3730338960886
forward train acc: top1 ->  60.57499843597412 ; top5 ->  81.78333080291748  and loss:  342.8611251115799
test acc: top1 ->  64.30599810676574 ; top5 ->  86.02399753875733  and loss:  1244.5890193134546
forward train acc: top1 ->  60.066665191650394 ; top5 ->  81.33333095550537  and loss:  349.97924160957336
test acc: top1 ->  64.40799812088012 ; top5 ->  86.06799752349853  and loss:  1237.4948448687792
forward train acc: top1 ->  60.70833179473877 ; top5 ->  81.5499974822998  and loss:  339.4854211807251
test acc: top1 ->  65.19999812660217 ; top5 ->  86.34799756927491  and loss:  1209.753667600453
forward train acc: top1 ->  61.32499839782715 ; top5 ->  82.60833087921142  and loss:  333.8230427503586
test acc: top1 ->  65.59599817352294 ; top5 ->  86.72999758453369  and loss:  1198.6354143619537
forward train acc: top1 ->  61.49166511535645 ; top5 ->  82.71666408538819  and loss:  329.8570439219475
test acc: top1 ->  65.58799800643921 ; top5 ->  86.67999751586915  and loss:  1198.4054936617613
forward train acc: top1 ->  61.99999839782715 ; top5 ->  82.7666640472412  and loss:  328.77094757556915
test acc: top1 ->  65.83999808044433 ; top5 ->  87.04199749450683  and loss:  1178.8484893590212
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  43 / 64 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -81.9940402507782 , diff:  81.9940402507782
adv train loss:  -83.20203411579132 , diff:  1.2079938650131226
adv train loss:  -83.8203535079956 , diff:  0.6183193922042847
************ all values are small in this layer **********
layer  4  adv train finish, try to retain  39
test acc: top1 ->  64.1119980884552 ; top5 ->  85.66199731292724  and loss:  1251.0644309520721
forward train acc: top1 ->  58.95833185195923 ; top5 ->  80.59166412353515  and loss:  358.16757571697235
test acc: top1 ->  62.691998218917846 ; top5 ->  84.9999975944519  and loss:  1310.6824734061956
forward train acc: top1 ->  57.69999862670898 ; top5 ->  79.50833091735839  and loss:  368.11903488636017
test acc: top1 ->  62.5499982837677 ; top5 ->  84.82199758300781  and loss:  1318.3619174584746
forward train acc: top1 ->  58.31666530609131 ; top5 ->  80.39166400909424  and loss:  365.4655929803848
test acc: top1 ->  62.45799825592041 ; top5 ->  84.92999757232666  and loss:  1319.739136621356
forward train acc: top1 ->  59.5499984741211 ; top5 ->  81.28333087921142  and loss:  351.4162001609802
test acc: top1 ->  64.10799815063477 ; top5 ->  85.91599750671386  and loss:  1258.4464121609926
forward train acc: top1 ->  60.01666501998901 ; top5 ->  81.39999748229981  and loss:  349.8807921409607
test acc: top1 ->  64.7759981426239 ; top5 ->  86.16799731140136  and loss:  1229.2682687044144
forward train acc: top1 ->  60.82499834060669 ; top5 ->  82.1999974822998  and loss:  341.01220136880875
test acc: top1 ->  64.9779981185913 ; top5 ->  86.34799753112793  and loss:  1220.908296957612
forward train acc: top1 ->  61.66666522979736 ; top5 ->  83.05833103179931  and loss:  329.43685162067413
test acc: top1 ->  65.45399815444947 ; top5 ->  86.78399756622315  and loss:  1197.3710984885693
forward train acc: top1 ->  61.51666501998901 ; top5 ->  82.38333065032958  and loss:  333.835850417614
test acc: top1 ->  65.56599805374145 ; top5 ->  86.89199746551513  and loss:  1190.6591038256884
forward train acc: top1 ->  61.40833179473877 ; top5 ->  82.03333076477051  and loss:  336.92554491758347
test acc: top1 ->  65.92199803161621 ; top5 ->  86.98199748840332  and loss:  1183.1326005011797
forward train acc: top1 ->  63.24166484832764 ; top5 ->  83.52499755859375  and loss:  317.42546832561493
test acc: top1 ->  66.12599816894532 ; top5 ->  87.12599744262695  and loss:  1176.9900389164686
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  40 / 64 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -83.3357150554657 , diff:  83.3357150554657
adv train loss:  -82.59406328201294 , diff:  0.7416517734527588
************ all values are small in this layer **********
layer  5  adv train finish, try to retain  28
test acc: top1 ->  60.60999828491211 ; top5 ->  83.18399760284424  and loss:  1395.5069219619036
forward train acc: top1 ->  58.58333204269409 ; top5 ->  80.69166423797607  and loss:  361.8040523529053
test acc: top1 ->  62.39399823913574 ; top5 ->  85.1139974456787  and loss:  1311.9450332075357
forward train acc: top1 ->  57.85833192825317 ; top5 ->  79.81666440963745  and loss:  367.5973297357559
test acc: top1 ->  61.609998189544676 ; top5 ->  84.56599750823975  and loss:  1337.1980905383825
forward train acc: top1 ->  58.033331871032715 ; top5 ->  80.124997215271  and loss:  367.73393738269806
test acc: top1 ->  62.22999810943603 ; top5 ->  85.02399758453369  and loss:  1318.9270961135626
forward train acc: top1 ->  59.141665344238284 ; top5 ->  81.35833061218261  and loss:  350.2220582962036
test acc: top1 ->  64.22799827079773 ; top5 ->  85.99199727172852  and loss:  1247.3496289253235
forward train acc: top1 ->  60.52499843597412 ; top5 ->  81.78333065032959  and loss:  344.2650230526924
test acc: top1 ->  64.62599818077088 ; top5 ->  86.2519974533081  and loss:  1230.2652457207441
forward train acc: top1 ->  60.16666501998901 ; top5 ->  81.54999725341797  and loss:  344.41226494312286
test acc: top1 ->  64.71599811553955 ; top5 ->  86.3319975402832  and loss:  1232.103211775422
forward train acc: top1 ->  61.10833169937134 ; top5 ->  82.07499759674073  and loss:  335.86012053489685
test acc: top1 ->  65.46399808692932 ; top5 ->  86.71199755706787  and loss:  1201.496172502637
forward train acc: top1 ->  62.23333169937134 ; top5 ->  82.54999732971191  and loss:  330.30963867902756
test acc: top1 ->  65.54799810791016 ; top5 ->  86.85199749450683  and loss:  1192.9598284065723
forward train acc: top1 ->  60.55833169937134 ; top5 ->  81.61666397094727  and loss:  340.09123253822327
test acc: top1 ->  65.65999818115235 ; top5 ->  86.8199975845337  and loss:  1186.221791997552
forward train acc: top1 ->  62.31666511535644 ; top5 ->  83.3666641998291  and loss:  321.8842363357544
test acc: top1 ->  66.08399808273316 ; top5 ->  87.07599748535156  and loss:  1174.4734101742506
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  29 / 64 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -79.1890594959259 , diff:  79.1890594959259
adv train loss:  -81.716339468956 , diff:  2.5272799730300903
adv train loss:  -80.26547586917877 , diff:  1.4508635997772217
adv train loss:  -81.26907444000244 , diff:  1.0035985708236694
adv train loss:  -83.71038568019867 , diff:  2.441311240196228
adv train loss:  -80.21484851837158 , diff:  3.4955371618270874
adv train loss:  -81.55672150850296 , diff:  1.3418729901313782
adv train loss:  -84.05413174629211 , diff:  2.497410237789154
adv train loss:  -78.95822113752365 , diff:  5.095910608768463
adv train loss:  -84.32525205612183 , diff:  5.367030918598175
************ all values are small in this layer **********
layer  6  adv train finish, try to retain  111
test acc: top1 ->  64.181998147583 ; top5 ->  85.80999750061035  and loss:  1244.7551811933517
forward train acc: top1 ->  59.491665077209475 ; top5 ->  80.58333097457886  and loss:  357.8162326812744
test acc: top1 ->  62.471998236083984 ; top5 ->  84.81799749298095  and loss:  1322.8023963272572
forward train acc: top1 ->  58.32499858856201 ; top5 ->  80.3749974822998  and loss:  360.99434876441956
test acc: top1 ->  62.94199809112549 ; top5 ->  85.51599743347168  and loss:  1283.9444388002157
forward train acc: top1 ->  58.174998683929445 ; top5 ->  80.34166408538819  and loss:  366.54953467845917
test acc: top1 ->  62.10199819831848 ; top5 ->  84.55199773254394  and loss:  1322.786945976317
forward train acc: top1 ->  59.78333179473877 ; top5 ->  81.52499748229981  and loss:  351.6824938058853
test acc: top1 ->  64.48799810256958 ; top5 ->  86.06799744567871  and loss:  1235.1959861069918
forward train acc: top1 ->  59.908331890106204 ; top5 ->  81.14999740600587  and loss:  352.5326122045517
test acc: top1 ->  64.72199800643921 ; top5 ->  86.3919974761963  and loss:  1216.1722987741232
forward train acc: top1 ->  60.2166650390625 ; top5 ->  81.3999976348877  and loss:  348.5977988243103
test acc: top1 ->  64.78799804763794 ; top5 ->  86.50199764709473  and loss:  1215.2090785354376
forward train acc: top1 ->  61.233331775665285 ; top5 ->  82.49999732971192  and loss:  332.69901502132416
test acc: top1 ->  65.6079980834961 ; top5 ->  86.96399737701417  and loss:  1190.8602343052626
forward train acc: top1 ->  62.11666494369507 ; top5 ->  82.9333306503296  and loss:  324.13097012043
test acc: top1 ->  65.55199818611145 ; top5 ->  86.88199748535156  and loss:  1193.0608175992966
forward train acc: top1 ->  62.14166496276855 ; top5 ->  83.48333072662354  and loss:  322.98556953668594
test acc: top1 ->  65.8579981349945 ; top5 ->  86.97999757385254  and loss:  1182.816756874323
forward train acc: top1 ->  62.891665058135985 ; top5 ->  82.99166423797607  and loss:  324.93540155887604
test acc: top1 ->  66.19599799499511 ; top5 ->  87.1519975402832  and loss:  1173.8776758760214
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  112 / 128 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -80.08134412765503 , diff:  80.08134412765503
adv train loss:  -83.78441298007965 , diff:  3.7030688524246216
adv train loss:  -82.85768330097198 , diff:  0.926729679107666
layer  7  adv train finish, try to retain  123
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -78.34198129177094 , diff:  78.34198129177094
adv train loss:  -79.02803909778595 , diff:  0.6860578060150146
layer  8  adv train finish, try to retain  106
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -81.35044705867767 , diff:  81.35044705867767
adv train loss:  -80.83582007884979 , diff:  0.5146269798278809
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  114
test acc: top1 ->  65.2279982131958 ; top5 ->  86.69599747619628  and loss:  1201.7931964695454
forward train acc: top1 ->  58.408331851959225 ; top5 ->  80.36666425704956  and loss:  363.1515316963196
test acc: top1 ->  62.93199809799194 ; top5 ->  85.32399758300781  and loss:  1292.2364693135023
forward train acc: top1 ->  58.60833179473877 ; top5 ->  80.60833084106446  and loss:  363.0425820350647
test acc: top1 ->  62.10599805145264 ; top5 ->  84.64799755859374  and loss:  1331.8250185623765
forward train acc: top1 ->  58.88333198547363 ; top5 ->  80.70833072662353  and loss:  362.0060656070709
test acc: top1 ->  62.42999821281433 ; top5 ->  84.87999744262696  and loss:  1318.0489473491907
forward train acc: top1 ->  60.09166519165039 ; top5 ->  81.17499719619751  and loss:  349.0174113512039
test acc: top1 ->  64.32599818477631 ; top5 ->  86.08599752807618  and loss:  1244.622554257512
forward train acc: top1 ->  61.041665172576906 ; top5 ->  81.64999759674072  and loss:  340.69089567661285
test acc: top1 ->  64.6819980922699 ; top5 ->  86.47799756164551  and loss:  1223.5937858223915
forward train acc: top1 ->  61.3416651725769 ; top5 ->  82.64166416168213  and loss:  334.5957821011543
test acc: top1 ->  64.86999802246093 ; top5 ->  86.46999769134521  and loss:  1218.5715690851212
forward train acc: top1 ->  61.491665019989014 ; top5 ->  82.29166431427002  and loss:  336.7927497625351
test acc: top1 ->  65.40999818572998 ; top5 ->  86.86999747009277  and loss:  1198.060561209917
forward train acc: top1 ->  62.5166650390625 ; top5 ->  83.14999748229981  and loss:  325.31750947237015
test acc: top1 ->  65.37799817581177 ; top5 ->  86.95199746246338  and loss:  1189.3833585232496
forward train acc: top1 ->  61.16666507720947 ; top5 ->  82.53333053588867  and loss:  331.1156885623932
test acc: top1 ->  65.91599804840088 ; top5 ->  87.22799757080078  and loss:  1172.6196459829807
forward train acc: top1 ->  63.14166496276855 ; top5 ->  83.54166412353516  and loss:  319.2983173727989
test acc: top1 ->  66.04199806442261 ; top5 ->  87.29799747924805  and loss:  1167.782777145505
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  115 / 128 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -79.83727598190308 , diff:  79.83727598190308
adv train loss:  -83.77444016933441 , diff:  3.9371641874313354
adv train loss:  -79.06078684329987 , diff:  4.713653326034546
adv train loss:  -80.44487851858139 , diff:  1.3840916752815247
adv train loss:  -82.41135060787201 , diff:  1.966472089290619
adv train loss:  -78.54469501972198 , diff:  3.8666555881500244
adv train loss:  -84.80883884429932 , diff:  6.2641438245773315
adv train loss:  -80.80506372451782 , diff:  4.003775119781494
adv train loss:  -80.47306728363037 , diff:  0.33199644088745117
layer  10  adv train finish, try to retain  99
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -78.75237607955933 , diff:  78.75237607955933
adv train loss:  -81.30152428150177 , diff:  2.549148201942444
adv train loss:  -82.570392370224 , diff:  1.268868088722229
adv train loss:  -84.57644820213318 , diff:  2.0060558319091797
adv train loss:  -81.91100490093231 , diff:  2.6654433012008667
adv train loss:  -79.0473148226738 , diff:  2.8636900782585144
adv train loss:  -80.85706454515457 , diff:  1.809749722480774
adv train loss:  -79.57638227939606 , diff:  1.2806822657585144
adv train loss:  -80.94122833013535 , diff:  1.3648460507392883
adv train loss:  -77.38813614845276 , diff:  3.5530921816825867
layer  11  adv train finish, try to retain  102
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -79.30366015434265 , diff:  79.30366015434265
adv train loss:  -78.47306168079376 , diff:  0.8305984735488892
layer  12  adv train finish, try to retain  102
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -82.07631766796112 , diff:  82.07631766796112
adv train loss:  -83.82820761203766 , diff:  1.751889944076538
adv train loss:  -81.84466421604156 , diff:  1.9835433959960938
adv train loss:  -81.02298539876938 , diff:  0.8216788172721863
layer  13  adv train finish, try to retain  90
>>>>>>> reverse layer  13  since no improvement >>>>>>>
---------------- start layer  14  ---------------
adv train loss:  -79.87439179420471 , diff:  79.87439179420471
adv train loss:  -80.19820845127106 , diff:  0.3238166570663452
layer  14  adv train finish, try to retain  253
>>>>>>> reverse layer  14  since no improvement >>>>>>>
---------------- start layer  15  ---------------
adv train loss:  -83.2224873304367 , diff:  83.2224873304367
adv train loss:  -82.37557888031006 , diff:  0.846908450126648
layer  15  adv train finish, try to retain  247
test acc: top1 ->  65.57399814987183 ; top5 ->  86.8999975112915  and loss:  1188.7159820646048
forward train acc: top1 ->  59.14166521072388 ; top5 ->  81.34999755859376  and loss:  350.06518590450287
test acc: top1 ->  62.68599820785523 ; top5 ->  85.01799758453369  and loss:  1301.4935679137707
forward train acc: top1 ->  58.53333183288574 ; top5 ->  80.34999725341797  and loss:  364.4089608192444
test acc: top1 ->  61.9159981924057 ; top5 ->  84.44799750366211  and loss:  1340.6955616921186
forward train acc: top1 ->  58.65833179473877 ; top5 ->  80.70833087921143  and loss:  359.20124340057373
test acc: top1 ->  61.63199825363159 ; top5 ->  84.52799753723144  and loss:  1344.4024625122547
forward train acc: top1 ->  59.54999858856201 ; top5 ->  81.35833095550537  and loss:  350.31518828868866
test acc: top1 ->  64.46999805297851 ; top5 ->  86.19599760437012  and loss:  1238.7965386509895
forward train acc: top1 ->  61.18333150863648 ; top5 ->  82.77499725341796  and loss:  333.4435600042343
test acc: top1 ->  64.89199803619385 ; top5 ->  86.3399975982666  and loss:  1219.1767221987247
forward train acc: top1 ->  60.24166519165039 ; top5 ->  81.64999755859375  and loss:  344.360004901886
test acc: top1 ->  65.14799814300537 ; top5 ->  86.54599766082764  and loss:  1212.6709768921137
forward train acc: top1 ->  61.29166509628296 ; top5 ->  82.15833087921142  and loss:  336.18705213069916
test acc: top1 ->  65.63399798583984 ; top5 ->  86.77799751281738  and loss:  1195.260187342763
forward train acc: top1 ->  61.39999851226807 ; top5 ->  82.2583304977417  and loss:  336.73622465133667
test acc: top1 ->  65.66399803085328 ; top5 ->  86.84399736480712  and loss:  1187.3770524710417
forward train acc: top1 ->  62.03333156585693 ; top5 ->  82.74166416168212  and loss:  326.9334613084793
test acc: top1 ->  66.13599813919068 ; top5 ->  87.25599752502441  and loss:  1174.4216785281897
forward train acc: top1 ->  62.816664943695066 ; top5 ->  83.0916641998291  and loss:  322.66308921575546
test acc: top1 ->  66.45999815826416 ; top5 ->  87.28999758758545  and loss:  1164.4829118549824
>>>>>>> reverse layer  15  since performance drop >>>>>>>
==> this epoch:  248 / 256 , inc:  1
---------------- start layer  16  ---------------
adv train loss:  -78.05598658323288 , diff:  78.05598658323288
adv train loss:  -81.38881188631058 , diff:  3.3328253030776978
adv train loss:  -82.96193206310272 , diff:  1.5731201767921448
adv train loss:  -82.47923964262009 , diff:  0.4826924204826355
************ all values are small in this layer **********
layer  16  adv train finish, try to retain  238
test acc: top1 ->  66.10599810180663 ; top5 ->  87.127997555542  and loss:  1176.0072760730982
forward train acc: top1 ->  59.69166528701782 ; top5 ->  81.5499974822998  and loss:  349.5940692424774
test acc: top1 ->  63.09599824829102 ; top5 ->  85.21799757995605  and loss:  1301.3078048080206
forward train acc: top1 ->  58.9333318901062 ; top5 ->  80.50833103179932  and loss:  354.6649738550186
test acc: top1 ->  62.82199816741943 ; top5 ->  85.13599749298096  and loss:  1301.812601223588
forward train acc: top1 ->  58.82499843597412 ; top5 ->  80.6166641998291  and loss:  358.0371092557907
test acc: top1 ->  62.62999823989868 ; top5 ->  85.01399748840332  and loss:  1317.5899637788534
forward train acc: top1 ->  59.61666524887085 ; top5 ->  81.0166644668579  and loss:  353.4922956228256
test acc: top1 ->  64.32799804992676 ; top5 ->  85.97799745483398  and loss:  1241.6913458555937
forward train acc: top1 ->  60.79166519165039 ; top5 ->  81.74999723434448  and loss:  344.14776492118835
test acc: top1 ->  64.54799813079833 ; top5 ->  86.26999756774903  and loss:  1226.1955708414316
forward train acc: top1 ->  61.11666524887085 ; top5 ->  82.41666408538818  and loss:  334.9616125226021
test acc: top1 ->  64.9879980834961 ; top5 ->  86.48999756011963  and loss:  1214.901406466961
forward train acc: top1 ->  61.424998416900635 ; top5 ->  82.18333072662354  and loss:  333.3484683036804
test acc: top1 ->  65.681998072052 ; top5 ->  86.81399747009277  and loss:  1191.491493359208
forward train acc: top1 ->  62.33333181381226 ; top5 ->  83.24999771118163  and loss:  326.9027913212776
test acc: top1 ->  65.85199817428588 ; top5 ->  86.9419974533081  and loss:  1182.2961023896933
forward train acc: top1 ->  61.91666530609131 ; top5 ->  82.4916641998291  and loss:  331.16508316993713
test acc: top1 ->  66.19799804153442 ; top5 ->  87.0659975616455  and loss:  1174.059767961502
forward train acc: top1 ->  61.5999986076355 ; top5 ->  82.81666412353516  and loss:  328.5423868894577
test acc: top1 ->  66.34399796218872 ; top5 ->  87.2699974609375  and loss:  1165.8296231627464
>>>>>>> reverse layer  16  since performance drop >>>>>>>
==> this epoch:  239 / 256 , inc:  1
---------------- start layer  17  ---------------
adv train loss:  -82.15692472457886 , diff:  82.15692472457886
adv train loss:  -80.77231979370117 , diff:  1.3846049308776855
adv train loss:  -79.14159798622131 , diff:  1.6307218074798584
adv train loss:  -81.22050893306732 , diff:  2.0789109468460083
adv train loss:  -79.45908892154694 , diff:  1.7614200115203857
adv train loss:  -80.90934371948242 , diff:  1.4502547979354858
adv train loss:  -82.1899745464325 , diff:  1.2806308269500732
adv train loss:  -79.39660501480103 , diff:  2.7933695316314697
adv train loss:  -81.57823920249939 , diff:  2.1816341876983643
adv train loss:  -82.05180382728577 , diff:  0.47356462478637695
************ all values are small in this layer **********
layer  17  adv train finish, try to retain  229
test acc: top1 ->  66.09799809646607 ; top5 ->  87.04599739685058  and loss:  1180.6345408558846
forward train acc: top1 ->  59.69166534423828 ; top5 ->  81.00833059310914  and loss:  352.022181391716
test acc: top1 ->  62.81399823265075 ; top5 ->  85.05799757843018  and loss:  1305.6393642425537
forward train acc: top1 ->  58.95833183288574 ; top5 ->  80.27499732971191  and loss:  359.7645083665848
test acc: top1 ->  62.547998235321046 ; top5 ->  84.70199754180908  and loss:  1314.3437963724136
forward train acc: top1 ->  58.6249986076355 ; top5 ->  80.29166400909423  and loss:  362.64304316043854
test acc: top1 ->  62.771998108291626 ; top5 ->  85.1419975402832  and loss:  1308.279032677412
forward train acc: top1 ->  59.79166507720947 ; top5 ->  81.06666429519653  and loss:  351.1507924795151
test acc: top1 ->  64.79199808425903 ; top5 ->  86.25399760284424  and loss:  1234.177135080099
forward train acc: top1 ->  60.341665267944336 ; top5 ->  81.72499752044678  and loss:  341.93384540081024
test acc: top1 ->  64.97599818496704 ; top5 ->  86.41199752960205  and loss:  1221.1631261110306
forward train acc: top1 ->  60.774998569488524 ; top5 ->  82.4583309173584  and loss:  337.7481920719147
test acc: top1 ->  65.09599802093506 ; top5 ->  86.61399755096436  and loss:  1211.9821023792028
forward train acc: top1 ->  61.69999849319458 ; top5 ->  82.59166416168213  and loss:  329.73955404758453
test acc: top1 ->  66.07399809188843 ; top5 ->  86.95799763488769  and loss:  1185.5275799632072
forward train acc: top1 ->  61.708331718444825 ; top5 ->  82.26666431427002  and loss:  333.5564533472061
test acc: top1 ->  66.10799806671143 ; top5 ->  87.05799757843018  and loss:  1179.036374911666
forward train acc: top1 ->  63.13333169937134 ; top5 ->  83.18333053588867  and loss:  321.7464923262596
test acc: top1 ->  66.24399807281495 ; top5 ->  87.0479976348877  and loss:  1176.0210901498795
forward train acc: top1 ->  61.908331813812254 ; top5 ->  82.70833072662353  and loss:  328.3291058540344
test acc: top1 ->  66.59399807739258 ; top5 ->  87.1939974395752  and loss:  1166.6278852671385
>>>>>>> reverse layer  17  since performance drop >>>>>>>
==> this epoch:  230 / 256 , inc:  1
---------------- start layer  18  ---------------
adv train loss:  -79.64143413305283 , diff:  79.64143413305283
adv train loss:  -84.37163746356964 , diff:  4.730203330516815
adv train loss:  -80.99939703941345 , diff:  3.372240424156189
adv train loss:  -80.49888384342194 , diff:  0.5005131959915161
************ all values are small in this layer **********
layer  18  adv train finish, try to retain  234
test acc: top1 ->  66.35799800872803 ; top5 ->  87.31399754486084  and loss:  1163.9090661406517
forward train acc: top1 ->  59.54166526794434 ; top5 ->  81.45833087921143  and loss:  350.93715715408325
test acc: top1 ->  63.04599823532104 ; top5 ->  85.34799760437012  and loss:  1300.607730999589
forward train acc: top1 ->  58.5499987411499 ; top5 ->  80.75833065032958  and loss:  356.1635684967041
test acc: top1 ->  62.80799818229675 ; top5 ->  85.15599760131836  and loss:  1298.4681777209044
forward train acc: top1 ->  58.49999862670899 ; top5 ->  80.66666431427002  and loss:  360.0044991970062
test acc: top1 ->  63.07199813156128 ; top5 ->  85.26199746398926  and loss:  1285.8595039248466
forward train acc: top1 ->  60.124998569488525 ; top5 ->  82.00833072662354  and loss:  342.2215647697449
test acc: top1 ->  64.6699980381012 ; top5 ->  86.28799744873047  and loss:  1228.1034296900034
forward train acc: top1 ->  61.08333181381226 ; top5 ->  81.73333095550537  and loss:  339.2458599805832
test acc: top1 ->  65.16599814071655 ; top5 ->  86.54599752197265  and loss:  1218.2072298526764
forward train acc: top1 ->  60.6916650390625 ; top5 ->  81.94999767303467  and loss:  338.02394580841064
test acc: top1 ->  65.24199814682007 ; top5 ->  86.57999735870361  and loss:  1211.85745793581
forward train acc: top1 ->  61.1083314704895 ; top5 ->  81.83333106994628  and loss:  338.60222935676575
test acc: top1 ->  65.82599820175172 ; top5 ->  86.95799745025634  and loss:  1189.3780904263258
forward train acc: top1 ->  62.25833160400391 ; top5 ->  82.93333080291748  and loss:  323.2856997847557
test acc: top1 ->  65.93199810333252 ; top5 ->  87.06199750518799  and loss:  1181.1651908308268
forward train acc: top1 ->  61.649998359680175 ; top5 ->  83.12499752044678  and loss:  325.4099459052086
test acc: top1 ->  66.15399797058106 ; top5 ->  87.1479975845337  and loss:  1176.693248808384
forward train acc: top1 ->  62.408331813812254 ; top5 ->  83.19999755859375  and loss:  323.52285718917847
test acc: top1 ->  66.35399816513062 ; top5 ->  87.27399743347168  and loss:  1167.2594351023436
>>>>>>> reverse layer  18  since performance drop >>>>>>>
==> this epoch:  235 / 256 , inc:  1
---------------- start layer  19  ---------------
adv train loss:  -80.15751349925995 , diff:  80.15751349925995
adv train loss:  -81.1232944726944 , diff:  0.9657809734344482
layer  19  adv train finish, try to retain  186
>>>>>>> reverse layer  19  since no improvement >>>>>>>
---------------- start layer  20  ---------------
adv train loss:  -82.50314962863922 , diff:  82.50314962863922
adv train loss:  -82.18327236175537 , diff:  0.3198772668838501
************ all values are small in this layer **********
layer  20  adv train finish, try to retain  224
test acc: top1 ->  66.11599812545776 ; top5 ->  87.16799751586915  and loss:  1175.148265182972
forward train acc: top1 ->  60.10833171844482 ; top5 ->  81.03333072662353  and loss:  349.18792366981506
test acc: top1 ->  63.21199819030762 ; top5 ->  85.50599753723145  and loss:  1283.373037904501
forward train acc: top1 ->  59.42499853134155 ; top5 ->  81.04166412353516  and loss:  356.4377101659775
test acc: top1 ->  62.941998148345945 ; top5 ->  85.1979976409912  and loss:  1305.0251550674438
forward train acc: top1 ->  60.066665172576904 ; top5 ->  81.1916639328003  and loss:  347.431418299675
test acc: top1 ->  62.57199815139771 ; top5 ->  84.88799762115478  and loss:  1320.1210610717535
forward train acc: top1 ->  60.47499841690063 ; top5 ->  81.85833061218261  and loss:  343.13496392965317
test acc: top1 ->  64.58799807167053 ; top5 ->  86.0719976196289  and loss:  1235.785669773817
forward train acc: top1 ->  60.241665325164796 ; top5 ->  81.63333065032958  and loss:  341.29772955179214
test acc: top1 ->  64.8139981880188 ; top5 ->  86.43799747161866  and loss:  1225.1078661829233
forward train acc: top1 ->  61.86666515350342 ; top5 ->  82.48333068847656  and loss:  335.05446738004684
test acc: top1 ->  65.01399820632935 ; top5 ->  86.56999749450684  and loss:  1215.5153765529394
forward train acc: top1 ->  61.908331813812254 ; top5 ->  83.03333095550538  and loss:  328.55737245082855
test acc: top1 ->  65.89799811019897 ; top5 ->  87.01399740905762  and loss:  1185.008317872882
forward train acc: top1 ->  62.79166496276856 ; top5 ->  83.32499740600586  and loss:  321.263740837574
test acc: top1 ->  65.96199811096191 ; top5 ->  87.07199741973876  and loss:  1181.4089361131191
forward train acc: top1 ->  62.64999839782715 ; top5 ->  83.36666391372681  and loss:  319.1993685364723
test acc: top1 ->  66.07199811782837 ; top5 ->  87.21599740600585  and loss:  1170.844731748104
forward train acc: top1 ->  61.52499814987183 ; top5 ->  82.44166389465332  and loss:  330.8916606903076
test acc: top1 ->  66.45199813537597 ; top5 ->  87.35599746704102  and loss:  1163.415376752615
>>>>>>> reverse layer  20  since performance drop >>>>>>>
==> this epoch:  225 / 256 , inc:  1
---------------- start layer  21  ---------------
adv train loss:  -81.6712018251419 , diff:  81.6712018251419
adv train loss:  -79.1858993768692 , diff:  2.485302448272705
adv train loss:  -81.27377116680145 , diff:  2.087871789932251
adv train loss:  -79.95664763450623 , diff:  1.317123532295227
adv train loss:  -83.69516187906265 , diff:  3.738514244556427
adv train loss:  -82.88953173160553 , diff:  0.8056301474571228
************ all values are small in this layer **********
layer  21  adv train finish, try to retain  241
test acc: top1 ->  66.40399809417724 ; top5 ->  87.31399750823975  and loss:  1165.1305532753468
forward train acc: top1 ->  59.324998722076415 ; top5 ->  80.51666431427002  and loss:  353.4387036561966
test acc: top1 ->  63.149998150634765 ; top5 ->  85.07399736175537  and loss:  1290.378192640841
forward train acc: top1 ->  58.96666511535645 ; top5 ->  81.42499774932861  and loss:  348.9673002958298
test acc: top1 ->  62.56599806957245 ; top5 ->  85.09799745330811  and loss:  1304.3892105817795
forward train acc: top1 ->  58.84999856948853 ; top5 ->  80.18333066940308  and loss:  360.3013674020767
test acc: top1 ->  62.907998120117185 ; top5 ->  85.26599751281738  and loss:  1293.9709496498108
forward train acc: top1 ->  60.458331966400145 ; top5 ->  81.6833307647705  and loss:  343.9540629386902
test acc: top1 ->  64.77999803009033 ; top5 ->  86.38999734497071  and loss:  1225.52036139369
forward train acc: top1 ->  61.033331813812254 ; top5 ->  81.76666418075561  and loss:  342.17853593826294
test acc: top1 ->  65.14599806747437 ; top5 ->  86.56799752807618  and loss:  1216.8583589494228
forward train acc: top1 ->  60.408331928253176 ; top5 ->  81.77499767303466  and loss:  345.8787308335304
test acc: top1 ->  65.21599806671142 ; top5 ->  86.58399740753174  and loss:  1214.524702936411
forward train acc: top1 ->  62.51666500091553 ; top5 ->  82.6499976348877  and loss:  328.475159406662
test acc: top1 ->  65.87199811401368 ; top5 ->  86.98799746856689  and loss:  1187.8403475284576
forward train acc: top1 ->  62.616665058135986 ; top5 ->  83.28333106994629  and loss:  322.07099425792694
test acc: top1 ->  66.06999814834595 ; top5 ->  87.13999756622314  and loss:  1177.7839415669441
forward train acc: top1 ->  62.32499835968017 ; top5 ->  82.5749974822998  and loss:  326.04420006275177
test acc: top1 ->  66.35399817581177 ; top5 ->  87.23599747161865  and loss:  1169.7626262903214
forward train acc: top1 ->  62.47499841690063 ; top5 ->  82.9749974822998  and loss:  323.2794287800789
test acc: top1 ->  66.643998122406 ; top5 ->  87.35399758453369  and loss:  1157.9295785427094
>>>>>>> reverse layer  21  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  22  ---------------
adv train loss:  -83.20752507448196 , diff:  83.20752507448196
adv train loss:  -79.03852772712708 , diff:  4.168997347354889
adv train loss:  -74.5155382156372 , diff:  4.522989511489868
adv train loss:  -81.34837520122528 , diff:  6.832836985588074
adv train loss:  -80.26276922225952 , diff:  1.0856059789657593
adv train loss:  -84.16838753223419 , diff:  3.9056183099746704
adv train loss:  -76.88107877969742 , diff:  7.287308752536774
adv train loss:  -78.25547957420349 , diff:  1.374400794506073
adv train loss:  -76.97972822189331 , diff:  1.2757513523101807
adv train loss:  -81.5348230600357 , diff:  4.555094838142395
layer  22  adv train finish, try to retain  181
test acc: top1 ->  66.09599807510376 ; top5 ->  86.94399760284423  and loss:  1181.627759233117
forward train acc: top1 ->  59.99166515350342 ; top5 ->  81.39999736785889  and loss:  346.5593491792679
test acc: top1 ->  63.003998210906985 ; top5 ->  85.27799764709472  and loss:  1291.7025002166629
forward train acc: top1 ->  59.049998683929445 ; top5 ->  81.04166410446167  and loss:  354.1438527107239
test acc: top1 ->  63.00999811553955 ; top5 ->  85.1879975730896  and loss:  1294.549448698759
forward train acc: top1 ->  59.47499858856201 ; top5 ->  81.0333304977417  and loss:  354.8462165594101
test acc: top1 ->  62.71599819488525 ; top5 ->  85.14799758911133  and loss:  1294.1493225693703
forward train acc: top1 ->  59.7416650390625 ; top5 ->  81.39166416168213  and loss:  346.8886822462082
test acc: top1 ->  64.50199816627503 ; top5 ->  86.33599756622314  and loss:  1227.2431297153234
forward train acc: top1 ->  60.22499841690063 ; top5 ->  81.79166408538818  and loss:  343.80718421936035
test acc: top1 ->  64.93799820404053 ; top5 ->  86.40399739837646  and loss:  1215.949085265398
forward train acc: top1 ->  61.79999849319458 ; top5 ->  82.34166439056396  and loss:  332.1612718105316
test acc: top1 ->  65.14999810409546 ; top5 ->  86.59799745788574  and loss:  1210.9613701850176
forward train acc: top1 ->  61.4833318901062 ; top5 ->  82.11666400909424  and loss:  335.1422735452652
test acc: top1 ->  65.923998097229 ; top5 ->  86.94599743499757  and loss:  1181.2327473461628
forward train acc: top1 ->  62.224998531341555 ; top5 ->  83.22499752044678  and loss:  325.6136386394501
test acc: top1 ->  66.05399805374145 ; top5 ->  87.06799742736817  and loss:  1172.3644331544638
forward train acc: top1 ->  62.09999841690063 ; top5 ->  82.57499759674073  and loss:  330.34251683950424
test acc: top1 ->  65.98799809494018 ; top5 ->  87.06399748535156  and loss:  1177.8833402246237
forward train acc: top1 ->  62.70833183288574 ; top5 ->  83.45833084106445  and loss:  320.60438138246536
test acc: top1 ->  66.44599809112549 ; top5 ->  87.38399752502441  and loss:  1164.7395425140858
>>>>>>> reverse layer  22  since performance drop >>>>>>>
==> this epoch:  208 / 256 , inc:  1
---------------- start layer  23  ---------------
adv train loss:  -84.0577854514122 , diff:  84.0577854514122
adv train loss:  -80.60133016109467 , diff:  3.4564552903175354
adv train loss:  -79.83083748817444 , diff:  0.770492672920227
layer  23  adv train finish, try to retain  184
test acc: top1 ->  66.00999799423218 ; top5 ->  87.13999744873047  and loss:  1179.2223188728094
forward train acc: top1 ->  59.89999845504761 ; top5 ->  81.47499740600585  and loss:  346.3324610590935
test acc: top1 ->  63.06999810256958 ; top5 ->  85.23599739227295  and loss:  1297.153363622725
forward train acc: top1 ->  58.74166542053223 ; top5 ->  80.70833080291749  and loss:  357.8833974003792
test acc: top1 ->  62.41199814414978 ; top5 ->  84.73599752807617  and loss:  1324.4701949357986
forward train acc: top1 ->  58.983331775665285 ; top5 ->  80.8916640472412  and loss:  358.34085059165955
test acc: top1 ->  63.29599803466797 ; top5 ->  85.36799753723145  and loss:  1284.887471795082
forward train acc: top1 ->  60.4083317565918 ; top5 ->  81.483330783844  and loss:  347.3470587134361
test acc: top1 ->  64.80599812088013 ; top5 ->  86.20999742889404  and loss:  1223.0896756500006
forward train acc: top1 ->  61.50833194732666 ; top5 ->  82.59999752044678  and loss:  334.1310578584671
test acc: top1 ->  64.92199801101684 ; top5 ->  86.21999757690429  and loss:  1225.5694298744202
forward train acc: top1 ->  60.94166511535644 ; top5 ->  82.2916641998291  and loss:  338.4080793261528
test acc: top1 ->  65.11999808502198 ; top5 ->  86.48999756469726  and loss:  1215.1776638105512
forward train acc: top1 ->  62.424998359680174 ; top5 ->  83.20833057403564  and loss:  324.0288686156273
test acc: top1 ->  66.0019981716156 ; top5 ->  86.8979974243164  and loss:  1183.3723607808352
forward train acc: top1 ->  62.55833162307739 ; top5 ->  83.08333076477051  and loss:  321.3127237558365
test acc: top1 ->  66.01199812774658 ; top5 ->  87.02999731903076  and loss:  1178.469894811511
forward train acc: top1 ->  62.2416650390625 ; top5 ->  82.6833306503296  and loss:  329.9402221441269
test acc: top1 ->  66.2799980255127 ; top5 ->  87.13199749298096  and loss:  1173.1886075288057
forward train acc: top1 ->  62.46666511535645 ; top5 ->  83.23333076477051  and loss:  320.02591437101364
test acc: top1 ->  66.60799805145264 ; top5 ->  87.34399743957519  and loss:  1163.137612849474
>>>>>>> reverse layer  23  since performance drop >>>>>>>
==> this epoch:  187 / 256 , inc:  1
---------------- start layer  24  ---------------
adv train loss:  -83.34309828281403 , diff:  83.34309828281403
adv train loss:  -80.65658211708069 , diff:  2.6865161657333374
adv train loss:  -82.01856136322021 , diff:  1.3619792461395264
adv train loss:  -84.9690271615982 , diff:  2.9504657983779907
adv train loss:  -78.96113419532776 , diff:  6.007892966270447
adv train loss:  -80.33531683683395 , diff:  1.374182641506195
adv train loss:  -78.946453332901 , diff:  1.3888635039329529
adv train loss:  -80.87245827913284 , diff:  1.926004946231842
adv train loss:  -82.09777271747589 , diff:  1.225314438343048
adv train loss:  -75.98271691799164 , diff:  6.115055799484253
************ all values are small in this layer **********
layer  24  adv train finish, try to retain  243
test acc: top1 ->  66.52199800186158 ; top5 ->  87.35999749298095  and loss:  1161.8179784715176
forward train acc: top1 ->  59.98333192825317 ; top5 ->  81.15833080291748  and loss:  348.7948016524315
test acc: top1 ->  63.45799827384949 ; top5 ->  85.38999749450683  and loss:  1277.7091420963407
forward train acc: top1 ->  59.78333173751831 ; top5 ->  81.32499738693237  and loss:  349.4868985414505
test acc: top1 ->  63.57599814605713 ; top5 ->  85.6479975982666  and loss:  1274.3876787871122
forward train acc: top1 ->  59.44999853134155 ; top5 ->  81.19999740600586  and loss:  352.36626505851746
test acc: top1 ->  62.46799813156128 ; top5 ->  84.97999752197266  and loss:  1310.232714869082
forward train acc: top1 ->  60.4083317565918 ; top5 ->  81.9249973678589  and loss:  343.83924371004105
test acc: top1 ->  64.84599808731079 ; top5 ->  86.33199748077392  and loss:  1225.155939295888
forward train acc: top1 ->  61.57499830245972 ; top5 ->  82.32499752044677  and loss:  331.54229152202606
test acc: top1 ->  64.97999816627502 ; top5 ->  86.47999751739502  and loss:  1218.6502532362938
forward train acc: top1 ->  60.90833168029785 ; top5 ->  82.01666408538819  and loss:  335.96782767772675
test acc: top1 ->  65.12599815063477 ; top5 ->  86.68799738006592  and loss:  1203.0386600196362
forward train acc: top1 ->  61.48333190917969 ; top5 ->  82.46666393280029  and loss:  333.69100946187973
test acc: top1 ->  66.02799799728393 ; top5 ->  87.14799751739503  and loss:  1177.7996784374118
forward train acc: top1 ->  62.0083318901062 ; top5 ->  82.66666416168214  and loss:  327.57707154750824
test acc: top1 ->  66.00199802780152 ; top5 ->  87.15199743652343  and loss:  1176.6703395545483
forward train acc: top1 ->  62.31666500091553 ; top5 ->  82.65833076477051  and loss:  325.84420293569565
test acc: top1 ->  66.19799804458619 ; top5 ->  87.2879974105835  and loss:  1164.7135406658053
forward train acc: top1 ->  63.44166501998902 ; top5 ->  83.61666400909424  and loss:  317.37204360961914
test acc: top1 ->  66.60399804191589 ; top5 ->  87.44999751586914  and loss:  1157.814999140799
>>>>>>> reverse layer  24  since performance drop >>>>>>>
==> this epoch:  244 / 256 , inc:  1
---------------- start layer  25  ---------------
adv train loss:  -78.0228214263916 , diff:  78.0228214263916
adv train loss:  -79.90470719337463 , diff:  1.8818857669830322
adv train loss:  -79.41682368516922 , diff:  0.4878835082054138
************ all values are small in this layer **********
layer  25  adv train finish, try to retain  241
test acc: top1 ->  66.41999813690185 ; top5 ->  87.36799753875732  and loss:  1161.0208822712302
forward train acc: top1 ->  59.82499858856201 ; top5 ->  81.63333091735839  and loss:  347.7429224252701
test acc: top1 ->  63.53999810066223 ; top5 ->  85.36799757537842  and loss:  1281.6290837675333
forward train acc: top1 ->  59.549998626708984 ; top5 ->  80.94999744415283  and loss:  356.4819358587265
test acc: top1 ->  62.499998141860964 ; top5 ->  84.75199760894776  and loss:  1316.3137157857418
forward train acc: top1 ->  59.17499876022339 ; top5 ->  81.30833095550537  and loss:  355.04007959365845
test acc: top1 ->  62.74799818725586 ; top5 ->  85.02599747009278  and loss:  1296.8682117313147
forward train acc: top1 ->  60.499998550415036 ; top5 ->  82.16666397094727  and loss:  338.8647428750992
test acc: top1 ->  65.15599811477661 ; top5 ->  86.42599745635987  and loss:  1207.5186468809843
forward train acc: top1 ->  61.224998264312745 ; top5 ->  82.12499725341797  and loss:  337.70154625177383
test acc: top1 ->  65.43199808502197 ; top5 ->  86.67999748992919  and loss:  1200.79240077734
forward train acc: top1 ->  61.60833173751831 ; top5 ->  82.48333072662354  and loss:  333.72161984443665
test acc: top1 ->  65.28399825134278 ; top5 ->  86.6519973739624  and loss:  1200.492194160819
forward train acc: top1 ->  62.024998302459714 ; top5 ->  83.23333068847656  and loss:  326.5343509912491
test acc: top1 ->  66.04599811668396 ; top5 ->  87.08999761505127  and loss:  1173.2243353128433
forward train acc: top1 ->  62.658331604003905 ; top5 ->  82.97499740600585  and loss:  322.4284154176712
test acc: top1 ->  66.19999800872803 ; top5 ->  87.22199752349853  and loss:  1173.4550944417715
forward train acc: top1 ->  62.458331718444825 ; top5 ->  82.6749973678589  and loss:  327.88178265094757
test acc: top1 ->  66.15599812927246 ; top5 ->  87.34199758148193  and loss:  1163.3688215315342
forward train acc: top1 ->  62.491665000915525 ; top5 ->  83.249997215271  and loss:  326.322692155838
test acc: top1 ->  66.5779980796814 ; top5 ->  87.4639975479126  and loss:  1152.93994769454
>>>>>>> reverse layer  25  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  26  ---------------
adv train loss:  -80.31156545877457 , diff:  80.31156545877457
adv train loss:  -76.56411159038544 , diff:  3.7474538683891296
adv train loss:  -77.01494604349136 , diff:  0.4508344531059265
layer  26  adv train finish, try to retain  500
test acc: top1 ->  66.21999800643921 ; top5 ->  87.32199755859375  and loss:  1170.1666675657034
forward train acc: top1 ->  60.24166515350342 ; top5 ->  81.87499752044678  and loss:  344.4124746322632
test acc: top1 ->  63.45199816436767 ; top5 ->  85.55399750976562  and loss:  1274.5491861701012
forward train acc: top1 ->  58.741665306091306 ; top5 ->  80.49166397094727  and loss:  358.37991881370544
test acc: top1 ->  63.33199817199707 ; top5 ->  85.42399760742188  and loss:  1278.997607588768
forward train acc: top1 ->  59.61666524887085 ; top5 ->  80.78333095550538  and loss:  355.8283840417862
test acc: top1 ->  63.013998152923584 ; top5 ->  85.34599753417969  and loss:  1284.7841446846724
forward train acc: top1 ->  60.33333158493042 ; top5 ->  81.28333059310913  and loss:  346.7139747142792
test acc: top1 ->  64.91399803161622 ; top5 ->  86.25799755706787  and loss:  1218.6082384884357
forward train acc: top1 ->  61.22499835968018 ; top5 ->  82.41666408538818  and loss:  335.6648961901665
test acc: top1 ->  65.12999812088013 ; top5 ->  86.64599736328125  and loss:  1208.5743185430765
forward train acc: top1 ->  61.25833158493042 ; top5 ->  82.44999700546265  and loss:  330.8885017633438
test acc: top1 ->  65.45599800567626 ; top5 ->  86.84399760437012  and loss:  1198.7558097094297
forward train acc: top1 ->  62.50833166122437 ; top5 ->  83.24166400909424  and loss:  324.5504187941551
test acc: top1 ->  65.89799821243287 ; top5 ->  87.01399739074706  and loss:  1180.979120478034
forward train acc: top1 ->  62.31666509628296 ; top5 ->  83.20833057403564  and loss:  325.9118029475212
test acc: top1 ->  66.29199807281495 ; top5 ->  87.10599745178223  and loss:  1167.1664812713861
forward train acc: top1 ->  62.2166650390625 ; top5 ->  82.92499771118165  and loss:  324.81047332286835
test acc: top1 ->  66.36199800872802 ; top5 ->  87.36199751586913  and loss:  1162.1118532121181
forward train acc: top1 ->  63.4083317565918 ; top5 ->  83.54166385650635  and loss:  316.83965969085693
test acc: top1 ->  66.55399808197022 ; top5 ->  87.5019974761963  and loss:  1151.7339892834425
>>>>>>> reverse layer  26  since performance drop >>>>>>>
==> this epoch:  502 / 512 , inc:  1
---------------- start layer  27  ---------------
adv train loss:  -80.04267024993896 , diff:  80.04267024993896
adv train loss:  -80.28400599956512 , diff:  0.24133574962615967
layer  27  adv train finish, try to retain  500
test acc: top1 ->  66.34799811706543 ; top5 ->  87.26799749450683  and loss:  1162.1523698419333
forward train acc: top1 ->  60.27499849319458 ; top5 ->  81.54999732971191  and loss:  345.5302796959877
test acc: top1 ->  63.6219981338501 ; top5 ->  85.53999746704102  and loss:  1273.9264326691628
forward train acc: top1 ->  59.81666511535644 ; top5 ->  80.72499759674072  and loss:  351.4178498983383
test acc: top1 ->  63.191998125076296 ; top5 ->  85.35999750366211  and loss:  1287.8224567174911
forward train acc: top1 ->  59.29166507720947 ; top5 ->  80.5583307647705  and loss:  355.3008997440338
test acc: top1 ->  63.577998226547244 ; top5 ->  85.49999758758545  and loss:  1281.5571758300066
forward train acc: top1 ->  60.041665134429934 ; top5 ->  81.5083307647705  and loss:  348.1491516828537
test acc: top1 ->  64.67799805908203 ; top5 ->  86.3459974609375  and loss:  1217.4576535075903
forward train acc: top1 ->  60.51666528701782 ; top5 ->  82.19999771118164  and loss:  341.08427143096924
test acc: top1 ->  65.24999806861878 ; top5 ->  86.61599762573242  and loss:  1203.4511587321758
forward train acc: top1 ->  60.91666522979736 ; top5 ->  82.10833084106446  and loss:  336.0761813521385
test acc: top1 ->  65.31199816017151 ; top5 ->  86.5319974029541  and loss:  1206.8829426318407
forward train acc: top1 ->  62.03333169937134 ; top5 ->  82.86666412353516  and loss:  326.018204331398
test acc: top1 ->  65.91599807815551 ; top5 ->  87.11999749145508  and loss:  1182.3584053367376
forward train acc: top1 ->  62.141665096282956 ; top5 ->  82.9333307647705  and loss:  324.61416643857956
test acc: top1 ->  66.31199804077148 ; top5 ->  87.28399741973877  and loss:  1164.1749422103167
forward train acc: top1 ->  62.1583317565918 ; top5 ->  82.62499794006348  and loss:  327.4415864944458
test acc: top1 ->  66.59399809646607 ; top5 ->  87.32999749145507  and loss:  1159.9417875036597
forward train acc: top1 ->  62.50833166122437 ; top5 ->  83.44999744415283  and loss:  322.5752328634262
test acc: top1 ->  66.84999797668458 ; top5 ->  87.54199735565186  and loss:  1151.456414066255
>>>>>>> reverse layer  27  since performance drop >>>>>>>
==> this epoch:  505 / 512 , inc:  1
---------------- start layer  28  ---------------
adv train loss:  -76.48235595226288 , diff:  76.48235595226288
adv train loss:  -81.19176137447357 , diff:  4.709405422210693
adv train loss:  -79.38838279247284 , diff:  1.8033785820007324
adv train loss:  -77.41726589202881 , diff:  1.9711169004440308
adv train loss:  -80.6716558933258 , diff:  3.254390001296997
adv train loss:  -79.2653443813324 , diff:  1.4063115119934082
adv train loss:  -79.11085522174835 , diff:  0.1544891595840454
layer  28  adv train finish, try to retain  501
test acc: top1 ->  66.51999812240601 ; top5 ->  87.41199747161865  and loss:  1163.895426698029
forward train acc: top1 ->  60.92499855041504 ; top5 ->  81.64166427612305  and loss:  338.74140298366547
test acc: top1 ->  63.725998122024535 ; top5 ->  85.64199750213623  and loss:  1282.17881539464
forward train acc: top1 ->  59.658331851959225 ; top5 ->  80.9916641998291  and loss:  350.87533164024353
test acc: top1 ->  63.93199809799194 ; top5 ->  85.8059974822998  and loss:  1261.079945936799
forward train acc: top1 ->  59.39999866485596 ; top5 ->  81.14166381835938  and loss:  352.32993590831757
test acc: top1 ->  63.53599815788269 ; top5 ->  85.45999745635986  and loss:  1280.9193179756403
forward train acc: top1 ->  60.00833185195923 ; top5 ->  81.82499759674073  and loss:  341.45183354616165
test acc: top1 ->  65.09199807739257 ; top5 ->  86.52199755401611  and loss:  1208.1892007142305
forward train acc: top1 ->  62.04166496276856 ; top5 ->  82.84999740600585  and loss:  327.5852209329605
test acc: top1 ->  65.36799813919068 ; top5 ->  86.75199760131837  and loss:  1207.5249319374561
forward train acc: top1 ->  61.22499856948853 ; top5 ->  82.24999752044678  and loss:  334.5732287764549
test acc: top1 ->  65.27999810180664 ; top5 ->  86.62599749908448  and loss:  1203.886769093573
forward train acc: top1 ->  61.3499986076355 ; top5 ->  82.89999736785889  and loss:  331.9402956366539
test acc: top1 ->  65.82199809188843 ; top5 ->  87.13999728240967  and loss:  1175.4427276551723
forward train acc: top1 ->  62.89999822616577 ; top5 ->  83.48333061218261  and loss:  319.3603397011757
test acc: top1 ->  66.32999813690185 ; top5 ->  87.37799760894775  and loss:  1163.7457815557718
forward train acc: top1 ->  63.18333162307739 ; top5 ->  83.9999974822998  and loss:  313.7890301346779
test acc: top1 ->  66.50199817810059 ; top5 ->  87.37199757537842  and loss:  1160.3437530770898
forward train acc: top1 ->  63.14166494369507 ; top5 ->  83.86666389465331  and loss:  310.47371929883957
test acc: top1 ->  66.76999809494019 ; top5 ->  87.54999750976563  and loss:  1151.3043392375112
>>>>>>> reverse layer  28  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  29  ---------------
adv train loss:  -78.20102155208588 , diff:  78.20102155208588
adv train loss:  -77.79501569271088 , diff:  0.406005859375
layer  29  adv train finish, try to retain  490
test acc: top1 ->  66.72799803161621 ; top5 ->  87.46199755706787  and loss:  1154.2106979340315
forward train acc: top1 ->  59.55833177566528 ; top5 ->  81.70833072662353  and loss:  346.131142616272
test acc: top1 ->  63.64799825019836 ; top5 ->  85.6839974746704  and loss:  1272.2868010252714
forward train acc: top1 ->  59.724998512268066 ; top5 ->  80.84999732971191  and loss:  351.78636157512665
test acc: top1 ->  63.389998187446594 ; top5 ->  85.44599747161865  and loss:  1285.8473016768694
forward train acc: top1 ->  58.97499858856201 ; top5 ->  80.96666427612304  and loss:  351.9587358236313
test acc: top1 ->  63.36999821166992 ; top5 ->  85.3959976196289  and loss:  1284.648828536272
forward train acc: top1 ->  60.499998550415036 ; top5 ->  82.05833087921143  and loss:  342.3636745810509
test acc: top1 ->  65.15799813156129 ; top5 ->  86.48799754486085  and loss:  1214.2475896850228
forward train acc: top1 ->  61.55833162307739 ; top5 ->  82.39999732971191  and loss:  332.8010312318802
test acc: top1 ->  65.25999814605713 ; top5 ->  86.62199739685059  and loss:  1205.6499577462673
forward train acc: top1 ->  61.358331756591795 ; top5 ->  82.11666416168212  and loss:  331.4365531206131
test acc: top1 ->  65.851998078537 ; top5 ->  86.76599745941162  and loss:  1192.2874242961407
forward train acc: top1 ->  62.43333162307739 ; top5 ->  82.79166408538818  and loss:  328.2810145020485
test acc: top1 ->  66.18599801864625 ; top5 ->  87.12799752655029  and loss:  1174.1953716129065
forward train acc: top1 ->  63.258331832885744 ; top5 ->  83.31666439056397  and loss:  319.0418300628662
test acc: top1 ->  66.47399799804687 ; top5 ->  87.29399753417968  and loss:  1162.5621882975101
forward train acc: top1 ->  62.74166521072388 ; top5 ->  83.44999740600586  and loss:  319.16776210069656
test acc: top1 ->  66.65399804992676 ; top5 ->  87.41399753265381  and loss:  1159.22426918149
forward train acc: top1 ->  62.449998474121095 ; top5 ->  83.84999744415283  and loss:  315.4695429801941
test acc: top1 ->  66.91599809188843 ; top5 ->  87.56999756011963  and loss:  1146.3028895109892
>>>>>>> reverse layer  29  since performance drop >>>>>>>
==> this epoch:  498 / 512 , inc:  1
---------------- start layer  30  ---------------
adv train loss:  -74.73211598396301 , diff:  74.73211598396301
adv train loss:  -76.94157218933105 , diff:  2.209456205368042
adv train loss:  -75.18854093551636 , diff:  1.7530312538146973
adv train loss:  -75.96028369665146 , diff:  0.7717427611351013
layer  30  adv train finish, try to retain  494
test acc: top1 ->  66.74999802703857 ; top5 ->  87.42199757080078  and loss:  1152.6160723268986
forward train acc: top1 ->  61.091665210723875 ; top5 ->  82.14999740600587  and loss:  335.896316409111
test acc: top1 ->  64.00999805583953 ; top5 ->  85.76999738922119  and loss:  1264.5687026381493
forward train acc: top1 ->  59.94999851226807 ; top5 ->  81.36666416168212  and loss:  351.1284716129303
test acc: top1 ->  63.13999808311463 ; top5 ->  85.23399763183593  and loss:  1297.127826794982
forward train acc: top1 ->  59.666665134429934 ; top5 ->  81.37499744415283  and loss:  350.1313898563385
test acc: top1 ->  63.48399811172485 ; top5 ->  85.36799758911133  and loss:  1274.0440912246704
forward train acc: top1 ->  61.45833168029785 ; top5 ->  82.17499732971191  and loss:  335.46694737672806
test acc: top1 ->  65.39599819564819 ; top5 ->  86.51799734497071  and loss:  1202.9328574687243
forward train acc: top1 ->  61.02499851226807 ; top5 ->  82.06666427612305  and loss:  337.5374073982239
test acc: top1 ->  65.44999815444946 ; top5 ->  86.633997555542  and loss:  1199.678485006094
forward train acc: top1 ->  61.758331813812255 ; top5 ->  82.15833044052124  and loss:  337.5585894584656
test acc: top1 ->  65.5759981765747 ; top5 ->  86.79199746246339  and loss:  1195.152170151472
forward train acc: top1 ->  61.8666650390625 ; top5 ->  82.74999759674073  and loss:  328.34917813539505
test acc: top1 ->  66.12599810638427 ; top5 ->  87.05599757995606  and loss:  1176.8170631080866
forward train acc: top1 ->  62.37499835968018 ; top5 ->  83.19999717712402  and loss:  319.0449917316437
test acc: top1 ->  66.4599980873108 ; top5 ->  87.239997605896  and loss:  1167.0843493938446
forward train acc: top1 ->  63.558331661224365 ; top5 ->  83.83333068847656  and loss:  312.3529121875763
test acc: top1 ->  66.48599803237914 ; top5 ->  87.32799757232667  and loss:  1160.0864534974098
forward train acc: top1 ->  62.20833154678345 ; top5 ->  83.14999740600587  and loss:  318.6939278244972
test acc: top1 ->  66.78999809646606 ; top5 ->  87.44799747619629  and loss:  1153.5241675376892
>>>>>>> reverse layer  30  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  31  ---------------
adv train loss:  -79.02249991893768 , diff:  79.02249991893768
adv train loss:  -83.19754362106323 , diff:  4.175043702125549
adv train loss:  -80.50164192914963 , diff:  2.6959016919136047
adv train loss:  -77.3629401922226 , diff:  3.1387017369270325
adv train loss:  -81.17128229141235 , diff:  3.8083420991897583
adv train loss:  -77.47935032844543 , diff:  3.691931962966919
adv train loss:  -77.96901428699493 , diff:  0.4896639585494995
layer  31  adv train finish, try to retain  502
>>>>>>> reverse layer  31  since no improvement >>>>>>>
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.01125, 0.01125, 0.0003955078125, 0.01125, 0.01125, 0.01125, 0.00052734375, 0.00052734375, 0.005625, 0.00052734375, 0.005625, 0.005625, 0.005625, 0.015, 0.0001318359375, 9.8876953125e-05, 0.000263671875, 0.00052734375, 0.000263671875, 0.0075, 0.000263671875, 0.000263671875, 0.0028125, 0.0028125, 0.000263671875, 0.000263671875, 4.94384765625e-05, 4.94384765625e-05, 4.94384765625e-05, 0.0001318359375, 4.94384765625e-05, 0.0001318359375]  wait [2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 4, 2, 3, 2, 0, 2, 2, 2, 2, 2, 2, 4, 4, 4, 2, 4, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 1
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  9  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -78.38722085952759 , diff:  78.38722085952759
adv train loss:  -80.70323741436005 , diff:  2.3160165548324585
adv train loss:  -77.87866163253784 , diff:  2.8245757818222046
adv train loss:  -77.86245119571686 , diff:  0.016210436820983887
layer  0  adv train finish, try to retain  48
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -83.52264451980591 , diff:  83.52264451980591
adv train loss:  -82.0971165895462 , diff:  1.4255279302597046
adv train loss:  -79.5654946565628 , diff:  2.5316219329833984
adv train loss:  -78.20918476581573 , diff:  1.3563098907470703
adv train loss:  -79.36938661336899 , diff:  1.1602018475532532
adv train loss:  -83.7101422548294 , diff:  4.340755641460419
adv train loss:  -81.23934817314148 , diff:  2.4707940816879272
adv train loss:  -83.47233068943024 , diff:  2.2329825162887573
adv train loss:  -83.74301791191101 , diff:  0.2706872224807739
layer  1  adv train finish, try to retain  48
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
adv train loss:  -76.56252896785736 , diff:  76.56252896785736
adv train loss:  -79.99621683359146 , diff:  3.4336878657341003
adv train loss:  -76.40473806858063 , diff:  3.5914787650108337
adv train loss:  -74.85305166244507 , diff:  1.551686406135559
adv train loss:  -81.07979917526245 , diff:  6.226747512817383
adv train loss:  -79.72853076457977 , diff:  1.3512684106826782
adv train loss:  -83.75959479808807 , diff:  4.031064033508301
adv train loss:  -76.88129329681396 , diff:  6.878301501274109
adv train loss:  -78.91560018062592 , diff:  2.0343068838119507
adv train loss:  -78.82867085933685 , diff:  0.0869293212890625
layer  3  adv train finish, try to retain  46
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -79.17440330982208 , diff:  79.17440330982208
adv train loss:  -81.56636142730713 , diff:  2.3919581174850464
adv train loss:  -82.7851939201355 , diff:  1.2188324928283691
adv train loss:  -82.17771190404892 , diff:  0.6074820160865784
layer  4  adv train finish, try to retain  38
test acc: top1 ->  64.7599982208252 ; top5 ->  86.34799754486085  and loss:  1227.031146749854
forward train acc: top1 ->  60.13333169937134 ; top5 ->  81.24166431427003  and loss:  349.9643571972847
test acc: top1 ->  63.05399820327759 ; top5 ->  85.03399741821289  and loss:  1299.0179865062237
forward train acc: top1 ->  58.99166521072388 ; top5 ->  80.56666423797607  and loss:  357.8660665154457
test acc: top1 ->  63.259998138046264 ; top5 ->  85.4159975189209  and loss:  1280.998231202364
forward train acc: top1 ->  59.1249986076355 ; top5 ->  80.79999740600586  and loss:  354.59334897994995
test acc: top1 ->  63.02599812545776 ; top5 ->  84.96999753723145  and loss:  1297.5771275758743
forward train acc: top1 ->  60.41666507720947 ; top5 ->  82.14166397094726  and loss:  338.0361889600754
test acc: top1 ->  65.20999803619385 ; top5 ->  86.40799748840332  and loss:  1213.7743903771043
forward train acc: top1 ->  60.791665134429934 ; top5 ->  82.08333080291749  and loss:  337.4524235725403
test acc: top1 ->  65.54799810791016 ; top5 ->  86.71799745483399  and loss:  1198.7425798773766
forward train acc: top1 ->  61.65833177566528 ; top5 ->  82.72499713897705  and loss:  331.76184916496277
test acc: top1 ->  65.79999816970825 ; top5 ->  86.82599761505126  and loss:  1193.3585444018245
forward train acc: top1 ->  62.14999824523926 ; top5 ->  82.39999732971191  and loss:  330.6535429954529
test acc: top1 ->  66.34799812927245 ; top5 ->  87.19599760131835  and loss:  1167.8056476414204
forward train acc: top1 ->  62.96666515350342 ; top5 ->  83.17499744415284  and loss:  321.7288276553154
test acc: top1 ->  66.61399808120727 ; top5 ->  87.29799753265381  and loss:  1157.9353179633617
forward train acc: top1 ->  63.38333168029785 ; top5 ->  83.29999744415284  and loss:  318.7901319861412
test acc: top1 ->  66.67599806213379 ; top5 ->  87.4219974761963  and loss:  1157.3809634894133
forward train acc: top1 ->  63.29999828338623 ; top5 ->  83.4416640472412  and loss:  318.0341674685478
test acc: top1 ->  66.94799798965454 ; top5 ->  87.55799750213623  and loss:  1145.1895507499576
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  40 / 64 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -80.53073012828827 , diff:  80.53073012828827
adv train loss:  -82.43466591835022 , diff:  1.9039357900619507
adv train loss:  -81.49884629249573 , diff:  0.9358196258544922
layer  5  adv train finish, try to retain  45
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -83.41810727119446 , diff:  83.41810727119446
adv train loss:  -77.85114026069641 , diff:  5.566967010498047
adv train loss:  -78.81226009130478 , diff:  0.9611198306083679
layer  6  adv train finish, try to retain  111
test acc: top1 ->  65.13399802856445 ; top5 ->  86.30199750366211  and loss:  1219.140933856368
forward train acc: top1 ->  60.41666524887085 ; top5 ->  81.57499740600586  and loss:  341.88014805316925
test acc: top1 ->  63.81999817352295 ; top5 ->  85.62999738769531  and loss:  1272.3393601179123
forward train acc: top1 ->  59.291665134429934 ; top5 ->  81.21666427612304  and loss:  349.3406639099121
test acc: top1 ->  63.55799811477661 ; top5 ->  85.52999749908447  and loss:  1266.3639150038362
forward train acc: top1 ->  59.849998455047604 ; top5 ->  81.9333307647705  and loss:  341.49592584371567
test acc: top1 ->  63.399998231506345 ; top5 ->  85.30599756622314  and loss:  1284.7209549099207
forward train acc: top1 ->  60.53333179473877 ; top5 ->  82.03333061218262  and loss:  339.95412772893906
test acc: top1 ->  65.30199811096192 ; top5 ->  86.38599747772217  and loss:  1212.5608585774899
forward train acc: top1 ->  60.96666513442993 ; top5 ->  82.73333065032959  and loss:  326.75484251976013
test acc: top1 ->  65.74599803695679 ; top5 ->  86.74799755554199  and loss:  1199.0820958465338
forward train acc: top1 ->  61.90833164215088 ; top5 ->  82.52499732971191  and loss:  329.70227551460266
test acc: top1 ->  65.60799804534912 ; top5 ->  86.69999750671387  and loss:  1203.643028974533
forward train acc: top1 ->  61.7416650390625 ; top5 ->  82.78333080291748  and loss:  330.06232208013535
test acc: top1 ->  66.34999806747436 ; top5 ->  87.26799754180908  and loss:  1168.875498175621
forward train acc: top1 ->  63.09166486740112 ; top5 ->  83.61666446685791  and loss:  315.60422337055206
test acc: top1 ->  66.6239979927063 ; top5 ->  87.49199744415283  and loss:  1158.3230157196522
forward train acc: top1 ->  63.41666496276856 ; top5 ->  83.62499752044678  and loss:  315.39058631658554
test acc: top1 ->  66.69799793243408 ; top5 ->  87.4899973449707  and loss:  1154.1620660722256
forward train acc: top1 ->  63.33333156585693 ; top5 ->  83.4416639328003  and loss:  317.15418845415115
test acc: top1 ->  66.98599806747437 ; top5 ->  87.61199741973877  and loss:  1147.6519362032413
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  112 / 128 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -81.0220775604248 , diff:  81.0220775604248
adv train loss:  -80.07260346412659 , diff:  0.9494740962982178
layer  7  adv train finish, try to retain  114
test acc: top1 ->  64.77799809532165 ; top5 ->  86.10399746398926  and loss:  1231.17424287647
forward train acc: top1 ->  60.3416650390625 ; top5 ->  81.51666423797607  and loss:  345.12685346603394
test acc: top1 ->  63.84799812927246 ; top5 ->  85.68799755249023  and loss:  1267.8390776216984
forward train acc: top1 ->  59.93333202362061 ; top5 ->  81.06666416168213  and loss:  351.5741910934448
test acc: top1 ->  62.92399822845459 ; top5 ->  85.24799752960205  and loss:  1292.5895444825292
forward train acc: top1 ->  59.49166521072388 ; top5 ->  81.11666439056397  and loss:  349.6284677386284
test acc: top1 ->  64.1259980884552 ; top5 ->  85.87999745025635  and loss:  1255.9761581420898
forward train acc: top1 ->  60.841665210723875 ; top5 ->  82.02499725341796  and loss:  341.9360865354538
test acc: top1 ->  65.32199820480346 ; top5 ->  86.70999749908447  and loss:  1201.493243843317
