DataParallel(
  (module): RESNET_BASIC_Mask(
    (net): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer3): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (4): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (5): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer4): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): BasicBlock(
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)
      (fc): Linear(in_features=512, out_features=1000, bias=True)
    )
    (mask): ParameterList(
        (0): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (1): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (2): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (3): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (4): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (5): Parameter containing: [torch.cuda.FloatTensor of size 64 (GPU 0)]
        (6): Parameter containing: [torch.cuda.FloatTensor of size 128 (GPU 0)]
        (7): Parameter containing: [torch.cuda.FloatTensor of size 128 (GPU 0)]
        (8): Parameter containing: [torch.cuda.FloatTensor of size 128 (GPU 0)]
        (9): Parameter containing: [torch.cuda.FloatTensor of size 128 (GPU 0)]
        (10): Parameter containing: [torch.cuda.FloatTensor of size 128 (GPU 0)]
        (11): Parameter containing: [torch.cuda.FloatTensor of size 128 (GPU 0)]
        (12): Parameter containing: [torch.cuda.FloatTensor of size 128 (GPU 0)]
        (13): Parameter containing: [torch.cuda.FloatTensor of size 128 (GPU 0)]
        (14): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
        (15): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
        (16): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
        (17): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
        (18): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
        (19): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
        (20): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
        (21): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
        (22): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
        (23): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
        (24): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
        (25): Parameter containing: [torch.cuda.FloatTensor of size 256 (GPU 0)]
        (26): Parameter containing: [torch.cuda.FloatTensor of size 512 (GPU 0)]
        (27): Parameter containing: [torch.cuda.FloatTensor of size 512 (GPU 0)]
        (28): Parameter containing: [torch.cuda.FloatTensor of size 512 (GPU 0)]
        (29): Parameter containing: [torch.cuda.FloatTensor of size 512 (GPU 0)]
        (30): Parameter containing: [torch.cuda.FloatTensor of size 512 (GPU 0)]
        (31): Parameter containing: [torch.cuda.FloatTensor of size 512 (GPU 0)]
      
    )
  )
)
eps:  [0.00015625, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 3.90625e-05, 3.90625e-05, 3.90625e-05, 3.90625e-05, 3.90625e-05, 3.90625e-05, 3.90625e-05, 3.90625e-05, 3.90625e-05, 3.90625e-05, 3.90625e-05, 3.90625e-05, 1.953125e-05, 1.953125e-05, 1.953125e-05, 1.953125e-05, 1.953125e-05, 1.953125e-05]
$$$$$$$$$$$$$ epoch  0  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -79.78435850143433 , diff:  79.78435850143433
adv train loss:  -81.50065982341766 , diff:  1.7163013219833374
layer  0  adv train finish, try to retain  62
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -79.0214433670044 , diff:  79.0214433670044
adv train loss:  -82.51041269302368 , diff:  3.488969326019287
layer  1  adv train finish, try to retain  62
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -76.07683086395264 , diff:  76.07683086395264
adv train loss:  -78.14628553390503 , diff:  2.0694546699523926
layer  2  adv train finish, try to retain  60
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -78.95428967475891 , diff:  78.95428967475891
adv train loss:  -77.209596991539 , diff:  1.7446926832199097
layer  3  adv train finish, try to retain  60
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -82.09058558940887 , diff:  82.09058558940887
adv train loss:  -79.57165402173996 , diff:  2.518931567668915
layer  4  adv train finish, try to retain  61
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -81.92296540737152 , diff:  81.92296540737152
adv train loss:  -81.75577509403229 , diff:  0.1671903133392334
layer  5  adv train finish, try to retain  62
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -78.91201132535934 , diff:  78.91201132535934
adv train loss:  -79.77043521404266 , diff:  0.8584238886833191
layer  6  adv train finish, try to retain  125
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -80.41565239429474 , diff:  80.41565239429474
adv train loss:  -79.81352019309998 , diff:  0.6021322011947632
layer  7  adv train finish, try to retain  126
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -76.0824978351593 , diff:  76.0824978351593
adv train loss:  -80.0482097864151 , diff:  3.9657119512557983
layer  8  adv train finish, try to retain  124
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -80.67849838733673 , diff:  80.67849838733673
adv train loss:  -81.95776951313019 , diff:  1.279271125793457
layer  9  adv train finish, try to retain  125
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -81.18854558467865 , diff:  81.18854558467865
adv train loss:  -79.01522493362427 , diff:  2.1733206510543823
layer  10  adv train finish, try to retain  127
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -77.82803809642792 , diff:  77.82803809642792
adv train loss:  -77.78468811511993 , diff:  0.0433499813079834
layer  11  adv train finish, try to retain  124
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -76.85645258426666 , diff:  76.85645258426666
adv train loss:  -79.55692052841187 , diff:  2.7004679441452026
layer  12  adv train finish, try to retain  127
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -79.94748401641846 , diff:  79.94748401641846
adv train loss:  -79.84793019294739 , diff:  0.09955382347106934
layer  13  adv train finish, try to retain  125
>>>>>>> reverse layer  13  since no improvement >>>>>>>
---------------- start layer  14  ---------------
adv train loss:  -77.31606936454773 , diff:  77.31606936454773
adv train loss:  -77.1290911436081 , diff:  0.18697822093963623
layer  14  adv train finish, try to retain  253
>>>>>>> reverse layer  14  since no improvement >>>>>>>
---------------- start layer  15  ---------------
adv train loss:  -79.49375236034393 , diff:  79.49375236034393
adv train loss:  -78.59521090984344 , diff:  0.8985414505004883
layer  15  adv train finish, try to retain  252
>>>>>>> reverse layer  15  since no improvement >>>>>>>
---------------- start layer  16  ---------------
adv train loss:  -81.05859613418579 , diff:  81.05859613418579
adv train loss:  -78.91683506965637 , diff:  2.141761064529419
layer  16  adv train finish, try to retain  252
>>>>>>> reverse layer  16  since no improvement >>>>>>>
---------------- start layer  17  ---------------
adv train loss:  -78.53987681865692 , diff:  78.53987681865692
adv train loss:  -80.11976265907288 , diff:  1.5798858404159546
layer  17  adv train finish, try to retain  251
>>>>>>> reverse layer  17  since no improvement >>>>>>>
---------------- start layer  18  ---------------
adv train loss:  -79.80448710918427 , diff:  79.80448710918427
adv train loss:  -80.00754868984222 , diff:  0.20306158065795898
layer  18  adv train finish, try to retain  251
>>>>>>> reverse layer  18  since no improvement >>>>>>>
---------------- start layer  19  ---------------
adv train loss:  -81.54109942913055 , diff:  81.54109942913055
adv train loss:  -80.72882622480392 , diff:  0.8122732043266296
layer  19  adv train finish, try to retain  254
>>>>>>> reverse layer  19  since no improvement >>>>>>>
---------------- start layer  20  ---------------
adv train loss:  -80.30689752101898 , diff:  80.30689752101898
adv train loss:  -78.11830389499664 , diff:  2.188593626022339
layer  20  adv train finish, try to retain  253
>>>>>>> reverse layer  20  since no improvement >>>>>>>
---------------- start layer  21  ---------------
adv train loss:  -79.13753926753998 , diff:  79.13753926753998
adv train loss:  -80.47367882728577 , diff:  1.3361395597457886
layer  21  adv train finish, try to retain  254
>>>>>>> reverse layer  21  since no improvement >>>>>>>
---------------- start layer  22  ---------------
adv train loss:  -76.40038418769836 , diff:  76.40038418769836
adv train loss:  -76.66028535366058 , diff:  0.25990116596221924
layer  22  adv train finish, try to retain  252
>>>>>>> reverse layer  22  since no improvement >>>>>>>
---------------- start layer  23  ---------------
adv train loss:  -80.19170898199081 , diff:  80.19170898199081
adv train loss:  -79.84432882070541 , diff:  0.3473801612854004
layer  23  adv train finish, try to retain  256
>>>>>>> reverse layer  23  since no improvement >>>>>>>
---------------- start layer  24  ---------------
adv train loss:  -80.22088944911957 , diff:  80.22088944911957
adv train loss:  -85.78216207027435 , diff:  5.561272621154785
layer  24  adv train finish, try to retain  254
>>>>>>> reverse layer  24  since no improvement >>>>>>>
---------------- start layer  25  ---------------
adv train loss:  -75.65321087837219 , diff:  75.65321087837219
adv train loss:  -76.83933126926422 , diff:  1.1861203908920288
layer  25  adv train finish, try to retain  253
>>>>>>> reverse layer  25  since no improvement >>>>>>>
---------------- start layer  26  ---------------
adv train loss:  -79.18112587928772 , diff:  79.18112587928772
adv train loss:  -79.42720091342926 , diff:  0.24607503414154053
layer  26  adv train finish, try to retain  506
>>>>>>> reverse layer  26  since no improvement >>>>>>>
---------------- start layer  27  ---------------
adv train loss:  -79.75143277645111 , diff:  79.75143277645111
adv train loss:  -77.1888039112091 , diff:  2.5626288652420044
layer  27  adv train finish, try to retain  507
>>>>>>> reverse layer  27  since no improvement >>>>>>>
---------------- start layer  28  ---------------
adv train loss:  -77.02310621738434 , diff:  77.02310621738434
adv train loss:  -79.36024117469788 , diff:  2.3371349573135376
layer  28  adv train finish, try to retain  510
>>>>>>> reverse layer  28  since no improvement >>>>>>>
---------------- start layer  29  ---------------
adv train loss:  -80.14163172245026 , diff:  80.14163172245026
adv train loss:  -81.3461743593216 , diff:  1.204542636871338
layer  29  adv train finish, try to retain  506
>>>>>>> reverse layer  29  since no improvement >>>>>>>
---------------- start layer  30  ---------------
adv train loss:  -79.88951551914215 , diff:  79.88951551914215
adv train loss:  -79.42439115047455 , diff:  0.46512436866760254
layer  30  adv train finish, try to retain  508
>>>>>>> reverse layer  30  since no improvement >>>>>>>
---------------- start layer  31  ---------------
adv train loss:  -79.63003480434418 , diff:  79.63003480434418
adv train loss:  -83.16298973560333 , diff:  3.5329549312591553
layer  31  adv train finish, try to retain  509
>>>>>>> reverse layer  31  since no improvement >>>>>>>
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.0003125, 0.0003125, 0.0003125, 0.0003125, 0.0003125, 0.0003125, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 3.90625e-05, 3.90625e-05, 3.90625e-05, 3.90625e-05, 3.90625e-05, 3.90625e-05]  wait [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 0
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  1  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -78.07289576530457 , diff:  78.07289576530457
adv train loss:  -79.15081679821014 , diff:  1.0779210329055786
layer  0  adv train finish, try to retain  59
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -81.41408252716064 , diff:  81.41408252716064
adv train loss:  -80.83910036087036 , diff:  0.5749821662902832
layer  1  adv train finish, try to retain  62
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -79.68913149833679 , diff:  79.68913149833679
adv train loss:  -77.4526731967926 , diff:  2.2364583015441895
layer  2  adv train finish, try to retain  57
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -80.31641554832458 , diff:  80.31641554832458
adv train loss:  -76.63973701000214 , diff:  3.6766785383224487
layer  3  adv train finish, try to retain  63
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -79.65811574459076 , diff:  79.65811574459076
adv train loss:  -81.66096782684326 , diff:  2.0028520822525024
layer  4  adv train finish, try to retain  61
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -79.7392646074295 , diff:  79.7392646074295
adv train loss:  -79.5698333978653 , diff:  0.16943120956420898
layer  5  adv train finish, try to retain  59
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -81.87069749832153 , diff:  81.87069749832153
adv train loss:  -81.3073501586914 , diff:  0.563347339630127
layer  6  adv train finish, try to retain  115
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -78.22654956579208 , diff:  78.22654956579208
adv train loss:  -78.82593202590942 , diff:  0.5993824601173401
layer  7  adv train finish, try to retain  125
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -82.38244414329529 , diff:  82.38244414329529
adv train loss:  -80.28889632225037 , diff:  2.093547821044922
layer  8  adv train finish, try to retain  125
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -80.61537009477615 , diff:  80.61537009477615
adv train loss:  -79.35505759716034 , diff:  1.2603124976158142
layer  9  adv train finish, try to retain  124
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -77.66993868350983 , diff:  77.66993868350983
adv train loss:  -76.2520581483841 , diff:  1.4178805351257324
layer  10  adv train finish, try to retain  122
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -79.6671347618103 , diff:  79.6671347618103
adv train loss:  -74.61038738489151 , diff:  5.056747376918793
layer  11  adv train finish, try to retain  123
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -77.60339319705963 , diff:  77.60339319705963
adv train loss:  -77.4570283293724 , diff:  0.14636486768722534
layer  12  adv train finish, try to retain  119
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -81.46567273139954 , diff:  81.46567273139954
adv train loss:  -83.15954148769379 , diff:  1.6938687562942505
layer  13  adv train finish, try to retain  125
>>>>>>> reverse layer  13  since no improvement >>>>>>>
---------------- start layer  14  ---------------
adv train loss:  -78.86999142169952 , diff:  78.86999142169952
adv train loss:  -78.76834136247635 , diff:  0.10165005922317505
layer  14  adv train finish, try to retain  247
test acc: top1 ->  66.032 ; top5 ->  86.938  and loss:  1104.567857325077
forward train acc: top1 ->  62.890625 ; top5 ->  83.421875  and loss:  320.0659331679344
test acc: top1 ->  66.85 ; top5 ->  87.392  and loss:  1078.1814742684364
forward train acc: top1 ->  63.3671875 ; top5 ->  83.4921875  and loss:  315.07754611968994
test acc: top1 ->  67.014 ; top5 ->  87.504  and loss:  1073.5256643295288
forward train acc: top1 ->  63.90625 ; top5 ->  83.703125  and loss:  306.8968600034714
test acc: top1 ->  67.028 ; top5 ->  87.668  and loss:  1070.0332135558128
forward train acc: top1 ->  63.734375 ; top5 ->  84.0078125  and loss:  307.71197932958603
test acc: top1 ->  67.06 ; top5 ->  87.726  and loss:  1066.6483498811722
forward train acc: top1 ->  63.6484375 ; top5 ->  83.8671875  and loss:  313.376482963562
test acc: top1 ->  67.126 ; top5 ->  87.756  and loss:  1064.012087494135
forward train acc: top1 ->  64.1015625 ; top5 ->  83.6875  and loss:  311.54565781354904
test acc: top1 ->  67.266 ; top5 ->  87.824  and loss:  1062.0326984226704
forward train acc: top1 ->  64.171875 ; top5 ->  84.203125  and loss:  306.25482028722763
test acc: top1 ->  67.252 ; top5 ->  87.81  and loss:  1061.0152530670166
forward train acc: top1 ->  62.796875 ; top5 ->  83.109375  and loss:  322.419176697731
test acc: top1 ->  67.23 ; top5 ->  87.812  and loss:  1058.7491320371628
forward train acc: top1 ->  63.390625 ; top5 ->  83.671875  and loss:  312.50249522924423
test acc: top1 ->  67.334 ; top5 ->  87.776  and loss:  1061.4251452088356
forward train acc: top1 ->  63.46875 ; top5 ->  83.859375  and loss:  314.252472281456
test acc: top1 ->  67.306 ; top5 ->  87.782  and loss:  1056.376543521881
>>>>>>> reverse layer  14  since performance drop >>>>>>>
==> this epoch:  250 / 256 , inc:  1
---------------- start layer  15  ---------------
adv train loss:  -75.46868145465851 , diff:  75.46868145465851
adv train loss:  -77.1623033285141 , diff:  1.6936218738555908
layer  15  adv train finish, try to retain  247
test acc: top1 ->  66.758 ; top5 ->  87.394  and loss:  1083.4440534710884
forward train acc: top1 ->  65.0390625 ; top5 ->  84.0546875  and loss:  304.85423386096954
test acc: top1 ->  67.174 ; top5 ->  87.768  and loss:  1065.3986462056637
forward train acc: top1 ->  63.6484375 ; top5 ->  83.984375  and loss:  311.0997247695923
test acc: top1 ->  67.324 ; top5 ->  87.728  and loss:  1060.8198989927769
forward train acc: top1 ->  64.1875 ; top5 ->  84.09375  and loss:  310.00635558366776
test acc: top1 ->  67.238 ; top5 ->  87.802  and loss:  1059.4078223705292
forward train acc: top1 ->  63.8203125 ; top5 ->  83.734375  and loss:  308.2511477470398
test acc: top1 ->  67.308 ; top5 ->  87.84  and loss:  1057.5192320644855
forward train acc: top1 ->  64.6953125 ; top5 ->  84.1328125  and loss:  311.4459174275398
test acc: top1 ->  67.424 ; top5 ->  87.874  and loss:  1056.5052654445171
forward train acc: top1 ->  63.96875 ; top5 ->  84.0078125  and loss:  311.1886305809021
test acc: top1 ->  67.522 ; top5 ->  87.898  and loss:  1053.986787378788
forward train acc: top1 ->  63.6484375 ; top5 ->  83.5234375  and loss:  311.720960855484
test acc: top1 ->  67.504 ; top5 ->  87.892  and loss:  1053.097863405943
forward train acc: top1 ->  64.6015625 ; top5 ->  84.3984375  and loss:  303.8956089615822
test acc: top1 ->  67.488 ; top5 ->  87.924  and loss:  1051.432531863451
forward train acc: top1 ->  64.78125 ; top5 ->  84.703125  and loss:  304.2576048374176
test acc: top1 ->  67.606 ; top5 ->  87.924  and loss:  1050.2844756245613
forward train acc: top1 ->  64.7421875 ; top5 ->  84.21875  and loss:  303.4576909542084
test acc: top1 ->  67.498 ; top5 ->  87.992  and loss:  1052.2931773364544
>>>>>>> reverse layer  15  since performance drop >>>>>>>
==> this epoch:  248 / 256 , inc:  1
---------------- start layer  16  ---------------
adv train loss:  -78.56700778007507 , diff:  78.56700778007507
adv train loss:  -78.3112844824791 , diff:  0.2557232975959778
layer  16  adv train finish, try to retain  251
>>>>>>> reverse layer  16  since no improvement >>>>>>>
---------------- start layer  17  ---------------
adv train loss:  -78.10162085294724 , diff:  78.10162085294724
adv train loss:  -76.33378684520721 , diff:  1.7678340077400208
layer  17  adv train finish, try to retain  251
>>>>>>> reverse layer  17  since no improvement >>>>>>>
---------------- start layer  18  ---------------
adv train loss:  -77.55583965778351 , diff:  77.55583965778351
adv train loss:  -76.35052889585495 , diff:  1.2053107619285583
layer  18  adv train finish, try to retain  253
>>>>>>> reverse layer  18  since no improvement >>>>>>>
---------------- start layer  19  ---------------
adv train loss:  -77.93316662311554 , diff:  77.93316662311554
adv train loss:  -77.53147631883621 , diff:  0.4016903042793274
layer  19  adv train finish, try to retain  249
>>>>>>> reverse layer  19  since no improvement >>>>>>>
---------------- start layer  20  ---------------
adv train loss:  -75.28620409965515 , diff:  75.28620409965515
adv train loss:  -74.37968051433563 , diff:  0.906523585319519
layer  20  adv train finish, try to retain  254
>>>>>>> reverse layer  20  since no improvement >>>>>>>
---------------- start layer  21  ---------------
adv train loss:  -74.39911091327667 , diff:  74.39911091327667
adv train loss:  -76.14118087291718 , diff:  1.742069959640503
layer  21  adv train finish, try to retain  250
>>>>>>> reverse layer  21  since no improvement >>>>>>>
---------------- start layer  22  ---------------
adv train loss:  -74.92384827136993 , diff:  74.92384827136993
adv train loss:  -77.36303663253784 , diff:  2.4391883611679077
layer  22  adv train finish, try to retain  251
>>>>>>> reverse layer  22  since no improvement >>>>>>>
---------------- start layer  23  ---------------
adv train loss:  -79.38813614845276 , diff:  79.38813614845276
adv train loss:  -74.45263653993607 , diff:  4.935499608516693
layer  23  adv train finish, try to retain  249
>>>>>>> reverse layer  23  since no improvement >>>>>>>
---------------- start layer  24  ---------------
adv train loss:  -76.35129487514496 , diff:  76.35129487514496
adv train loss:  -74.40506035089493 , diff:  1.9462345242500305
layer  24  adv train finish, try to retain  249
>>>>>>> reverse layer  24  since no improvement >>>>>>>
---------------- start layer  25  ---------------
adv train loss:  -79.12148946523666 , diff:  79.12148946523666
adv train loss:  -77.4600738286972 , diff:  1.6614156365394592
layer  25  adv train finish, try to retain  256
>>>>>>> reverse layer  25  since no improvement >>>>>>>
---------------- start layer  26  ---------------
adv train loss:  -78.11958909034729 , diff:  78.11958909034729
adv train loss:  -79.31892049312592 , diff:  1.1993314027786255
layer  26  adv train finish, try to retain  508
>>>>>>> reverse layer  26  since no improvement >>>>>>>
---------------- start layer  27  ---------------
adv train loss:  -75.97562736272812 , diff:  75.97562736272812
adv train loss:  -76.7305760383606 , diff:  0.7549486756324768
layer  27  adv train finish, try to retain  502
test acc: top1 ->  67.554 ; top5 ->  87.93  and loss:  1067.9996751248837
forward train acc: top1 ->  63.765625 ; top5 ->  83.7890625  and loss:  311.0559982061386
test acc: top1 ->  67.468 ; top5 ->  87.872  and loss:  1056.0491798520088
forward train acc: top1 ->  64.03125 ; top5 ->  84.0  and loss:  308.3152347803116
test acc: top1 ->  67.534 ; top5 ->  87.896  and loss:  1053.5223959386349
forward train acc: top1 ->  64.7578125 ; top5 ->  84.40625  and loss:  305.53656017780304
test acc: top1 ->  67.596 ; top5 ->  87.96  and loss:  1051.7425384819508
forward train acc: top1 ->  63.4453125 ; top5 ->  84.2890625  and loss:  306.73013854026794
test acc: top1 ->  67.646 ; top5 ->  88.05  and loss:  1052.2521924078465
forward train acc: top1 ->  64.4375 ; top5 ->  84.1953125  and loss:  299.9739512205124
test acc: top1 ->  67.696 ; top5 ->  88.046  and loss:  1050.5810389518738
forward train acc: top1 ->  64.078125 ; top5 ->  84.21875  and loss:  307.515612244606
test acc: top1 ->  67.636 ; top5 ->  88.032  and loss:  1050.2721200883389
forward train acc: top1 ->  63.84375 ; top5 ->  83.9375  and loss:  310.4537807703018
test acc: top1 ->  67.658 ; top5 ->  88.112  and loss:  1048.5137809216976
forward train acc: top1 ->  64.796875 ; top5 ->  84.21875  and loss:  303.1646130681038
test acc: top1 ->  67.734 ; top5 ->  88.116  and loss:  1047.347089201212
forward train acc: top1 ->  63.7421875 ; top5 ->  83.8515625  and loss:  312.6155631542206
test acc: top1 ->  67.75 ; top5 ->  88.144  and loss:  1045.6025227606297
forward train acc: top1 ->  64.3046875 ; top5 ->  84.453125  and loss:  303.25055277347565
test acc: top1 ->  67.724 ; top5 ->  88.1  and loss:  1049.266545176506
>>>>>>> reverse layer  27  since performance drop >>>>>>>
==> this epoch:  505 / 512 , inc:  1
---------------- start layer  28  ---------------
adv train loss:  -78.10687267780304 , diff:  78.10687267780304
adv train loss:  -76.72272574901581 , diff:  1.3841469287872314
layer  28  adv train finish, try to retain  505
>>>>>>> reverse layer  28  since no improvement >>>>>>>
---------------- start layer  29  ---------------
adv train loss:  -74.95023846626282 , diff:  74.95023846626282
adv train loss:  -72.83531558513641 , diff:  2.114922881126404
layer  29  adv train finish, try to retain  505
>>>>>>> reverse layer  29  since no improvement >>>>>>>
---------------- start layer  30  ---------------
adv train loss:  -78.06182813644409 , diff:  78.06182813644409
adv train loss:  -72.34887439012527 , diff:  5.712953746318817
layer  30  adv train finish, try to retain  504
>>>>>>> reverse layer  30  since no improvement >>>>>>>
---------------- start layer  31  ---------------
adv train loss:  -77.1482480764389 , diff:  77.1482480764389
adv train loss:  -81.8635665178299 , diff:  4.715318441390991
layer  31  adv train finish, try to retain  503
>>>>>>> reverse layer  31  since no improvement >>>>>>>
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.000625, 0.000625, 0.000625, 0.000625, 0.000625, 0.000625, 0.0003125, 0.0003125, 0.0003125, 0.0003125, 0.0003125, 0.0003125, 0.0003125, 0.0003125, 5.859375e-05, 5.859375e-05, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 0.00015625, 7.8125e-05, 2.9296875e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05, 7.8125e-05]  wait [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 0
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  2  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -75.92555147409439 , diff:  75.92555147409439
adv train loss:  -75.28248608112335 , diff:  0.6430653929710388
layer  0  adv train finish, try to retain  58
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -75.0787746310234 , diff:  75.0787746310234
adv train loss:  -77.5614298582077 , diff:  2.4826552271842957
layer  1  adv train finish, try to retain  61
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -75.01050817966461 , diff:  75.01050817966461
adv train loss:  -76.27382206916809 , diff:  1.263313889503479
layer  2  adv train finish, try to retain  58
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -75.85523700714111 , diff:  75.85523700714111
adv train loss:  -74.69573944807053 , diff:  1.1594975590705872
layer  3  adv train finish, try to retain  58
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -72.98617017269135 , diff:  72.98617017269135
adv train loss:  -75.21521830558777 , diff:  2.2290481328964233
layer  4  adv train finish, try to retain  48
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -79.523457467556 , diff:  79.523457467556
adv train loss:  -76.47107535600662 , diff:  3.0523821115493774
layer  5  adv train finish, try to retain  54
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -75.90313446521759 , diff:  75.90313446521759
adv train loss:  -73.98203009366989 , diff:  1.921104371547699
layer  6  adv train finish, try to retain  113
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -76.20657032728195 , diff:  76.20657032728195
adv train loss:  -75.56651246547699 , diff:  0.6400578618049622
layer  7  adv train finish, try to retain  120
test acc: top1 ->  67.44 ; top5 ->  87.904  and loss:  1055.172551393509
forward train acc: top1 ->  64.3828125 ; top5 ->  84.2421875  and loss:  306.61439603567123
test acc: top1 ->  67.706 ; top5 ->  88.026  and loss:  1047.7302950918674
forward train acc: top1 ->  64.1796875 ; top5 ->  84.234375  and loss:  308.99159401655197
test acc: top1 ->  67.73 ; top5 ->  88.1  and loss:  1047.8485711216927
forward train acc: top1 ->  64.3125 ; top5 ->  84.1640625  and loss:  306.4040286540985
test acc: top1 ->  67.666 ; top5 ->  88.112  and loss:  1048.180052280426
forward train acc: top1 ->  64.4453125 ; top5 ->  84.5234375  and loss:  301.31168407201767
test acc: top1 ->  67.754 ; top5 ->  88.162  and loss:  1043.2371246516705
forward train acc: top1 ->  64.8828125 ; top5 ->  84.1328125  and loss:  302.2828763127327
test acc: top1 ->  67.902 ; top5 ->  88.176  and loss:  1042.7277876138687
forward train acc: top1 ->  64.15625 ; top5 ->  84.03125  and loss:  307.4041047692299
test acc: top1 ->  67.816 ; top5 ->  88.164  and loss:  1042.0495333373547
forward train acc: top1 ->  63.8359375 ; top5 ->  84.2265625  and loss:  309.24219608306885
test acc: top1 ->  67.836 ; top5 ->  88.108  and loss:  1043.7470516264439
forward train acc: top1 ->  65.4296875 ; top5 ->  84.7265625  and loss:  297.35617858171463
test acc: top1 ->  67.858 ; top5 ->  88.224  and loss:  1041.7272332310677
forward train acc: top1 ->  64.796875 ; top5 ->  84.2734375  and loss:  303.1899462938309
test acc: top1 ->  67.888 ; top5 ->  88.196  and loss:  1041.3658410310745
forward train acc: top1 ->  64.6015625 ; top5 ->  84.3125  and loss:  304.9378220438957
test acc: top1 ->  67.978 ; top5 ->  88.166  and loss:  1041.7555403411388
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  122 / 128 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -73.3054267168045 , diff:  73.3054267168045
adv train loss:  -78.21565115451813 , diff:  4.910224437713623
layer  8  adv train finish, try to retain  121
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -72.8603019118309 , diff:  72.8603019118309
adv train loss:  -75.63349437713623 , diff:  2.7731924653053284
layer  9  adv train finish, try to retain  112
test acc: top1 ->  67.464 ; top5 ->  87.8  and loss:  1059.0256586670876
forward train acc: top1 ->  64.1484375 ; top5 ->  84.3515625  and loss:  305.4565438628197
test acc: top1 ->  67.78 ; top5 ->  88.106  and loss:  1043.1421651244164
forward train acc: top1 ->  64.7890625 ; top5 ->  84.5234375  and loss:  302.00418531894684
test acc: top1 ->  67.652 ; top5 ->  88.116  and loss:  1045.1482957601547
forward train acc: top1 ->  64.578125 ; top5 ->  83.65625  and loss:  311.8808982372284
test acc: top1 ->  67.73 ; top5 ->  88.104  and loss:  1044.5745926499367
forward train acc: top1 ->  64.7578125 ; top5 ->  84.328125  and loss:  302.88087022304535
test acc: top1 ->  67.786 ; top5 ->  88.136  and loss:  1041.7721580564976
forward train acc: top1 ->  64.1953125 ; top5 ->  84.3203125  and loss:  300.41982185840607
test acc: top1 ->  67.774 ; top5 ->  88.12  and loss:  1041.7892424166203
forward train acc: top1 ->  63.78125 ; top5 ->  84.2109375  and loss:  308.73579519987106
test acc: top1 ->  67.912 ; top5 ->  88.19  and loss:  1040.2414956092834
forward train acc: top1 ->  64.3046875 ; top5 ->  84.4296875  and loss:  304.258709192276
test acc: top1 ->  67.996 ; top5 ->  88.224  and loss:  1037.5675586163998
forward train acc: top1 ->  64.46875 ; top5 ->  83.8984375  and loss:  306.6313351392746
test acc: top1 ->  67.858 ; top5 ->  88.184  and loss:  1037.8471423983574
forward train acc: top1 ->  64.921875 ; top5 ->  84.4765625  and loss:  300.5202466249466
test acc: top1 ->  67.984 ; top5 ->  88.194  and loss:  1036.482448488474
forward train acc: top1 ->  64.359375 ; top5 ->  84.609375  and loss:  303.81559640169144
test acc: top1 ->  68.002 ; top5 ->  88.244  and loss:  1034.2548352479935
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  115 / 128 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -74.1570029258728 , diff:  74.1570029258728
adv train loss:  -73.7586841583252 , diff:  0.3983187675476074
layer  10  adv train finish, try to retain  122
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -73.04163253307343 , diff:  73.04163253307343
adv train loss:  -74.611641228199 , diff:  1.5700086951255798
layer  11  adv train finish, try to retain  120
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -76.05182987451553 , diff:  76.05182987451553
adv train loss:  -75.74437177181244 , diff:  0.3074581027030945
layer  12  adv train finish, try to retain  121
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -77.33647000789642 , diff:  77.33647000789642
adv train loss:  -76.64944684505463 , diff:  0.6870231628417969
layer  13  adv train finish, try to retain  121
>>>>>>> reverse layer  13  since no improvement >>>>>>>
---------------- start layer  14  ---------------
adv train loss:  -77.01463055610657 , diff:  77.01463055610657
adv train loss:  -74.88279789686203 , diff:  2.1318326592445374
layer  14  adv train finish, try to retain  253
>>>>>>> reverse layer  14  since no improvement >>>>>>>
---------------- start layer  15  ---------------
adv train loss:  -74.0570438504219 , diff:  74.0570438504219
adv train loss:  -70.77311319112778 , diff:  3.2839306592941284
layer  15  adv train finish, try to retain  253
>>>>>>> reverse layer  15  since no improvement >>>>>>>
---------------- start layer  16  ---------------
adv train loss:  -75.30306947231293 , diff:  75.30306947231293
adv train loss:  -73.2818968296051 , diff:  2.0211726427078247
layer  16  adv train finish, try to retain  242
>>>>>>> reverse layer  16  since no improvement >>>>>>>
---------------- start layer  17  ---------------
adv train loss:  -72.01294231414795 , diff:  72.01294231414795
adv train loss:  -74.30558949708939 , diff:  2.2926471829414368
layer  17  adv train finish, try to retain  249
>>>>>>> reverse layer  17  since no improvement >>>>>>>
---------------- start layer  18  ---------------
adv train loss:  -76.70724487304688 , diff:  76.70724487304688
adv train loss:  -78.63512301445007 , diff:  1.9278781414031982
layer  18  adv train finish, try to retain  246
>>>>>>> reverse layer  18  since no improvement >>>>>>>
---------------- start layer  19  ---------------
adv train loss:  -73.1617830991745 , diff:  73.1617830991745
adv train loss:  -72.48711156845093 , diff:  0.6746715307235718
layer  19  adv train finish, try to retain  242
>>>>>>> reverse layer  19  since no improvement >>>>>>>
---------------- start layer  20  ---------------
adv train loss:  -73.68837130069733 , diff:  73.68837130069733
adv train loss:  -78.43903708457947 , diff:  4.750665783882141
layer  20  adv train finish, try to retain  245
>>>>>>> reverse layer  20  since no improvement >>>>>>>
---------------- start layer  21  ---------------
adv train loss:  -70.90142512321472 , diff:  70.90142512321472
adv train loss:  -76.24672716856003 , diff:  5.345302045345306
layer  21  adv train finish, try to retain  243
>>>>>>> reverse layer  21  since no improvement >>>>>>>
---------------- start layer  22  ---------------
adv train loss:  -75.89908802509308 , diff:  75.89908802509308
adv train loss:  -75.802938580513 , diff:  0.09614944458007812
layer  22  adv train finish, try to retain  249
>>>>>>> reverse layer  22  since no improvement >>>>>>>
---------------- start layer  23  ---------------
adv train loss:  -76.74280852079391 , diff:  76.74280852079391
adv train loss:  -71.48450314998627 , diff:  5.258305370807648
layer  23  adv train finish, try to retain  241
>>>>>>> reverse layer  23  since no improvement >>>>>>>
---------------- start layer  24  ---------------
adv train loss:  -76.40602266788483 , diff:  76.40602266788483
adv train loss:  -76.13759958744049 , diff:  0.26842308044433594
layer  24  adv train finish, try to retain  244
>>>>>>> reverse layer  24  since no improvement >>>>>>>
---------------- start layer  25  ---------------
adv train loss:  -76.58958810567856 , diff:  76.58958810567856
adv train loss:  -77.1567177772522 , diff:  0.5671296715736389
layer  25  adv train finish, try to retain  243
>>>>>>> reverse layer  25  since no improvement >>>>>>>
---------------- start layer  26  ---------------
adv train loss:  -75.55215680599213 , diff:  75.55215680599213
adv train loss:  -73.6035772562027 , diff:  1.9485795497894287
layer  26  adv train finish, try to retain  500
test acc: top1 ->  67.858 ; top5 ->  88.084  and loss:  1040.2935713529587
forward train acc: top1 ->  64.3203125 ; top5 ->  84.4921875  and loss:  306.94724839925766
test acc: top1 ->  67.928 ; top5 ->  88.122  and loss:  1039.1573807001114
forward train acc: top1 ->  65.0234375 ; top5 ->  84.609375  and loss:  302.76689553260803
test acc: top1 ->  67.95 ; top5 ->  88.15  and loss:  1037.458609431982
forward train acc: top1 ->  64.34375 ; top5 ->  84.34375  and loss:  304.7735249400139
test acc: top1 ->  68.006 ; top5 ->  88.22  and loss:  1038.4013277888298
forward train acc: top1 ->  64.2734375 ; top5 ->  84.4609375  and loss:  305.4953481554985
test acc: top1 ->  67.958 ; top5 ->  88.24  and loss:  1036.0493097305298
forward train acc: top1 ->  64.328125 ; top5 ->  84.765625  and loss:  302.7782207131386
test acc: top1 ->  67.956 ; top5 ->  88.19  and loss:  1037.4468438327312
forward train acc: top1 ->  65.1015625 ; top5 ->  84.6953125  and loss:  299.92865431308746
test acc: top1 ->  68.014 ; top5 ->  88.326  and loss:  1032.423878699541
forward train acc: top1 ->  64.3671875 ; top5 ->  84.4921875  and loss:  303.4033177495003
test acc: top1 ->  68.154 ; top5 ->  88.248  and loss:  1033.3588701188564
forward train acc: top1 ->  65.0703125 ; top5 ->  84.796875  and loss:  296.2828289270401
test acc: top1 ->  68.096 ; top5 ->  88.242  and loss:  1033.3437898159027
forward train acc: top1 ->  65.6484375 ; top5 ->  84.9375  and loss:  294.90407621860504
test acc: top1 ->  68.054 ; top5 ->  88.286  and loss:  1029.6934738457203
forward train acc: top1 ->  65.15625 ; top5 ->  84.8515625  and loss:  297.6313065290451
test acc: top1 ->  68.086 ; top5 ->  88.306  and loss:  1030.9749878644943
>>>>>>> reverse layer  26  since performance drop >>>>>>>
==> this epoch:  502 / 512 , inc:  1
---------------- start layer  27  ---------------
adv train loss:  -75.19661265611649 , diff:  75.19661265611649
adv train loss:  -74.77556693553925 , diff:  0.42104572057724
layer  27  adv train finish, try to retain  509
>>>>>>> reverse layer  27  since no improvement >>>>>>>
---------------- start layer  28  ---------------
adv train loss:  -72.6858417391777 , diff:  72.6858417391777
adv train loss:  -76.0535808801651 , diff:  3.3677391409873962
layer  28  adv train finish, try to retain  501
test acc: top1 ->  68.112 ; top5 ->  88.306  and loss:  1033.501473814249
forward train acc: top1 ->  64.9375 ; top5 ->  84.515625  and loss:  303.39424073696136
test acc: top1 ->  68.038 ; top5 ->  88.246  and loss:  1034.0893267989159
forward train acc: top1 ->  64.703125 ; top5 ->  84.2578125  and loss:  302.63584810495377
test acc: top1 ->  68.156 ; top5 ->  88.296  and loss:  1031.7585846483707
forward train acc: top1 ->  65.6796875 ; top5 ->  84.78125  and loss:  296.2833629846573
test acc: top1 ->  68.244 ; top5 ->  88.328  and loss:  1032.0773048102856
forward train acc: top1 ->  65.71875 ; top5 ->  85.09375  and loss:  295.89363008737564
test acc: top1 ->  68.192 ; top5 ->  88.322  and loss:  1031.2304140627384
forward train acc: top1 ->  65.015625 ; top5 ->  84.796875  and loss:  299.1942030787468
test acc: top1 ->  68.218 ; top5 ->  88.372  and loss:  1031.6696361601353
forward train acc: top1 ->  65.1875 ; top5 ->  84.53125  and loss:  299.70783323049545
test acc: top1 ->  68.216 ; top5 ->  88.414  and loss:  1029.3615654110909
forward train acc: top1 ->  65.2578125 ; top5 ->  84.7578125  and loss:  298.7366092801094
test acc: top1 ->  68.22 ; top5 ->  88.42  and loss:  1030.8233614861965
forward train acc: top1 ->  65.03125 ; top5 ->  84.8671875  and loss:  298.0415287017822
test acc: top1 ->  68.094 ; top5 ->  88.404  and loss:  1032.048939526081
forward train acc: top1 ->  65.7109375 ; top5 ->  85.0703125  and loss:  294.9167508482933
test acc: top1 ->  68.29 ; top5 ->  88.468  and loss:  1025.6041667461395
forward train acc: top1 ->  64.734375 ; top5 ->  84.4921875  and loss:  300.9710711836815
test acc: top1 ->  68.25 ; top5 ->  88.44  and loss:  1027.9660976529121
>>>>>>> reverse layer  28  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  29  ---------------
adv train loss:  -75.48823714256287 , diff:  75.48823714256287
adv train loss:  -75.27465409040451 , diff:  0.2135830521583557
layer  29  adv train finish, try to retain  503
>>>>>>> reverse layer  29  since no improvement >>>>>>>
---------------- start layer  30  ---------------
adv train loss:  -74.32439643144608 , diff:  74.32439643144608
adv train loss:  -75.91503477096558 , diff:  1.5906383395195007
layer  30  adv train finish, try to retain  498
test acc: top1 ->  68.064 ; top5 ->  88.336  and loss:  1033.4993342757225
forward train acc: top1 ->  64.6328125 ; top5 ->  84.640625  and loss:  304.300465464592
test acc: top1 ->  67.996 ; top5 ->  88.326  and loss:  1032.0890178382397
forward train acc: top1 ->  64.8984375 ; top5 ->  84.734375  and loss:  301.7255799770355
test acc: top1 ->  68.18 ; top5 ->  88.292  and loss:  1029.256802290678
forward train acc: top1 ->  65.0859375 ; top5 ->  84.671875  and loss:  299.8204982280731
test acc: top1 ->  68.088 ; top5 ->  88.236  and loss:  1034.2144647836685
forward train acc: top1 ->  64.9921875 ; top5 ->  84.578125  and loss:  299.83436089754105
test acc: top1 ->  68.232 ; top5 ->  88.278  and loss:  1030.627775400877
forward train acc: top1 ->  64.8046875 ; top5 ->  84.8046875  and loss:  298.38695043325424
test acc: top1 ->  68.16 ; top5 ->  88.33  and loss:  1030.331424742937
forward train acc: top1 ->  64.7734375 ; top5 ->  84.6171875  and loss:  298.8197423815727
test acc: top1 ->  68.278 ; top5 ->  88.386  and loss:  1028.3919953107834
forward train acc: top1 ->  65.546875 ; top5 ->  84.9375  and loss:  294.5659428834915
test acc: top1 ->  68.374 ; top5 ->  88.418  and loss:  1026.3160098195076
forward train acc: top1 ->  64.7578125 ; top5 ->  84.4453125  and loss:  301.5262091755867
test acc: top1 ->  68.31 ; top5 ->  88.45  and loss:  1026.0589970350266
forward train acc: top1 ->  64.125 ; top5 ->  84.359375  and loss:  304.2778823375702
test acc: top1 ->  68.332 ; top5 ->  88.406  and loss:  1025.2478653490543
forward train acc: top1 ->  65.0546875 ; top5 ->  84.5625  and loss:  300.319881439209
test acc: top1 ->  68.452 ; top5 ->  88.456  and loss:  1024.105749487877
>>>>>>> reverse layer  30  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  31  ---------------
adv train loss:  -75.5745016336441 , diff:  75.5745016336441
adv train loss:  -74.21764862537384 , diff:  1.3568530082702637
layer  31  adv train finish, try to retain  502
>>>>>>> reverse layer  31  since no improvement >>>>>>>
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.00125, 0.00125, 0.00125, 0.00125, 0.00125, 0.00125, 0.000625, 0.000234375, 0.000625, 0.000234375, 0.000625, 0.000625, 0.000625, 0.000625, 0.0001171875, 0.0001171875, 0.0003125, 0.0003125, 0.0003125, 0.0003125, 0.0003125, 0.0003125, 0.0003125, 0.0003125, 0.0003125, 0.0003125, 5.859375e-05, 5.859375e-05, 5.859375e-05, 0.00015625, 5.859375e-05, 0.00015625]  wait [0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 2, 0]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 0
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  3  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -75.11277824640274 , diff:  75.11277824640274
adv train loss:  -72.84437757730484 , diff:  2.2684006690979004
layer  0  adv train finish, try to retain  55
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -73.26777750253677 , diff:  73.26777750253677
adv train loss:  -74.15922516584396 , diff:  0.8914476633071899
layer  1  adv train finish, try to retain  55
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -76.28649592399597 , diff:  76.28649592399597
adv train loss:  -73.18487256765366 , diff:  3.1016233563423157
layer  2  adv train finish, try to retain  48
test acc: top1 ->  66.13 ; top5 ->  86.802  and loss:  1106.1071768701077
forward train acc: top1 ->  64.25 ; top5 ->  83.8984375  and loss:  306.8243598341942
test acc: top1 ->  67.972 ; top5 ->  88.124  and loss:  1039.4185763001442
forward train acc: top1 ->  64.90625 ; top5 ->  85.03125  and loss:  294.80930000543594
test acc: top1 ->  68.04 ; top5 ->  88.082  and loss:  1038.2232967317104
forward train acc: top1 ->  64.484375 ; top5 ->  84.15625  and loss:  307.9344662427902
test acc: top1 ->  68.114 ; top5 ->  88.194  and loss:  1034.370010137558
forward train acc: top1 ->  65.2890625 ; top5 ->  84.984375  and loss:  297.04393845796585
test acc: top1 ->  68.08 ; top5 ->  88.19  and loss:  1034.3134253323078
forward train acc: top1 ->  65.234375 ; top5 ->  84.8515625  and loss:  298.1854901313782
test acc: top1 ->  68.18 ; top5 ->  88.242  and loss:  1032.9573423564434
forward train acc: top1 ->  64.765625 ; top5 ->  84.3515625  and loss:  302.5164967775345
test acc: top1 ->  68.232 ; top5 ->  88.26  and loss:  1029.599209189415
forward train acc: top1 ->  65.4375 ; top5 ->  84.5390625  and loss:  299.65717178583145
test acc: top1 ->  68.37 ; top5 ->  88.266  and loss:  1029.3468832671642
forward train acc: top1 ->  65.5703125 ; top5 ->  84.8984375  and loss:  293.46838515996933
test acc: top1 ->  68.184 ; top5 ->  88.246  and loss:  1029.8420510590076
forward train acc: top1 ->  65.2109375 ; top5 ->  84.5703125  and loss:  303.3559000492096
test acc: top1 ->  68.298 ; top5 ->  88.304  and loss:  1027.8080593645573
forward train acc: top1 ->  65.0546875 ; top5 ->  84.4453125  and loss:  300.6757460832596
test acc: top1 ->  68.312 ; top5 ->  88.356  and loss:  1027.9665513634682
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  57 / 64 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -76.55568420886993 , diff:  76.55568420886993
adv train loss:  -75.84910988807678 , diff:  0.7065743207931519
layer  3  adv train finish, try to retain  57
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -71.22163665294647 , diff:  71.22163665294647
adv train loss:  -72.98131966590881 , diff:  1.7596830129623413
layer  4  adv train finish, try to retain  55
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -74.59500753879547 , diff:  74.59500753879547
adv train loss:  -75.77657490968704 , diff:  1.181567370891571
layer  5  adv train finish, try to retain  54
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -74.26096224784851 , diff:  74.26096224784851
adv train loss:  -76.02857917547226 , diff:  1.7676169276237488
layer  6  adv train finish, try to retain  120
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -75.51517969369888 , diff:  75.51517969369888
adv train loss:  -72.67681258916855 , diff:  2.8383671045303345
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  121
test acc: top1 ->  68.156 ; top5 ->  88.212  and loss:  1031.5825451016426
forward train acc: top1 ->  65.125 ; top5 ->  84.734375  and loss:  297.31189036369324
test acc: top1 ->  68.418 ; top5 ->  88.446  and loss:  1022.5573901832104
forward train acc: top1 ->  65.1953125 ; top5 ->  84.515625  and loss:  298.90635246038437
test acc: top1 ->  68.446 ; top5 ->  88.514  and loss:  1023.1317604184151
forward train acc: top1 ->  65.203125 ; top5 ->  84.8828125  and loss:  297.93301606178284
test acc: top1 ->  68.264 ; top5 ->  88.482  and loss:  1026.233114540577
forward train acc: top1 ->  66.0859375 ; top5 ->  85.5  and loss:  288.42237704992294
test acc: top1 ->  68.33 ; top5 ->  88.578  and loss:  1023.0183913707733
forward train acc: top1 ->  64.9296875 ; top5 ->  84.625  and loss:  299.406331717968
test acc: top1 ->  68.452 ; top5 ->  88.48  and loss:  1024.3938803374767
forward train acc: top1 ->  65.65625 ; top5 ->  85.1953125  and loss:  291.1549597978592
test acc: top1 ->  68.456 ; top5 ->  88.496  and loss:  1019.8911661803722
forward train acc: top1 ->  65.4375 ; top5 ->  85.1328125  and loss:  296.71178168058395
test acc: top1 ->  68.474 ; top5 ->  88.482  and loss:  1021.947771936655
forward train acc: top1 ->  66.3515625 ; top5 ->  85.328125  and loss:  288.3670128583908
test acc: top1 ->  68.53 ; top5 ->  88.532  and loss:  1019.9944033324718
forward train acc: top1 ->  65.984375 ; top5 ->  85.2421875  and loss:  290.9154716730118
test acc: top1 ->  68.652 ; top5 ->  88.548  and loss:  1019.3372338712215
forward train acc: top1 ->  66.0546875 ; top5 ->  85.15625  and loss:  289.6397941708565
test acc: top1 ->  68.544 ; top5 ->  88.512  and loss:  1017.3554086387157
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  122 / 128 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -72.87360733747482 , diff:  72.87360733747482
adv train loss:  -73.62014156579971 , diff:  0.7465342283248901
layer  8  adv train finish, try to retain  108
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -73.44741189479828 , diff:  73.44741189479828
adv train loss:  -70.63475847244263 , diff:  2.812653422355652
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  114
test acc: top1 ->  68.274 ; top5 ->  88.378  and loss:  1029.4557507932186
forward train acc: top1 ->  64.9453125 ; top5 ->  84.4296875  and loss:  300.05581468343735
test acc: top1 ->  68.324 ; top5 ->  88.444  and loss:  1024.9857743382454
forward train acc: top1 ->  65.046875 ; top5 ->  85.078125  and loss:  295.8170645236969
test acc: top1 ->  68.326 ; top5 ->  88.506  and loss:  1025.241195321083
forward train acc: top1 ->  65.3984375 ; top5 ->  84.75  and loss:  298.2731668353081
test acc: top1 ->  68.378 ; top5 ->  88.47  and loss:  1022.54477378726
forward train acc: top1 ->  66.1875 ; top5 ->  84.7265625  and loss:  293.10212391614914
test acc: top1 ->  68.426 ; top5 ->  88.55  and loss:  1020.2331322431564
forward train acc: top1 ->  65.3671875 ; top5 ->  84.3984375  and loss:  296.3185341954231
test acc: top1 ->  68.462 ; top5 ->  88.5  and loss:  1020.6866812109947
forward train acc: top1 ->  65.8671875 ; top5 ->  85.2265625  and loss:  289.6297189593315
test acc: top1 ->  68.45 ; top5 ->  88.534  and loss:  1020.6764553189278
forward train acc: top1 ->  65.6796875 ; top5 ->  84.6875  and loss:  295.10279804468155
test acc: top1 ->  68.48 ; top5 ->  88.558  and loss:  1018.5292368233204
forward train acc: top1 ->  65.2265625 ; top5 ->  84.9296875  and loss:  296.81162947416306
test acc: top1 ->  68.506 ; top5 ->  88.566  and loss:  1022.1524587869644
forward train acc: top1 ->  65.8125 ; top5 ->  85.5234375  and loss:  291.01859098672867
test acc: top1 ->  68.484 ; top5 ->  88.552  and loss:  1019.9015261530876
forward train acc: top1 ->  65.4921875 ; top5 ->  84.671875  and loss:  295.2662926912308
test acc: top1 ->  68.648 ; top5 ->  88.554  and loss:  1017.5697586536407
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  115 / 128 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -75.79333114624023 , diff:  75.79333114624023
adv train loss:  -73.50870221853256 , diff:  2.284628927707672
layer  10  adv train finish, try to retain  107
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -74.84333544969559 , diff:  74.84333544969559
adv train loss:  -73.91781377792358 , diff:  0.9255216717720032
layer  11  adv train finish, try to retain  113
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -75.03197240829468 , diff:  75.03197240829468
adv train loss:  -72.50784075260162 , diff:  2.524131655693054
layer  12  adv train finish, try to retain  106
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -77.47839272022247 , diff:  77.47839272022247
adv train loss:  -75.98053646087646 , diff:  1.4978562593460083
layer  13  adv train finish, try to retain  107
>>>>>>> reverse layer  13  since no improvement >>>>>>>
---------------- start layer  14  ---------------
adv train loss:  -74.46766370534897 , diff:  74.46766370534897
adv train loss:  -71.07679134607315 , diff:  3.390872359275818
************ all values are small in this layer **********
layer  14  adv train finish, try to retain  249
test acc: top1 ->  68.304 ; top5 ->  88.48  and loss:  1023.7630841732025
forward train acc: top1 ->  64.796875 ; top5 ->  85.109375  and loss:  297.13937097787857
test acc: top1 ->  68.498 ; top5 ->  88.54  and loss:  1020.0409542918205
forward train acc: top1 ->  65.578125 ; top5 ->  85.140625  and loss:  293.0135816335678
test acc: top1 ->  68.426 ; top5 ->  88.508  and loss:  1018.2662200927734
forward train acc: top1 ->  66.4453125 ; top5 ->  85.7734375  and loss:  286.74162805080414
test acc: top1 ->  68.374 ; top5 ->  88.488  and loss:  1020.7149100899696
forward train acc: top1 ->  65.4140625 ; top5 ->  84.4609375  and loss:  298.11442071199417
test acc: top1 ->  68.52 ; top5 ->  88.574  and loss:  1018.6691672205925
forward train acc: top1 ->  65.984375 ; top5 ->  84.9765625  and loss:  291.80836391448975
test acc: top1 ->  68.584 ; top5 ->  88.512  and loss:  1019.4736819565296
forward train acc: top1 ->  65.15625 ; top5 ->  84.6875  and loss:  295.73882699012756
test acc: top1 ->  68.64 ; top5 ->  88.564  and loss:  1016.7960741817951
forward train acc: top1 ->  65.671875 ; top5 ->  85.0546875  and loss:  294.3094913959503
test acc: top1 ->  68.642 ; top5 ->  88.536  and loss:  1016.1436876654625
forward train acc: top1 ->  66.234375 ; top5 ->  85.34375  and loss:  291.26537251472473
test acc: top1 ->  68.776 ; top5 ->  88.58  and loss:  1015.5930961072445
forward train acc: top1 ->  66.1015625 ; top5 ->  85.4140625  and loss:  290.11253798007965
test acc: top1 ->  68.76 ; top5 ->  88.64  and loss:  1015.1293947696686
forward train acc: top1 ->  65.6953125 ; top5 ->  85.0859375  and loss:  294.1120536327362
test acc: top1 ->  68.698 ; top5 ->  88.574  and loss:  1016.6753616929054
>>>>>>> reverse layer  14  since performance drop >>>>>>>
==> this epoch:  250 / 256 , inc:  1
---------------- start layer  15  ---------------
adv train loss:  -70.80264222621918 , diff:  70.80264222621918
adv train loss:  -72.59038752317429 , diff:  1.7877452969551086
************ all values are small in this layer **********
layer  15  adv train finish, try to retain  247
test acc: top1 ->  68.32 ; top5 ->  88.35  and loss:  1029.7657676637173
forward train acc: top1 ->  64.625 ; top5 ->  84.609375  and loss:  301.0730844736099
test acc: top1 ->  68.384 ; top5 ->  88.378  and loss:  1024.0053592026234
forward train acc: top1 ->  64.9375 ; top5 ->  84.53125  and loss:  299.4755807518959
test acc: top1 ->  68.43 ; top5 ->  88.456  and loss:  1018.6757660806179
forward train acc: top1 ->  65.3515625 ; top5 ->  84.8828125  and loss:  296.1487680673599
test acc: top1 ->  68.566 ; top5 ->  88.416  and loss:  1019.0417442917824
forward train acc: top1 ->  65.7578125 ; top5 ->  85.234375  and loss:  293.3262633085251
test acc: top1 ->  68.478 ; top5 ->  88.436  and loss:  1020.0838167071342
forward train acc: top1 ->  64.9453125 ; top5 ->  84.8984375  and loss:  297.38316839933395
test acc: top1 ->  68.572 ; top5 ->  88.414  and loss:  1015.8742895126343
forward train acc: top1 ->  66.015625 ; top5 ->  85.0234375  and loss:  292.3179125189781
test acc: top1 ->  68.612 ; top5 ->  88.46  and loss:  1018.2508752346039
forward train acc: top1 ->  65.359375 ; top5 ->  84.984375  and loss:  293.7953763604164
test acc: top1 ->  68.582 ; top5 ->  88.452  and loss:  1017.1152585744858
forward train acc: top1 ->  65.90625 ; top5 ->  85.515625  and loss:  289.60381257534027
test acc: top1 ->  68.718 ; top5 ->  88.534  and loss:  1015.00402328372
forward train acc: top1 ->  65.3046875 ; top5 ->  85.140625  and loss:  295.89231610298157
test acc: top1 ->  68.654 ; top5 ->  88.53  and loss:  1014.1323291659355
forward train acc: top1 ->  65.09375 ; top5 ->  84.6484375  and loss:  299.5461666584015
test acc: top1 ->  68.63 ; top5 ->  88.566  and loss:  1014.5155331790447
>>>>>>> reverse layer  15  since performance drop >>>>>>>
==> this epoch:  248 / 256 , inc:  1
---------------- start layer  16  ---------------
adv train loss:  -73.24814081192017 , diff:  73.24814081192017
adv train loss:  -73.53501433134079 , diff:  0.2868735194206238
layer  16  adv train finish, try to retain  231
test acc: top1 ->  68.4 ; top5 ->  88.584  and loss:  1021.4074642956257
forward train acc: top1 ->  65.4609375 ; top5 ->  85.1484375  and loss:  293.56222170591354
test acc: top1 ->  68.48 ; top5 ->  88.598  and loss:  1020.8133528232574
forward train acc: top1 ->  65.421875 ; top5 ->  85.3046875  and loss:  291.04237228631973
test acc: top1 ->  68.394 ; top5 ->  88.618  and loss:  1021.1012616157532
forward train acc: top1 ->  65.1953125 ; top5 ->  85.296875  and loss:  295.90031123161316
test acc: top1 ->  68.394 ; top5 ->  88.53  and loss:  1019.7382634580135
forward train acc: top1 ->  65.5390625 ; top5 ->  84.8359375  and loss:  294.0834929943085
test acc: top1 ->  68.496 ; top5 ->  88.598  and loss:  1020.4918295741081
forward train acc: top1 ->  66.0859375 ; top5 ->  84.9296875  and loss:  294.1399781703949
test acc: top1 ->  68.592 ; top5 ->  88.552  and loss:  1016.9040472507477
forward train acc: top1 ->  66.0859375 ; top5 ->  85.25  and loss:  290.22073739767075
test acc: top1 ->  68.574 ; top5 ->  88.616  and loss:  1016.3557567596436
forward train acc: top1 ->  65.625 ; top5 ->  84.7734375  and loss:  295.48237788677216
test acc: top1 ->  68.624 ; top5 ->  88.568  and loss:  1017.1413397192955
forward train acc: top1 ->  65.6328125 ; top5 ->  85.4140625  and loss:  290.96834522485733
test acc: top1 ->  68.63 ; top5 ->  88.538  and loss:  1014.7884728610516
forward train acc: top1 ->  65.2421875 ; top5 ->  84.6875  and loss:  296.90391141176224
test acc: top1 ->  68.552 ; top5 ->  88.592  and loss:  1016.4760297238827
forward train acc: top1 ->  65.953125 ; top5 ->  84.5625  and loss:  291.3021749854088
test acc: top1 ->  68.558 ; top5 ->  88.66  and loss:  1014.7599059343338
>>>>>>> reverse layer  16  since performance drop >>>>>>>
==> this epoch:  239 / 256 , inc:  1
---------------- start layer  17  ---------------
adv train loss:  -73.43435102701187 , diff:  73.43435102701187
adv train loss:  -69.24220806360245 , diff:  4.192142963409424
layer  17  adv train finish, try to retain  245
>>>>>>> reverse layer  17  since no improvement >>>>>>>
---------------- start layer  18  ---------------
adv train loss:  -73.65171974897385 , diff:  73.65171974897385
adv train loss:  -76.68212699890137 , diff:  3.0304072499275208
layer  18  adv train finish, try to retain  239
>>>>>>> reverse layer  18  since no improvement >>>>>>>
---------------- start layer  19  ---------------
adv train loss:  -71.22684758901596 , diff:  71.22684758901596
adv train loss:  -70.27932399511337 , diff:  0.9475235939025879
layer  19  adv train finish, try to retain  241
>>>>>>> reverse layer  19  since no improvement >>>>>>>
---------------- start layer  20  ---------------
adv train loss:  -74.96323239803314 , diff:  74.96323239803314
adv train loss:  -69.6421000957489 , diff:  5.321132302284241
layer  20  adv train finish, try to retain  234
>>>>>>> reverse layer  20  since no improvement >>>>>>>
---------------- start layer  21  ---------------
adv train loss:  -71.00292754173279 , diff:  71.00292754173279
adv train loss:  -73.48089802265167 , diff:  2.4779704809188843
layer  21  adv train finish, try to retain  237
test acc: top1 ->  68.4 ; top5 ->  88.514  and loss:  1019.1725508868694
forward train acc: top1 ->  65.3515625 ; top5 ->  84.7421875  and loss:  297.05607867240906
test acc: top1 ->  68.414 ; top5 ->  88.504  and loss:  1021.0196290016174
forward train acc: top1 ->  66.109375 ; top5 ->  85.203125  and loss:  293.8982875943184
test acc: top1 ->  68.598 ; top5 ->  88.554  and loss:  1018.1360458433628
forward train acc: top1 ->  66.203125 ; top5 ->  85.15625  and loss:  290.87677466869354
test acc: top1 ->  68.626 ; top5 ->  88.544  and loss:  1017.925114095211
forward train acc: top1 ->  65.375 ; top5 ->  85.2265625  and loss:  294.9630205631256
test acc: top1 ->  68.658 ; top5 ->  88.562  and loss:  1017.6633702218533
forward train acc: top1 ->  65.5390625 ; top5 ->  84.8671875  and loss:  294.2019744515419
test acc: top1 ->  68.74 ; top5 ->  88.572  and loss:  1016.9615546762943
forward train acc: top1 ->  65.8515625 ; top5 ->  85.34375  and loss:  291.6407864689827
test acc: top1 ->  68.712 ; top5 ->  88.602  and loss:  1016.8795795440674
forward train acc: top1 ->  65.15625 ; top5 ->  84.7890625  and loss:  297.62373811006546
test acc: top1 ->  68.762 ; top5 ->  88.61  and loss:  1015.0275600850582
forward train acc: top1 ->  65.4296875 ; top5 ->  85.0625  and loss:  293.1398239135742
test acc: top1 ->  68.788 ; top5 ->  88.588  and loss:  1014.8574373424053
forward train acc: top1 ->  65.7890625 ; top5 ->  84.75  and loss:  294.5750050544739
test acc: top1 ->  68.71 ; top5 ->  88.648  and loss:  1016.137491196394
forward train acc: top1 ->  65.578125 ; top5 ->  84.8125  and loss:  292.4049553871155
test acc: top1 ->  68.704 ; top5 ->  88.606  and loss:  1015.1291390657425
>>>>>>> reverse layer  21  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  22  ---------------
adv train loss:  -73.79147392511368 , diff:  73.79147392511368
adv train loss:  -72.91790461540222 , diff:  0.8735693097114563
layer  22  adv train finish, try to retain  238
>>>>>>> reverse layer  22  since no improvement >>>>>>>
---------------- start layer  23  ---------------
adv train loss:  -73.10847675800323 , diff:  73.10847675800323
adv train loss:  -72.10088592767715 , diff:  1.0075908303260803
layer  23  adv train finish, try to retain  229
>>>>>>> reverse layer  23  since no improvement >>>>>>>
---------------- start layer  24  ---------------
adv train loss:  -71.92956703901291 , diff:  71.92956703901291
adv train loss:  -73.24326658248901 , diff:  1.3136995434761047
layer  24  adv train finish, try to retain  243
test acc: top1 ->  68.73 ; top5 ->  88.68  and loss:  1015.0834173858166
forward train acc: top1 ->  66.0078125 ; top5 ->  85.4609375  and loss:  290.18790459632874
test acc: top1 ->  68.708 ; top5 ->  88.586  and loss:  1014.5905819535255
forward train acc: top1 ->  65.6953125 ; top5 ->  84.9765625  and loss:  291.34985530376434
test acc: top1 ->  68.766 ; top5 ->  88.684  and loss:  1014.8374586999416
forward train acc: top1 ->  65.453125 ; top5 ->  84.859375  and loss:  294.58217376470566
test acc: top1 ->  68.81 ; top5 ->  88.626  and loss:  1016.9001516401768
forward train acc: top1 ->  66.3515625 ; top5 ->  85.7421875  and loss:  288.68746268749237
test acc: top1 ->  68.804 ; top5 ->  88.654  and loss:  1012.82357442379
forward train acc: top1 ->  65.578125 ; top5 ->  85.140625  and loss:  293.3194361925125
test acc: top1 ->  68.826 ; top5 ->  88.692  and loss:  1014.2301022410393
forward train acc: top1 ->  66.6640625 ; top5 ->  85.78125  and loss:  282.84442961215973
test acc: top1 ->  68.88 ; top5 ->  88.654  and loss:  1013.8416582047939
forward train acc: top1 ->  65.859375 ; top5 ->  85.265625  and loss:  290.4127675294876
test acc: top1 ->  68.92 ; top5 ->  88.688  and loss:  1010.4033038914204
forward train acc: top1 ->  66.171875 ; top5 ->  85.2265625  and loss:  290.7932449579239
test acc: top1 ->  68.81 ; top5 ->  88.664  and loss:  1010.6675630807877
forward train acc: top1 ->  66.0390625 ; top5 ->  85.578125  and loss:  289.1262581348419
test acc: top1 ->  68.936 ; top5 ->  88.726  and loss:  1010.3563935160637
forward train acc: top1 ->  65.359375 ; top5 ->  85.3515625  and loss:  290.2560248374939
test acc: top1 ->  68.942 ; top5 ->  88.716  and loss:  1011.5341607034206
>>>>>>> reverse layer  24  since performance drop >>>>>>>
==> this epoch:  244 / 256 , inc:  1
---------------- start layer  25  ---------------
adv train loss:  -72.54655230045319 , diff:  72.54655230045319
adv train loss:  -73.0393625497818 , diff:  0.4928102493286133
layer  25  adv train finish, try to retain  235
test acc: top1 ->  68.544 ; top5 ->  88.552  and loss:  1020.0205750763416
forward train acc: top1 ->  65.578125 ; top5 ->  84.75  and loss:  295.15176367759705
test acc: top1 ->  68.642 ; top5 ->  88.63  and loss:  1015.0659380853176
forward train acc: top1 ->  65.8671875 ; top5 ->  85.3359375  and loss:  289.9554145336151
test acc: top1 ->  68.698 ; top5 ->  88.678  and loss:  1014.6003117859364
forward train acc: top1 ->  64.9921875 ; top5 ->  84.7890625  and loss:  298.22641080617905
test acc: top1 ->  68.644 ; top5 ->  88.596  and loss:  1016.9012924432755
forward train acc: top1 ->  65.09375 ; top5 ->  84.9765625  and loss:  295.72906202077866
test acc: top1 ->  68.698 ; top5 ->  88.586  and loss:  1016.4396889805794
forward train acc: top1 ->  65.6171875 ; top5 ->  85.203125  and loss:  291.9419558644295
test acc: top1 ->  68.574 ; top5 ->  88.64  and loss:  1014.2043899595737
forward train acc: top1 ->  65.9453125 ; top5 ->  85.1484375  and loss:  293.51554334163666
test acc: top1 ->  68.74 ; top5 ->  88.578  and loss:  1015.1488625705242
forward train acc: top1 ->  65.3671875 ; top5 ->  85.0625  and loss:  294.11658251285553
test acc: top1 ->  68.842 ; top5 ->  88.59  and loss:  1013.8923496603966
forward train acc: top1 ->  66.2421875 ; top5 ->  85.28125  and loss:  291.0654606819153
test acc: top1 ->  68.78 ; top5 ->  88.638  and loss:  1010.5963310599327
forward train acc: top1 ->  65.9609375 ; top5 ->  84.9296875  and loss:  293.2307144999504
test acc: top1 ->  68.768 ; top5 ->  88.604  and loss:  1011.9471060633659
forward train acc: top1 ->  65.2734375 ; top5 ->  85.015625  and loss:  296.11648458242416
test acc: top1 ->  68.738 ; top5 ->  88.688  and loss:  1011.0359396636486
>>>>>>> reverse layer  25  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  26  ---------------
adv train loss:  -71.24407243728638 , diff:  71.24407243728638
adv train loss:  -73.37510991096497 , diff:  2.131037473678589
layer  26  adv train finish, try to retain  500
test acc: top1 ->  68.748 ; top5 ->  88.598  and loss:  1013.3307508528233
forward train acc: top1 ->  65.125 ; top5 ->  84.9453125  and loss:  299.0002293586731
test acc: top1 ->  68.766 ; top5 ->  88.556  and loss:  1018.3219131231308
forward train acc: top1 ->  66.5859375 ; top5 ->  85.6328125  and loss:  286.35872608423233
test acc: top1 ->  68.764 ; top5 ->  88.62  and loss:  1016.407662242651
forward train acc: top1 ->  65.484375 ; top5 ->  85.078125  and loss:  290.74646151065826
test acc: top1 ->  68.72 ; top5 ->  88.594  and loss:  1016.0710316300392
forward train acc: top1 ->  66.03125 ; top5 ->  85.4140625  and loss:  290.8948230743408
test acc: top1 ->  68.868 ; top5 ->  88.682  and loss:  1012.7604620456696
forward train acc: top1 ->  65.2734375 ; top5 ->  84.90625  and loss:  296.82679134607315
test acc: top1 ->  68.776 ; top5 ->  88.646  and loss:  1012.2605748176575
forward train acc: top1 ->  65.53125 ; top5 ->  85.2734375  and loss:  292.98086512088776
test acc: top1 ->  68.756 ; top5 ->  88.692  and loss:  1012.8065273463726
forward train acc: top1 ->  65.640625 ; top5 ->  85.0234375  and loss:  295.7716717720032
test acc: top1 ->  68.966 ; top5 ->  88.7  and loss:  1006.9643528163433
forward train acc: top1 ->  66.046875 ; top5 ->  85.296875  and loss:  289.81963235139847
test acc: top1 ->  68.902 ; top5 ->  88.678  and loss:  1010.3921747803688
forward train acc: top1 ->  66.3984375 ; top5 ->  85.71875  and loss:  284.3971220254898
test acc: top1 ->  68.924 ; top5 ->  88.698  and loss:  1008.0175786614418
forward train acc: top1 ->  66.9921875 ; top5 ->  85.84375  and loss:  282.29196286201477
test acc: top1 ->  68.896 ; top5 ->  88.724  and loss:  1009.803049236536
>>>>>>> reverse layer  26  since performance drop >>>>>>>
==> this epoch:  502 / 512 , inc:  1
---------------- start layer  27  ---------------
adv train loss:  -71.95670384168625 , diff:  71.95670384168625
adv train loss:  -74.34258526563644 , diff:  2.3858814239501953
layer  27  adv train finish, try to retain  504
test acc: top1 ->  68.682 ; top5 ->  88.59  and loss:  1013.4173300862312
forward train acc: top1 ->  65.703125 ; top5 ->  85.15625  and loss:  290.5479741692543
test acc: top1 ->  68.656 ; top5 ->  88.66  and loss:  1011.2712823748589
forward train acc: top1 ->  65.328125 ; top5 ->  84.8828125  and loss:  297.3238573670387
test acc: top1 ->  68.696 ; top5 ->  88.674  and loss:  1013.5047026276588
forward train acc: top1 ->  65.7421875 ; top5 ->  85.5546875  and loss:  290.89643543958664
test acc: top1 ->  68.79 ; top5 ->  88.688  and loss:  1012.2757062315941
forward train acc: top1 ->  65.4921875 ; top5 ->  84.796875  and loss:  295.0304872393608
test acc: top1 ->  68.826 ; top5 ->  88.778  and loss:  1011.1511642038822
forward train acc: top1 ->  65.7578125 ; top5 ->  85.2734375  and loss:  292.7125036716461
test acc: top1 ->  68.812 ; top5 ->  88.686  and loss:  1011.3724338412285
forward train acc: top1 ->  66.0546875 ; top5 ->  85.2890625  and loss:  288.6044535636902
test acc: top1 ->  68.892 ; top5 ->  88.738  and loss:  1009.3630358874798
forward train acc: top1 ->  65.953125 ; top5 ->  85.7890625  and loss:  284.61257749795914
test acc: top1 ->  68.814 ; top5 ->  88.776  and loss:  1010.0819084644318
forward train acc: top1 ->  65.859375 ; top5 ->  85.265625  and loss:  292.4837892651558
test acc: top1 ->  68.976 ; top5 ->  88.838  and loss:  1006.8053232431412
forward train acc: top1 ->  66.4765625 ; top5 ->  85.6328125  and loss:  287.6117796301842
test acc: top1 ->  68.854 ; top5 ->  88.726  and loss:  1007.8193942904472
forward train acc: top1 ->  65.21875 ; top5 ->  84.953125  and loss:  295.0200983285904
test acc: top1 ->  68.998 ; top5 ->  88.852  and loss:  1007.0753087997437
>>>>>>> reverse layer  27  since performance drop >>>>>>>
==> this epoch:  505 / 512 , inc:  1
---------------- start layer  28  ---------------
adv train loss:  -76.37069910764694 , diff:  76.37069910764694
adv train loss:  -71.09794443845749 , diff:  5.272754669189453
layer  28  adv train finish, try to retain  505
>>>>>>> reverse layer  28  since no improvement >>>>>>>
---------------- start layer  29  ---------------
adv train loss:  -67.85843342542648 , diff:  67.85843342542648
adv train loss:  -73.25249195098877 , diff:  5.394058525562286
layer  29  adv train finish, try to retain  490
test acc: top1 ->  68.88 ; top5 ->  88.798  and loss:  1009.4467541575432
forward train acc: top1 ->  65.6484375 ; top5 ->  84.96875  and loss:  293.8660760521889
test acc: top1 ->  69.014 ; top5 ->  88.678  and loss:  1008.2822215259075
forward train acc: top1 ->  66.1640625 ; top5 ->  85.3046875  and loss:  288.29717642068863
test acc: top1 ->  68.98 ; top5 ->  88.69  and loss:  1010.8412343859673
forward train acc: top1 ->  66.2265625 ; top5 ->  85.3671875  and loss:  292.4069572687149
test acc: top1 ->  68.75 ; top5 ->  88.704  and loss:  1010.2293526530266
forward train acc: top1 ->  65.453125 ; top5 ->  85.15625  and loss:  293.9086380004883
test acc: top1 ->  68.97 ; top5 ->  88.762  and loss:  1008.0427287220955
forward train acc: top1 ->  66.5859375 ; top5 ->  85.65625  and loss:  286.8326462507248
test acc: top1 ->  68.82 ; top5 ->  88.716  and loss:  1007.1457976102829
forward train acc: top1 ->  65.9765625 ; top5 ->  85.125  and loss:  291.0177989602089
test acc: top1 ->  69.028 ; top5 ->  88.756  and loss:  1007.5404587984085
forward train acc: top1 ->  66.21875 ; top5 ->  85.6796875  and loss:  286.6176652312279
test acc: top1 ->  69.018 ; top5 ->  88.814  and loss:  1006.0173223018646
forward train acc: top1 ->  65.5234375 ; top5 ->  84.8046875  and loss:  297.69357907772064
test acc: top1 ->  69.046 ; top5 ->  88.794  and loss:  1005.1207710206509
forward train acc: top1 ->  65.6640625 ; top5 ->  84.7890625  and loss:  291.28824853897095
test acc: top1 ->  68.978 ; top5 ->  88.798  and loss:  1006.4756074547768
forward train acc: top1 ->  66.1953125 ; top5 ->  85.609375  and loss:  287.9189227819443
test acc: top1 ->  69.14 ; top5 ->  88.778  and loss:  1004.1610432863235
>>>>>>> reverse layer  29  since performance drop >>>>>>>
==> this epoch:  498 / 512 , inc:  1
---------------- start layer  30  ---------------
adv train loss:  -71.15242779254913 , diff:  71.15242779254913
adv train loss:  -74.11052167415619 , diff:  2.9580938816070557
layer  30  adv train finish, try to retain  497
test acc: top1 ->  68.946 ; top5 ->  88.662  and loss:  1007.5751846730709
forward train acc: top1 ->  65.1328125 ; top5 ->  84.96875  and loss:  292.77837574481964
test acc: top1 ->  68.868 ; top5 ->  88.726  and loss:  1012.3376957178116
forward train acc: top1 ->  65.2109375 ; top5 ->  85.0390625  and loss:  295.32295322418213
test acc: top1 ->  68.764 ; top5 ->  88.646  and loss:  1011.3981688022614
forward train acc: top1 ->  66.03125 ; top5 ->  85.15625  and loss:  290.30828070640564
test acc: top1 ->  68.836 ; top5 ->  88.644  and loss:  1009.6346070766449
forward train acc: top1 ->  65.8828125 ; top5 ->  85.265625  and loss:  290.3130038380623
test acc: top1 ->  68.922 ; top5 ->  88.734  and loss:  1007.8877286911011
forward train acc: top1 ->  65.9296875 ; top5 ->  85.453125  and loss:  290.4100239276886
test acc: top1 ->  68.972 ; top5 ->  88.748  and loss:  1007.5744295120239
forward train acc: top1 ->  65.59375 ; top5 ->  85.15625  and loss:  291.51699340343475
test acc: top1 ->  68.886 ; top5 ->  88.794  and loss:  1008.5413762927055
forward train acc: top1 ->  66.015625 ; top5 ->  85.375  and loss:  292.0272633433342
test acc: top1 ->  68.83 ; top5 ->  88.758  and loss:  1008.443890273571
forward train acc: top1 ->  66.3515625 ; top5 ->  85.1171875  and loss:  291.7904483675957
test acc: top1 ->  69.036 ; top5 ->  88.77  and loss:  1005.4128130078316
forward train acc: top1 ->  66.6953125 ; top5 ->  85.546875  and loss:  288.2874821424484
test acc: top1 ->  68.938 ; top5 ->  88.782  and loss:  1002.1846812069416
forward train acc: top1 ->  66.4375 ; top5 ->  85.75  and loss:  284.266021668911
test acc: top1 ->  68.884 ; top5 ->  88.728  and loss:  1005.7443414330482
>>>>>>> reverse layer  30  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  31  ---------------
adv train loss:  -71.49373435974121 , diff:  71.49373435974121
adv train loss:  -74.191475212574 , diff:  2.697740852832794
layer  31  adv train finish, try to retain  489
test acc: top1 ->  68.568 ; top5 ->  88.53  and loss:  1020.8489357829094
forward train acc: top1 ->  65.96875 ; top5 ->  84.8046875  and loss:  297.5354894399643
test acc: top1 ->  68.524 ; top5 ->  88.6  and loss:  1018.6858687996864
forward train acc: top1 ->  66.8125 ; top5 ->  85.8984375  and loss:  285.71907621622086
test acc: top1 ->  68.712 ; top5 ->  88.674  and loss:  1014.2371775507927
forward train acc: top1 ->  66.375 ; top5 ->  85.4921875  and loss:  288.26683896780014
test acc: top1 ->  68.748 ; top5 ->  88.712  and loss:  1012.0464470386505
forward train acc: top1 ->  65.546875 ; top5 ->  85.1640625  and loss:  293.174717605114
test acc: top1 ->  68.79 ; top5 ->  88.76  and loss:  1008.6507300436497
forward train acc: top1 ->  65.25 ; top5 ->  85.09375  and loss:  297.21729385852814
test acc: top1 ->  68.862 ; top5 ->  88.744  and loss:  1007.8736193776131
forward train acc: top1 ->  66.875 ; top5 ->  85.921875  and loss:  283.86770337820053
test acc: top1 ->  68.854 ; top5 ->  88.752  and loss:  1007.3058065772057
forward train acc: top1 ->  65.671875 ; top5 ->  85.4453125  and loss:  293.1456741094589
test acc: top1 ->  68.984 ; top5 ->  88.742  and loss:  1004.7596881389618
forward train acc: top1 ->  65.5859375 ; top5 ->  85.0234375  and loss:  294.071433365345
test acc: top1 ->  68.968 ; top5 ->  88.822  and loss:  1005.7556171417236
forward train acc: top1 ->  66.2421875 ; top5 ->  85.2109375  and loss:  289.122268140316
test acc: top1 ->  69.062 ; top5 ->  88.766  and loss:  1005.2614468932152
forward train acc: top1 ->  65.5078125 ; top5 ->  85.2578125  and loss:  290.7065538764
test acc: top1 ->  68.888 ; top5 ->  88.776  and loss:  1005.9664848446846
>>>>>>> reverse layer  31  since performance drop >>>>>>>
==> this epoch:  501 / 512 , inc:  1
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.0025, 0.0025, 0.0009375, 0.0025, 0.0025, 0.0025, 0.00125, 0.00017578125, 0.00125, 0.00017578125, 0.00125, 0.00125, 0.00125, 0.00125, 8.7890625e-05, 8.7890625e-05, 0.000234375, 0.000625, 0.000625, 0.000625, 0.000625, 0.000234375, 0.000625, 0.000625, 0.000234375, 0.000234375, 4.39453125e-05, 4.39453125e-05, 0.0001171875, 0.0001171875, 4.39453125e-05, 0.0001171875]  wait [0, 0, 2, 0, 0, 0, 0, 4, 0, 4, 0, 0, 0, 0, 4, 4, 2, 0, 0, 0, 0, 2, 0, 0, 2, 2, 4, 4, 2, 2, 4, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 0
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  4  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -75.12311094999313 , diff:  75.12311094999313
adv train loss:  -71.67235797643661 , diff:  3.4507529735565186
layer  0  adv train finish, try to retain  54
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -72.87710112333298 , diff:  72.87710112333298
adv train loss:  -73.2982839345932 , diff:  0.4211828112602234
layer  1  adv train finish, try to retain  58
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -72.90613174438477 , diff:  72.90613174438477
adv train loss:  -70.74606704711914 , diff:  2.160064697265625
layer  2  adv train finish, try to retain  51
test acc: top1 ->  67.27 ; top5 ->  87.64  and loss:  1063.5585322380066
forward train acc: top1 ->  65.8671875 ; top5 ->  85.1953125  and loss:  293.1654282808304
test acc: top1 ->  68.582 ; top5 ->  88.508  and loss:  1016.5399910211563
forward train acc: top1 ->  66.125 ; top5 ->  85.328125  and loss:  290.3592075705528
test acc: top1 ->  68.698 ; top5 ->  88.666  and loss:  1014.2648305296898
forward train acc: top1 ->  65.3125 ; top5 ->  85.078125  and loss:  292.2655339837074
test acc: top1 ->  68.634 ; top5 ->  88.626  and loss:  1016.4230134487152
forward train acc: top1 ->  66.3203125 ; top5 ->  85.6875  and loss:  286.26351779699326
test acc: top1 ->  68.81 ; top5 ->  88.676  and loss:  1012.6129813790321
forward train acc: top1 ->  65.96875 ; top5 ->  85.4765625  and loss:  286.6331321001053
test acc: top1 ->  68.842 ; top5 ->  88.738  and loss:  1011.1729511618614
forward train acc: top1 ->  66.3046875 ; top5 ->  85.7109375  and loss:  289.2168116569519
test acc: top1 ->  68.936 ; top5 ->  88.786  and loss:  1008.0513101816177
forward train acc: top1 ->  65.84375 ; top5 ->  85.0390625  and loss:  291.43296164274216
test acc: top1 ->  68.862 ; top5 ->  88.752  and loss:  1007.7839782834053
forward train acc: top1 ->  65.7734375 ; top5 ->  85.375  and loss:  291.20164877176285
test acc: top1 ->  68.968 ; top5 ->  88.7  and loss:  1007.3327272832394
forward train acc: top1 ->  66.4609375 ; top5 ->  85.03125  and loss:  289.52363777160645
test acc: top1 ->  69.03 ; top5 ->  88.722  and loss:  1008.8064147531986
forward train acc: top1 ->  66.3203125 ; top5 ->  85.640625  and loss:  284.50454449653625
test acc: top1 ->  69.038 ; top5 ->  88.796  and loss:  1005.7294076681137
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  57 / 64 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -75.06434446573257 , diff:  75.06434446573257
adv train loss:  -70.75851321220398 , diff:  4.305831253528595
layer  3  adv train finish, try to retain  52
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -72.41657590866089 , diff:  72.41657590866089
adv train loss:  -73.72773319482803 , diff:  1.3111572861671448
layer  4  adv train finish, try to retain  52
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -72.30397272109985 , diff:  72.30397272109985
adv train loss:  -75.00029128789902 , diff:  2.696318566799164
layer  5  adv train finish, try to retain  48
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -70.4837173819542 , diff:  70.4837173819542
adv train loss:  -73.66809499263763 , diff:  3.184377610683441
layer  6  adv train finish, try to retain  108
test acc: top1 ->  66.168 ; top5 ->  87.054  and loss:  1107.3824107050896
forward train acc: top1 ->  65.4921875 ; top5 ->  84.7109375  and loss:  295.8752772808075
test acc: top1 ->  68.712 ; top5 ->  88.694  and loss:  1014.8758125007153
forward train acc: top1 ->  65.96875 ; top5 ->  85.0234375  and loss:  292.61497032642365
test acc: top1 ->  68.716 ; top5 ->  88.636  and loss:  1014.9707837700844
forward train acc: top1 ->  66.3125 ; top5 ->  85.5234375  and loss:  287.99878692626953
test acc: top1 ->  68.706 ; top5 ->  88.59  and loss:  1014.6408385038376
forward train acc: top1 ->  65.640625 ; top5 ->  85.4140625  and loss:  290.18050223588943
test acc: top1 ->  68.63 ; top5 ->  88.642  and loss:  1013.4169234931469
forward train acc: top1 ->  65.8125 ; top5 ->  85.21875  and loss:  290.8774349093437
test acc: top1 ->  68.718 ; top5 ->  88.688  and loss:  1010.7820543050766
forward train acc: top1 ->  65.6953125 ; top5 ->  85.03125  and loss:  295.6613122224808
test acc: top1 ->  68.784 ; top5 ->  88.754  and loss:  1009.7151387929916
forward train acc: top1 ->  66.109375 ; top5 ->  85.75  and loss:  286.21356600522995
test acc: top1 ->  68.836 ; top5 ->  88.786  and loss:  1010.0384119451046
forward train acc: top1 ->  66.0078125 ; top5 ->  85.6015625  and loss:  287.69324320554733
test acc: top1 ->  68.954 ; top5 ->  88.734  and loss:  1008.137426763773
forward train acc: top1 ->  65.8515625 ; top5 ->  85.0  and loss:  294.85420644283295
test acc: top1 ->  68.888 ; top5 ->  88.79  and loss:  1008.5684567689896
forward train acc: top1 ->  66.2578125 ; top5 ->  85.578125  and loss:  286.4045475721359
test acc: top1 ->  68.926 ; top5 ->  88.834  and loss:  1004.8980292379856
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  112 / 128 , inc:  1
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
adv train loss:  -71.06015473604202 , diff:  71.06015473604202
adv train loss:  -71.98024594783783 , diff:  0.9200912117958069
layer  8  adv train finish, try to retain  111
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
### skip layer  9 wait:  4  ###
---------------- start layer  10  ---------------
adv train loss:  -70.41756898164749 , diff:  70.41756898164749
adv train loss:  -69.91139042377472 , diff:  0.5061785578727722
layer  10  adv train finish, try to retain  107
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -69.95516705513 , diff:  69.95516705513
adv train loss:  -71.11602956056595 , diff:  1.1608625054359436
layer  11  adv train finish, try to retain  114
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -72.65077435970306 , diff:  72.65077435970306
adv train loss:  -71.35070687532425 , diff:  1.3000674843788147
layer  12  adv train finish, try to retain  104
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -72.86200267076492 , diff:  72.86200267076492
adv train loss:  -73.3394124507904 , diff:  0.4774097800254822
layer  13  adv train finish, try to retain  111
>>>>>>> reverse layer  13  since no improvement >>>>>>>
---------------- start layer  14  ---------------
### skip layer  14 wait:  4  ###
---------------- start layer  15  ---------------
### skip layer  15 wait:  4  ###
---------------- start layer  16  ---------------
adv train loss:  -71.68097907304764 , diff:  71.68097907304764
adv train loss:  -72.67482244968414 , diff:  0.9938433766365051
************ all values are small in this layer **********
layer  16  adv train finish, try to retain  238
test acc: top1 ->  68.932 ; top5 ->  88.748  and loss:  1006.9327052235603
forward train acc: top1 ->  65.9765625 ; top5 ->  85.59375  and loss:  289.80437248945236
test acc: top1 ->  69.064 ; top5 ->  88.794  and loss:  1007.8189272582531
forward train acc: top1 ->  65.75 ; top5 ->  85.2265625  and loss:  293.0408161878586
test acc: top1 ->  68.924 ; top5 ->  88.798  and loss:  1006.788653165102
forward train acc: top1 ->  65.8671875 ; top5 ->  85.2578125  and loss:  289.24135226011276
test acc: top1 ->  69.048 ; top5 ->  88.768  and loss:  1005.0978928506374
forward train acc: top1 ->  66.1640625 ; top5 ->  85.296875  and loss:  288.86482924222946
test acc: top1 ->  69.084 ; top5 ->  88.85  and loss:  1000.8172578811646
forward train acc: top1 ->  65.4375 ; top5 ->  84.8828125  and loss:  294.14569306373596
test acc: top1 ->  69.136 ; top5 ->  88.854  and loss:  1000.3963920176029
forward train acc: top1 ->  66.4921875 ; top5 ->  85.421875  and loss:  288.9670023918152
test acc: top1 ->  69.084 ; top5 ->  88.756  and loss:  1003.3635347783566
forward train acc: top1 ->  66.109375 ; top5 ->  85.71875  and loss:  285.9184142947197
test acc: top1 ->  69.126 ; top5 ->  88.81  and loss:  1001.1337557435036
forward train acc: top1 ->  66.265625 ; top5 ->  85.25  and loss:  288.20568746328354
test acc: top1 ->  69.152 ; top5 ->  88.882  and loss:  999.1183903813362
forward train acc: top1 ->  66.7109375 ; top5 ->  85.5703125  and loss:  285.26411151885986
test acc: top1 ->  69.174 ; top5 ->  88.864  and loss:  999.6953811049461
forward train acc: top1 ->  66.90625 ; top5 ->  85.9765625  and loss:  284.47246384620667
test acc: top1 ->  69.216 ; top5 ->  88.854  and loss:  998.7461618483067
>>>>>>> reverse layer  16  since performance drop >>>>>>>
==> this epoch:  239 / 256 , inc:  1
---------------- start layer  17  ---------------
adv train loss:  -71.43962562084198 , diff:  71.43962562084198
adv train loss:  -70.42364084720612 , diff:  1.0159847736358643
layer  17  adv train finish, try to retain  220
test acc: top1 ->  68.344 ; top5 ->  88.406  and loss:  1027.8374742865562
forward train acc: top1 ->  66.3359375 ; top5 ->  85.359375  and loss:  290.6714246869087
test acc: top1 ->  68.864 ; top5 ->  88.738  and loss:  1010.1322971880436
forward train acc: top1 ->  66.296875 ; top5 ->  85.5546875  and loss:  286.9027298092842
test acc: top1 ->  68.932 ; top5 ->  88.778  and loss:  1010.7353989183903
forward train acc: top1 ->  66.078125 ; top5 ->  85.1484375  and loss:  289.1699924468994
test acc: top1 ->  68.874 ; top5 ->  88.71  and loss:  1010.6163300573826
forward train acc: top1 ->  66.609375 ; top5 ->  85.7890625  and loss:  281.7292256951332
test acc: top1 ->  68.998 ; top5 ->  88.754  and loss:  1008.3081253170967
forward train acc: top1 ->  65.921875 ; top5 ->  85.3984375  and loss:  291.7702702283859
test acc: top1 ->  68.998 ; top5 ->  88.846  and loss:  1005.238383680582
forward train acc: top1 ->  66.578125 ; top5 ->  84.8984375  and loss:  289.4334269165993
test acc: top1 ->  69.11 ; top5 ->  88.87  and loss:  1005.1132657527924
forward train acc: top1 ->  65.703125 ; top5 ->  85.4453125  and loss:  289.09897124767303
test acc: top1 ->  69.048 ; top5 ->  88.854  and loss:  1005.8035700023174
forward train acc: top1 ->  66.0703125 ; top5 ->  85.1640625  and loss:  292.8051800131798
test acc: top1 ->  69.078 ; top5 ->  88.894  and loss:  1002.7109606266022
forward train acc: top1 ->  66.34375 ; top5 ->  85.578125  and loss:  284.80194717645645
test acc: top1 ->  69.05 ; top5 ->  88.848  and loss:  1004.1451032757759
forward train acc: top1 ->  66.65625 ; top5 ->  85.9296875  and loss:  285.38091427087784
test acc: top1 ->  69.116 ; top5 ->  88.858  and loss:  1004.6517593860626
>>>>>>> reverse layer  17  since performance drop >>>>>>>
==> this epoch:  230 / 256 , inc:  1
---------------- start layer  18  ---------------
adv train loss:  -70.09409189224243 , diff:  70.09409189224243
adv train loss:  -71.19016206264496 , diff:  1.0960701704025269
layer  18  adv train finish, try to retain  216
test acc: top1 ->  68.854 ; top5 ->  88.764  and loss:  1007.5234910845757
forward train acc: top1 ->  65.140625 ; top5 ->  84.4765625  and loss:  298.72142857313156
test acc: top1 ->  68.978 ; top5 ->  88.79  and loss:  1006.5264119207859
forward train acc: top1 ->  66.2578125 ; top5 ->  86.0546875  and loss:  285.40838450193405
test acc: top1 ->  68.98 ; top5 ->  88.816  and loss:  1005.1043846309185
forward train acc: top1 ->  66.0390625 ; top5 ->  85.15625  and loss:  287.9136648774147
test acc: top1 ->  69.118 ; top5 ->  88.808  and loss:  1005.8340181410313
forward train acc: top1 ->  65.671875 ; top5 ->  85.203125  and loss:  289.2850087881088
test acc: top1 ->  69.11 ; top5 ->  88.836  and loss:  1006.2680208086967
forward train acc: top1 ->  65.859375 ; top5 ->  85.8828125  and loss:  288.7110936641693
test acc: top1 ->  69.174 ; top5 ->  88.816  and loss:  1004.1616723537445
forward train acc: top1 ->  66.609375 ; top5 ->  85.7734375  and loss:  285.3052257299423
test acc: top1 ->  69.126 ; top5 ->  88.846  and loss:  1001.6539523303509
forward train acc: top1 ->  66.6484375 ; top5 ->  85.25  and loss:  287.52712446451187
test acc: top1 ->  69.108 ; top5 ->  88.824  and loss:  1003.2483628094196
forward train acc: top1 ->  65.9765625 ; top5 ->  85.4765625  and loss:  289.3421439528465
test acc: top1 ->  69.226 ; top5 ->  88.844  and loss:  1002.1628036499023
forward train acc: top1 ->  66.515625 ; top5 ->  85.5703125  and loss:  287.5389095544815
test acc: top1 ->  69.21 ; top5 ->  88.924  and loss:  1000.1785724461079
forward train acc: top1 ->  67.3046875 ; top5 ->  85.875  and loss:  283.39473432302475
test acc: top1 ->  69.308 ; top5 ->  88.89  and loss:  999.3475073873997
>>>>>>> reverse layer  18  since performance drop >>>>>>>
==> this epoch:  235 / 256 , inc:  1
---------------- start layer  19  ---------------
adv train loss:  -73.10887867212296 , diff:  73.10887867212296
adv train loss:  -73.36205840110779 , diff:  0.25317972898483276
layer  19  adv train finish, try to retain  228
>>>>>>> reverse layer  19  since no improvement >>>>>>>
---------------- start layer  20  ---------------
adv train loss:  -72.4741759300232 , diff:  72.4741759300232
adv train loss:  -71.93553632497787 , diff:  0.5386396050453186
layer  20  adv train finish, try to retain  218
test acc: top1 ->  68.866 ; top5 ->  88.702  and loss:  1009.1125515699387
forward train acc: top1 ->  66.203125 ; top5 ->  84.9921875  and loss:  291.3018223643303
test acc: top1 ->  69.064 ; top5 ->  88.786  and loss:  1006.9127366542816
forward train acc: top1 ->  66.3046875 ; top5 ->  85.7578125  and loss:  286.25248593091965
test acc: top1 ->  69.1 ; top5 ->  88.766  and loss:  1003.2686086893082
forward train acc: top1 ->  65.4296875 ; top5 ->  85.2265625  and loss:  292.90593987703323
test acc: top1 ->  69.0 ; top5 ->  88.762  and loss:  1005.7010270357132
forward train acc: top1 ->  65.7421875 ; top5 ->  85.5234375  and loss:  290.94754523038864
test acc: top1 ->  69.08 ; top5 ->  88.772  and loss:  1001.8150621652603
forward train acc: top1 ->  66.8984375 ; top5 ->  85.671875  and loss:  284.3633642196655
test acc: top1 ->  69.056 ; top5 ->  88.846  and loss:  1001.8632352650166
forward train acc: top1 ->  65.2734375 ; top5 ->  85.1484375  and loss:  297.08153134584427
test acc: top1 ->  69.16 ; top5 ->  88.776  and loss:  1002.4933356940746
forward train acc: top1 ->  66.1328125 ; top5 ->  85.4296875  and loss:  285.13735008239746
test acc: top1 ->  69.194 ; top5 ->  88.916  and loss:  998.5781029760838
forward train acc: top1 ->  66.9375 ; top5 ->  85.6015625  and loss:  282.8200715780258
test acc: top1 ->  69.188 ; top5 ->  88.852  and loss:  999.2141487598419
forward train acc: top1 ->  66.1171875 ; top5 ->  84.8046875  and loss:  291.6959028840065
test acc: top1 ->  69.198 ; top5 ->  88.954  and loss:  998.8868965506554
forward train acc: top1 ->  66.796875 ; top5 ->  85.6328125  and loss:  282.2256174683571
test acc: top1 ->  69.212 ; top5 ->  88.882  and loss:  999.8686554133892
>>>>>>> reverse layer  20  since performance drop >>>>>>>
==> this epoch:  225 / 256 , inc:  1
---------------- start layer  21  ---------------
adv train loss:  -72.00214552879333 , diff:  72.00214552879333
adv train loss:  -66.68585109710693 , diff:  5.316294431686401
************ all values are small in this layer **********
layer  21  adv train finish, try to retain  241
test acc: top1 ->  69.282 ; top5 ->  88.848  and loss:  999.5366399288177
forward train acc: top1 ->  65.3046875 ; top5 ->  85.1796875  and loss:  293.768406689167
test acc: top1 ->  69.168 ; top5 ->  88.886  and loss:  1000.4409330785275
forward train acc: top1 ->  66.421875 ; top5 ->  85.3046875  and loss:  283.93976002931595
test acc: top1 ->  69.238 ; top5 ->  88.81  and loss:  1000.788278311491
forward train acc: top1 ->  66.0 ; top5 ->  85.7109375  and loss:  283.30166256427765
test acc: top1 ->  69.328 ; top5 ->  88.836  and loss:  999.2062995433807
forward train acc: top1 ->  66.5859375 ; top5 ->  85.5390625  and loss:  286.1626180410385
test acc: top1 ->  69.31 ; top5 ->  88.868  and loss:  1000.0886763334274
forward train acc: top1 ->  66.875 ; top5 ->  85.953125  and loss:  281.132746219635
test acc: top1 ->  69.278 ; top5 ->  88.89  and loss:  999.5187977552414
forward train acc: top1 ->  67.171875 ; top5 ->  86.2109375  and loss:  277.2115774154663
test acc: top1 ->  69.314 ; top5 ->  88.954  and loss:  998.592740625143
forward train acc: top1 ->  67.03125 ; top5 ->  85.6484375  and loss:  282.7768238186836
test acc: top1 ->  69.304 ; top5 ->  88.936  and loss:  999.0353472828865
forward train acc: top1 ->  66.1796875 ; top5 ->  85.1328125  and loss:  288.1662942767143
test acc: top1 ->  69.326 ; top5 ->  88.906  and loss:  997.3923521339893
forward train acc: top1 ->  66.546875 ; top5 ->  86.0625  and loss:  283.393570125103
test acc: top1 ->  69.354 ; top5 ->  88.944  and loss:  994.5428456664085
forward train acc: top1 ->  67.1484375 ; top5 ->  85.625  and loss:  282.67374432086945
test acc: top1 ->  69.356 ; top5 ->  88.894  and loss:  997.257331430912
>>>>>>> reverse layer  21  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  22  ---------------
adv train loss:  -70.71465051174164 , diff:  70.71465051174164
adv train loss:  -68.89871549606323 , diff:  1.8159350156784058
layer  22  adv train finish, try to retain  229
>>>>>>> reverse layer  22  since no improvement >>>>>>>
---------------- start layer  23  ---------------
adv train loss:  -73.39431619644165 , diff:  73.39431619644165
adv train loss:  -71.7149943113327 , diff:  1.6793218851089478
layer  23  adv train finish, try to retain  228
>>>>>>> reverse layer  23  since no improvement >>>>>>>
---------------- start layer  24  ---------------
adv train loss:  -73.79920452833176 , diff:  73.79920452833176
adv train loss:  -69.75132203102112 , diff:  4.047882497310638
************ all values are small in this layer **********
layer  24  adv train finish, try to retain  243
test acc: top1 ->  69.242 ; top5 ->  88.946  and loss:  1000.6764017045498
forward train acc: top1 ->  66.4140625 ; top5 ->  85.265625  and loss:  287.8131490945816
test acc: top1 ->  69.138 ; top5 ->  88.806  and loss:  1000.3769986927509
forward train acc: top1 ->  66.328125 ; top5 ->  85.71875  and loss:  285.17173743247986
test acc: top1 ->  69.268 ; top5 ->  88.842  and loss:  999.1385585069656
forward train acc: top1 ->  66.203125 ; top5 ->  85.3046875  and loss:  290.26336801052094
test acc: top1 ->  69.068 ; top5 ->  88.868  and loss:  998.1870141029358
forward train acc: top1 ->  66.5 ; top5 ->  85.40625  and loss:  290.0513758659363
test acc: top1 ->  69.226 ; top5 ->  88.898  and loss:  996.9112223386765
forward train acc: top1 ->  67.140625 ; top5 ->  86.046875  and loss:  279.8131893277168
test acc: top1 ->  69.248 ; top5 ->  88.896  and loss:  994.7602508664131
forward train acc: top1 ->  67.3046875 ; top5 ->  85.9140625  and loss:  276.4437335729599
test acc: top1 ->  69.268 ; top5 ->  88.92  and loss:  994.3548604249954
forward train acc: top1 ->  66.4140625 ; top5 ->  85.8359375  and loss:  283.2663235068321
test acc: top1 ->  69.278 ; top5 ->  88.86  and loss:  996.0108898580074
forward train acc: top1 ->  65.5 ; top5 ->  84.90625  and loss:  294.0839207172394
test acc: top1 ->  69.184 ; top5 ->  88.84  and loss:  996.9230269491673
forward train acc: top1 ->  67.1015625 ; top5 ->  85.7734375  and loss:  280.9004525542259
test acc: top1 ->  69.224 ; top5 ->  88.91  and loss:  995.6418289542198
forward train acc: top1 ->  66.3828125 ; top5 ->  85.2890625  and loss:  287.3717802166939
test acc: top1 ->  69.324 ; top5 ->  88.992  and loss:  993.0719448924065
>>>>>>> reverse layer  24  since performance drop >>>>>>>
==> this epoch:  244 / 256 , inc:  1
---------------- start layer  25  ---------------
adv train loss:  -72.71634292602539 , diff:  72.71634292602539
adv train loss:  -73.58750426769257 , diff:  0.8711613416671753
************ all values are small in this layer **********
layer  25  adv train finish, try to retain  241
test acc: top1 ->  69.136 ; top5 ->  88.838  and loss:  1005.5824110507965
forward train acc: top1 ->  66.1796875 ; top5 ->  85.65625  and loss:  286.38111037015915
test acc: top1 ->  69.17 ; top5 ->  88.906  and loss:  999.1956462860107
forward train acc: top1 ->  66.3046875 ; top5 ->  85.53125  and loss:  287.075031876564
test acc: top1 ->  69.208 ; top5 ->  88.86  and loss:  1000.1516069769859
forward train acc: top1 ->  66.25 ; top5 ->  85.3671875  and loss:  286.538469851017
test acc: top1 ->  69.236 ; top5 ->  88.952  and loss:  998.8627701997757
forward train acc: top1 ->  66.2109375 ; top5 ->  85.1796875  and loss:  290.13873225450516
test acc: top1 ->  69.192 ; top5 ->  88.852  and loss:  999.5433225631714
forward train acc: top1 ->  66.6328125 ; top5 ->  85.2578125  and loss:  286.75792294740677
test acc: top1 ->  69.306 ; top5 ->  88.938  and loss:  996.2003106474876
forward train acc: top1 ->  67.0 ; top5 ->  86.0859375  and loss:  280.81448644399643
test acc: top1 ->  69.346 ; top5 ->  88.91  and loss:  999.384833753109
forward train acc: top1 ->  67.015625 ; top5 ->  86.1953125  and loss:  277.9762725830078
test acc: top1 ->  69.398 ; top5 ->  88.982  and loss:  994.1418810486794
forward train acc: top1 ->  66.3046875 ; top5 ->  85.3046875  and loss:  286.4301036000252
test acc: top1 ->  69.324 ; top5 ->  89.042  and loss:  996.6189040541649
forward train acc: top1 ->  66.4453125 ; top5 ->  85.984375  and loss:  282.3281587958336
test acc: top1 ->  69.346 ; top5 ->  88.938  and loss:  996.8134259581566
forward train acc: top1 ->  66.6171875 ; top5 ->  85.6328125  and loss:  284.2121253013611
test acc: top1 ->  69.38 ; top5 ->  88.956  and loss:  992.3236210346222
>>>>>>> reverse layer  25  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  26  ---------------
### skip layer  26 wait:  4  ###
---------------- start layer  27  ---------------
### skip layer  27 wait:  4  ###
---------------- start layer  28  ---------------
adv train loss:  -71.184157371521 , diff:  71.184157371521
adv train loss:  -71.41475957632065 , diff:  0.2306022047996521
************ all values are small in this layer **********
layer  28  adv train finish, try to retain  502
test acc: top1 ->  69.394 ; top5 ->  89.024  and loss:  994.1625407636166
forward train acc: top1 ->  65.8671875 ; top5 ->  85.453125  and loss:  288.304154753685
test acc: top1 ->  69.324 ; top5 ->  88.986  and loss:  999.3821021914482
forward train acc: top1 ->  66.4921875 ; top5 ->  85.640625  and loss:  287.8130733370781
test acc: top1 ->  69.342 ; top5 ->  88.99  and loss:  996.8436906337738
forward train acc: top1 ->  65.671875 ; top5 ->  85.3515625  and loss:  291.2407509088516
test acc: top1 ->  69.08 ; top5 ->  88.898  and loss:  1000.5734965503216
forward train acc: top1 ->  66.1953125 ; top5 ->  85.84375  and loss:  287.691053211689
test acc: top1 ->  69.164 ; top5 ->  88.912  and loss:  996.9212648272514
forward train acc: top1 ->  66.71875 ; top5 ->  85.8046875  and loss:  284.784851372242
test acc: top1 ->  69.156 ; top5 ->  88.896  and loss:  999.0263550877571
forward train acc: top1 ->  66.296875 ; top5 ->  85.828125  and loss:  281.16541147232056
test acc: top1 ->  69.32 ; top5 ->  88.98  and loss:  995.923350661993
forward train acc: top1 ->  67.125 ; top5 ->  86.046875  and loss:  277.79022097587585
test acc: top1 ->  69.328 ; top5 ->  88.892  and loss:  997.5183155834675
forward train acc: top1 ->  66.359375 ; top5 ->  85.6875  and loss:  285.46855783462524
test acc: top1 ->  69.364 ; top5 ->  88.956  and loss:  996.4447448253632
forward train acc: top1 ->  66.6796875 ; top5 ->  85.75  and loss:  286.1064051389694
test acc: top1 ->  69.32 ; top5 ->  89.054  and loss:  993.5878325998783
forward train acc: top1 ->  66.8125 ; top5 ->  85.3515625  and loss:  285.5211284160614
test acc: top1 ->  69.348 ; top5 ->  89.014  and loss:  996.5337512791157
>>>>>>> reverse layer  28  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  29  ---------------
adv train loss:  -73.10278713703156 , diff:  73.10278713703156
adv train loss:  -69.6483822464943 , diff:  3.454404890537262
************ all values are small in this layer **********
layer  29  adv train finish, try to retain  497
test acc: top1 ->  69.332 ; top5 ->  88.986  and loss:  994.4509692788124
forward train acc: top1 ->  66.2265625 ; top5 ->  85.96875  and loss:  283.2249460220337
test acc: top1 ->  69.278 ; top5 ->  88.902  and loss:  999.2408410310745
forward train acc: top1 ->  66.734375 ; top5 ->  85.9609375  and loss:  283.2504886984825
test acc: top1 ->  69.248 ; top5 ->  89.022  and loss:  996.5180813968182
forward train acc: top1 ->  66.0859375 ; top5 ->  85.0703125  and loss:  289.1891129016876
test acc: top1 ->  69.2 ; top5 ->  88.988  and loss:  997.0885389447212
forward train acc: top1 ->  65.9921875 ; top5 ->  85.7265625  and loss:  287.01834881305695
test acc: top1 ->  69.23 ; top5 ->  88.966  and loss:  993.905909538269
forward train acc: top1 ->  66.8515625 ; top5 ->  85.9765625  and loss:  281.6400107741356
test acc: top1 ->  69.214 ; top5 ->  88.926  and loss:  997.7954060435295
forward train acc: top1 ->  65.9765625 ; top5 ->  85.703125  and loss:  287.9851571917534
test acc: top1 ->  69.232 ; top5 ->  89.046  and loss:  993.784689605236
forward train acc: top1 ->  66.6875 ; top5 ->  85.8203125  and loss:  281.85659551620483
test acc: top1 ->  69.276 ; top5 ->  89.022  and loss:  995.2928195893764
forward train acc: top1 ->  66.4921875 ; top5 ->  85.6875  and loss:  284.3908953666687
test acc: top1 ->  69.252 ; top5 ->  89.008  and loss:  993.874333679676
forward train acc: top1 ->  66.1171875 ; top5 ->  85.796875  and loss:  285.9273276925087
test acc: top1 ->  69.356 ; top5 ->  89.068  and loss:  992.9146947264671
forward train acc: top1 ->  66.0078125 ; top5 ->  85.328125  and loss:  286.8166687488556
test acc: top1 ->  69.304 ; top5 ->  88.99  and loss:  994.1563053429127
>>>>>>> reverse layer  29  since performance drop >>>>>>>
==> this epoch:  498 / 512 , inc:  1
---------------- start layer  30  ---------------
### skip layer  30 wait:  4  ###
---------------- start layer  31  ---------------
adv train loss:  -69.32740497589111 , diff:  69.32740497589111
adv train loss:  -71.13336819410324 , diff:  1.8059632182121277
************ all values are small in this layer **********
layer  31  adv train finish, try to retain  500
test acc: top1 ->  68.846 ; top5 ->  88.788  and loss:  1009.4692885577679
forward train acc: top1 ->  65.5546875 ; top5 ->  84.75  and loss:  295.7845767736435
test acc: top1 ->  68.906 ; top5 ->  88.902  and loss:  1003.0396298766136
forward train acc: top1 ->  66.1171875 ; top5 ->  85.421875  and loss:  290.8708696961403
test acc: top1 ->  68.988 ; top5 ->  88.864  and loss:  1002.0340331196785
forward train acc: top1 ->  65.8828125 ; top5 ->  85.2109375  and loss:  292.85052824020386
test acc: top1 ->  69.176 ; top5 ->  88.958  and loss:  1000.7353599667549
forward train acc: top1 ->  66.5390625 ; top5 ->  85.9921875  and loss:  279.0851750969887
test acc: top1 ->  69.162 ; top5 ->  89.002  and loss:  997.9093463122845
forward train acc: top1 ->  66.0 ; top5 ->  85.4609375  and loss:  289.1555271744728
test acc: top1 ->  69.194 ; top5 ->  88.918  and loss:  998.9600868523121
forward train acc: top1 ->  66.8984375 ; top5 ->  85.8515625  and loss:  280.4081310033798
test acc: top1 ->  69.162 ; top5 ->  88.976  and loss:  997.1944938004017
forward train acc: top1 ->  66.1640625 ; top5 ->  85.703125  and loss:  287.199298620224
test acc: top1 ->  69.356 ; top5 ->  88.986  and loss:  993.6907924115658
forward train acc: top1 ->  65.515625 ; top5 ->  85.546875  and loss:  291.55217975378036
test acc: top1 ->  69.408 ; top5 ->  89.024  and loss:  992.4798702895641
forward train acc: top1 ->  66.4140625 ; top5 ->  85.625  and loss:  283.2134856581688
test acc: top1 ->  69.344 ; top5 ->  88.962  and loss:  995.4976541399956
forward train acc: top1 ->  65.9296875 ; top5 ->  84.984375  and loss:  290.3947476744652
test acc: top1 ->  69.358 ; top5 ->  89.022  and loss:  994.6030131280422
>>>>>>> reverse layer  31  since performance drop >>>>>>>
==> this epoch:  501 / 512 , inc:  1
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.005, 0.005, 0.000703125, 0.005, 0.005, 0.005, 0.0009375, 0.00017578125, 0.0025, 0.00017578125, 0.0025, 0.0025, 0.0025, 0.0025, 8.7890625e-05, 8.7890625e-05, 0.00017578125, 0.00046875, 0.00046875, 0.00125, 0.00046875, 0.00017578125, 0.00125, 0.00125, 0.00017578125, 0.00017578125, 4.39453125e-05, 4.39453125e-05, 8.7890625e-05, 8.7890625e-05, 4.39453125e-05, 8.7890625e-05]  wait [0, 0, 4, 0, 0, 0, 2, 3, 0, 3, 0, 0, 0, 0, 3, 3, 4, 2, 2, 0, 2, 4, 0, 0, 4, 4, 3, 3, 4, 4, 3, 4]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 0
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  5  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -69.7944016456604 , diff:  69.7944016456604
adv train loss:  -69.92421042919159 , diff:  0.12980878353118896
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  36
test acc: top1 ->  59.316 ; top5 ->  81.706  and loss:  1385.5530622005463
forward train acc: top1 ->  65.484375 ; top5 ->  85.375  and loss:  295.7048432826996
test acc: top1 ->  68.684 ; top5 ->  88.562  and loss:  1015.4159890413284
forward train acc: top1 ->  65.796875 ; top5 ->  85.3125  and loss:  293.09309136867523
test acc: top1 ->  68.748 ; top5 ->  88.71  and loss:  1011.1725235581398
forward train acc: top1 ->  65.671875 ; top5 ->  85.515625  and loss:  288.5191298723221
test acc: top1 ->  68.904 ; top5 ->  88.782  and loss:  1008.7774267792702
forward train acc: top1 ->  65.734375 ; top5 ->  85.4375  and loss:  294.6294239759445
test acc: top1 ->  68.96 ; top5 ->  88.8  and loss:  1007.5689252018929
forward train acc: top1 ->  66.40625 ; top5 ->  85.4609375  and loss:  288.54351752996445
test acc: top1 ->  69.018 ; top5 ->  88.806  and loss:  1006.3769654631615
forward train acc: top1 ->  65.9921875 ; top5 ->  85.453125  and loss:  286.01809227466583
test acc: top1 ->  68.996 ; top5 ->  88.832  and loss:  1003.2844031751156
forward train acc: top1 ->  66.2578125 ; top5 ->  85.15625  and loss:  289.60107147693634
test acc: top1 ->  69.14 ; top5 ->  88.806  and loss:  1003.7436413764954
forward train acc: top1 ->  66.0078125 ; top5 ->  85.09375  and loss:  290.93360936641693
test acc: top1 ->  69.118 ; top5 ->  88.864  and loss:  1002.0322008132935
forward train acc: top1 ->  66.0 ; top5 ->  85.4375  and loss:  286.8493267297745
test acc: top1 ->  69.146 ; top5 ->  88.902  and loss:  1000.334384649992
forward train acc: top1 ->  67.234375 ; top5 ->  85.40625  and loss:  284.39023649692535
test acc: top1 ->  69.188 ; top5 ->  88.896  and loss:  999.4991811215878
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  37 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -75.2059201002121 , diff:  75.2059201002121
adv train loss:  -73.92804455757141 , diff:  1.277875542640686
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  43
test acc: top1 ->  64.0 ; top5 ->  85.46  and loss:  1180.8411746621132
forward train acc: top1 ->  65.7421875 ; top5 ->  85.2890625  and loss:  295.78367298841476
test acc: top1 ->  68.828 ; top5 ->  88.7  and loss:  1008.3646318912506
forward train acc: top1 ->  66.21875 ; top5 ->  85.5625  and loss:  288.0544609427452
test acc: top1 ->  68.99 ; top5 ->  88.78  and loss:  1005.5417348742485
forward train acc: top1 ->  65.9453125 ; top5 ->  85.53125  and loss:  287.8529798388481
test acc: top1 ->  69.026 ; top5 ->  88.724  and loss:  1007.5186886191368
forward train acc: top1 ->  66.671875 ; top5 ->  85.8046875  and loss:  284.01562106609344
test acc: top1 ->  69.178 ; top5 ->  88.842  and loss:  1003.5223540663719
forward train acc: top1 ->  67.1796875 ; top5 ->  85.8828125  and loss:  278.2324661016464
test acc: top1 ->  69.286 ; top5 ->  88.816  and loss:  1000.0420041382313
forward train acc: top1 ->  66.3828125 ; top5 ->  85.828125  and loss:  284.1340185403824
test acc: top1 ->  69.246 ; top5 ->  88.888  and loss:  998.1162594556808
forward train acc: top1 ->  65.953125 ; top5 ->  85.7109375  and loss:  287.04828530550003
test acc: top1 ->  69.212 ; top5 ->  88.856  and loss:  1000.936732262373
forward train acc: top1 ->  66.84375 ; top5 ->  85.5625  and loss:  284.48387384414673
test acc: top1 ->  69.326 ; top5 ->  88.894  and loss:  997.7925073206425
forward train acc: top1 ->  66.484375 ; top5 ->  85.6796875  and loss:  284.1708745956421
test acc: top1 ->  69.306 ; top5 ->  88.876  and loss:  996.8704723119736
forward train acc: top1 ->  66.1484375 ; top5 ->  85.546875  and loss:  288.0093910098076
test acc: top1 ->  69.354 ; top5 ->  88.85  and loss:  999.4445201456547
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  44 / 64 , inc:  1
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
adv train loss:  -70.48694425821304 , diff:  70.48694425821304
adv train loss:  -73.22960042953491 , diff:  2.742656171321869
************ all values are small in this layer **********
layer  3  adv train finish, try to retain  42
test acc: top1 ->  65.592 ; top5 ->  86.71  and loss:  1120.1944380998611
forward train acc: top1 ->  66.3125 ; top5 ->  85.796875  and loss:  289.1880220770836
test acc: top1 ->  68.778 ; top5 ->  88.758  and loss:  1012.8830225765705
forward train acc: top1 ->  66.1796875 ; top5 ->  85.6875  and loss:  285.4943284392357
test acc: top1 ->  68.944 ; top5 ->  88.75  and loss:  1013.1851778924465
forward train acc: top1 ->  66.140625 ; top5 ->  84.9453125  and loss:  290.50762408971786
test acc: top1 ->  68.998 ; top5 ->  88.828  and loss:  1009.5039983093739
forward train acc: top1 ->  66.8515625 ; top5 ->  85.5546875  and loss:  286.9992822408676
test acc: top1 ->  69.014 ; top5 ->  88.84  and loss:  1006.3894256055355
forward train acc: top1 ->  65.953125 ; top5 ->  85.6875  and loss:  286.3316992521286
test acc: top1 ->  69.022 ; top5 ->  88.85  and loss:  1006.6615791618824
forward train acc: top1 ->  66.765625 ; top5 ->  86.21875  and loss:  280.3215420246124
test acc: top1 ->  69.108 ; top5 ->  88.826  and loss:  1006.1391194462776
forward train acc: top1 ->  66.3359375 ; top5 ->  85.25  and loss:  286.50463849306107
test acc: top1 ->  69.064 ; top5 ->  88.87  and loss:  1006.8926504254341
forward train acc: top1 ->  66.1953125 ; top5 ->  85.453125  and loss:  288.6376219391823
test acc: top1 ->  69.202 ; top5 ->  88.928  and loss:  1003.6821466982365
forward train acc: top1 ->  66.6796875 ; top5 ->  86.0078125  and loss:  280.7199645638466
test acc: top1 ->  69.184 ; top5 ->  88.944  and loss:  1001.0372541248798
forward train acc: top1 ->  66.03125 ; top5 ->  85.6875  and loss:  285.56895089149475
test acc: top1 ->  69.232 ; top5 ->  88.956  and loss:  1001.6144406795502
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  43 / 64 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -72.9895630478859 , diff:  72.9895630478859
adv train loss:  -72.48388302326202 , diff:  0.5056800246238708
************ all values are small in this layer **********
layer  4  adv train finish, try to retain  39
test acc: top1 ->  66.84 ; top5 ->  87.332  and loss:  1080.593305528164
forward train acc: top1 ->  66.1640625 ; top5 ->  85.46875  and loss:  287.30867540836334
test acc: top1 ->  69.158 ; top5 ->  88.776  and loss:  1006.2896082401276
forward train acc: top1 ->  66.171875 ; top5 ->  85.296875  and loss:  284.4471299648285
test acc: top1 ->  69.256 ; top5 ->  88.846  and loss:  1001.7766721844673
forward train acc: top1 ->  66.0390625 ; top5 ->  85.8359375  and loss:  286.6328910589218
test acc: top1 ->  69.28 ; top5 ->  88.902  and loss:  998.0007720589638
forward train acc: top1 ->  66.21875 ; top5 ->  85.4609375  and loss:  289.6425216794014
test acc: top1 ->  69.29 ; top5 ->  88.89  and loss:  1000.0888049304485
forward train acc: top1 ->  66.6640625 ; top5 ->  85.8515625  and loss:  282.1501907110214
test acc: top1 ->  69.416 ; top5 ->  88.948  and loss:  996.9803200960159
forward train acc: top1 ->  66.2421875 ; top5 ->  85.6484375  and loss:  288.55402433872223
test acc: top1 ->  69.36 ; top5 ->  88.944  and loss:  1000.0968105494976
forward train acc: top1 ->  66.1640625 ; top5 ->  85.3203125  and loss:  286.7654480934143
test acc: top1 ->  69.482 ; top5 ->  88.93  and loss:  996.1055777072906
forward train acc: top1 ->  67.0390625 ; top5 ->  85.671875  and loss:  281.5733248591423
test acc: top1 ->  69.47 ; top5 ->  88.976  and loss:  991.9948694109917
forward train acc: top1 ->  66.2890625 ; top5 ->  85.5859375  and loss:  285.7560797929764
test acc: top1 ->  69.49 ; top5 ->  88.972  and loss:  992.0025087594986
forward train acc: top1 ->  67.5 ; top5 ->  86.34375  and loss:  278.2499333024025
test acc: top1 ->  69.518 ; top5 ->  88.96  and loss:  993.7821165323257
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  40 / 64 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -69.6228061914444 , diff:  69.6228061914444
adv train loss:  -73.64758056402206 , diff:  4.024774372577667
************ all values are small in this layer **********
layer  5  adv train finish, try to retain  28
test acc: top1 ->  65.45 ; top5 ->  86.338  and loss:  1135.3339157104492
forward train acc: top1 ->  66.375 ; top5 ->  85.8046875  and loss:  283.002561211586
test acc: top1 ->  68.92 ; top5 ->  88.714  and loss:  1006.2841092646122
forward train acc: top1 ->  66.578125 ; top5 ->  85.8515625  and loss:  284.25855028629303
test acc: top1 ->  69.046 ; top5 ->  88.788  and loss:  1001.1612539291382
forward train acc: top1 ->  67.0234375 ; top5 ->  85.390625  and loss:  283.1934681534767
test acc: top1 ->  69.052 ; top5 ->  88.818  and loss:  1003.120397746563
forward train acc: top1 ->  66.9140625 ; top5 ->  86.140625  and loss:  281.18688333034515
test acc: top1 ->  69.176 ; top5 ->  88.796  and loss:  1003.6627200841904
forward train acc: top1 ->  66.8984375 ; top5 ->  85.65625  and loss:  280.7565610408783
test acc: top1 ->  69.14 ; top5 ->  88.892  and loss:  998.5828855037689
forward train acc: top1 ->  66.9453125 ; top5 ->  85.84375  and loss:  284.12979996204376
test acc: top1 ->  69.206 ; top5 ->  88.848  and loss:  1001.0435574948788
forward train acc: top1 ->  66.59375 ; top5 ->  85.65625  and loss:  286.693063557148
test acc: top1 ->  69.252 ; top5 ->  88.886  and loss:  996.4527066648006
forward train acc: top1 ->  66.8125 ; top5 ->  85.7578125  and loss:  282.71378898620605
test acc: top1 ->  69.26 ; top5 ->  88.85  and loss:  998.3839564919472
forward train acc: top1 ->  66.7890625 ; top5 ->  85.7109375  and loss:  283.62037271261215
test acc: top1 ->  69.282 ; top5 ->  88.884  and loss:  996.6929987967014
forward train acc: top1 ->  66.8359375 ; top5 ->  85.6328125  and loss:  280.5007693171501
test acc: top1 ->  69.32 ; top5 ->  88.906  and loss:  995.2123652398586
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  29 / 64 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -70.51101565361023 , diff:  70.51101565361023
adv train loss:  -70.35451298952103 , diff:  0.15650266408920288
layer  6  adv train finish, try to retain  112
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
adv train loss:  -71.08730614185333 , diff:  71.08730614185333
adv train loss:  -71.37666511535645 , diff:  0.2893589735031128
layer  8  adv train finish, try to retain  101
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
### skip layer  9 wait:  3  ###
---------------- start layer  10  ---------------
adv train loss:  -70.24321901798248 , diff:  70.24321901798248
adv train loss:  -72.6757560968399 , diff:  2.432537078857422
layer  10  adv train finish, try to retain  99
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -71.78040528297424 , diff:  71.78040528297424
adv train loss:  -70.60535365343094 , diff:  1.1750516295433044
layer  11  adv train finish, try to retain  97
test acc: top1 ->  68.052 ; top5 ->  88.152  and loss:  1043.7394573688507
forward train acc: top1 ->  66.2265625 ; top5 ->  85.40625  and loss:  286.1856384277344
test acc: top1 ->  69.164 ; top5 ->  88.87  and loss:  1001.0360681116581
forward train acc: top1 ->  66.8046875 ; top5 ->  86.59375  and loss:  276.25674748420715
test acc: top1 ->  69.354 ; top5 ->  88.934  and loss:  997.3219129741192
forward train acc: top1 ->  66.7265625 ; top5 ->  85.1640625  and loss:  287.84411758184433
test acc: top1 ->  69.298 ; top5 ->  88.858  and loss:  999.8910155892372
forward train acc: top1 ->  66.1796875 ; top5 ->  85.6328125  and loss:  287.5844565629959
test acc: top1 ->  69.492 ; top5 ->  88.966  and loss:  996.2379502356052
forward train acc: top1 ->  67.203125 ; top5 ->  86.0  and loss:  281.9326801300049
test acc: top1 ->  69.452 ; top5 ->  88.964  and loss:  994.6064390838146
forward train acc: top1 ->  66.6875 ; top5 ->  85.9453125  and loss:  279.3839001059532
test acc: top1 ->  69.474 ; top5 ->  88.99  and loss:  993.8368926942348
forward train acc: top1 ->  67.28125 ; top5 ->  86.2734375  and loss:  276.12698072195053
test acc: top1 ->  69.492 ; top5 ->  89.022  and loss:  993.12122631073
forward train acc: top1 ->  66.578125 ; top5 ->  85.5  and loss:  286.5938249230385
test acc: top1 ->  69.496 ; top5 ->  89.034  and loss:  991.743710398674
forward train acc: top1 ->  66.75 ; top5 ->  85.7109375  and loss:  283.88138431310654
test acc: top1 ->  69.48 ; top5 ->  89.026  and loss:  991.2627722620964
forward train acc: top1 ->  67.265625 ; top5 ->  86.5078125  and loss:  277.3222097158432
test acc: top1 ->  69.458 ; top5 ->  88.998  and loss:  991.1539390981197
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -73.39553880691528 , diff:  73.39553880691528
adv train loss:  -70.36880910396576 , diff:  3.026729702949524
layer  12  adv train finish, try to retain  96
test acc: top1 ->  68.358 ; top5 ->  88.366  and loss:  1022.0608049035072
forward train acc: top1 ->  66.28125 ; top5 ->  85.2421875  and loss:  287.49885481595993
test acc: top1 ->  69.066 ; top5 ->  88.754  and loss:  1002.3213509619236
forward train acc: top1 ->  65.3046875 ; top5 ->  84.8046875  and loss:  295.35769087076187
test acc: top1 ->  68.992 ; top5 ->  88.826  and loss:  1002.4522388875484
forward train acc: top1 ->  66.5 ; top5 ->  85.53125  and loss:  286.5246885418892
test acc: top1 ->  69.108 ; top5 ->  88.92  and loss:  998.1463046967983
forward train acc: top1 ->  66.4609375 ; top5 ->  85.671875  and loss:  283.6926966905594
test acc: top1 ->  69.128 ; top5 ->  88.944  and loss:  996.75822275877
forward train acc: top1 ->  66.8125 ; top5 ->  86.09375  and loss:  280.9266344308853
test acc: top1 ->  69.142 ; top5 ->  89.038  and loss:  995.2621165812016
forward train acc: top1 ->  66.8359375 ; top5 ->  85.5078125  and loss:  282.1143196821213
test acc: top1 ->  69.1 ; top5 ->  88.966  and loss:  995.0449831485748
forward train acc: top1 ->  67.28125 ; top5 ->  86.2109375  and loss:  279.5192401409149
test acc: top1 ->  69.174 ; top5 ->  88.95  and loss:  994.0032488405704
forward train acc: top1 ->  67.0234375 ; top5 ->  85.578125  and loss:  284.7565286755562
test acc: top1 ->  69.194 ; top5 ->  89.072  and loss:  991.9448524415493
forward train acc: top1 ->  67.0703125 ; top5 ->  85.6171875  and loss:  282.7565543651581
test acc: top1 ->  69.314 ; top5 ->  89.098  and loss:  991.1344176828861
forward train acc: top1 ->  66.484375 ; top5 ->  85.9765625  and loss:  278.8945567011833
test acc: top1 ->  69.272 ; top5 ->  88.992  and loss:  992.1614370048046
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  98 / 128 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -70.77894133329391 , diff:  70.77894133329391
adv train loss:  -70.27043664455414 , diff:  0.5085046887397766
layer  13  adv train finish, try to retain  101
>>>>>>> reverse layer  13  since no improvement >>>>>>>
---------------- start layer  14  ---------------
### skip layer  14 wait:  3  ###
---------------- start layer  15  ---------------
### skip layer  15 wait:  3  ###
---------------- start layer  16  ---------------
### skip layer  16 wait:  4  ###
---------------- start layer  17  ---------------
adv train loss:  -70.75375980138779 , diff:  70.75375980138779
adv train loss:  -70.25584751367569 , diff:  0.49791228771209717
************ all values are small in this layer **********
layer  17  adv train finish, try to retain  229
test acc: top1 ->  69.23 ; top5 ->  89.01  and loss:  996.1336523890495
forward train acc: top1 ->  66.6015625 ; top5 ->  85.3046875  and loss:  288.4527024626732
test acc: top1 ->  69.278 ; top5 ->  89.066  and loss:  992.3332978188992
forward train acc: top1 ->  66.7734375 ; top5 ->  85.8203125  and loss:  282.1039075255394
test acc: top1 ->  69.3 ; top5 ->  89.09  and loss:  993.1855520904064
forward train acc: top1 ->  66.4375 ; top5 ->  85.6953125  and loss:  283.8508546948433
test acc: top1 ->  69.338 ; top5 ->  89.098  and loss:  994.3577152490616
forward train acc: top1 ->  66.3203125 ; top5 ->  85.90625  and loss:  284.49312072992325
test acc: top1 ->  69.374 ; top5 ->  89.108  and loss:  990.1654928922653
forward train acc: top1 ->  66.28125 ; top5 ->  86.0  and loss:  284.9950315952301
test acc: top1 ->  69.572 ; top5 ->  89.118  and loss:  989.5518558621407
forward train acc: top1 ->  67.1953125 ; top5 ->  86.4296875  and loss:  276.2389625310898
test acc: top1 ->  69.432 ; top5 ->  89.112  and loss:  991.7269700169563
forward train acc: top1 ->  66.765625 ; top5 ->  85.8203125  and loss:  278.50231939554214
test acc: top1 ->  69.482 ; top5 ->  89.126  and loss:  990.161397755146
forward train acc: top1 ->  66.8984375 ; top5 ->  86.171875  and loss:  281.96623665094376
test acc: top1 ->  69.532 ; top5 ->  89.166  and loss:  989.0362921953201
forward train acc: top1 ->  66.1796875 ; top5 ->  85.640625  and loss:  285.83845269680023
test acc: top1 ->  69.406 ; top5 ->  89.068  and loss:  991.3263679742813
forward train acc: top1 ->  66.75 ; top5 ->  86.078125  and loss:  280.1873939037323
test acc: top1 ->  69.504 ; top5 ->  89.152  and loss:  988.8185800611973
>>>>>>> reverse layer  17  since performance drop >>>>>>>
==> this epoch:  230 / 256 , inc:  1
---------------- start layer  18  ---------------
adv train loss:  -68.61824268102646 , diff:  68.61824268102646
adv train loss:  -72.29219377040863 , diff:  3.6739510893821716
************ all values are small in this layer **********
layer  18  adv train finish, try to retain  234
test acc: top1 ->  69.31 ; top5 ->  89.09  and loss:  987.9973802864552
forward train acc: top1 ->  67.109375 ; top5 ->  85.5078125  and loss:  281.96692675352097
test acc: top1 ->  69.518 ; top5 ->  89.062  and loss:  988.9004535377026
forward train acc: top1 ->  66.2265625 ; top5 ->  85.5078125  and loss:  283.6439761519432
test acc: top1 ->  69.458 ; top5 ->  89.102  and loss:  991.0747494697571
forward train acc: top1 ->  66.546875 ; top5 ->  85.671875  and loss:  285.58024603128433
test acc: top1 ->  69.446 ; top5 ->  89.052  and loss:  989.0736767351627
forward train acc: top1 ->  66.7734375 ; top5 ->  85.6953125  and loss:  281.12210869789124
test acc: top1 ->  69.598 ; top5 ->  89.116  and loss:  986.6716868877411
forward train acc: top1 ->  66.75 ; top5 ->  85.7890625  and loss:  282.95560425519943
test acc: top1 ->  69.458 ; top5 ->  89.06  and loss:  988.5256499052048
forward train acc: top1 ->  66.140625 ; top5 ->  85.3125  and loss:  286.26831793785095
test acc: top1 ->  69.604 ; top5 ->  89.182  and loss:  986.1101478934288
forward train acc: top1 ->  66.8359375 ; top5 ->  85.453125  and loss:  282.6096577644348
test acc: top1 ->  69.67 ; top5 ->  89.122  and loss:  983.7878645658493
forward train acc: top1 ->  67.5 ; top5 ->  86.078125  and loss:  279.27833050489426
test acc: top1 ->  69.528 ; top5 ->  89.138  and loss:  986.5506928563118
forward train acc: top1 ->  67.078125 ; top5 ->  86.1796875  and loss:  280.2120035290718
test acc: top1 ->  69.602 ; top5 ->  89.158  and loss:  985.9646623134613
forward train acc: top1 ->  66.9921875 ; top5 ->  85.6328125  and loss:  285.54618644714355
test acc: top1 ->  69.716 ; top5 ->  89.186  and loss:  984.8755137324333
>>>>>>> reverse layer  18  since performance drop >>>>>>>
==> this epoch:  235 / 256 , inc:  1
---------------- start layer  19  ---------------
adv train loss:  -71.62896007299423 , diff:  71.62896007299423
adv train loss:  -69.9710003733635 , diff:  1.6579596996307373
layer  19  adv train finish, try to retain  216
>>>>>>> reverse layer  19  since no improvement >>>>>>>
---------------- start layer  20  ---------------
adv train loss:  -71.47525465488434 , diff:  71.47525465488434
adv train loss:  -69.76917690038681 , diff:  1.706077754497528
************ all values are small in this layer **********
layer  20  adv train finish, try to retain  224
test acc: top1 ->  69.626 ; top5 ->  89.092  and loss:  987.8633571267128
forward train acc: top1 ->  67.0546875 ; top5 ->  85.5234375  and loss:  287.309810757637
test acc: top1 ->  69.508 ; top5 ->  89.052  and loss:  990.2036202251911
forward train acc: top1 ->  66.4140625 ; top5 ->  85.8125  and loss:  282.8864400982857
test acc: top1 ->  69.48 ; top5 ->  88.938  and loss:  991.2282351851463
forward train acc: top1 ->  66.6796875 ; top5 ->  85.171875  and loss:  286.94736927747726
test acc: top1 ->  69.496 ; top5 ->  88.93  and loss:  990.7875534296036
forward train acc: top1 ->  66.7109375 ; top5 ->  85.9921875  and loss:  283.5383294224739
test acc: top1 ->  69.46 ; top5 ->  88.982  and loss:  990.4591457247734
forward train acc: top1 ->  67.0859375 ; top5 ->  86.1171875  and loss:  279.4649322628975
test acc: top1 ->  69.542 ; top5 ->  88.93  and loss:  989.167513012886
forward train acc: top1 ->  67.015625 ; top5 ->  85.5  and loss:  282.37024450302124
test acc: top1 ->  69.548 ; top5 ->  89.014  and loss:  990.4325330555439
forward train acc: top1 ->  66.5390625 ; top5 ->  85.84375  and loss:  283.00310534238815
test acc: top1 ->  69.47 ; top5 ->  89.026  and loss:  988.930484443903
forward train acc: top1 ->  66.7578125 ; top5 ->  86.0859375  and loss:  281.6132964491844
test acc: top1 ->  69.646 ; top5 ->  89.09  and loss:  986.4766804873943
forward train acc: top1 ->  66.4296875 ; top5 ->  86.0390625  and loss:  281.6573761701584
test acc: top1 ->  69.586 ; top5 ->  89.084  and loss:  986.694269746542
forward train acc: top1 ->  66.921875 ; top5 ->  85.6328125  and loss:  279.6023308634758
test acc: top1 ->  69.546 ; top5 ->  89.008  and loss:  988.8694503307343
>>>>>>> reverse layer  20  since performance drop >>>>>>>
==> this epoch:  225 / 256 , inc:  1
---------------- start layer  21  ---------------
### skip layer  21 wait:  4  ###
---------------- start layer  22  ---------------
adv train loss:  -70.09536474943161 , diff:  70.09536474943161
adv train loss:  -67.73583316802979 , diff:  2.359531581401825
layer  22  adv train finish, try to retain  221
>>>>>>> reverse layer  22  since no improvement >>>>>>>
---------------- start layer  23  ---------------
adv train loss:  -70.06410229206085 , diff:  70.06410229206085
adv train loss:  -66.8459700345993 , diff:  3.218132257461548
layer  23  adv train finish, try to retain  213
>>>>>>> reverse layer  23  since no improvement >>>>>>>
---------------- start layer  24  ---------------
### skip layer  24 wait:  4  ###
---------------- start layer  25  ---------------
### skip layer  25 wait:  4  ###
---------------- start layer  26  ---------------
### skip layer  26 wait:  3  ###
---------------- start layer  27  ---------------
### skip layer  27 wait:  3  ###
---------------- start layer  28  ---------------
### skip layer  28 wait:  4  ###
---------------- start layer  29  ---------------
### skip layer  29 wait:  4  ###
---------------- start layer  30  ---------------
### skip layer  30 wait:  3  ###
---------------- start layer  31  ---------------
### skip layer  31 wait:  4  ###
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.00375, 0.00375, 0.000703125, 0.00375, 0.00375, 0.00375, 0.001875, 0.00017578125, 0.005, 0.00017578125, 0.005, 0.001875, 0.001875, 0.005, 8.7890625e-05, 8.7890625e-05, 0.00017578125, 0.0003515625, 0.0003515625, 0.0025, 0.0003515625, 0.00017578125, 0.0025, 0.0025, 0.00017578125, 0.00017578125, 4.39453125e-05, 4.39453125e-05, 8.7890625e-05, 8.7890625e-05, 4.39453125e-05, 8.7890625e-05]  wait [2, 2, 3, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 0, 2, 2, 3, 4, 4, 0, 4, 3, 0, 0, 3, 3, 2, 2, 3, 3, 2, 3]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 0
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  6  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -71.13987421989441 , diff:  71.13987421989441
adv train loss:  -70.57737255096436 , diff:  0.5625016689300537
layer  0  adv train finish, try to retain  46
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -72.5983476638794 , diff:  72.5983476638794
adv train loss:  -72.5272890329361 , diff:  0.07105863094329834
layer  1  adv train finish, try to retain  54
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
adv train loss:  -71.67770731449127 , diff:  71.67770731449127
adv train loss:  -70.57559961080551 , diff:  1.1021077036857605
layer  3  adv train finish, try to retain  56
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -70.33673840761185 , diff:  70.33673840761185
adv train loss:  -66.93864750862122 , diff:  3.398090898990631
layer  4  adv train finish, try to retain  43
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -70.46368932723999 , diff:  70.46368932723999
adv train loss:  -69.58757042884827 , diff:  0.8761188983917236
layer  5  adv train finish, try to retain  51
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -67.70035779476166 , diff:  67.70035779476166
adv train loss:  -69.37225711345673 , diff:  1.6718993186950684
layer  6  adv train finish, try to retain  106
test acc: top1 ->  67.614 ; top5 ->  87.988  and loss:  1049.6425105929375
forward train acc: top1 ->  66.328125 ; top5 ->  85.75  and loss:  283.3852987885475
test acc: top1 ->  69.206 ; top5 ->  88.874  and loss:  997.6956904828548
forward train acc: top1 ->  66.8359375 ; top5 ->  85.6875  and loss:  284.4234555363655
test acc: top1 ->  69.264 ; top5 ->  88.978  and loss:  995.4214895963669
forward train acc: top1 ->  66.71875 ; top5 ->  85.7890625  and loss:  281.3026631474495
test acc: top1 ->  69.15 ; top5 ->  88.904  and loss:  996.4228484034538
forward train acc: top1 ->  66.5625 ; top5 ->  85.890625  and loss:  281.7125304341316
test acc: top1 ->  69.388 ; top5 ->  88.972  and loss:  994.1219512224197
forward train acc: top1 ->  66.34375 ; top5 ->  86.0234375  and loss:  282.932728767395
test acc: top1 ->  69.352 ; top5 ->  88.95  and loss:  990.5087823867798
forward train acc: top1 ->  66.5703125 ; top5 ->  85.4296875  and loss:  287.29462492465973
test acc: top1 ->  69.402 ; top5 ->  88.94  and loss:  991.8792910277843
forward train acc: top1 ->  67.859375 ; top5 ->  86.234375  and loss:  276.11468559503555
test acc: top1 ->  69.392 ; top5 ->  88.906  and loss:  992.8207890987396
forward train acc: top1 ->  66.8515625 ; top5 ->  85.9453125  and loss:  281.35654562711716
test acc: top1 ->  69.318 ; top5 ->  88.918  and loss:  991.7702913284302
forward train acc: top1 ->  66.2578125 ; top5 ->  85.5  and loss:  284.19201946258545
test acc: top1 ->  69.438 ; top5 ->  88.944  and loss:  989.5069517493248
forward train acc: top1 ->  67.09375 ; top5 ->  85.828125  and loss:  279.00082141160965
test acc: top1 ->  69.448 ; top5 ->  88.984  and loss:  990.6786209046841
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  112 / 128 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -66.86947691440582 , diff:  66.86947691440582
adv train loss:  -68.80252432823181 , diff:  1.9330474138259888
layer  7  adv train finish, try to retain  123
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -70.34931969642639 , diff:  70.34931969642639
adv train loss:  -68.13450866937637 , diff:  2.2148110270500183
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  86
test acc: top1 ->  67.618 ; top5 ->  88.014  and loss:  1051.3366379737854
forward train acc: top1 ->  65.875 ; top5 ->  85.015625  and loss:  290.98255133628845
test acc: top1 ->  69.14 ; top5 ->  88.858  and loss:  999.7826581597328
forward train acc: top1 ->  66.1640625 ; top5 ->  85.40625  and loss:  286.9404159784317
test acc: top1 ->  69.176 ; top5 ->  88.928  and loss:  998.2945449352264
forward train acc: top1 ->  66.65625 ; top5 ->  85.8828125  and loss:  281.45598816871643
test acc: top1 ->  69.24 ; top5 ->  88.972  and loss:  996.6546492576599
forward train acc: top1 ->  66.4765625 ; top5 ->  85.671875  and loss:  283.98264080286026
test acc: top1 ->  69.272 ; top5 ->  88.97  and loss:  994.0216062664986
forward train acc: top1 ->  66.34375 ; top5 ->  85.6796875  and loss:  284.93105405569077
test acc: top1 ->  69.328 ; top5 ->  88.954  and loss:  994.4443372488022
forward train acc: top1 ->  66.9765625 ; top5 ->  85.6484375  and loss:  280.3747048377991
test acc: top1 ->  69.354 ; top5 ->  88.96  and loss:  993.7369059920311
forward train acc: top1 ->  66.7578125 ; top5 ->  85.796875  and loss:  278.9795029759407
test acc: top1 ->  69.33 ; top5 ->  88.976  and loss:  993.5121849775314
forward train acc: top1 ->  67.3359375 ; top5 ->  85.9765625  and loss:  280.0118591785431
test acc: top1 ->  69.466 ; top5 ->  88.97  and loss:  991.1250631213188
forward train acc: top1 ->  66.359375 ; top5 ->  86.015625  and loss:  285.13580495119095
test acc: top1 ->  69.398 ; top5 ->  89.02  and loss:  992.3419868946075
forward train acc: top1 ->  66.3125 ; top5 ->  85.5546875  and loss:  282.43348413705826
test acc: top1 ->  69.358 ; top5 ->  88.966  and loss:  992.8936978280544
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -70.3714736700058 , diff:  70.3714736700058
adv train loss:  -68.64845496416092 , diff:  1.7230187058448792
layer  9  adv train finish, try to retain  121
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -70.3857604265213 , diff:  70.3857604265213
adv train loss:  -70.26566219329834 , diff:  0.12009823322296143
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  86
test acc: top1 ->  68.558 ; top5 ->  88.626  and loss:  1017.2798163890839
forward train acc: top1 ->  67.0859375 ; top5 ->  85.7890625  and loss:  280.84213757514954
test acc: top1 ->  69.276 ; top5 ->  88.97  and loss:  997.4596194922924
forward train acc: top1 ->  66.359375 ; top5 ->  85.40625  and loss:  287.3365536928177
test acc: top1 ->  69.194 ; top5 ->  89.03  and loss:  999.4302334189415
forward train acc: top1 ->  66.703125 ; top5 ->  85.5546875  and loss:  281.7094929218292
test acc: top1 ->  69.324 ; top5 ->  88.946  and loss:  998.1218078136444
forward train acc: top1 ->  66.0078125 ; top5 ->  85.4453125  and loss:  290.3013277053833
test acc: top1 ->  69.342 ; top5 ->  89.016  and loss:  995.0378728806973
forward train acc: top1 ->  67.1328125 ; top5 ->  86.0390625  and loss:  279.2903373837471
test acc: top1 ->  69.338 ; top5 ->  89.032  and loss:  996.2121622264385
forward train acc: top1 ->  66.734375 ; top5 ->  85.796875  and loss:  280.84672379493713
test acc: top1 ->  69.226 ; top5 ->  89.014  and loss:  994.2128102183342
forward train acc: top1 ->  67.53125 ; top5 ->  85.390625  and loss:  281.74530351161957
test acc: top1 ->  69.388 ; top5 ->  88.996  and loss:  993.1093685925007
forward train acc: top1 ->  66.1953125 ; top5 ->  85.6015625  and loss:  282.6088846921921
test acc: top1 ->  69.454 ; top5 ->  88.996  and loss:  989.7865638136864
forward train acc: top1 ->  66.4921875 ; top5 ->  86.28125  and loss:  278.68068677186966
test acc: top1 ->  69.386 ; top5 ->  89.028  and loss:  990.8943857848644
forward train acc: top1 ->  66.4921875 ; top5 ->  85.3828125  and loss:  288.7099475264549
test acc: top1 ->  69.456 ; top5 ->  89.09  and loss:  988.2790468335152
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -71.26947605609894 , diff:  71.26947605609894
adv train loss:  -71.36894994974136 , diff:  0.09947389364242554
layer  11  adv train finish, try to retain  100
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -71.03719413280487 , diff:  71.03719413280487
adv train loss:  -69.82557237148285 , diff:  1.2116217613220215
layer  12  adv train finish, try to retain  110
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -70.32896542549133 , diff:  70.32896542549133
adv train loss:  -71.83716410398483 , diff:  1.5081986784934998
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  70
test acc: top1 ->  65.988 ; top5 ->  86.604  and loss:  1122.0277952551842
forward train acc: top1 ->  66.7265625 ; top5 ->  85.46875  and loss:  282.3148744106293
test acc: top1 ->  69.078 ; top5 ->  88.736  and loss:  1006.4442965388298
forward train acc: top1 ->  66.59375 ; top5 ->  85.375  and loss:  287.4814586043358
test acc: top1 ->  69.144 ; top5 ->  88.858  and loss:  1003.1052760481834
forward train acc: top1 ->  66.0390625 ; top5 ->  85.4921875  and loss:  288.8845221400261
test acc: top1 ->  69.128 ; top5 ->  88.874  and loss:  1001.4537298381329
forward train acc: top1 ->  66.4453125 ; top5 ->  85.2890625  and loss:  284.7175717949867
test acc: top1 ->  69.136 ; top5 ->  88.904  and loss:  1001.8732091188431
forward train acc: top1 ->  66.40625 ; top5 ->  85.7734375  and loss:  283.5880823135376
test acc: top1 ->  69.244 ; top5 ->  88.978  and loss:  997.6781108677387
forward train acc: top1 ->  66.8125 ; top5 ->  85.7890625  and loss:  283.77442145347595
test acc: top1 ->  69.16 ; top5 ->  88.922  and loss:  997.1923508048058
forward train acc: top1 ->  66.28125 ; top5 ->  85.796875  and loss:  282.85594391822815
test acc: top1 ->  69.28 ; top5 ->  89.05  and loss:  994.2951118648052
forward train acc: top1 ->  67.2578125 ; top5 ->  86.0234375  and loss:  277.8998478651047
test acc: top1 ->  69.272 ; top5 ->  89.026  and loss:  994.884699255228
forward train acc: top1 ->  66.421875 ; top5 ->  85.421875  and loss:  287.98045086860657
test acc: top1 ->  69.31 ; top5 ->  89.046  and loss:  993.900358915329
forward train acc: top1 ->  66.5234375 ; top5 ->  85.5703125  and loss:  287.49203193187714
test acc: top1 ->  69.248 ; top5 ->  88.98  and loss:  997.7276203036308
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  71 / 128 , inc:  1
---------------- start layer  14  ---------------
adv train loss:  -69.31467652320862 , diff:  69.31467652320862
adv train loss:  -70.40694773197174 , diff:  1.0922712087631226
layer  14  adv train finish, try to retain  251
>>>>>>> reverse layer  14  since no improvement >>>>>>>
---------------- start layer  15  ---------------
adv train loss:  -67.22500663995743 , diff:  67.22500663995743
adv train loss:  -72.82379019260406 , diff:  5.598783552646637
layer  15  adv train finish, try to retain  250
>>>>>>> reverse layer  15  since no improvement >>>>>>>
---------------- start layer  16  ---------------
### skip layer  16 wait:  3  ###
---------------- start layer  17  ---------------
### skip layer  17 wait:  4  ###
---------------- start layer  18  ---------------
### skip layer  18 wait:  4  ###
---------------- start layer  19  ---------------
adv train loss:  -70.00482654571533 , diff:  70.00482654571533
adv train loss:  -70.97492605447769 , diff:  0.9700995087623596
layer  19  adv train finish, try to retain  185
>>>>>>> reverse layer  19  since no improvement >>>>>>>
---------------- start layer  20  ---------------
### skip layer  20 wait:  4  ###
---------------- start layer  21  ---------------
### skip layer  21 wait:  3  ###
---------------- start layer  22  ---------------
adv train loss:  -70.66337096691132 , diff:  70.66337096691132
adv train loss:  -68.12574356794357 , diff:  2.537627398967743
layer  22  adv train finish, try to retain  200
test acc: top1 ->  69.2 ; top5 ->  88.946  and loss:  1001.6833733916283
forward train acc: top1 ->  66.1015625 ; top5 ->  85.2421875  and loss:  287.6398259997368
test acc: top1 ->  69.438 ; top5 ->  89.026  and loss:  991.9640373885632
forward train acc: top1 ->  67.15625 ; top5 ->  85.8671875  and loss:  278.125985622406
test acc: top1 ->  69.366 ; top5 ->  89.008  and loss:  993.8838555216789
forward train acc: top1 ->  67.609375 ; top5 ->  85.8125  and loss:  278.9107202887535
test acc: top1 ->  69.344 ; top5 ->  89.01  and loss:  996.2819496393204
forward train acc: top1 ->  66.703125 ; top5 ->  85.90625  and loss:  281.65742856264114
test acc: top1 ->  69.588 ; top5 ->  89.07  and loss:  993.3022885620594
forward train acc: top1 ->  66.7265625 ; top5 ->  86.2734375  and loss:  280.0498107075691
test acc: top1 ->  69.6 ; top5 ->  89.01  and loss:  991.6571010351181
forward train acc: top1 ->  67.0703125 ; top5 ->  85.96875  and loss:  280.7154352068901
test acc: top1 ->  69.502 ; top5 ->  89.01  and loss:  991.3396057486534
forward train acc: top1 ->  67.265625 ; top5 ->  86.5390625  and loss:  276.60794377326965
test acc: top1 ->  69.56 ; top5 ->  89.016  and loss:  990.993169516325
forward train acc: top1 ->  67.3515625 ; top5 ->  86.2734375  and loss:  277.05828696489334
test acc: top1 ->  69.6 ; top5 ->  89.042  and loss:  989.6394131183624
forward train acc: top1 ->  66.8046875 ; top5 ->  85.59375  and loss:  285.4173014163971
test acc: top1 ->  69.654 ; top5 ->  89.146  and loss:  987.7342002093792
forward train acc: top1 ->  67.1171875 ; top5 ->  86.21875  and loss:  276.775042116642
test acc: top1 ->  69.538 ; top5 ->  89.002  and loss:  990.8173176646233
>>>>>>> reverse layer  22  since performance drop >>>>>>>
==> this epoch:  208 / 256 , inc:  1
---------------- start layer  23  ---------------
adv train loss:  -72.4720368385315 , diff:  72.4720368385315
adv train loss:  -69.40562427043915 , diff:  3.066412568092346
layer  23  adv train finish, try to retain  200
>>>>>>> reverse layer  23  since no improvement >>>>>>>
---------------- start layer  24  ---------------
### skip layer  24 wait:  3  ###
---------------- start layer  25  ---------------
### skip layer  25 wait:  3  ###
---------------- start layer  26  ---------------
adv train loss:  -72.63141983747482 , diff:  72.63141983747482
adv train loss:  -69.31596374511719 , diff:  3.3154560923576355
layer  26  adv train finish, try to retain  502
>>>>>>> reverse layer  26  since no improvement >>>>>>>
---------------- start layer  27  ---------------
adv train loss:  -70.92552179098129 , diff:  70.92552179098129
adv train loss:  -71.19172424077988 , diff:  0.266202449798584
layer  27  adv train finish, try to retain  506
>>>>>>> reverse layer  27  since no improvement >>>>>>>
---------------- start layer  28  ---------------
### skip layer  28 wait:  3  ###
---------------- start layer  29  ---------------
### skip layer  29 wait:  3  ###
---------------- start layer  30  ---------------
adv train loss:  -69.51651227474213 , diff:  69.51651227474213
adv train loss:  -70.8168592453003 , diff:  1.3003469705581665
layer  30  adv train finish, try to retain  509
>>>>>>> reverse layer  30  since no improvement >>>>>>>
---------------- start layer  31  ---------------
### skip layer  31 wait:  3  ###
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.0075, 0.0075, 0.000703125, 0.0075, 0.0075, 0.0075, 0.00140625, 0.0003515625, 0.00375, 0.0003515625, 0.00375, 0.00375, 0.00375, 0.00375, 0.00017578125, 0.00017578125, 0.00017578125, 0.0003515625, 0.0003515625, 0.005, 0.0003515625, 0.00017578125, 0.001875, 0.005, 0.00017578125, 0.00017578125, 8.7890625e-05, 8.7890625e-05, 8.7890625e-05, 8.7890625e-05, 8.7890625e-05, 8.7890625e-05]  wait [2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 0, 3, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 0
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  7  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -69.17038416862488 , diff:  69.17038416862488
adv train loss:  -67.90096282958984 , diff:  1.2694213390350342
layer  0  adv train finish, try to retain  47
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -71.07700228691101 , diff:  71.07700228691101
adv train loss:  -69.12280458211899 , diff:  1.9541977047920227
layer  1  adv train finish, try to retain  50
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -69.35665142536163 , diff:  69.35665142536163
adv train loss:  -67.21188741922379 , diff:  2.144764006137848
************ all values are small in this layer **********
layer  2  adv train finish, try to retain  56
test acc: top1 ->  69.106 ; top5 ->  88.912  and loss:  1001.8659510016441
forward train acc: top1 ->  67.2890625 ; top5 ->  85.734375  and loss:  280.4262496829033
test acc: top1 ->  69.424 ; top5 ->  89.09  and loss:  990.368701249361
forward train acc: top1 ->  66.2578125 ; top5 ->  85.8125  and loss:  288.67609852552414
test acc: top1 ->  69.426 ; top5 ->  89.028  and loss:  993.0978936851025
forward train acc: top1 ->  67.8984375 ; top5 ->  86.203125  and loss:  274.28012973070145
test acc: top1 ->  69.384 ; top5 ->  89.014  and loss:  989.9654775261879
forward train acc: top1 ->  66.84375 ; top5 ->  86.0234375  and loss:  280.063906788826
test acc: top1 ->  69.544 ; top5 ->  89.084  and loss:  984.7903625965118
forward train acc: top1 ->  67.609375 ; top5 ->  86.0390625  and loss:  273.3003925681114
test acc: top1 ->  69.508 ; top5 ->  89.074  and loss:  988.0650426745415
forward train acc: top1 ->  67.484375 ; top5 ->  85.8515625  and loss:  280.4126601219177
test acc: top1 ->  69.532 ; top5 ->  89.13  and loss:  987.223491102457
forward train acc: top1 ->  66.7265625 ; top5 ->  85.8671875  and loss:  281.46501809358597
test acc: top1 ->  69.59 ; top5 ->  89.124  and loss:  986.3011772334576
forward train acc: top1 ->  67.875 ; top5 ->  86.6328125  and loss:  269.03054535388947
test acc: top1 ->  69.606 ; top5 ->  89.194  and loss:  984.7017024457455
forward train acc: top1 ->  66.46875 ; top5 ->  85.625  and loss:  283.78280824422836
test acc: top1 ->  69.468 ; top5 ->  89.08  and loss:  986.2695400714874
forward train acc: top1 ->  67.3359375 ; top5 ->  86.09375  and loss:  277.51020085811615
test acc: top1 ->  69.656 ; top5 ->  89.134  and loss:  982.6892541646957
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  57 / 64 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -66.68770921230316 , diff:  66.68770921230316
adv train loss:  -68.82626056671143 , diff:  2.138551354408264
layer  3  adv train finish, try to retain  47
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -69.71173119544983 , diff:  69.71173119544983
adv train loss:  -71.02908998727798 , diff:  1.3173587918281555
layer  4  adv train finish, try to retain  47
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -71.90696120262146 , diff:  71.90696120262146
adv train loss:  -70.28406512737274 , diff:  1.6228960752487183
layer  5  adv train finish, try to retain  50
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
adv train loss:  -70.11032164096832 , diff:  70.11032164096832
adv train loss:  -71.50227844715118 , diff:  1.3919568061828613
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  121
test acc: top1 ->  69.17 ; top5 ->  88.862  and loss:  1000.9291323423386
forward train acc: top1 ->  66.9453125 ; top5 ->  85.546875  and loss:  282.008817255497
test acc: top1 ->  69.474 ; top5 ->  89.02  and loss:  991.2050874233246
forward train acc: top1 ->  66.6640625 ; top5 ->  85.9765625  and loss:  282.7229428291321
test acc: top1 ->  69.4 ; top5 ->  89.024  and loss:  994.9637637734413
forward train acc: top1 ->  67.21875 ; top5 ->  86.46875  and loss:  276.1895728111267
test acc: top1 ->  69.378 ; top5 ->  89.036  and loss:  992.9047381281853
forward train acc: top1 ->  66.2421875 ; top5 ->  85.609375  and loss:  287.21277987957
test acc: top1 ->  69.468 ; top5 ->  89.106  and loss:  987.5074517726898
forward train acc: top1 ->  66.3203125 ; top5 ->  85.59375  and loss:  288.2204179763794
test acc: top1 ->  69.474 ; top5 ->  89.218  and loss:  988.0908117890358
forward train acc: top1 ->  66.8515625 ; top5 ->  85.65625  and loss:  282.45696598291397
test acc: top1 ->  69.602 ; top5 ->  89.148  and loss:  985.9727393984795
forward train acc: top1 ->  67.1484375 ; top5 ->  85.703125  and loss:  278.40892589092255
test acc: top1 ->  69.572 ; top5 ->  89.178  and loss:  984.0829074680805
forward train acc: top1 ->  67.828125 ; top5 ->  86.3125  and loss:  274.94249790906906
test acc: top1 ->  69.562 ; top5 ->  89.242  and loss:  984.3352890014648
forward train acc: top1 ->  66.9296875 ; top5 ->  85.359375  and loss:  283.84104084968567
test acc: top1 ->  69.68 ; top5 ->  89.24  and loss:  984.7827915847301
forward train acc: top1 ->  66.9609375 ; top5 ->  85.8515625  and loss:  281.12282621860504
test acc: top1 ->  69.616 ; top5 ->  89.22  and loss:  986.7482319176197
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  122 / 128 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -71.48334628343582 , diff:  71.48334628343582
adv train loss:  -68.67207926511765 , diff:  2.8112670183181763
layer  8  adv train finish, try to retain  91
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -67.50936508178711 , diff:  67.50936508178711
adv train loss:  -66.97159826755524 , diff:  0.5377668142318726
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  114
test acc: top1 ->  69.152 ; top5 ->  88.89  and loss:  1002.8762689828873
forward train acc: top1 ->  67.3125 ; top5 ->  85.796875  and loss:  280.2665308713913
test acc: top1 ->  69.506 ; top5 ->  89.1  and loss:  992.272258669138
forward train acc: top1 ->  66.359375 ; top5 ->  85.84375  and loss:  284.02313059568405
test acc: top1 ->  69.51 ; top5 ->  89.098  and loss:  989.9105522930622
forward train acc: top1 ->  67.578125 ; top5 ->  86.03125  and loss:  273.22656387090683
test acc: top1 ->  69.608 ; top5 ->  89.084  and loss:  991.9957489967346
forward train acc: top1 ->  67.390625 ; top5 ->  86.6171875  and loss:  274.46491527557373
test acc: top1 ->  69.644 ; top5 ->  89.134  and loss:  986.3716115951538
forward train acc: top1 ->  66.5625 ; top5 ->  86.2578125  and loss:  280.8258311152458
test acc: top1 ->  69.782 ; top5 ->  89.2  and loss:  987.3886982500553
forward train acc: top1 ->  66.4921875 ; top5 ->  85.953125  and loss:  282.8995082974434
test acc: top1 ->  69.704 ; top5 ->  89.174  and loss:  985.5080944895744
forward train acc: top1 ->  66.375 ; top5 ->  85.140625  and loss:  289.81013518571854
test acc: top1 ->  69.598 ; top5 ->  89.174  and loss:  987.6994143128395
forward train acc: top1 ->  67.21875 ; top5 ->  86.546875  and loss:  274.58946973085403
test acc: top1 ->  69.656 ; top5 ->  89.138  and loss:  985.9712535738945
forward train acc: top1 ->  66.9296875 ; top5 ->  85.875  and loss:  281.1343147754669
test acc: top1 ->  69.616 ; top5 ->  89.168  and loss:  982.5953924059868
forward train acc: top1 ->  67.4375 ; top5 ->  86.2421875  and loss:  273.86790257692337
test acc: top1 ->  69.68 ; top5 ->  89.15  and loss:  983.1160073876381
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  115 / 128 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -67.06732749938965 , diff:  67.06732749938965
adv train loss:  -70.11613154411316 , diff:  3.0488040447235107
layer  10  adv train finish, try to retain  88
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -70.91790413856506 , diff:  70.91790413856506
adv train loss:  -68.9601759314537 , diff:  1.9577282071113586
layer  11  adv train finish, try to retain  94
test acc: top1 ->  67.152 ; top5 ->  87.556  and loss:  1078.6754883527756
forward train acc: top1 ->  66.578125 ; top5 ->  85.9765625  and loss:  281.5715898871422
test acc: top1 ->  69.284 ; top5 ->  88.982  and loss:  993.2223137021065
forward train acc: top1 ->  66.609375 ; top5 ->  85.7890625  and loss:  281.54216879606247
test acc: top1 ->  69.41 ; top5 ->  89.084  and loss:  992.1287072896957
forward train acc: top1 ->  67.7109375 ; top5 ->  86.2890625  and loss:  277.67992919683456
test acc: top1 ->  69.436 ; top5 ->  89.088  and loss:  992.6397817730904
forward train acc: top1 ->  66.5546875 ; top5 ->  85.96875  and loss:  280.62509578466415
test acc: top1 ->  69.454 ; top5 ->  89.066  and loss:  993.8122491836548
forward train acc: top1 ->  67.0546875 ; top5 ->  85.71875  and loss:  282.3723704814911
test acc: top1 ->  69.422 ; top5 ->  89.178  and loss:  989.2107127904892
forward train acc: top1 ->  66.8515625 ; top5 ->  85.5234375  and loss:  285.6806479692459
test acc: top1 ->  69.516 ; top5 ->  89.118  and loss:  989.8508498668671
forward train acc: top1 ->  66.6796875 ; top5 ->  85.7578125  and loss:  283.2188193202019
test acc: top1 ->  69.502 ; top5 ->  89.156  and loss:  988.4423815310001
forward train acc: top1 ->  67.03125 ; top5 ->  86.28125  and loss:  281.2397614121437
test acc: top1 ->  69.664 ; top5 ->  89.126  and loss:  988.5958251059055
forward train acc: top1 ->  67.21875 ; top5 ->  86.0078125  and loss:  279.18628466129303
test acc: top1 ->  69.632 ; top5 ->  89.136  and loss:  986.9213197827339
forward train acc: top1 ->  67.3984375 ; top5 ->  86.0390625  and loss:  279.6213266849518
test acc: top1 ->  69.676 ; top5 ->  89.098  and loss:  987.8355786204338
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -67.78963643312454 , diff:  67.78963643312454
adv train loss:  -69.7401607632637 , diff:  1.9505243301391602
layer  12  adv train finish, try to retain  89
test acc: top1 ->  68.836 ; top5 ->  88.598  and loss:  1012.779263406992
forward train acc: top1 ->  67.40625 ; top5 ->  86.1953125  and loss:  279.52770161628723
test acc: top1 ->  69.416 ; top5 ->  89.01  and loss:  993.7737025022507
forward train acc: top1 ->  66.4453125 ; top5 ->  85.21875  and loss:  287.23144114017487
test acc: top1 ->  69.36 ; top5 ->  89.008  and loss:  993.3137340545654
forward train acc: top1 ->  66.59375 ; top5 ->  85.578125  and loss:  284.9961531162262
test acc: top1 ->  69.448 ; top5 ->  88.994  and loss:  995.4885599017143
forward train acc: top1 ->  66.984375 ; top5 ->  85.6328125  and loss:  282.1671965122223
test acc: top1 ->  69.516 ; top5 ->  89.116  and loss:  988.4931189715862
forward train acc: top1 ->  67.3984375 ; top5 ->  86.234375  and loss:  281.0336775779724
test acc: top1 ->  69.582 ; top5 ->  89.112  and loss:  989.3372613191605
forward train acc: top1 ->  67.0390625 ; top5 ->  85.9140625  and loss:  281.13979882001877
test acc: top1 ->  69.576 ; top5 ->  89.132  and loss:  987.7096320986748
forward train acc: top1 ->  66.40625 ; top5 ->  86.25  and loss:  280.8866736292839
test acc: top1 ->  69.624 ; top5 ->  89.188  and loss:  986.8914000988007
forward train acc: top1 ->  66.953125 ; top5 ->  85.7890625  and loss:  281.5246804356575
test acc: top1 ->  69.608 ; top5 ->  89.146  and loss:  985.5907248556614
forward train acc: top1 ->  66.859375 ; top5 ->  85.734375  and loss:  282.91313564777374
test acc: top1 ->  69.556 ; top5 ->  89.14  and loss:  986.0909293591976
forward train acc: top1 ->  67.03125 ; top5 ->  86.6484375  and loss:  275.2928436398506
test acc: top1 ->  69.624 ; top5 ->  89.164  and loss:  985.7587162256241
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  98 / 128 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -70.87442964315414 , diff:  70.87442964315414
adv train loss:  -72.60329842567444 , diff:  1.7288687825202942
layer  13  adv train finish, try to retain  97
>>>>>>> reverse layer  13  since no improvement >>>>>>>
---------------- start layer  14  ---------------
adv train loss:  -70.07585728168488 , diff:  70.07585728168488
adv train loss:  -70.17842644453049 , diff:  0.10256916284561157
layer  14  adv train finish, try to retain  245
test acc: top1 ->  69.406 ; top5 ->  88.922  and loss:  999.3370886743069
forward train acc: top1 ->  66.546875 ; top5 ->  86.015625  and loss:  281.0048054456711
test acc: top1 ->  69.598 ; top5 ->  89.092  and loss:  987.398628115654
forward train acc: top1 ->  66.484375 ; top5 ->  85.40625  and loss:  285.0270364880562
test acc: top1 ->  69.728 ; top5 ->  89.144  and loss:  987.5354673266411
forward train acc: top1 ->  66.625 ; top5 ->  85.7734375  and loss:  285.53798657655716
test acc: top1 ->  69.634 ; top5 ->  89.108  and loss:  987.4401226639748
forward train acc: top1 ->  67.5703125 ; top5 ->  86.8125  and loss:  270.49040377140045
test acc: top1 ->  69.666 ; top5 ->  89.154  and loss:  984.6217287480831
forward train acc: top1 ->  66.96875 ; top5 ->  85.9453125  and loss:  277.5131403207779
test acc: top1 ->  69.776 ; top5 ->  89.134  and loss:  983.4770472943783
forward train acc: top1 ->  67.4375 ; top5 ->  86.4140625  and loss:  276.6614075899124
test acc: top1 ->  69.688 ; top5 ->  89.108  and loss:  983.5419596135616
forward train acc: top1 ->  67.140625 ; top5 ->  86.265625  and loss:  277.0019234418869
test acc: top1 ->  69.746 ; top5 ->  89.138  and loss:  982.8670610487461
forward train acc: top1 ->  67.0703125 ; top5 ->  86.109375  and loss:  282.436641395092
test acc: top1 ->  69.834 ; top5 ->  89.15  and loss:  984.2094867229462
forward train acc: top1 ->  67.4453125 ; top5 ->  86.296875  and loss:  276.1880896091461
test acc: top1 ->  69.758 ; top5 ->  89.17  and loss:  984.8059377372265
forward train acc: top1 ->  67.3359375 ; top5 ->  86.28125  and loss:  274.9800498485565
test acc: top1 ->  69.842 ; top5 ->  89.136  and loss:  983.3620572388172
>>>>>>> reverse layer  14  since performance drop >>>>>>>
==> this epoch:  250 / 256 , inc:  1
---------------- start layer  15  ---------------
adv train loss:  -71.57116967439651 , diff:  71.57116967439651
adv train loss:  -69.26710265874863 , diff:  2.304067015647888
layer  15  adv train finish, try to retain  245
test acc: top1 ->  69.108 ; top5 ->  88.86  and loss:  1004.2053218483925
forward train acc: top1 ->  67.1953125 ; top5 ->  86.3671875  and loss:  276.94587528705597
test acc: top1 ->  69.356 ; top5 ->  88.97  and loss:  993.2180358469486
forward train acc: top1 ->  66.671875 ; top5 ->  85.671875  and loss:  281.58088690042496
test acc: top1 ->  69.404 ; top5 ->  88.978  and loss:  992.2442150115967
forward train acc: top1 ->  67.328125 ; top5 ->  86.1796875  and loss:  277.9201350212097
test acc: top1 ->  69.452 ; top5 ->  89.074  and loss:  989.6888721585274
forward train acc: top1 ->  66.5625 ; top5 ->  86.296875  and loss:  280.6217975616455
test acc: top1 ->  69.512 ; top5 ->  89.056  and loss:  988.62001568079
forward train acc: top1 ->  66.5 ; top5 ->  86.234375  and loss:  281.83231967687607
test acc: top1 ->  69.472 ; top5 ->  89.162  and loss:  985.8924696445465
forward train acc: top1 ->  67.3671875 ; top5 ->  86.2109375  and loss:  274.77011543512344
test acc: top1 ->  69.59 ; top5 ->  89.178  and loss:  984.1293104290962
forward train acc: top1 ->  66.625 ; top5 ->  85.7578125  and loss:  282.83003038167953
test acc: top1 ->  69.666 ; top5 ->  89.1  and loss:  984.8468671739101
forward train acc: top1 ->  66.5625 ; top5 ->  86.28125  and loss:  279.2332618832588
test acc: top1 ->  69.614 ; top5 ->  89.17  and loss:  982.5658885538578
forward train acc: top1 ->  67.6171875 ; top5 ->  86.84375  and loss:  271.02979546785355
test acc: top1 ->  69.698 ; top5 ->  89.21  and loss:  984.0617433786392
forward train acc: top1 ->  67.1953125 ; top5 ->  85.9140625  and loss:  279.62964034080505
test acc: top1 ->  69.7 ; top5 ->  89.152  and loss:  985.3159575760365
>>>>>>> reverse layer  15  since performance drop >>>>>>>
==> this epoch:  248 / 256 , inc:  1
---------------- start layer  16  ---------------
adv train loss:  -65.49234580993652 , diff:  65.49234580993652
adv train loss:  -67.92809504270554 , diff:  2.4357492327690125
layer  16  adv train finish, try to retain  240
>>>>>>> reverse layer  16  since no improvement >>>>>>>
---------------- start layer  17  ---------------
### skip layer  17 wait:  3  ###
---------------- start layer  18  ---------------
### skip layer  18 wait:  3  ###
---------------- start layer  19  ---------------
adv train loss:  -71.28313118219376 , diff:  71.28313118219376
adv train loss:  -72.88845008611679 , diff:  1.6053189039230347
************ all values are small in this layer **********
layer  19  adv train finish, try to retain  178
test acc: top1 ->  69.046 ; top5 ->  88.87  and loss:  1001.0354724526405
forward train acc: top1 ->  67.0546875 ; top5 ->  85.5703125  and loss:  281.942190349102
test acc: top1 ->  69.234 ; top5 ->  88.97  and loss:  995.489620745182
forward train acc: top1 ->  67.0703125 ; top5 ->  85.921875  and loss:  280.79344016313553
test acc: top1 ->  69.336 ; top5 ->  89.108  and loss:  990.9788210391998
forward train acc: top1 ->  67.296875 ; top5 ->  86.75  and loss:  273.1094070672989
test acc: top1 ->  69.534 ; top5 ->  89.088  and loss:  989.1759075522423
forward train acc: top1 ->  67.1015625 ; top5 ->  85.734375  and loss:  283.4760065674782
test acc: top1 ->  69.458 ; top5 ->  89.088  and loss:  988.0953344702721
forward train acc: top1 ->  67.2578125 ; top5 ->  86.2890625  and loss:  278.09582084417343
test acc: top1 ->  69.528 ; top5 ->  89.162  and loss:  986.0905773043633
forward train acc: top1 ->  66.2109375 ; top5 ->  85.4765625  and loss:  286.6265552043915
test acc: top1 ->  69.632 ; top5 ->  89.222  and loss:  984.5602548718452
forward train acc: top1 ->  67.8203125 ; top5 ->  86.5859375  and loss:  272.72569185495377
test acc: top1 ->  69.564 ; top5 ->  89.164  and loss:  985.019536614418
forward train acc: top1 ->  67.640625 ; top5 ->  86.5546875  and loss:  272.61294412612915
test acc: top1 ->  69.664 ; top5 ->  89.16  and loss:  984.5834733247757
forward train acc: top1 ->  66.9921875 ; top5 ->  85.9609375  and loss:  280.6019222140312
test acc: top1 ->  69.648 ; top5 ->  89.132  and loss:  985.0776700973511
forward train acc: top1 ->  66.375 ; top5 ->  85.65625  and loss:  286.3883352279663
test acc: top1 ->  69.69 ; top5 ->  89.198  and loss:  986.5890983343124
>>>>>>> reverse layer  19  since performance drop >>>>>>>
==> this epoch:  179 / 256 , inc:  1
---------------- start layer  20  ---------------
### skip layer  20 wait:  3  ###
---------------- start layer  21  ---------------
adv train loss:  -70.81027179956436 , diff:  70.81027179956436
adv train loss:  -68.52537071704865 , diff:  2.2849010825157166
layer  21  adv train finish, try to retain  240
test acc: top1 ->  69.884 ; top5 ->  89.168  and loss:  980.9346535503864
forward train acc: top1 ->  67.6953125 ; top5 ->  86.7578125  and loss:  272.7581169605255
test acc: top1 ->  69.702 ; top5 ->  89.222  and loss:  983.2326994538307
forward train acc: top1 ->  68.125 ; top5 ->  86.5859375  and loss:  270.4938930273056
test acc: top1 ->  69.746 ; top5 ->  89.304  and loss:  984.4592646956444
forward train acc: top1 ->  67.1328125 ; top5 ->  85.7890625  and loss:  278.30962496995926
test acc: top1 ->  69.732 ; top5 ->  89.138  and loss:  984.3624558746815
forward train acc: top1 ->  67.328125 ; top5 ->  86.1171875  and loss:  277.3357959985733
test acc: top1 ->  69.738 ; top5 ->  89.154  and loss:  981.4143179059029
forward train acc: top1 ->  67.171875 ; top5 ->  86.015625  and loss:  279.3763762116432
test acc: top1 ->  69.726 ; top5 ->  89.2  and loss:  981.6875823736191
forward train acc: top1 ->  67.5 ; top5 ->  86.46875  and loss:  271.6155240535736
test acc: top1 ->  69.702 ; top5 ->  89.13  and loss:  982.4095919728279
forward train acc: top1 ->  67.5390625 ; top5 ->  86.046875  and loss:  276.74855774641037
test acc: top1 ->  69.698 ; top5 ->  89.226  and loss:  981.0504000782967
forward train acc: top1 ->  67.375 ; top5 ->  86.3046875  and loss:  278.07881808280945
test acc: top1 ->  69.802 ; top5 ->  89.256  and loss:  979.7044702768326
forward train acc: top1 ->  67.046875 ; top5 ->  86.0859375  and loss:  277.8473408818245
test acc: top1 ->  69.814 ; top5 ->  89.292  and loss:  979.1828026473522
forward train acc: top1 ->  66.890625 ; top5 ->  86.0390625  and loss:  279.5054021477699
test acc: top1 ->  69.796 ; top5 ->  89.258  and loss:  980.0453559458256
>>>>>>> reverse layer  21  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  22  ---------------
adv train loss:  -68.39979100227356 , diff:  68.39979100227356
adv train loss:  -67.3702986240387 , diff:  1.0294923782348633
layer  22  adv train finish, try to retain  203
test acc: top1 ->  69.568 ; top5 ->  89.104  and loss:  992.028108894825
forward train acc: top1 ->  67.1015625 ; top5 ->  85.625  and loss:  281.2619559764862
test acc: top1 ->  69.634 ; top5 ->  89.116  and loss:  986.3215429782867
forward train acc: top1 ->  66.7578125 ; top5 ->  85.75  and loss:  279.3623910546303
test acc: top1 ->  69.638 ; top5 ->  89.08  and loss:  989.136014521122
forward train acc: top1 ->  68.078125 ; top5 ->  86.5859375  and loss:  270.7119104862213
test acc: top1 ->  69.43 ; top5 ->  89.076  and loss:  986.8192870020866
forward train acc: top1 ->  67.78125 ; top5 ->  86.140625  and loss:  277.99660420417786
test acc: top1 ->  69.734 ; top5 ->  89.15  and loss:  984.1700296401978
forward train acc: top1 ->  67.6171875 ; top5 ->  86.0546875  and loss:  276.51602363586426
test acc: top1 ->  69.614 ; top5 ->  89.152  and loss:  984.5222605466843
forward train acc: top1 ->  66.8125 ; top5 ->  86.1328125  and loss:  280.2252744436264
test acc: top1 ->  69.626 ; top5 ->  89.178  and loss:  986.1781613826752
forward train acc: top1 ->  67.390625 ; top5 ->  86.640625  and loss:  271.26181477308273
test acc: top1 ->  69.596 ; top5 ->  89.186  and loss:  984.0677906870842
forward train acc: top1 ->  67.234375 ; top5 ->  85.859375  and loss:  282.53249204158783
test acc: top1 ->  69.814 ; top5 ->  89.202  and loss:  981.3269813656807
forward train acc: top1 ->  67.15625 ; top5 ->  86.0546875  and loss:  278.02816927433014
test acc: top1 ->  69.672 ; top5 ->  89.172  and loss:  981.0723423957825
forward train acc: top1 ->  67.3046875 ; top5 ->  86.0546875  and loss:  276.47630113363266
test acc: top1 ->  69.712 ; top5 ->  89.164  and loss:  981.0623221993446
>>>>>>> reverse layer  22  since performance drop >>>>>>>
==> this epoch:  208 / 256 , inc:  1
---------------- start layer  23  ---------------
adv train loss:  -65.66698503494263 , diff:  65.66698503494263
adv train loss:  -68.19997024536133 , diff:  2.532985210418701
************ all values are small in this layer **********
layer  23  adv train finish, try to retain  186
test acc: top1 ->  69.478 ; top5 ->  88.948  and loss:  1000.2707634568214
forward train acc: top1 ->  67.421875 ; top5 ->  86.5078125  and loss:  274.2392187714577
test acc: top1 ->  69.446 ; top5 ->  88.998  and loss:  991.2007582783699
forward train acc: top1 ->  66.7890625 ; top5 ->  85.9921875  and loss:  281.8869156241417
test acc: top1 ->  69.542 ; top5 ->  89.106  and loss:  990.9822465181351
forward train acc: top1 ->  66.34375 ; top5 ->  86.1796875  and loss:  282.40499395132065
test acc: top1 ->  69.514 ; top5 ->  89.062  and loss:  990.8191765546799
forward train acc: top1 ->  66.53125 ; top5 ->  85.3984375  and loss:  285.84922057390213
test acc: top1 ->  69.676 ; top5 ->  89.17  and loss:  989.433883368969
forward train acc: top1 ->  67.078125 ; top5 ->  85.8984375  and loss:  279.0928847193718
test acc: top1 ->  69.716 ; top5 ->  89.172  and loss:  986.9983529448509
forward train acc: top1 ->  66.828125 ; top5 ->  86.0625  and loss:  282.28954035043716
test acc: top1 ->  69.794 ; top5 ->  89.202  and loss:  985.929135799408
forward train acc: top1 ->  67.2890625 ; top5 ->  85.71875  and loss:  278.72242444753647
test acc: top1 ->  69.742 ; top5 ->  89.194  and loss:  985.2892053723335
forward train acc: top1 ->  66.5234375 ; top5 ->  85.9375  and loss:  283.3749529719353
test acc: top1 ->  69.74 ; top5 ->  89.228  and loss:  985.0197846889496
forward train acc: top1 ->  67.625 ; top5 ->  86.3671875  and loss:  277.00590765476227
test acc: top1 ->  69.698 ; top5 ->  89.21  and loss:  985.4191670417786
forward train acc: top1 ->  67.3984375 ; top5 ->  86.0859375  and loss:  276.7156081199646
test acc: top1 ->  69.778 ; top5 ->  89.214  and loss:  983.043093919754
>>>>>>> reverse layer  23  since performance drop >>>>>>>
==> this epoch:  187 / 256 , inc:  1
---------------- start layer  24  ---------------
adv train loss:  -70.64101505279541 , diff:  70.64101505279541
adv train loss:  -67.21982139348984 , diff:  3.4211936593055725
layer  24  adv train finish, try to retain  236
test acc: top1 ->  69.668 ; top5 ->  89.248  and loss:  981.7675197124481
forward train acc: top1 ->  67.0546875 ; top5 ->  85.8125  and loss:  281.12840700149536
test acc: top1 ->  69.74 ; top5 ->  89.302  and loss:  982.6241326928139
forward train acc: top1 ->  66.7421875 ; top5 ->  86.1484375  and loss:  279.1484691500664
test acc: top1 ->  69.698 ; top5 ->  89.26  and loss:  981.5243089795113
forward train acc: top1 ->  66.609375 ; top5 ->  85.5078125  and loss:  286.77796733379364
test acc: top1 ->  69.558 ; top5 ->  89.188  and loss:  985.5478521585464
forward train acc: top1 ->  66.8125 ; top5 ->  86.296875  and loss:  277.693245947361
test acc: top1 ->  69.642 ; top5 ->  89.212  and loss:  983.4569628834724
forward train acc: top1 ->  66.6875 ; top5 ->  85.609375  and loss:  286.180172085762
test acc: top1 ->  69.81 ; top5 ->  89.262  and loss:  980.8403094410896
forward train acc: top1 ->  67.6875 ; top5 ->  86.3125  and loss:  272.0719510912895
test acc: top1 ->  69.89 ; top5 ->  89.286  and loss:  979.0974426269531
forward train acc: top1 ->  66.8203125 ; top5 ->  86.0625  and loss:  277.7609516978264
test acc: top1 ->  69.856 ; top5 ->  89.326  and loss:  979.9175353050232
forward train acc: top1 ->  67.7265625 ; top5 ->  86.90625  and loss:  269.89457392692566
test acc: top1 ->  69.814 ; top5 ->  89.34  and loss:  977.8738020062447
forward train acc: top1 ->  67.640625 ; top5 ->  86.125  and loss:  276.24323266744614
test acc: top1 ->  69.954 ; top5 ->  89.35  and loss:  975.9247592687607
forward train acc: top1 ->  67.6875 ; top5 ->  86.0703125  and loss:  275.1868305206299
test acc: top1 ->  69.958 ; top5 ->  89.308  and loss:  977.530431240797
>>>>>>> reverse layer  24  since performance drop >>>>>>>
==> this epoch:  244 / 256 , inc:  1
---------------- start layer  25  ---------------
adv train loss:  -70.04628884792328 , diff:  70.04628884792328
adv train loss:  -71.96720767021179 , diff:  1.9209188222885132
layer  25  adv train finish, try to retain  241
test acc: top1 ->  69.728 ; top5 ->  89.252  and loss:  980.3083311319351
forward train acc: top1 ->  67.2109375 ; top5 ->  86.46875  and loss:  275.5189846754074
test acc: top1 ->  69.758 ; top5 ->  89.27  and loss:  980.1156241297722
forward train acc: top1 ->  67.21875 ; top5 ->  86.1328125  and loss:  279.5575202703476
test acc: top1 ->  69.774 ; top5 ->  89.224  and loss:  981.8819895982742
forward train acc: top1 ->  66.8984375 ; top5 ->  86.09375  and loss:  278.36664205789566
test acc: top1 ->  69.718 ; top5 ->  89.254  and loss:  984.1321105360985
forward train acc: top1 ->  67.015625 ; top5 ->  86.2109375  and loss:  279.4369720816612
test acc: top1 ->  69.762 ; top5 ->  89.194  and loss:  980.7956575751305
forward train acc: top1 ->  66.8671875 ; top5 ->  85.9140625  and loss:  278.38784581422806
test acc: top1 ->  69.81 ; top5 ->  89.23  and loss:  977.8487349152565
forward train acc: top1 ->  67.2734375 ; top5 ->  86.0078125  and loss:  279.25061494112015
test acc: top1 ->  69.848 ; top5 ->  89.254  and loss:  978.1257224082947
forward train acc: top1 ->  66.9140625 ; top5 ->  86.296875  and loss:  280.00418305397034
test acc: top1 ->  69.884 ; top5 ->  89.216  and loss:  979.609582722187
forward train acc: top1 ->  67.0859375 ; top5 ->  86.0859375  and loss:  281.5412608385086
test acc: top1 ->  69.898 ; top5 ->  89.274  and loss:  976.499913752079
forward train acc: top1 ->  67.6875 ; top5 ->  86.3984375  and loss:  276.3328619003296
test acc: top1 ->  69.824 ; top5 ->  89.23  and loss:  975.1052234768867
forward train acc: top1 ->  67.2421875 ; top5 ->  86.140625  and loss:  278.68603014945984
test acc: top1 ->  69.942 ; top5 ->  89.316  and loss:  976.4180942773819
>>>>>>> reverse layer  25  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  26  ---------------
adv train loss:  -69.88837456703186 , diff:  69.88837456703186
adv train loss:  -68.84652787446976 , diff:  1.0418466925621033
layer  26  adv train finish, try to retain  500
test acc: top1 ->  69.794 ; top5 ->  89.188  and loss:  982.3343151211739
forward train acc: top1 ->  66.875 ; top5 ->  85.890625  and loss:  280.5350663661957
test acc: top1 ->  69.802 ; top5 ->  89.248  and loss:  980.8262838721275
forward train acc: top1 ->  66.890625 ; top5 ->  85.75  and loss:  281.76337736845016
test acc: top1 ->  69.63 ; top5 ->  89.15  and loss:  982.507363319397
forward train acc: top1 ->  67.5234375 ; top5 ->  86.2265625  and loss:  277.1821835041046
test acc: top1 ->  69.786 ; top5 ->  89.24  and loss:  980.2991903424263
forward train acc: top1 ->  67.203125 ; top5 ->  85.9453125  and loss:  279.10224056243896
test acc: top1 ->  69.876 ; top5 ->  89.19  and loss:  983.0659028291702
forward train acc: top1 ->  66.9375 ; top5 ->  85.40625  and loss:  283.9703611135483
test acc: top1 ->  69.792 ; top5 ->  89.156  and loss:  980.4721996188164
forward train acc: top1 ->  67.9140625 ; top5 ->  86.6015625  and loss:  269.3689047694206
test acc: top1 ->  69.946 ; top5 ->  89.202  and loss:  979.3808444142342
forward train acc: top1 ->  67.4140625 ; top5 ->  86.234375  and loss:  278.2056856751442
test acc: top1 ->  69.88 ; top5 ->  89.194  and loss:  976.7925218939781
forward train acc: top1 ->  67.671875 ; top5 ->  86.40625  and loss:  275.859356880188
test acc: top1 ->  69.986 ; top5 ->  89.272  and loss:  976.5737602114677
forward train acc: top1 ->  67.7109375 ; top5 ->  86.5546875  and loss:  276.51299995183945
test acc: top1 ->  69.936 ; top5 ->  89.222  and loss:  975.4874148368835
forward train acc: top1 ->  66.9453125 ; top5 ->  86.6484375  and loss:  274.7281824350357
test acc: top1 ->  69.868 ; top5 ->  89.224  and loss:  979.2851890921593
>>>>>>> reverse layer  26  since performance drop >>>>>>>
==> this epoch:  502 / 512 , inc:  1
---------------- start layer  27  ---------------
adv train loss:  -70.06962752342224 , diff:  70.06962752342224
adv train loss:  -67.60456722974777 , diff:  2.465060293674469
layer  27  adv train finish, try to retain  495
test acc: top1 ->  69.582 ; top5 ->  89.116  and loss:  984.723711013794
forward train acc: top1 ->  66.5703125 ; top5 ->  85.875  and loss:  282.14503729343414
test acc: top1 ->  69.696 ; top5 ->  89.1  and loss:  982.8587637543678
forward train acc: top1 ->  66.75 ; top5 ->  86.046875  and loss:  281.03406792879105
test acc: top1 ->  69.65 ; top5 ->  89.026  and loss:  983.6822034418583
forward train acc: top1 ->  66.953125 ; top5 ->  85.53125  and loss:  283.39466494321823
test acc: top1 ->  69.696 ; top5 ->  89.074  and loss:  985.7354626655579
forward train acc: top1 ->  67.6796875 ; top5 ->  86.1171875  and loss:  275.3023279309273
test acc: top1 ->  69.778 ; top5 ->  89.188  and loss:  981.0596138238907
forward train acc: top1 ->  66.5390625 ; top5 ->  85.703125  and loss:  281.311362862587
test acc: top1 ->  69.756 ; top5 ->  89.178  and loss:  983.0031710267067
forward train acc: top1 ->  67.1796875 ; top5 ->  86.4296875  and loss:  277.14304208755493
test acc: top1 ->  69.714 ; top5 ->  89.17  and loss:  982.7245038747787
forward train acc: top1 ->  66.84375 ; top5 ->  85.828125  and loss:  282.93287950754166
test acc: top1 ->  69.824 ; top5 ->  89.198  and loss:  980.7154634594917
forward train acc: top1 ->  67.421875 ; top5 ->  86.1328125  and loss:  276.57919377088547
test acc: top1 ->  69.87 ; top5 ->  89.16  and loss:  978.9370393157005
forward train acc: top1 ->  67.5546875 ; top5 ->  85.8828125  and loss:  278.53974080085754
test acc: top1 ->  69.872 ; top5 ->  89.206  and loss:  978.5677088499069
forward train acc: top1 ->  67.703125 ; top5 ->  86.6328125  and loss:  274.8054424524307
test acc: top1 ->  69.864 ; top5 ->  89.156  and loss:  978.0601165294647
>>>>>>> reverse layer  27  since performance drop >>>>>>>
==> this epoch:  505 / 512 , inc:  1
---------------- start layer  28  ---------------
adv train loss:  -70.98507386445999 , diff:  70.98507386445999
adv train loss:  -68.32291179895401 , diff:  2.6621620655059814
layer  28  adv train finish, try to retain  501
test acc: top1 ->  69.858 ; top5 ->  89.28  and loss:  981.6968943476677
forward train acc: top1 ->  67.1015625 ; top5 ->  85.7421875  and loss:  278.7894452214241
test acc: top1 ->  69.752 ; top5 ->  89.246  and loss:  982.3450616598129
forward train acc: top1 ->  67.3359375 ; top5 ->  86.015625  and loss:  277.3994257450104
test acc: top1 ->  69.874 ; top5 ->  89.216  and loss:  981.0008553266525
forward train acc: top1 ->  67.0703125 ; top5 ->  86.1015625  and loss:  276.37137442827225
test acc: top1 ->  69.84 ; top5 ->  89.242  and loss:  980.390732049942
forward train acc: top1 ->  67.859375 ; top5 ->  86.4765625  and loss:  272.56629610061646
test acc: top1 ->  69.99 ; top5 ->  89.216  and loss:  978.5227900147438
forward train acc: top1 ->  67.578125 ; top5 ->  86.1953125  and loss:  277.58895909786224
test acc: top1 ->  69.896 ; top5 ->  89.204  and loss:  976.3419677615166
forward train acc: top1 ->  67.8671875 ; top5 ->  86.5078125  and loss:  271.79216343164444
test acc: top1 ->  70.01 ; top5 ->  89.32  and loss:  974.9121843576431
forward train acc: top1 ->  67.671875 ; top5 ->  86.1171875  and loss:  271.3075201511383
test acc: top1 ->  70.0 ; top5 ->  89.344  and loss:  975.3072724342346
forward train acc: top1 ->  67.484375 ; top5 ->  85.7890625  and loss:  277.0741829276085
test acc: top1 ->  69.928 ; top5 ->  89.35  and loss:  973.4176182150841
forward train acc: top1 ->  68.0078125 ; top5 ->  86.515625  and loss:  270.6571180820465
test acc: top1 ->  70.018 ; top5 ->  89.368  and loss:  975.6571323871613
forward train acc: top1 ->  67.1171875 ; top5 ->  86.375  and loss:  276.7899670600891
test acc: top1 ->  69.986 ; top5 ->  89.318  and loss:  974.5327982902527
>>>>>>> reverse layer  28  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  29  ---------------
adv train loss:  -66.94142055511475 , diff:  66.94142055511475
adv train loss:  -67.08299690485 , diff:  0.14157634973526
layer  29  adv train finish, try to retain  492
test acc: top1 ->  69.982 ; top5 ->  89.378  and loss:  975.5721592903137
forward train acc: top1 ->  67.8359375 ; top5 ->  86.28125  and loss:  273.4290038347244
test acc: top1 ->  69.82 ; top5 ->  89.238  and loss:  979.3490097522736
forward train acc: top1 ->  67.375 ; top5 ->  85.765625  and loss:  275.0475424528122
test acc: top1 ->  69.898 ; top5 ->  89.242  and loss:  979.6659790277481
forward train acc: top1 ->  66.78125 ; top5 ->  85.7421875  and loss:  284.07449746131897
test acc: top1 ->  69.744 ; top5 ->  89.26  and loss:  979.5514394938946
forward train acc: top1 ->  67.28125 ; top5 ->  86.0859375  and loss:  279.58118718862534
test acc: top1 ->  69.908 ; top5 ->  89.354  and loss:  975.4427968263626
forward train acc: top1 ->  67.3828125 ; top5 ->  86.0859375  and loss:  276.98107570409775
test acc: top1 ->  69.988 ; top5 ->  89.294  and loss:  975.6768977046013
forward train acc: top1 ->  67.796875 ; top5 ->  86.6484375  and loss:  276.1250280737877
test acc: top1 ->  69.886 ; top5 ->  89.342  and loss:  975.6882578134537
forward train acc: top1 ->  66.546875 ; top5 ->  86.1328125  and loss:  282.38655412197113
test acc: top1 ->  69.936 ; top5 ->  89.316  and loss:  977.4805359840393
forward train acc: top1 ->  67.265625 ; top5 ->  86.546875  and loss:  272.42487758398056
test acc: top1 ->  69.854 ; top5 ->  89.288  and loss:  975.7684388756752
forward train acc: top1 ->  67.5078125 ; top5 ->  86.421875  and loss:  274.6000429391861
test acc: top1 ->  69.94 ; top5 ->  89.368  and loss:  973.3211424946785
forward train acc: top1 ->  67.3984375 ; top5 ->  86.265625  and loss:  276.01365453004837
test acc: top1 ->  69.936 ; top5 ->  89.354  and loss:  975.7683551907539
>>>>>>> reverse layer  29  since performance drop >>>>>>>
==> this epoch:  498 / 512 , inc:  1
---------------- start layer  30  ---------------
adv train loss:  -69.7989513874054 , diff:  69.7989513874054
adv train loss:  -71.23700720071793 , diff:  1.4380558133125305
layer  30  adv train finish, try to retain  503
>>>>>>> reverse layer  30  since no improvement >>>>>>>
---------------- start layer  31  ---------------
adv train loss:  -72.04351198673248 , diff:  72.04351198673248
adv train loss:  -69.04431760311127 , diff:  2.999194383621216
layer  31  adv train finish, try to retain  498
test acc: top1 ->  69.602 ; top5 ->  89.086  and loss:  990.4390569329262
forward train acc: top1 ->  66.984375 ; top5 ->  85.9453125  and loss:  278.97094333171844
test acc: top1 ->  69.77 ; top5 ->  89.128  and loss:  984.756500184536
forward train acc: top1 ->  67.6171875 ; top5 ->  86.53125  and loss:  275.2503760457039
test acc: top1 ->  69.88 ; top5 ->  89.1  and loss:  981.9863776564598
forward train acc: top1 ->  67.46875 ; top5 ->  85.8125  and loss:  282.7711126804352
test acc: top1 ->  69.89 ; top5 ->  89.246  and loss:  979.9414573907852
forward train acc: top1 ->  67.171875 ; top5 ->  86.3125  and loss:  277.99086582660675
test acc: top1 ->  69.858 ; top5 ->  89.174  and loss:  980.8339150547981
forward train acc: top1 ->  66.9375 ; top5 ->  85.578125  and loss:  279.9006821513176
test acc: top1 ->  69.9 ; top5 ->  89.196  and loss:  982.2882215976715
forward train acc: top1 ->  66.921875 ; top5 ->  86.2109375  and loss:  278.1930397748947
test acc: top1 ->  70.01 ; top5 ->  89.156  and loss:  976.4874620437622
forward train acc: top1 ->  67.828125 ; top5 ->  86.4921875  and loss:  275.60648626089096
test acc: top1 ->  69.912 ; top5 ->  89.138  and loss:  979.9445208311081
forward train acc: top1 ->  67.34375 ; top5 ->  86.3203125  and loss:  274.9515947699547
test acc: top1 ->  70.006 ; top5 ->  89.172  and loss:  977.1955680847168
forward train acc: top1 ->  67.109375 ; top5 ->  86.421875  and loss:  278.51188802719116
test acc: top1 ->  70.014 ; top5 ->  89.154  and loss:  974.6567360162735
forward train acc: top1 ->  67.7578125 ; top5 ->  86.5703125  and loss:  270.2546042203903
test acc: top1 ->  70.046 ; top5 ->  89.174  and loss:  974.5353618264198
>>>>>>> reverse layer  31  since performance drop >>>>>>>
==> this epoch:  501 / 512 , inc:  1
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.015, 0.015, 0.00052734375, 0.015, 0.015, 0.015, 0.00140625, 0.000263671875, 0.0075, 0.000263671875, 0.0075, 0.0028125, 0.0028125, 0.0075, 0.0001318359375, 0.0001318359375, 0.0003515625, 0.0003515625, 0.0003515625, 0.00375, 0.0003515625, 0.0001318359375, 0.00140625, 0.00375, 0.0001318359375, 0.0001318359375, 6.591796875e-05, 6.591796875e-05, 6.591796875e-05, 6.591796875e-05, 0.00017578125, 6.591796875e-05]  wait [2, 2, 4, 2, 2, 2, 3, 4, 2, 4, 2, 4, 4, 2, 4, 4, 2, 2, 2, 2, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 0
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  8  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -68.87389051914215 , diff:  68.87389051914215
adv train loss:  -71.50339579582214 , diff:  2.6295052766799927
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  36
test acc: top1 ->  65.52 ; top5 ->  86.612  and loss:  1125.12382209301
forward train acc: top1 ->  66.2734375 ; top5 ->  85.2578125  and loss:  286.33800423145294
test acc: top1 ->  69.168 ; top5 ->  88.81  and loss:  999.8676260113716
forward train acc: top1 ->  66.640625 ; top5 ->  85.53125  and loss:  286.2329097390175
test acc: top1 ->  69.2 ; top5 ->  88.956  and loss:  1000.1829058527946
forward train acc: top1 ->  66.390625 ; top5 ->  85.78125  and loss:  285.16254246234894
test acc: top1 ->  69.442 ; top5 ->  88.972  and loss:  995.7861213684082
forward train acc: top1 ->  67.125 ; top5 ->  86.265625  and loss:  278.9171499013901
test acc: top1 ->  69.382 ; top5 ->  89.056  and loss:  992.107254087925
forward train acc: top1 ->  66.4609375 ; top5 ->  85.375  and loss:  290.40411323308945
test acc: top1 ->  69.396 ; top5 ->  89.1  and loss:  990.2989134788513
forward train acc: top1 ->  66.4921875 ; top5 ->  86.15625  and loss:  278.0119963288307
test acc: top1 ->  69.526 ; top5 ->  89.098  and loss:  987.7821018099785
forward train acc: top1 ->  67.1015625 ; top5 ->  85.4375  and loss:  282.32033890485764
test acc: top1 ->  69.58 ; top5 ->  89.08  and loss:  987.5503660440445
forward train acc: top1 ->  67.7421875 ; top5 ->  86.0390625  and loss:  274.4795700907707
test acc: top1 ->  69.616 ; top5 ->  89.158  and loss:  987.0822259783745
forward train acc: top1 ->  66.875 ; top5 ->  86.3125  and loss:  274.96388840675354
test acc: top1 ->  69.734 ; top5 ->  89.168  and loss:  983.4220170378685
forward train acc: top1 ->  66.75 ; top5 ->  85.3515625  and loss:  284.9711661338806
test acc: top1 ->  69.66 ; top5 ->  89.176  and loss:  983.0322075486183
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  37 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -69.86267709732056 , diff:  69.86267709732056
adv train loss:  -70.55541670322418 , diff:  0.6927396059036255
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  43
test acc: top1 ->  64.838 ; top5 ->  85.93  and loss:  1173.7225314378738
forward train acc: top1 ->  65.4140625 ; top5 ->  85.6328125  and loss:  290.80280989408493
test acc: top1 ->  69.498 ; top5 ->  88.948  and loss:  995.6755335330963
forward train acc: top1 ->  66.2265625 ; top5 ->  85.234375  and loss:  286.38049280643463
test acc: top1 ->  69.572 ; top5 ->  89.078  and loss:  986.5279458761215
forward train acc: top1 ->  67.09375 ; top5 ->  85.6796875  and loss:  283.7282845377922
test acc: top1 ->  69.72 ; top5 ->  89.028  and loss:  986.7850452065468
forward train acc: top1 ->  66.96875 ; top5 ->  85.9140625  and loss:  279.524297952652
test acc: top1 ->  69.732 ; top5 ->  89.146  and loss:  983.1017206907272
forward train acc: top1 ->  67.40625 ; top5 ->  85.8828125  and loss:  276.9134304523468
test acc: top1 ->  69.832 ; top5 ->  89.138  and loss:  981.9517315626144
forward train acc: top1 ->  67.1640625 ; top5 ->  86.2734375  and loss:  275.8574616909027
test acc: top1 ->  69.714 ; top5 ->  89.196  and loss:  980.9172961711884
forward train acc: top1 ->  67.0234375 ; top5 ->  85.71875  and loss:  278.29242426157
test acc: top1 ->  69.876 ; top5 ->  89.182  and loss:  980.017404794693
forward train acc: top1 ->  67.578125 ; top5 ->  85.9453125  and loss:  274.5618598461151
test acc: top1 ->  69.974 ; top5 ->  89.208  and loss:  979.2122546434402
forward train acc: top1 ->  67.046875 ; top5 ->  86.234375  and loss:  275.98933231830597
test acc: top1 ->  69.926 ; top5 ->  89.19  and loss:  979.400185406208
forward train acc: top1 ->  67.5390625 ; top5 ->  86.3359375  and loss:  274.6147531270981
test acc: top1 ->  69.906 ; top5 ->  89.206  and loss:  978.7118956446648
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  44 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -74.96533584594727 , diff:  74.96533584594727
adv train loss:  -70.8875766992569 , diff:  4.077759146690369
layer  2  adv train finish, try to retain  59
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -74.91489338874817 , diff:  74.91489338874817
adv train loss:  -72.4851685166359 , diff:  2.429724872112274
************ all values are small in this layer **********
layer  3  adv train finish, try to retain  42
test acc: top1 ->  63.56 ; top5 ->  85.166  and loss:  1196.558884382248
forward train acc: top1 ->  66.2578125 ; top5 ->  85.6015625  and loss:  285.658067882061
test acc: top1 ->  69.326 ; top5 ->  89.02  and loss:  996.5578272938728
forward train acc: top1 ->  66.9765625 ; top5 ->  86.0  and loss:  276.4756566286087
test acc: top1 ->  69.268 ; top5 ->  89.018  and loss:  993.5731291770935
forward train acc: top1 ->  66.2265625 ; top5 ->  85.515625  and loss:  286.687110722065
test acc: top1 ->  69.368 ; top5 ->  89.092  and loss:  991.5733134746552
forward train acc: top1 ->  66.7109375 ; top5 ->  85.390625  and loss:  282.62584030628204
test acc: top1 ->  69.454 ; top5 ->  89.054  and loss:  991.5379956960678
forward train acc: top1 ->  67.1484375 ; top5 ->  86.1171875  and loss:  279.60705041885376
test acc: top1 ->  69.496 ; top5 ->  89.098  and loss:  989.891292989254
forward train acc: top1 ->  66.609375 ; top5 ->  85.7265625  and loss:  283.87943732738495
test acc: top1 ->  69.476 ; top5 ->  89.09  and loss:  989.1606281995773
forward train acc: top1 ->  67.28125 ; top5 ->  85.6796875  and loss:  277.90766310691833
test acc: top1 ->  69.546 ; top5 ->  89.11  and loss:  988.6533542275429
forward train acc: top1 ->  66.859375 ; top5 ->  85.8046875  and loss:  279.7731124162674
test acc: top1 ->  69.502 ; top5 ->  89.102  and loss:  986.4839130043983
forward train acc: top1 ->  67.3984375 ; top5 ->  86.0  and loss:  278.4823040366173
test acc: top1 ->  69.586 ; top5 ->  89.178  and loss:  985.3418391346931
forward train acc: top1 ->  67.15625 ; top5 ->  85.4453125  and loss:  280.7779314517975
test acc: top1 ->  69.542 ; top5 ->  89.14  and loss:  985.5563377141953
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  43 / 64 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -68.8403924703598 , diff:  68.8403924703598
adv train loss:  -67.89289116859436 , diff:  0.9475013017654419
************ all values are small in this layer **********
layer  4  adv train finish, try to retain  39
test acc: top1 ->  66.65 ; top5 ->  87.328  and loss:  1082.997633755207
forward train acc: top1 ->  67.109375 ; top5 ->  85.9375  and loss:  280.7151674628258
test acc: top1 ->  69.484 ; top5 ->  89.066  and loss:  989.8393812179565
forward train acc: top1 ->  67.5625 ; top5 ->  86.25  and loss:  278.4321487545967
test acc: top1 ->  69.51 ; top5 ->  89.118  and loss:  985.7377811074257
forward train acc: top1 ->  66.7265625 ; top5 ->  85.6328125  and loss:  281.65919917821884
test acc: top1 ->  69.604 ; top5 ->  89.13  and loss:  987.0021918416023
forward train acc: top1 ->  66.90625 ; top5 ->  85.484375  and loss:  280.9795201420784
test acc: top1 ->  69.728 ; top5 ->  89.214  and loss:  983.5093919634819
forward train acc: top1 ->  67.546875 ; top5 ->  86.4140625  and loss:  277.34053587913513
test acc: top1 ->  69.76 ; top5 ->  89.186  and loss:  982.2531204819679
forward train acc: top1 ->  67.125 ; top5 ->  86.0234375  and loss:  278.10197055339813
test acc: top1 ->  69.874 ; top5 ->  89.18  and loss:  979.8978217840195
forward train acc: top1 ->  66.9296875 ; top5 ->  86.125  and loss:  278.30125790834427
test acc: top1 ->  69.912 ; top5 ->  89.258  and loss:  978.3036624789238
forward train acc: top1 ->  67.0 ; top5 ->  85.765625  and loss:  279.15530955791473
test acc: top1 ->  69.938 ; top5 ->  89.27  and loss:  979.4696009755135
forward train acc: top1 ->  67.6796875 ; top5 ->  86.59375  and loss:  272.4168514609337
test acc: top1 ->  69.92 ; top5 ->  89.216  and loss:  976.9336144924164
forward train acc: top1 ->  67.140625 ; top5 ->  86.40625  and loss:  277.556593477726
test acc: top1 ->  69.854 ; top5 ->  89.192  and loss:  977.5988689661026
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  40 / 64 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -68.03039413690567 , diff:  68.03039413690567
adv train loss:  -69.45503169298172 , diff:  1.4246375560760498
************ all values are small in this layer **********
layer  5  adv train finish, try to retain  28
test acc: top1 ->  66.804 ; top5 ->  87.134  and loss:  1092.101221382618
forward train acc: top1 ->  67.734375 ; top5 ->  86.34375  and loss:  275.65179628133774
test acc: top1 ->  69.592 ; top5 ->  89.148  and loss:  984.8411785364151
forward train acc: top1 ->  67.046875 ; top5 ->  85.84375  and loss:  282.629862010479
test acc: top1 ->  69.604 ; top5 ->  89.16  and loss:  981.111601293087
forward train acc: top1 ->  67.359375 ; top5 ->  85.921875  and loss:  274.4838775396347
test acc: top1 ->  69.678 ; top5 ->  89.09  and loss:  985.2476843893528
forward train acc: top1 ->  66.859375 ; top5 ->  86.3984375  and loss:  277.76778692007065
test acc: top1 ->  69.724 ; top5 ->  89.186  and loss:  982.756981253624
forward train acc: top1 ->  67.03125 ; top5 ->  85.796875  and loss:  277.46223175525665
test acc: top1 ->  69.744 ; top5 ->  89.142  and loss:  982.3715807795525
forward train acc: top1 ->  67.6171875 ; top5 ->  86.5390625  and loss:  273.3425639271736
test acc: top1 ->  69.684 ; top5 ->  89.172  and loss:  979.9671815633774
forward train acc: top1 ->  67.8359375 ; top5 ->  86.46875  and loss:  274.0252022743225
test acc: top1 ->  69.752 ; top5 ->  89.176  and loss:  978.1092851758003
forward train acc: top1 ->  67.5078125 ; top5 ->  86.09375  and loss:  276.0973363518715
test acc: top1 ->  69.862 ; top5 ->  89.25  and loss:  976.89082634449
forward train acc: top1 ->  67.2890625 ; top5 ->  86.3046875  and loss:  275.40630066394806
test acc: top1 ->  69.858 ; top5 ->  89.19  and loss:  978.9890394806862
forward train acc: top1 ->  68.3046875 ; top5 ->  86.421875  and loss:  271.26577240228653
test acc: top1 ->  69.81 ; top5 ->  89.204  and loss:  977.9627206921577
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  29 / 64 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -70.23416984081268 , diff:  70.23416984081268
adv train loss:  -70.6515662074089 , diff:  0.4173963665962219
layer  6  adv train finish, try to retain  107
test acc: top1 ->  66.322 ; top5 ->  87.22  and loss:  1092.3301500082016
forward train acc: top1 ->  66.5 ; top5 ->  85.5859375  and loss:  285.7685657143593
test acc: top1 ->  69.67 ; top5 ->  89.106  and loss:  983.153727710247
forward train acc: top1 ->  67.3046875 ; top5 ->  86.234375  and loss:  277.12003457546234
test acc: top1 ->  69.746 ; top5 ->  89.106  and loss:  982.8431241512299
forward train acc: top1 ->  67.5390625 ; top5 ->  85.859375  and loss:  275.58823239803314
test acc: top1 ->  69.816 ; top5 ->  89.154  and loss:  980.0293576717377
forward train acc: top1 ->  67.390625 ; top5 ->  85.625  and loss:  281.1282601952553
test acc: top1 ->  69.868 ; top5 ->  89.258  and loss:  976.8906016349792
forward train acc: top1 ->  67.4375 ; top5 ->  86.4296875  and loss:  277.33789271116257
test acc: top1 ->  69.882 ; top5 ->  89.142  and loss:  980.0799613595009
forward train acc: top1 ->  67.1953125 ; top5 ->  85.8359375  and loss:  278.53267365694046
test acc: top1 ->  70.018 ; top5 ->  89.25  and loss:  977.5773161053658
forward train acc: top1 ->  67.078125 ; top5 ->  86.265625  and loss:  276.5748666524887
test acc: top1 ->  70.058 ; top5 ->  89.262  and loss:  973.5503947138786
forward train acc: top1 ->  67.7109375 ; top5 ->  86.5703125  and loss:  272.273285984993
test acc: top1 ->  70.0 ; top5 ->  89.24  and loss:  977.1715607047081
forward train acc: top1 ->  67.578125 ; top5 ->  86.6328125  and loss:  273.683552980423
test acc: top1 ->  70.01 ; top5 ->  89.222  and loss:  977.5901398658752
forward train acc: top1 ->  67.9765625 ; top5 ->  86.2890625  and loss:  272.56210166215897
test acc: top1 ->  69.918 ; top5 ->  89.222  and loss:  977.9075397849083
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  112 / 128 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -68.43065720796585 , diff:  68.43065720796585
adv train loss:  -67.96344244480133 , diff:  0.46721476316452026
layer  7  adv train finish, try to retain  120
test acc: top1 ->  69.546 ; top5 ->  89.05  and loss:  988.1307269334793
forward train acc: top1 ->  67.609375 ; top5 ->  86.265625  and loss:  276.9978928565979
test acc: top1 ->  69.782 ; top5 ->  89.17  and loss:  980.0759430527687
forward train acc: top1 ->  68.4765625 ; top5 ->  86.7890625  and loss:  266.8070587515831
test acc: top1 ->  69.964 ; top5 ->  89.278  and loss:  979.8135585784912
forward train acc: top1 ->  67.1796875 ; top5 ->  85.8125  and loss:  281.5596509575844
test acc: top1 ->  70.02 ; top5 ->  89.294  and loss:  975.4493693709373
forward train acc: top1 ->  67.3203125 ; top5 ->  86.1015625  and loss:  276.9587503671646
test acc: top1 ->  70.032 ; top5 ->  89.212  and loss:  974.2095609903336
forward train acc: top1 ->  67.2734375 ; top5 ->  86.3828125  and loss:  275.61238569021225
test acc: top1 ->  70.122 ; top5 ->  89.336  and loss:  972.2477229833603
forward train acc: top1 ->  67.25 ; top5 ->  86.2265625  and loss:  274.1671162247658
test acc: top1 ->  70.074 ; top5 ->  89.272  and loss:  975.0276299715042
forward train acc: top1 ->  67.984375 ; top5 ->  86.234375  and loss:  272.2992148399353
test acc: top1 ->  70.144 ; top5 ->  89.334  and loss:  973.1212593019009
forward train acc: top1 ->  67.71875 ; top5 ->  86.4765625  and loss:  268.1035975217819
test acc: top1 ->  70.18 ; top5 ->  89.352  and loss:  971.0592292845249
forward train acc: top1 ->  67.640625 ; top5 ->  86.46875  and loss:  272.3080453276634
test acc: top1 ->  70.072 ; top5 ->  89.328  and loss:  969.7317000627518
forward train acc: top1 ->  67.8671875 ; top5 ->  86.34375  and loss:  275.05498284101486
test acc: top1 ->  70.152 ; top5 ->  89.302  and loss:  971.7045077979565
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  122 / 128 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -66.41205608844757 , diff:  66.41205608844757
adv train loss:  -68.44720494747162 , diff:  2.035148859024048
layer  8  adv train finish, try to retain  82
test acc: top1 ->  67.334 ; top5 ->  87.71  and loss:  1071.0699454545975
forward train acc: top1 ->  66.7890625 ; top5 ->  86.21875  and loss:  278.756784260273
test acc: top1 ->  69.52 ; top5 ->  89.08  and loss:  989.2357949614525
forward train acc: top1 ->  67.3203125 ; top5 ->  86.1640625  and loss:  278.3568142056465
test acc: top1 ->  69.526 ; top5 ->  89.054  and loss:  991.1231270432472
forward train acc: top1 ->  67.5390625 ; top5 ->  86.359375  and loss:  277.10104036331177
test acc: top1 ->  69.624 ; top5 ->  89.046  and loss:  988.7578864097595
forward train acc: top1 ->  67.6015625 ; top5 ->  86.0390625  and loss:  277.84030532836914
test acc: top1 ->  69.706 ; top5 ->  89.146  and loss:  984.8828964233398
forward train acc: top1 ->  67.109375 ; top5 ->  86.140625  and loss:  278.9100106358528
test acc: top1 ->  69.678 ; top5 ->  89.152  and loss:  985.9503867030144
forward train acc: top1 ->  66.15625 ; top5 ->  85.8515625  and loss:  283.0467857122421
test acc: top1 ->  69.67 ; top5 ->  89.102  and loss:  984.289644241333
forward train acc: top1 ->  67.453125 ; top5 ->  86.0078125  and loss:  277.07036167383194
test acc: top1 ->  69.852 ; top5 ->  89.156  and loss:  981.572647869587
forward train acc: top1 ->  66.9921875 ; top5 ->  86.2734375  and loss:  276.9709306359291
test acc: top1 ->  69.684 ; top5 ->  89.124  and loss:  984.3298144340515
forward train acc: top1 ->  67.7109375 ; top5 ->  86.3984375  and loss:  271.46022152900696
test acc: top1 ->  69.764 ; top5 ->  89.112  and loss:  982.4413439035416
forward train acc: top1 ->  67.4296875 ; top5 ->  86.21875  and loss:  275.2909718155861
test acc: top1 ->  69.766 ; top5 ->  89.178  and loss:  983.5510500967503
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -69.34642112255096 , diff:  69.34642112255096
adv train loss:  -71.15871000289917 , diff:  1.8122888803482056
layer  9  adv train finish, try to retain  116
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -70.41729265451431 , diff:  70.41729265451431
adv train loss:  -66.2782928943634 , diff:  4.138999760150909
layer  10  adv train finish, try to retain  81
test acc: top1 ->  68.62 ; top5 ->  88.588  and loss:  1020.8434271216393
forward train acc: top1 ->  67.03125 ; top5 ->  86.6015625  and loss:  275.1962282061577
test acc: top1 ->  69.57 ; top5 ->  89.174  and loss:  988.0093013048172
forward train acc: top1 ->  66.671875 ; top5 ->  85.9296875  and loss:  282.2035531401634
test acc: top1 ->  69.53 ; top5 ->  89.17  and loss:  989.1875375509262
forward train acc: top1 ->  66.8828125 ; top5 ->  86.078125  and loss:  277.55528151988983
test acc: top1 ->  69.588 ; top5 ->  89.16  and loss:  985.9301755428314
forward train acc: top1 ->  66.671875 ; top5 ->  85.8984375  and loss:  278.3781815767288
test acc: top1 ->  69.686 ; top5 ->  89.222  and loss:  986.1331504583359
forward train acc: top1 ->  67.3515625 ; top5 ->  85.9140625  and loss:  277.30165207386017
test acc: top1 ->  69.808 ; top5 ->  89.214  and loss:  983.4297722280025
forward train acc: top1 ->  68.015625 ; top5 ->  86.5546875  and loss:  269.1182543039322
test acc: top1 ->  69.79 ; top5 ->  89.256  and loss:  980.1428126692772
forward train acc: top1 ->  67.7109375 ; top5 ->  86.578125  and loss:  272.6156389117241
test acc: top1 ->  69.812 ; top5 ->  89.27  and loss:  980.6993810534477
forward train acc: top1 ->  67.828125 ; top5 ->  86.90625  and loss:  270.71709299087524
test acc: top1 ->  69.8 ; top5 ->  89.262  and loss:  981.9079684615135
forward train acc: top1 ->  67.3203125 ; top5 ->  86.109375  and loss:  276.668294608593
test acc: top1 ->  69.88 ; top5 ->  89.322  and loss:  979.8869733512402
forward train acc: top1 ->  66.859375 ; top5 ->  86.1015625  and loss:  276.5935138463974
test acc: top1 ->  69.868 ; top5 ->  89.328  and loss:  978.7742585539818
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -71.9754131436348 , diff:  71.9754131436348
adv train loss:  -67.93673878908157 , diff:  4.038674354553223
layer  11  adv train finish, try to retain  97
test acc: top1 ->  68.216 ; top5 ->  88.322  and loss:  1033.814742922783
forward train acc: top1 ->  67.53125 ; top5 ->  86.6953125  and loss:  276.36752074956894
test acc: top1 ->  69.882 ; top5 ->  89.234  and loss:  981.5738782286644
forward train acc: top1 ->  67.125 ; top5 ->  85.890625  and loss:  281.00578635931015
test acc: top1 ->  69.944 ; top5 ->  89.276  and loss:  977.7092885971069
forward train acc: top1 ->  66.6796875 ; top5 ->  85.8359375  and loss:  278.4402695298195
test acc: top1 ->  69.902 ; top5 ->  89.3  and loss:  978.7715344429016
forward train acc: top1 ->  67.2578125 ; top5 ->  86.15625  and loss:  279.985180914402
test acc: top1 ->  70.012 ; top5 ->  89.37  and loss:  975.3616522550583
forward train acc: top1 ->  67.78125 ; top5 ->  86.296875  and loss:  271.78268444538116
test acc: top1 ->  70.1 ; top5 ->  89.348  and loss:  973.0879687070847
forward train acc: top1 ->  67.4921875 ; top5 ->  85.828125  and loss:  280.35609209537506
test acc: top1 ->  70.112 ; top5 ->  89.27  and loss:  973.5172294974327
forward train acc: top1 ->  67.21875 ; top5 ->  86.21875  and loss:  275.3566372990608
test acc: top1 ->  70.062 ; top5 ->  89.29  and loss:  972.5597999691963
forward train acc: top1 ->  67.65625 ; top5 ->  86.7421875  and loss:  271.8618052005768
test acc: top1 ->  70.02 ; top5 ->  89.348  and loss:  973.0393707752228
forward train acc: top1 ->  67.3828125 ; top5 ->  86.328125  and loss:  273.1519020795822
test acc: top1 ->  70.046 ; top5 ->  89.332  and loss:  972.4109180569649
forward train acc: top1 ->  67.5078125 ; top5 ->  86.6015625  and loss:  274.65163946151733
test acc: top1 ->  70.13 ; top5 ->  89.354  and loss:  972.2577818632126
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -69.42938631772995 , diff:  69.42938631772995
adv train loss:  -67.96701157093048 , diff:  1.462374746799469
layer  12  adv train finish, try to retain  101
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -69.31242942810059 , diff:  69.31242942810059
adv train loss:  -70.7352643609047 , diff:  1.4228349328041077
layer  13  adv train finish, try to retain  88
>>>>>>> reverse layer  13  since no improvement >>>>>>>
---------------- start layer  14  ---------------
adv train loss:  -66.87814456224442 , diff:  66.87814456224442
adv train loss:  -70.34772193431854 , diff:  3.469577372074127
layer  14  adv train finish, try to retain  247
test acc: top1 ->  69.81 ; top5 ->  89.226  and loss:  981.0707372426987
forward train acc: top1 ->  66.890625 ; top5 ->  85.6953125  and loss:  280.9883766770363
test acc: top1 ->  69.976 ; top5 ->  89.234  and loss:  977.9200183749199
forward train acc: top1 ->  67.859375 ; top5 ->  86.1015625  and loss:  276.15045338869095
test acc: top1 ->  69.828 ; top5 ->  89.276  and loss:  977.3822281360626
forward train acc: top1 ->  67.734375 ; top5 ->  86.515625  and loss:  271.8739700913429
test acc: top1 ->  70.09 ; top5 ->  89.31  and loss:  976.3120355606079
forward train acc: top1 ->  67.84375 ; top5 ->  86.5234375  and loss:  271.7741884589195
test acc: top1 ->  70.106 ; top5 ->  89.26  and loss:  975.5139053463936
forward train acc: top1 ->  67.4921875 ; top5 ->  86.0390625  and loss:  278.2473198771477
test acc: top1 ->  70.112 ; top5 ->  89.406  and loss:  972.3735654354095
forward train acc: top1 ->  67.7578125 ; top5 ->  86.2734375  and loss:  273.59098356962204
test acc: top1 ->  70.104 ; top5 ->  89.324  and loss:  973.0367369651794
forward train acc: top1 ->  67.8125 ; top5 ->  86.5078125  and loss:  275.3232758641243
test acc: top1 ->  70.046 ; top5 ->  89.352  and loss:  975.4542304873466
forward train acc: top1 ->  67.8046875 ; top5 ->  86.8828125  and loss:  271.22083550691605
test acc: top1 ->  70.168 ; top5 ->  89.312  and loss:  973.580072581768
forward train acc: top1 ->  68.2890625 ; top5 ->  86.140625  and loss:  272.9672228693962
test acc: top1 ->  70.114 ; top5 ->  89.276  and loss:  973.4966978430748
forward train acc: top1 ->  67.234375 ; top5 ->  86.734375  and loss:  271.90505850315094
test acc: top1 ->  70.1 ; top5 ->  89.294  and loss:  973.3637863397598
>>>>>>> reverse layer  14  since performance drop >>>>>>>
==> this epoch:  250 / 256 , inc:  1
---------------- start layer  15  ---------------
adv train loss:  -68.20552843809128 , diff:  68.20552843809128
adv train loss:  -67.98073428869247 , diff:  0.2247941493988037
layer  15  adv train finish, try to retain  246
test acc: top1 ->  69.478 ; top5 ->  89.068  and loss:  991.4758051633835
forward train acc: top1 ->  67.5078125 ; top5 ->  86.359375  and loss:  275.7413714528084
test acc: top1 ->  69.794 ; top5 ->  89.112  and loss:  980.4274590611458
forward train acc: top1 ->  67.421875 ; top5 ->  85.9609375  and loss:  278.5733321905136
test acc: top1 ->  69.826 ; top5 ->  89.128  and loss:  978.8925645947456
forward train acc: top1 ->  66.890625 ; top5 ->  85.59375  and loss:  281.99665224552155
test acc: top1 ->  69.926 ; top5 ->  89.13  and loss:  981.2097506523132
forward train acc: top1 ->  67.4765625 ; top5 ->  86.21875  and loss:  270.80194276571274
test acc: top1 ->  69.882 ; top5 ->  89.154  and loss:  978.9798449277878
forward train acc: top1 ->  67.8828125 ; top5 ->  86.2578125  and loss:  273.48137933015823
test acc: top1 ->  69.956 ; top5 ->  89.16  and loss:  976.8526888489723
forward train acc: top1 ->  67.8203125 ; top5 ->  86.5  and loss:  270.63058614730835
test acc: top1 ->  69.924 ; top5 ->  89.21  and loss:  974.6450378894806
forward train acc: top1 ->  67.5234375 ; top5 ->  85.7578125  and loss:  278.5225755572319
test acc: top1 ->  69.994 ; top5 ->  89.164  and loss:  975.1653469204903
forward train acc: top1 ->  68.0625 ; top5 ->  86.609375  and loss:  270.8247836828232
test acc: top1 ->  69.962 ; top5 ->  89.232  and loss:  975.2293491959572
forward train acc: top1 ->  67.71875 ; top5 ->  86.578125  and loss:  272.27338486909866
test acc: top1 ->  69.932 ; top5 ->  89.268  and loss:  975.1999527812004
forward train acc: top1 ->  67.5703125 ; top5 ->  86.171875  and loss:  275.00111931562424
test acc: top1 ->  70.014 ; top5 ->  89.25  and loss:  975.0602732300758
>>>>>>> reverse layer  15  since performance drop >>>>>>>
==> this epoch:  248 / 256 , inc:  1
---------------- start layer  16  ---------------
adv train loss:  -68.26042091846466 , diff:  68.26042091846466
adv train loss:  -70.53056800365448 , diff:  2.2701470851898193
************ all values are small in this layer **********
layer  16  adv train finish, try to retain  238
test acc: top1 ->  69.892 ; top5 ->  89.234  and loss:  976.8121358752251
forward train acc: top1 ->  67.6328125 ; top5 ->  86.4375  and loss:  272.1314782500267
test acc: top1 ->  70.024 ; top5 ->  89.33  and loss:  975.9569477438927
forward train acc: top1 ->  68.0859375 ; top5 ->  86.4296875  and loss:  275.23201245069504
test acc: top1 ->  70.044 ; top5 ->  89.238  and loss:  974.3848953843117
forward train acc: top1 ->  67.859375 ; top5 ->  86.4140625  and loss:  275.15943586826324
test acc: top1 ->  69.886 ; top5 ->  89.35  and loss:  974.370859503746
forward train acc: top1 ->  68.0390625 ; top5 ->  86.6171875  and loss:  270.990194439888
test acc: top1 ->  70.004 ; top5 ->  89.346  and loss:  971.6952221393585
forward train acc: top1 ->  68.09375 ; top5 ->  86.546875  and loss:  271.4569723010063
test acc: top1 ->  70.022 ; top5 ->  89.328  and loss:  971.4436700344086
forward train acc: top1 ->  67.171875 ; top5 ->  86.015625  and loss:  278.72842913866043
test acc: top1 ->  70.008 ; top5 ->  89.386  and loss:  972.1368699669838
forward train acc: top1 ->  67.9140625 ; top5 ->  85.921875  and loss:  275.8581554889679
test acc: top1 ->  69.998 ; top5 ->  89.424  and loss:  971.7907192111015
forward train acc: top1 ->  67.796875 ; top5 ->  86.5  and loss:  272.21516531705856
test acc: top1 ->  69.942 ; top5 ->  89.38  and loss:  973.9669108986855
forward train acc: top1 ->  67.171875 ; top5 ->  86.1015625  and loss:  276.6124935746193
test acc: top1 ->  70.112 ; top5 ->  89.408  and loss:  970.4883371591568
forward train acc: top1 ->  67.859375 ; top5 ->  86.578125  and loss:  269.16064888238907
test acc: top1 ->  70.086 ; top5 ->  89.418  and loss:  969.726961016655
>>>>>>> reverse layer  16  since performance drop >>>>>>>
==> this epoch:  239 / 256 , inc:  1
---------------- start layer  17  ---------------
adv train loss:  -68.55212914943695 , diff:  68.55212914943695
adv train loss:  -70.70451575517654 , diff:  2.1523866057395935
************ all values are small in this layer **********
layer  17  adv train finish, try to retain  229
test acc: top1 ->  69.41 ; top5 ->  89.058  and loss:  993.6551948189735
forward train acc: top1 ->  67.375 ; top5 ->  85.90625  and loss:  278.002993285656
test acc: top1 ->  69.858 ; top5 ->  89.232  and loss:  978.4260920882225
forward train acc: top1 ->  67.2421875 ; top5 ->  86.2890625  and loss:  275.26088654994965
test acc: top1 ->  69.784 ; top5 ->  89.238  and loss:  978.6319189667702
forward train acc: top1 ->  67.8203125 ; top5 ->  86.5703125  and loss:  271.4278280735016
test acc: top1 ->  69.908 ; top5 ->  89.234  and loss:  974.8664408028126
forward train acc: top1 ->  67.9609375 ; top5 ->  86.71875  and loss:  269.0748407840729
test acc: top1 ->  69.988 ; top5 ->  89.346  and loss:  975.346461057663
forward train acc: top1 ->  67.71875 ; top5 ->  86.359375  and loss:  275.18125915527344
test acc: top1 ->  69.962 ; top5 ->  89.328  and loss:  973.7829032540321
forward train acc: top1 ->  67.671875 ; top5 ->  86.15625  and loss:  273.8326410651207
test acc: top1 ->  69.982 ; top5 ->  89.28  and loss:  976.8687560558319
forward train acc: top1 ->  67.125 ; top5 ->  85.9765625  and loss:  278.23459404706955
test acc: top1 ->  70.042 ; top5 ->  89.342  and loss:  971.0279005765915
forward train acc: top1 ->  67.59375 ; top5 ->  86.234375  and loss:  276.3965169787407
test acc: top1 ->  70.048 ; top5 ->  89.352  and loss:  971.8683778941631
forward train acc: top1 ->  67.0859375 ; top5 ->  86.3515625  and loss:  276.68500006198883
test acc: top1 ->  70.142 ; top5 ->  89.358  and loss:  973.9586728811264
forward train acc: top1 ->  67.6484375 ; top5 ->  86.125  and loss:  273.0955473780632
test acc: top1 ->  70.12 ; top5 ->  89.33  and loss:  974.622941672802
>>>>>>> reverse layer  17  since performance drop >>>>>>>
==> this epoch:  230 / 256 , inc:  1
---------------- start layer  18  ---------------
adv train loss:  -67.99641823768616 , diff:  67.99641823768616
adv train loss:  -66.56664800643921 , diff:  1.4297702312469482
************ all values are small in this layer **********
layer  18  adv train finish, try to retain  234
test acc: top1 ->  70.058 ; top5 ->  89.292  and loss:  975.6634908914566
forward train acc: top1 ->  67.453125 ; top5 ->  85.90625  and loss:  276.0921207666397
test acc: top1 ->  70.038 ; top5 ->  89.24  and loss:  977.9578585028648
forward train acc: top1 ->  67.2890625 ; top5 ->  85.921875  and loss:  276.63433814048767
test acc: top1 ->  70.084 ; top5 ->  89.244  and loss:  976.147062599659
forward train acc: top1 ->  67.46875 ; top5 ->  86.609375  and loss:  272.9386082291603
test acc: top1 ->  69.938 ; top5 ->  89.204  and loss:  980.6072382926941
forward train acc: top1 ->  67.3203125 ; top5 ->  86.078125  and loss:  276.58510768413544
test acc: top1 ->  70.126 ; top5 ->  89.26  and loss:  976.388184428215
forward train acc: top1 ->  67.828125 ; top5 ->  86.1328125  and loss:  274.9174247980118
test acc: top1 ->  70.096 ; top5 ->  89.336  and loss:  973.4505943655968
forward train acc: top1 ->  66.875 ; top5 ->  86.3046875  and loss:  275.27477157115936
test acc: top1 ->  70.17 ; top5 ->  89.34  and loss:  974.0336083769798
forward train acc: top1 ->  67.7421875 ; top5 ->  86.1875  and loss:  273.64504104852676
test acc: top1 ->  70.134 ; top5 ->  89.31  and loss:  973.4210891723633
forward train acc: top1 ->  67.625 ; top5 ->  86.7265625  and loss:  272.38113337755203
test acc: top1 ->  70.19 ; top5 ->  89.372  and loss:  974.2651200890541
forward train acc: top1 ->  67.71875 ; top5 ->  86.390625  and loss:  271.7266881465912
test acc: top1 ->  70.182 ; top5 ->  89.26  and loss:  972.544971704483
forward train acc: top1 ->  67.7109375 ; top5 ->  86.5546875  and loss:  273.69590061903
test acc: top1 ->  70.182 ; top5 ->  89.328  and loss:  971.2325939536095
>>>>>>> reverse layer  18  since performance drop >>>>>>>
==> this epoch:  235 / 256 , inc:  1
---------------- start layer  19  ---------------
adv train loss:  -65.8308658003807 , diff:  65.8308658003807
adv train loss:  -68.76300722360611 , diff:  2.932141423225403
layer  19  adv train finish, try to retain  185
>>>>>>> reverse layer  19  since no improvement >>>>>>>
---------------- start layer  20  ---------------
adv train loss:  -67.74832266569138 , diff:  67.74832266569138
adv train loss:  -67.33501070737839 , diff:  0.4133119583129883
************ all values are small in this layer **********
layer  20  adv train finish, try to retain  224
test acc: top1 ->  70.052 ; top5 ->  89.234  and loss:  976.9904115200043
forward train acc: top1 ->  67.546875 ; top5 ->  86.2109375  and loss:  276.4142230153084
test acc: top1 ->  70.048 ; top5 ->  89.326  and loss:  974.5555540919304
forward train acc: top1 ->  67.75 ; top5 ->  86.03125  and loss:  276.62894028425217
test acc: top1 ->  69.96 ; top5 ->  89.318  and loss:  975.0339246988297
forward train acc: top1 ->  67.6796875 ; top5 ->  86.03125  and loss:  277.46474009752274
test acc: top1 ->  70.102 ; top5 ->  89.292  and loss:  972.9698427915573
forward train acc: top1 ->  67.3984375 ; top5 ->  86.1796875  and loss:  278.9281202554703
test acc: top1 ->  69.956 ; top5 ->  89.276  and loss:  974.0439658761024
forward train acc: top1 ->  67.2734375 ; top5 ->  85.953125  and loss:  274.8782616853714
test acc: top1 ->  70.058 ; top5 ->  89.402  and loss:  971.7203649878502
forward train acc: top1 ->  68.1015625 ; top5 ->  86.625  and loss:  270.999471783638
test acc: top1 ->  70.03 ; top5 ->  89.388  and loss:  973.8593603372574
forward train acc: top1 ->  67.2734375 ; top5 ->  86.3125  and loss:  273.92264407873154
test acc: top1 ->  70.014 ; top5 ->  89.366  and loss:  970.9297308325768
forward train acc: top1 ->  67.171875 ; top5 ->  86.2578125  and loss:  279.1545022726059
test acc: top1 ->  70.016 ; top5 ->  89.382  and loss:  969.2654258608818
forward train acc: top1 ->  67.703125 ; top5 ->  86.7734375  and loss:  270.7837832570076
test acc: top1 ->  70.122 ; top5 ->  89.416  and loss:  969.7937830090523
forward train acc: top1 ->  67.7421875 ; top5 ->  85.7421875  and loss:  279.9332403540611
test acc: top1 ->  70.086 ; top5 ->  89.372  and loss:  969.8835607767105
>>>>>>> reverse layer  20  since performance drop >>>>>>>
==> this epoch:  225 / 256 , inc:  1
---------------- start layer  21  ---------------
adv train loss:  -69.25304168462753 , diff:  69.25304168462753
adv train loss:  -65.70678228139877 , diff:  3.5462594032287598
layer  21  adv train finish, try to retain  248
>>>>>>> reverse layer  21  since no improvement >>>>>>>
---------------- start layer  22  ---------------
adv train loss:  -71.3638134598732 , diff:  71.3638134598732
adv train loss:  -69.2448000907898 , diff:  2.1190133690834045
layer  22  adv train finish, try to retain  218
>>>>>>> reverse layer  22  since no improvement >>>>>>>
---------------- start layer  23  ---------------
adv train loss:  -65.4127305150032 , diff:  65.4127305150032
adv train loss:  -69.01339310407639 , diff:  3.600662589073181
layer  23  adv train finish, try to retain  198
>>>>>>> reverse layer  23  since no improvement >>>>>>>
---------------- start layer  24  ---------------
adv train loss:  -67.43300521373749 , diff:  67.43300521373749
adv train loss:  -67.88595086336136 , diff:  0.45294564962387085
layer  24  adv train finish, try to retain  247
>>>>>>> reverse layer  24  since no improvement >>>>>>>
---------------- start layer  25  ---------------
adv train loss:  -68.36685490608215 , diff:  68.36685490608215
adv train loss:  -66.7544509768486 , diff:  1.612403929233551
layer  25  adv train finish, try to retain  244
>>>>>>> reverse layer  25  since no improvement >>>>>>>
---------------- start layer  26  ---------------
adv train loss:  -69.05258613824844 , diff:  69.05258613824844
adv train loss:  -66.40873354673386 , diff:  2.6438525915145874
layer  26  adv train finish, try to retain  502
>>>>>>> reverse layer  26  since no improvement >>>>>>>
---------------- start layer  27  ---------------
adv train loss:  -68.20466136932373 , diff:  68.20466136932373
adv train loss:  -67.52821868658066 , diff:  0.6764426827430725
layer  27  adv train finish, try to retain  506
>>>>>>> reverse layer  27  since no improvement >>>>>>>
---------------- start layer  28  ---------------
adv train loss:  -64.28069859743118 , diff:  64.28069859743118
adv train loss:  -69.92925816774368 , diff:  5.6485595703125
layer  28  adv train finish, try to retain  503
>>>>>>> reverse layer  28  since no improvement >>>>>>>
---------------- start layer  29  ---------------
adv train loss:  -70.18494135141373 , diff:  70.18494135141373
adv train loss:  -68.6888957619667 , diff:  1.4960455894470215
layer  29  adv train finish, try to retain  502
>>>>>>> reverse layer  29  since no improvement >>>>>>>
---------------- start layer  30  ---------------
adv train loss:  -68.8855710029602 , diff:  68.8855710029602
adv train loss:  -66.69630891084671 , diff:  2.189262092113495
layer  30  adv train finish, try to retain  485
test acc: top1 ->  69.692 ; top5 ->  89.226  and loss:  978.6284046769142
forward train acc: top1 ->  67.6875 ; top5 ->  86.6640625  and loss:  275.1341495513916
test acc: top1 ->  69.614 ; top5 ->  89.164  and loss:  981.9773279428482
forward train acc: top1 ->  66.8515625 ; top5 ->  85.8203125  and loss:  283.0058863759041
test acc: top1 ->  69.876 ; top5 ->  89.212  and loss:  978.8921627998352
forward train acc: top1 ->  67.125 ; top5 ->  86.1015625  and loss:  278.63639318943024
test acc: top1 ->  69.856 ; top5 ->  89.13  and loss:  982.621199786663
forward train acc: top1 ->  67.6953125 ; top5 ->  86.5390625  and loss:  273.75855618715286
test acc: top1 ->  69.878 ; top5 ->  89.242  and loss:  977.3892592191696
forward train acc: top1 ->  68.0546875 ; top5 ->  86.859375  and loss:  269.2969337105751
test acc: top1 ->  69.972 ; top5 ->  89.296  and loss:  977.5693339109421
forward train acc: top1 ->  67.7734375 ; top5 ->  86.3046875  and loss:  273.4168578386307
test acc: top1 ->  69.908 ; top5 ->  89.3  and loss:  975.4254720211029
forward train acc: top1 ->  67.6640625 ; top5 ->  86.203125  and loss:  276.1521881222725
test acc: top1 ->  69.972 ; top5 ->  89.318  and loss:  975.896364569664
forward train acc: top1 ->  67.78125 ; top5 ->  86.03125  and loss:  278.11723428964615
test acc: top1 ->  70.026 ; top5 ->  89.318  and loss:  974.1674839258194
forward train acc: top1 ->  68.125 ; top5 ->  86.7734375  and loss:  267.32850950956345
test acc: top1 ->  69.954 ; top5 ->  89.322  and loss:  975.8650462329388
forward train acc: top1 ->  67.0859375 ; top5 ->  85.921875  and loss:  279.4155378937721
test acc: top1 ->  70.054 ; top5 ->  89.296  and loss:  974.968487560749
>>>>>>> reverse layer  30  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  31  ---------------
adv train loss:  -65.5663013458252 , diff:  65.5663013458252
adv train loss:  -68.7243874669075 , diff:  3.158086121082306
layer  31  adv train finish, try to retain  501
>>>>>>> reverse layer  31  since no improvement >>>>>>>
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.01125, 0.01125, 0.0010546875, 0.01125, 0.01125, 0.01125, 0.0010546875, 0.00019775390625, 0.005625, 0.00052734375, 0.005625, 0.002109375, 0.005625, 0.015, 9.8876953125e-05, 9.8876953125e-05, 0.000263671875, 0.000263671875, 0.000263671875, 0.0075, 0.000263671875, 0.000263671875, 0.0028125, 0.0075, 0.000263671875, 0.000263671875, 0.0001318359375, 0.0001318359375, 0.0001318359375, 0.0001318359375, 0.0001318359375, 0.0001318359375]  wait [2, 2, 2, 2, 2, 2, 3, 4, 2, 2, 2, 4, 2, 0, 4, 4, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 1
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  9  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -69.37130403518677 , diff:  69.37130403518677
adv train loss:  -70.23703503608704 , diff:  0.8657310009002686
layer  0  adv train finish, try to retain  39
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -67.39985764026642 , diff:  67.39985764026642
adv train loss:  -71.22634017467499 , diff:  3.8264825344085693
layer  1  adv train finish, try to retain  46
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -66.4880992770195 , diff:  66.4880992770195
adv train loss:  -67.92862719297409 , diff:  1.4405279159545898
layer  2  adv train finish, try to retain  50
test acc: top1 ->  69.248 ; top5 ->  88.932  and loss:  1001.2670153975487
forward train acc: top1 ->  67.734375 ; top5 ->  86.5546875  and loss:  270.9182963371277
test acc: top1 ->  69.956 ; top5 ->  89.262  and loss:  979.9503242969513
forward train acc: top1 ->  68.453125 ; top5 ->  87.0  and loss:  267.3721060156822
test acc: top1 ->  69.994 ; top5 ->  89.19  and loss:  978.1653133630753
forward train acc: top1 ->  67.75 ; top5 ->  86.2109375  and loss:  276.09013253450394
test acc: top1 ->  69.998 ; top5 ->  89.172  and loss:  980.644947707653
forward train acc: top1 ->  66.8828125 ; top5 ->  86.2734375  and loss:  275.2383471131325
test acc: top1 ->  70.014 ; top5 ->  89.312  and loss:  975.8461386561394
forward train acc: top1 ->  67.359375 ; top5 ->  86.4765625  and loss:  273.75254505872726
test acc: top1 ->  70.07 ; top5 ->  89.294  and loss:  978.1105009317398
forward train acc: top1 ->  67.7109375 ; top5 ->  86.28125  and loss:  272.8562898635864
test acc: top1 ->  70.096 ; top5 ->  89.336  and loss:  974.6974254250526
forward train acc: top1 ->  68.3046875 ; top5 ->  86.5234375  and loss:  270.7301554083824
test acc: top1 ->  70.06 ; top5 ->  89.324  and loss:  972.19211769104
forward train acc: top1 ->  67.734375 ; top5 ->  86.7109375  and loss:  271.9109782576561
test acc: top1 ->  70.14 ; top5 ->  89.318  and loss:  973.2467187345028
forward train acc: top1 ->  68.0078125 ; top5 ->  86.625  and loss:  271.5773077607155
test acc: top1 ->  70.194 ; top5 ->  89.314  and loss:  971.7400034666061
forward train acc: top1 ->  67.578125 ; top5 ->  86.390625  and loss:  275.0283641219139
test acc: top1 ->  70.156 ; top5 ->  89.324  and loss:  971.8095806241035
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  57 / 64 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -69.79948312044144 , diff:  69.79948312044144
adv train loss:  -67.04307287931442 , diff:  2.756410241127014
layer  3  adv train finish, try to retain  45
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -68.42309254407883 , diff:  68.42309254407883
adv train loss:  -64.43975847959518 , diff:  3.9833340644836426
layer  4  adv train finish, try to retain  42
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -69.50173425674438 , diff:  69.50173425674438
adv train loss:  -66.34429717063904 , diff:  3.1574370861053467
layer  5  adv train finish, try to retain  44
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
### skip layer  6 wait:  3  ###
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
adv train loss:  -66.6698814034462 , diff:  66.6698814034462
adv train loss:  -65.85896068811417 , diff:  0.8109207153320312
layer  8  adv train finish, try to retain  89
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -68.04130679368973 , diff:  68.04130679368973
adv train loss:  -70.57804495096207 , diff:  2.536738157272339
layer  9  adv train finish, try to retain  122
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -65.30557352304459 , diff:  65.30557352304459
adv train loss:  -66.94203507900238 , diff:  1.6364615559577942
layer  10  adv train finish, try to retain  93
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
### skip layer  11 wait:  4  ###
---------------- start layer  12  ---------------
adv train loss:  -67.50972217321396 , diff:  67.50972217321396
adv train loss:  -66.56358927488327 , diff:  0.9461328983306885
layer  12  adv train finish, try to retain  91
test acc: top1 ->  69.144 ; top5 ->  88.782  and loss:  1006.7093625664711
forward train acc: top1 ->  67.3984375 ; top5 ->  85.984375  and loss:  275.96279752254486
test acc: top1 ->  69.866 ; top5 ->  89.202  and loss:  980.3614204525948
forward train acc: top1 ->  67.734375 ; top5 ->  86.1640625  and loss:  275.5876013636589
test acc: top1 ->  69.85 ; top5 ->  89.264  and loss:  979.7439213395119
forward train acc: top1 ->  67.34375 ; top5 ->  85.9375  and loss:  278.17473942041397
test acc: top1 ->  69.9 ; top5 ->  89.272  and loss:  980.1666721105576
forward train acc: top1 ->  68.03125 ; top5 ->  86.65625  and loss:  269.9005438089371
test acc: top1 ->  69.944 ; top5 ->  89.302  and loss:  976.1284901499748
forward train acc: top1 ->  67.3125 ; top5 ->  86.078125  and loss:  275.29467433691025
test acc: top1 ->  69.99 ; top5 ->  89.354  and loss:  976.8366910219193
forward train acc: top1 ->  67.0625 ; top5 ->  85.9453125  and loss:  277.72069025039673
test acc: top1 ->  70.042 ; top5 ->  89.368  and loss:  973.3699730634689
forward train acc: top1 ->  67.2265625 ; top5 ->  86.203125  and loss:  274.86630630493164
test acc: top1 ->  69.968 ; top5 ->  89.35  and loss:  976.3733899593353
forward train acc: top1 ->  67.640625 ; top5 ->  86.3359375  and loss:  272.01527643203735
test acc: top1 ->  70.004 ; top5 ->  89.35  and loss:  974.4856031239033
forward train acc: top1 ->  67.125 ; top5 ->  86.203125  and loss:  276.3499143719673
test acc: top1 ->  70.134 ; top5 ->  89.34  and loss:  972.9066193699837
forward train acc: top1 ->  67.8828125 ; top5 ->  86.953125  and loss:  270.1917824149132
test acc: top1 ->  70.114 ; top5 ->  89.376  and loss:  973.9023163318634
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  98 / 128 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -69.70978212356567 , diff:  69.70978212356567
adv train loss:  -67.59963715076447 , diff:  2.1101449728012085
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  70
test acc: top1 ->  66.728 ; top5 ->  87.318  and loss:  1093.002981543541
forward train acc: top1 ->  67.4765625 ; top5 ->  86.3359375  and loss:  275.14951264858246
test acc: top1 ->  69.494 ; top5 ->  89.012  and loss:  990.188322365284
forward train acc: top1 ->  66.8515625 ; top5 ->  86.125  and loss:  282.35993111133575
test acc: top1 ->  69.654 ; top5 ->  89.124  and loss:  988.706042945385
forward train acc: top1 ->  66.6484375 ; top5 ->  86.0078125  and loss:  279.98655331134796
test acc: top1 ->  69.806 ; top5 ->  89.142  and loss:  984.1299482584
forward train acc: top1 ->  67.296875 ; top5 ->  86.109375  and loss:  278.2901938557625
test acc: top1 ->  69.718 ; top5 ->  89.154  and loss:  987.877487719059
forward train acc: top1 ->  67.3203125 ; top5 ->  85.671875  and loss:  281.03062814474106
test acc: top1 ->  69.776 ; top5 ->  89.114  and loss:  981.5485742092133
forward train acc: top1 ->  67.890625 ; top5 ->  86.1640625  and loss:  274.8038999438286
test acc: top1 ->  69.716 ; top5 ->  89.186  and loss:  981.8998581469059
forward train acc: top1 ->  67.9296875 ; top5 ->  86.5  and loss:  268.9076998233795
test acc: top1 ->  69.842 ; top5 ->  89.176  and loss:  981.1058539152145
forward train acc: top1 ->  67.5234375 ; top5 ->  87.0  and loss:  269.97755962610245
test acc: top1 ->  69.734 ; top5 ->  89.132  and loss:  981.8868392109871
forward train acc: top1 ->  67.421875 ; top5 ->  86.21875  and loss:  275.2608907818794
test acc: top1 ->  69.71 ; top5 ->  89.214  and loss:  978.4174526929855
forward train acc: top1 ->  67.59375 ; top5 ->  86.640625  and loss:  269.55209493637085
test acc: top1 ->  69.866 ; top5 ->  89.206  and loss:  979.4936049282551
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  71 / 128 , inc:  1
---------------- start layer  14  ---------------
### skip layer  14 wait:  4  ###
---------------- start layer  15  ---------------
### skip layer  15 wait:  4  ###
---------------- start layer  16  ---------------
adv train loss:  -68.12905156612396 , diff:  68.12905156612396
adv train loss:  -70.96372413635254 , diff:  2.8346725702285767
layer  16  adv train finish, try to retain  240
>>>>>>> reverse layer  16  since no improvement >>>>>>>
---------------- start layer  17  ---------------
adv train loss:  -67.828808426857 , diff:  67.828808426857
adv train loss:  -67.97329032421112 , diff:  0.14448189735412598
layer  17  adv train finish, try to retain  241
>>>>>>> reverse layer  17  since no improvement >>>>>>>
---------------- start layer  18  ---------------
adv train loss:  -67.01690638065338 , diff:  67.01690638065338
adv train loss:  -73.44204008579254 , diff:  6.42513370513916
layer  18  adv train finish, try to retain  233
test acc: top1 ->  70.036 ; top5 ->  89.258  and loss:  972.4040338695049
forward train acc: top1 ->  67.8359375 ; top5 ->  86.390625  and loss:  273.1181990504265
test acc: top1 ->  70.142 ; top5 ->  89.344  and loss:  973.4718761146069
forward train acc: top1 ->  67.609375 ; top5 ->  86.421875  and loss:  276.3833774924278
test acc: top1 ->  70.106 ; top5 ->  89.314  and loss:  972.714571416378
forward train acc: top1 ->  67.3359375 ; top5 ->  86.15625  and loss:  272.2824102640152
test acc: top1 ->  69.952 ; top5 ->  89.29  and loss:  974.9029330909252
forward train acc: top1 ->  67.9609375 ; top5 ->  86.2265625  and loss:  273.33281087875366
test acc: top1 ->  70.082 ; top5 ->  89.36  and loss:  971.5996614694595
forward train acc: top1 ->  67.8046875 ; top5 ->  86.7578125  and loss:  265.50233268737793
test acc: top1 ->  70.202 ; top5 ->  89.408  and loss:  969.9112589359283
forward train acc: top1 ->  67.8359375 ; top5 ->  86.5390625  and loss:  270.63172447681427
test acc: top1 ->  70.194 ; top5 ->  89.338  and loss:  969.7710909843445
forward train acc: top1 ->  68.1484375 ; top5 ->  87.0078125  and loss:  264.85683768987656
test acc: top1 ->  70.246 ; top5 ->  89.384  and loss:  967.5914868712425
forward train acc: top1 ->  69.296875 ; top5 ->  87.5234375  and loss:  259.60841685533524
test acc: top1 ->  70.358 ; top5 ->  89.384  and loss:  966.9689771533012
forward train acc: top1 ->  68.0390625 ; top5 ->  86.3359375  and loss:  276.3605563044548
test acc: top1 ->  70.282 ; top5 ->  89.372  and loss:  968.5846478641033
forward train acc: top1 ->  68.4140625 ; top5 ->  86.390625  and loss:  273.624018907547
test acc: top1 ->  70.336 ; top5 ->  89.394  and loss:  965.4875224232674
>>>>>>> reverse layer  18  since performance drop >>>>>>>
==> this epoch:  235 / 256 , inc:  1
---------------- start layer  19  ---------------
adv train loss:  -65.50312077999115 , diff:  65.50312077999115
adv train loss:  -67.7446756362915 , diff:  2.241554856300354
layer  19  adv train finish, try to retain  171
test acc: top1 ->  69.606 ; top5 ->  89.042  and loss:  988.1852054595947
forward train acc: top1 ->  66.8984375 ; top5 ->  86.1484375  and loss:  278.5229957103729
test acc: top1 ->  69.96 ; top5 ->  89.198  and loss:  982.3608573079109
forward train acc: top1 ->  67.7890625 ; top5 ->  86.4765625  and loss:  273.232769548893
test acc: top1 ->  69.73 ; top5 ->  89.186  and loss:  982.2587932944298
forward train acc: top1 ->  67.671875 ; top5 ->  86.59375  and loss:  273.6930550932884
test acc: top1 ->  69.72 ; top5 ->  89.192  and loss:  983.8112089037895
forward train acc: top1 ->  68.5 ; top5 ->  87.09375  and loss:  266.4568255543709
test acc: top1 ->  69.948 ; top5 ->  89.23  and loss:  980.3969921469688
forward train acc: top1 ->  67.671875 ; top5 ->  86.4921875  and loss:  272.0753264427185
test acc: top1 ->  69.884 ; top5 ->  89.232  and loss:  979.1082997322083
forward train acc: top1 ->  67.265625 ; top5 ->  86.0859375  and loss:  277.9880341887474
test acc: top1 ->  69.858 ; top5 ->  89.244  and loss:  978.8204803466797
forward train acc: top1 ->  67.9296875 ; top5 ->  86.4375  and loss:  272.12741643190384
test acc: top1 ->  69.958 ; top5 ->  89.226  and loss:  978.9104560017586
forward train acc: top1 ->  67.3828125 ; top5 ->  86.03125  and loss:  278.8624350428581
test acc: top1 ->  69.954 ; top5 ->  89.312  and loss:  978.4557908773422
forward train acc: top1 ->  67.125 ; top5 ->  86.4375  and loss:  273.05385118722916
test acc: top1 ->  69.94 ; top5 ->  89.322  and loss:  975.362736582756
forward train acc: top1 ->  67.90625 ; top5 ->  86.5546875  and loss:  268.6594434976578
test acc: top1 ->  69.956 ; top5 ->  89.344  and loss:  977.3964560627937
>>>>>>> reverse layer  19  since performance drop >>>>>>>
==> this epoch:  179 / 256 , inc:  1
---------------- start layer  20  ---------------
adv train loss:  -64.94619888067245 , diff:  64.94619888067245
adv train loss:  -67.51796001195908 , diff:  2.571761131286621
layer  20  adv train finish, try to retain  242
>>>>>>> reverse layer  20  since no improvement >>>>>>>
---------------- start layer  21  ---------------
adv train loss:  -68.28695863485336 , diff:  68.28695863485336
adv train loss:  -70.08840757608414 , diff:  1.801448941230774
layer  21  adv train finish, try to retain  240
test acc: top1 ->  70.042 ; top5 ->  89.338  and loss:  978.7010508179665
forward train acc: top1 ->  67.7890625 ; top5 ->  86.4140625  and loss:  271.7448236346245
test acc: top1 ->  70.06 ; top5 ->  89.338  and loss:  975.1348236203194
forward train acc: top1 ->  67.9921875 ; top5 ->  86.5234375  and loss:  271.6459517478943
test acc: top1 ->  70.086 ; top5 ->  89.354  and loss:  975.2715001702309
forward train acc: top1 ->  68.1015625 ; top5 ->  86.1484375  and loss:  272.50657308101654
test acc: top1 ->  70.05 ; top5 ->  89.308  and loss:  975.5630015730858
forward train acc: top1 ->  67.65625 ; top5 ->  86.328125  and loss:  272.15744465589523
test acc: top1 ->  70.064 ; top5 ->  89.308  and loss:  974.3235747218132
forward train acc: top1 ->  67.6171875 ; top5 ->  86.5546875  and loss:  273.13758504390717
test acc: top1 ->  70.206 ; top5 ->  89.382  and loss:  968.4716146588326
forward train acc: top1 ->  67.9609375 ; top5 ->  86.515625  and loss:  273.1214406490326
test acc: top1 ->  70.112 ; top5 ->  89.382  and loss:  971.8269186019897
forward train acc: top1 ->  68.0078125 ; top5 ->  86.625  and loss:  267.27680975198746
test acc: top1 ->  70.216 ; top5 ->  89.404  and loss:  968.9587539434433
forward train acc: top1 ->  68.46875 ; top5 ->  86.84375  and loss:  266.4036794304848
test acc: top1 ->  70.262 ; top5 ->  89.378  and loss:  968.2860396504402
forward train acc: top1 ->  67.625 ; top5 ->  86.2109375  and loss:  274.4738398194313
test acc: top1 ->  70.22 ; top5 ->  89.412  and loss:  966.874271094799
forward train acc: top1 ->  67.734375 ; top5 ->  86.5078125  and loss:  272.90215212106705
test acc: top1 ->  70.298 ; top5 ->  89.402  and loss:  967.749766856432
>>>>>>> reverse layer  21  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  22  ---------------
adv train loss:  -70.98088204860687 , diff:  70.98088204860687
adv train loss:  -67.99913847446442 , diff:  2.981743574142456
layer  22  adv train finish, try to retain  197
test acc: top1 ->  69.944 ; top5 ->  89.21  and loss:  982.82921859622
forward train acc: top1 ->  67.53125 ; top5 ->  86.65625  and loss:  274.0154185295105
test acc: top1 ->  70.068 ; top5 ->  89.278  and loss:  977.0169332623482
forward train acc: top1 ->  67.3984375 ; top5 ->  86.4921875  and loss:  272.0520853996277
test acc: top1 ->  69.916 ; top5 ->  89.208  and loss:  979.7816351056099
forward train acc: top1 ->  66.8046875 ; top5 ->  86.25  and loss:  280.1431093811989
test acc: top1 ->  69.99 ; top5 ->  89.122  and loss:  976.4288242459297
forward train acc: top1 ->  68.0078125 ; top5 ->  86.609375  and loss:  271.1034445762634
test acc: top1 ->  70.056 ; top5 ->  89.184  and loss:  975.4034346938133
forward train acc: top1 ->  67.53125 ; top5 ->  86.5234375  and loss:  273.4566280245781
test acc: top1 ->  69.986 ; top5 ->  89.31  and loss:  973.4965192377567
forward train acc: top1 ->  67.3828125 ; top5 ->  86.1484375  and loss:  276.0368400812149
test acc: top1 ->  70.096 ; top5 ->  89.302  and loss:  973.8075749576092
forward train acc: top1 ->  67.765625 ; top5 ->  86.796875  and loss:  272.5449770092964
test acc: top1 ->  70.052 ; top5 ->  89.34  and loss:  972.4118645787239
forward train acc: top1 ->  67.1953125 ; top5 ->  86.2265625  and loss:  276.13293516635895
test acc: top1 ->  70.128 ; top5 ->  89.33  and loss:  972.8665612339973
forward train acc: top1 ->  67.5703125 ; top5 ->  86.3828125  and loss:  272.95634400844574
test acc: top1 ->  70.136 ; top5 ->  89.322  and loss:  971.7257215380669
forward train acc: top1 ->  67.875 ; top5 ->  86.625  and loss:  270.7865387201309
test acc: top1 ->  70.17 ; top5 ->  89.332  and loss:  968.2472984194756
>>>>>>> reverse layer  22  since performance drop >>>>>>>
==> this epoch:  208 / 256 , inc:  1
---------------- start layer  23  ---------------
adv train loss:  -69.74501913785934 , diff:  69.74501913785934
adv train loss:  -63.563910365104675 , diff:  6.181108772754669
layer  23  adv train finish, try to retain  165
test acc: top1 ->  69.788 ; top5 ->  89.022  and loss:  994.7634168863297
forward train acc: top1 ->  68.0625 ; top5 ->  86.359375  and loss:  271.1083416938782
test acc: top1 ->  69.898 ; top5 ->  89.26  and loss:  976.0937443673611
forward train acc: top1 ->  67.984375 ; top5 ->  86.6953125  and loss:  272.4431958794594
test acc: top1 ->  69.856 ; top5 ->  89.24  and loss:  979.6291945576668
forward train acc: top1 ->  68.40625 ; top5 ->  86.9140625  and loss:  267.6985040307045
test acc: top1 ->  69.93 ; top5 ->  89.198  and loss:  980.1801252961159
forward train acc: top1 ->  67.421875 ; top5 ->  86.1015625  and loss:  280.03607165813446
test acc: top1 ->  69.96 ; top5 ->  89.322  and loss:  974.9068515896797
forward train acc: top1 ->  67.8515625 ; top5 ->  86.734375  and loss:  267.6379100680351
test acc: top1 ->  70.04 ; top5 ->  89.332  and loss:  973.8845593035221
forward train acc: top1 ->  67.453125 ; top5 ->  86.1875  and loss:  273.31586050987244
test acc: top1 ->  70.008 ; top5 ->  89.3  and loss:  975.7412537932396
forward train acc: top1 ->  67.765625 ; top5 ->  86.34375  and loss:  278.2676265835762
test acc: top1 ->  70.06 ; top5 ->  89.296  and loss:  975.1212785243988
forward train acc: top1 ->  67.140625 ; top5 ->  86.0390625  and loss:  280.3145601153374
test acc: top1 ->  70.078 ; top5 ->  89.328  and loss:  972.2351991832256
forward train acc: top1 ->  68.03125 ; top5 ->  86.421875  and loss:  272.8923652768135
test acc: top1 ->  70.162 ; top5 ->  89.384  and loss:  971.7771135568619
forward train acc: top1 ->  68.046875 ; top5 ->  86.5859375  and loss:  270.50021481513977
test acc: top1 ->  70.104 ; top5 ->  89.308  and loss:  969.4221357703209
>>>>>>> reverse layer  23  since performance drop >>>>>>>
==> this epoch:  187 / 256 , inc:  1
---------------- start layer  24  ---------------
adv train loss:  -66.50542771816254 , diff:  66.50542771816254
adv train loss:  -66.81947666406631 , diff:  0.3140489459037781
layer  24  adv train finish, try to retain  241
test acc: top1 ->  70.05 ; top5 ->  89.358  and loss:  969.6728454232216
forward train acc: top1 ->  67.8359375 ; top5 ->  86.53125  and loss:  274.14994114637375
test acc: top1 ->  70.182 ; top5 ->  89.348  and loss:  969.494098752737
forward train acc: top1 ->  67.8203125 ; top5 ->  86.484375  and loss:  272.52134108543396
test acc: top1 ->  70.176 ; top5 ->  89.358  and loss:  970.4924474358559
forward train acc: top1 ->  68.1640625 ; top5 ->  86.90625  and loss:  269.58820766210556
test acc: top1 ->  70.28 ; top5 ->  89.376  and loss:  969.8467435240746
forward train acc: top1 ->  67.6484375 ; top5 ->  86.40625  and loss:  275.0824272632599
test acc: top1 ->  70.19 ; top5 ->  89.436  and loss:  967.9004333615303
forward train acc: top1 ->  67.7265625 ; top5 ->  85.8125  and loss:  275.9820512533188
test acc: top1 ->  70.24 ; top5 ->  89.392  and loss:  967.4561037421227
forward train acc: top1 ->  68.671875 ; top5 ->  86.4609375  and loss:  270.96504044532776
test acc: top1 ->  70.232 ; top5 ->  89.46  and loss:  966.8208701014519
forward train acc: top1 ->  67.5078125 ; top5 ->  86.4296875  and loss:  273.63183587789536
test acc: top1 ->  70.3 ; top5 ->  89.46  and loss:  963.8210408687592
forward train acc: top1 ->  68.1484375 ; top5 ->  86.65625  and loss:  268.6588146686554
test acc: top1 ->  70.322 ; top5 ->  89.484  and loss:  963.7431950569153
forward train acc: top1 ->  68.28125 ; top5 ->  86.90625  and loss:  264.96569871902466
test acc: top1 ->  70.43 ; top5 ->  89.478  and loss:  965.0981356501579
forward train acc: top1 ->  68.3046875 ; top5 ->  86.609375  and loss:  270.41123765707016
test acc: top1 ->  70.344 ; top5 ->  89.486  and loss:  962.97264534235
>>>>>>> reverse layer  24  since performance drop >>>>>>>
==> this epoch:  244 / 256 , inc:  1
---------------- start layer  25  ---------------
adv train loss:  -66.15555083751678 , diff:  66.15555083751678
adv train loss:  -67.09873306751251 , diff:  0.9431822299957275
layer  25  adv train finish, try to retain  242
>>>>>>> reverse layer  25  since no improvement >>>>>>>
---------------- start layer  26  ---------------
adv train loss:  -72.17657566070557 , diff:  72.17657566070557
adv train loss:  -68.56821048259735 , diff:  3.6083651781082153
layer  26  adv train finish, try to retain  496
test acc: top1 ->  70.038 ; top5 ->  89.328  and loss:  972.4934963285923
forward train acc: top1 ->  67.28125 ; top5 ->  86.0625  and loss:  277.92524641752243
test acc: top1 ->  70.058 ; top5 ->  89.326  and loss:  972.9092161655426
forward train acc: top1 ->  67.53125 ; top5 ->  86.640625  and loss:  268.94006645679474
test acc: top1 ->  70.062 ; top5 ->  89.352  and loss:  972.7605599761009
forward train acc: top1 ->  68.046875 ; top5 ->  86.3359375  and loss:  274.62549924850464
test acc: top1 ->  70.086 ; top5 ->  89.3  and loss:  974.6580052375793
forward train acc: top1 ->  68.3515625 ; top5 ->  87.1328125  and loss:  264.01268565654755
test acc: top1 ->  70.006 ; top5 ->  89.372  and loss:  971.7298329472542
forward train acc: top1 ->  68.0703125 ; top5 ->  86.34375  and loss:  271.0604350566864
test acc: top1 ->  70.202 ; top5 ->  89.432  and loss:  968.7983360290527
forward train acc: top1 ->  67.9921875 ; top5 ->  86.9140625  and loss:  266.1634120941162
test acc: top1 ->  70.13 ; top5 ->  89.41  and loss:  971.5473221540451
forward train acc: top1 ->  67.609375 ; top5 ->  86.5703125  and loss:  269.3065342903137
test acc: top1 ->  70.218 ; top5 ->  89.412  and loss:  969.5010772943497
forward train acc: top1 ->  67.7890625 ; top5 ->  86.421875  and loss:  274.8964175581932
test acc: top1 ->  70.342 ; top5 ->  89.466  and loss:  966.6497928500175
forward train acc: top1 ->  67.359375 ; top5 ->  86.5234375  and loss:  272.3275149464607
test acc: top1 ->  70.344 ; top5 ->  89.502  and loss:  967.8490866422653
forward train acc: top1 ->  67.7578125 ; top5 ->  86.8984375  and loss:  271.001750767231
test acc: top1 ->  70.378 ; top5 ->  89.494  and loss:  966.6066933274269
>>>>>>> reverse layer  26  since performance drop >>>>>>>
==> this epoch:  502 / 512 , inc:  1
---------------- start layer  27  ---------------
adv train loss:  -67.35438948869705 , diff:  67.35438948869705
adv train loss:  -69.29334270954132 , diff:  1.9389532208442688
layer  27  adv train finish, try to retain  490
test acc: top1 ->  69.806 ; top5 ->  89.132  and loss:  984.6553518772125
forward train acc: top1 ->  68.0625 ; top5 ->  86.65625  and loss:  270.70616775751114
test acc: top1 ->  69.748 ; top5 ->  89.1  and loss:  981.4871222674847
forward train acc: top1 ->  66.890625 ; top5 ->  86.1953125  and loss:  277.48470133543015
test acc: top1 ->  70.01 ; top5 ->  89.31  and loss:  979.3170354366302
forward train acc: top1 ->  67.3984375 ; top5 ->  86.5234375  and loss:  273.7893952727318
test acc: top1 ->  69.936 ; top5 ->  89.322  and loss:  975.6430795788765
forward train acc: top1 ->  67.9609375 ; top5 ->  86.375  and loss:  269.7025512456894
test acc: top1 ->  69.87 ; top5 ->  89.3  and loss:  975.6781368255615
forward train acc: top1 ->  67.5703125 ; top5 ->  86.484375  and loss:  273.4006926417351
test acc: top1 ->  69.972 ; top5 ->  89.318  and loss:  975.0131966769695
forward train acc: top1 ->  68.375 ; top5 ->  86.484375  and loss:  272.2258942723274
test acc: top1 ->  70.052 ; top5 ->  89.332  and loss:  974.4079896211624
forward train acc: top1 ->  67.4609375 ; top5 ->  86.5  and loss:  275.44758355617523
test acc: top1 ->  70.124 ; top5 ->  89.338  and loss:  973.2603903412819
forward train acc: top1 ->  67.4609375 ; top5 ->  86.0703125  and loss:  277.81039011478424
test acc: top1 ->  70.086 ; top5 ->  89.312  and loss:  974.0965449213982
forward train acc: top1 ->  67.6484375 ; top5 ->  86.59375  and loss:  271.61027777194977
test acc: top1 ->  70.132 ; top5 ->  89.41  and loss:  969.9935343265533
forward train acc: top1 ->  68.0859375 ; top5 ->  86.609375  and loss:  273.06817626953125
test acc: top1 ->  70.092 ; top5 ->  89.37  and loss:  971.9758430123329
>>>>>>> reverse layer  27  since performance drop >>>>>>>
==> this epoch:  505 / 512 , inc:  1
---------------- start layer  28  ---------------
adv train loss:  -67.60244470834732 , diff:  67.60244470834732
adv train loss:  -66.75025773048401 , diff:  0.8521869778633118
layer  28  adv train finish, try to retain  492
test acc: top1 ->  70.092 ; top5 ->  89.422  and loss:  975.2966643571854
forward train acc: top1 ->  67.15625 ; top5 ->  86.71875  and loss:  273.4851487874985
test acc: top1 ->  70.018 ; top5 ->  89.342  and loss:  977.3159446120262
forward train acc: top1 ->  67.40625 ; top5 ->  85.8125  and loss:  277.35569286346436
test acc: top1 ->  70.052 ; top5 ->  89.376  and loss:  973.8045988082886
forward train acc: top1 ->  68.2109375 ; top5 ->  86.6796875  and loss:  270.39623045921326
test acc: top1 ->  70.002 ; top5 ->  89.33  and loss:  974.4916934370995
forward train acc: top1 ->  68.359375 ; top5 ->  86.375  and loss:  271.492555975914
test acc: top1 ->  70.0 ; top5 ->  89.416  and loss:  974.6343896389008
forward train acc: top1 ->  67.2890625 ; top5 ->  86.234375  and loss:  275.2576607465744
test acc: top1 ->  70.206 ; top5 ->  89.412  and loss:  971.3570287823677
forward train acc: top1 ->  68.0859375 ; top5 ->  86.625  and loss:  270.44675636291504
test acc: top1 ->  70.098 ; top5 ->  89.364  and loss:  970.4881696105003
forward train acc: top1 ->  67.875 ; top5 ->  86.375  and loss:  270.08293122053146
test acc: top1 ->  70.192 ; top5 ->  89.41  and loss:  966.0244588255882
forward train acc: top1 ->  68.0703125 ; top5 ->  87.03125  and loss:  268.7697466611862
test acc: top1 ->  70.242 ; top5 ->  89.468  and loss:  970.3782496452332
forward train acc: top1 ->  67.6015625 ; top5 ->  86.2109375  and loss:  273.153889298439
test acc: top1 ->  70.28 ; top5 ->  89.522  and loss:  969.1872844696045
forward train acc: top1 ->  66.8125 ; top5 ->  86.5390625  and loss:  272.404103577137
test acc: top1 ->  70.364 ; top5 ->  89.47  and loss:  966.6583243608475
>>>>>>> reverse layer  28  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  29  ---------------
adv train loss:  -68.03273713588715 , diff:  68.03273713588715
adv train loss:  -68.0681876540184 , diff:  0.035450518131256104
layer  29  adv train finish, try to retain  497
test acc: top1 ->  70.254 ; top5 ->  89.53  and loss:  966.7374551296234
forward train acc: top1 ->  67.453125 ; top5 ->  86.3125  and loss:  274.9886738061905
test acc: top1 ->  70.078 ; top5 ->  89.462  and loss:  972.197570502758
forward train acc: top1 ->  68.1796875 ; top5 ->  86.609375  and loss:  271.31370383501053
test acc: top1 ->  70.07 ; top5 ->  89.426  and loss:  969.8180404305458
forward train acc: top1 ->  68.7265625 ; top5 ->  86.890625  and loss:  264.19042390584946
test acc: top1 ->  69.986 ; top5 ->  89.39  and loss:  973.0362360477448
forward train acc: top1 ->  67.34375 ; top5 ->  86.515625  and loss:  275.7393729686737
test acc: top1 ->  70.222 ; top5 ->  89.488  and loss:  966.5065185427666
forward train acc: top1 ->  67.1953125 ; top5 ->  86.15625  and loss:  277.8181575536728
test acc: top1 ->  70.342 ; top5 ->  89.51  and loss:  966.3775096535683
forward train acc: top1 ->  68.1875 ; top5 ->  86.390625  and loss:  268.4359181523323
test acc: top1 ->  70.198 ; top5 ->  89.528  and loss:  967.0451992154121
forward train acc: top1 ->  67.5859375 ; top5 ->  86.609375  and loss:  269.2088474035263
test acc: top1 ->  70.294 ; top5 ->  89.522  and loss:  967.0225960612297
forward train acc: top1 ->  67.671875 ; top5 ->  86.4921875  and loss:  273.97964465618134
test acc: top1 ->  70.29 ; top5 ->  89.504  and loss:  965.2139559388161
forward train acc: top1 ->  68.171875 ; top5 ->  86.65625  and loss:  271.99638015031815
test acc: top1 ->  70.348 ; top5 ->  89.51  and loss:  961.649949669838
forward train acc: top1 ->  67.8671875 ; top5 ->  86.5078125  and loss:  272.8079010248184
test acc: top1 ->  70.278 ; top5 ->  89.54  and loss:  962.8872900605202
>>>>>>> reverse layer  29  since performance drop >>>>>>>
==> this epoch:  498 / 512 , inc:  1
---------------- start layer  30  ---------------
adv train loss:  -68.63861095905304 , diff:  68.63861095905304
adv train loss:  -67.68795084953308 , diff:  0.9506601095199585
layer  30  adv train finish, try to retain  494
test acc: top1 ->  70.046 ; top5 ->  89.378  and loss:  967.2995349168777
forward train acc: top1 ->  67.6484375 ; top5 ->  86.8046875  and loss:  271.4984576702118
test acc: top1 ->  70.126 ; top5 ->  89.438  and loss:  972.9618532657623
forward train acc: top1 ->  68.265625 ; top5 ->  86.625  and loss:  265.90081429481506
test acc: top1 ->  70.178 ; top5 ->  89.39  and loss:  970.850905418396
forward train acc: top1 ->  67.96875 ; top5 ->  86.265625  and loss:  275.4055987596512
test acc: top1 ->  70.256 ; top5 ->  89.446  and loss:  970.6434909105301
forward train acc: top1 ->  67.765625 ; top5 ->  86.71875  and loss:  272.8206141591072
test acc: top1 ->  70.232 ; top5 ->  89.494  and loss:  969.4288458228111
forward train acc: top1 ->  67.390625 ; top5 ->  85.8671875  and loss:  277.8054053783417
test acc: top1 ->  70.242 ; top5 ->  89.466  and loss:  967.4603209495544
forward train acc: top1 ->  68.1875 ; top5 ->  86.1015625  and loss:  271.14299488067627
test acc: top1 ->  70.186 ; top5 ->  89.482  and loss:  967.8950756788254
forward train acc: top1 ->  68.453125 ; top5 ->  86.8984375  and loss:  263.30347204208374
test acc: top1 ->  70.234 ; top5 ->  89.454  and loss:  968.9219698905945
forward train acc: top1 ->  68.171875 ; top5 ->  86.3203125  and loss:  272.9179009795189
test acc: top1 ->  70.31 ; top5 ->  89.464  and loss:  968.9231307506561
forward train acc: top1 ->  67.9765625 ; top5 ->  86.0625  and loss:  275.93666660785675
test acc: top1 ->  70.25 ; top5 ->  89.452  and loss:  966.9997254014015
forward train acc: top1 ->  68.109375 ; top5 ->  86.75  and loss:  271.3096559047699
test acc: top1 ->  70.248 ; top5 ->  89.518  and loss:  967.5123482942581
>>>>>>> reverse layer  30  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  31  ---------------
adv train loss:  -70.39775729179382 , diff:  70.39775729179382
adv train loss:  -66.85096997022629 , diff:  3.5467873215675354
layer  31  adv train finish, try to retain  501
>>>>>>> reverse layer  31  since no improvement >>>>>>>
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.0225, 0.0225, 0.000791015625, 0.0225, 0.0225, 0.0225, 0.0010546875, 0.00019775390625, 0.01125, 0.0010546875, 0.01125, 0.002109375, 0.00421875, 0.01125, 9.8876953125e-05, 9.8876953125e-05, 0.00052734375, 0.00052734375, 0.00019775390625, 0.005625, 0.00052734375, 0.00019775390625, 0.002109375, 0.005625, 0.00019775390625, 0.00052734375, 9.8876953125e-05, 9.8876953125e-05, 9.8876953125e-05, 9.8876953125e-05, 9.8876953125e-05, 0.000263671875]  wait [2, 2, 4, 2, 2, 2, 2, 3, 2, 2, 2, 3, 4, 2, 3, 3, 2, 2, 4, 2, 2, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 1
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  10  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -64.2136595249176 , diff:  64.2136595249176
adv train loss:  -68.69682455062866 , diff:  4.48316502571106
layer  0  adv train finish, try to retain  41
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -71.08670210838318 , diff:  71.08670210838318
adv train loss:  -69.48700100183487 , diff:  1.5997011065483093
layer  1  adv train finish, try to retain  48
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -67.32445025444031 , diff:  67.32445025444031
adv train loss:  -70.94476991891861 , diff:  3.620319664478302
layer  2  adv train finish, try to retain  55
test acc: top1 ->  69.16 ; top5 ->  88.828  and loss:  1006.2592338323593
forward train acc: top1 ->  68.2265625 ; top5 ->  86.6328125  and loss:  270.26200473308563
test acc: top1 ->  70.134 ; top5 ->  89.322  and loss:  974.3677716851234
forward train acc: top1 ->  66.90625 ; top5 ->  86.15625  and loss:  278.47442746162415
test acc: top1 ->  70.276 ; top5 ->  89.258  and loss:  972.4694710373878
forward train acc: top1 ->  67.7734375 ; top5 ->  86.1796875  and loss:  278.3094921708107
test acc: top1 ->  70.184 ; top5 ->  89.274  and loss:  974.2387971878052
forward train acc: top1 ->  68.34375 ; top5 ->  86.3828125  and loss:  268.49445939064026
test acc: top1 ->  70.182 ; top5 ->  89.262  and loss:  972.0737171769142
forward train acc: top1 ->  67.1953125 ; top5 ->  86.4296875  and loss:  274.1770060658455
test acc: top1 ->  70.322 ; top5 ->  89.362  and loss:  968.3520457148552
forward train acc: top1 ->  67.6796875 ; top5 ->  86.8125  and loss:  268.87252473831177
test acc: top1 ->  70.324 ; top5 ->  89.32  and loss:  968.6843880414963
forward train acc: top1 ->  68.34375 ; top5 ->  87.3046875  and loss:  262.0568252801895
test acc: top1 ->  70.268 ; top5 ->  89.336  and loss:  969.2803936600685
forward train acc: top1 ->  67.65625 ; top5 ->  86.6015625  and loss:  272.7469462156296
test acc: top1 ->  70.3 ; top5 ->  89.354  and loss:  969.7059591412544
forward train acc: top1 ->  68.0390625 ; top5 ->  86.7578125  and loss:  269.33223021030426
test acc: top1 ->  70.392 ; top5 ->  89.386  and loss:  970.4682642519474
forward train acc: top1 ->  68.1640625 ; top5 ->  86.6171875  and loss:  268.299745619297
test acc: top1 ->  70.364 ; top5 ->  89.434  and loss:  966.5587396621704
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  57 / 64 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -69.32026189565659 , diff:  69.32026189565659
adv train loss:  -66.28827393054962 , diff:  3.031987965106964
layer  3  adv train finish, try to retain  42
test acc: top1 ->  66.542 ; top5 ->  87.298  and loss:  1095.1169179081917
forward train acc: top1 ->  66.921875 ; top5 ->  85.7265625  and loss:  280.3777901530266
test acc: top1 ->  69.758 ; top5 ->  89.144  and loss:  986.2207850217819
forward train acc: top1 ->  67.5859375 ; top5 ->  86.390625  and loss:  275.2656203508377
test acc: top1 ->  69.782 ; top5 ->  89.228  and loss:  981.9368050098419
forward train acc: top1 ->  67.609375 ; top5 ->  85.9296875  and loss:  276.4104931950569
test acc: top1 ->  69.728 ; top5 ->  89.254  and loss:  983.5434867143631
forward train acc: top1 ->  67.296875 ; top5 ->  85.7578125  and loss:  278.8685192465782
test acc: top1 ->  69.874 ; top5 ->  89.242  and loss:  978.3508353829384
forward train acc: top1 ->  67.4921875 ; top5 ->  86.78125  and loss:  273.1202496290207
test acc: top1 ->  69.934 ; top5 ->  89.272  and loss:  976.7617317438126
forward train acc: top1 ->  67.6484375 ; top5 ->  86.203125  and loss:  276.63282519578934
test acc: top1 ->  69.932 ; top5 ->  89.278  and loss:  976.4128352403641
forward train acc: top1 ->  68.21875 ; top5 ->  86.4921875  and loss:  269.34624910354614
test acc: top1 ->  69.952 ; top5 ->  89.274  and loss:  976.4675127267838
forward train acc: top1 ->  67.375 ; top5 ->  85.8828125  and loss:  277.53332483768463
test acc: top1 ->  70.03 ; top5 ->  89.338  and loss:  974.515290081501
forward train acc: top1 ->  68.09375 ; top5 ->  86.6171875  and loss:  271.24161273241043
test acc: top1 ->  70.036 ; top5 ->  89.364  and loss:  973.59081864357
forward train acc: top1 ->  67.7890625 ; top5 ->  86.6171875  and loss:  273.8651964068413
test acc: top1 ->  70.074 ; top5 ->  89.436  and loss:  973.3442777395248
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  43 / 64 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -68.93619215488434 , diff:  68.93619215488434
adv train loss:  -66.76863592863083 , diff:  2.1675562262535095
layer  4  adv train finish, try to retain  34
test acc: top1 ->  67.882 ; top5 ->  88.018  and loss:  1043.3387106657028
forward train acc: top1 ->  67.3984375 ; top5 ->  86.3125  and loss:  277.19427382946014
test acc: top1 ->  69.972 ; top5 ->  89.192  and loss:  979.9774979352951
forward train acc: top1 ->  68.4609375 ; top5 ->  86.453125  and loss:  272.5431985259056
test acc: top1 ->  69.962 ; top5 ->  89.206  and loss:  978.5188601613045
forward train acc: top1 ->  67.5234375 ; top5 ->  86.0625  and loss:  272.5364189147949
test acc: top1 ->  70.104 ; top5 ->  89.152  and loss:  979.8854679465294
forward train acc: top1 ->  67.265625 ; top5 ->  86.640625  and loss:  274.89187002182007
test acc: top1 ->  70.102 ; top5 ->  89.25  and loss:  975.8594015240669
forward train acc: top1 ->  68.8828125 ; top5 ->  86.53125  and loss:  267.4893242716789
test acc: top1 ->  70.22 ; top5 ->  89.266  and loss:  973.5095506906509
forward train acc: top1 ->  67.7265625 ; top5 ->  86.6875  and loss:  272.35923302173615
test acc: top1 ->  70.218 ; top5 ->  89.28  and loss:  975.950250685215
forward train acc: top1 ->  68.078125 ; top5 ->  86.703125  and loss:  268.53048688173294
test acc: top1 ->  70.172 ; top5 ->  89.314  and loss:  973.8378992676735
forward train acc: top1 ->  68.46875 ; top5 ->  86.84375  and loss:  267.38972955942154
test acc: top1 ->  70.216 ; top5 ->  89.412  and loss:  971.9671841263771
forward train acc: top1 ->  67.4453125 ; top5 ->  86.2109375  and loss:  274.49053090810776
test acc: top1 ->  70.18 ; top5 ->  89.274  and loss:  973.3571882843971
forward train acc: top1 ->  67.9296875 ; top5 ->  86.234375  and loss:  272.0913972854614
test acc: top1 ->  70.21 ; top5 ->  89.41  and loss:  970.983849644661
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  40 / 64 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -67.92724561691284 , diff:  67.92724561691284
adv train loss:  -71.47820341587067 , diff:  3.5509577989578247
layer  5  adv train finish, try to retain  43
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -68.5916041135788 , diff:  68.5916041135788
adv train loss:  -71.11489516496658 , diff:  2.523291051387787
layer  6  adv train finish, try to retain  109
test acc: top1 ->  66.81 ; top5 ->  87.366  and loss:  1084.1710374951363
forward train acc: top1 ->  67.734375 ; top5 ->  86.3046875  and loss:  275.0818883776665
test acc: top1 ->  69.976 ; top5 ->  89.24  and loss:  976.8173968195915
forward train acc: top1 ->  68.453125 ; top5 ->  86.7578125  and loss:  270.4339716434479
test acc: top1 ->  70.046 ; top5 ->  89.274  and loss:  974.3323753774166
forward train acc: top1 ->  67.578125 ; top5 ->  86.5234375  and loss:  273.88941383361816
test acc: top1 ->  70.046 ; top5 ->  89.308  and loss:  976.9679946899414
forward train acc: top1 ->  68.390625 ; top5 ->  86.6875  and loss:  270.91447603702545
test acc: top1 ->  70.152 ; top5 ->  89.336  and loss:  971.726052582264
forward train acc: top1 ->  67.75 ; top5 ->  86.625  and loss:  270.4027111530304
test acc: top1 ->  70.172 ; top5 ->  89.316  and loss:  972.3345694243908
forward train acc: top1 ->  67.5390625 ; top5 ->  86.3828125  and loss:  273.46887588500977
test acc: top1 ->  70.266 ; top5 ->  89.356  and loss:  969.7019702196121
forward train acc: top1 ->  67.8515625 ; top5 ->  87.0  and loss:  269.8555165529251
test acc: top1 ->  70.238 ; top5 ->  89.398  and loss:  970.7722127735615
forward train acc: top1 ->  67.7109375 ; top5 ->  86.59375  and loss:  271.2127829194069
test acc: top1 ->  70.252 ; top5 ->  89.396  and loss:  969.0962495207787
forward train acc: top1 ->  68.0078125 ; top5 ->  86.421875  and loss:  273.78730887174606
test acc: top1 ->  70.236 ; top5 ->  89.398  and loss:  969.149139970541
forward train acc: top1 ->  67.359375 ; top5 ->  86.640625  and loss:  271.54427766799927
test acc: top1 ->  70.246 ; top5 ->  89.41  and loss:  969.5003642737865
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  112 / 128 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -66.99589323997498 , diff:  66.99589323997498
adv train loss:  -66.48017448186874 , diff:  0.5157187581062317
layer  7  adv train finish, try to retain  121
test acc: top1 ->  69.55 ; top5 ->  88.978  and loss:  987.0872205197811
forward train acc: top1 ->  67.390625 ; top5 ->  86.1640625  and loss:  275.8697860240936
test acc: top1 ->  70.228 ; top5 ->  89.34  and loss:  973.4296292066574
forward train acc: top1 ->  67.5078125 ; top5 ->  86.09375  and loss:  276.1592969894409
test acc: top1 ->  70.17 ; top5 ->  89.366  and loss:  972.9383065700531
forward train acc: top1 ->  68.25 ; top5 ->  86.921875  and loss:  266.8377058506012
test acc: top1 ->  70.244 ; top5 ->  89.422  and loss:  970.0434215664864
forward train acc: top1 ->  67.890625 ; top5 ->  86.328125  and loss:  271.9521120786667
test acc: top1 ->  70.316 ; top5 ->  89.436  and loss:  965.5962257385254
forward train acc: top1 ->  68.0859375 ; top5 ->  86.3359375  and loss:  270.4438136816025
test acc: top1 ->  70.334 ; top5 ->  89.46  and loss:  966.8386377692223
forward train acc: top1 ->  68.125 ; top5 ->  86.609375  and loss:  271.7226915359497
test acc: top1 ->  70.468 ; top5 ->  89.446  and loss:  966.190820723772
forward train acc: top1 ->  67.9140625 ; top5 ->  86.3203125  and loss:  273.0748375058174
test acc: top1 ->  70.438 ; top5 ->  89.424  and loss:  965.1028564572334
forward train acc: top1 ->  67.9453125 ; top5 ->  86.1328125  and loss:  268.1050770878792
test acc: top1 ->  70.496 ; top5 ->  89.494  and loss:  964.3824905157089
forward train acc: top1 ->  68.484375 ; top5 ->  86.78125  and loss:  270.92981308698654
test acc: top1 ->  70.438 ; top5 ->  89.486  and loss:  963.4634938240051
forward train acc: top1 ->  68.2265625 ; top5 ->  86.59375  and loss:  269.84100168943405
test acc: top1 ->  70.45 ; top5 ->  89.52  and loss:  963.6380232572556
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  122 / 128 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -69.68814605474472 , diff:  69.68814605474472
adv train loss:  -68.59103053808212 , diff:  1.0971155166625977
layer  8  adv train finish, try to retain  69
test acc: top1 ->  67.65 ; top5 ->  87.888  and loss:  1057.7589612603188
forward train acc: top1 ->  66.734375 ; top5 ->  85.7421875  and loss:  281.43940818309784
test acc: top1 ->  69.606 ; top5 ->  89.218  and loss:  989.3201669454575
forward train acc: top1 ->  66.3515625 ; top5 ->  85.4453125  and loss:  285.5232338309288
test acc: top1 ->  69.666 ; top5 ->  89.202  and loss:  983.3841881155968
forward train acc: top1 ->  67.6484375 ; top5 ->  86.234375  and loss:  274.04733937978745
test acc: top1 ->  69.516 ; top5 ->  89.132  and loss:  985.0908805131912
forward train acc: top1 ->  67.3515625 ; top5 ->  85.7578125  and loss:  280.92208909988403
test acc: top1 ->  69.726 ; top5 ->  89.206  and loss:  981.5788475871086
forward train acc: top1 ->  67.90625 ; top5 ->  86.7578125  and loss:  270.8835937976837
test acc: top1 ->  69.736 ; top5 ->  89.256  and loss:  982.5117748975754
forward train acc: top1 ->  67.5625 ; top5 ->  86.015625  and loss:  275.7264934182167
test acc: top1 ->  69.7 ; top5 ->  89.25  and loss:  980.8563247323036
forward train acc: top1 ->  67.96875 ; top5 ->  86.5234375  and loss:  271.89652621746063
test acc: top1 ->  69.792 ; top5 ->  89.294  and loss:  977.0048199892044
forward train acc: top1 ->  67.4765625 ; top5 ->  85.90625  and loss:  277.95001643896103
test acc: top1 ->  69.8 ; top5 ->  89.262  and loss:  981.0765872597694
forward train acc: top1 ->  66.9609375 ; top5 ->  85.8359375  and loss:  281.9377762079239
test acc: top1 ->  69.816 ; top5 ->  89.336  and loss:  975.569433093071
forward train acc: top1 ->  67.7109375 ; top5 ->  86.5  and loss:  274.32887691259384
test acc: top1 ->  69.768 ; top5 ->  89.29  and loss:  978.2171994447708
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -67.05382108688354 , diff:  67.05382108688354
adv train loss:  -68.57890558242798 , diff:  1.5250844955444336
layer  9  adv train finish, try to retain  114
test acc: top1 ->  69.362 ; top5 ->  88.84  and loss:  999.4317613840103
forward train acc: top1 ->  67.765625 ; top5 ->  86.5703125  and loss:  272.9705294370651
test acc: top1 ->  70.19 ; top5 ->  89.358  and loss:  974.6006230711937
forward train acc: top1 ->  67.546875 ; top5 ->  86.3359375  and loss:  274.5124712586403
test acc: top1 ->  70.248 ; top5 ->  89.406  and loss:  972.2373181581497
forward train acc: top1 ->  67.875 ; top5 ->  86.7890625  and loss:  269.59933519363403
test acc: top1 ->  70.194 ; top5 ->  89.392  and loss:  971.8902376294136
forward train acc: top1 ->  67.75 ; top5 ->  86.203125  and loss:  271.70037150382996
test acc: top1 ->  70.37 ; top5 ->  89.4  and loss:  970.1200953722
forward train acc: top1 ->  67.9609375 ; top5 ->  86.6328125  and loss:  270.83942502737045
test acc: top1 ->  70.386 ; top5 ->  89.456  and loss:  968.5467824041843
forward train acc: top1 ->  67.9609375 ; top5 ->  86.4609375  and loss:  269.32677096128464
test acc: top1 ->  70.414 ; top5 ->  89.494  and loss:  966.3841615617275
forward train acc: top1 ->  67.859375 ; top5 ->  86.359375  and loss:  276.18432611227036
test acc: top1 ->  70.388 ; top5 ->  89.504  and loss:  968.2184948027134
forward train acc: top1 ->  68.1953125 ; top5 ->  86.828125  and loss:  266.44420379400253
test acc: top1 ->  70.448 ; top5 ->  89.532  and loss:  964.716424882412
forward train acc: top1 ->  68.4765625 ; top5 ->  86.7421875  and loss:  269.5270867943764
test acc: top1 ->  70.442 ; top5 ->  89.526  and loss:  963.4576380848885
forward train acc: top1 ->  67.9921875 ; top5 ->  86.5390625  and loss:  271.3194342851639
test acc: top1 ->  70.554 ; top5 ->  89.516  and loss:  964.2306553721428
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  115 / 128 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -68.01092779636383 , diff:  68.01092779636383
adv train loss:  -66.12705606222153 , diff:  1.8838717341423035
layer  10  adv train finish, try to retain  78
test acc: top1 ->  69.122 ; top5 ->  88.698  and loss:  1010.8171502947807
forward train acc: top1 ->  67.8125 ; top5 ->  86.5703125  and loss:  271.76585549116135
test acc: top1 ->  69.874 ; top5 ->  89.246  and loss:  981.940946996212
forward train acc: top1 ->  67.75 ; top5 ->  86.4375  and loss:  272.9617603421211
test acc: top1 ->  69.97 ; top5 ->  89.296  and loss:  979.3825145959854
forward train acc: top1 ->  68.21875 ; top5 ->  86.515625  and loss:  269.7862829566002
test acc: top1 ->  69.924 ; top5 ->  89.296  and loss:  975.5488268136978
forward train acc: top1 ->  67.5703125 ; top5 ->  86.625  and loss:  272.3697898387909
test acc: top1 ->  69.986 ; top5 ->  89.236  and loss:  978.6351854801178
forward train acc: top1 ->  67.4140625 ; top5 ->  86.046875  and loss:  276.0969770550728
test acc: top1 ->  70.074 ; top5 ->  89.268  and loss:  976.5211592316628
forward train acc: top1 ->  67.1171875 ; top5 ->  86.15625  and loss:  276.7374937534332
test acc: top1 ->  70.104 ; top5 ->  89.284  and loss:  974.8886276483536
forward train acc: top1 ->  67.6796875 ; top5 ->  86.625  and loss:  270.25328677892685
test acc: top1 ->  70.066 ; top5 ->  89.34  and loss:  972.7312284111977
forward train acc: top1 ->  67.578125 ; top5 ->  86.2265625  and loss:  273.2837700843811
test acc: top1 ->  70.19 ; top5 ->  89.356  and loss:  972.4290723204613
forward train acc: top1 ->  68.5859375 ; top5 ->  86.953125  and loss:  266.5686088204384
test acc: top1 ->  70.124 ; top5 ->  89.312  and loss:  972.8672779202461
forward train acc: top1 ->  67.9609375 ; top5 ->  86.375  and loss:  271.07459980249405
test acc: top1 ->  70.09 ; top5 ->  89.306  and loss:  971.5237649679184
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -69.499751329422 , diff:  69.499751329422
adv train loss:  -66.15882819890976 , diff:  3.3409231305122375
layer  11  adv train finish, try to retain  98
test acc: top1 ->  69.708 ; top5 ->  89.214  and loss:  984.6540039777756
forward train acc: top1 ->  66.984375 ; top5 ->  85.96875  and loss:  281.1546673178673
test acc: top1 ->  70.048 ; top5 ->  89.362  and loss:  975.4444671273232
forward train acc: top1 ->  67.9375 ; top5 ->  86.6328125  and loss:  269.4804285168648
test acc: top1 ->  70.058 ; top5 ->  89.34  and loss:  973.5232101082802
forward train acc: top1 ->  67.203125 ; top5 ->  85.953125  and loss:  277.893315076828
test acc: top1 ->  70.122 ; top5 ->  89.412  and loss:  972.6179531216621
forward train acc: top1 ->  67.8828125 ; top5 ->  86.7421875  and loss:  268.9632270336151
test acc: top1 ->  70.308 ; top5 ->  89.448  and loss:  968.9326303601265
forward train acc: top1 ->  67.2734375 ; top5 ->  85.40625  and loss:  281.3124846816063
test acc: top1 ->  70.122 ; top5 ->  89.486  and loss:  968.6169629693031
forward train acc: top1 ->  67.5234375 ; top5 ->  86.25  and loss:  272.1486473083496
test acc: top1 ->  70.324 ; top5 ->  89.524  and loss:  965.1690083742142
forward train acc: top1 ->  67.7578125 ; top5 ->  86.3984375  and loss:  270.5220076441765
test acc: top1 ->  70.25 ; top5 ->  89.51  and loss:  967.1100839972496
forward train acc: top1 ->  68.3984375 ; top5 ->  86.890625  and loss:  267.53572529554367
test acc: top1 ->  70.23 ; top5 ->  89.52  and loss:  965.5848050117493
forward train acc: top1 ->  68.484375 ; top5 ->  86.71875  and loss:  268.66845870018005
test acc: top1 ->  70.294 ; top5 ->  89.54  and loss:  966.8056151270866
forward train acc: top1 ->  67.7421875 ; top5 ->  86.3671875  and loss:  272.5780620574951
test acc: top1 ->  70.292 ; top5 ->  89.5  and loss:  965.1460560560226
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -66.11435985565186 , diff:  66.11435985565186
adv train loss:  -66.76748263835907 , diff:  0.6531227827072144
layer  12  adv train finish, try to retain  90
test acc: top1 ->  69.22 ; top5 ->  88.972  and loss:  995.8518150448799
forward train acc: top1 ->  68.0859375 ; top5 ->  86.5  and loss:  271.80195158720016
test acc: top1 ->  70.076 ; top5 ->  89.454  and loss:  974.205873131752
forward train acc: top1 ->  67.2578125 ; top5 ->  86.3671875  and loss:  277.15150117874146
test acc: top1 ->  70.104 ; top5 ->  89.36  and loss:  971.9856840968132
forward train acc: top1 ->  68.8203125 ; top5 ->  86.9453125  and loss:  264.4930379986763
test acc: top1 ->  70.078 ; top5 ->  89.342  and loss:  972.5396646857262
forward train acc: top1 ->  68.546875 ; top5 ->  86.859375  and loss:  264.48490566015244
test acc: top1 ->  70.114 ; top5 ->  89.37  and loss:  974.4789535403252
forward train acc: top1 ->  67.078125 ; top5 ->  86.109375  and loss:  278.4378083348274
test acc: top1 ->  70.08 ; top5 ->  89.412  and loss:  973.9684315919876
forward train acc: top1 ->  68.3125 ; top5 ->  86.734375  and loss:  269.5967761874199
test acc: top1 ->  70.104 ; top5 ->  89.392  and loss:  969.8061385154724
forward train acc: top1 ->  67.890625 ; top5 ->  86.703125  and loss:  271.6252935528755
test acc: top1 ->  70.242 ; top5 ->  89.408  and loss:  969.9467751979828
forward train acc: top1 ->  68.5234375 ; top5 ->  86.578125  and loss:  269.1484025120735
test acc: top1 ->  70.12 ; top5 ->  89.412  and loss:  970.4081901311874
forward train acc: top1 ->  67.8515625 ; top5 ->  86.84375  and loss:  269.01235485076904
test acc: top1 ->  70.184 ; top5 ->  89.402  and loss:  969.5234915614128
forward train acc: top1 ->  67.234375 ; top5 ->  86.6015625  and loss:  273.9707400202751
test acc: top1 ->  70.19 ; top5 ->  89.418  and loss:  969.4212594628334
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  98 / 128 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -66.31721657514572 , diff:  66.31721657514572
adv train loss:  -68.532102227211 , diff:  2.214885652065277
layer  13  adv train finish, try to retain  80
>>>>>>> reverse layer  13  since no improvement >>>>>>>
---------------- start layer  14  ---------------
adv train loss:  -66.66316521167755 , diff:  66.66316521167755
adv train loss:  -68.32909995317459 , diff:  1.6659347414970398
layer  14  adv train finish, try to retain  249
test acc: top1 ->  70.25 ; top5 ->  89.45  and loss:  967.202777326107
forward train acc: top1 ->  68.2734375 ; top5 ->  86.609375  and loss:  271.42660373449326
test acc: top1 ->  70.348 ; top5 ->  89.384  and loss:  967.9534120559692
forward train acc: top1 ->  68.03125 ; top5 ->  86.3671875  and loss:  271.8049045205116
test acc: top1 ->  70.318 ; top5 ->  89.46  and loss:  967.4693196117878
forward train acc: top1 ->  67.5390625 ; top5 ->  86.703125  and loss:  272.87128698825836
test acc: top1 ->  70.18 ; top5 ->  89.438  and loss:  967.0498097538948
forward train acc: top1 ->  68.34375 ; top5 ->  86.4453125  and loss:  269.40457224845886
test acc: top1 ->  70.316 ; top5 ->  89.472  and loss:  965.1148446202278
forward train acc: top1 ->  68.0 ; top5 ->  86.53125  and loss:  269.5198922753334
test acc: top1 ->  70.318 ; top5 ->  89.558  and loss:  965.4299152195454
forward train acc: top1 ->  68.1328125 ; top5 ->  86.484375  and loss:  272.19635486602783
test acc: top1 ->  70.388 ; top5 ->  89.46  and loss:  964.2265914678574
forward train acc: top1 ->  68.6875 ; top5 ->  87.1640625  and loss:  264.3955731391907
test acc: top1 ->  70.382 ; top5 ->  89.588  and loss:  961.5079076886177
forward train acc: top1 ->  68.7578125 ; top5 ->  86.8671875  and loss:  265.3692992925644
test acc: top1 ->  70.428 ; top5 ->  89.568  and loss:  962.2676618695259
forward train acc: top1 ->  68.4921875 ; top5 ->  86.6328125  and loss:  269.6312034726143
test acc: top1 ->  70.5 ; top5 ->  89.56  and loss:  959.6258468925953
forward train acc: top1 ->  68.6875 ; top5 ->  87.109375  and loss:  264.3693650364876
test acc: top1 ->  70.396 ; top5 ->  89.542  and loss:  961.3562273681164
>>>>>>> reverse layer  14  since performance drop >>>>>>>
==> this epoch:  250 / 256 , inc:  1
---------------- start layer  15  ---------------
adv train loss:  -68.53794395923615 , diff:  68.53794395923615
adv train loss:  -64.7436209321022 , diff:  3.7943230271339417
layer  15  adv train finish, try to retain  251
>>>>>>> reverse layer  15  since no improvement >>>>>>>
---------------- start layer  16  ---------------
adv train loss:  -66.65031170845032 , diff:  66.65031170845032
adv train loss:  -64.79138493537903 , diff:  1.858926773071289
layer  16  adv train finish, try to retain  232
test acc: top1 ->  70.296 ; top5 ->  89.396  and loss:  968.3278201520443
forward train acc: top1 ->  67.7421875 ; top5 ->  86.28125  and loss:  271.36636394262314
test acc: top1 ->  70.362 ; top5 ->  89.428  and loss:  966.6972138881683
forward train acc: top1 ->  67.2578125 ; top5 ->  86.3359375  and loss:  275.68477696180344
test acc: top1 ->  70.252 ; top5 ->  89.402  and loss:  967.4815439283848
forward train acc: top1 ->  67.9609375 ; top5 ->  87.265625  and loss:  265.32559818029404
test acc: top1 ->  70.312 ; top5 ->  89.46  and loss:  967.0760725140572
forward train acc: top1 ->  68.1484375 ; top5 ->  86.7734375  and loss:  269.0147426724434
test acc: top1 ->  70.258 ; top5 ->  89.448  and loss:  967.1146765053272
forward train acc: top1 ->  67.625 ; top5 ->  86.3203125  and loss:  276.06594556570053
test acc: top1 ->  70.286 ; top5 ->  89.494  and loss:  966.4785246551037
forward train acc: top1 ->  67.1015625 ; top5 ->  86.578125  and loss:  276.0878419280052
test acc: top1 ->  70.302 ; top5 ->  89.474  and loss:  965.0887319743633
forward train acc: top1 ->  68.0625 ; top5 ->  86.8515625  and loss:  270.72513741254807
test acc: top1 ->  70.274 ; top5 ->  89.536  and loss:  966.4402958154678
forward train acc: top1 ->  68.1875 ; top5 ->  86.5390625  and loss:  271.42199712991714
test acc: top1 ->  70.42 ; top5 ->  89.562  and loss:  963.4516847729683
forward train acc: top1 ->  68.0390625 ; top5 ->  86.7421875  and loss:  267.3359094262123
test acc: top1 ->  70.432 ; top5 ->  89.542  and loss:  963.0247406065464
forward train acc: top1 ->  67.859375 ; top5 ->  86.75  and loss:  270.6022828221321
test acc: top1 ->  70.41 ; top5 ->  89.532  and loss:  962.2892993092537
>>>>>>> reverse layer  16  since performance drop >>>>>>>
==> this epoch:  239 / 256 , inc:  1
---------------- start layer  17  ---------------
adv train loss:  -67.30094712972641 , diff:  67.30094712972641
adv train loss:  -65.6101303100586 , diff:  1.6908168196678162
layer  17  adv train finish, try to retain  226
test acc: top1 ->  70.016 ; top5 ->  89.34  and loss:  974.8826030492783
forward train acc: top1 ->  67.78125 ; top5 ->  86.609375  and loss:  269.1600528359413
test acc: top1 ->  70.108 ; top5 ->  89.408  and loss:  971.5457991957664
forward train acc: top1 ->  68.8046875 ; top5 ->  86.7421875  and loss:  267.24934285879135
test acc: top1 ->  70.152 ; top5 ->  89.374  and loss:  969.5922149121761
forward train acc: top1 ->  68.2421875 ; top5 ->  86.3984375  and loss:  269.5485447049141
test acc: top1 ->  70.148 ; top5 ->  89.406  and loss:  971.3805445432663
forward train acc: top1 ->  68.4609375 ; top5 ->  86.796875  and loss:  265.7041845321655
test acc: top1 ->  70.144 ; top5 ->  89.448  and loss:  969.8605653047562
forward train acc: top1 ->  67.8984375 ; top5 ->  86.546875  and loss:  270.9718909263611
test acc: top1 ->  70.254 ; top5 ->  89.468  and loss:  968.0591105818748
forward train acc: top1 ->  68.5234375 ; top5 ->  86.1875  and loss:  271.49318397045135
test acc: top1 ->  70.176 ; top5 ->  89.452  and loss:  965.2835545539856
forward train acc: top1 ->  67.90625 ; top5 ->  86.5  and loss:  271.44695061445236
test acc: top1 ->  70.22 ; top5 ->  89.466  and loss:  967.5393204092979
forward train acc: top1 ->  68.6640625 ; top5 ->  86.4453125  and loss:  268.49491864442825
test acc: top1 ->  70.3 ; top5 ->  89.51  and loss:  964.4886994957924
forward train acc: top1 ->  67.859375 ; top5 ->  86.2421875  and loss:  275.18229818344116
test acc: top1 ->  70.254 ; top5 ->  89.474  and loss:  966.5180194973946
forward train acc: top1 ->  67.6015625 ; top5 ->  86.234375  and loss:  269.3530384302139
test acc: top1 ->  70.158 ; top5 ->  89.456  and loss:  965.3908185362816
>>>>>>> reverse layer  17  since performance drop >>>>>>>
==> this epoch:  230 / 256 , inc:  1
---------------- start layer  18  ---------------
adv train loss:  -65.48674446344376 , diff:  65.48674446344376
adv train loss:  -66.20579481124878 , diff:  0.7190503478050232
layer  18  adv train finish, try to retain  244
>>>>>>> reverse layer  18  since no improvement >>>>>>>
---------------- start layer  19  ---------------
adv train loss:  -66.51467138528824 , diff:  66.51467138528824
adv train loss:  -68.42986381053925 , diff:  1.915192425251007
layer  19  adv train finish, try to retain  183
>>>>>>> reverse layer  19  since no improvement >>>>>>>
---------------- start layer  20  ---------------
adv train loss:  -65.59440433979034 , diff:  65.59440433979034
adv train loss:  -67.53493225574493 , diff:  1.9405279159545898
layer  20  adv train finish, try to retain  231
>>>>>>> reverse layer  20  since no improvement >>>>>>>
---------------- start layer  21  ---------------
adv train loss:  -64.05400478839874 , diff:  64.05400478839874
adv train loss:  -64.3236476778984 , diff:  0.2696428894996643
layer  21  adv train finish, try to retain  237
test acc: top1 ->  70.43 ; top5 ->  89.476  and loss:  966.5421263873577
forward train acc: top1 ->  68.4609375 ; top5 ->  86.34375  and loss:  270.41333943605423
test acc: top1 ->  70.314 ; top5 ->  89.434  and loss:  969.2139125466347
forward train acc: top1 ->  68.0234375 ; top5 ->  86.3203125  and loss:  275.47666907310486
test acc: top1 ->  70.246 ; top5 ->  89.476  and loss:  966.5428822636604
forward train acc: top1 ->  68.7265625 ; top5 ->  87.234375  and loss:  263.9490436911583
test acc: top1 ->  70.334 ; top5 ->  89.482  and loss:  967.6243422031403
forward train acc: top1 ->  68.3359375 ; top5 ->  86.65625  and loss:  268.2833608984947
test acc: top1 ->  70.366 ; top5 ->  89.536  and loss:  963.9354642629623
forward train acc: top1 ->  68.5234375 ; top5 ->  87.0703125  and loss:  264.2345697283745
test acc: top1 ->  70.324 ; top5 ->  89.488  and loss:  964.1632235646248
forward train acc: top1 ->  68.125 ; top5 ->  86.5859375  and loss:  273.8619038462639
test acc: top1 ->  70.416 ; top5 ->  89.502  and loss:  962.5984220504761
forward train acc: top1 ->  68.6484375 ; top5 ->  86.6640625  and loss:  266.17308962345123
test acc: top1 ->  70.422 ; top5 ->  89.478  and loss:  964.1940923929214
forward train acc: top1 ->  68.7734375 ; top5 ->  86.875  and loss:  265.2470244169235
test acc: top1 ->  70.444 ; top5 ->  89.45  and loss:  960.5036556720734
forward train acc: top1 ->  68.515625 ; top5 ->  86.78125  and loss:  265.40875029563904
test acc: top1 ->  70.528 ; top5 ->  89.604  and loss:  961.0042479634285
forward train acc: top1 ->  68.5625 ; top5 ->  86.59375  and loss:  267.2318329811096
test acc: top1 ->  70.54 ; top5 ->  89.538  and loss:  960.8632467985153
>>>>>>> reverse layer  21  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  22  ---------------
adv train loss:  -62.909275472164154 , diff:  62.909275472164154
adv train loss:  -64.49483555555344 , diff:  1.5855600833892822
layer  22  adv train finish, try to retain  208
>>>>>>> reverse layer  22  since no improvement >>>>>>>
---------------- start layer  23  ---------------
adv train loss:  -62.99597930908203 , diff:  62.99597930908203
adv train loss:  -63.64040023088455 , diff:  0.6444209218025208
layer  23  adv train finish, try to retain  187
>>>>>>> reverse layer  23  since no improvement >>>>>>>
---------------- start layer  24  ---------------
adv train loss:  -66.81589484214783 , diff:  66.81589484214783
adv train loss:  -68.37118810415268 , diff:  1.5552932620048523
layer  24  adv train finish, try to retain  244
>>>>>>> reverse layer  24  since no improvement >>>>>>>
---------------- start layer  25  ---------------
adv train loss:  -66.50204253196716 , diff:  66.50204253196716
adv train loss:  -69.83878982067108 , diff:  3.3367472887039185
layer  25  adv train finish, try to retain  232
test acc: top1 ->  69.97 ; top5 ->  89.21  and loss:  976.6422563791275
forward train acc: top1 ->  68.09375 ; top5 ->  86.796875  and loss:  269.3328359723091
test acc: top1 ->  70.35 ; top5 ->  89.484  and loss:  966.294471681118
forward train acc: top1 ->  67.46875 ; top5 ->  86.546875  and loss:  274.49356919527054
test acc: top1 ->  70.398 ; top5 ->  89.462  and loss:  964.50477039814
forward train acc: top1 ->  68.734375 ; top5 ->  87.09375  and loss:  266.13104194402695
test acc: top1 ->  70.292 ; top5 ->  89.394  and loss:  966.6353333592415
forward train acc: top1 ->  68.46875 ; top5 ->  87.109375  and loss:  261.3231731057167
test acc: top1 ->  70.434 ; top5 ->  89.476  and loss:  964.1893454790115
forward train acc: top1 ->  68.609375 ; top5 ->  86.734375  and loss:  266.2830989956856
test acc: top1 ->  70.452 ; top5 ->  89.476  and loss:  962.4727603793144
forward train acc: top1 ->  68.703125 ; top5 ->  87.046875  and loss:  263.3545202612877
test acc: top1 ->  70.46 ; top5 ->  89.522  and loss:  963.983972966671
forward train acc: top1 ->  68.3671875 ; top5 ->  86.4375  and loss:  266.0924942493439
test acc: top1 ->  70.48 ; top5 ->  89.436  and loss:  962.3604606986046
forward train acc: top1 ->  68.6015625 ; top5 ->  87.09375  and loss:  265.96869599819183
test acc: top1 ->  70.552 ; top5 ->  89.584  and loss:  959.6426389217377
forward train acc: top1 ->  68.4765625 ; top5 ->  87.1796875  and loss:  265.03587532043457
test acc: top1 ->  70.402 ; top5 ->  89.544  and loss:  962.857294023037
forward train acc: top1 ->  68.046875 ; top5 ->  86.4375  and loss:  269.20277827978134
test acc: top1 ->  70.56 ; top5 ->  89.552  and loss:  959.6601397097111
>>>>>>> reverse layer  25  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  26  ---------------
adv train loss:  -67.25453370809555 , diff:  67.25453370809555
adv train loss:  -65.46320301294327 , diff:  1.7913306951522827
layer  26  adv train finish, try to retain  496
test acc: top1 ->  70.204 ; top5 ->  89.336  and loss:  972.5563789308071
forward train acc: top1 ->  67.6875 ; top5 ->  86.5625  and loss:  273.1269242167473
test acc: top1 ->  70.298 ; top5 ->  89.412  and loss:  969.4499940574169
forward train acc: top1 ->  68.625 ; top5 ->  87.015625  and loss:  266.0906144976616
test acc: top1 ->  70.214 ; top5 ->  89.406  and loss:  965.5214702188969
forward train acc: top1 ->  68.234375 ; top5 ->  86.90625  and loss:  265.221003472805
test acc: top1 ->  70.286 ; top5 ->  89.398  and loss:  967.2553262114525
forward train acc: top1 ->  68.453125 ; top5 ->  86.5390625  and loss:  268.22515869140625
test acc: top1 ->  70.23 ; top5 ->  89.406  and loss:  964.2786627113819
forward train acc: top1 ->  68.8125 ; top5 ->  86.8671875  and loss:  268.0535334944725
test acc: top1 ->  70.23 ; top5 ->  89.442  and loss:  966.1601960062981
forward train acc: top1 ->  68.4453125 ; top5 ->  87.1953125  and loss:  263.73825734853745
test acc: top1 ->  70.306 ; top5 ->  89.506  and loss:  963.5877957344055
forward train acc: top1 ->  67.9765625 ; top5 ->  86.953125  and loss:  267.71545273065567
test acc: top1 ->  70.276 ; top5 ->  89.506  and loss:  964.6326163709164
forward train acc: top1 ->  68.3671875 ; top5 ->  86.640625  and loss:  268.6763146519661
test acc: top1 ->  70.366 ; top5 ->  89.522  and loss:  963.4278680682182
forward train acc: top1 ->  68.9453125 ; top5 ->  87.265625  and loss:  261.02312445640564
test acc: top1 ->  70.274 ; top5 ->  89.52  and loss:  961.1118438243866
forward train acc: top1 ->  69.2734375 ; top5 ->  87.1640625  and loss:  263.18961745500565
test acc: top1 ->  70.362 ; top5 ->  89.5  and loss:  962.4346576929092
>>>>>>> reverse layer  26  since performance drop >>>>>>>
==> this epoch:  502 / 512 , inc:  1
---------------- start layer  27  ---------------
adv train loss:  -65.3006083369255 , diff:  65.3006083369255
adv train loss:  -68.70650672912598 , diff:  3.40589839220047
layer  27  adv train finish, try to retain  497
test acc: top1 ->  69.868 ; top5 ->  89.338  and loss:  971.5105866193771
forward train acc: top1 ->  67.34375 ; top5 ->  86.0078125  and loss:  279.4014620780945
test acc: top1 ->  70.09 ; top5 ->  89.356  and loss:  971.8969863057137
forward train acc: top1 ->  68.4375 ; top5 ->  86.9375  and loss:  268.2273649573326
test acc: top1 ->  70.222 ; top5 ->  89.406  and loss:  969.0983161330223
forward train acc: top1 ->  67.9140625 ; top5 ->  86.4609375  and loss:  271.9492101073265
test acc: top1 ->  70.088 ; top5 ->  89.434  and loss:  969.9761038422585
forward train acc: top1 ->  67.78125 ; top5 ->  86.2734375  and loss:  273.35206919908524
test acc: top1 ->  70.146 ; top5 ->  89.49  and loss:  969.2729954719543
forward train acc: top1 ->  67.890625 ; top5 ->  86.5390625  and loss:  273.8543298840523
test acc: top1 ->  70.158 ; top5 ->  89.448  and loss:  968.5202400684357
forward train acc: top1 ->  68.125 ; top5 ->  86.3125  and loss:  270.44029635190964
test acc: top1 ->  70.344 ; top5 ->  89.478  and loss:  964.5231407880783
forward train acc: top1 ->  67.8828125 ; top5 ->  86.7265625  and loss:  272.92791801691055
test acc: top1 ->  70.26 ; top5 ->  89.476  and loss:  965.2800030112267
forward train acc: top1 ->  68.1875 ; top5 ->  86.5625  and loss:  270.1334380507469
test acc: top1 ->  70.298 ; top5 ->  89.57  and loss:  964.6698896288872
forward train acc: top1 ->  68.484375 ; top5 ->  86.953125  and loss:  265.07387512922287
test acc: top1 ->  70.36 ; top5 ->  89.57  and loss:  965.3721237182617
forward train acc: top1 ->  68.5078125 ; top5 ->  86.984375  and loss:  265.96326142549515
test acc: top1 ->  70.294 ; top5 ->  89.542  and loss:  964.6825590729713
>>>>>>> reverse layer  27  since performance drop >>>>>>>
==> this epoch:  505 / 512 , inc:  1
---------------- start layer  28  ---------------
adv train loss:  -66.08671909570694 , diff:  66.08671909570694
adv train loss:  -68.16863685846329 , diff:  2.0819177627563477
layer  28  adv train finish, try to retain  502
test acc: top1 ->  70.488 ; top5 ->  89.57  and loss:  970.0250469446182
forward train acc: top1 ->  67.984375 ; top5 ->  86.6015625  and loss:  271.78730142116547
test acc: top1 ->  70.284 ; top5 ->  89.582  and loss:  965.0160928964615
forward train acc: top1 ->  68.2890625 ; top5 ->  86.9296875  and loss:  267.5579277873039
test acc: top1 ->  70.198 ; top5 ->  89.49  and loss:  969.043884396553
forward train acc: top1 ->  67.71875 ; top5 ->  86.6171875  and loss:  270.95489525794983
test acc: top1 ->  70.246 ; top5 ->  89.604  and loss:  965.1566132307053
forward train acc: top1 ->  68.703125 ; top5 ->  87.0625  and loss:  263.01620876789093
test acc: top1 ->  70.29 ; top5 ->  89.524  and loss:  964.0685669779778
forward train acc: top1 ->  68.7421875 ; top5 ->  86.8671875  and loss:  264.28472477197647
test acc: top1 ->  70.262 ; top5 ->  89.552  and loss:  965.4992332458496
forward train acc: top1 ->  68.8046875 ; top5 ->  87.09375  and loss:  266.91535407304764
test acc: top1 ->  70.342 ; top5 ->  89.584  and loss:  963.0998287200928
forward train acc: top1 ->  68.5703125 ; top5 ->  86.984375  and loss:  267.53755325078964
test acc: top1 ->  70.326 ; top5 ->  89.554  and loss:  963.8971303105354
forward train acc: top1 ->  68.4921875 ; top5 ->  87.0546875  and loss:  262.18389642238617
test acc: top1 ->  70.33 ; top5 ->  89.56  and loss:  964.9130914211273
forward train acc: top1 ->  68.2265625 ; top5 ->  86.3125  and loss:  273.2766435146332
test acc: top1 ->  70.292 ; top5 ->  89.544  and loss:  962.2363016009331
forward train acc: top1 ->  68.28125 ; top5 ->  86.484375  and loss:  269.2080931067467
test acc: top1 ->  70.384 ; top5 ->  89.62  and loss:  962.0358190536499
>>>>>>> reverse layer  28  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  29  ---------------
adv train loss:  -64.93684256076813 , diff:  64.93684256076813
adv train loss:  -64.90931451320648 , diff:  0.027528047561645508
layer  29  adv train finish, try to retain  495
test acc: top1 ->  70.3 ; top5 ->  89.536  and loss:  971.1124388575554
forward train acc: top1 ->  68.1875 ; top5 ->  86.984375  and loss:  265.65445816516876
test acc: top1 ->  70.194 ; top5 ->  89.568  and loss:  966.1096891760826
forward train acc: top1 ->  67.5078125 ; top5 ->  86.765625  and loss:  269.89859688282013
test acc: top1 ->  70.172 ; top5 ->  89.498  and loss:  971.5667063593864
forward train acc: top1 ->  68.2890625 ; top5 ->  86.9453125  and loss:  266.6873046159744
test acc: top1 ->  70.196 ; top5 ->  89.5  and loss:  968.8816930651665
forward train acc: top1 ->  68.3671875 ; top5 ->  86.7890625  and loss:  264.7184516787529
test acc: top1 ->  70.236 ; top5 ->  89.498  and loss:  966.2712797522545
forward train acc: top1 ->  68.015625 ; top5 ->  86.6640625  and loss:  271.7099732160568
test acc: top1 ->  70.296 ; top5 ->  89.596  and loss:  963.0280933976173
forward train acc: top1 ->  67.3984375 ; top5 ->  86.2421875  and loss:  272.81141114234924
test acc: top1 ->  70.338 ; top5 ->  89.566  and loss:  962.9434459209442
forward train acc: top1 ->  68.1328125 ; top5 ->  86.765625  and loss:  268.75380486249924
test acc: top1 ->  70.318 ; top5 ->  89.562  and loss:  961.8587642908096
forward train acc: top1 ->  67.921875 ; top5 ->  86.8984375  and loss:  266.65116387605667
test acc: top1 ->  70.444 ; top5 ->  89.574  and loss:  961.2852455377579
forward train acc: top1 ->  68.4140625 ; top5 ->  86.8203125  and loss:  269.72821921110153
test acc: top1 ->  70.3 ; top5 ->  89.518  and loss:  962.8916030526161
forward train acc: top1 ->  68.3046875 ; top5 ->  86.4375  and loss:  271.19741505384445
test acc: top1 ->  70.396 ; top5 ->  89.546  and loss:  961.6199850440025
>>>>>>> reverse layer  29  since performance drop >>>>>>>
==> this epoch:  498 / 512 , inc:  1
---------------- start layer  30  ---------------
adv train loss:  -71.7247708439827 , diff:  71.7247708439827
adv train loss:  -67.06699472665787 , diff:  4.657776117324829
layer  30  adv train finish, try to retain  492
test acc: top1 ->  69.976 ; top5 ->  89.384  and loss:  967.7390711307526
forward train acc: top1 ->  68.7109375 ; top5 ->  87.2890625  and loss:  261.4421365261078
test acc: top1 ->  70.262 ; top5 ->  89.5  and loss:  967.4525564908981
forward train acc: top1 ->  67.90625 ; top5 ->  86.5703125  and loss:  270.455538213253
test acc: top1 ->  70.36 ; top5 ->  89.534  and loss:  964.9471462368965
forward train acc: top1 ->  68.40625 ; top5 ->  86.84375  and loss:  265.7809457182884
test acc: top1 ->  70.16 ; top5 ->  89.55  and loss:  966.670425593853
forward train acc: top1 ->  68.0234375 ; top5 ->  86.3671875  and loss:  272.33434522151947
test acc: top1 ->  70.386 ; top5 ->  89.612  and loss:  963.4790481925011
forward train acc: top1 ->  68.4765625 ; top5 ->  86.8984375  and loss:  266.2667205929756
test acc: top1 ->  70.242 ; top5 ->  89.522  and loss:  961.791099011898
forward train acc: top1 ->  68.5546875 ; top5 ->  86.9921875  and loss:  264.38299614191055
test acc: top1 ->  70.366 ; top5 ->  89.644  and loss:  962.2346905469894
forward train acc: top1 ->  68.0 ; top5 ->  86.7421875  and loss:  271.5146446824074
test acc: top1 ->  70.398 ; top5 ->  89.548  and loss:  960.8435333967209
forward train acc: top1 ->  68.296875 ; top5 ->  86.59375  and loss:  269.96925032138824
test acc: top1 ->  70.388 ; top5 ->  89.606  and loss:  960.6245620250702
forward train acc: top1 ->  68.4375 ; top5 ->  86.9375  and loss:  267.004762172699
test acc: top1 ->  70.396 ; top5 ->  89.638  and loss:  960.1297004818916
forward train acc: top1 ->  67.8984375 ; top5 ->  85.96875  and loss:  275.15658670663834
test acc: top1 ->  70.398 ; top5 ->  89.65  and loss:  958.6969342827797
>>>>>>> reverse layer  30  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  31  ---------------
adv train loss:  -63.196116387844086 , diff:  63.196116387844086
adv train loss:  -66.11375570297241 , diff:  2.9176393151283264
layer  31  adv train finish, try to retain  480
test acc: top1 ->  69.726 ; top5 ->  89.212  and loss:  981.360031247139
forward train acc: top1 ->  67.546875 ; top5 ->  86.0390625  and loss:  278.6480410695076
test acc: top1 ->  69.922 ; top5 ->  89.23  and loss:  977.4492215514183
forward train acc: top1 ->  67.890625 ; top5 ->  86.2421875  and loss:  272.39639538526535
test acc: top1 ->  69.984 ; top5 ->  89.358  and loss:  975.284452855587
forward train acc: top1 ->  68.2578125 ; top5 ->  86.5546875  and loss:  271.30743527412415
test acc: top1 ->  70.166 ; top5 ->  89.36  and loss:  971.4138969779015
forward train acc: top1 ->  67.8984375 ; top5 ->  86.6328125  and loss:  271.73923790454865
test acc: top1 ->  70.118 ; top5 ->  89.412  and loss:  970.8377372026443
forward train acc: top1 ->  67.328125 ; top5 ->  86.015625  and loss:  276.4948592185974
test acc: top1 ->  70.152 ; top5 ->  89.426  and loss:  970.8331631422043
forward train acc: top1 ->  68.0546875 ; top5 ->  86.21875  and loss:  273.23984426259995
test acc: top1 ->  70.2 ; top5 ->  89.458  and loss:  968.7800184488297
forward train acc: top1 ->  67.9453125 ; top5 ->  86.21875  and loss:  273.57493829727173
test acc: top1 ->  70.22 ; top5 ->  89.404  and loss:  968.4100672602654
forward train acc: top1 ->  67.8125 ; top5 ->  86.6015625  and loss:  270.6969041824341
test acc: top1 ->  70.196 ; top5 ->  89.468  and loss:  969.4454160928726
forward train acc: top1 ->  67.765625 ; top5 ->  86.53125  and loss:  272.5507547259331
test acc: top1 ->  70.254 ; top5 ->  89.464  and loss:  966.6213431954384
forward train acc: top1 ->  68.5859375 ; top5 ->  86.984375  and loss:  265.75227051973343
test acc: top1 ->  70.44 ; top5 ->  89.542  and loss:  963.3511233329773
>>>>>>> reverse layer  31  since performance drop >>>>>>>
==> this epoch:  501 / 512 , inc:  1
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.045, 0.045, 0.00059326171875, 0.016875, 0.016875, 0.045, 0.000791015625, 0.0001483154296875, 0.0084375, 0.000791015625, 0.0084375, 0.00158203125, 0.0031640625, 0.0225, 7.415771484375e-05, 0.00019775390625, 0.0003955078125, 0.0003955078125, 0.0003955078125, 0.01125, 0.0010546875, 0.0001483154296875, 0.00421875, 0.01125, 0.0003955078125, 0.0003955078125, 7.415771484375e-05, 7.415771484375e-05, 7.415771484375e-05, 7.415771484375e-05, 7.415771484375e-05, 0.00019775390625]  wait [0, 0, 4, 2, 2, 0, 2, 3, 2, 2, 2, 3, 4, 0, 3, 1, 2, 2, 2, 0, 0, 4, 2, 0, 2, 2, 4, 4, 4, 4, 4, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 2
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  11  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -66.42108649015427 , diff:  66.42108649015427
adv train loss:  -65.59551376104355 , diff:  0.8255727291107178
layer  0  adv train finish, try to retain  38
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -65.0075256228447 , diff:  65.0075256228447
adv train loss:  -70.82004743814468 , diff:  5.812521815299988
layer  1  adv train finish, try to retain  44
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
adv train loss:  -67.62190002202988 , diff:  67.62190002202988
adv train loss:  -66.0838452577591 , diff:  1.5380547642707825
layer  3  adv train finish, try to retain  42
test acc: top1 ->  64.65 ; top5 ->  86.148  and loss:  1160.5191988945007
forward train acc: top1 ->  67.6484375 ; top5 ->  86.6171875  and loss:  272.0239378809929
test acc: top1 ->  69.494 ; top5 ->  89.186  and loss:  987.4424250125885
forward train acc: top1 ->  67.0703125 ; top5 ->  86.3203125  and loss:  278.7202092409134
test acc: top1 ->  69.788 ; top5 ->  89.252  and loss:  983.3069481253624
forward train acc: top1 ->  67.4765625 ; top5 ->  85.9921875  and loss:  276.99920415878296
test acc: top1 ->  69.718 ; top5 ->  89.28  and loss:  981.0361459255219
forward train acc: top1 ->  67.3671875 ; top5 ->  86.15625  and loss:  276.4612965583801
test acc: top1 ->  69.904 ; top5 ->  89.296  and loss:  976.7980401515961
forward train acc: top1 ->  68.265625 ; top5 ->  86.5703125  and loss:  267.44163864851
test acc: top1 ->  69.948 ; top5 ->  89.33  and loss:  976.8639315962791
forward train acc: top1 ->  67.578125 ; top5 ->  86.265625  and loss:  274.4498549103737
test acc: top1 ->  69.95 ; top5 ->  89.42  and loss:  976.1841677427292
forward train acc: top1 ->  68.4375 ; top5 ->  86.28125  and loss:  270.73001730442047
test acc: top1 ->  69.986 ; top5 ->  89.396  and loss:  973.8634188175201
forward train acc: top1 ->  67.8203125 ; top5 ->  86.390625  and loss:  273.14053547382355
test acc: top1 ->  69.954 ; top5 ->  89.4  and loss:  974.1799821853638
forward train acc: top1 ->  67.921875 ; top5 ->  86.6875  and loss:  270.14796644449234
test acc: top1 ->  70.198 ; top5 ->  89.42  and loss:  973.5824810862541
forward train acc: top1 ->  67.75 ; top5 ->  86.546875  and loss:  271.281113922596
test acc: top1 ->  69.976 ; top5 ->  89.338  and loss:  972.6438592076302
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  43 / 64 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -69.81922435760498 , diff:  69.81922435760498
adv train loss:  -70.79013031721115 , diff:  0.9709059596061707
layer  4  adv train finish, try to retain  41
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -65.7000897526741 , diff:  65.7000897526741
adv train loss:  -68.86712503433228 , diff:  3.1670352816581726
layer  5  adv train finish, try to retain  44
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -66.99290263652802 , diff:  66.99290263652802
adv train loss:  -65.88394123315811 , diff:  1.1089614033699036
layer  6  adv train finish, try to retain  109
test acc: top1 ->  68.348 ; top5 ->  88.374  and loss:  1029.232259452343
forward train acc: top1 ->  67.8359375 ; top5 ->  86.8125  and loss:  271.6450389623642
test acc: top1 ->  70.164 ; top5 ->  89.446  and loss:  972.3777429461479
forward train acc: top1 ->  68.4453125 ; top5 ->  86.765625  and loss:  266.2260229587555
test acc: top1 ->  70.042 ; top5 ->  89.386  and loss:  972.016090452671
forward train acc: top1 ->  68.296875 ; top5 ->  86.71875  and loss:  268.74993073940277
test acc: top1 ->  70.104 ; top5 ->  89.468  and loss:  971.7589234113693
forward train acc: top1 ->  67.5546875 ; top5 ->  86.578125  and loss:  272.5277517437935
test acc: top1 ->  70.17 ; top5 ->  89.422  and loss:  969.9670565724373
forward train acc: top1 ->  67.5546875 ; top5 ->  86.09375  and loss:  274.6216830611229
test acc: top1 ->  70.212 ; top5 ->  89.4  and loss:  973.2572211027145
forward train acc: top1 ->  68.6484375 ; top5 ->  86.5078125  and loss:  268.6015934944153
test acc: top1 ->  70.32 ; top5 ->  89.414  and loss:  967.1153226494789
forward train acc: top1 ->  68.1875 ; top5 ->  86.453125  and loss:  272.236141204834
test acc: top1 ->  70.358 ; top5 ->  89.504  and loss:  966.2188717722893
forward train acc: top1 ->  67.7578125 ; top5 ->  86.1484375  and loss:  272.4153002500534
test acc: top1 ->  70.432 ; top5 ->  89.506  and loss:  965.6672044992447
forward train acc: top1 ->  68.6875 ; top5 ->  86.96875  and loss:  263.1741104722023
test acc: top1 ->  70.368 ; top5 ->  89.492  and loss:  966.7932323217392
forward train acc: top1 ->  68.9296875 ; top5 ->  86.8828125  and loss:  265.14324003458023
test acc: top1 ->  70.416 ; top5 ->  89.508  and loss:  963.1542549133301
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  112 / 128 , inc:  1
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
adv train loss:  -65.35639077425003 , diff:  65.35639077425003
adv train loss:  -66.19142085313797 , diff:  0.8350300788879395
layer  8  adv train finish, try to retain  84
test acc: top1 ->  68.578 ; top5 ->  88.574  and loss:  1020.2114977240562
forward train acc: top1 ->  66.7578125 ; top5 ->  86.3203125  and loss:  276.3602257370949
test acc: top1 ->  69.896 ; top5 ->  89.254  and loss:  980.5412172079086
forward train acc: top1 ->  67.0546875 ; top5 ->  86.1484375  and loss:  279.5310035943985
test acc: top1 ->  70.078 ; top5 ->  89.32  and loss:  975.2414057850838
forward train acc: top1 ->  67.6171875 ; top5 ->  86.8125  and loss:  270.93034195899963
test acc: top1 ->  69.842 ; top5 ->  89.21  and loss:  977.6843928694725
forward train acc: top1 ->  67.4609375 ; top5 ->  86.65625  and loss:  271.8593991994858
test acc: top1 ->  70.002 ; top5 ->  89.322  and loss:  974.7730273008347
forward train acc: top1 ->  67.3984375 ; top5 ->  86.4765625  and loss:  273.1653802394867
test acc: top1 ->  70.054 ; top5 ->  89.332  and loss:  973.9736796617508
forward train acc: top1 ->  67.7421875 ; top5 ->  86.46875  and loss:  272.2025950551033
test acc: top1 ->  69.966 ; top5 ->  89.328  and loss:  971.9067227244377
forward train acc: top1 ->  68.40625 ; top5 ->  86.8359375  and loss:  268.5906825661659
test acc: top1 ->  70.146 ; top5 ->  89.374  and loss:  969.5690155029297
forward train acc: top1 ->  68.828125 ; top5 ->  86.671875  and loss:  263.9117887020111
test acc: top1 ->  70.11 ; top5 ->  89.38  and loss:  970.003244638443
forward train acc: top1 ->  68.703125 ; top5 ->  86.828125  and loss:  265.42927926778793
test acc: top1 ->  70.052 ; top5 ->  89.386  and loss:  969.1499537825584
forward train acc: top1 ->  68.0 ; top5 ->  86.4609375  and loss:  271.7452107667923
test acc: top1 ->  70.128 ; top5 ->  89.386  and loss:  967.66498452425
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -68.39834427833557 , diff:  68.39834427833557
adv train loss:  -66.92098915576935 , diff:  1.4773551225662231
layer  9  adv train finish, try to retain  116
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -65.85838437080383 , diff:  65.85838437080383
adv train loss:  -66.90795147418976 , diff:  1.0495671033859253
layer  10  adv train finish, try to retain  87
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
### skip layer  11 wait:  3  ###
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
adv train loss:  -69.76156705617905 , diff:  69.76156705617905
adv train loss:  -67.85856050252914 , diff:  1.9030065536499023
layer  13  adv train finish, try to retain  75
>>>>>>> reverse layer  13  since no improvement >>>>>>>
---------------- start layer  14  ---------------
### skip layer  14 wait:  3  ###
---------------- start layer  15  ---------------
adv train loss:  -68.83122611045837 , diff:  68.83122611045837
adv train loss:  -66.4604617357254 , diff:  2.370764374732971
layer  15  adv train finish, try to retain  247
test acc: top1 ->  69.58 ; top5 ->  89.218  and loss:  981.3169785141945
forward train acc: top1 ->  68.015625 ; top5 ->  86.140625  and loss:  271.97449374198914
test acc: top1 ->  70.162 ; top5 ->  89.52  and loss:  968.0022256970406
forward train acc: top1 ->  67.328125 ; top5 ->  86.578125  and loss:  273.65801751613617
test acc: top1 ->  70.148 ; top5 ->  89.506  and loss:  967.3015520572662
forward train acc: top1 ->  67.890625 ; top5 ->  86.5390625  and loss:  269.51540046930313
test acc: top1 ->  70.254 ; top5 ->  89.476  and loss:  967.9506199359894
forward train acc: top1 ->  68.5703125 ; top5 ->  86.7578125  and loss:  265.2394413948059
test acc: top1 ->  70.24 ; top5 ->  89.522  and loss:  967.1197308301926
forward train acc: top1 ->  68.2109375 ; top5 ->  86.765625  and loss:  268.49840009212494
test acc: top1 ->  70.336 ; top5 ->  89.54  and loss:  963.6270761489868
forward train acc: top1 ->  68.78125 ; top5 ->  87.0390625  and loss:  265.26786893606186
test acc: top1 ->  70.34 ; top5 ->  89.562  and loss:  963.8212514519691
forward train acc: top1 ->  68.1640625 ; top5 ->  86.9453125  and loss:  266.074476480484
test acc: top1 ->  70.324 ; top5 ->  89.556  and loss:  963.4957465529442
forward train acc: top1 ->  68.2109375 ; top5 ->  86.78125  and loss:  266.37530928850174
test acc: top1 ->  70.306 ; top5 ->  89.578  and loss:  963.4888608455658
forward train acc: top1 ->  67.828125 ; top5 ->  86.828125  and loss:  268.5173524618149
test acc: top1 ->  70.406 ; top5 ->  89.6  and loss:  960.6571642756462
forward train acc: top1 ->  68.3515625 ; top5 ->  87.1171875  and loss:  262.7085202932358
test acc: top1 ->  70.45 ; top5 ->  89.6  and loss:  961.5246316194534
>>>>>>> reverse layer  15  since performance drop >>>>>>>
==> this epoch:  248 / 256 , inc:  1
---------------- start layer  16  ---------------
adv train loss:  -71.16086685657501 , diff:  71.16086685657501
adv train loss:  -69.49875897169113 , diff:  1.6621078848838806
layer  16  adv train finish, try to retain  233
test acc: top1 ->  70.112 ; top5 ->  89.414  and loss:  972.7248896360397
forward train acc: top1 ->  67.8515625 ; top5 ->  86.5859375  and loss:  265.99807727336884
test acc: top1 ->  70.264 ; top5 ->  89.314  and loss:  970.8991211652756
forward train acc: top1 ->  68.484375 ; top5 ->  87.0546875  and loss:  266.3939393758774
test acc: top1 ->  70.23 ; top5 ->  89.492  and loss:  968.772825717926
forward train acc: top1 ->  69.109375 ; top5 ->  87.7265625  and loss:  256.71778094768524
test acc: top1 ->  70.32 ; top5 ->  89.49  and loss:  967.4095826148987
forward train acc: top1 ->  68.4296875 ; top5 ->  87.0859375  and loss:  265.04482877254486
test acc: top1 ->  70.348 ; top5 ->  89.558  and loss:  965.7258137464523
forward train acc: top1 ->  69.3515625 ; top5 ->  87.1328125  and loss:  261.51296389102936
test acc: top1 ->  70.37 ; top5 ->  89.592  and loss:  964.2679722905159
forward train acc: top1 ->  68.4375 ; top5 ->  87.0546875  and loss:  265.22388869524
test acc: top1 ->  70.424 ; top5 ->  89.514  and loss:  965.1552309393883
forward train acc: top1 ->  69.4375 ; top5 ->  87.4140625  and loss:  257.60232841968536
test acc: top1 ->  70.488 ; top5 ->  89.588  and loss:  963.1412135362625
forward train acc: top1 ->  68.7421875 ; top5 ->  86.9921875  and loss:  266.5677426457405
test acc: top1 ->  70.47 ; top5 ->  89.506  and loss:  961.6710629463196
forward train acc: top1 ->  67.828125 ; top5 ->  86.5546875  and loss:  269.96606373786926
test acc: top1 ->  70.462 ; top5 ->  89.554  and loss:  964.3242002129555
forward train acc: top1 ->  67.9296875 ; top5 ->  86.6015625  and loss:  271.51072055101395
test acc: top1 ->  70.506 ; top5 ->  89.548  and loss:  961.6732757687569
>>>>>>> reverse layer  16  since performance drop >>>>>>>
==> this epoch:  239 / 256 , inc:  1
---------------- start layer  17  ---------------
adv train loss:  -71.69304776191711 , diff:  71.69304776191711
adv train loss:  -67.37018889188766 , diff:  4.3228588700294495
layer  17  adv train finish, try to retain  238
>>>>>>> reverse layer  17  since no improvement >>>>>>>
---------------- start layer  18  ---------------
adv train loss:  -63.77296566963196 , diff:  63.77296566963196
adv train loss:  -65.67995685338974 , diff:  1.906991183757782
layer  18  adv train finish, try to retain  235
>>>>>>> reverse layer  18  since no improvement >>>>>>>
---------------- start layer  19  ---------------
adv train loss:  -65.69359827041626 , diff:  65.69359827041626
adv train loss:  -63.671653270721436 , diff:  2.021944999694824
layer  19  adv train finish, try to retain  161
test acc: top1 ->  69.83 ; top5 ->  89.128  and loss:  980.6810349225998
forward train acc: top1 ->  68.671875 ; top5 ->  86.9765625  and loss:  265.5925940275192
test acc: top1 ->  69.942 ; top5 ->  89.206  and loss:  980.9109995365143
forward train acc: top1 ->  68.328125 ; top5 ->  86.4296875  and loss:  267.84714406728745
test acc: top1 ->  69.914 ; top5 ->  89.354  and loss:  975.8772104978561
forward train acc: top1 ->  67.984375 ; top5 ->  86.703125  and loss:  266.6779935359955
test acc: top1 ->  70.022 ; top5 ->  89.368  and loss:  973.1448757648468
forward train acc: top1 ->  68.0390625 ; top5 ->  86.671875  and loss:  269.3853008747101
test acc: top1 ->  70.078 ; top5 ->  89.316  and loss:  974.2964304685593
forward train acc: top1 ->  67.8828125 ; top5 ->  86.1796875  and loss:  275.1625932455063
test acc: top1 ->  70.034 ; top5 ->  89.352  and loss:  973.3925285339355
forward train acc: top1 ->  68.0625 ; top5 ->  87.046875  and loss:  267.5942994952202
test acc: top1 ->  70.11 ; top5 ->  89.426  and loss:  969.714193046093
forward train acc: top1 ->  68.34375 ; top5 ->  87.2890625  and loss:  262.2756254673004
test acc: top1 ->  70.162 ; top5 ->  89.476  and loss:  967.0084898471832
forward train acc: top1 ->  67.78125 ; top5 ->  86.796875  and loss:  268.6558964252472
test acc: top1 ->  70.1 ; top5 ->  89.414  and loss:  970.2159912586212
forward train acc: top1 ->  68.046875 ; top5 ->  86.2265625  and loss:  272.7586467862129
test acc: top1 ->  70.134 ; top5 ->  89.464  and loss:  967.1685230135918
forward train acc: top1 ->  67.609375 ; top5 ->  86.3046875  and loss:  275.2706382870674
test acc: top1 ->  70.108 ; top5 ->  89.382  and loss:  969.032973408699
>>>>>>> reverse layer  19  since performance drop >>>>>>>
==> this epoch:  179 / 256 , inc:  1
---------------- start layer  20  ---------------
adv train loss:  -66.02981328964233 , diff:  66.02981328964233
adv train loss:  -65.93508857488632 , diff:  0.09472471475601196
layer  20  adv train finish, try to retain  221
test acc: top1 ->  70.312 ; top5 ->  89.598  and loss:  963.3030987977982
forward train acc: top1 ->  68.4765625 ; top5 ->  86.921875  and loss:  265.8422871828079
test acc: top1 ->  70.22 ; top5 ->  89.5  and loss:  966.7626655697823
forward train acc: top1 ->  67.9453125 ; top5 ->  86.84375  and loss:  268.41407853364944
test acc: top1 ->  70.344 ; top5 ->  89.586  and loss:  964.3366110920906
forward train acc: top1 ->  68.5390625 ; top5 ->  86.6953125  and loss:  268.3379451036453
test acc: top1 ->  70.21 ; top5 ->  89.5  and loss:  965.4435442686081
forward train acc: top1 ->  68.640625 ; top5 ->  86.8515625  and loss:  264.86191207170486
test acc: top1 ->  70.274 ; top5 ->  89.504  and loss:  966.7079978585243
forward train acc: top1 ->  68.0 ; top5 ->  86.203125  and loss:  272.31796872615814
test acc: top1 ->  70.324 ; top5 ->  89.622  and loss:  962.5627822875977
forward train acc: top1 ->  68.8828125 ; top5 ->  86.8984375  and loss:  261.4926223754883
test acc: top1 ->  70.346 ; top5 ->  89.592  and loss:  962.8481607437134
forward train acc: top1 ->  67.7265625 ; top5 ->  86.71875  and loss:  268.0873302221298
test acc: top1 ->  70.358 ; top5 ->  89.586  and loss:  960.6933487653732
forward train acc: top1 ->  68.109375 ; top5 ->  86.890625  and loss:  264.7789821624756
test acc: top1 ->  70.368 ; top5 ->  89.57  and loss:  961.7950446009636
forward train acc: top1 ->  68.9140625 ; top5 ->  86.9609375  and loss:  261.7111741900444
test acc: top1 ->  70.438 ; top5 ->  89.584  and loss:  959.4649326205254
forward train acc: top1 ->  68.203125 ; top5 ->  86.703125  and loss:  264.61373031139374
test acc: top1 ->  70.422 ; top5 ->  89.636  and loss:  960.12299233675
>>>>>>> reverse layer  20  since performance drop >>>>>>>
==> this epoch:  225 / 256 , inc:  1
---------------- start layer  21  ---------------
### skip layer  21 wait:  4  ###
---------------- start layer  22  ---------------
adv train loss:  -67.6770955324173 , diff:  67.6770955324173
adv train loss:  -68.58176177740097 , diff:  0.9046662449836731
layer  22  adv train finish, try to retain  175
test acc: top1 ->  70.122 ; top5 ->  89.346  and loss:  977.8044404387474
forward train acc: top1 ->  67.7890625 ; top5 ->  86.7109375  and loss:  270.05925595760345
test acc: top1 ->  70.012 ; top5 ->  89.344  and loss:  974.5986403822899
forward train acc: top1 ->  68.5078125 ; top5 ->  87.3203125  and loss:  264.1071172952652
test acc: top1 ->  70.168 ; top5 ->  89.346  and loss:  970.1003492474556
forward train acc: top1 ->  68.125 ; top5 ->  86.78125  and loss:  268.1321635246277
test acc: top1 ->  70.18 ; top5 ->  89.348  and loss:  969.8047411441803
forward train acc: top1 ->  68.1015625 ; top5 ->  86.7421875  and loss:  269.55559277534485
test acc: top1 ->  70.236 ; top5 ->  89.386  and loss:  969.0302693247795
forward train acc: top1 ->  68.4453125 ; top5 ->  86.9453125  and loss:  264.8442115187645
test acc: top1 ->  70.306 ; top5 ->  89.372  and loss:  967.017584323883
forward train acc: top1 ->  68.203125 ; top5 ->  86.5859375  and loss:  268.89650124311447
test acc: top1 ->  70.334 ; top5 ->  89.438  and loss:  967.5317643284798
forward train acc: top1 ->  67.75 ; top5 ->  86.3203125  and loss:  273.1403934955597
test acc: top1 ->  70.316 ; top5 ->  89.418  and loss:  966.5540841817856
forward train acc: top1 ->  68.203125 ; top5 ->  86.7109375  and loss:  267.60994124412537
test acc: top1 ->  70.31 ; top5 ->  89.442  and loss:  965.3282272815704
forward train acc: top1 ->  67.65625 ; top5 ->  86.21875  and loss:  272.84817910194397
test acc: top1 ->  70.322 ; top5 ->  89.444  and loss:  964.9390757679939
forward train acc: top1 ->  68.265625 ; top5 ->  86.8515625  and loss:  267.62893068790436
test acc: top1 ->  70.41 ; top5 ->  89.506  and loss:  964.2282964587212
>>>>>>> reverse layer  22  since performance drop >>>>>>>
==> this epoch:  208 / 256 , inc:  1
---------------- start layer  23  ---------------
adv train loss:  -65.9829130768776 , diff:  65.9829130768776
adv train loss:  -67.76404595375061 , diff:  1.7811328768730164
layer  23  adv train finish, try to retain  156
test acc: top1 ->  69.646 ; top5 ->  88.984  and loss:  1004.2040886282921
forward train acc: top1 ->  67.578125 ; top5 ->  86.015625  and loss:  275.3737905025482
test acc: top1 ->  70.054 ; top5 ->  89.376  and loss:  973.0259366631508
forward train acc: top1 ->  67.609375 ; top5 ->  85.9140625  and loss:  279.0750761628151
test acc: top1 ->  70.082 ; top5 ->  89.348  and loss:  973.8620804548264
forward train acc: top1 ->  67.703125 ; top5 ->  86.953125  and loss:  268.7737835049629
test acc: top1 ->  70.108 ; top5 ->  89.364  and loss:  969.3476625680923
forward train acc: top1 ->  68.6796875 ; top5 ->  87.1171875  and loss:  264.3713657259941
test acc: top1 ->  70.106 ; top5 ->  89.366  and loss:  969.788754761219
forward train acc: top1 ->  67.953125 ; top5 ->  86.4140625  and loss:  273.73526191711426
test acc: top1 ->  70.136 ; top5 ->  89.476  and loss:  968.3422849178314
forward train acc: top1 ->  68.4765625 ; top5 ->  87.0234375  and loss:  266.7977347970009
test acc: top1 ->  70.116 ; top5 ->  89.4  and loss:  967.3936939239502
forward train acc: top1 ->  68.3671875 ; top5 ->  86.5859375  and loss:  268.84576267004013
test acc: top1 ->  70.12 ; top5 ->  89.45  and loss:  966.7633427977562
forward train acc: top1 ->  67.828125 ; top5 ->  86.5703125  and loss:  270.1396732330322
test acc: top1 ->  70.252 ; top5 ->  89.45  and loss:  967.5990822315216
forward train acc: top1 ->  68.4609375 ; top5 ->  87.0234375  and loss:  262.36626714468
test acc: top1 ->  70.24 ; top5 ->  89.496  and loss:  964.970843732357
forward train acc: top1 ->  68.0859375 ; top5 ->  87.3125  and loss:  266.34375804662704
test acc: top1 ->  70.272 ; top5 ->  89.488  and loss:  962.841418325901
>>>>>>> reverse layer  23  since performance drop >>>>>>>
==> this epoch:  187 / 256 , inc:  1
---------------- start layer  24  ---------------
adv train loss:  -68.50362157821655 , diff:  68.50362157821655
adv train loss:  -66.72570097446442 , diff:  1.7779206037521362
layer  24  adv train finish, try to retain  233
test acc: top1 ->  70.066 ; top5 ->  89.456  and loss:  970.4500961899757
forward train acc: top1 ->  68.265625 ; top5 ->  86.90625  and loss:  264.9864284992218
test acc: top1 ->  70.168 ; top5 ->  89.588  and loss:  963.4763007760048
forward train acc: top1 ->  67.6953125 ; top5 ->  86.5625  and loss:  273.1669851541519
test acc: top1 ->  70.312 ; top5 ->  89.522  and loss:  966.038542509079
forward train acc: top1 ->  67.6640625 ; top5 ->  86.7109375  and loss:  270.6749186515808
test acc: top1 ->  70.272 ; top5 ->  89.49  and loss:  966.4325265884399
forward train acc: top1 ->  68.5390625 ; top5 ->  86.921875  and loss:  266.5093777179718
test acc: top1 ->  70.22 ; top5 ->  89.512  and loss:  965.7395244240761
forward train acc: top1 ->  68.3046875 ; top5 ->  86.8359375  and loss:  269.4593058228493
test acc: top1 ->  70.24 ; top5 ->  89.504  and loss:  965.9805479049683
forward train acc: top1 ->  68.9609375 ; top5 ->  87.2265625  and loss:  262.92903715372086
test acc: top1 ->  70.382 ; top5 ->  89.554  and loss:  961.0658674240112
forward train acc: top1 ->  69.296875 ; top5 ->  86.921875  and loss:  261.42638874053955
test acc: top1 ->  70.458 ; top5 ->  89.628  and loss:  958.7651886343956
forward train acc: top1 ->  68.265625 ; top5 ->  86.453125  and loss:  270.32028502225876
test acc: top1 ->  70.44 ; top5 ->  89.642  and loss:  960.1705223321915
forward train acc: top1 ->  68.7421875 ; top5 ->  86.828125  and loss:  265.4958046078682
test acc: top1 ->  70.464 ; top5 ->  89.646  and loss:  959.3235885500908
forward train acc: top1 ->  68.4140625 ; top5 ->  86.8046875  and loss:  266.0422900915146
test acc: top1 ->  70.464 ; top5 ->  89.576  and loss:  959.3126733899117
>>>>>>> reverse layer  24  since performance drop >>>>>>>
==> this epoch:  244 / 256 , inc:  1
---------------- start layer  25  ---------------
adv train loss:  -64.5770274400711 , diff:  64.5770274400711
adv train loss:  -68.9758203625679 , diff:  4.398792922496796
layer  25  adv train finish, try to retain  234
test acc: top1 ->  70.224 ; top5 ->  89.522  and loss:  966.3033358454704
forward train acc: top1 ->  68.3046875 ; top5 ->  86.578125  and loss:  268.8452104330063
test acc: top1 ->  70.326 ; top5 ->  89.548  and loss:  964.6068916916847
forward train acc: top1 ->  68.09375 ; top5 ->  86.4453125  and loss:  272.04083436727524
test acc: top1 ->  70.414 ; top5 ->  89.606  and loss:  962.3666596412659
forward train acc: top1 ->  68.0234375 ; top5 ->  86.8515625  and loss:  265.94384902715683
test acc: top1 ->  70.302 ; top5 ->  89.562  and loss:  962.8429616093636
forward train acc: top1 ->  68.828125 ; top5 ->  86.984375  and loss:  263.42588555812836
test acc: top1 ->  70.442 ; top5 ->  89.638  and loss:  959.6577750444412
forward train acc: top1 ->  68.3984375 ; top5 ->  86.90625  and loss:  265.53153598308563
test acc: top1 ->  70.426 ; top5 ->  89.648  and loss:  958.8528645634651
forward train acc: top1 ->  67.859375 ; top5 ->  86.6171875  and loss:  269.1361457705498
test acc: top1 ->  70.496 ; top5 ->  89.66  and loss:  959.2310378551483
forward train acc: top1 ->  67.9375 ; top5 ->  86.515625  and loss:  268.48302537202835
test acc: top1 ->  70.426 ; top5 ->  89.69  and loss:  958.1309643387794
forward train acc: top1 ->  67.4140625 ; top5 ->  86.59375  and loss:  270.76465314626694
test acc: top1 ->  70.432 ; top5 ->  89.704  and loss:  958.2856957316399
forward train acc: top1 ->  68.4296875 ; top5 ->  87.171875  and loss:  259.3239275217056
test acc: top1 ->  70.456 ; top5 ->  89.7  and loss:  955.6907181739807
forward train acc: top1 ->  67.8671875 ; top5 ->  86.9375  and loss:  266.3119852542877
test acc: top1 ->  70.542 ; top5 ->  89.736  and loss:  957.8263284564018
>>>>>>> reverse layer  25  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  26  ---------------
### skip layer  26 wait:  4  ###
---------------- start layer  27  ---------------
### skip layer  27 wait:  4  ###
---------------- start layer  28  ---------------
### skip layer  28 wait:  4  ###
---------------- start layer  29  ---------------
### skip layer  29 wait:  4  ###
---------------- start layer  30  ---------------
### skip layer  30 wait:  4  ###
---------------- start layer  31  ---------------
adv train loss:  -67.89152228832245 , diff:  67.89152228832245
adv train loss:  -67.35890352725983 , diff:  0.5326187610626221
layer  31  adv train finish, try to retain  478
test acc: top1 ->  69.936 ; top5 ->  89.27  and loss:  979.4973120093346
forward train acc: top1 ->  67.734375 ; top5 ->  86.046875  and loss:  276.4503800868988
test acc: top1 ->  70.036 ; top5 ->  89.438  and loss:  970.9000583291054
forward train acc: top1 ->  67.5234375 ; top5 ->  86.609375  and loss:  271.8256726861
test acc: top1 ->  70.106 ; top5 ->  89.398  and loss:  967.0399148464203
forward train acc: top1 ->  68.34375 ; top5 ->  87.125  and loss:  265.82511729002
test acc: top1 ->  70.242 ; top5 ->  89.524  and loss:  964.2800164222717
forward train acc: top1 ->  68.578125 ; top5 ->  86.9921875  and loss:  266.6098901629448
test acc: top1 ->  70.228 ; top5 ->  89.532  and loss:  964.281359910965
forward train acc: top1 ->  68.171875 ; top5 ->  87.3125  and loss:  262.8192192912102
test acc: top1 ->  70.302 ; top5 ->  89.598  and loss:  961.2948188185692
forward train acc: top1 ->  68.15625 ; top5 ->  86.9453125  and loss:  270.96387481689453
test acc: top1 ->  70.294 ; top5 ->  89.562  and loss:  960.8422955274582
forward train acc: top1 ->  68.0703125 ; top5 ->  86.8203125  and loss:  270.5789303779602
test acc: top1 ->  70.38 ; top5 ->  89.568  and loss:  960.6045259237289
forward train acc: top1 ->  68.4296875 ; top5 ->  86.359375  and loss:  267.24020928144455
test acc: top1 ->  70.396 ; top5 ->  89.582  and loss:  960.6326244473457
forward train acc: top1 ->  68.0 ; top5 ->  86.7578125  and loss:  269.5026886463165
test acc: top1 ->  70.402 ; top5 ->  89.6  and loss:  958.1365991830826
forward train acc: top1 ->  67.8671875 ; top5 ->  86.7265625  and loss:  269.79178607463837
test acc: top1 ->  70.318 ; top5 ->  89.598  and loss:  957.2744069099426
>>>>>>> reverse layer  31  since performance drop >>>>>>>
==> this epoch:  501 / 512 , inc:  1
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.09, 0.09, 0.00059326171875, 0.01265625, 0.03375, 0.09, 0.00059326171875, 0.0001483154296875, 0.006328125, 0.00158203125, 0.016875, 0.00158203125, 0.0031640625, 0.045, 7.415771484375e-05, 0.0001483154296875, 0.000296630859375, 0.000791015625, 0.000791015625, 0.0084375, 0.000791015625, 0.0001483154296875, 0.0031640625, 0.0084375, 0.000296630859375, 0.000296630859375, 7.415771484375e-05, 7.415771484375e-05, 7.415771484375e-05, 7.415771484375e-05, 7.415771484375e-05, 0.0001483154296875]  wait [0, 0, 3, 4, 2, 0, 4, 2, 4, 2, 2, 2, 3, 0, 2, 3, 4, 2, 2, 2, 2, 3, 4, 2, 4, 4, 3, 3, 3, 3, 3, 4]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 2
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  12  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -65.54419207572937 , diff:  65.54419207572937
adv train loss:  -64.13551044464111 , diff:  1.4086816310882568
layer  0  adv train finish, try to retain  24
test acc: top1 ->  62.536 ; top5 ->  84.262  and loss:  1239.4320713877678
forward train acc: top1 ->  66.2890625 ; top5 ->  85.046875  and loss:  288.2752721309662
test acc: top1 ->  69.382 ; top5 ->  89.104  and loss:  994.2127411961555
forward train acc: top1 ->  66.8203125 ; top5 ->  85.921875  and loss:  280.89394986629486
test acc: top1 ->  69.456 ; top5 ->  89.132  and loss:  989.7741504311562
forward train acc: top1 ->  67.6328125 ; top5 ->  86.1875  and loss:  275.9060164093971
test acc: top1 ->  69.694 ; top5 ->  89.254  and loss:  982.2682836055756
forward train acc: top1 ->  66.9921875 ; top5 ->  86.15625  and loss:  277.43262535333633
test acc: top1 ->  69.902 ; top5 ->  89.386  and loss:  977.2667607069016
forward train acc: top1 ->  68.0078125 ; top5 ->  86.4140625  and loss:  270.12545984983444
test acc: top1 ->  69.894 ; top5 ->  89.326  and loss:  978.6517357230186
forward train acc: top1 ->  68.140625 ; top5 ->  86.4375  and loss:  270.55657213926315
test acc: top1 ->  69.986 ; top5 ->  89.392  and loss:  975.2555103898048
forward train acc: top1 ->  68.09375 ; top5 ->  86.7421875  and loss:  268.36356222629547
test acc: top1 ->  69.866 ; top5 ->  89.348  and loss:  975.2806106209755
forward train acc: top1 ->  67.75 ; top5 ->  86.6328125  and loss:  269.48174583911896
test acc: top1 ->  70.108 ; top5 ->  89.44  and loss:  972.9877645373344
forward train acc: top1 ->  67.046875 ; top5 ->  86.0390625  and loss:  278.2153019309044
test acc: top1 ->  70.086 ; top5 ->  89.492  and loss:  971.4063415527344
forward train acc: top1 ->  68.0625 ; top5 ->  87.0546875  and loss:  265.2933183312416
test acc: top1 ->  70.008 ; top5 ->  89.42  and loss:  973.2593883872032
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  37 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -72.1790611743927 , diff:  72.1790611743927
adv train loss:  -70.88453388214111 , diff:  1.294527292251587
layer  1  adv train finish, try to retain  45
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
adv train loss:  -65.84861075878143 , diff:  65.84861075878143
adv train loss:  -67.29925805330276 , diff:  1.4506472945213318
layer  4  adv train finish, try to retain  28
test acc: top1 ->  69.074 ; top5 ->  88.666  and loss:  1007.4929755926132
forward train acc: top1 ->  67.828125 ; top5 ->  86.25  and loss:  275.39358818531036
test acc: top1 ->  70.088 ; top5 ->  89.438  and loss:  972.8973708152771
forward train acc: top1 ->  68.671875 ; top5 ->  86.8359375  and loss:  262.37976133823395
test acc: top1 ->  70.094 ; top5 ->  89.38  and loss:  972.2620823383331
forward train acc: top1 ->  68.6640625 ; top5 ->  86.78125  and loss:  266.10088473558426
test acc: top1 ->  69.996 ; top5 ->  89.37  and loss:  972.852773308754
forward train acc: top1 ->  68.0390625 ; top5 ->  86.359375  and loss:  271.771575152874
test acc: top1 ->  70.198 ; top5 ->  89.482  and loss:  968.7018268704414
forward train acc: top1 ->  68.328125 ; top5 ->  86.484375  and loss:  269.80039286613464
test acc: top1 ->  70.256 ; top5 ->  89.43  and loss:  969.1087464690208
forward train acc: top1 ->  68.5546875 ; top5 ->  87.1953125  and loss:  263.95939564704895
test acc: top1 ->  70.224 ; top5 ->  89.432  and loss:  966.0319535136223
forward train acc: top1 ->  67.7265625 ; top5 ->  86.5859375  and loss:  269.3992327451706
test acc: top1 ->  70.348 ; top5 ->  89.506  and loss:  965.1882277131081
forward train acc: top1 ->  69.3203125 ; top5 ->  87.1875  and loss:  259.92098528146744
test acc: top1 ->  70.406 ; top5 ->  89.566  and loss:  962.851590514183
forward train acc: top1 ->  68.5625 ; top5 ->  87.0703125  and loss:  260.3205173611641
test acc: top1 ->  70.306 ; top5 ->  89.504  and loss:  962.404491186142
forward train acc: top1 ->  68.6875 ; top5 ->  86.65625  and loss:  265.3392373919487
test acc: top1 ->  70.222 ; top5 ->  89.538  and loss:  962.7574660181999
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  40 / 64 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -67.31715136766434 , diff:  67.31715136766434
adv train loss:  -69.76907461881638 , diff:  2.4519232511520386
layer  5  adv train finish, try to retain  44
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
adv train loss:  -69.8927413225174 , diff:  69.8927413225174
adv train loss:  -65.27701985836029 , diff:  4.6157214641571045
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  121
test acc: top1 ->  69.54 ; top5 ->  89.084  and loss:  986.6414450407028
forward train acc: top1 ->  68.0078125 ; top5 ->  86.9609375  and loss:  263.2982977628708
test acc: top1 ->  70.334 ; top5 ->  89.508  and loss:  966.9684935212135
forward train acc: top1 ->  69.140625 ; top5 ->  87.5234375  and loss:  258.8589699268341
test acc: top1 ->  70.296 ; top5 ->  89.564  and loss:  966.3964650034904
forward train acc: top1 ->  68.6015625 ; top5 ->  86.3203125  and loss:  270.54852467775345
test acc: top1 ->  70.29 ; top5 ->  89.53  and loss:  967.3146498799324
forward train acc: top1 ->  68.953125 ; top5 ->  86.8984375  and loss:  263.6402819156647
test acc: top1 ->  70.476 ; top5 ->  89.526  and loss:  964.2540336251259
forward train acc: top1 ->  68.3046875 ; top5 ->  86.875  and loss:  262.8768185377121
test acc: top1 ->  70.422 ; top5 ->  89.52  and loss:  964.5327401161194
forward train acc: top1 ->  68.1484375 ; top5 ->  86.625  and loss:  269.417886197567
test acc: top1 ->  70.408 ; top5 ->  89.526  and loss:  961.9574831724167
forward train acc: top1 ->  68.8828125 ; top5 ->  86.8203125  and loss:  263.4741788506508
test acc: top1 ->  70.474 ; top5 ->  89.634  and loss:  962.1767001748085
forward train acc: top1 ->  68.4921875 ; top5 ->  86.984375  and loss:  265.72749453783035
test acc: top1 ->  70.502 ; top5 ->  89.624  and loss:  960.8482271432877
forward train acc: top1 ->  68.328125 ; top5 ->  86.7890625  and loss:  270.644147336483
test acc: top1 ->  70.462 ; top5 ->  89.588  and loss:  961.7064087986946
forward train acc: top1 ->  68.9453125 ; top5 ->  87.2578125  and loss:  261.2673344016075
test acc: top1 ->  70.42 ; top5 ->  89.518  and loss:  961.8505730032921
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  122 / 128 , inc:  1
---------------- start layer  8  ---------------
### skip layer  8 wait:  4  ###
---------------- start layer  9  ---------------
adv train loss:  -63.7924964427948 , diff:  63.7924964427948
adv train loss:  -65.10407954454422 , diff:  1.3115831017494202
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  114
test acc: top1 ->  70.27 ; top5 ->  89.528  and loss:  970.3515836596489
forward train acc: top1 ->  68.2578125 ; top5 ->  87.1171875  and loss:  263.3965513706207
test acc: top1 ->  70.356 ; top5 ->  89.61  and loss:  964.4737411141396
forward train acc: top1 ->  68.28125 ; top5 ->  86.6640625  and loss:  271.46518701314926
test acc: top1 ->  70.362 ; top5 ->  89.49  and loss:  966.5490339398384
forward train acc: top1 ->  68.625 ; top5 ->  86.84375  and loss:  267.26517927646637
test acc: top1 ->  70.368 ; top5 ->  89.522  and loss:  964.9570673108101
forward train acc: top1 ->  68.1328125 ; top5 ->  86.96875  and loss:  267.6110483407974
test acc: top1 ->  70.38 ; top5 ->  89.574  and loss:  962.2271106243134
forward train acc: top1 ->  68.0703125 ; top5 ->  87.0546875  and loss:  266.16135889291763
test acc: top1 ->  70.49 ; top5 ->  89.618  and loss:  962.962718129158
forward train acc: top1 ->  68.4765625 ; top5 ->  86.8671875  and loss:  264.42412692308426
test acc: top1 ->  70.414 ; top5 ->  89.582  and loss:  963.6070608496666
forward train acc: top1 ->  67.671875 ; top5 ->  86.4609375  and loss:  271.55027306079865
test acc: top1 ->  70.462 ; top5 ->  89.64  and loss:  960.3073799014091
forward train acc: top1 ->  68.2890625 ; top5 ->  86.9296875  and loss:  264.21757757663727
test acc: top1 ->  70.482 ; top5 ->  89.704  and loss:  958.1990110874176
forward train acc: top1 ->  69.1171875 ; top5 ->  87.296875  and loss:  262.2646750807762
test acc: top1 ->  70.532 ; top5 ->  89.652  and loss:  958.4377200007439
forward train acc: top1 ->  68.6328125 ; top5 ->  87.25  and loss:  264.24109971523285
test acc: top1 ->  70.516 ; top5 ->  89.626  and loss:  958.6389866471291
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  115 / 128 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -67.25308901071548 , diff:  67.25308901071548
adv train loss:  -67.08272165060043 , diff:  0.17036736011505127
layer  10  adv train finish, try to retain  74
test acc: top1 ->  68.33 ; top5 ->  88.348  and loss:  1037.83222168684
forward train acc: top1 ->  68.1015625 ; top5 ->  86.53125  and loss:  271.354478597641
test acc: top1 ->  69.894 ; top5 ->  89.386  and loss:  979.2007385492325
forward train acc: top1 ->  67.5078125 ; top5 ->  85.9921875  and loss:  275.2492209672928
test acc: top1 ->  70.012 ; top5 ->  89.378  and loss:  973.2255026102066
forward train acc: top1 ->  68.0859375 ; top5 ->  86.8515625  and loss:  268.8114879131317
test acc: top1 ->  69.942 ; top5 ->  89.28  and loss:  977.288818359375
forward train acc: top1 ->  68.15625 ; top5 ->  86.640625  and loss:  266.95463567972183
test acc: top1 ->  70.144 ; top5 ->  89.424  and loss:  971.9705644249916
forward train acc: top1 ->  68.203125 ; top5 ->  86.578125  and loss:  269.97282832860947
test acc: top1 ->  70.176 ; top5 ->  89.506  and loss:  969.2602694630623
forward train acc: top1 ->  68.078125 ; top5 ->  86.4375  and loss:  271.1424061655998
test acc: top1 ->  70.156 ; top5 ->  89.522  and loss:  968.5354121923447
forward train acc: top1 ->  68.59375 ; top5 ->  87.1640625  and loss:  266.0248281955719
test acc: top1 ->  70.264 ; top5 ->  89.508  and loss:  968.1418871283531
forward train acc: top1 ->  67.96875 ; top5 ->  87.109375  and loss:  266.5402034521103
test acc: top1 ->  70.214 ; top5 ->  89.462  and loss:  968.0502593517303
forward train acc: top1 ->  68.015625 ; top5 ->  86.7265625  and loss:  268.4118344783783
test acc: top1 ->  70.224 ; top5 ->  89.548  and loss:  966.0305225849152
forward train acc: top1 ->  68.109375 ; top5 ->  86.3203125  and loss:  271.8243175148964
test acc: top1 ->  70.288 ; top5 ->  89.54  and loss:  965.9983443021774
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -67.04321575164795 , diff:  67.04321575164795
adv train loss:  -67.703684091568 , diff:  0.660468339920044
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  99
test acc: top1 ->  70.04 ; top5 ->  89.378  and loss:  975.6774820685387
forward train acc: top1 ->  67.9140625 ; top5 ->  86.578125  and loss:  270.3767476081848
test acc: top1 ->  70.294 ; top5 ->  89.522  and loss:  964.417171895504
forward train acc: top1 ->  68.1796875 ; top5 ->  86.65625  and loss:  270.13015472888947
test acc: top1 ->  70.356 ; top5 ->  89.466  and loss:  964.2117763757706
forward train acc: top1 ->  67.5234375 ; top5 ->  86.3046875  and loss:  273.5213348865509
test acc: top1 ->  70.458 ; top5 ->  89.422  and loss:  964.7694253325462
forward train acc: top1 ->  68.65625 ; top5 ->  86.96875  and loss:  265.69496220350266
test acc: top1 ->  70.526 ; top5 ->  89.532  and loss:  961.8292601108551
forward train acc: top1 ->  67.6640625 ; top5 ->  86.2265625  and loss:  272.6269260644913
test acc: top1 ->  70.468 ; top5 ->  89.53  and loss:  960.9511338472366
forward train acc: top1 ->  68.875 ; top5 ->  86.9765625  and loss:  263.7782592177391
test acc: top1 ->  70.48 ; top5 ->  89.486  and loss:  963.7906593084335
forward train acc: top1 ->  68.4375 ; top5 ->  87.171875  and loss:  264.36619704961777
test acc: top1 ->  70.546 ; top5 ->  89.482  and loss:  961.2563932538033
forward train acc: top1 ->  68.578125 ; top5 ->  87.0234375  and loss:  266.03250551223755
test acc: top1 ->  70.566 ; top5 ->  89.548  and loss:  960.4311695694923
forward train acc: top1 ->  68.3984375 ; top5 ->  86.890625  and loss:  263.0999907255173
test acc: top1 ->  70.6 ; top5 ->  89.55  and loss:  957.3449151515961
forward train acc: top1 ->  68.046875 ; top5 ->  86.609375  and loss:  268.6725653409958
test acc: top1 ->  70.532 ; top5 ->  89.496  and loss:  960.9097548127174
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  12  ---------------
### skip layer  12 wait:  3  ###
---------------- start layer  13  ---------------
adv train loss:  -65.86647933721542 , diff:  65.86647933721542
adv train loss:  -65.00965684652328 , diff:  0.8568224906921387
layer  13  adv train finish, try to retain  84
>>>>>>> reverse layer  13  since no improvement >>>>>>>
---------------- start layer  14  ---------------
adv train loss:  -64.92158091068268 , diff:  64.92158091068268
adv train loss:  -65.68218922615051 , diff:  0.7606083154678345
************ all values are small in this layer **********
layer  14  adv train finish, try to retain  249
test acc: top1 ->  70.356 ; top5 ->  89.5  and loss:  964.5652891993523
forward train acc: top1 ->  68.171875 ; top5 ->  86.75  and loss:  265.06581622362137
test acc: top1 ->  70.58 ; top5 ->  89.55  and loss:  960.8726043701172
forward train acc: top1 ->  68.1796875 ; top5 ->  86.5234375  and loss:  273.26169949769974
test acc: top1 ->  70.488 ; top5 ->  89.606  and loss:  963.286670923233
forward train acc: top1 ->  68.734375 ; top5 ->  87.3203125  and loss:  260.2244803905487
test acc: top1 ->  70.486 ; top5 ->  89.544  and loss:  962.4645498394966
forward train acc: top1 ->  68.8671875 ; top5 ->  86.484375  and loss:  267.5553524494171
test acc: top1 ->  70.498 ; top5 ->  89.526  and loss:  963.1284110546112
forward train acc: top1 ->  68.7109375 ; top5 ->  87.1484375  and loss:  261.29280281066895
test acc: top1 ->  70.58 ; top5 ->  89.58  and loss:  960.1814091801643
forward train acc: top1 ->  68.5546875 ; top5 ->  86.9453125  and loss:  266.22285157442093
test acc: top1 ->  70.602 ; top5 ->  89.598  and loss:  959.21631026268
forward train acc: top1 ->  67.984375 ; top5 ->  86.3671875  and loss:  271.00857067108154
test acc: top1 ->  70.634 ; top5 ->  89.612  and loss:  957.6222451925278
forward train acc: top1 ->  68.4296875 ; top5 ->  86.6796875  and loss:  268.3405417203903
test acc: top1 ->  70.588 ; top5 ->  89.614  and loss:  957.9317290782928
forward train acc: top1 ->  68.9375 ; top5 ->  87.1484375  and loss:  259.9558977484703
test acc: top1 ->  70.662 ; top5 ->  89.622  and loss:  956.53119379282
forward train acc: top1 ->  67.7890625 ; top5 ->  86.6796875  and loss:  268.6892357468605
test acc: top1 ->  70.58 ; top5 ->  89.618  and loss:  960.3232728838921
>>>>>>> reverse layer  14  since performance drop >>>>>>>
==> this epoch:  250 / 256 , inc:  1
---------------- start layer  15  ---------------
### skip layer  15 wait:  3  ###
---------------- start layer  16  ---------------
### skip layer  16 wait:  4  ###
---------------- start layer  17  ---------------
adv train loss:  -68.18825769424438 , diff:  68.18825769424438
adv train loss:  -69.84424763917923 , diff:  1.655989944934845
layer  17  adv train finish, try to retain  218
test acc: top1 ->  70.174 ; top5 ->  89.418  and loss:  978.3732314109802
forward train acc: top1 ->  68.546875 ; top5 ->  87.078125  and loss:  265.0510915517807
test acc: top1 ->  70.394 ; top5 ->  89.492  and loss:  966.1075939536095
forward train acc: top1 ->  68.5 ; top5 ->  86.8984375  and loss:  266.21258944272995
test acc: top1 ->  70.416 ; top5 ->  89.464  and loss:  967.8192930817604
forward train acc: top1 ->  68.703125 ; top5 ->  86.7734375  and loss:  267.1971272826195
test acc: top1 ->  70.398 ; top5 ->  89.508  and loss:  965.4531369805336
forward train acc: top1 ->  68.1171875 ; top5 ->  86.6484375  and loss:  269.6251168847084
test acc: top1 ->  70.472 ; top5 ->  89.534  and loss:  963.4395813941956
forward train acc: top1 ->  68.2578125 ; top5 ->  86.46875  and loss:  268.98897886276245
test acc: top1 ->  70.472 ; top5 ->  89.57  and loss:  959.622340798378
forward train acc: top1 ->  67.984375 ; top5 ->  86.859375  and loss:  270.20852959156036
test acc: top1 ->  70.486 ; top5 ->  89.592  and loss:  958.5145571827888
forward train acc: top1 ->  69.03125 ; top5 ->  86.84375  and loss:  263.6660208106041
test acc: top1 ->  70.504 ; top5 ->  89.566  and loss:  957.2671440839767
forward train acc: top1 ->  68.6796875 ; top5 ->  87.2734375  and loss:  262.4940207004547
test acc: top1 ->  70.598 ; top5 ->  89.576  and loss:  956.6860523819923
forward train acc: top1 ->  68.7421875 ; top5 ->  87.046875  and loss:  262.0338619351387
test acc: top1 ->  70.494 ; top5 ->  89.638  and loss:  958.9257490038872
forward train acc: top1 ->  68.71875 ; top5 ->  87.109375  and loss:  264.5661478638649
test acc: top1 ->  70.548 ; top5 ->  89.604  and loss:  956.7838543653488
>>>>>>> reverse layer  17  since performance drop >>>>>>>
==> this epoch:  230 / 256 , inc:  1
---------------- start layer  18  ---------------
adv train loss:  -66.13559454679489 , diff:  66.13559454679489
adv train loss:  -62.507014751434326 , diff:  3.628579795360565
layer  18  adv train finish, try to retain  219
test acc: top1 ->  70.386 ; top5 ->  89.464  and loss:  960.9155552387238
forward train acc: top1 ->  68.6875 ; top5 ->  86.7421875  and loss:  265.5904317498207
test acc: top1 ->  70.438 ; top5 ->  89.478  and loss:  966.7246795892715
forward train acc: top1 ->  68.890625 ; top5 ->  87.125  and loss:  261.9901487827301
test acc: top1 ->  70.4 ; top5 ->  89.448  and loss:  967.3023768663406
forward train acc: top1 ->  69.140625 ; top5 ->  87.296875  and loss:  260.53905618190765
test acc: top1 ->  70.468 ; top5 ->  89.532  and loss:  963.010989844799
forward train acc: top1 ->  69.5 ; top5 ->  87.390625  and loss:  257.70934784412384
test acc: top1 ->  70.468 ; top5 ->  89.516  and loss:  962.7536896467209
forward train acc: top1 ->  68.859375 ; top5 ->  86.859375  and loss:  265.69465708732605
test acc: top1 ->  70.49 ; top5 ->  89.54  and loss:  960.2757153511047
forward train acc: top1 ->  68.5703125 ; top5 ->  87.265625  and loss:  262.2321906685829
test acc: top1 ->  70.632 ; top5 ->  89.622  and loss:  959.2549664974213
forward train acc: top1 ->  69.109375 ; top5 ->  86.4609375  and loss:  267.14162796735764
test acc: top1 ->  70.63 ; top5 ->  89.624  and loss:  960.4305956959724
forward train acc: top1 ->  67.875 ; top5 ->  86.5234375  and loss:  273.49021995067596
test acc: top1 ->  70.644 ; top5 ->  89.576  and loss:  958.879164814949
forward train acc: top1 ->  68.6640625 ; top5 ->  87.34375  and loss:  262.3645725250244
test acc: top1 ->  70.636 ; top5 ->  89.614  and loss:  960.097321331501
forward train acc: top1 ->  68.4375 ; top5 ->  87.2734375  and loss:  262.0215629339218
test acc: top1 ->  70.614 ; top5 ->  89.64  and loss:  956.893778860569
>>>>>>> reverse layer  18  since performance drop >>>>>>>
==> this epoch:  235 / 256 , inc:  1
---------------- start layer  19  ---------------
adv train loss:  -65.78489190340042 , diff:  65.78489190340042
adv train loss:  -67.64708685874939 , diff:  1.8621949553489685
layer  19  adv train finish, try to retain  173
test acc: top1 ->  69.88 ; top5 ->  89.192  and loss:  979.4569026231766
forward train acc: top1 ->  68.78125 ; top5 ->  86.40625  and loss:  271.3489256501198
test acc: top1 ->  70.086 ; top5 ->  89.334  and loss:  972.4578576683998
forward train acc: top1 ->  67.875 ; top5 ->  86.828125  and loss:  269.0230683684349
test acc: top1 ->  70.202 ; top5 ->  89.31  and loss:  972.905753493309
forward train acc: top1 ->  68.328125 ; top5 ->  86.3515625  and loss:  267.3243885040283
test acc: top1 ->  70.144 ; top5 ->  89.36  and loss:  970.5349682569504
forward train acc: top1 ->  68.421875 ; top5 ->  86.6171875  and loss:  269.70026183128357
test acc: top1 ->  70.274 ; top5 ->  89.396  and loss:  967.5778566002846
forward train acc: top1 ->  68.4453125 ; top5 ->  86.6953125  and loss:  266.3000866174698
test acc: top1 ->  70.21 ; top5 ->  89.444  and loss:  966.4820358753204
forward train acc: top1 ->  68.359375 ; top5 ->  87.2109375  and loss:  262.8117924928665
test acc: top1 ->  70.368 ; top5 ->  89.458  and loss:  963.823371887207
forward train acc: top1 ->  67.96875 ; top5 ->  87.2109375  and loss:  265.407521545887
test acc: top1 ->  70.396 ; top5 ->  89.486  and loss:  963.5263113975525
forward train acc: top1 ->  68.1328125 ; top5 ->  86.5625  and loss:  271.7960591316223
test acc: top1 ->  70.362 ; top5 ->  89.516  and loss:  962.5911914110184
forward train acc: top1 ->  68.0078125 ; top5 ->  86.796875  and loss:  270.79862731695175
test acc: top1 ->  70.298 ; top5 ->  89.486  and loss:  963.9027444720268
forward train acc: top1 ->  68.6796875 ; top5 ->  87.09375  and loss:  262.1007897257805
test acc: top1 ->  70.47 ; top5 ->  89.484  and loss:  962.4210356473923
>>>>>>> reverse layer  19  since performance drop >>>>>>>
==> this epoch:  179 / 256 , inc:  1
---------------- start layer  20  ---------------
adv train loss:  -59.901107370853424 , diff:  59.901107370853424
adv train loss:  -66.9502524137497 , diff:  7.049145042896271
layer  20  adv train finish, try to retain  216
test acc: top1 ->  70.542 ; top5 ->  89.528  and loss:  962.514082968235
forward train acc: top1 ->  68.1328125 ; top5 ->  86.9453125  and loss:  267.3391743302345
test acc: top1 ->  70.464 ; top5 ->  89.554  and loss:  960.7242961525917
forward train acc: top1 ->  67.984375 ; top5 ->  86.8671875  and loss:  265.7418274283409
test acc: top1 ->  70.524 ; top5 ->  89.524  and loss:  961.8037267923355
forward train acc: top1 ->  68.3984375 ; top5 ->  86.78125  and loss:  266.93437898159027
test acc: top1 ->  70.384 ; top5 ->  89.508  and loss:  960.7326856851578
forward train acc: top1 ->  68.3515625 ; top5 ->  86.921875  and loss:  266.00393509864807
test acc: top1 ->  70.424 ; top5 ->  89.516  and loss:  960.2386993765831
forward train acc: top1 ->  68.328125 ; top5 ->  86.8125  and loss:  265.4189608693123
test acc: top1 ->  70.51 ; top5 ->  89.54  and loss:  959.8085076212883
forward train acc: top1 ->  68.8046875 ; top5 ->  87.015625  and loss:  266.96860790252686
test acc: top1 ->  70.506 ; top5 ->  89.584  and loss:  960.006926715374
forward train acc: top1 ->  67.671875 ; top5 ->  87.0390625  and loss:  266.20375061035156
test acc: top1 ->  70.574 ; top5 ->  89.558  and loss:  958.6211925148964
forward train acc: top1 ->  68.3203125 ; top5 ->  86.6171875  and loss:  265.32926964759827
test acc: top1 ->  70.586 ; top5 ->  89.542  and loss:  957.4158178567886
forward train acc: top1 ->  69.3828125 ; top5 ->  87.46875  and loss:  255.11504298448563
test acc: top1 ->  70.638 ; top5 ->  89.586  and loss:  955.3941624164581
forward train acc: top1 ->  69.015625 ; top5 ->  87.109375  and loss:  262.12865883111954
test acc: top1 ->  70.588 ; top5 ->  89.614  and loss:  955.9961614608765
>>>>>>> reverse layer  20  since performance drop >>>>>>>
==> this epoch:  225 / 256 , inc:  1
---------------- start layer  21  ---------------
### skip layer  21 wait:  3  ###
---------------- start layer  22  ---------------
### skip layer  22 wait:  4  ###
---------------- start layer  23  ---------------
adv train loss:  -66.0656858086586 , diff:  66.0656858086586
adv train loss:  -65.43657994270325 , diff:  0.6291058659553528
layer  23  adv train finish, try to retain  161
test acc: top1 ->  70.15 ; top5 ->  89.282  and loss:  976.5847986340523
forward train acc: top1 ->  68.59375 ; top5 ->  87.09375  and loss:  261.80344158411026
test acc: top1 ->  70.266 ; top5 ->  89.424  and loss:  973.078654229641
forward train acc: top1 ->  67.4375 ; top5 ->  86.5546875  and loss:  270.0026223063469
test acc: top1 ->  70.274 ; top5 ->  89.348  and loss:  973.7539836168289
forward train acc: top1 ->  67.921875 ; top5 ->  86.6953125  and loss:  271.9644031524658
test acc: top1 ->  70.382 ; top5 ->  89.474  and loss:  967.1569694876671
forward train acc: top1 ->  68.515625 ; top5 ->  86.7421875  and loss:  268.96942657232285
test acc: top1 ->  70.402 ; top5 ->  89.47  and loss:  969.4074466228485
forward train acc: top1 ->  68.25 ; top5 ->  86.5234375  and loss:  264.85570138692856
test acc: top1 ->  70.438 ; top5 ->  89.48  and loss:  965.4044027924538
forward train acc: top1 ->  68.640625 ; top5 ->  87.3125  and loss:  264.15184396505356
test acc: top1 ->  70.486 ; top5 ->  89.532  and loss:  963.7565346360207
forward train acc: top1 ->  69.0078125 ; top5 ->  87.0390625  and loss:  261.74872875213623
test acc: top1 ->  70.448 ; top5 ->  89.512  and loss:  964.1953971385956
forward train acc: top1 ->  68.0859375 ; top5 ->  86.859375  and loss:  269.5909901857376
test acc: top1 ->  70.486 ; top5 ->  89.488  and loss:  963.933203458786
forward train acc: top1 ->  68.734375 ; top5 ->  87.171875  and loss:  260.55533134937286
test acc: top1 ->  70.466 ; top5 ->  89.534  and loss:  962.505466401577
forward train acc: top1 ->  68.3359375 ; top5 ->  86.90625  and loss:  267.17445933818817
test acc: top1 ->  70.482 ; top5 ->  89.544  and loss:  962.0630270242691
>>>>>>> reverse layer  23  since performance drop >>>>>>>
==> this epoch:  187 / 256 , inc:  1
---------------- start layer  24  ---------------
### skip layer  24 wait:  4  ###
---------------- start layer  25  ---------------
### skip layer  25 wait:  4  ###
---------------- start layer  26  ---------------
### skip layer  26 wait:  3  ###
---------------- start layer  27  ---------------
### skip layer  27 wait:  3  ###
---------------- start layer  28  ---------------
### skip layer  28 wait:  3  ###
---------------- start layer  29  ---------------
### skip layer  29 wait:  3  ###
---------------- start layer  30  ---------------
### skip layer  30 wait:  3  ###
---------------- start layer  31  ---------------
### skip layer  31 wait:  4  ###
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.0675, 0.18, 0.00059326171875, 0.01265625, 0.0253125, 0.18, 0.00059326171875, 0.00011123657226562501, 0.006328125, 0.0011865234375, 0.01265625, 0.0011865234375, 0.0031640625, 0.09, 5.561828613281251e-05, 0.0001483154296875, 0.000296630859375, 0.00059326171875, 0.00059326171875, 0.006328125, 0.00059326171875, 0.0001483154296875, 0.0031640625, 0.006328125, 0.000296630859375, 0.000296630859375, 7.415771484375e-05, 7.415771484375e-05, 7.415771484375e-05, 7.415771484375e-05, 7.415771484375e-05, 0.0001483154296875]  wait [2, 0, 2, 3, 4, 0, 3, 4, 3, 4, 4, 4, 2, 0, 4, 2, 3, 4, 4, 4, 4, 2, 3, 4, 3, 3, 2, 2, 2, 2, 2, 3]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 2
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  13  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -67.27670633792877 , diff:  67.27670633792877
adv train loss:  -64.67435336112976 , diff:  2.6023529767990112
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  36
test acc: top1 ->  62.554 ; top5 ->  84.076  and loss:  1243.2625840902328
forward train acc: top1 ->  67.4921875 ; top5 ->  86.7890625  and loss:  269.1259535551071
test acc: top1 ->  70.08 ; top5 ->  89.338  and loss:  972.9085301756859
forward train acc: top1 ->  68.1640625 ; top5 ->  86.8828125  and loss:  268.9637328386307
test acc: top1 ->  70.234 ; top5 ->  89.37  and loss:  972.660923242569
forward train acc: top1 ->  68.4140625 ; top5 ->  86.78125  and loss:  267.4584729671478
test acc: top1 ->  70.212 ; top5 ->  89.484  and loss:  970.1383120417595
forward train acc: top1 ->  68.484375 ; top5 ->  86.6484375  and loss:  268.8327531218529
test acc: top1 ->  70.328 ; top5 ->  89.486  and loss:  970.0166889429092
forward train acc: top1 ->  68.171875 ; top5 ->  86.578125  and loss:  267.9377734065056
test acc: top1 ->  70.288 ; top5 ->  89.41  and loss:  967.1815809011459
forward train acc: top1 ->  68.875 ; top5 ->  87.015625  and loss:  261.7206190228462
test acc: top1 ->  70.46 ; top5 ->  89.478  and loss:  966.7213199138641
forward train acc: top1 ->  68.53125 ; top5 ->  87.0234375  and loss:  263.3534126877785
test acc: top1 ->  70.364 ; top5 ->  89.522  and loss:  964.7095514535904
forward train acc: top1 ->  68.171875 ; top5 ->  86.9140625  and loss:  266.3145041465759
test acc: top1 ->  70.406 ; top5 ->  89.514  and loss:  963.2764604687691
forward train acc: top1 ->  68.796875 ; top5 ->  87.515625  and loss:  263.35413855314255
test acc: top1 ->  70.408 ; top5 ->  89.55  and loss:  963.3605108261108
forward train acc: top1 ->  68.828125 ; top5 ->  86.6484375  and loss:  263.30860966444016
test acc: top1 ->  70.482 ; top5 ->  89.538  and loss:  962.6650260090828
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  37 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -67.27628690004349 , diff:  67.27628690004349
adv train loss:  -71.91125375032425 , diff:  4.634966850280762
layer  1  adv train finish, try to retain  38
test acc: top1 ->  61.742 ; top5 ->  83.332  and loss:  1284.6848766207695
forward train acc: top1 ->  66.859375 ; top5 ->  85.75  and loss:  282.9280542731285
test acc: top1 ->  69.646 ; top5 ->  89.052  and loss:  989.0966684818268
forward train acc: top1 ->  68.0390625 ; top5 ->  87.0703125  and loss:  268.64351385831833
test acc: top1 ->  70.02 ; top5 ->  89.346  and loss:  979.7050404548645
forward train acc: top1 ->  68.1015625 ; top5 ->  86.4453125  and loss:  268.71375316381454
test acc: top1 ->  69.992 ; top5 ->  89.328  and loss:  978.1384562849998
forward train acc: top1 ->  68.015625 ; top5 ->  86.6875  and loss:  267.3660727739334
test acc: top1 ->  70.164 ; top5 ->  89.418  and loss:  972.9634414315224
forward train acc: top1 ->  67.765625 ; top5 ->  86.1796875  and loss:  275.52841317653656
test acc: top1 ->  70.036 ; top5 ->  89.388  and loss:  972.8182443976402
forward train acc: top1 ->  68.4296875 ; top5 ->  86.640625  and loss:  269.6572768688202
test acc: top1 ->  70.112 ; top5 ->  89.456  and loss:  972.6162678599358
forward train acc: top1 ->  67.7578125 ; top5 ->  86.6640625  and loss:  269.98864060640335
test acc: top1 ->  70.138 ; top5 ->  89.494  and loss:  968.3400402069092
forward train acc: top1 ->  68.0625 ; top5 ->  86.84375  and loss:  267.50805139541626
test acc: top1 ->  70.13 ; top5 ->  89.47  and loss:  970.8176057934761
forward train acc: top1 ->  68.3125 ; top5 ->  87.0703125  and loss:  265.81476867198944
test acc: top1 ->  70.22 ; top5 ->  89.484  and loss:  969.1960948705673
forward train acc: top1 ->  68.1796875 ; top5 ->  86.8671875  and loss:  268.1784569621086
test acc: top1 ->  70.286 ; top5 ->  89.478  and loss:  967.9981640577316
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  44 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -69.45101290941238 , diff:  69.45101290941238
adv train loss:  -67.23611986637115 , diff:  2.2148930430412292
layer  2  adv train finish, try to retain  61
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
adv train loss:  -69.34842777252197 , diff:  69.34842777252197
adv train loss:  -67.96304094791412 , diff:  1.3853868246078491
layer  5  adv train finish, try to retain  31
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
### skip layer  6 wait:  3  ###
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
### skip layer  8 wait:  3  ###
---------------- start layer  9  ---------------
### skip layer  9 wait:  4  ###
---------------- start layer  10  ---------------
### skip layer  10 wait:  4  ###
---------------- start layer  11  ---------------
### skip layer  11 wait:  4  ###
---------------- start layer  12  ---------------
adv train loss:  -71.49293166399002 , diff:  71.49293166399002
adv train loss:  -63.74466687440872 , diff:  7.748264789581299
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  97
test acc: top1 ->  68.798 ; top5 ->  88.532  and loss:  1022.1152307987213
forward train acc: top1 ->  68.09375 ; top5 ->  86.9453125  and loss:  267.25456446409225
test acc: top1 ->  70.002 ; top5 ->  89.26  and loss:  975.0176439881325
forward train acc: top1 ->  68.4375 ; top5 ->  86.59375  and loss:  265.41802495718
test acc: top1 ->  70.146 ; top5 ->  89.446  and loss:  967.8471891880035
forward train acc: top1 ->  68.6171875 ; top5 ->  87.203125  and loss:  261.2834607362747
test acc: top1 ->  70.298 ; top5 ->  89.416  and loss:  969.2004492282867
forward train acc: top1 ->  68.421875 ; top5 ->  86.703125  and loss:  264.28473913669586
test acc: top1 ->  70.248 ; top5 ->  89.526  and loss:  966.0510854125023
forward train acc: top1 ->  68.703125 ; top5 ->  87.2890625  and loss:  263.4319526553154
test acc: top1 ->  70.266 ; top5 ->  89.538  and loss:  965.3471233844757
forward train acc: top1 ->  68.2109375 ; top5 ->  86.8984375  and loss:  267.8774154782295
test acc: top1 ->  70.354 ; top5 ->  89.596  and loss:  965.0067002773285
forward train acc: top1 ->  68.9296875 ; top5 ->  87.09375  and loss:  264.533649623394
test acc: top1 ->  70.362 ; top5 ->  89.572  and loss:  963.7335218191147
forward train acc: top1 ->  68.953125 ; top5 ->  87.0546875  and loss:  262.94159454107285
test acc: top1 ->  70.472 ; top5 ->  89.492  and loss:  963.0061677694321
forward train acc: top1 ->  69.2890625 ; top5 ->  87.0546875  and loss:  261.0582151412964
test acc: top1 ->  70.474 ; top5 ->  89.548  and loss:  964.7398805618286
forward train acc: top1 ->  68.6484375 ; top5 ->  86.828125  and loss:  264.0843189358711
test acc: top1 ->  70.454 ; top5 ->  89.588  and loss:  960.6036767363548
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  98 / 128 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -65.68056225776672 , diff:  65.68056225776672
adv train loss:  -67.41520875692368 , diff:  1.734646499156952
layer  13  adv train finish, try to retain  64
test acc: top1 ->  66.794 ; top5 ->  87.256  and loss:  1088.4245620965958
forward train acc: top1 ->  67.328125 ; top5 ->  86.5234375  and loss:  274.4651479125023
test acc: top1 ->  69.988 ; top5 ->  89.24  and loss:  979.0396394729614
forward train acc: top1 ->  68.765625 ; top5 ->  86.921875  and loss:  264.8634307384491
test acc: top1 ->  69.934 ; top5 ->  89.26  and loss:  976.4635057449341
forward train acc: top1 ->  67.8671875 ; top5 ->  86.2734375  and loss:  272.2882254719734
test acc: top1 ->  70.144 ; top5 ->  89.234  and loss:  977.6741279363632
forward train acc: top1 ->  68.3203125 ; top5 ->  86.3984375  and loss:  269.6562821865082
test acc: top1 ->  70.036 ; top5 ->  89.266  and loss:  976.9168117642403
forward train acc: top1 ->  69.15625 ; top5 ->  87.2265625  and loss:  263.6922736763954
test acc: top1 ->  70.266 ; top5 ->  89.364  and loss:  970.8186436891556
forward train acc: top1 ->  67.96875 ; top5 ->  86.6171875  and loss:  270.7958365082741
test acc: top1 ->  70.168 ; top5 ->  89.308  and loss:  972.6970152258873
forward train acc: top1 ->  68.734375 ; top5 ->  86.6796875  and loss:  266.6190700531006
test acc: top1 ->  70.218 ; top5 ->  89.292  and loss:  969.8017839193344
forward train acc: top1 ->  67.9765625 ; top5 ->  86.7265625  and loss:  267.6212629079819
test acc: top1 ->  70.264 ; top5 ->  89.344  and loss:  969.2060520648956
forward train acc: top1 ->  67.984375 ; top5 ->  87.1328125  and loss:  269.12016904354095
test acc: top1 ->  70.214 ; top5 ->  89.338  and loss:  970.5198757648468
forward train acc: top1 ->  68.1953125 ; top5 ->  86.9765625  and loss:  266.00023221969604
test acc: top1 ->  70.278 ; top5 ->  89.352  and loss:  967.9815143942833
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  71 / 128 , inc:  1
---------------- start layer  14  ---------------
### skip layer  14 wait:  4  ###
---------------- start layer  15  ---------------
adv train loss:  -67.2038100361824 , diff:  67.2038100361824
adv train loss:  -66.71771985292435 , diff:  0.48609018325805664
************ all values are small in this layer **********
layer  15  adv train finish, try to retain  247
test acc: top1 ->  69.754 ; top5 ->  89.264  and loss:  982.8894677758217
forward train acc: top1 ->  68.71875 ; top5 ->  87.2109375  and loss:  260.2469462752342
test acc: top1 ->  70.304 ; top5 ->  89.492  and loss:  972.2388482093811
forward train acc: top1 ->  68.5859375 ; top5 ->  86.78125  and loss:  267.4386071562767
test acc: top1 ->  70.372 ; top5 ->  89.576  and loss:  966.403279364109
forward train acc: top1 ->  68.59375 ; top5 ->  87.21875  and loss:  265.7275317311287
test acc: top1 ->  70.368 ; top5 ->  89.626  and loss:  965.9128093719482
forward train acc: top1 ->  68.8125 ; top5 ->  87.015625  and loss:  263.96274799108505
test acc: top1 ->  70.452 ; top5 ->  89.572  and loss:  963.7100936770439
forward train acc: top1 ->  68.4375 ; top5 ->  87.0  and loss:  267.1110830307007
test acc: top1 ->  70.458 ; top5 ->  89.578  and loss:  963.1842619180679
forward train acc: top1 ->  68.0625 ; top5 ->  86.578125  and loss:  268.4737174510956
test acc: top1 ->  70.476 ; top5 ->  89.612  and loss:  959.3273972272873
forward train acc: top1 ->  69.1875 ; top5 ->  86.875  and loss:  265.619949221611
test acc: top1 ->  70.512 ; top5 ->  89.608  and loss:  959.7160604000092
forward train acc: top1 ->  68.09375 ; top5 ->  86.7890625  and loss:  267.495092689991
test acc: top1 ->  70.484 ; top5 ->  89.648  and loss:  960.2581711411476
forward train acc: top1 ->  69.2109375 ; top5 ->  87.078125  and loss:  260.7022137641907
test acc: top1 ->  70.592 ; top5 ->  89.658  and loss:  958.8743115663528
forward train acc: top1 ->  68.5625 ; top5 ->  87.203125  and loss:  264.4067084789276
test acc: top1 ->  70.55 ; top5 ->  89.594  and loss:  959.9513206481934
>>>>>>> reverse layer  15  since performance drop >>>>>>>
==> this epoch:  248 / 256 , inc:  1
---------------- start layer  16  ---------------
### skip layer  16 wait:  3  ###
---------------- start layer  17  ---------------
### skip layer  17 wait:  4  ###
---------------- start layer  18  ---------------
### skip layer  18 wait:  4  ###
---------------- start layer  19  ---------------
### skip layer  19 wait:  4  ###
---------------- start layer  20  ---------------
### skip layer  20 wait:  4  ###
---------------- start layer  21  ---------------
adv train loss:  -68.48337078094482 , diff:  68.48337078094482
adv train loss:  -65.73067271709442 , diff:  2.752698063850403
************ all values are small in this layer **********
layer  21  adv train finish, try to retain  241
test acc: top1 ->  70.422 ; top5 ->  89.578  and loss:  961.841939508915
forward train acc: top1 ->  69.25 ; top5 ->  87.78125  and loss:  256.6718366742134
test acc: top1 ->  70.64 ; top5 ->  89.674  and loss:  959.2192642688751
forward train acc: top1 ->  69.171875 ; top5 ->  87.140625  and loss:  261.75059056282043
test acc: top1 ->  70.566 ; top5 ->  89.612  and loss:  960.9867276549339
forward train acc: top1 ->  68.484375 ; top5 ->  86.9453125  and loss:  268.9412027001381
test acc: top1 ->  70.44 ; top5 ->  89.62  and loss:  960.2774805426598
forward train acc: top1 ->  68.921875 ; top5 ->  86.8125  and loss:  264.3303437232971
test acc: top1 ->  70.578 ; top5 ->  89.642  and loss:  958.6065844893456
forward train acc: top1 ->  68.9765625 ; top5 ->  87.1171875  and loss:  261.83896791934967
test acc: top1 ->  70.642 ; top5 ->  89.688  and loss:  955.9406235814095
forward train acc: top1 ->  69.0234375 ; top5 ->  87.4375  and loss:  258.7661814689636
test acc: top1 ->  70.596 ; top5 ->  89.678  and loss:  957.15033441782
forward train acc: top1 ->  68.5 ; top5 ->  86.9609375  and loss:  263.6568964123726
test acc: top1 ->  70.558 ; top5 ->  89.642  and loss:  956.0319830775261
forward train acc: top1 ->  68.7421875 ; top5 ->  87.0625  and loss:  262.3928470611572
test acc: top1 ->  70.63 ; top5 ->  89.664  and loss:  955.8579108715057
forward train acc: top1 ->  69.7265625 ; top5 ->  87.3046875  and loss:  260.074584543705
test acc: top1 ->  70.722 ; top5 ->  89.732  and loss:  955.3297687768936
forward train acc: top1 ->  69.2109375 ; top5 ->  87.375  and loss:  259.60225600004196
test acc: top1 ->  70.624 ; top5 ->  89.676  and loss:  956.958318054676
>>>>>>> reverse layer  21  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  22  ---------------
### skip layer  22 wait:  3  ###
---------------- start layer  23  ---------------
### skip layer  23 wait:  4  ###
---------------- start layer  24  ---------------
### skip layer  24 wait:  3  ###
---------------- start layer  25  ---------------
### skip layer  25 wait:  3  ###
---------------- start layer  26  ---------------
adv train loss:  -67.77436292171478 , diff:  67.77436292171478
adv train loss:  -67.1955434679985 , diff:  0.5788194537162781
************ all values are small in this layer **********
layer  26  adv train finish, try to retain  501
test acc: top1 ->  70.426 ; top5 ->  89.604  and loss:  959.245098888874
forward train acc: top1 ->  68.609375 ; top5 ->  87.234375  and loss:  262.67706257104874
test acc: top1 ->  70.446 ; top5 ->  89.62  and loss:  962.616946876049
forward train acc: top1 ->  68.890625 ; top5 ->  87.21875  and loss:  259.96782064437866
test acc: top1 ->  70.466 ; top5 ->  89.626  and loss:  960.1619056463242
forward train acc: top1 ->  68.6328125 ; top5 ->  87.2890625  and loss:  265.211559176445
test acc: top1 ->  70.364 ; top5 ->  89.518  and loss:  964.3301695585251
forward train acc: top1 ->  68.8203125 ; top5 ->  87.2421875  and loss:  257.3879489302635
test acc: top1 ->  70.596 ; top5 ->  89.644  and loss:  959.7833057045937
forward train acc: top1 ->  69.21875 ; top5 ->  87.2109375  and loss:  261.41416096687317
test acc: top1 ->  70.552 ; top5 ->  89.554  and loss:  961.0359324216843
forward train acc: top1 ->  68.9140625 ; top5 ->  86.7734375  and loss:  262.5217616558075
test acc: top1 ->  70.558 ; top5 ->  89.626  and loss:  958.3763880133629
forward train acc: top1 ->  67.890625 ; top5 ->  86.6640625  and loss:  269.79752653837204
test acc: top1 ->  70.572 ; top5 ->  89.576  and loss:  957.1178237199783
forward train acc: top1 ->  69.484375 ; top5 ->  87.2421875  and loss:  259.17494863271713
test acc: top1 ->  70.546 ; top5 ->  89.672  and loss:  956.7676232457161
forward train acc: top1 ->  69.7421875 ; top5 ->  87.28125  and loss:  256.8137083053589
test acc: top1 ->  70.702 ; top5 ->  89.682  and loss:  955.905791580677
forward train acc: top1 ->  68.4375 ; top5 ->  86.65625  and loss:  266.06639951467514
test acc: top1 ->  70.694 ; top5 ->  89.668  and loss:  956.5528138279915
>>>>>>> reverse layer  26  since performance drop >>>>>>>
==> this epoch:  502 / 512 , inc:  1
---------------- start layer  27  ---------------
adv train loss:  -64.66315126419067 , diff:  64.66315126419067
adv train loss:  -67.69894796609879 , diff:  3.0357967019081116
************ all values are small in this layer **********
layer  27  adv train finish, try to retain  504
test acc: top1 ->  70.54 ; top5 ->  89.708  and loss:  961.0408622026443
forward train acc: top1 ->  68.2265625 ; top5 ->  86.59375  and loss:  270.0525884628296
test acc: top1 ->  70.63 ; top5 ->  89.654  and loss:  960.45853972435
forward train acc: top1 ->  69.21875 ; top5 ->  87.3828125  and loss:  261.0148446559906
test acc: top1 ->  70.492 ; top5 ->  89.668  and loss:  961.9598103761673
forward train acc: top1 ->  68.0625 ; top5 ->  86.9609375  and loss:  264.33117628097534
test acc: top1 ->  70.414 ; top5 ->  89.634  and loss:  961.5203384757042
forward train acc: top1 ->  68.5078125 ; top5 ->  86.7734375  and loss:  265.1240732073784
test acc: top1 ->  70.494 ; top5 ->  89.668  and loss:  962.8134984374046
forward train acc: top1 ->  68.3828125 ; top5 ->  86.734375  and loss:  267.0724233984947
test acc: top1 ->  70.604 ; top5 ->  89.742  and loss:  956.2156763076782
forward train acc: top1 ->  68.046875 ; top5 ->  86.7890625  and loss:  267.7795653939247
test acc: top1 ->  70.562 ; top5 ->  89.766  and loss:  957.6290273070335
forward train acc: top1 ->  69.6796875 ; top5 ->  87.734375  and loss:  254.943778693676
test acc: top1 ->  70.564 ; top5 ->  89.74  and loss:  956.0343882441521
forward train acc: top1 ->  68.8359375 ; top5 ->  87.1875  and loss:  261.3874745965004
test acc: top1 ->  70.674 ; top5 ->  89.764  and loss:  954.3053780794144
forward train acc: top1 ->  68.9609375 ; top5 ->  87.4453125  and loss:  257.31536746025085
test acc: top1 ->  70.55 ; top5 ->  89.756  and loss:  954.6799192428589
forward train acc: top1 ->  68.9453125 ; top5 ->  86.7265625  and loss:  266.15345883369446
test acc: top1 ->  70.624 ; top5 ->  89.786  and loss:  952.9466097950935
>>>>>>> reverse layer  27  since performance drop >>>>>>>
==> this epoch:  505 / 512 , inc:  1
---------------- start layer  28  ---------------
adv train loss:  -67.50117552280426 , diff:  67.50117552280426
adv train loss:  -64.41014248132706 , diff:  3.0910330414772034
************ all values are small in this layer **********
layer  28  adv train finish, try to retain  502
test acc: top1 ->  70.744 ; top5 ->  89.758  and loss:  956.2366381287575
forward train acc: top1 ->  68.4140625 ; top5 ->  86.6484375  and loss:  267.4846560359001
test acc: top1 ->  70.552 ; top5 ->  89.77  and loss:  957.5244255661964
forward train acc: top1 ->  68.5234375 ; top5 ->  87.0546875  and loss:  261.1672440767288
test acc: top1 ->  70.63 ; top5 ->  89.624  and loss:  956.8108909726143
forward train acc: top1 ->  68.5 ; top5 ->  87.40625  and loss:  260.5262026190758
test acc: top1 ->  70.54 ; top5 ->  89.684  and loss:  955.0604344010353
forward train acc: top1 ->  69.0546875 ; top5 ->  86.984375  and loss:  263.03758430480957
test acc: top1 ->  70.664 ; top5 ->  89.74  and loss:  955.4109101295471
forward train acc: top1 ->  68.6484375 ; top5 ->  86.6953125  and loss:  262.94838082790375
test acc: top1 ->  70.682 ; top5 ->  89.732  and loss:  954.021385550499
forward train acc: top1 ->  69.21875 ; top5 ->  87.046875  and loss:  261.79264682531357
test acc: top1 ->  70.648 ; top5 ->  89.772  and loss:  954.2868985533714
forward train acc: top1 ->  69.0546875 ; top5 ->  87.2578125  and loss:  259.99947971105576
test acc: top1 ->  70.722 ; top5 ->  89.744  and loss:  953.277923643589
forward train acc: top1 ->  68.640625 ; top5 ->  87.3984375  and loss:  261.61301958560944
test acc: top1 ->  70.692 ; top5 ->  89.746  and loss:  953.512138068676
forward train acc: top1 ->  68.9921875 ; top5 ->  87.359375  and loss:  261.4291386604309
test acc: top1 ->  70.682 ; top5 ->  89.682  and loss:  953.9133098125458
forward train acc: top1 ->  68.859375 ; top5 ->  86.8984375  and loss:  263.00712525844574
test acc: top1 ->  70.672 ; top5 ->  89.784  and loss:  952.9708575606346
>>>>>>> reverse layer  28  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  29  ---------------
adv train loss:  -65.1336658000946 , diff:  65.1336658000946
adv train loss:  -66.36344462633133 , diff:  1.2297788262367249
************ all values are small in this layer **********
layer  29  adv train finish, try to retain  497
test acc: top1 ->  70.708 ; top5 ->  89.702  and loss:  954.6572217345238
forward train acc: top1 ->  69.09375 ; top5 ->  87.4609375  and loss:  257.75610184669495
test acc: top1 ->  70.592 ; top5 ->  89.656  and loss:  958.0070525407791
forward train acc: top1 ->  68.015625 ; top5 ->  86.890625  and loss:  267.76427298784256
test acc: top1 ->  70.636 ; top5 ->  89.684  and loss:  958.0567500591278
forward train acc: top1 ->  68.921875 ; top5 ->  87.484375  and loss:  259.95296347141266
test acc: top1 ->  70.696 ; top5 ->  89.716  and loss:  955.2131152749062
forward train acc: top1 ->  68.8125 ; top5 ->  87.4140625  and loss:  261.9628304243088
test acc: top1 ->  70.58 ; top5 ->  89.732  and loss:  952.7533128261566
forward train acc: top1 ->  68.8515625 ; top5 ->  87.1015625  and loss:  261.7803440093994
test acc: top1 ->  70.698 ; top5 ->  89.694  and loss:  956.368014395237
forward train acc: top1 ->  68.71875 ; top5 ->  87.0703125  and loss:  264.3943910598755
test acc: top1 ->  70.668 ; top5 ->  89.704  and loss:  953.0609936118126
forward train acc: top1 ->  69.265625 ; top5 ->  86.96875  and loss:  261.23838555812836
test acc: top1 ->  70.624 ; top5 ->  89.756  and loss:  953.3506789803505
forward train acc: top1 ->  69.2265625 ; top5 ->  87.6171875  and loss:  257.6547731757164
test acc: top1 ->  70.678 ; top5 ->  89.754  and loss:  954.0778866410255
forward train acc: top1 ->  69.5 ; top5 ->  87.265625  and loss:  256.7334969639778
test acc: top1 ->  70.718 ; top5 ->  89.878  and loss:  948.8023437261581
forward train acc: top1 ->  68.6015625 ; top5 ->  87.078125  and loss:  262.2677000761032
test acc: top1 ->  70.774 ; top5 ->  89.872  and loss:  950.1622557640076
>>>>>>> reverse layer  29  since performance drop >>>>>>>
==> this epoch:  498 / 512 , inc:  1
---------------- start layer  30  ---------------
adv train loss:  -65.07107937335968 , diff:  65.07107937335968
adv train loss:  -64.52086335420609 , diff:  0.550216019153595
************ all values are small in this layer **********
layer  30  adv train finish, try to retain  502
test acc: top1 ->  70.596 ; top5 ->  89.7  and loss:  954.3480129241943
forward train acc: top1 ->  68.671875 ; top5 ->  87.234375  and loss:  259.78330194950104
test acc: top1 ->  70.6 ; top5 ->  89.72  and loss:  958.0026825070381
forward train acc: top1 ->  68.5625 ; top5 ->  86.7734375  and loss:  264.5615664124489
test acc: top1 ->  70.552 ; top5 ->  89.61  and loss:  961.4781318306923
forward train acc: top1 ->  68.9921875 ; top5 ->  87.421875  and loss:  260.90276592969894
test acc: top1 ->  70.456 ; top5 ->  89.668  and loss:  958.0384076833725
forward train acc: top1 ->  68.7734375 ; top5 ->  87.1484375  and loss:  260.3915601372719
test acc: top1 ->  70.58 ; top5 ->  89.714  and loss:  956.7817772626877
forward train acc: top1 ->  68.8046875 ; top5 ->  87.3984375  and loss:  260.6916806101799
test acc: top1 ->  70.754 ; top5 ->  89.724  and loss:  954.8569880723953
forward train acc: top1 ->  69.1171875 ; top5 ->  87.515625  and loss:  257.1907272338867
test acc: top1 ->  70.694 ; top5 ->  89.732  and loss:  955.7346147894859
forward train acc: top1 ->  69.2421875 ; top5 ->  87.0703125  and loss:  260.9296278357506
test acc: top1 ->  70.756 ; top5 ->  89.738  and loss:  956.9880954623222
forward train acc: top1 ->  69.1640625 ; top5 ->  87.40625  and loss:  259.23020058870316
test acc: top1 ->  70.796 ; top5 ->  89.742  and loss:  954.4043355584145
forward train acc: top1 ->  69.265625 ; top5 ->  87.3203125  and loss:  260.57498240470886
test acc: top1 ->  70.82 ; top5 ->  89.788  and loss:  952.3944175839424
forward train acc: top1 ->  69.2265625 ; top5 ->  87.265625  and loss:  258.93378180265427
test acc: top1 ->  70.734 ; top5 ->  89.75  and loss:  955.9069559574127
>>>>>>> reverse layer  30  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  31  ---------------
### skip layer  31 wait:  3  ###
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.050625, 0.135, 0.0011865234375, 0.01265625, 0.0253125, 0.36, 0.00059326171875, 0.00011123657226562501, 0.006328125, 0.0011865234375, 0.01265625, 0.0011865234375, 0.002373046875, 0.0675, 5.561828613281251e-05, 0.00011123657226562501, 0.000296630859375, 0.00059326171875, 0.00059326171875, 0.006328125, 0.00059326171875, 0.00011123657226562501, 0.0031640625, 0.006328125, 0.000296630859375, 0.000296630859375, 5.561828613281251e-05, 5.561828613281251e-05, 5.561828613281251e-05, 5.561828613281251e-05, 5.561828613281251e-05, 0.0001483154296875]  wait [4, 2, 2, 2, 3, 0, 2, 3, 2, 3, 3, 3, 4, 2, 3, 4, 2, 3, 3, 3, 3, 4, 2, 3, 2, 2, 4, 4, 4, 4, 4, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 2
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  14  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
adv train loss:  -69.82949703931808 , diff:  69.82949703931808
adv train loss:  -66.44105195999146 , diff:  3.3884450793266296
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  43
test acc: top1 ->  65.034 ; top5 ->  85.984  and loss:  1159.863759458065
forward train acc: top1 ->  66.9375 ; top5 ->  85.7421875  and loss:  281.9823961853981
test acc: top1 ->  70.228 ; top5 ->  89.352  and loss:  971.6264721155167
forward train acc: top1 ->  68.4140625 ; top5 ->  87.0  and loss:  264.92687368392944
test acc: top1 ->  70.118 ; top5 ->  89.352  and loss:  971.9415919184685
forward train acc: top1 ->  67.734375 ; top5 ->  86.25  and loss:  272.14790481328964
test acc: top1 ->  70.186 ; top5 ->  89.394  and loss:  971.1253369450569
forward train acc: top1 ->  68.6796875 ; top5 ->  87.0625  and loss:  262.4991744160652
test acc: top1 ->  70.45 ; top5 ->  89.426  and loss:  968.2368094325066
forward train acc: top1 ->  68.5078125 ; top5 ->  87.03125  and loss:  263.1105701327324
test acc: top1 ->  70.382 ; top5 ->  89.482  and loss:  965.091523706913
forward train acc: top1 ->  68.234375 ; top5 ->  86.375  and loss:  271.01825124025345
test acc: top1 ->  70.47 ; top5 ->  89.55  and loss:  963.5390846133232
forward train acc: top1 ->  68.1015625 ; top5 ->  86.6484375  and loss:  268.8074577450752
test acc: top1 ->  70.38 ; top5 ->  89.56  and loss:  964.3428828716278
forward train acc: top1 ->  69.0 ; top5 ->  86.8203125  and loss:  263.64539474248886
test acc: top1 ->  70.568 ; top5 ->  89.568  and loss:  961.2386473417282
forward train acc: top1 ->  68.09375 ; top5 ->  86.6640625  and loss:  266.82480627298355
test acc: top1 ->  70.44 ; top5 ->  89.518  and loss:  960.7839076519012
forward train acc: top1 ->  68.1484375 ; top5 ->  86.5546875  and loss:  267.9782235622406
test acc: top1 ->  70.504 ; top5 ->  89.576  and loss:  960.389174759388
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  44 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -66.85268062353134 , diff:  66.85268062353134
adv train loss:  -69.44050711393356 , diff:  2.5878264904022217
layer  2  adv train finish, try to retain  49
test acc: top1 ->  69.176 ; top5 ->  88.974  and loss:  1000.0457142591476
forward train acc: top1 ->  68.8359375 ; top5 ->  86.7890625  and loss:  266.32643961906433
test acc: top1 ->  70.408 ; top5 ->  89.566  and loss:  963.5698997378349
forward train acc: top1 ->  68.2734375 ; top5 ->  87.1953125  and loss:  264.7638643980026
test acc: top1 ->  70.472 ; top5 ->  89.546  and loss:  965.8204262852669
forward train acc: top1 ->  68.578125 ; top5 ->  86.59375  and loss:  266.100117623806
test acc: top1 ->  70.502 ; top5 ->  89.576  and loss:  964.0047894716263
forward train acc: top1 ->  68.578125 ; top5 ->  87.0703125  and loss:  263.08985179662704
test acc: top1 ->  70.536 ; top5 ->  89.614  and loss:  961.428180873394
forward train acc: top1 ->  68.4453125 ; top5 ->  86.5234375  and loss:  269.8240379691124
test acc: top1 ->  70.554 ; top5 ->  89.618  and loss:  959.107586979866
forward train acc: top1 ->  68.4765625 ; top5 ->  87.015625  and loss:  262.78784239292145
test acc: top1 ->  70.49 ; top5 ->  89.628  and loss:  960.1784918904305
forward train acc: top1 ->  68.4296875 ; top5 ->  87.046875  and loss:  264.20208686590195
test acc: top1 ->  70.568 ; top5 ->  89.694  and loss:  958.7398233413696
forward train acc: top1 ->  68.4921875 ; top5 ->  86.65625  and loss:  269.84771716594696
test acc: top1 ->  70.624 ; top5 ->  89.716  and loss:  959.1309840679169
forward train acc: top1 ->  68.6640625 ; top5 ->  87.0546875  and loss:  264.8200217485428
test acc: top1 ->  70.612 ; top5 ->  89.694  and loss:  958.8027681112289
forward train acc: top1 ->  69.2578125 ; top5 ->  87.265625  and loss:  259.3963762521744
test acc: top1 ->  70.624 ; top5 ->  89.654  and loss:  957.0488948822021
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  57 / 64 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -65.95857256650925 , diff:  65.95857256650925
adv train loss:  -65.99925595521927 , diff:  0.04068338871002197
layer  3  adv train finish, try to retain  41
test acc: top1 ->  66.01 ; top5 ->  86.948  and loss:  1109.8828999996185
forward train acc: top1 ->  68.46875 ; top5 ->  86.859375  and loss:  269.2953181862831
test acc: top1 ->  70.084 ; top5 ->  89.372  and loss:  973.7583529949188
forward train acc: top1 ->  68.4453125 ; top5 ->  86.921875  and loss:  266.45558255910873
test acc: top1 ->  70.218 ; top5 ->  89.556  and loss:  971.4622097611427
forward train acc: top1 ->  69.015625 ; top5 ->  86.9375  and loss:  262.1337553858757
test acc: top1 ->  70.322 ; top5 ->  89.522  and loss:  968.3611616492271
forward train acc: top1 ->  69.09375 ; top5 ->  87.0859375  and loss:  261.9523411989212
test acc: top1 ->  70.448 ; top5 ->  89.624  and loss:  964.0550917387009
forward train acc: top1 ->  67.5078125 ; top5 ->  86.59375  and loss:  271.4735875725746
test acc: top1 ->  70.44 ; top5 ->  89.536  and loss:  966.1597313284874
forward train acc: top1 ->  68.7578125 ; top5 ->  87.09375  and loss:  263.1619095802307
test acc: top1 ->  70.382 ; top5 ->  89.556  and loss:  965.6149367690086
forward train acc: top1 ->  67.875 ; top5 ->  86.65625  and loss:  268.8919845223427
test acc: top1 ->  70.65 ; top5 ->  89.692  and loss:  960.5445132851601
forward train acc: top1 ->  68.890625 ; top5 ->  86.8984375  and loss:  264.30115634202957
test acc: top1 ->  70.52 ; top5 ->  89.628  and loss:  961.2442564368248
forward train acc: top1 ->  68.6015625 ; top5 ->  87.40625  and loss:  262.0352082848549
test acc: top1 ->  70.66 ; top5 ->  89.586  and loss:  960.5477750301361
forward train acc: top1 ->  68.6875 ; top5 ->  86.78125  and loss:  264.7669529914856
test acc: top1 ->  70.578 ; top5 ->  89.626  and loss:  961.9768286943436
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  43 / 64 , inc:  1
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
adv train loss:  -66.51666849851608 , diff:  66.51666849851608
adv train loss:  -65.2096009850502 , diff:  1.3070675134658813
layer  5  adv train finish, try to retain  14
test acc: top1 ->  64.0 ; top5 ->  85.528  and loss:  1190.1723642349243
forward train acc: top1 ->  68.4140625 ; top5 ->  87.0703125  and loss:  264.63958752155304
test acc: top1 ->  70.172 ; top5 ->  89.316  and loss:  973.9344841241837
forward train acc: top1 ->  68.46875 ; top5 ->  86.71875  and loss:  266.5331911444664
test acc: top1 ->  70.266 ; top5 ->  89.344  and loss:  967.9818723797798
forward train acc: top1 ->  69.171875 ; top5 ->  87.0078125  and loss:  261.1143653988838
test acc: top1 ->  70.24 ; top5 ->  89.438  and loss:  968.2144013643265
forward train acc: top1 ->  68.1796875 ; top5 ->  86.2890625  and loss:  269.2819076180458
test acc: top1 ->  70.248 ; top5 ->  89.528  and loss:  964.1456555724144
forward train acc: top1 ->  68.8671875 ; top5 ->  87.3046875  and loss:  259.40807169675827
test acc: top1 ->  70.32 ; top5 ->  89.506  and loss:  968.9287824034691
forward train acc: top1 ->  68.28125 ; top5 ->  86.4921875  and loss:  267.70584321022034
test acc: top1 ->  70.408 ; top5 ->  89.568  and loss:  965.4966641068459
forward train acc: top1 ->  69.421875 ; top5 ->  87.46875  and loss:  259.37740594148636
test acc: top1 ->  70.428 ; top5 ->  89.6  and loss:  963.6918296813965
forward train acc: top1 ->  68.9375 ; top5 ->  87.125  and loss:  262.8904678821564
test acc: top1 ->  70.388 ; top5 ->  89.496  and loss:  964.6631284952164
forward train acc: top1 ->  68.9140625 ; top5 ->  87.3203125  and loss:  262.8468043208122
test acc: top1 ->  70.462 ; top5 ->  89.558  and loss:  962.1032212376595
forward train acc: top1 ->  68.703125 ; top5 ->  87.265625  and loss:  265.19255316257477
test acc: top1 ->  70.512 ; top5 ->  89.56  and loss:  961.2393319606781
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  29 / 64 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -67.70727902650833 , diff:  67.70727902650833
adv train loss:  -66.57987529039383 , diff:  1.127403736114502
layer  6  adv train finish, try to retain  114
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
adv train loss:  -65.04141235351562 , diff:  65.04141235351562
adv train loss:  -67.54425543546677 , diff:  2.5028430819511414
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  86
test acc: top1 ->  68.15 ; top5 ->  88.31  and loss:  1036.1592520475388
forward train acc: top1 ->  68.375 ; top5 ->  87.234375  and loss:  265.63431268930435
test acc: top1 ->  70.182 ; top5 ->  89.38  and loss:  969.8915401101112
forward train acc: top1 ->  67.9921875 ; top5 ->  86.5  and loss:  271.4290047287941
test acc: top1 ->  70.246 ; top5 ->  89.444  and loss:  966.9316005110741
forward train acc: top1 ->  68.1953125 ; top5 ->  86.5234375  and loss:  270.5150793790817
test acc: top1 ->  70.2 ; top5 ->  89.406  and loss:  969.9585670828819
forward train acc: top1 ->  67.2734375 ; top5 ->  86.453125  and loss:  274.8922687768936
test acc: top1 ->  70.21 ; top5 ->  89.452  and loss:  963.9232370257378
forward train acc: top1 ->  68.5390625 ; top5 ->  86.75  and loss:  268.0912718176842
test acc: top1 ->  70.318 ; top5 ->  89.476  and loss:  963.7448381185532
forward train acc: top1 ->  68.7265625 ; top5 ->  87.0859375  and loss:  262.0905122756958
test acc: top1 ->  70.298 ; top5 ->  89.536  and loss:  962.600220143795
forward train acc: top1 ->  68.6953125 ; top5 ->  86.78125  and loss:  266.6624965071678
test acc: top1 ->  70.314 ; top5 ->  89.524  and loss:  962.8213814496994
forward train acc: top1 ->  68.9453125 ; top5 ->  86.9296875  and loss:  262.92857921123505
test acc: top1 ->  70.456 ; top5 ->  89.552  and loss:  962.217866897583
forward train acc: top1 ->  69.375 ; top5 ->  87.1484375  and loss:  261.21748185157776
test acc: top1 ->  70.436 ; top5 ->  89.586  and loss:  959.531519472599
forward train acc: top1 ->  68.6953125 ; top5 ->  87.1328125  and loss:  264.34830218553543
test acc: top1 ->  70.44 ; top5 ->  89.566  and loss:  960.3170207738876
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  9  ---------------
### skip layer  9 wait:  3  ###
---------------- start layer  10  ---------------
### skip layer  10 wait:  3  ###
---------------- start layer  11  ---------------
### skip layer  11 wait:  3  ###
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
adv train loss:  -64.0216013789177 , diff:  64.0216013789177
adv train loss:  -66.74827313423157 , diff:  2.7266717553138733
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  70
test acc: top1 ->  67.538 ; top5 ->  87.654  and loss:  1066.664901971817
forward train acc: top1 ->  68.59375 ; top5 ->  86.921875  and loss:  266.5129364132881
test acc: top1 ->  70.084 ; top5 ->  89.32  and loss:  972.5894136428833
forward train acc: top1 ->  68.109375 ; top5 ->  86.5703125  and loss:  270.4045099020004
test acc: top1 ->  70.156 ; top5 ->  89.332  and loss:  971.966848552227
forward train acc: top1 ->  68.9140625 ; top5 ->  87.25  and loss:  262.86363965272903
test acc: top1 ->  70.298 ; top5 ->  89.428  and loss:  968.2803079485893
forward train acc: top1 ->  67.8359375 ; top5 ->  86.5859375  and loss:  270.6652005314827
test acc: top1 ->  70.248 ; top5 ->  89.326  and loss:  969.3869218230247
forward train acc: top1 ->  68.3046875 ; top5 ->  86.6484375  and loss:  269.33358228206635
test acc: top1 ->  70.328 ; top5 ->  89.4  and loss:  968.8929275274277
forward train acc: top1 ->  68.8828125 ; top5 ->  87.234375  and loss:  260.68394988775253
test acc: top1 ->  70.404 ; top5 ->  89.504  and loss:  965.9392343759537
forward train acc: top1 ->  68.65625 ; top5 ->  86.8125  and loss:  266.36773324012756
test acc: top1 ->  70.266 ; top5 ->  89.394  and loss:  965.5098243951797
forward train acc: top1 ->  68.6171875 ; top5 ->  86.546875  and loss:  264.4773077368736
test acc: top1 ->  70.444 ; top5 ->  89.498  and loss:  962.7831330299377
forward train acc: top1 ->  68.796875 ; top5 ->  86.96875  and loss:  266.3985068202019
test acc: top1 ->  70.498 ; top5 ->  89.488  and loss:  965.1297852396965
forward train acc: top1 ->  68.171875 ; top5 ->  87.015625  and loss:  265.5166274905205
test acc: top1 ->  70.524 ; top5 ->  89.474  and loss:  961.0786443352699
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  71 / 128 , inc:  1
---------------- start layer  14  ---------------
### skip layer  14 wait:  3  ###
---------------- start layer  15  ---------------
### skip layer  15 wait:  4  ###
---------------- start layer  16  ---------------
adv train loss:  -66.55308508872986 , diff:  66.55308508872986
adv train loss:  -62.02120119333267 , diff:  4.531883895397186
layer  16  adv train finish, try to retain  243
>>>>>>> reverse layer  16  since no improvement >>>>>>>
---------------- start layer  17  ---------------
### skip layer  17 wait:  3  ###
---------------- start layer  18  ---------------
### skip layer  18 wait:  3  ###
---------------- start layer  19  ---------------
### skip layer  19 wait:  3  ###
---------------- start layer  20  ---------------
### skip layer  20 wait:  3  ###
---------------- start layer  21  ---------------
### skip layer  21 wait:  4  ###
---------------- start layer  22  ---------------
adv train loss:  -65.42824649810791 , diff:  65.42824649810791
adv train loss:  -64.1094970703125 , diff:  1.3187494277954102
************ all values are small in this layer **********
layer  22  adv train finish, try to retain  207
test acc: top1 ->  70.426 ; top5 ->  89.606  and loss:  962.9385828375816
forward train acc: top1 ->  68.4453125 ; top5 ->  86.5625  and loss:  267.3261728286743
test acc: top1 ->  70.536 ; top5 ->  89.644  and loss:  960.6527964472771
forward train acc: top1 ->  68.859375 ; top5 ->  87.140625  and loss:  258.79100239276886
test acc: top1 ->  70.502 ; top5 ->  89.594  and loss:  961.8874251842499
forward train acc: top1 ->  68.71875 ; top5 ->  86.9375  and loss:  263.8142217993736
test acc: top1 ->  70.532 ; top5 ->  89.568  and loss:  958.1086094975471
forward train acc: top1 ->  68.5859375 ; top5 ->  86.7578125  and loss:  266.30350959300995
test acc: top1 ->  70.572 ; top5 ->  89.666  and loss:  957.3442002534866
forward train acc: top1 ->  69.171875 ; top5 ->  87.3125  and loss:  261.9099665284157
test acc: top1 ->  70.614 ; top5 ->  89.664  and loss:  955.9105650186539
forward train acc: top1 ->  68.6171875 ; top5 ->  87.34375  and loss:  259.2176698446274
test acc: top1 ->  70.606 ; top5 ->  89.712  and loss:  954.3622945547104
forward train acc: top1 ->  69.7578125 ; top5 ->  87.8125  and loss:  254.37804758548737
test acc: top1 ->  70.634 ; top5 ->  89.706  and loss:  953.7618992328644
forward train acc: top1 ->  68.6640625 ; top5 ->  87.1328125  and loss:  261.61528420448303
test acc: top1 ->  70.63 ; top5 ->  89.688  and loss:  954.6662796139717
forward train acc: top1 ->  68.7421875 ; top5 ->  86.8046875  and loss:  266.99998754262924
test acc: top1 ->  70.728 ; top5 ->  89.718  and loss:  952.7789723873138
forward train acc: top1 ->  68.328125 ; top5 ->  87.0625  and loss:  267.47334802150726
test acc: top1 ->  70.696 ; top5 ->  89.692  and loss:  953.7315804958344
>>>>>>> reverse layer  22  since performance drop >>>>>>>
==> this epoch:  208 / 256 , inc:  1
---------------- start layer  23  ---------------
### skip layer  23 wait:  3  ###
---------------- start layer  24  ---------------
adv train loss:  -65.10042840242386 , diff:  65.10042840242386
adv train loss:  -62.68615847826004 , diff:  2.4142699241638184
layer  24  adv train finish, try to retain  237
test acc: top1 ->  70.576 ; top5 ->  89.658  and loss:  959.3361125588417
forward train acc: top1 ->  69.359375 ; top5 ->  87.4296875  and loss:  255.98948353528976
test acc: top1 ->  70.63 ; top5 ->  89.732  and loss:  958.6233875155449
forward train acc: top1 ->  68.5703125 ; top5 ->  86.9296875  and loss:  263.2653680443764
test acc: top1 ->  70.628 ; top5 ->  89.682  and loss:  958.0294892787933
forward train acc: top1 ->  69.5859375 ; top5 ->  87.484375  and loss:  256.9378611445427
test acc: top1 ->  70.602 ; top5 ->  89.61  and loss:  958.4622142314911
forward train acc: top1 ->  68.8046875 ; top5 ->  87.1875  and loss:  262.6794266104698
test acc: top1 ->  70.63 ; top5 ->  89.734  and loss:  955.5500917434692
forward train acc: top1 ->  69.1953125 ; top5 ->  87.3671875  and loss:  256.4010081291199
test acc: top1 ->  70.742 ; top5 ->  89.722  and loss:  954.4033030867577
forward train acc: top1 ->  68.828125 ; top5 ->  87.40625  and loss:  261.9590220451355
test acc: top1 ->  70.688 ; top5 ->  89.746  and loss:  954.8252929449081
forward train acc: top1 ->  69.3125 ; top5 ->  86.9765625  and loss:  261.0756005048752
test acc: top1 ->  70.744 ; top5 ->  89.716  and loss:  954.6571885943413
forward train acc: top1 ->  68.96875 ; top5 ->  87.25  and loss:  262.1121025085449
test acc: top1 ->  70.638 ; top5 ->  89.736  and loss:  954.3709781169891
forward train acc: top1 ->  68.4921875 ; top5 ->  87.296875  and loss:  260.4915000796318
test acc: top1 ->  70.706 ; top5 ->  89.812  and loss:  951.5508188605309
forward train acc: top1 ->  68.8984375 ; top5 ->  86.890625  and loss:  265.90292316675186
test acc: top1 ->  70.722 ; top5 ->  89.742  and loss:  952.7528793215752
>>>>>>> reverse layer  24  since performance drop >>>>>>>
==> this epoch:  244 / 256 , inc:  1
---------------- start layer  25  ---------------
adv train loss:  -66.24262064695358 , diff:  66.24262064695358
adv train loss:  -66.81713408231735 , diff:  0.5745134353637695
layer  25  adv train finish, try to retain  235
test acc: top1 ->  70.41 ; top5 ->  89.64  and loss:  961.4745149612427
forward train acc: top1 ->  68.765625 ; top5 ->  87.5390625  and loss:  257.21410489082336
test acc: top1 ->  70.578 ; top5 ->  89.604  and loss:  958.2454826831818
forward train acc: top1 ->  69.1328125 ; top5 ->  86.9921875  and loss:  260.0910730957985
test acc: top1 ->  70.518 ; top5 ->  89.624  and loss:  958.6219855546951
forward train acc: top1 ->  68.7734375 ; top5 ->  86.640625  and loss:  264.62272530794144
test acc: top1 ->  70.42 ; top5 ->  89.634  and loss:  961.2897235155106
forward train acc: top1 ->  68.15625 ; top5 ->  86.6171875  and loss:  269.1247352361679
test acc: top1 ->  70.516 ; top5 ->  89.554  and loss:  957.600758433342
forward train acc: top1 ->  68.625 ; top5 ->  86.9453125  and loss:  266.5182286500931
test acc: top1 ->  70.65 ; top5 ->  89.662  and loss:  955.834038734436
forward train acc: top1 ->  69.1484375 ; top5 ->  87.5390625  and loss:  259.3360161781311
test acc: top1 ->  70.492 ; top5 ->  89.644  and loss:  955.0465425252914
forward train acc: top1 ->  68.546875 ; top5 ->  87.1484375  and loss:  264.6030203104019
test acc: top1 ->  70.612 ; top5 ->  89.71  and loss:  956.3517053723335
forward train acc: top1 ->  69.15625 ; top5 ->  86.984375  and loss:  262.3189095854759
test acc: top1 ->  70.626 ; top5 ->  89.724  and loss:  954.9732219576836
forward train acc: top1 ->  69.2734375 ; top5 ->  87.3828125  and loss:  258.0623314976692
test acc: top1 ->  70.576 ; top5 ->  89.702  and loss:  953.016271173954
forward train acc: top1 ->  68.9765625 ; top5 ->  87.1875  and loss:  260.9510207772255
test acc: top1 ->  70.582 ; top5 ->  89.698  and loss:  954.6193261146545
>>>>>>> reverse layer  25  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  26  ---------------
### skip layer  26 wait:  4  ###
---------------- start layer  27  ---------------
### skip layer  27 wait:  4  ###
---------------- start layer  28  ---------------
### skip layer  28 wait:  4  ###
---------------- start layer  29  ---------------
### skip layer  29 wait:  4  ###
---------------- start layer  30  ---------------
### skip layer  30 wait:  4  ###
---------------- start layer  31  ---------------
adv train loss:  -65.55445331335068 , diff:  65.55445331335068
adv train loss:  -64.64686006307602 , diff:  0.9075932502746582
************ all values are small in this layer **********
layer  31  adv train finish, try to retain  500
test acc: top1 ->  70.402 ; top5 ->  89.58  and loss:  961.3485614657402
forward train acc: top1 ->  68.1171875 ; top5 ->  86.6484375  and loss:  265.7522460222244
test acc: top1 ->  70.474 ; top5 ->  89.55  and loss:  961.7161595821381
forward train acc: top1 ->  68.0390625 ; top5 ->  86.2265625  and loss:  270.81210684776306
test acc: top1 ->  70.584 ; top5 ->  89.614  and loss:  958.575653731823
forward train acc: top1 ->  68.9921875 ; top5 ->  86.9609375  and loss:  260.8203871846199
test acc: top1 ->  70.558 ; top5 ->  89.574  and loss:  958.9039354324341
forward train acc: top1 ->  69.1015625 ; top5 ->  87.078125  and loss:  263.8088252544403
test acc: top1 ->  70.596 ; top5 ->  89.606  and loss:  957.4748092889786
forward train acc: top1 ->  68.9296875 ; top5 ->  87.2890625  and loss:  261.7506520152092
test acc: top1 ->  70.65 ; top5 ->  89.658  and loss:  954.9484627842903
forward train acc: top1 ->  69.3046875 ; top5 ->  87.2421875  and loss:  257.84331780672073
test acc: top1 ->  70.696 ; top5 ->  89.69  and loss:  954.8179455399513
forward train acc: top1 ->  68.8046875 ; top5 ->  87.25  and loss:  260.79002690315247
test acc: top1 ->  70.708 ; top5 ->  89.632  and loss:  955.3443338274956
forward train acc: top1 ->  68.3515625 ; top5 ->  87.015625  and loss:  265.26093566417694
test acc: top1 ->  70.752 ; top5 ->  89.704  and loss:  955.7314918637276
forward train acc: top1 ->  68.609375 ; top5 ->  86.9453125  and loss:  264.2953467965126
test acc: top1 ->  70.738 ; top5 ->  89.764  and loss:  953.9492676258087
forward train acc: top1 ->  68.546875 ; top5 ->  87.1484375  and loss:  266.7884537577629
test acc: top1 ->  70.684 ; top5 ->  89.754  and loss:  952.7266902327538
>>>>>>> reverse layer  31  since performance drop >>>>>>>
==> this epoch:  501 / 512 , inc:  1
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.050625, 0.10125, 0.0008898925781250001, 0.0094921875, 0.0253125, 0.27, 0.0011865234375, 0.00011123657226562501, 0.00474609375, 0.0011865234375, 0.01265625, 0.0011865234375, 0.002373046875, 0.050625, 5.561828613281251e-05, 0.00011123657226562501, 0.00059326171875, 0.00059326171875, 0.00059326171875, 0.006328125, 0.00059326171875, 0.00011123657226562501, 0.002373046875, 0.006328125, 0.00022247314453125003, 0.00022247314453125003, 5.561828613281251e-05, 5.561828613281251e-05, 5.561828613281251e-05, 5.561828613281251e-05, 5.561828613281251e-05, 0.00011123657226562501]  wait [3, 4, 4, 4, 2, 2, 2, 2, 4, 2, 2, 2, 3, 4, 2, 3, 2, 2, 2, 2, 2, 3, 4, 2, 4, 4, 3, 3, 3, 3, 3, 4]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 2
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  15  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -64.44218629598618 , diff:  64.44218629598618
adv train loss:  -65.57348841428757 , diff:  1.1313021183013916
layer  0  adv train finish, try to retain  41
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -65.63531482219696 , diff:  65.63531482219696
adv train loss:  -66.69531911611557 , diff:  1.0600042939186096
layer  1  adv train finish, try to retain  39
test acc: top1 ->  65.676 ; top5 ->  86.374  and loss:  1135.543585062027
forward train acc: top1 ->  67.7578125 ; top5 ->  86.8203125  and loss:  270.6160720586777
test acc: top1 ->  70.104 ; top5 ->  89.452  and loss:  971.6395769715309
forward train acc: top1 ->  68.734375 ; top5 ->  87.109375  and loss:  260.7350828051567
test acc: top1 ->  70.238 ; top5 ->  89.496  and loss:  967.4570617675781
forward train acc: top1 ->  68.4921875 ; top5 ->  86.8359375  and loss:  263.6681985259056
test acc: top1 ->  70.226 ; top5 ->  89.474  and loss:  969.7835367918015
forward train acc: top1 ->  68.4921875 ; top5 ->  86.5  and loss:  269.51540845632553
test acc: top1 ->  70.304 ; top5 ->  89.54  and loss:  964.2009593844414
forward train acc: top1 ->  68.6171875 ; top5 ->  87.109375  and loss:  261.78325909376144
test acc: top1 ->  70.43 ; top5 ->  89.65  and loss:  965.4125283360481
forward train acc: top1 ->  68.296875 ; top5 ->  87.2109375  and loss:  260.6651132106781
test acc: top1 ->  70.31 ; top5 ->  89.55  and loss:  964.9276809692383
forward train acc: top1 ->  68.515625 ; top5 ->  87.15625  and loss:  263.79621720314026
test acc: top1 ->  70.446 ; top5 ->  89.576  and loss:  961.9412754178047
forward train acc: top1 ->  68.484375 ; top5 ->  86.8671875  and loss:  266.22962963581085
test acc: top1 ->  70.554 ; top5 ->  89.68  and loss:  960.0210952162743
forward train acc: top1 ->  69.3046875 ; top5 ->  87.625  and loss:  258.84762954711914
test acc: top1 ->  70.492 ; top5 ->  89.64  and loss:  960.880480825901
forward train acc: top1 ->  68.859375 ; top5 ->  87.328125  and loss:  259.86091697216034
test acc: top1 ->  70.484 ; top5 ->  89.664  and loss:  961.6294586062431
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  44 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -65.62445563077927 , diff:  65.62445563077927
adv train loss:  -68.58966827392578 , diff:  2.965212643146515
layer  2  adv train finish, try to retain  56
test acc: top1 ->  69.024 ; top5 ->  88.596  and loss:  1010.861494064331
forward train acc: top1 ->  68.171875 ; top5 ->  86.609375  and loss:  271.6676412820816
test acc: top1 ->  70.378 ; top5 ->  89.656  and loss:  959.4238090515137
forward train acc: top1 ->  68.75 ; top5 ->  87.109375  and loss:  263.51209288835526
test acc: top1 ->  70.31 ; top5 ->  89.682  and loss:  962.050943672657
forward train acc: top1 ->  68.8984375 ; top5 ->  87.484375  and loss:  259.90661984682083
test acc: top1 ->  70.406 ; top5 ->  89.722  and loss:  959.7021176815033
forward train acc: top1 ->  69.0546875 ; top5 ->  87.3125  and loss:  258.8347090482712
test acc: top1 ->  70.494 ; top5 ->  89.722  and loss:  958.8525605201721
forward train acc: top1 ->  69.125 ; top5 ->  87.09375  and loss:  259.7892173528671
test acc: top1 ->  70.554 ; top5 ->  89.746  and loss:  956.7766310572624
forward train acc: top1 ->  68.921875 ; top5 ->  87.7109375  and loss:  260.7239003777504
test acc: top1 ->  70.618 ; top5 ->  89.794  and loss:  954.6622008085251
forward train acc: top1 ->  69.65625 ; top5 ->  87.3203125  and loss:  256.5860045552254
test acc: top1 ->  70.702 ; top5 ->  89.78  and loss:  955.2416765093803
forward train acc: top1 ->  68.734375 ; top5 ->  87.3125  and loss:  261.07086646556854
test acc: top1 ->  70.638 ; top5 ->  89.78  and loss:  954.6197964549065
forward train acc: top1 ->  69.03125 ; top5 ->  87.25  and loss:  264.32573515176773
test acc: top1 ->  70.59 ; top5 ->  89.766  and loss:  955.5815362930298
forward train acc: top1 ->  69.875 ; top5 ->  87.9375  and loss:  253.67463737726212
test acc: top1 ->  70.68 ; top5 ->  89.764  and loss:  951.4807255268097
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  57 / 64 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -65.98062884807587 , diff:  65.98062884807587
adv train loss:  -65.94598245620728 , diff:  0.03464639186859131
************ all values are small in this layer **********
layer  3  adv train finish, try to retain  42
test acc: top1 ->  66.678 ; top5 ->  87.398  and loss:  1090.1496824026108
forward train acc: top1 ->  68.2421875 ; top5 ->  87.0625  and loss:  265.8398465514183
test acc: top1 ->  70.288 ; top5 ->  89.562  and loss:  967.1133417487144
forward train acc: top1 ->  68.375 ; top5 ->  86.734375  and loss:  266.7173578739166
test acc: top1 ->  70.35 ; top5 ->  89.628  and loss:  965.377357006073
forward train acc: top1 ->  68.8046875 ; top5 ->  87.5234375  and loss:  261.6277664899826
test acc: top1 ->  70.352 ; top5 ->  89.578  and loss:  966.8804636597633
forward train acc: top1 ->  68.2578125 ; top5 ->  86.7734375  and loss:  266.02526116371155
test acc: top1 ->  70.374 ; top5 ->  89.632  and loss:  965.7673033475876
forward train acc: top1 ->  68.8359375 ; top5 ->  86.6953125  and loss:  264.9137108325958
test acc: top1 ->  70.38 ; top5 ->  89.656  and loss:  964.2885360121727
forward train acc: top1 ->  69.2265625 ; top5 ->  87.1796875  and loss:  259.34218657016754
test acc: top1 ->  70.526 ; top5 ->  89.67  and loss:  960.4443655014038
forward train acc: top1 ->  68.234375 ; top5 ->  86.65625  and loss:  267.93907630443573
test acc: top1 ->  70.516 ; top5 ->  89.704  and loss:  963.7078751325607
forward train acc: top1 ->  69.375 ; top5 ->  87.2109375  and loss:  260.5870831608772
test acc: top1 ->  70.462 ; top5 ->  89.722  and loss:  959.1841051578522
forward train acc: top1 ->  68.9140625 ; top5 ->  86.8828125  and loss:  262.63968962430954
test acc: top1 ->  70.344 ; top5 ->  89.72  and loss:  963.0157240629196
forward train acc: top1 ->  67.6796875 ; top5 ->  86.4453125  and loss:  268.3944798707962
test acc: top1 ->  70.494 ; top5 ->  89.69  and loss:  960.6482261419296
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  43 / 64 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -65.7979142665863 , diff:  65.7979142665863
adv train loss:  -62.46876722574234 , diff:  3.3291470408439636
layer  4  adv train finish, try to retain  38
test acc: top1 ->  69.512 ; top5 ->  88.954  and loss:  997.7323390245438
forward train acc: top1 ->  68.3671875 ; top5 ->  86.1953125  and loss:  270.51298516988754
test acc: top1 ->  70.346 ; top5 ->  89.512  and loss:  966.1569581627846
forward train acc: top1 ->  68.75 ; top5 ->  87.2265625  and loss:  261.9127922654152
test acc: top1 ->  70.362 ; top5 ->  89.442  and loss:  967.0921014547348
forward train acc: top1 ->  68.3515625 ; top5 ->  86.875  and loss:  265.91855174303055
test acc: top1 ->  70.416 ; top5 ->  89.586  and loss:  965.2863224744797
forward train acc: top1 ->  67.9609375 ; top5 ->  86.8515625  and loss:  265.2992339730263
test acc: top1 ->  70.462 ; top5 ->  89.564  and loss:  964.005033493042
forward train acc: top1 ->  68.6953125 ; top5 ->  86.796875  and loss:  264.3298922777176
test acc: top1 ->  70.608 ; top5 ->  89.608  and loss:  959.6200212240219
forward train acc: top1 ->  69.046875 ; top5 ->  86.8984375  and loss:  261.2919777035713
test acc: top1 ->  70.492 ; top5 ->  89.588  and loss:  959.6070306897163
forward train acc: top1 ->  68.8203125 ; top5 ->  87.1171875  and loss:  264.22585570812225
test acc: top1 ->  70.536 ; top5 ->  89.636  and loss:  960.0842537879944
forward train acc: top1 ->  68.265625 ; top5 ->  87.2890625  and loss:  261.2259059548378
test acc: top1 ->  70.566 ; top5 ->  89.568  and loss:  960.5699949264526
forward train acc: top1 ->  68.8671875 ; top5 ->  87.1875  and loss:  259.870913207531
test acc: top1 ->  70.606 ; top5 ->  89.598  and loss:  958.5682781338692
forward train acc: top1 ->  68.6015625 ; top5 ->  87.515625  and loss:  259.97487729787827
test acc: top1 ->  70.62 ; top5 ->  89.666  and loss:  957.510746717453
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  40 / 64 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -68.91357487440109 , diff:  68.91357487440109
adv train loss:  -65.35483539104462 , diff:  3.558739483356476
************ all values are small in this layer **********
layer  5  adv train finish, try to retain  28
test acc: top1 ->  65.36 ; top5 ->  86.144  and loss:  1145.0975319743156
forward train acc: top1 ->  69.03125 ; top5 ->  87.6015625  and loss:  259.1023208498955
test acc: top1 ->  70.074 ; top5 ->  89.514  and loss:  969.9672157168388
forward train acc: top1 ->  69.0078125 ; top5 ->  86.71875  and loss:  264.76445269584656
test acc: top1 ->  70.174 ; top5 ->  89.394  and loss:  968.8492350578308
forward train acc: top1 ->  68.46875 ; top5 ->  86.8046875  and loss:  266.53460347652435
test acc: top1 ->  70.384 ; top5 ->  89.442  and loss:  966.6871310472488
forward train acc: top1 ->  69.3671875 ; top5 ->  87.203125  and loss:  257.66883474588394
test acc: top1 ->  70.486 ; top5 ->  89.502  and loss:  962.9174836874008
forward train acc: top1 ->  68.5859375 ; top5 ->  86.9140625  and loss:  267.6028588414192
test acc: top1 ->  70.424 ; top5 ->  89.486  and loss:  964.4955117702484
forward train acc: top1 ->  69.3359375 ; top5 ->  87.03125  and loss:  260.73522901535034
test acc: top1 ->  70.518 ; top5 ->  89.552  and loss:  959.523256778717
forward train acc: top1 ->  68.4765625 ; top5 ->  86.90625  and loss:  266.42485308647156
test acc: top1 ->  70.548 ; top5 ->  89.562  and loss:  960.4947853684425
forward train acc: top1 ->  68.2578125 ; top5 ->  86.4765625  and loss:  267.3126377463341
test acc: top1 ->  70.558 ; top5 ->  89.572  and loss:  960.2802360653877
forward train acc: top1 ->  69.0703125 ; top5 ->  87.4140625  and loss:  257.5249053835869
test acc: top1 ->  70.574 ; top5 ->  89.588  and loss:  960.1338528394699
forward train acc: top1 ->  68.6875 ; top5 ->  87.6640625  and loss:  259.7785611152649
test acc: top1 ->  70.524 ; top5 ->  89.56  and loss:  959.4733771085739
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  29 / 64 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -65.39951884746552 , diff:  65.39951884746552
adv train loss:  -65.43705183267593 , diff:  0.0375329852104187
layer  6  adv train finish, try to retain  108
test acc: top1 ->  67.654 ; top5 ->  88.088  and loss:  1053.9015494585037
forward train acc: top1 ->  69.46875 ; top5 ->  87.140625  and loss:  257.81533259153366
test acc: top1 ->  70.128 ; top5 ->  89.43  and loss:  968.4829248189926
forward train acc: top1 ->  68.1484375 ; top5 ->  86.6796875  and loss:  269.869989156723
test acc: top1 ->  70.352 ; top5 ->  89.482  and loss:  965.7600172162056
forward train acc: top1 ->  68.78125 ; top5 ->  87.15625  and loss:  260.6201391816139
test acc: top1 ->  70.414 ; top5 ->  89.4  and loss:  969.3290137052536
forward train acc: top1 ->  68.65625 ; top5 ->  87.0703125  and loss:  262.8797379732132
test acc: top1 ->  70.434 ; top5 ->  89.6  and loss:  965.2191769480705
forward train acc: top1 ->  68.1640625 ; top5 ->  86.2265625  and loss:  272.51920384168625
test acc: top1 ->  70.412 ; top5 ->  89.552  and loss:  965.5503888726234
forward train acc: top1 ->  68.421875 ; top5 ->  86.875  and loss:  264.174315571785
test acc: top1 ->  70.546 ; top5 ->  89.654  and loss:  962.2949648499489
forward train acc: top1 ->  69.0859375 ; top5 ->  87.609375  and loss:  258.05549132823944
test acc: top1 ->  70.604 ; top5 ->  89.682  and loss:  962.4704420566559
forward train acc: top1 ->  68.8984375 ; top5 ->  87.296875  and loss:  261.6809260249138
test acc: top1 ->  70.53 ; top5 ->  89.616  and loss:  962.5642133355141
forward train acc: top1 ->  68.2265625 ; top5 ->  86.640625  and loss:  263.925745010376
test acc: top1 ->  70.544 ; top5 ->  89.604  and loss:  957.6547208428383
forward train acc: top1 ->  68.4765625 ; top5 ->  87.046875  and loss:  265.9309642314911
test acc: top1 ->  70.684 ; top5 ->  89.694  and loss:  956.5450402498245
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  112 / 128 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -69.16537463665009 , diff:  69.16537463665009
adv train loss:  -61.56648397445679 , diff:  7.598890662193298
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  121
test acc: top1 ->  70.022 ; top5 ->  89.394  and loss:  973.7675242424011
forward train acc: top1 ->  68.65625 ; top5 ->  86.703125  and loss:  266.48118287324905
test acc: top1 ->  70.534 ; top5 ->  89.656  and loss:  961.0823446512222
forward train acc: top1 ->  68.59375 ; top5 ->  86.8671875  and loss:  264.6468583345413
test acc: top1 ->  70.528 ; top5 ->  89.62  and loss:  959.8487358689308
forward train acc: top1 ->  68.0703125 ; top5 ->  86.8671875  and loss:  267.2199457883835
test acc: top1 ->  70.446 ; top5 ->  89.648  and loss:  959.6593343019485
forward train acc: top1 ->  68.59375 ; top5 ->  87.1640625  and loss:  261.10771429538727
test acc: top1 ->  70.482 ; top5 ->  89.702  and loss:  957.6724429130554
forward train acc: top1 ->  68.390625 ; top5 ->  86.7734375  and loss:  266.8538303375244
test acc: top1 ->  70.586 ; top5 ->  89.726  and loss:  957.8031533956528
forward train acc: top1 ->  68.7890625 ; top5 ->  87.375  and loss:  260.37919503450394
test acc: top1 ->  70.636 ; top5 ->  89.65  and loss:  956.7824229001999
forward train acc: top1 ->  69.109375 ; top5 ->  87.46875  and loss:  257.86768478155136
test acc: top1 ->  70.718 ; top5 ->  89.718  and loss:  954.2442928552628
forward train acc: top1 ->  69.7421875 ; top5 ->  87.703125  and loss:  256.8836725950241
test acc: top1 ->  70.686 ; top5 ->  89.67  and loss:  954.5902619361877
forward train acc: top1 ->  69.2265625 ; top5 ->  87.015625  and loss:  262.6928034424782
test acc: top1 ->  70.698 ; top5 ->  89.742  and loss:  955.269535779953
forward train acc: top1 ->  69.2578125 ; top5 ->  87.21875  and loss:  258.7512955069542
test acc: top1 ->  70.69 ; top5 ->  89.756  and loss:  954.4987565279007
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  122 / 128 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -61.3345103263855 , diff:  61.3345103263855
adv train loss:  -64.27590942382812 , diff:  2.941399097442627
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  86
test acc: top1 ->  68.75 ; top5 ->  88.614  and loss:  1020.5809225440025
forward train acc: top1 ->  68.421875 ; top5 ->  86.1953125  and loss:  271.38015258312225
test acc: top1 ->  70.264 ; top5 ->  89.356  and loss:  967.5274578928947
forward train acc: top1 ->  68.1796875 ; top5 ->  86.828125  and loss:  268.43016904592514
test acc: top1 ->  70.222 ; top5 ->  89.494  and loss:  967.3252715468407
forward train acc: top1 ->  68.28125 ; top5 ->  86.921875  and loss:  263.8206706047058
test acc: top1 ->  70.26 ; top5 ->  89.514  and loss:  964.8460549712181
forward train acc: top1 ->  69.171875 ; top5 ->  87.2265625  and loss:  259.9554808139801
test acc: top1 ->  70.412 ; top5 ->  89.502  and loss:  964.0099958777428
forward train acc: top1 ->  68.0703125 ; top5 ->  86.7734375  and loss:  266.71246314048767
test acc: top1 ->  70.422 ; top5 ->  89.484  and loss:  962.2719799280167
forward train acc: top1 ->  69.03125 ; top5 ->  87.140625  and loss:  260.12144380807877
test acc: top1 ->  70.396 ; top5 ->  89.576  and loss:  959.6312609314919
forward train acc: top1 ->  69.515625 ; top5 ->  86.921875  and loss:  261.31987631320953
test acc: top1 ->  70.5 ; top5 ->  89.568  and loss:  961.2246850728989
forward train acc: top1 ->  68.8125 ; top5 ->  87.0  and loss:  266.2349885106087
test acc: top1 ->  70.446 ; top5 ->  89.63  and loss:  960.2797321081161
forward train acc: top1 ->  68.234375 ; top5 ->  87.1953125  and loss:  266.46306693553925
test acc: top1 ->  70.548 ; top5 ->  89.612  and loss:  958.5990512371063
forward train acc: top1 ->  69.4921875 ; top5 ->  87.0703125  and loss:  260.2148330807686
test acc: top1 ->  70.606 ; top5 ->  89.68  and loss:  958.3640370965004
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -67.30696845054626 , diff:  67.30696845054626
adv train loss:  -63.58310133218765 , diff:  3.723867118358612
layer  9  adv train finish, try to retain  103
test acc: top1 ->  68.876 ; top5 ->  88.814  and loss:  1010.2751806378365
forward train acc: top1 ->  68.21875 ; top5 ->  86.8359375  and loss:  268.0415087938309
test acc: top1 ->  70.168 ; top5 ->  89.47  and loss:  969.9720712900162
forward train acc: top1 ->  68.75 ; top5 ->  87.1171875  and loss:  266.5356407761574
test acc: top1 ->  70.21 ; top5 ->  89.47  and loss:  970.7353808283806
forward train acc: top1 ->  68.0390625 ; top5 ->  86.7734375  and loss:  269.4826155900955
test acc: top1 ->  70.164 ; top5 ->  89.512  and loss:  969.4157900810242
forward train acc: top1 ->  69.296875 ; top5 ->  86.9453125  and loss:  262.17700749635696
test acc: top1 ->  70.242 ; top5 ->  89.542  and loss:  964.5448607206345
forward train acc: top1 ->  68.3828125 ; top5 ->  86.859375  and loss:  265.1727489233017
test acc: top1 ->  70.374 ; top5 ->  89.568  and loss:  963.2929506897926
forward train acc: top1 ->  68.8828125 ; top5 ->  87.2109375  and loss:  261.7473265528679
test acc: top1 ->  70.326 ; top5 ->  89.626  and loss:  962.8147286772728
forward train acc: top1 ->  68.25 ; top5 ->  87.484375  and loss:  263.9184445142746
test acc: top1 ->  70.402 ; top5 ->  89.596  and loss:  960.5285353064537
forward train acc: top1 ->  68.0234375 ; top5 ->  86.4921875  and loss:  268.7750752568245
test acc: top1 ->  70.47 ; top5 ->  89.65  and loss:  961.2302894592285
forward train acc: top1 ->  69.3515625 ; top5 ->  87.4140625  and loss:  257.23517370224
test acc: top1 ->  70.438 ; top5 ->  89.652  and loss:  962.1059965491295
forward train acc: top1 ->  69.1875 ; top5 ->  87.1484375  and loss:  258.97518825531006
test acc: top1 ->  70.466 ; top5 ->  89.632  and loss:  959.7036259770393
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  115 / 128 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -64.48088133335114 , diff:  64.48088133335114
adv train loss:  -63.11291515827179 , diff:  1.3679661750793457
layer  10  adv train finish, try to retain  78
test acc: top1 ->  69.166 ; top5 ->  88.858  and loss:  1007.4481040239334
forward train acc: top1 ->  68.484375 ; top5 ->  86.5390625  and loss:  267.6789682507515
test acc: top1 ->  70.216 ; top5 ->  89.43  and loss:  970.7994547486305
forward train acc: top1 ->  68.3515625 ; top5 ->  86.78125  and loss:  265.2812812924385
test acc: top1 ->  70.31 ; top5 ->  89.506  and loss:  969.5866255164146
forward train acc: top1 ->  68.265625 ; top5 ->  86.734375  and loss:  266.6177667975426
test acc: top1 ->  70.322 ; top5 ->  89.546  and loss:  968.4481302499771
forward train acc: top1 ->  67.90625 ; top5 ->  86.90625  and loss:  267.92412185668945
test acc: top1 ->  70.342 ; top5 ->  89.542  and loss:  962.7070116996765
forward train acc: top1 ->  68.5703125 ; top5 ->  86.9609375  and loss:  267.1374507546425
test acc: top1 ->  70.418 ; top5 ->  89.556  and loss:  962.493779540062
forward train acc: top1 ->  68.4140625 ; top5 ->  86.8359375  and loss:  267.2363644838333
test acc: top1 ->  70.402 ; top5 ->  89.544  and loss:  961.5145406126976
forward train acc: top1 ->  69.0546875 ; top5 ->  87.359375  and loss:  258.7809325456619
test acc: top1 ->  70.456 ; top5 ->  89.602  and loss:  959.501606285572
forward train acc: top1 ->  68.8359375 ; top5 ->  87.3984375  and loss:  261.2019656300545
test acc: top1 ->  70.472 ; top5 ->  89.63  and loss:  958.9256225824356
forward train acc: top1 ->  68.3515625 ; top5 ->  87.0078125  and loss:  262.1777386069298
test acc: top1 ->  70.368 ; top5 ->  89.586  and loss:  960.9999343156815
forward train acc: top1 ->  68.4453125 ; top5 ->  86.90625  and loss:  264.46338099241257
test acc: top1 ->  70.462 ; top5 ->  89.646  and loss:  961.176937520504
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -64.1879254579544 , diff:  64.1879254579544
adv train loss:  -66.12504506111145 , diff:  1.9371196031570435
layer  11  adv train finish, try to retain  111
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -64.88010668754578 , diff:  64.88010668754578
adv train loss:  -65.76964437961578 , diff:  0.8895376920700073
layer  12  adv train finish, try to retain  107
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -67.86564421653748 , diff:  67.86564421653748
adv train loss:  -65.06246626377106 , diff:  2.8031779527664185
layer  13  adv train finish, try to retain  66
test acc: top1 ->  66.578 ; top5 ->  86.9  and loss:  1105.624755859375
forward train acc: top1 ->  68.171875 ; top5 ->  86.421875  and loss:  269.31477493047714
test acc: top1 ->  70.026 ; top5 ->  89.37  and loss:  972.0277400016785
forward train acc: top1 ->  68.5078125 ; top5 ->  86.75  and loss:  266.6166381239891
test acc: top1 ->  70.124 ; top5 ->  89.432  and loss:  969.4346117377281
forward train acc: top1 ->  68.21875 ; top5 ->  86.5078125  and loss:  271.063035428524
test acc: top1 ->  70.036 ; top5 ->  89.42  and loss:  971.1392441987991
forward train acc: top1 ->  68.234375 ; top5 ->  87.015625  and loss:  264.7843060493469
test acc: top1 ->  70.204 ; top5 ->  89.494  and loss:  967.9644490480423
forward train acc: top1 ->  68.328125 ; top5 ->  86.9765625  and loss:  262.34510242938995
test acc: top1 ->  70.258 ; top5 ->  89.458  and loss:  964.2566640973091
forward train acc: top1 ->  69.0 ; top5 ->  87.0078125  and loss:  266.1780796647072
test acc: top1 ->  70.288 ; top5 ->  89.48  and loss:  962.2966643571854
forward train acc: top1 ->  69.0234375 ; top5 ->  87.3046875  and loss:  259.81707698106766
test acc: top1 ->  70.318 ; top5 ->  89.5  and loss:  960.4047161340714
forward train acc: top1 ->  68.2578125 ; top5 ->  87.0  and loss:  264.56581646203995
test acc: top1 ->  70.366 ; top5 ->  89.536  and loss:  962.1637446284294
forward train acc: top1 ->  68.8046875 ; top5 ->  87.0625  and loss:  264.5133658051491
test acc: top1 ->  70.34 ; top5 ->  89.582  and loss:  960.877043068409
forward train acc: top1 ->  68.9296875 ; top5 ->  87.296875  and loss:  262.9789580106735
test acc: top1 ->  70.304 ; top5 ->  89.58  and loss:  962.9735410809517
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  71 / 128 , inc:  1
---------------- start layer  14  ---------------
adv train loss:  -64.51220721006393 , diff:  64.51220721006393
adv train loss:  -64.03171676397324 , diff:  0.48049044609069824
layer  14  adv train finish, try to retain  249
test acc: top1 ->  70.516 ; top5 ->  89.648  and loss:  962.1347832679749
forward train acc: top1 ->  70.0703125 ; top5 ->  87.609375  and loss:  253.57258641719818
test acc: top1 ->  70.808 ; top5 ->  89.73  and loss:  953.9834037423134
forward train acc: top1 ->  68.828125 ; top5 ->  86.953125  and loss:  266.09793162345886
test acc: top1 ->  70.572 ; top5 ->  89.654  and loss:  957.6294795870781
forward train acc: top1 ->  68.84375 ; top5 ->  87.015625  and loss:  264.84104335308075
test acc: top1 ->  70.662 ; top5 ->  89.624  and loss:  957.2592871189117
forward train acc: top1 ->  69.015625 ; top5 ->  87.53125  and loss:  258.3193637728691
test acc: top1 ->  70.574 ; top5 ->  89.6  and loss:  955.0416209101677
forward train acc: top1 ->  68.59375 ; top5 ->  87.234375  and loss:  265.901792883873
test acc: top1 ->  70.59 ; top5 ->  89.666  and loss:  955.7306697368622
forward train acc: top1 ->  69.28125 ; top5 ->  87.3671875  and loss:  260.30146795511246
test acc: top1 ->  70.652 ; top5 ->  89.708  and loss:  953.4671751260757
forward train acc: top1 ->  68.984375 ; top5 ->  87.65625  and loss:  260.1810558438301
test acc: top1 ->  70.73 ; top5 ->  89.718  and loss:  951.7261615991592
forward train acc: top1 ->  68.828125 ; top5 ->  86.8828125  and loss:  262.10866087675095
test acc: top1 ->  70.768 ; top5 ->  89.728  and loss:  949.7577468752861
forward train acc: top1 ->  70.0234375 ; top5 ->  87.78125  and loss:  253.34873682260513
test acc: top1 ->  70.738 ; top5 ->  89.684  and loss:  949.5938839316368
forward train acc: top1 ->  69.7734375 ; top5 ->  87.40625  and loss:  256.94617986679077
test acc: top1 ->  70.826 ; top5 ->  89.742  and loss:  949.0494239330292
>>>>>>> reverse layer  14  since performance drop >>>>>>>
==> this epoch:  250 / 256 , inc:  1
---------------- start layer  15  ---------------
adv train loss:  -65.81907802820206 , diff:  65.81907802820206
adv train loss:  -67.86038911342621 , diff:  2.0413110852241516
************ all values are small in this layer **********
layer  15  adv train finish, try to retain  247
test acc: top1 ->  70.102 ; top5 ->  89.36  and loss:  976.6515770554543
forward train acc: top1 ->  68.1953125 ; top5 ->  86.4765625  and loss:  270.4332721233368
test acc: top1 ->  70.322 ; top5 ->  89.65  and loss:  963.1675490140915
forward train acc: top1 ->  68.46875 ; top5 ->  87.1796875  and loss:  261.9181628227234
test acc: top1 ->  70.392 ; top5 ->  89.622  and loss:  961.2418189048767
forward train acc: top1 ->  68.3515625 ; top5 ->  87.125  and loss:  263.00147354602814
test acc: top1 ->  70.446 ; top5 ->  89.69  and loss:  959.2921923995018
forward train acc: top1 ->  68.9140625 ; top5 ->  87.6796875  and loss:  258.70971781015396
test acc: top1 ->  70.486 ; top5 ->  89.632  and loss:  959.8243224024773
forward train acc: top1 ->  68.859375 ; top5 ->  87.1875  and loss:  263.32120031118393
test acc: top1 ->  70.436 ; top5 ->  89.69  and loss:  960.2980702519417
forward train acc: top1 ->  69.1015625 ; top5 ->  86.96875  and loss:  260.9850655198097
test acc: top1 ->  70.63 ; top5 ->  89.756  and loss:  957.7430648207664
forward train acc: top1 ->  69.4140625 ; top5 ->  87.1953125  and loss:  259.28429901599884
test acc: top1 ->  70.624 ; top5 ->  89.664  and loss:  955.8228898048401
forward train acc: top1 ->  69.3671875 ; top5 ->  86.9140625  and loss:  260.2840448021889
test acc: top1 ->  70.664 ; top5 ->  89.722  and loss:  955.5785976052284
forward train acc: top1 ->  69.2578125 ; top5 ->  87.3828125  and loss:  259.35782366991043
test acc: top1 ->  70.668 ; top5 ->  89.788  and loss:  954.8311673402786
forward train acc: top1 ->  68.890625 ; top5 ->  86.9453125  and loss:  266.18707859516144
test acc: top1 ->  70.662 ; top5 ->  89.74  and loss:  953.1566212773323
>>>>>>> reverse layer  15  since performance drop >>>>>>>
==> this epoch:  248 / 256 , inc:  1
---------------- start layer  16  ---------------
adv train loss:  -64.99001199007034 , diff:  64.99001199007034
adv train loss:  -64.48552644252777 , diff:  0.504485547542572
layer  16  adv train finish, try to retain  222
test acc: top1 ->  70.404 ; top5 ->  89.534  and loss:  962.9790185689926
forward train acc: top1 ->  69.0 ; top5 ->  87.546875  and loss:  258.7392697930336
test acc: top1 ->  70.574 ; top5 ->  89.648  and loss:  961.4905673265457
forward train acc: top1 ->  68.2734375 ; top5 ->  86.734375  and loss:  269.5596905350685
test acc: top1 ->  70.524 ; top5 ->  89.644  and loss:  959.1632407307625
forward train acc: top1 ->  69.1015625 ; top5 ->  86.875  and loss:  261.2421952486038
test acc: top1 ->  70.526 ; top5 ->  89.588  and loss:  958.6007642745972
forward train acc: top1 ->  69.03125 ; top5 ->  86.9140625  and loss:  262.9261000752449
test acc: top1 ->  70.566 ; top5 ->  89.604  and loss:  959.3989260196686
forward train acc: top1 ->  69.0625 ; top5 ->  87.0859375  and loss:  263.95175009965897
test acc: top1 ->  70.668 ; top5 ->  89.672  and loss:  958.6107687354088
forward train acc: top1 ->  68.8984375 ; top5 ->  87.0234375  and loss:  262.4467689394951
test acc: top1 ->  70.762 ; top5 ->  89.728  and loss:  955.0274712443352
forward train acc: top1 ->  69.265625 ; top5 ->  87.703125  and loss:  256.6277960538864
test acc: top1 ->  70.642 ; top5 ->  89.698  and loss:  953.8522751331329
forward train acc: top1 ->  69.0234375 ; top5 ->  87.1875  and loss:  260.28850412368774
test acc: top1 ->  70.638 ; top5 ->  89.674  and loss:  955.3339710831642
forward train acc: top1 ->  68.8359375 ; top5 ->  87.234375  and loss:  260.2393556833267
test acc: top1 ->  70.744 ; top5 ->  89.746  and loss:  953.6187759041786
forward train acc: top1 ->  69.21875 ; top5 ->  87.7109375  and loss:  258.99960523843765
test acc: top1 ->  70.71 ; top5 ->  89.716  and loss:  954.6069244742393
>>>>>>> reverse layer  16  since performance drop >>>>>>>
==> this epoch:  239 / 256 , inc:  1
---------------- start layer  17  ---------------
adv train loss:  -64.89911687374115 , diff:  64.89911687374115
adv train loss:  -65.1454963684082 , diff:  0.24637949466705322
layer  17  adv train finish, try to retain  219
test acc: top1 ->  70.356 ; top5 ->  89.544  and loss:  965.7567976117134
forward train acc: top1 ->  69.625 ; top5 ->  87.4765625  and loss:  261.01201605796814
test acc: top1 ->  70.6 ; top5 ->  89.624  and loss:  957.9945495128632
forward train acc: top1 ->  68.8125 ; top5 ->  87.2265625  and loss:  258.7224090099335
test acc: top1 ->  70.512 ; top5 ->  89.578  and loss:  960.6515316367149
forward train acc: top1 ->  67.9375 ; top5 ->  86.84375  and loss:  268.56872951984406
test acc: top1 ->  70.57 ; top5 ->  89.668  and loss:  961.0150575637817
forward train acc: top1 ->  68.5 ; top5 ->  87.25  and loss:  260.838985145092
test acc: top1 ->  70.608 ; top5 ->  89.706  and loss:  956.6509960889816
forward train acc: top1 ->  68.984375 ; top5 ->  87.1015625  and loss:  264.1158199906349
test acc: top1 ->  70.604 ; top5 ->  89.762  and loss:  954.1373925209045
forward train acc: top1 ->  68.4921875 ; top5 ->  86.8125  and loss:  267.3615950345993
test acc: top1 ->  70.6 ; top5 ->  89.762  and loss:  953.4144510626793
forward train acc: top1 ->  68.9609375 ; top5 ->  87.359375  and loss:  259.4707969427109
test acc: top1 ->  70.69 ; top5 ->  89.734  and loss:  954.380329310894
forward train acc: top1 ->  68.1953125 ; top5 ->  86.875  and loss:  264.6737223267555
test acc: top1 ->  70.682 ; top5 ->  89.762  and loss:  951.3136342763901
forward train acc: top1 ->  69.2265625 ; top5 ->  87.1484375  and loss:  261.1794646382332
test acc: top1 ->  70.744 ; top5 ->  89.764  and loss:  952.2651362419128
forward train acc: top1 ->  68.71875 ; top5 ->  87.3203125  and loss:  259.95257717370987
test acc: top1 ->  70.736 ; top5 ->  89.746  and loss:  951.9727472662926
>>>>>>> reverse layer  17  since performance drop >>>>>>>
==> this epoch:  230 / 256 , inc:  1
---------------- start layer  18  ---------------
adv train loss:  -63.45413535833359 , diff:  63.45413535833359
adv train loss:  -66.23121356964111 , diff:  2.7770782113075256
layer  18  adv train finish, try to retain  236
>>>>>>> reverse layer  18  since no improvement >>>>>>>
---------------- start layer  19  ---------------
adv train loss:  -65.54586362838745 , diff:  65.54586362838745
adv train loss:  -66.78212088346481 , diff:  1.236257255077362
************ all values are small in this layer **********
layer  19  adv train finish, try to retain  178
test acc: top1 ->  70.244 ; top5 ->  89.618  and loss:  962.6020330190659
forward train acc: top1 ->  68.53125 ; top5 ->  87.1640625  and loss:  264.153928399086
test acc: top1 ->  70.282 ; top5 ->  89.55  and loss:  962.640676677227
forward train acc: top1 ->  68.75 ; top5 ->  86.8125  and loss:  264.175992667675
test acc: top1 ->  70.334 ; top5 ->  89.504  and loss:  964.797345995903
forward train acc: top1 ->  68.9765625 ; top5 ->  87.1328125  and loss:  263.9215840101242
test acc: top1 ->  70.272 ; top5 ->  89.446  and loss:  965.0993239283562
forward train acc: top1 ->  68.3515625 ; top5 ->  86.8359375  and loss:  264.97561979293823
test acc: top1 ->  70.29 ; top5 ->  89.432  and loss:  964.8001468777657
forward train acc: top1 ->  67.734375 ; top5 ->  86.4296875  and loss:  271.80839252471924
test acc: top1 ->  70.36 ; top5 ->  89.526  and loss:  960.3551179170609
forward train acc: top1 ->  68.9609375 ; top5 ->  86.4140625  and loss:  264.57435780763626
test acc: top1 ->  70.428 ; top5 ->  89.582  and loss:  959.2647109031677
forward train acc: top1 ->  69.0546875 ; top5 ->  87.21875  and loss:  262.4673600792885
test acc: top1 ->  70.422 ; top5 ->  89.582  and loss:  959.942334651947
forward train acc: top1 ->  69.1796875 ; top5 ->  87.1640625  and loss:  260.6417667865753
test acc: top1 ->  70.432 ; top5 ->  89.52  and loss:  958.0449860095978
forward train acc: top1 ->  68.484375 ; top5 ->  87.2734375  and loss:  263.49836498498917
test acc: top1 ->  70.548 ; top5 ->  89.63  and loss:  955.8567853569984
forward train acc: top1 ->  68.9296875 ; top5 ->  87.484375  and loss:  260.6861260533333
test acc: top1 ->  70.526 ; top5 ->  89.574  and loss:  957.8834505677223
>>>>>>> reverse layer  19  since performance drop >>>>>>>
==> this epoch:  179 / 256 , inc:  1
---------------- start layer  20  ---------------
adv train loss:  -66.38019001483917 , diff:  66.38019001483917
adv train loss:  -65.72269201278687 , diff:  0.6574980020523071
layer  20  adv train finish, try to retain  228
>>>>>>> reverse layer  20  since no improvement >>>>>>>
---------------- start layer  21  ---------------
adv train loss:  -64.7627602815628 , diff:  64.7627602815628
adv train loss:  -66.81865620613098 , diff:  2.0558959245681763
************ all values are small in this layer **********
layer  21  adv train finish, try to retain  241
test acc: top1 ->  70.79 ; top5 ->  89.69  and loss:  953.816390812397
forward train acc: top1 ->  69.3515625 ; top5 ->  87.5078125  and loss:  259.8732570409775
test acc: top1 ->  70.754 ; top5 ->  89.702  and loss:  956.4804570674896
forward train acc: top1 ->  69.375 ; top5 ->  87.328125  and loss:  256.3721981048584
test acc: top1 ->  70.698 ; top5 ->  89.766  and loss:  956.2887247204781
forward train acc: top1 ->  69.8359375 ; top5 ->  87.6328125  and loss:  255.37606984376907
test acc: top1 ->  70.61 ; top5 ->  89.762  and loss:  953.1317248940468
forward train acc: top1 ->  69.671875 ; top5 ->  88.03125  and loss:  251.66409188508987
test acc: top1 ->  70.71 ; top5 ->  89.78  and loss:  950.6194186210632
forward train acc: top1 ->  69.4609375 ; top5 ->  87.6953125  and loss:  257.77595591545105
test acc: top1 ->  70.79 ; top5 ->  89.77  and loss:  949.344383597374
forward train acc: top1 ->  69.3125 ; top5 ->  87.7578125  and loss:  257.3842008113861
test acc: top1 ->  70.76 ; top5 ->  89.782  and loss:  950.7493194937706
forward train acc: top1 ->  69.609375 ; top5 ->  87.453125  and loss:  257.49668645858765
test acc: top1 ->  70.886 ; top5 ->  89.816  and loss:  949.763419687748
forward train acc: top1 ->  69.46875 ; top5 ->  87.7421875  and loss:  255.57331484556198
test acc: top1 ->  70.894 ; top5 ->  89.778  and loss:  947.9771356582642
forward train acc: top1 ->  69.3671875 ; top5 ->  87.4921875  and loss:  258.28249710798264
test acc: top1 ->  70.91 ; top5 ->  89.816  and loss:  949.9940410852432
forward train acc: top1 ->  69.4296875 ; top5 ->  87.1015625  and loss:  259.7397103905678
test acc: top1 ->  70.966 ; top5 ->  89.826  and loss:  949.5603835582733
>>>>>>> reverse layer  21  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  22  ---------------
adv train loss:  -65.44204246997833 , diff:  65.44204246997833
adv train loss:  -62.5427365899086 , diff:  2.8993058800697327
layer  22  adv train finish, try to retain  189
test acc: top1 ->  70.336 ; top5 ->  89.56  and loss:  965.2935235500336
forward train acc: top1 ->  69.5625 ; top5 ->  87.4375  and loss:  256.04654836654663
test acc: top1 ->  70.294 ; top5 ->  89.57  and loss:  962.2446041107178
forward train acc: top1 ->  68.6015625 ; top5 ->  87.203125  and loss:  261.583377122879
test acc: top1 ->  70.472 ; top5 ->  89.54  and loss:  961.1885482668877
forward train acc: top1 ->  68.3046875 ; top5 ->  86.9765625  and loss:  266.5795489549637
test acc: top1 ->  70.496 ; top5 ->  89.524  and loss:  963.0029580593109
forward train acc: top1 ->  68.7421875 ; top5 ->  87.2265625  and loss:  261.5390228033066
test acc: top1 ->  70.618 ; top5 ->  89.64  and loss:  957.4232369661331
forward train acc: top1 ->  68.6328125 ; top5 ->  86.953125  and loss:  263.2962093949318
test acc: top1 ->  70.62 ; top5 ->  89.654  and loss:  958.8535156846046
forward train acc: top1 ->  69.4609375 ; top5 ->  87.3671875  and loss:  259.00052469968796
test acc: top1 ->  70.56 ; top5 ->  89.652  and loss:  957.3076329827309
forward train acc: top1 ->  69.390625 ; top5 ->  87.6640625  and loss:  256.00896859169006
test acc: top1 ->  70.61 ; top5 ->  89.696  and loss:  958.9365062117577
forward train acc: top1 ->  69.1875 ; top5 ->  87.453125  and loss:  259.986843585968
test acc: top1 ->  70.646 ; top5 ->  89.682  and loss:  956.5857391953468
forward train acc: top1 ->  69.359375 ; top5 ->  87.390625  and loss:  259.5379236340523
test acc: top1 ->  70.676 ; top5 ->  89.752  and loss:  954.0496990680695
forward train acc: top1 ->  69.5546875 ; top5 ->  87.671875  and loss:  254.50584053993225
test acc: top1 ->  70.634 ; top5 ->  89.742  and loss:  953.2806597352028
>>>>>>> reverse layer  22  since performance drop >>>>>>>
==> this epoch:  208 / 256 , inc:  1
---------------- start layer  23  ---------------
adv train loss:  -63.94928938150406 , diff:  63.94928938150406
adv train loss:  -64.891816675663 , diff:  0.9425272941589355
************ all values are small in this layer **********
layer  23  adv train finish, try to retain  186
test acc: top1 ->  70.38 ; top5 ->  89.448  and loss:  972.0744180083275
forward train acc: top1 ->  68.9296875 ; top5 ->  87.15625  and loss:  265.3407847881317
test acc: top1 ->  70.574 ; top5 ->  89.558  and loss:  959.0912104249
forward train acc: top1 ->  68.796875 ; top5 ->  87.2734375  and loss:  264.1578877568245
test acc: top1 ->  70.548 ; top5 ->  89.606  and loss:  961.8036096692085
forward train acc: top1 ->  68.6328125 ; top5 ->  87.0078125  and loss:  262.46159332990646
test acc: top1 ->  70.544 ; top5 ->  89.684  and loss:  961.1606489419937
forward train acc: top1 ->  68.9609375 ; top5 ->  86.640625  and loss:  266.3091977238655
test acc: top1 ->  70.602 ; top5 ->  89.648  and loss:  959.368166744709
forward train acc: top1 ->  69.171875 ; top5 ->  87.3515625  and loss:  257.1262895464897
test acc: top1 ->  70.756 ; top5 ->  89.71  and loss:  953.9151772260666
forward train acc: top1 ->  68.375 ; top5 ->  87.4921875  and loss:  260.871350646019
test acc: top1 ->  70.724 ; top5 ->  89.732  and loss:  953.7999714016914
forward train acc: top1 ->  69.2109375 ; top5 ->  87.4609375  and loss:  256.4401646256447
test acc: top1 ->  70.79 ; top5 ->  89.816  and loss:  953.3014141917229
forward train acc: top1 ->  68.1875 ; top5 ->  86.8359375  and loss:  265.1360132098198
test acc: top1 ->  70.824 ; top5 ->  89.768  and loss:  953.1936937570572
forward train acc: top1 ->  68.796875 ; top5 ->  87.578125  and loss:  257.5731679201126
test acc: top1 ->  70.706 ; top5 ->  89.78  and loss:  952.0670161247253
forward train acc: top1 ->  68.4609375 ; top5 ->  86.875  and loss:  269.10821145772934
test acc: top1 ->  70.718 ; top5 ->  89.746  and loss:  955.8851609230042
>>>>>>> reverse layer  23  since performance drop >>>>>>>
==> this epoch:  187 / 256 , inc:  1
---------------- start layer  24  ---------------
adv train loss:  -66.50295692682266 , diff:  66.50295692682266
adv train loss:  -65.86897218227386 , diff:  0.6339847445487976
************ all values are small in this layer **********
layer  24  adv train finish, try to retain  243
test acc: top1 ->  70.706 ; top5 ->  89.802  and loss:  951.8293288350105
forward train acc: top1 ->  68.578125 ; top5 ->  86.8671875  and loss:  262.8956265449524
test acc: top1 ->  70.774 ; top5 ->  89.764  and loss:  953.7821934819221
forward train acc: top1 ->  69.4296875 ; top5 ->  87.515625  and loss:  255.06900495290756
test acc: top1 ->  70.784 ; top5 ->  89.77  and loss:  954.0535627603531
forward train acc: top1 ->  68.46875 ; top5 ->  87.0  and loss:  264.93316304683685
test acc: top1 ->  70.788 ; top5 ->  89.712  and loss:  952.2833307981491
forward train acc: top1 ->  69.2109375 ; top5 ->  87.4453125  and loss:  261.953291118145
test acc: top1 ->  70.866 ; top5 ->  89.7  and loss:  950.9881073832512
forward train acc: top1 ->  69.421875 ; top5 ->  87.5703125  and loss:  254.64639097452164
test acc: top1 ->  70.742 ; top5 ->  89.778  and loss:  950.7386046648026
forward train acc: top1 ->  69.0 ; top5 ->  87.6328125  and loss:  260.6463448405266
test acc: top1 ->  70.87 ; top5 ->  89.76  and loss:  950.5234780311584
forward train acc: top1 ->  68.4921875 ; top5 ->  87.0546875  and loss:  263.8835098743439
test acc: top1 ->  70.926 ; top5 ->  89.804  and loss:  950.2150567770004
forward train acc: top1 ->  69.1796875 ; top5 ->  87.2265625  and loss:  259.97984915971756
test acc: top1 ->  70.942 ; top5 ->  89.796  and loss:  948.8124597668648
forward train acc: top1 ->  69.7109375 ; top5 ->  87.8828125  and loss:  253.53327757120132
test acc: top1 ->  70.914 ; top5 ->  89.82  and loss:  945.3115210533142
forward train acc: top1 ->  69.53125 ; top5 ->  88.125  and loss:  251.9766213297844
test acc: top1 ->  70.878 ; top5 ->  89.804  and loss:  950.1546555757523
>>>>>>> reverse layer  24  since performance drop >>>>>>>
==> this epoch:  244 / 256 , inc:  1
---------------- start layer  25  ---------------
adv train loss:  -62.70161956548691 , diff:  62.70161956548691
adv train loss:  -66.80149281024933 , diff:  4.099873244762421
************ all values are small in this layer **********
layer  25  adv train finish, try to retain  241
test acc: top1 ->  70.53 ; top5 ->  89.7  and loss:  959.417962551117
forward train acc: top1 ->  69.65625 ; top5 ->  87.5390625  and loss:  254.63610845804214
test acc: top1 ->  70.698 ; top5 ->  89.678  and loss:  957.897271335125
forward train acc: top1 ->  69.0078125 ; top5 ->  87.1640625  and loss:  262.78417217731476
test acc: top1 ->  70.652 ; top5 ->  89.686  and loss:  958.6746038794518
forward train acc: top1 ->  69.46875 ; top5 ->  87.1640625  and loss:  257.9758043885231
test acc: top1 ->  70.8 ; top5 ->  89.72  and loss:  954.203496336937
forward train acc: top1 ->  68.9140625 ; top5 ->  86.8828125  and loss:  258.260456264019
test acc: top1 ->  70.856 ; top5 ->  89.634  and loss:  954.4293449521065
forward train acc: top1 ->  69.2890625 ; top5 ->  87.3515625  and loss:  256.04358798265457
test acc: top1 ->  70.756 ; top5 ->  89.728  and loss:  951.8551126718521
forward train acc: top1 ->  69.4140625 ; top5 ->  87.140625  and loss:  260.35413950681686
test acc: top1 ->  70.772 ; top5 ->  89.704  and loss:  954.4167242050171
forward train acc: top1 ->  69.3671875 ; top5 ->  87.1640625  and loss:  258.41681683063507
test acc: top1 ->  70.778 ; top5 ->  89.764  and loss:  950.397467315197
forward train acc: top1 ->  69.4140625 ; top5 ->  87.71875  and loss:  255.09922188520432
test acc: top1 ->  70.858 ; top5 ->  89.804  and loss:  948.1937197446823
forward train acc: top1 ->  68.9375 ; top5 ->  87.3359375  and loss:  261.07004034519196
test acc: top1 ->  70.942 ; top5 ->  89.722  and loss:  949.8048059940338
forward train acc: top1 ->  69.1328125 ; top5 ->  86.9609375  and loss:  262.4852073788643
test acc: top1 ->  70.872 ; top5 ->  89.784  and loss:  947.6977146863937
>>>>>>> reverse layer  25  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  26  ---------------
adv train loss:  -61.36054480075836 , diff:  61.36054480075836
adv train loss:  -63.84478771686554 , diff:  2.4842429161071777
layer  26  adv train finish, try to retain  499
test acc: top1 ->  70.66 ; top5 ->  89.664  and loss:  951.4103609919548
forward train acc: top1 ->  68.734375 ; top5 ->  87.28125  and loss:  263.0444937944412
test acc: top1 ->  70.61 ; top5 ->  89.642  and loss:  954.2429994344711
forward train acc: top1 ->  69.5625 ; top5 ->  87.5390625  and loss:  253.66136699914932
test acc: top1 ->  70.772 ; top5 ->  89.688  and loss:  954.8492815494537
forward train acc: top1 ->  69.5 ; top5 ->  87.546875  and loss:  257.3760715126991
test acc: top1 ->  70.748 ; top5 ->  89.71  and loss:  952.3871381282806
forward train acc: top1 ->  69.0234375 ; top5 ->  87.1171875  and loss:  260.31102949380875
test acc: top1 ->  70.872 ; top5 ->  89.724  and loss:  954.2189837098122
forward train acc: top1 ->  69.3828125 ; top5 ->  87.234375  and loss:  260.34997820854187
test acc: top1 ->  70.834 ; top5 ->  89.734  and loss:  950.3882462978363
forward train acc: top1 ->  69.53125 ; top5 ->  87.90625  and loss:  254.60236203670502
test acc: top1 ->  70.832 ; top5 ->  89.714  and loss:  952.6892147660255
forward train acc: top1 ->  69.2890625 ; top5 ->  87.5234375  and loss:  258.7035777568817
test acc: top1 ->  70.888 ; top5 ->  89.79  and loss:  951.6515503525734
forward train acc: top1 ->  68.9453125 ; top5 ->  86.921875  and loss:  261.21788388490677
test acc: top1 ->  70.902 ; top5 ->  89.768  and loss:  948.352133333683
forward train acc: top1 ->  69.2734375 ; top5 ->  87.234375  and loss:  257.88639628887177
test acc: top1 ->  70.89 ; top5 ->  89.78  and loss:  950.8356392979622
forward train acc: top1 ->  68.890625 ; top5 ->  87.1171875  and loss:  262.57959401607513
test acc: top1 ->  70.91 ; top5 ->  89.746  and loss:  949.7399045825005
>>>>>>> reverse layer  26  since performance drop >>>>>>>
==> this epoch:  502 / 512 , inc:  1
---------------- start layer  27  ---------------
adv train loss:  -68.82577931880951 , diff:  68.82577931880951
adv train loss:  -66.61285638809204 , diff:  2.2129229307174683
layer  27  adv train finish, try to retain  501
test acc: top1 ->  70.6 ; top5 ->  89.632  and loss:  957.9277021884918
forward train acc: top1 ->  69.828125 ; top5 ->  87.125  and loss:  256.11676836013794
test acc: top1 ->  70.626 ; top5 ->  89.706  and loss:  957.4044759273529
forward train acc: top1 ->  68.609375 ; top5 ->  87.2265625  and loss:  261.05816328525543
test acc: top1 ->  70.636 ; top5 ->  89.806  and loss:  954.990571975708
forward train acc: top1 ->  69.359375 ; top5 ->  87.7109375  and loss:  253.64790660142899
test acc: top1 ->  70.716 ; top5 ->  89.76  and loss:  952.9512956142426
forward train acc: top1 ->  68.71875 ; top5 ->  87.0546875  and loss:  261.0646630525589
test acc: top1 ->  70.744 ; top5 ->  89.836  and loss:  952.3540226817131
forward train acc: top1 ->  69.28125 ; top5 ->  87.546875  and loss:  256.46908140182495
test acc: top1 ->  70.704 ; top5 ->  89.776  and loss:  952.3868902921677
forward train acc: top1 ->  69.234375 ; top5 ->  86.8046875  and loss:  261.59390008449554
test acc: top1 ->  70.726 ; top5 ->  89.808  and loss:  954.3290829062462
forward train acc: top1 ->  68.90625 ; top5 ->  87.1796875  and loss:  261.32445108890533
test acc: top1 ->  70.78 ; top5 ->  89.856  and loss:  951.9869797825813
forward train acc: top1 ->  68.9609375 ; top5 ->  87.28125  and loss:  259.0497888326645
test acc: top1 ->  70.794 ; top5 ->  89.84  and loss:  952.1934817433357
forward train acc: top1 ->  69.234375 ; top5 ->  87.265625  and loss:  257.83180248737335
test acc: top1 ->  70.792 ; top5 ->  89.78  and loss:  951.7428345680237
forward train acc: top1 ->  68.640625 ; top5 ->  86.8515625  and loss:  265.77415788173676
test acc: top1 ->  70.782 ; top5 ->  89.84  and loss:  948.6338393688202
>>>>>>> reverse layer  27  since performance drop >>>>>>>
==> this epoch:  505 / 512 , inc:  1
---------------- start layer  28  ---------------
adv train loss:  -67.39966893196106 , diff:  67.39966893196106
adv train loss:  -63.77055633068085 , diff:  3.6291126012802124
layer  28  adv train finish, try to retain  508
>>>>>>> reverse layer  28  since no improvement >>>>>>>
---------------- start layer  29  ---------------
adv train loss:  -66.75367468595505 , diff:  66.75367468595505
adv train loss:  -68.2618824839592 , diff:  1.5082077980041504
layer  29  adv train finish, try to retain  503
>>>>>>> reverse layer  29  since no improvement >>>>>>>
---------------- start layer  30  ---------------
adv train loss:  -63.70273643732071 , diff:  63.70273643732071
adv train loss:  -67.06595104932785 , diff:  3.363214612007141
layer  30  adv train finish, try to retain  507
>>>>>>> reverse layer  30  since no improvement >>>>>>>
---------------- start layer  31  ---------------
adv train loss:  -63.940532982349396 , diff:  63.940532982349396
adv train loss:  -67.29792362451553 , diff:  3.3573906421661377
************ all values are small in this layer **********
layer  31  adv train finish, try to retain  500
test acc: top1 ->  70.622 ; top5 ->  89.612  and loss:  962.9005784988403
forward train acc: top1 ->  68.9609375 ; top5 ->  87.03125  and loss:  261.48191463947296
test acc: top1 ->  70.682 ; top5 ->  89.658  and loss:  957.5289719104767
forward train acc: top1 ->  68.4921875 ; top5 ->  87.015625  and loss:  262.9420145750046
test acc: top1 ->  70.696 ; top5 ->  89.588  and loss:  959.5617843270302
forward train acc: top1 ->  69.3359375 ; top5 ->  87.6015625  and loss:  257.53348565101624
test acc: top1 ->  70.54 ; top5 ->  89.668  and loss:  959.24542927742
forward train acc: top1 ->  69.5390625 ; top5 ->  87.8125  and loss:  253.49513798952103
test acc: top1 ->  70.712 ; top5 ->  89.766  and loss:  953.5009788870811
forward train acc: top1 ->  69.984375 ; top5 ->  87.4296875  and loss:  255.67947047948837
test acc: top1 ->  70.81 ; top5 ->  89.728  and loss:  953.7539947032928
forward train acc: top1 ->  68.9296875 ; top5 ->  86.8125  and loss:  262.5488752126694
test acc: top1 ->  70.794 ; top5 ->  89.774  and loss:  951.0541645288467
forward train acc: top1 ->  69.453125 ; top5 ->  87.71875  and loss:  256.95206117630005
test acc: top1 ->  70.754 ; top5 ->  89.698  and loss:  952.8008933663368
forward train acc: top1 ->  69.40625 ; top5 ->  87.3984375  and loss:  260.6183277964592
test acc: top1 ->  70.85 ; top5 ->  89.82  and loss:  950.2046305537224
forward train acc: top1 ->  68.96875 ; top5 ->  86.890625  and loss:  263.82401609420776
test acc: top1 ->  70.85 ; top5 ->  89.772  and loss:  951.5640876889229
forward train acc: top1 ->  69.4140625 ; top5 ->  87.5234375  and loss:  256.1481720805168
test acc: top1 ->  70.784 ; top5 ->  89.776  and loss:  953.784322142601
>>>>>>> reverse layer  31  since performance drop >>>>>>>
==> this epoch:  501 / 512 , inc:  1
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.10125, 0.0759375, 0.0006674194335937501, 0.007119140625000001, 0.018984375, 0.2025, 0.0008898925781250001, 8.342742919921877e-05, 0.0035595703125000005, 0.0008898925781250001, 0.0094921875, 0.002373046875, 0.00474609375, 0.03796875, 4.1713714599609384e-05, 8.342742919921877e-05, 0.00044494628906250006, 0.00044494628906250006, 0.0011865234375, 0.00474609375, 0.0011865234375, 8.342742919921877e-05, 0.0017797851562500002, 0.00474609375, 0.00016685485839843753, 0.00016685485839843753, 4.1713714599609384e-05, 4.1713714599609384e-05, 0.00011123657226562501, 0.00011123657226562501, 0.00011123657226562501, 8.342742919921877e-05]  wait [1, 4, 4, 4, 2, 2, 2, 2, 4, 2, 2, 0, 1, 4, 2, 3, 2, 2, 0, 2, 0, 3, 4, 2, 4, 4, 3, 3, 1, 1, 1, 4]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 3
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  16  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -64.72447967529297 , diff:  64.72447967529297
adv train loss:  -67.36721158027649 , diff:  2.6427319049835205
layer  0  adv train finish, try to retain  37
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
### skip layer  1 wait:  4  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
adv train loss:  -61.865722596645355 , diff:  61.865722596645355
adv train loss:  -62.65845328569412 , diff:  0.7927306890487671
layer  4  adv train finish, try to retain  30
test acc: top1 ->  68.076 ; top5 ->  88.244  and loss:  1038.5510971546173
forward train acc: top1 ->  68.6484375 ; top5 ->  86.96875  and loss:  263.5017080307007
test acc: top1 ->  70.36 ; top5 ->  89.568  and loss:  966.486982345581
forward train acc: top1 ->  68.984375 ; top5 ->  87.0234375  and loss:  264.74782478809357
test acc: top1 ->  70.396 ; top5 ->  89.58  and loss:  965.8050404191017
forward train acc: top1 ->  68.203125 ; top5 ->  87.125  and loss:  266.03664195537567
test acc: top1 ->  70.426 ; top5 ->  89.626  and loss:  962.9651619195938
forward train acc: top1 ->  69.3203125 ; top5 ->  87.078125  and loss:  261.3948821425438
test acc: top1 ->  70.608 ; top5 ->  89.598  and loss:  959.3651124835014
forward train acc: top1 ->  69.4375 ; top5 ->  87.3125  and loss:  258.865678191185
test acc: top1 ->  70.462 ; top5 ->  89.668  and loss:  962.4807522892952
forward train acc: top1 ->  68.703125 ; top5 ->  86.84375  and loss:  263.5110077857971
test acc: top1 ->  70.574 ; top5 ->  89.66  and loss:  960.6804178357124
forward train acc: top1 ->  68.671875 ; top5 ->  87.1796875  and loss:  262.28664284944534
test acc: top1 ->  70.602 ; top5 ->  89.664  and loss:  957.1225538849831
forward train acc: top1 ->  69.2578125 ; top5 ->  87.0  and loss:  257.1205803155899
test acc: top1 ->  70.604 ; top5 ->  89.724  and loss:  957.4845266342163
forward train acc: top1 ->  68.5390625 ; top5 ->  86.734375  and loss:  267.47918635606766
test acc: top1 ->  70.64 ; top5 ->  89.686  and loss:  957.7821900248528
forward train acc: top1 ->  69.1171875 ; top5 ->  87.6484375  and loss:  256.39938521385193
test acc: top1 ->  70.744 ; top5 ->  89.748  and loss:  956.2839124798775
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  40 / 64 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -67.01829534769058 , diff:  67.01829534769058
adv train loss:  -67.53599292039871 , diff:  0.5176975727081299
************ all values are small in this layer **********
layer  5  adv train finish, try to retain  28
test acc: top1 ->  66.892 ; top5 ->  87.226  and loss:  1088.9927378892899
forward train acc: top1 ->  68.9375 ; top5 ->  87.453125  and loss:  262.24272602796555
test acc: top1 ->  70.41 ; top5 ->  89.546  and loss:  966.2833546996117
forward train acc: top1 ->  68.7734375 ; top5 ->  87.015625  and loss:  261.8372086286545
test acc: top1 ->  70.496 ; top5 ->  89.662  and loss:  963.7226660251617
forward train acc: top1 ->  69.046875 ; top5 ->  87.2734375  and loss:  259.09812968969345
test acc: top1 ->  70.418 ; top5 ->  89.548  and loss:  967.0305374264717
forward train acc: top1 ->  68.9140625 ; top5 ->  87.25  and loss:  260.1443917155266
test acc: top1 ->  70.522 ; top5 ->  89.606  and loss:  962.2536936402321
forward train acc: top1 ->  68.8671875 ; top5 ->  87.1171875  and loss:  260.62997275590897
test acc: top1 ->  70.544 ; top5 ->  89.684  and loss:  957.7853099703789
forward train acc: top1 ->  68.671875 ; top5 ->  87.3125  and loss:  264.7103880047798
test acc: top1 ->  70.72 ; top5 ->  89.73  and loss:  954.2250242233276
forward train acc: top1 ->  69.2734375 ; top5 ->  87.1875  and loss:  260.7849682569504
test acc: top1 ->  70.65 ; top5 ->  89.686  and loss:  956.4819369316101
forward train acc: top1 ->  69.703125 ; top5 ->  87.609375  and loss:  256.09542137384415
test acc: top1 ->  70.666 ; top5 ->  89.728  and loss:  954.8487071990967
forward train acc: top1 ->  69.171875 ; top5 ->  87.0703125  and loss:  264.3509908914566
test acc: top1 ->  70.632 ; top5 ->  89.748  and loss:  953.8697975873947
forward train acc: top1 ->  69.34375 ; top5 ->  87.2734375  and loss:  259.9364451766014
test acc: top1 ->  70.77 ; top5 ->  89.77  and loss:  953.3352534770966
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  29 / 64 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -66.47409242391586 , diff:  66.47409242391586
adv train loss:  -66.43438798189163 , diff:  0.03970444202423096
layer  6  adv train finish, try to retain  111
test acc: top1 ->  69.004 ; top5 ->  88.964  and loss:  1005.3326395750046
forward train acc: top1 ->  68.6953125 ; top5 ->  86.9609375  and loss:  265.3635119795799
test acc: top1 ->  70.45 ; top5 ->  89.656  and loss:  959.2471832633018
forward train acc: top1 ->  68.59375 ; top5 ->  86.6015625  and loss:  263.7953188419342
test acc: top1 ->  70.454 ; top5 ->  89.628  and loss:  962.5499071478844
forward train acc: top1 ->  68.8984375 ; top5 ->  87.5078125  and loss:  258.21574598550797
test acc: top1 ->  70.53 ; top5 ->  89.676  and loss:  958.5359545350075
forward train acc: top1 ->  68.390625 ; top5 ->  86.7578125  and loss:  263.38521736860275
test acc: top1 ->  70.488 ; top5 ->  89.726  and loss:  958.2464064359665
forward train acc: top1 ->  69.328125 ; top5 ->  87.4140625  and loss:  258.9131401181221
test acc: top1 ->  70.596 ; top5 ->  89.716  and loss:  954.2162994742393
forward train acc: top1 ->  69.625 ; top5 ->  87.109375  and loss:  256.1479611992836
test acc: top1 ->  70.61 ; top5 ->  89.786  and loss:  956.0411887764931
forward train acc: top1 ->  68.9296875 ; top5 ->  87.28125  and loss:  259.2547336816788
test acc: top1 ->  70.668 ; top5 ->  89.756  and loss:  952.4139154553413
forward train acc: top1 ->  69.15625 ; top5 ->  87.421875  and loss:  258.53708505630493
test acc: top1 ->  70.672 ; top5 ->  89.736  and loss:  952.0343941450119
forward train acc: top1 ->  68.390625 ; top5 ->  87.0625  and loss:  261.0066338777542
test acc: top1 ->  70.756 ; top5 ->  89.714  and loss:  953.7808565497398
forward train acc: top1 ->  69.3515625 ; top5 ->  87.3828125  and loss:  260.84820622205734
test acc: top1 ->  70.726 ; top5 ->  89.79  and loss:  949.439572095871
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  112 / 128 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -63.38452476263046 , diff:  63.38452476263046
adv train loss:  -61.46367400884628 , diff:  1.9208507537841797
layer  7  adv train finish, try to retain  124
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
### skip layer  8 wait:  4  ###
---------------- start layer  9  ---------------
adv train loss:  -65.13330614566803 , diff:  65.13330614566803
adv train loss:  -65.62524336576462 , diff:  0.49193722009658813
layer  9  adv train finish, try to retain  95
test acc: top1 ->  69.136 ; top5 ->  88.998  and loss:  999.8665933012962
forward train acc: top1 ->  68.0546875 ; top5 ->  86.3203125  and loss:  270.2538958787918
test acc: top1 ->  70.24 ; top5 ->  89.578  and loss:  969.7398255467415
forward train acc: top1 ->  68.3125 ; top5 ->  86.8046875  and loss:  268.07977455854416
test acc: top1 ->  70.228 ; top5 ->  89.564  and loss:  965.2269697785378
forward train acc: top1 ->  68.9609375 ; top5 ->  87.1484375  and loss:  262.57531744241714
test acc: top1 ->  70.288 ; top5 ->  89.66  and loss:  964.6455752849579
forward train acc: top1 ->  68.953125 ; top5 ->  86.9921875  and loss:  261.86315858364105
test acc: top1 ->  70.348 ; top5 ->  89.646  and loss:  961.2987604141235
forward train acc: top1 ->  69.4453125 ; top5 ->  86.8984375  and loss:  261.52590334415436
test acc: top1 ->  70.34 ; top5 ->  89.71  and loss:  961.6844447851181
forward train acc: top1 ->  68.8515625 ; top5 ->  87.15625  and loss:  261.3359851241112
test acc: top1 ->  70.394 ; top5 ->  89.698  and loss:  959.0157709121704
forward train acc: top1 ->  68.71875 ; top5 ->  86.640625  and loss:  264.4935402870178
test acc: top1 ->  70.42 ; top5 ->  89.696  and loss:  957.4284310340881
forward train acc: top1 ->  68.640625 ; top5 ->  87.3828125  and loss:  260.5728633403778
test acc: top1 ->  70.498 ; top5 ->  89.644  and loss:  955.9482552409172
forward train acc: top1 ->  69.1875 ; top5 ->  87.4375  and loss:  256.0794914364815
test acc: top1 ->  70.448 ; top5 ->  89.736  and loss:  958.1146341562271
forward train acc: top1 ->  68.1328125 ; top5 ->  86.5546875  and loss:  269.65414077043533
test acc: top1 ->  70.544 ; top5 ->  89.722  and loss:  958.2368069887161
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  115 / 128 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -63.17381834983826 , diff:  63.17381834983826
adv train loss:  -64.75868690013885 , diff:  1.5848685503005981
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  86
test acc: top1 ->  69.644 ; top5 ->  89.066  and loss:  989.7386772036552
forward train acc: top1 ->  68.65625 ; top5 ->  86.8203125  and loss:  265.17041593790054
test acc: top1 ->  70.41 ; top5 ->  89.544  and loss:  964.0103144645691
forward train acc: top1 ->  69.796875 ; top5 ->  87.5  and loss:  254.50519782304764
test acc: top1 ->  70.462 ; top5 ->  89.512  and loss:  964.2288352847099
forward train acc: top1 ->  68.0 ; top5 ->  86.6640625  and loss:  269.42101526260376
test acc: top1 ->  70.382 ; top5 ->  89.698  and loss:  962.8500193953514
forward train acc: top1 ->  68.875 ; top5 ->  87.0546875  and loss:  264.3879216313362
test acc: top1 ->  70.48 ; top5 ->  89.722  and loss:  962.1347465515137
forward train acc: top1 ->  69.265625 ; top5 ->  87.390625  and loss:  259.5848619937897
test acc: top1 ->  70.42 ; top5 ->  89.676  and loss:  957.875568985939
forward train acc: top1 ->  69.359375 ; top5 ->  87.25  and loss:  257.8580794930458
test acc: top1 ->  70.506 ; top5 ->  89.668  and loss:  959.9709158539772
forward train acc: top1 ->  68.6640625 ; top5 ->  86.9296875  and loss:  261.8930571079254
test acc: top1 ->  70.578 ; top5 ->  89.74  and loss:  958.7437882423401
forward train acc: top1 ->  68.8203125 ; top5 ->  87.390625  and loss:  261.6307365298271
test acc: top1 ->  70.614 ; top5 ->  89.78  and loss:  955.6686275601387
forward train acc: top1 ->  69.546875 ; top5 ->  87.328125  and loss:  260.1656737923622
test acc: top1 ->  70.59 ; top5 ->  89.742  and loss:  957.1671615242958
forward train acc: top1 ->  68.6953125 ; top5 ->  87.2578125  and loss:  263.690422475338
test acc: top1 ->  70.618 ; top5 ->  89.732  and loss:  954.6815748810768
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -62.22889882326126 , diff:  62.22889882326126
adv train loss:  -61.67309367656708 , diff:  0.5558051466941833
layer  11  adv train finish, try to retain  104
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -64.57770466804504 , diff:  64.57770466804504
adv train loss:  -63.959375858306885 , diff:  0.6183288097381592
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  97
test acc: top1 ->  69.804 ; top5 ->  89.314  and loss:  980.9145402908325
forward train acc: top1 ->  69.3515625 ; top5 ->  87.2421875  and loss:  258.902394592762
test acc: top1 ->  70.418 ; top5 ->  89.58  and loss:  964.467344045639
forward train acc: top1 ->  69.6328125 ; top5 ->  87.4140625  and loss:  256.23366743326187
test acc: top1 ->  70.6 ; top5 ->  89.724  and loss:  956.0704346895218
forward train acc: top1 ->  68.7734375 ; top5 ->  87.1875  and loss:  261.71671134233475
test acc: top1 ->  70.588 ; top5 ->  89.708  and loss:  958.0153762102127
forward train acc: top1 ->  69.1015625 ; top5 ->  87.0703125  and loss:  260.1887887120247
test acc: top1 ->  70.676 ; top5 ->  89.702  and loss:  958.1339073181152
forward train acc: top1 ->  69.0390625 ; top5 ->  87.1171875  and loss:  260.38468277454376
test acc: top1 ->  70.738 ; top5 ->  89.774  and loss:  954.1693620681763
forward train acc: top1 ->  68.9765625 ; top5 ->  87.125  and loss:  261.6063285470009
test acc: top1 ->  70.686 ; top5 ->  89.768  and loss:  954.5682817101479
forward train acc: top1 ->  68.890625 ; top5 ->  86.8515625  and loss:  264.0761102437973
test acc: top1 ->  70.736 ; top5 ->  89.764  and loss:  954.9602520465851
forward train acc: top1 ->  68.7890625 ; top5 ->  87.1171875  and loss:  262.2689768075943
test acc: top1 ->  70.71 ; top5 ->  89.78  and loss:  953.1909056305885
forward train acc: top1 ->  68.9296875 ; top5 ->  87.1640625  and loss:  260.31591379642487
test acc: top1 ->  70.698 ; top5 ->  89.818  and loss:  951.9643775820732
forward train acc: top1 ->  69.3515625 ; top5 ->  87.28125  and loss:  260.87027275562286
test acc: top1 ->  70.702 ; top5 ->  89.78  and loss:  954.5893703103065
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  98 / 128 , inc:  1
---------------- start layer  13  ---------------
### skip layer  13 wait:  4  ###
---------------- start layer  14  ---------------
adv train loss:  -63.36088848114014 , diff:  63.36088848114014
adv train loss:  -62.98485088348389 , diff:  0.37603759765625
layer  14  adv train finish, try to retain  254
>>>>>>> reverse layer  14  since no improvement >>>>>>>
---------------- start layer  15  ---------------
### skip layer  15 wait:  3  ###
---------------- start layer  16  ---------------
adv train loss:  -62.89128404855728 , diff:  62.89128404855728
adv train loss:  -63.28505653142929 , diff:  0.3937724828720093
layer  16  adv train finish, try to retain  229
test acc: top1 ->  70.404 ; top5 ->  89.702  and loss:  960.7115978598595
forward train acc: top1 ->  69.0625 ; top5 ->  87.0703125  and loss:  261.69707679748535
test acc: top1 ->  70.544 ; top5 ->  89.668  and loss:  959.4338965415955
forward train acc: top1 ->  69.3203125 ; top5 ->  87.40625  and loss:  255.8992492556572
test acc: top1 ->  70.502 ; top5 ->  89.726  and loss:  954.131126999855
forward train acc: top1 ->  68.453125 ; top5 ->  86.921875  and loss:  266.1067394018173
test acc: top1 ->  70.56 ; top5 ->  89.7  and loss:  954.3460540175438
forward train acc: top1 ->  69.09375 ; top5 ->  87.1875  and loss:  259.53770196437836
test acc: top1 ->  70.666 ; top5 ->  89.756  and loss:  953.0939357876778
forward train acc: top1 ->  69.375 ; top5 ->  87.6328125  and loss:  255.29447656869888
test acc: top1 ->  70.69 ; top5 ->  89.796  and loss:  951.3050622940063
forward train acc: top1 ->  68.65625 ; top5 ->  87.2421875  and loss:  262.3260722756386
test acc: top1 ->  70.692 ; top5 ->  89.77  and loss:  955.511173427105
forward train acc: top1 ->  69.375 ; top5 ->  87.109375  and loss:  260.2685697078705
test acc: top1 ->  70.682 ; top5 ->  89.78  and loss:  955.0299953222275
forward train acc: top1 ->  68.6328125 ; top5 ->  86.953125  and loss:  263.81814879179
test acc: top1 ->  70.728 ; top5 ->  89.8  and loss:  951.4029612541199
forward train acc: top1 ->  69.8671875 ; top5 ->  87.5625  and loss:  254.6432750225067
test acc: top1 ->  70.742 ; top5 ->  89.752  and loss:  952.736149251461
forward train acc: top1 ->  68.484375 ; top5 ->  87.328125  and loss:  260.469571352005
test acc: top1 ->  70.792 ; top5 ->  89.796  and loss:  951.9820072054863
>>>>>>> reverse layer  16  since performance drop >>>>>>>
==> this epoch:  239 / 256 , inc:  1
---------------- start layer  17  ---------------
adv train loss:  -60.49022960662842 , diff:  60.49022960662842
adv train loss:  -64.42130035161972 , diff:  3.9310707449913025
layer  17  adv train finish, try to retain  229
test acc: top1 ->  70.492 ; top5 ->  89.628  and loss:  957.7437050938606
forward train acc: top1 ->  69.4453125 ; top5 ->  87.3515625  and loss:  256.78238421678543
test acc: top1 ->  70.85 ; top5 ->  89.796  and loss:  954.882088124752
forward train acc: top1 ->  68.6953125 ; top5 ->  87.171875  and loss:  263.93000650405884
test acc: top1 ->  70.74 ; top5 ->  89.74  and loss:  954.7211453914642
forward train acc: top1 ->  68.6015625 ; top5 ->  86.90625  and loss:  265.18251889944077
test acc: top1 ->  70.738 ; top5 ->  89.784  and loss:  953.9429495334625
forward train acc: top1 ->  68.4375 ; top5 ->  87.2578125  and loss:  260.9586101770401
test acc: top1 ->  70.774 ; top5 ->  89.766  and loss:  953.9700812101364
forward train acc: top1 ->  68.3828125 ; top5 ->  87.1953125  and loss:  261.8549152612686
test acc: top1 ->  70.83 ; top5 ->  89.798  and loss:  952.4965284466743
forward train acc: top1 ->  69.5703125 ; top5 ->  87.21875  and loss:  259.0883814692497
test acc: top1 ->  70.87 ; top5 ->  89.854  and loss:  951.2179526090622
forward train acc: top1 ->  69.2421875 ; top5 ->  87.515625  and loss:  260.08266139030457
test acc: top1 ->  70.796 ; top5 ->  89.816  and loss:  951.8800395727158
forward train acc: top1 ->  69.2890625 ; top5 ->  87.4765625  and loss:  258.5546700358391
test acc: top1 ->  70.912 ; top5 ->  89.902  and loss:  949.8407555222511
forward train acc: top1 ->  69.984375 ; top5 ->  87.8046875  and loss:  254.55448853969574
test acc: top1 ->  70.988 ; top5 ->  89.856  and loss:  950.6768327951431
forward train acc: top1 ->  69.3125 ; top5 ->  87.8515625  and loss:  257.57197791337967
test acc: top1 ->  70.896 ; top5 ->  89.84  and loss:  950.2125573158264
>>>>>>> reverse layer  17  since performance drop >>>>>>>
==> this epoch:  230 / 256 , inc:  1
---------------- start layer  18  ---------------
adv train loss:  -64.41810244321823 , diff:  64.41810244321823
adv train loss:  -66.54445940256119 , diff:  2.1263569593429565
layer  18  adv train finish, try to retain  219
test acc: top1 ->  70.576 ; top5 ->  89.73  and loss:  954.1329712867737
forward train acc: top1 ->  68.6796875 ; top5 ->  87.375  and loss:  260.9109479188919
test acc: top1 ->  70.608 ; top5 ->  89.788  and loss:  958.2362752556801
forward train acc: top1 ->  68.8125 ; top5 ->  87.3984375  and loss:  261.37193381786346
test acc: top1 ->  70.712 ; top5 ->  89.73  and loss:  958.8986249566078
forward train acc: top1 ->  69.671875 ; top5 ->  86.9296875  and loss:  259.70326793193817
test acc: top1 ->  70.586 ; top5 ->  89.756  and loss:  956.898192346096
forward train acc: top1 ->  69.3515625 ; top5 ->  87.4375  and loss:  256.0841282606125
test acc: top1 ->  70.724 ; top5 ->  89.776  and loss:  955.5829548835754
forward train acc: top1 ->  69.4453125 ; top5 ->  87.265625  and loss:  260.33309572935104
test acc: top1 ->  70.692 ; top5 ->  89.714  and loss:  954.0515332818031
forward train acc: top1 ->  69.953125 ; top5 ->  87.5234375  and loss:  253.95357191562653
test acc: top1 ->  70.664 ; top5 ->  89.742  and loss:  956.7116314768791
forward train acc: top1 ->  69.03125 ; top5 ->  87.140625  and loss:  258.6770054101944
test acc: top1 ->  70.754 ; top5 ->  89.826  and loss:  951.7773952484131
forward train acc: top1 ->  69.3671875 ; top5 ->  87.046875  and loss:  259.1861986517906
test acc: top1 ->  70.808 ; top5 ->  89.85  and loss:  952.3728892803192
forward train acc: top1 ->  69.453125 ; top5 ->  87.6015625  and loss:  254.73747551441193
test acc: top1 ->  70.754 ; top5 ->  89.838  and loss:  950.0042278766632
forward train acc: top1 ->  68.921875 ; top5 ->  87.546875  and loss:  257.57192301750183
test acc: top1 ->  70.86 ; top5 ->  89.842  and loss:  948.9024320840836
>>>>>>> reverse layer  18  since performance drop >>>>>>>
==> this epoch:  235 / 256 , inc:  1
---------------- start layer  19  ---------------
adv train loss:  -65.68974143266678 , diff:  65.68974143266678
adv train loss:  -65.22846341133118 , diff:  0.4612780213356018
************ all values are small in this layer **********
layer  19  adv train finish, try to retain  178
test acc: top1 ->  70.352 ; top5 ->  89.392  and loss:  970.9136164784431
forward train acc: top1 ->  68.9375 ; top5 ->  86.828125  and loss:  262.8782336115837
test acc: top1 ->  70.496 ; top5 ->  89.666  and loss:  962.5139439702034
forward train acc: top1 ->  69.0703125 ; top5 ->  86.8125  and loss:  264.72501170635223
test acc: top1 ->  70.542 ; top5 ->  89.606  and loss:  961.5519458651543
forward train acc: top1 ->  69.515625 ; top5 ->  87.40625  and loss:  256.7307074666023
test acc: top1 ->  70.576 ; top5 ->  89.604  and loss:  963.6068466305733
forward train acc: top1 ->  69.203125 ; top5 ->  87.0859375  and loss:  264.1575751900673
test acc: top1 ->  70.62 ; top5 ->  89.72  and loss:  960.8806420564651
forward train acc: top1 ->  68.421875 ; top5 ->  86.71875  and loss:  265.94133085012436
test acc: top1 ->  70.702 ; top5 ->  89.718  and loss:  958.2558345198631
forward train acc: top1 ->  68.9375 ; top5 ->  86.9765625  and loss:  264.8842048048973
test acc: top1 ->  70.604 ; top5 ->  89.776  and loss:  955.6737233996391
forward train acc: top1 ->  69.1640625 ; top5 ->  87.4140625  and loss:  258.6713908314705
test acc: top1 ->  70.592 ; top5 ->  89.72  and loss:  958.0209958553314
forward train acc: top1 ->  68.984375 ; top5 ->  87.03125  and loss:  264.73473566770554
test acc: top1 ->  70.61 ; top5 ->  89.812  and loss:  953.7919843196869
forward train acc: top1 ->  69.34375 ; top5 ->  87.453125  and loss:  257.18166649341583
test acc: top1 ->  70.738 ; top5 ->  89.79  and loss:  955.1237644553185
forward train acc: top1 ->  68.75 ; top5 ->  87.140625  and loss:  261.33341109752655
test acc: top1 ->  70.774 ; top5 ->  89.846  and loss:  954.786824285984
>>>>>>> reverse layer  19  since performance drop >>>>>>>
==> this epoch:  179 / 256 , inc:  1
---------------- start layer  20  ---------------
adv train loss:  -66.5991320014 , diff:  66.5991320014
adv train loss:  -61.73617726564407 , diff:  4.86295473575592
layer  20  adv train finish, try to retain  220
test acc: top1 ->  70.636 ; top5 ->  89.698  and loss:  966.3274555206299
forward train acc: top1 ->  69.0625 ; top5 ->  87.1171875  and loss:  261.66455113887787
test acc: top1 ->  70.66 ; top5 ->  89.692  and loss:  958.2962116599083
forward train acc: top1 ->  68.78125 ; top5 ->  87.140625  and loss:  264.4166359901428
test acc: top1 ->  70.696 ; top5 ->  89.704  and loss:  954.8063792586327
forward train acc: top1 ->  68.828125 ; top5 ->  87.1796875  and loss:  261.77336382865906
test acc: top1 ->  70.684 ; top5 ->  89.738  and loss:  955.1478751301765
forward train acc: top1 ->  69.359375 ; top5 ->  87.5859375  and loss:  256.983831346035
test acc: top1 ->  70.752 ; top5 ->  89.766  and loss:  952.3933589458466
forward train acc: top1 ->  68.84375 ; top5 ->  87.640625  and loss:  259.9982624053955
test acc: top1 ->  70.688 ; top5 ->  89.822  and loss:  954.074544608593
forward train acc: top1 ->  68.3125 ; top5 ->  87.0  and loss:  265.0526769757271
test acc: top1 ->  70.836 ; top5 ->  89.858  and loss:  952.845456302166
forward train acc: top1 ->  69.6640625 ; top5 ->  87.40625  and loss:  258.0877859592438
test acc: top1 ->  70.69 ; top5 ->  89.862  and loss:  950.6613017320633
forward train acc: top1 ->  69.4140625 ; top5 ->  87.34375  and loss:  257.7891526222229
test acc: top1 ->  70.694 ; top5 ->  89.888  and loss:  951.1164292693138
forward train acc: top1 ->  68.890625 ; top5 ->  87.03125  and loss:  261.51737558841705
test acc: top1 ->  70.734 ; top5 ->  89.87  and loss:  951.1125910878181
forward train acc: top1 ->  69.28125 ; top5 ->  87.1640625  and loss:  259.322509765625
test acc: top1 ->  70.8 ; top5 ->  89.842  and loss:  948.7015864849091
>>>>>>> reverse layer  20  since performance drop >>>>>>>
==> this epoch:  225 / 256 , inc:  1
---------------- start layer  21  ---------------
### skip layer  21 wait:  3  ###
---------------- start layer  22  ---------------
### skip layer  22 wait:  4  ###
---------------- start layer  23  ---------------
adv train loss:  -65.14148372411728 , diff:  65.14148372411728
adv train loss:  -68.25834602117538 , diff:  3.1168622970581055
************ all values are small in this layer **********
layer  23  adv train finish, try to retain  186
test acc: top1 ->  70.568 ; top5 ->  89.726  and loss:  959.8187425136566
forward train acc: top1 ->  69.3359375 ; top5 ->  87.4765625  and loss:  257.83833807706833
test acc: top1 ->  70.67 ; top5 ->  89.812  and loss:  957.4667779803276
forward train acc: top1 ->  69.2421875 ; top5 ->  87.265625  and loss:  259.0434910058975
test acc: top1 ->  70.618 ; top5 ->  89.756  and loss:  960.226180434227
forward train acc: top1 ->  68.0859375 ; top5 ->  86.5859375  and loss:  268.81009244918823
test acc: top1 ->  70.57 ; top5 ->  89.7  and loss:  955.870799779892
forward train acc: top1 ->  69.40625 ; top5 ->  87.2265625  and loss:  258.34205335378647
test acc: top1 ->  70.7 ; top5 ->  89.74  and loss:  954.464881837368
forward train acc: top1 ->  68.6953125 ; top5 ->  87.1796875  and loss:  264.7157881259918
test acc: top1 ->  70.704 ; top5 ->  89.78  and loss:  952.7123987674713
forward train acc: top1 ->  68.890625 ; top5 ->  87.1015625  and loss:  260.6548390388489
test acc: top1 ->  70.646 ; top5 ->  89.7  and loss:  955.8507362008095
forward train acc: top1 ->  68.65625 ; top5 ->  87.2890625  and loss:  260.7755352258682
test acc: top1 ->  70.752 ; top5 ->  89.796  and loss:  953.7483300566673
forward train acc: top1 ->  69.296875 ; top5 ->  87.5078125  and loss:  257.0079486966133
test acc: top1 ->  70.834 ; top5 ->  89.782  and loss:  950.8775622844696
forward train acc: top1 ->  69.84375 ; top5 ->  87.9453125  and loss:  253.60690373182297
test acc: top1 ->  70.744 ; top5 ->  89.786  and loss:  953.7228055000305
forward train acc: top1 ->  69.3359375 ; top5 ->  87.09375  and loss:  258.30293959379196
test acc: top1 ->  70.768 ; top5 ->  89.74  and loss:  950.800582587719
>>>>>>> reverse layer  23  since performance drop >>>>>>>
==> this epoch:  187 / 256 , inc:  1
---------------- start layer  24  ---------------
### skip layer  24 wait:  4  ###
---------------- start layer  25  ---------------
### skip layer  25 wait:  4  ###
---------------- start layer  26  ---------------
### skip layer  26 wait:  3  ###
---------------- start layer  27  ---------------
### skip layer  27 wait:  3  ###
---------------- start layer  28  ---------------
adv train loss:  -66.56139189004898 , diff:  66.56139189004898
adv train loss:  -63.036255061626434 , diff:  3.5251368284225464
************ all values are small in this layer **********
layer  28  adv train finish, try to retain  502
test acc: top1 ->  70.782 ; top5 ->  89.838  and loss:  950.863586962223
forward train acc: top1 ->  68.828125 ; top5 ->  87.25  and loss:  260.45283061265945
test acc: top1 ->  70.762 ; top5 ->  89.882  and loss:  952.2348858714104
forward train acc: top1 ->  69.1171875 ; top5 ->  87.2109375  and loss:  261.20307993888855
test acc: top1 ->  70.764 ; top5 ->  89.85  and loss:  950.805489718914
forward train acc: top1 ->  69.21875 ; top5 ->  87.0546875  and loss:  260.3335212469101
test acc: top1 ->  70.804 ; top5 ->  89.88  and loss:  950.6401373147964
forward train acc: top1 ->  69.5 ; top5 ->  87.890625  and loss:  254.68044435977936
test acc: top1 ->  70.79 ; top5 ->  89.826  and loss:  951.7198823094368
forward train acc: top1 ->  69.6640625 ; top5 ->  87.640625  and loss:  256.90012323856354
test acc: top1 ->  70.856 ; top5 ->  89.798  and loss:  950.5057427287102
forward train acc: top1 ->  69.3125 ; top5 ->  87.5390625  and loss:  256.56846272945404
test acc: top1 ->  70.91 ; top5 ->  89.884  and loss:  948.3606548905373
forward train acc: top1 ->  69.515625 ; top5 ->  87.0078125  and loss:  259.3232400417328
test acc: top1 ->  70.976 ; top5 ->  89.87  and loss:  946.7082842588425
forward train acc: top1 ->  69.609375 ; top5 ->  87.9609375  and loss:  251.0978786945343
test acc: top1 ->  70.894 ; top5 ->  89.836  and loss:  947.5846224427223
forward train acc: top1 ->  69.3203125 ; top5 ->  87.5859375  and loss:  258.6884822845459
test acc: top1 ->  70.998 ; top5 ->  89.916  and loss:  945.6670822501183
forward train acc: top1 ->  70.1484375 ; top5 ->  87.78125  and loss:  250.22750389575958
test acc: top1 ->  70.94 ; top5 ->  89.854  and loss:  945.8258125782013
>>>>>>> reverse layer  28  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  29  ---------------
adv train loss:  -62.588395178318024 , diff:  62.588395178318024
adv train loss:  -64.9746230840683 , diff:  2.3862279057502747
************ all values are small in this layer **********
layer  29  adv train finish, try to retain  497
test acc: top1 ->  70.942 ; top5 ->  89.87  and loss:  950.4123488664627
forward train acc: top1 ->  69.046875 ; top5 ->  87.328125  and loss:  259.7765669822693
test acc: top1 ->  70.802 ; top5 ->  89.8  and loss:  950.4523537755013
forward train acc: top1 ->  68.6796875 ; top5 ->  87.0234375  and loss:  261.83095598220825
test acc: top1 ->  70.794 ; top5 ->  89.78  and loss:  951.9909241199493
forward train acc: top1 ->  68.25 ; top5 ->  87.515625  and loss:  260.4075874686241
test acc: top1 ->  70.766 ; top5 ->  89.748  and loss:  951.8311105370522
forward train acc: top1 ->  68.3203125 ; top5 ->  86.828125  and loss:  265.76446944475174
test acc: top1 ->  70.838 ; top5 ->  89.766  and loss:  952.0575209259987
forward train acc: top1 ->  69.078125 ; top5 ->  87.421875  and loss:  259.50533986091614
test acc: top1 ->  70.78 ; top5 ->  89.84  and loss:  949.4713150262833
forward train acc: top1 ->  69.625 ; top5 ->  88.1953125  and loss:  249.60171902179718
test acc: top1 ->  70.73 ; top5 ->  89.826  and loss:  949.484124481678
forward train acc: top1 ->  69.6328125 ; top5 ->  87.3828125  and loss:  255.83493506908417
test acc: top1 ->  70.808 ; top5 ->  89.868  and loss:  945.8516901731491
forward train acc: top1 ->  69.78125 ; top5 ->  87.8515625  and loss:  254.00116300582886
test acc: top1 ->  70.85 ; top5 ->  89.86  and loss:  950.3192244768143
forward train acc: top1 ->  69.5546875 ; top5 ->  87.1484375  and loss:  260.22675067186356
test acc: top1 ->  70.84 ; top5 ->  89.872  and loss:  945.5571799874306
forward train acc: top1 ->  69.53125 ; top5 ->  87.6328125  and loss:  253.34538716077805
test acc: top1 ->  70.924 ; top5 ->  89.91  and loss:  946.3318264484406
>>>>>>> reverse layer  29  since performance drop >>>>>>>
==> this epoch:  498 / 512 , inc:  1
---------------- start layer  30  ---------------
adv train loss:  -64.78315103054047 , diff:  64.78315103054047
adv train loss:  -64.17986309528351 , diff:  0.603287935256958
************ all values are small in this layer **********
layer  30  adv train finish, try to retain  502
test acc: top1 ->  70.704 ; top5 ->  89.804  and loss:  950.8782557249069
forward train acc: top1 ->  69.6015625 ; top5 ->  87.234375  and loss:  259.1614448428154
test acc: top1 ->  70.64 ; top5 ->  89.784  and loss:  953.1545144319534
forward train acc: top1 ->  68.8203125 ; top5 ->  87.5625  and loss:  260.46876257658005
test acc: top1 ->  70.788 ; top5 ->  89.826  and loss:  953.2745536565781
forward train acc: top1 ->  68.921875 ; top5 ->  87.4140625  and loss:  257.2430018186569
test acc: top1 ->  70.75 ; top5 ->  89.742  and loss:  954.2700546979904
forward train acc: top1 ->  69.2109375 ; top5 ->  87.2265625  and loss:  258.2911512851715
test acc: top1 ->  70.846 ; top5 ->  89.884  and loss:  950.3284669518471
forward train acc: top1 ->  69.4140625 ; top5 ->  87.234375  and loss:  260.1628289818764
test acc: top1 ->  70.89 ; top5 ->  89.832  and loss:  950.1779943108559
forward train acc: top1 ->  69.703125 ; top5 ->  87.4921875  and loss:  254.78164744377136
test acc: top1 ->  70.87 ; top5 ->  89.758  and loss:  949.8806647062302
forward train acc: top1 ->  69.1953125 ; top5 ->  87.3828125  and loss:  257.48827904462814
test acc: top1 ->  70.838 ; top5 ->  89.826  and loss:  950.7812309861183
forward train acc: top1 ->  69.625 ; top5 ->  87.65625  and loss:  257.65336495637894
test acc: top1 ->  70.896 ; top5 ->  89.864  and loss:  949.477743268013
forward train acc: top1 ->  70.140625 ; top5 ->  87.9453125  and loss:  250.5366917848587
test acc: top1 ->  70.998 ; top5 ->  89.898  and loss:  947.0094600319862
forward train acc: top1 ->  69.8671875 ; top5 ->  87.9140625  and loss:  249.6851705312729
test acc: top1 ->  70.962 ; top5 ->  89.902  and loss:  947.4047549366951
>>>>>>> reverse layer  30  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  31  ---------------
### skip layer  31 wait:  4  ###
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.2025, 0.0759375, 0.0006674194335937501, 0.007119140625000001, 0.014238281250000002, 0.151875, 0.0006674194335937501, 0.00016685485839843753, 0.0035595703125000005, 0.0006674194335937501, 0.007119140625000001, 0.00474609375, 0.0035595703125000005, 0.03796875, 8.342742919921877e-05, 8.342742919921877e-05, 0.00033370971679687507, 0.00033370971679687507, 0.0008898925781250001, 0.0035595703125000005, 0.0008898925781250001, 8.342742919921877e-05, 0.0017797851562500002, 0.0035595703125000005, 0.00016685485839843753, 0.00016685485839843753, 4.1713714599609384e-05, 4.1713714599609384e-05, 8.342742919921877e-05, 8.342742919921877e-05, 8.342742919921877e-05, 8.342742919921877e-05]  wait [1, 3, 3, 3, 4, 4, 4, 2, 3, 4, 4, 0, 3, 3, 2, 2, 4, 4, 2, 4, 2, 2, 3, 4, 3, 3, 2, 2, 3, 3, 3, 3]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 3
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  17  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -66.61291044950485 , diff:  66.61291044950485
adv train loss:  -66.05105137825012 , diff:  0.5618590712547302
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  36
test acc: top1 ->  61.804 ; top5 ->  83.624  and loss:  1268.9681729078293
forward train acc: top1 ->  68.3359375 ; top5 ->  87.0546875  and loss:  262.8607551455498
test acc: top1 ->  70.452 ; top5 ->  89.608  and loss:  960.8985627889633
forward train acc: top1 ->  69.40625 ; top5 ->  87.2578125  and loss:  257.80237382650375
test acc: top1 ->  70.564 ; top5 ->  89.644  and loss:  959.0973380208015
forward train acc: top1 ->  69.125 ; top5 ->  87.2578125  and loss:  260.97590559720993
test acc: top1 ->  70.558 ; top5 ->  89.698  and loss:  961.1619471907616
forward train acc: top1 ->  68.9296875 ; top5 ->  87.15625  and loss:  259.59539633989334
test acc: top1 ->  70.7 ; top5 ->  89.756  and loss:  956.4932218790054
forward train acc: top1 ->  69.0625 ; top5 ->  87.125  and loss:  262.1481141448021
test acc: top1 ->  70.696 ; top5 ->  89.722  and loss:  954.8341717123985
forward train acc: top1 ->  68.8046875 ; top5 ->  87.328125  and loss:  262.22796934843063
test acc: top1 ->  70.798 ; top5 ->  89.722  and loss:  952.9099339842796
forward train acc: top1 ->  68.59375 ; top5 ->  87.1796875  and loss:  262.1457886695862
test acc: top1 ->  70.75 ; top5 ->  89.792  and loss:  950.569585621357
forward train acc: top1 ->  68.6328125 ; top5 ->  86.8046875  and loss:  265.71815061569214
test acc: top1 ->  70.806 ; top5 ->  89.718  and loss:  950.5301032066345
forward train acc: top1 ->  69.8359375 ; top5 ->  87.953125  and loss:  249.73231208324432
test acc: top1 ->  70.838 ; top5 ->  89.666  and loss:  951.5437252521515
forward train acc: top1 ->  69.359375 ; top5 ->  87.2265625  and loss:  257.79503631591797
test acc: top1 ->  70.796 ; top5 ->  89.822  and loss:  949.1126170158386
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  37 / 64 , inc:  1
---------------- start layer  1  ---------------
### skip layer  1 wait:  3  ###
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
adv train loss:  -61.88459932804108 , diff:  61.88459932804108
adv train loss:  -61.87124264240265 , diff:  0.013356685638427734
layer  7  adv train finish, try to retain  122
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
### skip layer  8 wait:  3  ###
---------------- start layer  9  ---------------
### skip layer  9 wait:  4  ###
---------------- start layer  10  ---------------
### skip layer  10 wait:  4  ###
---------------- start layer  11  ---------------
adv train loss:  -65.63568598031998 , diff:  65.63568598031998
adv train loss:  -63.69819074869156 , diff:  1.937495231628418
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  99
test acc: top1 ->  69.056 ; top5 ->  88.69  and loss:  1013.5496894717216
forward train acc: top1 ->  69.03125 ; top5 ->  87.1640625  and loss:  260.31790721416473
test acc: top1 ->  70.654 ; top5 ->  89.612  and loss:  961.0877651572227
forward train acc: top1 ->  68.9765625 ; top5 ->  87.0390625  and loss:  261.8907040953636
test acc: top1 ->  70.7 ; top5 ->  89.612  and loss:  961.701603770256
forward train acc: top1 ->  68.46875 ; top5 ->  87.1015625  and loss:  262.04403805732727
test acc: top1 ->  70.742 ; top5 ->  89.69  and loss:  957.9601657390594
forward train acc: top1 ->  69.203125 ; top5 ->  87.546875  and loss:  256.90617376565933
test acc: top1 ->  70.7 ; top5 ->  89.69  and loss:  955.8233169913292
forward train acc: top1 ->  69.484375 ; top5 ->  86.9140625  and loss:  260.09773790836334
test acc: top1 ->  70.868 ; top5 ->  89.692  and loss:  954.5363178253174
forward train acc: top1 ->  68.953125 ; top5 ->  87.5546875  and loss:  258.9197077155113
test acc: top1 ->  70.936 ; top5 ->  89.696  and loss:  952.469880759716
forward train acc: top1 ->  69.4375 ; top5 ->  87.6484375  and loss:  255.41820341348648
test acc: top1 ->  70.87 ; top5 ->  89.704  and loss:  951.703365623951
forward train acc: top1 ->  69.578125 ; top5 ->  87.6484375  and loss:  254.47170627117157
test acc: top1 ->  70.978 ; top5 ->  89.708  and loss:  950.7471091151237
forward train acc: top1 ->  68.515625 ; top5 ->  86.9609375  and loss:  262.9452944993973
test acc: top1 ->  70.914 ; top5 ->  89.688  and loss:  951.0274895429611
forward train acc: top1 ->  70.1796875 ; top5 ->  87.578125  and loss:  254.28371196985245
test acc: top1 ->  70.986 ; top5 ->  89.72  and loss:  950.854836165905
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  12  ---------------
### skip layer  12 wait:  3  ###
---------------- start layer  13  ---------------
### skip layer  13 wait:  3  ###
---------------- start layer  14  ---------------
adv train loss:  -63.15664321184158 , diff:  63.15664321184158
adv train loss:  -62.235397696495056 , diff:  0.9212455153465271
layer  14  adv train finish, try to retain  249
test acc: top1 ->  70.76 ; top5 ->  89.654  and loss:  956.176199734211
forward train acc: top1 ->  69.1640625 ; top5 ->  87.4296875  and loss:  255.96576994657516
test acc: top1 ->  70.87 ; top5 ->  89.828  and loss:  951.2997480034828
forward train acc: top1 ->  69.625 ; top5 ->  87.203125  and loss:  258.6826026439667
test acc: top1 ->  70.864 ; top5 ->  89.756  and loss:  952.1994162797928
forward train acc: top1 ->  69.5078125 ; top5 ->  87.0546875  and loss:  258.4632839560509
test acc: top1 ->  70.83 ; top5 ->  89.762  and loss:  951.0869041085243
forward train acc: top1 ->  69.078125 ; top5 ->  87.1015625  and loss:  259.4242639541626
test acc: top1 ->  70.856 ; top5 ->  89.814  and loss:  948.906922519207
forward train acc: top1 ->  68.9453125 ; top5 ->  87.421875  and loss:  261.38611686229706
test acc: top1 ->  70.996 ; top5 ->  89.842  and loss:  951.9175778031349
forward train acc: top1 ->  69.359375 ; top5 ->  87.5703125  and loss:  255.34104949235916
test acc: top1 ->  70.882 ; top5 ->  89.764  and loss:  950.894539296627
forward train acc: top1 ->  69.046875 ; top5 ->  87.546875  and loss:  257.9083371758461
test acc: top1 ->  70.96 ; top5 ->  89.914  and loss:  947.7439906001091
forward train acc: top1 ->  69.3203125 ; top5 ->  86.96875  and loss:  260.33128893375397
test acc: top1 ->  70.96 ; top5 ->  89.874  and loss:  948.776455104351
forward train acc: top1 ->  69.2890625 ; top5 ->  87.3515625  and loss:  259.23244577646255
test acc: top1 ->  70.882 ; top5 ->  89.714  and loss:  948.676337480545
forward train acc: top1 ->  68.84375 ; top5 ->  87.53125  and loss:  258.71199947595596
test acc: top1 ->  70.974 ; top5 ->  89.828  and loss:  946.4035163521767
>>>>>>> reverse layer  14  since performance drop >>>>>>>
==> this epoch:  250 / 256 , inc:  1
---------------- start layer  15  ---------------
adv train loss:  -63.094168961048126 , diff:  63.094168961048126
adv train loss:  -67.42088264226913 , diff:  4.326713681221008
layer  15  adv train finish, try to retain  254
>>>>>>> reverse layer  15  since no improvement >>>>>>>
---------------- start layer  16  ---------------
### skip layer  16 wait:  4  ###
---------------- start layer  17  ---------------
### skip layer  17 wait:  4  ###
---------------- start layer  18  ---------------
adv train loss:  -61.78261041641235 , diff:  61.78261041641235
adv train loss:  -63.629041373729706 , diff:  1.8464309573173523
layer  18  adv train finish, try to retain  215
test acc: top1 ->  70.726 ; top5 ->  89.728  and loss:  952.729237139225
forward train acc: top1 ->  68.640625 ; top5 ->  87.3203125  and loss:  261.77000564336777
test acc: top1 ->  70.768 ; top5 ->  89.764  and loss:  956.0878462195396
forward train acc: top1 ->  69.8125 ; top5 ->  87.7421875  and loss:  254.4677096605301
test acc: top1 ->  70.736 ; top5 ->  89.742  and loss:  954.8922553658485
forward train acc: top1 ->  69.421875 ; top5 ->  87.734375  and loss:  254.03214275836945
test acc: top1 ->  70.728 ; top5 ->  89.724  and loss:  953.9870653748512
forward train acc: top1 ->  68.6953125 ; top5 ->  86.9375  and loss:  261.84870171546936
test acc: top1 ->  70.862 ; top5 ->  89.732  and loss:  953.7520674467087
forward train acc: top1 ->  69.34375 ; top5 ->  88.25  and loss:  254.6612325310707
test acc: top1 ->  70.796 ; top5 ->  89.766  and loss:  952.0513287186623
forward train acc: top1 ->  68.9140625 ; top5 ->  87.21875  and loss:  260.40059810876846
test acc: top1 ->  70.876 ; top5 ->  89.772  and loss:  949.2466435432434
forward train acc: top1 ->  68.6953125 ; top5 ->  87.3828125  and loss:  260.6690102815628
test acc: top1 ->  70.952 ; top5 ->  89.692  and loss:  949.5367319583893
forward train acc: top1 ->  69.375 ; top5 ->  87.796875  and loss:  253.55971556901932
test acc: top1 ->  70.902 ; top5 ->  89.742  and loss:  950.9953625202179
forward train acc: top1 ->  68.84375 ; top5 ->  87.421875  and loss:  258.52829641103745
test acc: top1 ->  70.862 ; top5 ->  89.766  and loss:  948.8177312612534
forward train acc: top1 ->  69.5 ; top5 ->  87.390625  and loss:  255.25438725948334
test acc: top1 ->  71.006 ; top5 ->  89.79  and loss:  947.784486591816
>>>>>>> reverse layer  18  since performance drop >>>>>>>
==> this epoch:  235 / 256 , inc:  1
---------------- start layer  19  ---------------
### skip layer  19 wait:  4  ###
---------------- start layer  20  ---------------
adv train loss:  -64.95212465524673 , diff:  64.95212465524673
adv train loss:  -65.1635012626648 , diff:  0.2113766074180603
layer  20  adv train finish, try to retain  217
test acc: top1 ->  70.66 ; top5 ->  89.754  and loss:  958.5469570755959
forward train acc: top1 ->  69.234375 ; top5 ->  87.09375  and loss:  259.5729277729988
test acc: top1 ->  70.83 ; top5 ->  89.704  and loss:  954.191120505333
forward train acc: top1 ->  69.6171875 ; top5 ->  87.6328125  and loss:  255.83407044410706
test acc: top1 ->  70.744 ; top5 ->  89.776  and loss:  954.955670773983
forward train acc: top1 ->  69.6484375 ; top5 ->  87.5390625  and loss:  256.0197762846947
test acc: top1 ->  70.662 ; top5 ->  89.708  and loss:  953.7417518496513
forward train acc: top1 ->  69.0 ; top5 ->  87.5234375  and loss:  258.3250765800476
test acc: top1 ->  70.794 ; top5 ->  89.766  and loss:  952.6300916075706
forward train acc: top1 ->  69.4375 ; top5 ->  87.40625  and loss:  255.0428186058998
test acc: top1 ->  70.772 ; top5 ->  89.812  and loss:  950.6635983586311
forward train acc: top1 ->  69.7265625 ; top5 ->  87.46875  and loss:  254.39236533641815
test acc: top1 ->  70.858 ; top5 ->  89.766  and loss:  949.7254393696785
forward train acc: top1 ->  69.6015625 ; top5 ->  87.15625  and loss:  257.7644690275192
test acc: top1 ->  70.886 ; top5 ->  89.804  and loss:  949.0939902067184
forward train acc: top1 ->  69.1171875 ; top5 ->  87.296875  and loss:  260.4312180876732
test acc: top1 ->  70.834 ; top5 ->  89.776  and loss:  950.2083904743195
forward train acc: top1 ->  69.0078125 ; top5 ->  87.390625  and loss:  260.5497586131096
test acc: top1 ->  70.866 ; top5 ->  89.77  and loss:  950.0679149627686
forward train acc: top1 ->  70.09375 ; top5 ->  87.4375  and loss:  252.85615527629852
test acc: top1 ->  70.95 ; top5 ->  89.822  and loss:  949.1660157442093
>>>>>>> reverse layer  20  since performance drop >>>>>>>
==> this epoch:  225 / 256 , inc:  1
---------------- start layer  21  ---------------
adv train loss:  -65.82974964380264 , diff:  65.82974964380264
adv train loss:  -62.92272758483887 , diff:  2.9070220589637756
layer  21  adv train finish, try to retain  253
>>>>>>> reverse layer  21  since no improvement >>>>>>>
---------------- start layer  22  ---------------
### skip layer  22 wait:  3  ###
---------------- start layer  23  ---------------
### skip layer  23 wait:  4  ###
---------------- start layer  24  ---------------
### skip layer  24 wait:  3  ###
---------------- start layer  25  ---------------
### skip layer  25 wait:  3  ###
---------------- start layer  26  ---------------
adv train loss:  -66.03386402130127 , diff:  66.03386402130127
adv train loss:  -61.72329103946686 , diff:  4.310572981834412
layer  26  adv train finish, try to retain  504
>>>>>>> reverse layer  26  since no improvement >>>>>>>
---------------- start layer  27  ---------------
adv train loss:  -66.64033246040344 , diff:  66.64033246040344
adv train loss:  -64.17609298229218 , diff:  2.464239478111267
layer  27  adv train finish, try to retain  505
>>>>>>> reverse layer  27  since no improvement >>>>>>>
---------------- start layer  28  ---------------
### skip layer  28 wait:  3  ###
---------------- start layer  29  ---------------
### skip layer  29 wait:  3  ###
---------------- start layer  30  ---------------
### skip layer  30 wait:  3  ###
---------------- start layer  31  ---------------
### skip layer  31 wait:  3  ###
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.151875, 0.0759375, 0.0006674194335937501, 0.007119140625000001, 0.014238281250000002, 0.151875, 0.0006674194335937501, 0.00033370971679687507, 0.0035595703125000005, 0.0006674194335937501, 0.007119140625000001, 0.0035595703125000005, 0.0035595703125000005, 0.03796875, 6.257057189941408e-05, 0.00016685485839843753, 0.00033370971679687507, 0.00033370971679687507, 0.0006674194335937501, 0.0035595703125000005, 0.0006674194335937501, 0.00016685485839843753, 0.0017797851562500002, 0.0035595703125000005, 0.00016685485839843753, 0.00016685485839843753, 8.342742919921877e-05, 8.342742919921877e-05, 8.342742919921877e-05, 8.342742919921877e-05, 8.342742919921877e-05, 8.342742919921877e-05]  wait [3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 4, 2, 3, 3, 4, 3, 4, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 3
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  18  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -64.46121495962143 , diff:  64.46121495962143
adv train loss:  -63.71715575456619 , diff:  0.7440592050552368
layer  0  adv train finish, try to retain  31
test acc: top1 ->  64.76 ; top5 ->  85.722  and loss:  1164.2613003849983
forward train acc: top1 ->  68.2734375 ; top5 ->  87.03125  and loss:  264.7585973739624
test acc: top1 ->  70.258 ; top5 ->  89.442  and loss:  969.3930019140244
forward train acc: top1 ->  68.96875 ; top5 ->  87.1328125  and loss:  260.55112105607986
test acc: top1 ->  70.386 ; top5 ->  89.474  and loss:  964.180738568306
forward train acc: top1 ->  68.2265625 ; top5 ->  86.7578125  and loss:  266.13482850790024
test acc: top1 ->  70.532 ; top5 ->  89.54  and loss:  963.2808715701103
forward train acc: top1 ->  68.2265625 ; top5 ->  87.203125  and loss:  263.0561321377754
test acc: top1 ->  70.668 ; top5 ->  89.672  and loss:  957.6644355654716
forward train acc: top1 ->  68.765625 ; top5 ->  87.0  and loss:  260.88042962551117
test acc: top1 ->  70.704 ; top5 ->  89.648  and loss:  958.9339061379433
forward train acc: top1 ->  69.8125 ; top5 ->  87.6328125  and loss:  256.7064864039421
test acc: top1 ->  70.78 ; top5 ->  89.69  and loss:  957.1265852451324
forward train acc: top1 ->  70.125 ; top5 ->  87.6171875  and loss:  255.83650237321854
test acc: top1 ->  70.76 ; top5 ->  89.662  and loss:  956.5467329025269
forward train acc: top1 ->  69.2578125 ; top5 ->  87.1484375  and loss:  260.30518889427185
test acc: top1 ->  70.842 ; top5 ->  89.668  and loss:  955.4141494631767
forward train acc: top1 ->  69.328125 ; top5 ->  87.40625  and loss:  255.33930492401123
test acc: top1 ->  70.934 ; top5 ->  89.664  and loss:  954.0793340802193
forward train acc: top1 ->  68.09375 ; top5 ->  86.890625  and loss:  266.4237242937088
test acc: top1 ->  70.756 ; top5 ->  89.678  and loss:  955.3931139707565
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  37 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -68.58029317855835 , diff:  68.58029317855835
adv train loss:  -70.95527815818787 , diff:  2.3749849796295166
layer  1  adv train finish, try to retain  41
test acc: top1 ->  67.61 ; top5 ->  87.674  and loss:  1063.116836309433
forward train acc: top1 ->  68.0 ; top5 ->  86.6640625  and loss:  268.21576952934265
test acc: top1 ->  70.644 ; top5 ->  89.554  and loss:  963.423557817936
forward train acc: top1 ->  69.6484375 ; top5 ->  87.6171875  and loss:  251.95116180181503
test acc: top1 ->  70.53 ; top5 ->  89.554  and loss:  961.8509623408318
forward train acc: top1 ->  68.6171875 ; top5 ->  86.8046875  and loss:  265.0105547904968
test acc: top1 ->  70.548 ; top5 ->  89.546  and loss:  961.343119263649
forward train acc: top1 ->  69.46875 ; top5 ->  87.546875  and loss:  257.8166928291321
test acc: top1 ->  70.612 ; top5 ->  89.644  and loss:  959.310097515583
forward train acc: top1 ->  70.0234375 ; top5 ->  87.6796875  and loss:  253.0868220925331
test acc: top1 ->  70.66 ; top5 ->  89.628  and loss:  957.817570745945
forward train acc: top1 ->  70.0625 ; top5 ->  87.703125  and loss:  252.99487787485123
test acc: top1 ->  70.658 ; top5 ->  89.6  and loss:  959.4982157945633
forward train acc: top1 ->  69.4453125 ; top5 ->  86.953125  and loss:  259.8926645517349
test acc: top1 ->  70.732 ; top5 ->  89.626  and loss:  957.6219332814217
forward train acc: top1 ->  69.75 ; top5 ->  87.046875  and loss:  259.01561909914017
test acc: top1 ->  70.772 ; top5 ->  89.642  and loss:  954.3620848059654
forward train acc: top1 ->  69.234375 ; top5 ->  87.015625  and loss:  260.5842955708504
test acc: top1 ->  70.74 ; top5 ->  89.732  and loss:  955.6124423146248
forward train acc: top1 ->  69.09375 ; top5 ->  86.9921875  and loss:  263.23923087120056
test acc: top1 ->  70.696 ; top5 ->  89.662  and loss:  955.1999098062515
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  44 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -66.49484121799469 , diff:  66.49484121799469
adv train loss:  -63.93251496553421 , diff:  2.5623262524604797
layer  2  adv train finish, try to retain  58
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -65.69211959838867 , diff:  65.69211959838867
adv train loss:  -64.29740852117538 , diff:  1.3947110772132874
layer  3  adv train finish, try to retain  50
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -65.99755257368088 , diff:  65.99755257368088
adv train loss:  -65.11579257249832 , diff:  0.8817600011825562
layer  4  adv train finish, try to retain  41
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -68.07495945692062 , diff:  68.07495945692062
adv train loss:  -65.80201995372772 , diff:  2.2729395031929016
layer  5  adv train finish, try to retain  26
test acc: top1 ->  65.944 ; top5 ->  86.7  and loss:  1117.2669172286987
forward train acc: top1 ->  69.0703125 ; top5 ->  87.1328125  and loss:  260.1873712539673
test acc: top1 ->  70.412 ; top5 ->  89.556  and loss:  961.199812233448
forward train acc: top1 ->  69.9609375 ; top5 ->  88.0546875  and loss:  253.88162207603455
test acc: top1 ->  70.508 ; top5 ->  89.592  and loss:  962.3636763095856
forward train acc: top1 ->  69.359375 ; top5 ->  87.453125  and loss:  261.90685522556305
test acc: top1 ->  70.64 ; top5 ->  89.674  and loss:  961.5925355553627
forward train acc: top1 ->  68.890625 ; top5 ->  87.3125  and loss:  258.19796895980835
test acc: top1 ->  70.532 ; top5 ->  89.632  and loss:  960.2816464304924
forward train acc: top1 ->  68.640625 ; top5 ->  87.109375  and loss:  264.87645494937897
test acc: top1 ->  70.648 ; top5 ->  89.602  and loss:  959.4599995017052
forward train acc: top1 ->  69.390625 ; top5 ->  87.140625  and loss:  260.6486902832985
test acc: top1 ->  70.688 ; top5 ->  89.72  and loss:  955.4021592140198
forward train acc: top1 ->  69.296875 ; top5 ->  87.6953125  and loss:  259.1168473958969
test acc: top1 ->  70.866 ; top5 ->  89.75  and loss:  952.8220283389091
forward train acc: top1 ->  69.0859375 ; top5 ->  87.625  and loss:  254.95310997962952
test acc: top1 ->  70.884 ; top5 ->  89.742  and loss:  953.7864788174629
forward train acc: top1 ->  69.15625 ; top5 ->  87.203125  and loss:  260.5719977617264
test acc: top1 ->  70.782 ; top5 ->  89.672  and loss:  956.7705134153366
forward train acc: top1 ->  69.5234375 ; top5 ->  87.4296875  and loss:  256.4518574476242
test acc: top1 ->  70.81 ; top5 ->  89.72  and loss:  953.4259596467018
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  29 / 64 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -65.42098373174667 , diff:  65.42098373174667
adv train loss:  -66.21619433164597 , diff:  0.795210599899292
layer  6  adv train finish, try to retain  100
test acc: top1 ->  65.454 ; top5 ->  86.392  and loss:  1133.5807797312737
forward train acc: top1 ->  69.0546875 ; top5 ->  87.3359375  and loss:  258.5898325443268
test acc: top1 ->  70.43 ; top5 ->  89.492  and loss:  968.4710713028908
forward train acc: top1 ->  68.6171875 ; top5 ->  87.2265625  and loss:  259.6560592055321
test acc: top1 ->  70.44 ; top5 ->  89.578  and loss:  965.6166308522224
forward train acc: top1 ->  68.3515625 ; top5 ->  87.2578125  and loss:  262.283964574337
test acc: top1 ->  70.498 ; top5 ->  89.602  and loss:  963.5670465230942
forward train acc: top1 ->  68.7578125 ; top5 ->  87.375  and loss:  258.2358077764511
test acc: top1 ->  70.496 ; top5 ->  89.584  and loss:  962.6772566437721
forward train acc: top1 ->  68.9375 ; top5 ->  87.078125  and loss:  260.04933416843414
test acc: top1 ->  70.626 ; top5 ->  89.664  and loss:  959.0857030153275
forward train acc: top1 ->  68.9609375 ; top5 ->  87.1015625  and loss:  261.34288001060486
test acc: top1 ->  70.488 ; top5 ->  89.652  and loss:  960.3540825247765
forward train acc: top1 ->  68.4609375 ; top5 ->  87.203125  and loss:  266.0343379378319
test acc: top1 ->  70.676 ; top5 ->  89.682  and loss:  957.2166479229927
forward train acc: top1 ->  69.7421875 ; top5 ->  87.2421875  and loss:  259.4444726705551
test acc: top1 ->  70.57 ; top5 ->  89.62  and loss:  959.6012127399445
forward train acc: top1 ->  69.609375 ; top5 ->  87.4375  and loss:  256.77088648080826
test acc: top1 ->  70.694 ; top5 ->  89.64  and loss:  955.0719561576843
forward train acc: top1 ->  69.3125 ; top5 ->  87.7109375  and loss:  258.93383741378784
test acc: top1 ->  70.674 ; top5 ->  89.692  and loss:  956.8638772964478
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  112 / 128 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -64.60875982046127 , diff:  64.60875982046127
adv train loss:  -63.26329064369202 , diff:  1.3454691767692566
layer  7  adv train finish, try to retain  117
test acc: top1 ->  69.922 ; top5 ->  89.068  and loss:  990.0379036068916
forward train acc: top1 ->  68.8984375 ; top5 ->  87.0703125  and loss:  261.8993145227432
test acc: top1 ->  70.786 ; top5 ->  89.666  and loss:  964.328056037426
forward train acc: top1 ->  68.96875 ; top5 ->  87.125  and loss:  264.32058930397034
test acc: top1 ->  70.698 ; top5 ->  89.774  and loss:  957.7973890304565
forward train acc: top1 ->  69.1484375 ; top5 ->  87.3046875  and loss:  258.7415791749954
test acc: top1 ->  70.778 ; top5 ->  89.676  and loss:  959.0893552899361
forward train acc: top1 ->  69.2109375 ; top5 ->  87.1640625  and loss:  258.447832942009
test acc: top1 ->  70.912 ; top5 ->  89.766  and loss:  954.3218877315521
forward train acc: top1 ->  69.53125 ; top5 ->  87.3515625  and loss:  260.1909525990486
test acc: top1 ->  70.892 ; top5 ->  89.712  and loss:  952.8350676894188
forward train acc: top1 ->  69.140625 ; top5 ->  87.1015625  and loss:  262.6423314809799
test acc: top1 ->  70.852 ; top5 ->  89.64  and loss:  954.8444660305977
forward train acc: top1 ->  69.5625 ; top5 ->  87.625  and loss:  254.9700259566307
test acc: top1 ->  70.896 ; top5 ->  89.74  and loss:  953.2550410032272
forward train acc: top1 ->  69.4140625 ; top5 ->  87.375  and loss:  258.89994460344315
test acc: top1 ->  70.83 ; top5 ->  89.674  and loss:  954.414866745472
forward train acc: top1 ->  69.8515625 ; top5 ->  87.59375  and loss:  257.7036992907524
test acc: top1 ->  70.916 ; top5 ->  89.754  and loss:  952.0059828162193
forward train acc: top1 ->  69.953125 ; top5 ->  87.7421875  and loss:  250.02885991334915
test acc: top1 ->  70.87 ; top5 ->  89.776  and loss:  951.0769907236099
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  122 / 128 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -63.84996521472931 , diff:  63.84996521472931
adv train loss:  -64.09258550405502 , diff:  0.2426202893257141
layer  8  adv train finish, try to retain  98
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -68.79963433742523 , diff:  68.79963433742523
adv train loss:  -64.08154052495956 , diff:  4.718093812465668
layer  9  adv train finish, try to retain  117
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -63.3558269739151 , diff:  63.3558269739151
adv train loss:  -66.9136575460434 , diff:  3.557830572128296
layer  10  adv train finish, try to retain  84
test acc: top1 ->  69.64 ; top5 ->  89.168  and loss:  989.2027074098587
forward train acc: top1 ->  68.8828125 ; top5 ->  87.1015625  and loss:  260.3047868013382
test acc: top1 ->  70.414 ; top5 ->  89.65  and loss:  962.6117589473724
forward train acc: top1 ->  69.328125 ; top5 ->  87.3046875  and loss:  257.11330342292786
test acc: top1 ->  70.702 ; top5 ->  89.678  and loss:  956.5503757596016
forward train acc: top1 ->  69.171875 ; top5 ->  87.375  and loss:  260.16040474176407
test acc: top1 ->  70.558 ; top5 ->  89.66  and loss:  958.6774264574051
forward train acc: top1 ->  69.09375 ; top5 ->  87.40625  and loss:  257.1931497454643
test acc: top1 ->  70.754 ; top5 ->  89.73  and loss:  955.1460513472557
forward train acc: top1 ->  68.65625 ; top5 ->  87.046875  and loss:  259.33920615911484
test acc: top1 ->  70.718 ; top5 ->  89.732  and loss:  956.9106341004372
forward train acc: top1 ->  69.2578125 ; top5 ->  87.25  and loss:  258.04840368032455
test acc: top1 ->  70.724 ; top5 ->  89.804  and loss:  954.396442592144
forward train acc: top1 ->  69.296875 ; top5 ->  87.390625  and loss:  256.8516976237297
test acc: top1 ->  70.776 ; top5 ->  89.814  and loss:  950.7963559031487
forward train acc: top1 ->  69.6875 ; top5 ->  87.0546875  and loss:  257.9345402121544
test acc: top1 ->  70.792 ; top5 ->  89.754  and loss:  954.011835038662
forward train acc: top1 ->  69.1640625 ; top5 ->  86.8203125  and loss:  261.5164678096771
test acc: top1 ->  70.758 ; top5 ->  89.74  and loss:  954.8597462773323
forward train acc: top1 ->  69.03125 ; top5 ->  87.3828125  and loss:  260.408954679966
test acc: top1 ->  70.884 ; top5 ->  89.828  and loss:  951.3170377016068
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -63.98864281177521 , diff:  63.98864281177521
adv train loss:  -62.460402727127075 , diff:  1.5282400846481323
layer  11  adv train finish, try to retain  102
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -63.02837008237839 , diff:  63.02837008237839
adv train loss:  -66.52956342697144 , diff:  3.501193344593048
layer  12  adv train finish, try to retain  104
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -68.272012591362 , diff:  68.272012591362
adv train loss:  -64.84418880939484 , diff:  3.427823781967163
layer  13  adv train finish, try to retain  67
test acc: top1 ->  68.794 ; top5 ->  88.398  and loss:  1024.8850617408752
forward train acc: top1 ->  68.7734375 ; top5 ->  87.2734375  and loss:  263.50465816259384
test acc: top1 ->  70.498 ; top5 ->  89.464  and loss:  967.6210671067238
forward train acc: top1 ->  68.625 ; top5 ->  87.3828125  and loss:  259.8575516939163
test acc: top1 ->  70.564 ; top5 ->  89.502  and loss:  963.7509196996689
forward train acc: top1 ->  69.546875 ; top5 ->  87.25  and loss:  259.0243252515793
test acc: top1 ->  70.742 ; top5 ->  89.498  and loss:  963.7224620580673
forward train acc: top1 ->  68.609375 ; top5 ->  87.1015625  and loss:  264.96682435274124
test acc: top1 ->  70.692 ; top5 ->  89.53  and loss:  961.7685757875443
forward train acc: top1 ->  68.875 ; top5 ->  87.546875  and loss:  257.0713548064232
test acc: top1 ->  70.682 ; top5 ->  89.502  and loss:  960.8639116883278
forward train acc: top1 ->  69.421875 ; top5 ->  87.40625  and loss:  254.88060492277145
test acc: top1 ->  70.774 ; top5 ->  89.57  and loss:  958.4358038902283
forward train acc: top1 ->  69.015625 ; top5 ->  87.2890625  and loss:  261.2612218260765
test acc: top1 ->  70.878 ; top5 ->  89.592  and loss:  955.3170999288559
forward train acc: top1 ->  68.953125 ; top5 ->  87.34375  and loss:  259.3563490509987
test acc: top1 ->  70.77 ; top5 ->  89.624  and loss:  957.378709256649
forward train acc: top1 ->  69.5703125 ; top5 ->  87.1484375  and loss:  260.05530166625977
test acc: top1 ->  70.804 ; top5 ->  89.636  and loss:  954.9529954195023
forward train acc: top1 ->  69.1796875 ; top5 ->  86.8671875  and loss:  263.0930808186531
test acc: top1 ->  70.826 ; top5 ->  89.606  and loss:  954.5546663999557
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  71 / 128 , inc:  1
---------------- start layer  14  ---------------
adv train loss:  -68.0416390299797 , diff:  68.0416390299797
adv train loss:  -63.12409150600433 , diff:  4.917547523975372
layer  14  adv train finish, try to retain  250
>>>>>>> reverse layer  14  since no improvement >>>>>>>
---------------- start layer  15  ---------------
adv train loss:  -66.6048446893692 , diff:  66.6048446893692
adv train loss:  -64.48016685247421 , diff:  2.124677836894989
layer  15  adv train finish, try to retain  236
test acc: top1 ->  69.762 ; top5 ->  89.022  and loss:  987.9686308503151
forward train acc: top1 ->  68.546875 ; top5 ->  87.328125  and loss:  265.1521463394165
test acc: top1 ->  70.46 ; top5 ->  89.456  and loss:  963.6257597208023
forward train acc: top1 ->  69.1640625 ; top5 ->  87.4765625  and loss:  256.81644731760025
test acc: top1 ->  70.612 ; top5 ->  89.616  and loss:  960.4777734279633
forward train acc: top1 ->  69.0078125 ; top5 ->  87.1796875  and loss:  261.663365483284
test acc: top1 ->  70.604 ; top5 ->  89.64  and loss:  959.0724152326584
forward train acc: top1 ->  69.234375 ; top5 ->  87.75  and loss:  254.10996049642563
test acc: top1 ->  70.7 ; top5 ->  89.66  and loss:  957.2120662927628
forward train acc: top1 ->  68.7265625 ; top5 ->  87.34375  and loss:  260.86932480335236
test acc: top1 ->  70.756 ; top5 ->  89.72  and loss:  955.00704818964
forward train acc: top1 ->  69.3359375 ; top5 ->  87.796875  and loss:  253.3111071586609
test acc: top1 ->  70.736 ; top5 ->  89.728  and loss:  953.7105292677879
forward train acc: top1 ->  69.609375 ; top5 ->  87.3671875  and loss:  257.51718443632126
test acc: top1 ->  70.82 ; top5 ->  89.774  and loss:  952.9862579703331
forward train acc: top1 ->  69.1015625 ; top5 ->  87.109375  and loss:  261.0994095802307
test acc: top1 ->  70.804 ; top5 ->  89.764  and loss:  952.5488747954369
forward train acc: top1 ->  69.0 ; top5 ->  87.15625  and loss:  263.9662996530533
test acc: top1 ->  70.798 ; top5 ->  89.722  and loss:  954.0332880020142
forward train acc: top1 ->  69.21875 ; top5 ->  87.625  and loss:  257.80219584703445
test acc: top1 ->  70.826 ; top5 ->  89.718  and loss:  953.1141091585159
>>>>>>> reverse layer  15  since performance drop >>>>>>>
==> this epoch:  248 / 256 , inc:  1
---------------- start layer  16  ---------------
adv train loss:  -61.716520607471466 , diff:  61.716520607471466
adv train loss:  -65.1376788020134 , diff:  3.421158194541931
layer  16  adv train finish, try to retain  238
test acc: top1 ->  70.428 ; top5 ->  89.722  and loss:  957.4073328971863
forward train acc: top1 ->  69.1640625 ; top5 ->  87.3828125  and loss:  260.77470791339874
test acc: top1 ->  70.842 ; top5 ->  89.788  and loss:  955.3102850914001
forward train acc: top1 ->  68.9921875 ; top5 ->  87.125  and loss:  261.65272265672684
test acc: top1 ->  70.596 ; top5 ->  89.744  and loss:  955.5852252244949
forward train acc: top1 ->  68.8359375 ; top5 ->  87.2265625  and loss:  261.86600333452225
test acc: top1 ->  70.898 ; top5 ->  89.816  and loss:  950.9192364811897
forward train acc: top1 ->  68.9296875 ; top5 ->  87.65625  and loss:  254.90034139156342
test acc: top1 ->  70.82 ; top5 ->  89.814  and loss:  950.7327080965042
forward train acc: top1 ->  69.921875 ; top5 ->  87.5390625  and loss:  255.28707975149155
test acc: top1 ->  70.89 ; top5 ->  89.878  and loss:  947.4323759675026
forward train acc: top1 ->  69.078125 ; top5 ->  87.21875  and loss:  259.99773025512695
test acc: top1 ->  70.852 ; top5 ->  89.854  and loss:  950.0001909732819
forward train acc: top1 ->  69.5546875 ; top5 ->  87.6015625  and loss:  255.40104526281357
test acc: top1 ->  70.838 ; top5 ->  89.852  and loss:  949.1548469662666
forward train acc: top1 ->  69.1796875 ; top5 ->  87.3515625  and loss:  258.65815699100494
test acc: top1 ->  70.962 ; top5 ->  89.866  and loss:  947.080019891262
forward train acc: top1 ->  69.8984375 ; top5 ->  87.8828125  and loss:  252.1732023358345
test acc: top1 ->  70.934 ; top5 ->  89.924  and loss:  947.2803516983986
forward train acc: top1 ->  68.9609375 ; top5 ->  87.65625  and loss:  257.41981112957
test acc: top1 ->  71.022 ; top5 ->  89.878  and loss:  946.247023999691
>>>>>>> reverse layer  16  since performance drop >>>>>>>
==> this epoch:  239 / 256 , inc:  1
---------------- start layer  17  ---------------
adv train loss:  -61.044620871543884 , diff:  61.044620871543884
adv train loss:  -60.130945801734924 , diff:  0.91367506980896
layer  17  adv train finish, try to retain  237
>>>>>>> reverse layer  17  since no improvement >>>>>>>
---------------- start layer  18  ---------------
adv train loss:  -61.92250770330429 , diff:  61.92250770330429
adv train loss:  -63.81453436613083 , diff:  1.892026662826538
layer  18  adv train finish, try to retain  220
test acc: top1 ->  70.768 ; top5 ->  89.774  and loss:  950.6058990359306
forward train acc: top1 ->  69.3671875 ; top5 ->  87.421875  and loss:  258.7073327302933
test acc: top1 ->  70.664 ; top5 ->  89.776  and loss:  952.6998193860054
forward train acc: top1 ->  69.09375 ; top5 ->  87.546875  and loss:  258.2298100590706
test acc: top1 ->  70.79 ; top5 ->  89.774  and loss:  952.1766359210014
forward train acc: top1 ->  69.1875 ; top5 ->  87.3515625  and loss:  259.05177611112595
test acc: top1 ->  70.718 ; top5 ->  89.804  and loss:  953.978045642376
forward train acc: top1 ->  69.0078125 ; top5 ->  87.046875  and loss:  260.88955849409103
test acc: top1 ->  70.84 ; top5 ->  89.902  and loss:  947.5270424485207
forward train acc: top1 ->  69.484375 ; top5 ->  87.6015625  and loss:  254.9511780142784
test acc: top1 ->  70.926 ; top5 ->  89.814  and loss:  950.235577404499
forward train acc: top1 ->  69.109375 ; top5 ->  87.40625  and loss:  257.31108635663986
test acc: top1 ->  70.848 ; top5 ->  89.906  and loss:  948.1862701773643
forward train acc: top1 ->  70.1171875 ; top5 ->  87.3828125  and loss:  252.1290163397789
test acc: top1 ->  70.914 ; top5 ->  89.904  and loss:  945.6160293221474
forward train acc: top1 ->  69.8203125 ; top5 ->  87.5  and loss:  252.78999215364456
test acc: top1 ->  70.864 ; top5 ->  89.882  and loss:  947.5114570856094
forward train acc: top1 ->  69.9921875 ; top5 ->  87.859375  and loss:  252.54790592193604
test acc: top1 ->  71.012 ; top5 ->  89.896  and loss:  944.2522401809692
forward train acc: top1 ->  70.0234375 ; top5 ->  87.609375  and loss:  252.9241075515747
test acc: top1 ->  70.988 ; top5 ->  89.866  and loss:  947.5270430445671
>>>>>>> reverse layer  18  since performance drop >>>>>>>
==> this epoch:  235 / 256 , inc:  1
---------------- start layer  19  ---------------
adv train loss:  -64.44078892469406 , diff:  64.44078892469406
adv train loss:  -66.14179801940918 , diff:  1.7010090947151184
layer  19  adv train finish, try to retain  198
>>>>>>> reverse layer  19  since no improvement >>>>>>>
---------------- start layer  20  ---------------
adv train loss:  -64.330710709095 , diff:  64.330710709095
adv train loss:  -64.25635033845901 , diff:  0.07436037063598633
layer  20  adv train finish, try to retain  221
test acc: top1 ->  70.762 ; top5 ->  89.81  and loss:  951.5904566645622
forward train acc: top1 ->  68.7734375 ; top5 ->  87.2734375  and loss:  259.84049141407013
test acc: top1 ->  70.788 ; top5 ->  89.758  and loss:  952.9179560542107
forward train acc: top1 ->  69.515625 ; top5 ->  87.6875  and loss:  252.2784742116928
test acc: top1 ->  70.906 ; top5 ->  89.86  and loss:  950.4620227217674
forward train acc: top1 ->  70.140625 ; top5 ->  87.6953125  and loss:  250.51103734970093
test acc: top1 ->  70.842 ; top5 ->  89.742  and loss:  954.8392608165741
forward train acc: top1 ->  69.453125 ; top5 ->  88.1484375  and loss:  253.01484328508377
test acc: top1 ->  70.968 ; top5 ->  89.806  and loss:  950.1527346968651
forward train acc: top1 ->  69.078125 ; top5 ->  87.3828125  and loss:  259.6332604289055
test acc: top1 ->  70.968 ; top5 ->  89.774  and loss:  948.4778218865395
forward train acc: top1 ->  69.8046875 ; top5 ->  87.421875  and loss:  258.2789899110794
test acc: top1 ->  70.866 ; top5 ->  89.806  and loss:  947.0934754014015
forward train acc: top1 ->  69.375 ; top5 ->  87.2109375  and loss:  255.5426573753357
test acc: top1 ->  70.962 ; top5 ->  89.838  and loss:  947.7048006057739
forward train acc: top1 ->  69.890625 ; top5 ->  88.109375  and loss:  250.04051411151886
test acc: top1 ->  70.97 ; top5 ->  89.864  and loss:  943.66770195961
forward train acc: top1 ->  69.3984375 ; top5 ->  87.53125  and loss:  255.40197032690048
test acc: top1 ->  71.058 ; top5 ->  89.86  and loss:  945.1271470189095
forward train acc: top1 ->  69.7265625 ; top5 ->  87.859375  and loss:  254.2202427983284
test acc: top1 ->  70.974 ; top5 ->  89.898  and loss:  945.6335973739624
>>>>>>> reverse layer  20  since performance drop >>>>>>>
==> this epoch:  225 / 256 , inc:  1
---------------- start layer  21  ---------------
adv train loss:  -64.8555583357811 , diff:  64.8555583357811
adv train loss:  -64.77557927370071 , diff:  0.0799790620803833
layer  21  adv train finish, try to retain  249
>>>>>>> reverse layer  21  since no improvement >>>>>>>
---------------- start layer  22  ---------------
adv train loss:  -63.89863783121109 , diff:  63.89863783121109
adv train loss:  -65.41328370571136 , diff:  1.5146458745002747
layer  22  adv train finish, try to retain  215
>>>>>>> reverse layer  22  since no improvement >>>>>>>
---------------- start layer  23  ---------------
adv train loss:  -64.28244435787201 , diff:  64.28244435787201
adv train loss:  -64.45197069644928 , diff:  0.1695263385772705
layer  23  adv train finish, try to retain  195
>>>>>>> reverse layer  23  since no improvement >>>>>>>
---------------- start layer  24  ---------------
adv train loss:  -65.32325637340546 , diff:  65.32325637340546
adv train loss:  -63.59321981668472 , diff:  1.7300365567207336
layer  24  adv train finish, try to retain  242
test acc: top1 ->  70.988 ; top5 ->  89.908  and loss:  949.9409794807434
forward train acc: top1 ->  69.953125 ; top5 ->  87.3203125  and loss:  259.35616087913513
test acc: top1 ->  70.778 ; top5 ->  89.874  and loss:  948.5410401821136
forward train acc: top1 ->  68.8828125 ; top5 ->  87.0078125  and loss:  264.4081665277481
test acc: top1 ->  70.888 ; top5 ->  89.876  and loss:  951.1474817991257
forward train acc: top1 ->  70.03125 ; top5 ->  87.6484375  and loss:  252.61217921972275
test acc: top1 ->  70.99 ; top5 ->  89.932  and loss:  948.1734269857407
forward train acc: top1 ->  69.6171875 ; top5 ->  87.40625  and loss:  255.58484029769897
test acc: top1 ->  70.882 ; top5 ->  89.862  and loss:  947.6275674700737
forward train acc: top1 ->  69.34375 ; top5 ->  87.75  and loss:  257.3187335729599
test acc: top1 ->  70.976 ; top5 ->  89.916  and loss:  946.6928640007973
forward train acc: top1 ->  69.6484375 ; top5 ->  87.484375  and loss:  254.37086790800095
test acc: top1 ->  71.004 ; top5 ->  89.886  and loss:  945.5690516233444
forward train acc: top1 ->  69.7265625 ; top5 ->  87.5703125  and loss:  252.7199524641037
test acc: top1 ->  70.962 ; top5 ->  89.896  and loss:  946.4489256739616
forward train acc: top1 ->  69.375 ; top5 ->  87.46875  and loss:  255.0386312007904
test acc: top1 ->  70.964 ; top5 ->  89.93  and loss:  946.231805741787
forward train acc: top1 ->  69.90625 ; top5 ->  87.5703125  and loss:  252.26103204488754
test acc: top1 ->  71.074 ; top5 ->  89.94  and loss:  944.3919828534126
forward train acc: top1 ->  69.625 ; top5 ->  87.890625  and loss:  250.59555733203888
test acc: top1 ->  71.024 ; top5 ->  89.918  and loss:  945.1906386017799
>>>>>>> reverse layer  24  since performance drop >>>>>>>
==> this epoch:  244 / 256 , inc:  1
---------------- start layer  25  ---------------
adv train loss:  -64.78008741140366 , diff:  64.78008741140366
adv train loss:  -63.07641267776489 , diff:  1.7036747336387634
layer  25  adv train finish, try to retain  249
>>>>>>> reverse layer  25  since no improvement >>>>>>>
---------------- start layer  26  ---------------
adv train loss:  -64.82645553350449 , diff:  64.82645553350449
adv train loss:  -63.12226128578186 , diff:  1.7041942477226257
layer  26  adv train finish, try to retain  501
test acc: top1 ->  70.912 ; top5 ->  89.772  and loss:  952.7319437265396
forward train acc: top1 ->  69.1015625 ; top5 ->  87.203125  and loss:  258.14899057149887
test acc: top1 ->  70.942 ; top5 ->  89.844  and loss:  949.3074608445168
forward train acc: top1 ->  69.40625 ; top5 ->  87.421875  and loss:  258.34349089860916
test acc: top1 ->  70.872 ; top5 ->  89.782  and loss:  952.4150893688202
forward train acc: top1 ->  69.9375 ; top5 ->  87.59375  and loss:  253.52287131547928
test acc: top1 ->  70.828 ; top5 ->  89.84  and loss:  951.8567191362381
forward train acc: top1 ->  69.6875 ; top5 ->  87.21875  and loss:  256.31119883060455
test acc: top1 ->  71.118 ; top5 ->  89.876  and loss:  948.0163249969482
forward train acc: top1 ->  69.7734375 ; top5 ->  87.4296875  and loss:  254.8798624277115
test acc: top1 ->  71.044 ; top5 ->  89.884  and loss:  945.7204389572144
forward train acc: top1 ->  69.8125 ; top5 ->  87.9375  and loss:  253.27230459451675
test acc: top1 ->  70.968 ; top5 ->  89.772  and loss:  946.6652457714081
forward train acc: top1 ->  69.40625 ; top5 ->  87.640625  and loss:  257.97820895910263
test acc: top1 ->  71.09 ; top5 ->  89.798  and loss:  945.8899146318436
forward train acc: top1 ->  69.4140625 ; top5 ->  88.03125  and loss:  253.94331151247025
test acc: top1 ->  71.104 ; top5 ->  89.884  and loss:  945.8985396027565
forward train acc: top1 ->  69.4765625 ; top5 ->  87.5  and loss:  257.06722116470337
test acc: top1 ->  71.066 ; top5 ->  89.866  and loss:  946.3849009275436
forward train acc: top1 ->  69.0703125 ; top5 ->  87.46875  and loss:  255.32807821035385
test acc: top1 ->  71.136 ; top5 ->  89.868  and loss:  944.3688215613365
>>>>>>> reverse layer  26  since performance drop >>>>>>>
==> this epoch:  502 / 512 , inc:  1
---------------- start layer  27  ---------------
adv train loss:  -64.84068155288696 , diff:  64.84068155288696
adv train loss:  -62.176636815071106 , diff:  2.664044737815857
layer  27  adv train finish, try to retain  501
test acc: top1 ->  70.944 ; top5 ->  89.772  and loss:  950.0089938044548
forward train acc: top1 ->  69.2734375 ; top5 ->  87.34375  and loss:  258.03270983695984
test acc: top1 ->  70.964 ; top5 ->  89.772  and loss:  951.5669726729393
forward train acc: top1 ->  69.2421875 ; top5 ->  87.28125  and loss:  258.7386823296547
test acc: top1 ->  70.772 ; top5 ->  89.746  and loss:  949.6496781110764
forward train acc: top1 ->  69.4765625 ; top5 ->  87.1015625  and loss:  258.2418904900551
test acc: top1 ->  70.81 ; top5 ->  89.754  and loss:  951.0446966290474
forward train acc: top1 ->  69.5859375 ; top5 ->  87.1171875  and loss:  259.05734276771545
test acc: top1 ->  70.944 ; top5 ->  89.8  and loss:  951.6830946803093
forward train acc: top1 ->  69.7265625 ; top5 ->  87.7890625  and loss:  253.15467083454132
test acc: top1 ->  70.966 ; top5 ->  89.868  and loss:  947.1582917571068
forward train acc: top1 ->  69.21875 ; top5 ->  87.5859375  and loss:  254.69256740808487
test acc: top1 ->  70.974 ; top5 ->  89.788  and loss:  947.226870059967
forward train acc: top1 ->  69.421875 ; top5 ->  86.8828125  and loss:  258.1846138238907
test acc: top1 ->  71.046 ; top5 ->  89.816  and loss:  948.7967404127121
forward train acc: top1 ->  69.4453125 ; top5 ->  87.3515625  and loss:  260.070074737072
test acc: top1 ->  70.99 ; top5 ->  89.882  and loss:  947.2522389292717
forward train acc: top1 ->  69.46875 ; top5 ->  87.609375  and loss:  256.30988496541977
test acc: top1 ->  71.0 ; top5 ->  89.854  and loss:  945.7357314229012
forward train acc: top1 ->  68.921875 ; top5 ->  87.46875  and loss:  257.1683748960495
test acc: top1 ->  70.988 ; top5 ->  89.928  and loss:  944.1909934878349
>>>>>>> reverse layer  27  since performance drop >>>>>>>
==> this epoch:  505 / 512 , inc:  1
---------------- start layer  28  ---------------
adv train loss:  -67.13547694683075 , diff:  67.13547694683075
adv train loss:  -63.19322341680527 , diff:  3.942253530025482
layer  28  adv train finish, try to retain  502
test acc: top1 ->  71.044 ; top5 ->  89.79  and loss:  949.5730702877045
forward train acc: top1 ->  69.6328125 ; top5 ->  87.421875  and loss:  258.1136289834976
test acc: top1 ->  70.844 ; top5 ->  89.824  and loss:  950.3603708744049
forward train acc: top1 ->  69.1640625 ; top5 ->  87.09375  and loss:  263.2062051296234
test acc: top1 ->  70.832 ; top5 ->  89.79  and loss:  949.2296228408813
forward train acc: top1 ->  69.640625 ; top5 ->  87.59375  and loss:  253.33443212509155
test acc: top1 ->  70.77 ; top5 ->  89.778  and loss:  953.9045145511627
forward train acc: top1 ->  69.9453125 ; top5 ->  87.7734375  and loss:  254.3693050146103
test acc: top1 ->  70.888 ; top5 ->  89.836  and loss:  949.2617504000664
forward train acc: top1 ->  68.625 ; top5 ->  87.296875  and loss:  258.76360642910004
test acc: top1 ->  70.918 ; top5 ->  89.83  and loss:  950.0006803870201
forward train acc: top1 ->  69.8984375 ; top5 ->  87.2265625  and loss:  256.3852360844612
test acc: top1 ->  71.012 ; top5 ->  89.872  and loss:  945.6783812046051
forward train acc: top1 ->  70.078125 ; top5 ->  87.5703125  and loss:  252.65253376960754
test acc: top1 ->  70.928 ; top5 ->  89.892  and loss:  945.3093215227127
forward train acc: top1 ->  70.140625 ; top5 ->  87.3828125  and loss:  256.5499538183212
test acc: top1 ->  71.034 ; top5 ->  89.804  and loss:  946.9787344932556
forward train acc: top1 ->  69.8203125 ; top5 ->  87.7890625  and loss:  252.73641860485077
test acc: top1 ->  71.006 ; top5 ->  89.878  and loss:  944.3126497864723
forward train acc: top1 ->  70.4921875 ; top5 ->  88.03125  and loss:  246.61956185102463
test acc: top1 ->  71.018 ; top5 ->  89.926  and loss:  945.1163599491119
>>>>>>> reverse layer  28  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  29  ---------------
adv train loss:  -64.66696017980576 , diff:  64.66696017980576
adv train loss:  -64.4567020535469 , diff:  0.2102581262588501
layer  29  adv train finish, try to retain  504
>>>>>>> reverse layer  29  since no improvement >>>>>>>
---------------- start layer  30  ---------------
adv train loss:  -65.62048161029816 , diff:  65.62048161029816
adv train loss:  -63.34128534793854 , diff:  2.279196262359619
layer  30  adv train finish, try to retain  503
>>>>>>> reverse layer  30  since no improvement >>>>>>>
---------------- start layer  31  ---------------
adv train loss:  -62.54566276073456 , diff:  62.54566276073456
adv train loss:  -62.25619959831238 , diff:  0.2894631624221802
layer  31  adv train finish, try to retain  504
>>>>>>> reverse layer  31  since no improvement >>>>>>>
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.11390625000000001, 0.05695312500000001, 0.0013348388671875003, 0.014238281250000002, 0.028476562500000004, 0.11390625000000001, 0.0005005645751953126, 0.0002502822875976563, 0.007119140625000001, 0.0013348388671875003, 0.005339355468750001, 0.007119140625000001, 0.007119140625000001, 0.028476562500000004, 0.00012514114379882815, 0.00012514114379882815, 0.0002502822875976563, 0.0006674194335937501, 0.0005005645751953126, 0.007119140625000001, 0.0005005645751953126, 0.00033370971679687507, 0.0035595703125000005, 0.007119140625000001, 0.00012514114379882815, 0.00033370971679687507, 6.257057189941408e-05, 6.257057189941408e-05, 6.257057189941408e-05, 0.00016685485839843753, 0.00016685485839843753, 0.00016685485839843753]  wait [3, 2, 0, 0, 1, 3, 3, 2, 0, 1, 3, 0, 0, 2, 2, 2, 3, 1, 4, 1, 4, 0, 0, 1, 2, 0, 2, 2, 2, 0, 0, 0]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 4
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  19  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
adv train loss:  -66.64838761091232 , diff:  66.64838761091232
adv train loss:  -64.3251673579216 , diff:  2.3232202529907227
layer  1  adv train finish, try to retain  38
test acc: top1 ->  65.762 ; top5 ->  86.642  and loss:  1120.8988198637962
forward train acc: top1 ->  68.46875 ; top5 ->  86.84375  and loss:  264.51820009946823
test acc: top1 ->  70.596 ; top5 ->  89.604  and loss:  961.5421687960625
forward train acc: top1 ->  68.703125 ; top5 ->  87.171875  and loss:  265.2114509344101
test acc: top1 ->  70.618 ; top5 ->  89.648  and loss:  960.4927138090134
forward train acc: top1 ->  69.625 ; top5 ->  87.671875  and loss:  253.05225712060928
test acc: top1 ->  70.498 ; top5 ->  89.594  and loss:  959.4590069651604
forward train acc: top1 ->  69.03125 ; top5 ->  87.125  and loss:  261.2147579193115
test acc: top1 ->  70.712 ; top5 ->  89.696  and loss:  957.5293921232224
forward train acc: top1 ->  69.5546875 ; top5 ->  87.375  and loss:  255.39474314451218
test acc: top1 ->  70.63 ; top5 ->  89.662  and loss:  958.9749336838722
forward train acc: top1 ->  69.4921875 ; top5 ->  87.640625  and loss:  253.4943801164627
test acc: top1 ->  70.602 ; top5 ->  89.708  and loss:  954.8786246180534
forward train acc: top1 ->  69.6796875 ; top5 ->  87.6484375  and loss:  253.8267229795456
test acc: top1 ->  70.68 ; top5 ->  89.656  and loss:  956.0061232447624
forward train acc: top1 ->  69.28125 ; top5 ->  87.296875  and loss:  258.7259575724602
test acc: top1 ->  70.732 ; top5 ->  89.714  and loss:  955.5478448867798
forward train acc: top1 ->  69.6015625 ; top5 ->  87.7890625  and loss:  254.46009349822998
test acc: top1 ->  70.718 ; top5 ->  89.672  and loss:  954.4635400176048
forward train acc: top1 ->  68.8671875 ; top5 ->  87.3046875  and loss:  264.29393780231476
test acc: top1 ->  70.774 ; top5 ->  89.792  and loss:  952.261544585228
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  44 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -63.306692719459534 , diff:  63.306692719459534
adv train loss:  -64.1985052227974 , diff:  0.8918125033378601
layer  2  adv train finish, try to retain  53
test acc: top1 ->  70.25 ; top5 ->  89.38  and loss:  968.8201423883438
forward train acc: top1 ->  69.2265625 ; top5 ->  87.4609375  and loss:  255.7301258444786
test acc: top1 ->  70.886 ; top5 ->  89.696  and loss:  953.8442072868347
forward train acc: top1 ->  69.109375 ; top5 ->  87.15625  and loss:  257.05552822351456
test acc: top1 ->  70.806 ; top5 ->  89.71  and loss:  954.0287170410156
forward train acc: top1 ->  69.8828125 ; top5 ->  87.65625  and loss:  252.93019378185272
test acc: top1 ->  70.816 ; top5 ->  89.714  and loss:  952.4688383936882
forward train acc: top1 ->  69.5390625 ; top5 ->  87.34375  and loss:  253.79151356220245
test acc: top1 ->  70.846 ; top5 ->  89.726  and loss:  952.656704723835
forward train acc: top1 ->  69.6875 ; top5 ->  87.640625  and loss:  256.47136306762695
test acc: top1 ->  70.932 ; top5 ->  89.746  and loss:  948.0878881812096
forward train acc: top1 ->  68.953125 ; top5 ->  87.421875  and loss:  258.745624601841
test acc: top1 ->  70.952 ; top5 ->  89.748  and loss:  946.6064332127571
forward train acc: top1 ->  68.7578125 ; top5 ->  87.75  and loss:  257.609465777874
test acc: top1 ->  70.95 ; top5 ->  89.768  and loss:  947.9173957109451
forward train acc: top1 ->  70.0 ; top5 ->  87.65625  and loss:  253.91792553663254
test acc: top1 ->  70.99 ; top5 ->  89.824  and loss:  946.2243263721466
forward train acc: top1 ->  69.8203125 ; top5 ->  87.328125  and loss:  255.24675983190536
test acc: top1 ->  70.904 ; top5 ->  89.702  and loss:  948.3845821022987
forward train acc: top1 ->  70.609375 ; top5 ->  87.921875  and loss:  248.01596266031265
test acc: top1 ->  70.958 ; top5 ->  89.852  and loss:  944.790137052536
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  57 / 64 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -64.290432035923 , diff:  64.290432035923
adv train loss:  -64.58793526887894 , diff:  0.2975032329559326
layer  3  adv train finish, try to retain  43
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -62.36279970407486 , diff:  62.36279970407486
adv train loss:  -63.96970373392105 , diff:  1.6069040298461914
layer  4  adv train finish, try to retain  30
test acc: top1 ->  69.428 ; top5 ->  88.986  and loss:  999.640126645565
forward train acc: top1 ->  69.1875 ; top5 ->  87.375  and loss:  258.6729670763016
test acc: top1 ->  70.654 ; top5 ->  89.618  and loss:  959.4467242360115
forward train acc: top1 ->  68.8359375 ; top5 ->  87.2109375  and loss:  257.25793397426605
test acc: top1 ->  70.808 ; top5 ->  89.67  and loss:  956.8950071930885
forward train acc: top1 ->  68.796875 ; top5 ->  87.015625  and loss:  259.9511743783951
test acc: top1 ->  70.572 ; top5 ->  89.594  and loss:  961.0882502794266
forward train acc: top1 ->  69.09375 ; top5 ->  87.453125  and loss:  258.5573553442955
test acc: top1 ->  70.748 ; top5 ->  89.688  and loss:  956.3505048155785
forward train acc: top1 ->  69.5625 ; top5 ->  87.4140625  and loss:  254.35765272378922
test acc: top1 ->  70.744 ; top5 ->  89.71  and loss:  955.1164122223854
forward train acc: top1 ->  69.53125 ; top5 ->  87.5078125  and loss:  254.87494671344757
test acc: top1 ->  70.76 ; top5 ->  89.708  and loss:  955.3225735425949
forward train acc: top1 ->  69.3125 ; top5 ->  87.515625  and loss:  257.1810137629509
test acc: top1 ->  70.822 ; top5 ->  89.74  and loss:  953.1785787343979
forward train acc: top1 ->  69.0 ; top5 ->  87.421875  and loss:  257.52449929714203
test acc: top1 ->  70.932 ; top5 ->  89.814  and loss:  952.5428023338318
forward train acc: top1 ->  69.203125 ; top5 ->  87.421875  and loss:  257.49938440322876
test acc: top1 ->  70.852 ; top5 ->  89.772  and loss:  953.0363910794258
forward train acc: top1 ->  69.2265625 ; top5 ->  87.453125  and loss:  258.07283025979996
test acc: top1 ->  70.896 ; top5 ->  89.796  and loss:  950.1567310094833
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  40 / 64 , inc:  1
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  3  ###
---------------- start layer  7  ---------------
adv train loss:  -61.40213018655777 , diff:  61.40213018655777
adv train loss:  -67.40058326721191 , diff:  5.998453080654144
layer  7  adv train finish, try to retain  119
test acc: top1 ->  70.338 ; top5 ->  89.316  and loss:  974.4175726175308
forward train acc: top1 ->  68.6875 ; top5 ->  86.875  and loss:  262.62150025367737
test acc: top1 ->  70.826 ; top5 ->  89.694  and loss:  953.2195726037025
forward train acc: top1 ->  68.8671875 ; top5 ->  87.1796875  and loss:  262.77166825532913
test acc: top1 ->  70.846 ; top5 ->  89.74  and loss:  951.4339234232903
forward train acc: top1 ->  68.6328125 ; top5 ->  87.2109375  and loss:  261.35404229164124
test acc: top1 ->  70.818 ; top5 ->  89.728  and loss:  952.852590739727
forward train acc: top1 ->  69.578125 ; top5 ->  87.390625  and loss:  257.291958630085
test acc: top1 ->  70.806 ; top5 ->  89.744  and loss:  949.2582214474678
forward train acc: top1 ->  69.96875 ; top5 ->  87.3046875  and loss:  253.5007545351982
test acc: top1 ->  70.846 ; top5 ->  89.748  and loss:  949.2440048456192
forward train acc: top1 ->  69.609375 ; top5 ->  87.4765625  and loss:  257.85729748010635
test acc: top1 ->  70.864 ; top5 ->  89.778  and loss:  949.9760731458664
forward train acc: top1 ->  69.4765625 ; top5 ->  87.421875  and loss:  256.6354777812958
test acc: top1 ->  70.922 ; top5 ->  89.824  and loss:  945.5889070630074
forward train acc: top1 ->  69.4609375 ; top5 ->  87.265625  and loss:  259.9894261956215
test acc: top1 ->  70.914 ; top5 ->  89.782  and loss:  947.4522294402122
forward train acc: top1 ->  69.546875 ; top5 ->  87.671875  and loss:  255.65938693284988
test acc: top1 ->  70.91 ; top5 ->  89.796  and loss:  949.0300315022469
forward train acc: top1 ->  69.609375 ; top5 ->  87.9296875  and loss:  251.0001096725464
test acc: top1 ->  70.96 ; top5 ->  89.83  and loss:  946.683520257473
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  122 / 128 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -64.42753654718399 , diff:  64.42753654718399
adv train loss:  -64.42272883653641 , diff:  0.004807710647583008
layer  8  adv train finish, try to retain  94
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -61.699224174022675 , diff:  61.699224174022675
adv train loss:  -62.39900803565979 , diff:  0.6997838616371155
layer  9  adv train finish, try to retain  96
test acc: top1 ->  69.988 ; top5 ->  89.334  and loss:  980.8161532282829
forward train acc: top1 ->  68.5 ; top5 ->  87.6015625  and loss:  258.4143387079239
test acc: top1 ->  70.404 ; top5 ->  89.638  and loss:  961.4582688808441
forward train acc: top1 ->  68.9296875 ; top5 ->  87.4375  and loss:  256.75403636693954
test acc: top1 ->  70.54 ; top5 ->  89.738  and loss:  957.7587180733681
forward train acc: top1 ->  68.71875 ; top5 ->  87.265625  and loss:  262.4257401227951
test acc: top1 ->  70.676 ; top5 ->  89.716  and loss:  957.5825740098953
forward train acc: top1 ->  68.8359375 ; top5 ->  87.3671875  and loss:  260.94896227121353
test acc: top1 ->  70.708 ; top5 ->  89.766  and loss:  953.6911145448685
forward train acc: top1 ->  69.7890625 ; top5 ->  87.7265625  and loss:  254.8482112288475
test acc: top1 ->  70.718 ; top5 ->  89.768  and loss:  954.8998953700066
forward train acc: top1 ->  68.796875 ; top5 ->  87.4921875  and loss:  259.6225864291191
test acc: top1 ->  70.754 ; top5 ->  89.75  and loss:  950.8897612094879
forward train acc: top1 ->  69.09375 ; top5 ->  87.4453125  and loss:  262.26353335380554
test acc: top1 ->  70.624 ; top5 ->  89.708  and loss:  950.9540569186211
forward train acc: top1 ->  69.6484375 ; top5 ->  87.3046875  and loss:  253.05277782678604
test acc: top1 ->  70.732 ; top5 ->  89.778  and loss:  950.3918313980103
forward train acc: top1 ->  69.234375 ; top5 ->  87.0234375  and loss:  257.63205575942993
test acc: top1 ->  70.788 ; top5 ->  89.804  and loss:  950.8437923789024
forward train acc: top1 ->  69.7265625 ; top5 ->  87.265625  and loss:  258.5068029165268
test acc: top1 ->  70.8 ; top5 ->  89.79  and loss:  949.2673660516739
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  115 / 128 , inc:  1
---------------- start layer  10  ---------------
### skip layer  10 wait:  3  ###
---------------- start layer  11  ---------------
adv train loss:  -61.49234336614609 , diff:  61.49234336614609
adv train loss:  -64.38027656078339 , diff:  2.8879331946372986
layer  11  adv train finish, try to retain  88
test acc: top1 ->  69.48 ; top5 ->  88.964  and loss:  998.7880724072456
forward train acc: top1 ->  69.109375 ; top5 ->  87.3046875  and loss:  259.79712653160095
test acc: top1 ->  70.726 ; top5 ->  89.722  and loss:  956.6202027201653
forward train acc: top1 ->  69.5390625 ; top5 ->  87.375  and loss:  258.1254808306694
test acc: top1 ->  70.714 ; top5 ->  89.716  and loss:  956.2194929718971
forward train acc: top1 ->  69.734375 ; top5 ->  87.2734375  and loss:  256.53622102737427
test acc: top1 ->  70.684 ; top5 ->  89.71  and loss:  954.4109655618668
forward train acc: top1 ->  68.6171875 ; top5 ->  86.9765625  and loss:  261.98450845479965
test acc: top1 ->  70.858 ; top5 ->  89.742  and loss:  952.7940428256989
forward train acc: top1 ->  69.3515625 ; top5 ->  87.1484375  and loss:  261.1748596429825
test acc: top1 ->  70.796 ; top5 ->  89.706  and loss:  951.2611422538757
forward train acc: top1 ->  70.1953125 ; top5 ->  87.546875  and loss:  251.44290924072266
test acc: top1 ->  70.934 ; top5 ->  89.85  and loss:  948.6753402352333
forward train acc: top1 ->  69.2890625 ; top5 ->  87.40625  and loss:  259.3359884619713
test acc: top1 ->  70.814 ; top5 ->  89.856  and loss:  946.1280681490898
forward train acc: top1 ->  69.234375 ; top5 ->  87.3203125  and loss:  257.39619129896164
test acc: top1 ->  70.948 ; top5 ->  89.842  and loss:  948.6031445860863
forward train acc: top1 ->  69.8671875 ; top5 ->  87.8125  and loss:  255.90276384353638
test acc: top1 ->  71.01 ; top5 ->  89.852  and loss:  944.3785850405693
forward train acc: top1 ->  69.7265625 ; top5 ->  87.5390625  and loss:  257.7861647605896
test acc: top1 ->  70.918 ; top5 ->  89.878  and loss:  946.4134039282799
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -63.20758122205734 , diff:  63.20758122205734
adv train loss:  -64.951955139637 , diff:  1.7443739175796509
layer  12  adv train finish, try to retain  81
test acc: top1 ->  69.728 ; top5 ->  89.078  and loss:  991.7012363672256
forward train acc: top1 ->  68.6171875 ; top5 ->  86.90625  and loss:  264.51835346221924
test acc: top1 ->  70.594 ; top5 ->  89.602  and loss:  959.9499378800392
forward train acc: top1 ->  69.640625 ; top5 ->  87.6484375  and loss:  254.79381674528122
test acc: top1 ->  70.596 ; top5 ->  89.658  and loss:  960.2524724006653
forward train acc: top1 ->  69.28125 ; top5 ->  87.3046875  and loss:  258.6606088876724
test acc: top1 ->  70.534 ; top5 ->  89.59  and loss:  961.6166333556175
forward train acc: top1 ->  69.3046875 ; top5 ->  87.4453125  and loss:  256.41366297006607
test acc: top1 ->  70.674 ; top5 ->  89.636  and loss:  957.7117981314659
forward train acc: top1 ->  69.8046875 ; top5 ->  87.3125  and loss:  252.22417563199997
test acc: top1 ->  70.78 ; top5 ->  89.734  and loss:  954.1921702623367
forward train acc: top1 ->  69.1875 ; top5 ->  87.8125  and loss:  254.9118782877922
test acc: top1 ->  70.858 ; top5 ->  89.722  and loss:  952.5864745378494
forward train acc: top1 ->  69.34375 ; top5 ->  87.71875  and loss:  257.19251120090485
test acc: top1 ->  70.862 ; top5 ->  89.76  and loss:  950.591388463974
forward train acc: top1 ->  69.0703125 ; top5 ->  87.140625  and loss:  261.88309079408646
test acc: top1 ->  70.79 ; top5 ->  89.708  and loss:  951.8557310700417
forward train acc: top1 ->  69.34375 ; top5 ->  87.5390625  and loss:  251.86196088790894
test acc: top1 ->  70.862 ; top5 ->  89.756  and loss:  951.7134299278259
forward train acc: top1 ->  69.6484375 ; top5 ->  87.46875  and loss:  253.98368668556213
test acc: top1 ->  70.848 ; top5 ->  89.774  and loss:  950.3969873785973
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  98 / 128 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -63.94479703903198 , diff:  63.94479703903198
adv train loss:  -65.37870573997498 , diff:  1.4339087009429932
layer  13  adv train finish, try to retain  73
>>>>>>> reverse layer  13  since no improvement >>>>>>>
---------------- start layer  14  ---------------
adv train loss:  -64.26721841096878 , diff:  64.26721841096878
adv train loss:  -66.46395826339722 , diff:  2.1967398524284363
layer  14  adv train finish, try to retain  250
>>>>>>> reverse layer  14  since no improvement >>>>>>>
---------------- start layer  15  ---------------
adv train loss:  -62.89888924360275 , diff:  62.89888924360275
adv train loss:  -66.42250245809555 , diff:  3.523613214492798
layer  15  adv train finish, try to retain  242
test acc: top1 ->  70.058 ; top5 ->  89.368  and loss:  975.8551209568977
forward train acc: top1 ->  69.3984375 ; top5 ->  86.8671875  and loss:  263.9996535181999
test acc: top1 ->  70.55 ; top5 ->  89.62  and loss:  959.4892373681068
forward train acc: top1 ->  69.1640625 ; top5 ->  87.4375  and loss:  258.66310089826584
test acc: top1 ->  70.486 ; top5 ->  89.67  and loss:  960.2063611745834
forward train acc: top1 ->  68.65625 ; top5 ->  87.453125  and loss:  258.4545111656189
test acc: top1 ->  70.602 ; top5 ->  89.618  and loss:  960.0636646747589
forward train acc: top1 ->  69.359375 ; top5 ->  87.71875  and loss:  255.4282784461975
test acc: top1 ->  70.672 ; top5 ->  89.74  and loss:  954.2045461535454
forward train acc: top1 ->  69.53125 ; top5 ->  87.453125  and loss:  258.07809227705
test acc: top1 ->  70.794 ; top5 ->  89.78  and loss:  953.5665251016617
forward train acc: top1 ->  69.03125 ; top5 ->  87.5234375  and loss:  254.0457136631012
test acc: top1 ->  70.804 ; top5 ->  89.806  and loss:  955.8116253614426
forward train acc: top1 ->  70.2890625 ; top5 ->  87.4453125  and loss:  252.88232201337814
test acc: top1 ->  70.962 ; top5 ->  89.794  and loss:  952.3477315306664
forward train acc: top1 ->  69.3125 ; top5 ->  87.3046875  and loss:  256.699912071228
test acc: top1 ->  70.932 ; top5 ->  89.764  and loss:  950.8283809423447
forward train acc: top1 ->  68.953125 ; top5 ->  87.4453125  and loss:  258.02326786518097
test acc: top1 ->  71.028 ; top5 ->  89.794  and loss:  950.8642190694809
forward train acc: top1 ->  69.40625 ; top5 ->  86.96875  and loss:  260.62852746248245
test acc: top1 ->  70.974 ; top5 ->  89.8  and loss:  950.1457041501999
>>>>>>> reverse layer  15  since performance drop >>>>>>>
==> this epoch:  248 / 256 , inc:  1
---------------- start layer  16  ---------------
### skip layer  16 wait:  3  ###
---------------- start layer  17  ---------------
adv train loss:  -64.58681452274323 , diff:  64.58681452274323
adv train loss:  -63.727035999298096 , diff:  0.8597785234451294
layer  17  adv train finish, try to retain  225
test acc: top1 ->  70.334 ; top5 ->  89.424  and loss:  980.7515141367912
forward train acc: top1 ->  69.2578125 ; top5 ->  87.5625  and loss:  257.7487023472786
test acc: top1 ->  70.708 ; top5 ->  89.682  and loss:  962.6264092922211
forward train acc: top1 ->  69.109375 ; top5 ->  86.703125  and loss:  261.1560790538788
test acc: top1 ->  70.694 ; top5 ->  89.676  and loss:  958.7799689173698
forward train acc: top1 ->  69.1171875 ; top5 ->  86.84375  and loss:  263.3135979771614
test acc: top1 ->  70.81 ; top5 ->  89.724  and loss:  956.1200024485588
forward train acc: top1 ->  69.5546875 ; top5 ->  87.359375  and loss:  258.6407214999199
test acc: top1 ->  70.774 ; top5 ->  89.728  and loss:  956.7813652157784
forward train acc: top1 ->  68.8671875 ; top5 ->  87.1015625  and loss:  258.58620125055313
test acc: top1 ->  70.826 ; top5 ->  89.756  and loss:  954.6780681014061
forward train acc: top1 ->  69.8125 ; top5 ->  87.921875  and loss:  252.51970994472504
test acc: top1 ->  70.746 ; top5 ->  89.734  and loss:  953.9220938682556
forward train acc: top1 ->  69.0703125 ; top5 ->  87.1640625  and loss:  257.12474620342255
test acc: top1 ->  70.884 ; top5 ->  89.764  and loss:  953.1338159441948
forward train acc: top1 ->  69.296875 ; top5 ->  87.1796875  and loss:  260.3640692830086
test acc: top1 ->  70.876 ; top5 ->  89.792  and loss:  952.4113536477089
forward train acc: top1 ->  69.7421875 ; top5 ->  87.5390625  and loss:  255.4831258058548
test acc: top1 ->  70.858 ; top5 ->  89.808  and loss:  951.2718106508255
forward train acc: top1 ->  68.921875 ; top5 ->  87.296875  and loss:  258.72523003816605
test acc: top1 ->  70.832 ; top5 ->  89.804  and loss:  950.7349769473076
>>>>>>> reverse layer  17  since performance drop >>>>>>>
==> this epoch:  230 / 256 , inc:  1
---------------- start layer  18  ---------------
### skip layer  18 wait:  4  ###
---------------- start layer  19  ---------------
adv train loss:  -63.40543353557587 , diff:  63.40543353557587
adv train loss:  -61.89206898212433 , diff:  1.513364553451538
layer  19  adv train finish, try to retain  173
test acc: top1 ->  70.436 ; top5 ->  89.49  and loss:  966.5249249339104
forward train acc: top1 ->  69.5859375 ; top5 ->  87.65625  and loss:  254.25675809383392
test acc: top1 ->  70.558 ; top5 ->  89.628  and loss:  959.7555814385414
forward train acc: top1 ->  69.7265625 ; top5 ->  87.234375  and loss:  256.34125953912735
test acc: top1 ->  70.516 ; top5 ->  89.684  and loss:  959.9662138223648
forward train acc: top1 ->  68.6875 ; top5 ->  87.3828125  and loss:  260.0652920603752
test acc: top1 ->  70.694 ; top5 ->  89.67  and loss:  959.4846846461296
forward train acc: top1 ->  68.5078125 ; top5 ->  87.2421875  and loss:  260.07128316164017
test acc: top1 ->  70.764 ; top5 ->  89.702  and loss:  956.7020551562309
forward train acc: top1 ->  69.25 ; top5 ->  87.21875  and loss:  259.66585898399353
test acc: top1 ->  70.746 ; top5 ->  89.68  and loss:  956.8076344132423
forward train acc: top1 ->  69.234375 ; top5 ->  87.7265625  and loss:  256.0810607075691
test acc: top1 ->  70.766 ; top5 ->  89.74  and loss:  952.6344223022461
forward train acc: top1 ->  69.046875 ; top5 ->  87.1171875  and loss:  258.7474491596222
test acc: top1 ->  70.802 ; top5 ->  89.754  and loss:  951.5579617023468
forward train acc: top1 ->  69.875 ; top5 ->  88.0859375  and loss:  252.3557224869728
test acc: top1 ->  70.81 ; top5 ->  89.752  and loss:  951.3064696788788
forward train acc: top1 ->  70.0859375 ; top5 ->  87.6875  and loss:  254.8602727651596
test acc: top1 ->  70.858 ; top5 ->  89.724  and loss:  953.4501357674599
forward train acc: top1 ->  69.421875 ; top5 ->  87.5703125  and loss:  256.6956456899643
test acc: top1 ->  70.914 ; top5 ->  89.758  and loss:  950.6357398033142
>>>>>>> reverse layer  19  since performance drop >>>>>>>
==> this epoch:  179 / 256 , inc:  1
---------------- start layer  20  ---------------
### skip layer  20 wait:  4  ###
---------------- start layer  21  ---------------
adv train loss:  -62.80319559574127 , diff:  62.80319559574127
adv train loss:  -64.1250491142273 , diff:  1.321853518486023
layer  21  adv train finish, try to retain  235
test acc: top1 ->  70.964 ; top5 ->  89.814  and loss:  946.8909413218498
forward train acc: top1 ->  69.265625 ; top5 ->  87.5625  and loss:  256.39875185489655
test acc: top1 ->  70.962 ; top5 ->  89.792  and loss:  947.6887094974518
forward train acc: top1 ->  70.0 ; top5 ->  87.4609375  and loss:  256.6869713664055
test acc: top1 ->  70.842 ; top5 ->  89.76  and loss:  950.9163541197777
forward train acc: top1 ->  69.3984375 ; top5 ->  87.7734375  and loss:  253.9949695467949
test acc: top1 ->  70.814 ; top5 ->  89.694  and loss:  952.0024436116219
forward train acc: top1 ->  69.265625 ; top5 ->  87.34375  and loss:  259.09532219171524
test acc: top1 ->  70.954 ; top5 ->  89.738  and loss:  946.9476590156555
forward train acc: top1 ->  69.4765625 ; top5 ->  87.7421875  and loss:  255.79889678955078
test acc: top1 ->  70.87 ; top5 ->  89.788  and loss:  947.4967157840729
forward train acc: top1 ->  69.5546875 ; top5 ->  87.6953125  and loss:  254.9620577096939
test acc: top1 ->  70.884 ; top5 ->  89.816  and loss:  945.2229745388031
forward train acc: top1 ->  69.0625 ; top5 ->  87.2265625  and loss:  258.0730719566345
test acc: top1 ->  71.014 ; top5 ->  89.826  and loss:  942.5497028231621
forward train acc: top1 ->  69.8828125 ; top5 ->  87.5390625  and loss:  255.13336607813835
test acc: top1 ->  70.914 ; top5 ->  89.79  and loss:  943.9415058493614
forward train acc: top1 ->  69.671875 ; top5 ->  87.671875  and loss:  254.12404209375381
test acc: top1 ->  71.082 ; top5 ->  89.866  and loss:  946.155110180378
forward train acc: top1 ->  69.9453125 ; top5 ->  87.734375  and loss:  252.4415636062622
test acc: top1 ->  71.066 ; top5 ->  89.808  and loss:  942.6559563279152
>>>>>>> reverse layer  21  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  22  ---------------
adv train loss:  -62.305563509464264 , diff:  62.305563509464264
adv train loss:  -63.51920682191849 , diff:  1.2136433124542236
layer  22  adv train finish, try to retain  183
test acc: top1 ->  70.584 ; top5 ->  89.522  and loss:  969.0618797540665
forward train acc: top1 ->  69.6171875 ; top5 ->  87.6875  and loss:  256.2112882733345
test acc: top1 ->  70.562 ; top5 ->  89.566  and loss:  960.1609570980072
forward train acc: top1 ->  69.265625 ; top5 ->  87.4765625  and loss:  257.35042107105255
test acc: top1 ->  70.718 ; top5 ->  89.526  and loss:  957.9732769727707
forward train acc: top1 ->  69.015625 ; top5 ->  87.4453125  and loss:  260.3196182847023
test acc: top1 ->  70.556 ; top5 ->  89.586  and loss:  960.4802884459496
forward train acc: top1 ->  69.4296875 ; top5 ->  87.171875  and loss:  259.55244368314743
test acc: top1 ->  70.622 ; top5 ->  89.732  and loss:  953.5127013325691
forward train acc: top1 ->  68.96875 ; top5 ->  87.34375  and loss:  259.90047776699066
test acc: top1 ->  70.754 ; top5 ->  89.804  and loss:  950.9966711997986
forward train acc: top1 ->  69.0390625 ; top5 ->  87.2421875  and loss:  259.28616404533386
test acc: top1 ->  70.702 ; top5 ->  89.714  and loss:  952.6613355278969
forward train acc: top1 ->  70.015625 ; top5 ->  87.6875  and loss:  252.2515240907669
test acc: top1 ->  70.88 ; top5 ->  89.746  and loss:  951.5095745921135
forward train acc: top1 ->  69.0859375 ; top5 ->  87.734375  and loss:  256.6574471592903
test acc: top1 ->  70.792 ; top5 ->  89.742  and loss:  951.2262098193169
forward train acc: top1 ->  69.5234375 ; top5 ->  87.4453125  and loss:  256.2476766705513
test acc: top1 ->  70.764 ; top5 ->  89.764  and loss:  950.4697378873825
forward train acc: top1 ->  69.921875 ; top5 ->  87.546875  and loss:  255.9539013504982
test acc: top1 ->  70.818 ; top5 ->  89.808  and loss:  949.8113462328911
>>>>>>> reverse layer  22  since performance drop >>>>>>>
==> this epoch:  208 / 256 , inc:  1
---------------- start layer  23  ---------------
adv train loss:  -64.82118541002274 , diff:  64.82118541002274
adv train loss:  -63.15105038881302 , diff:  1.6701350212097168
layer  23  adv train finish, try to retain  179
test acc: top1 ->  70.668 ; top5 ->  89.482  and loss:  969.101861000061
forward train acc: top1 ->  69.9140625 ; top5 ->  87.8046875  and loss:  252.9779703617096
test acc: top1 ->  70.58 ; top5 ->  89.7  and loss:  960.719389796257
forward train acc: top1 ->  69.2734375 ; top5 ->  87.5390625  and loss:  255.60617727041245
test acc: top1 ->  70.73 ; top5 ->  89.656  and loss:  955.8625012636185
forward train acc: top1 ->  68.8359375 ; top5 ->  87.2265625  and loss:  263.1934540271759
test acc: top1 ->  70.77 ; top5 ->  89.668  and loss:  956.0797362327576
forward train acc: top1 ->  69.1015625 ; top5 ->  87.03125  and loss:  260.73290383815765
test acc: top1 ->  70.66 ; top5 ->  89.588  and loss:  958.8844171762466
forward train acc: top1 ->  69.4921875 ; top5 ->  87.3515625  and loss:  259.5701547861099
test acc: top1 ->  70.832 ; top5 ->  89.646  and loss:  954.7393448352814
forward train acc: top1 ->  69.9375 ; top5 ->  87.5078125  and loss:  255.1069067120552
test acc: top1 ->  70.738 ; top5 ->  89.68  and loss:  953.4978730082512
forward train acc: top1 ->  69.1953125 ; top5 ->  86.9765625  and loss:  260.37745064496994
test acc: top1 ->  70.746 ; top5 ->  89.71  and loss:  953.2502889037132
forward train acc: top1 ->  69.625 ; top5 ->  87.34375  and loss:  258.2403421998024
test acc: top1 ->  70.854 ; top5 ->  89.734  and loss:  951.3679984211922
forward train acc: top1 ->  69.2890625 ; top5 ->  87.4453125  and loss:  257.252010345459
test acc: top1 ->  70.826 ; top5 ->  89.79  and loss:  948.7309674620628
forward train acc: top1 ->  69.8359375 ; top5 ->  88.1328125  and loss:  250.16254341602325
test acc: top1 ->  70.846 ; top5 ->  89.774  and loss:  950.7505404949188
>>>>>>> reverse layer  23  since performance drop >>>>>>>
==> this epoch:  187 / 256 , inc:  1
---------------- start layer  24  ---------------
adv train loss:  -62.55266809463501 , diff:  62.55266809463501
adv train loss:  -64.50602221488953 , diff:  1.9533541202545166
layer  24  adv train finish, try to retain  244
>>>>>>> reverse layer  24  since no improvement >>>>>>>
---------------- start layer  25  ---------------
adv train loss:  -65.03623926639557 , diff:  65.03623926639557
adv train loss:  -60.67009377479553 , diff:  4.366145491600037
layer  25  adv train finish, try to retain  226
test acc: top1 ->  70.486 ; top5 ->  89.628  and loss:  961.0397523641586
forward train acc: top1 ->  69.015625 ; top5 ->  87.4765625  and loss:  257.1532949209213
test acc: top1 ->  70.722 ; top5 ->  89.704  and loss:  953.9974571466446
forward train acc: top1 ->  68.90625 ; top5 ->  87.3984375  and loss:  259.2481199502945
test acc: top1 ->  70.85 ; top5 ->  89.706  and loss:  951.6531847119331
forward train acc: top1 ->  70.2890625 ; top5 ->  87.8359375  and loss:  249.3051010966301
test acc: top1 ->  70.802 ; top5 ->  89.77  and loss:  951.4230194687843
forward train acc: top1 ->  69.4921875 ; top5 ->  87.7109375  and loss:  253.9066470861435
test acc: top1 ->  70.824 ; top5 ->  89.774  and loss:  950.1549845337868
forward train acc: top1 ->  69.2734375 ; top5 ->  87.78125  and loss:  255.93149310350418
test acc: top1 ->  70.826 ; top5 ->  89.868  and loss:  949.9231118559837
forward train acc: top1 ->  69.59375 ; top5 ->  87.796875  and loss:  253.9799302816391
test acc: top1 ->  70.92 ; top5 ->  89.846  and loss:  948.507741689682
forward train acc: top1 ->  68.875 ; top5 ->  87.5546875  and loss:  258.7699201107025
test acc: top1 ->  70.954 ; top5 ->  89.894  and loss:  948.1838611960411
forward train acc: top1 ->  69.34375 ; top5 ->  87.5703125  and loss:  256.3265324831009
test acc: top1 ->  70.932 ; top5 ->  89.896  and loss:  945.0035467743874
forward train acc: top1 ->  69.7421875 ; top5 ->  87.9921875  and loss:  250.84290730953217
test acc: top1 ->  70.896 ; top5 ->  89.876  and loss:  945.5783064365387
forward train acc: top1 ->  70.15625 ; top5 ->  88.0390625  and loss:  249.2799961566925
test acc: top1 ->  70.976 ; top5 ->  89.908  and loss:  947.0921593308449
>>>>>>> reverse layer  25  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  26  ---------------
adv train loss:  -61.92927831411362 , diff:  61.92927831411362
adv train loss:  -64.95251816511154 , diff:  3.023239850997925
layer  26  adv train finish, try to retain  505
>>>>>>> reverse layer  26  since no improvement >>>>>>>
---------------- start layer  27  ---------------
adv train loss:  -67.7320294380188 , diff:  67.7320294380188
adv train loss:  -62.215512216091156 , diff:  5.516517221927643
layer  27  adv train finish, try to retain  498
test acc: top1 ->  70.796 ; top5 ->  89.824  and loss:  955.492976307869
forward train acc: top1 ->  70.0078125 ; top5 ->  87.7421875  and loss:  252.28066664934158
test acc: top1 ->  70.734 ; top5 ->  89.806  and loss:  952.380374789238
forward train acc: top1 ->  69.40625 ; top5 ->  87.53125  and loss:  255.48788809776306
test acc: top1 ->  70.744 ; top5 ->  89.806  and loss:  955.9533434510231
forward train acc: top1 ->  69.5 ; top5 ->  87.625  and loss:  254.57981956005096
test acc: top1 ->  70.748 ; top5 ->  89.732  and loss:  956.3181327581406
forward train acc: top1 ->  69.6328125 ; top5 ->  87.6171875  and loss:  254.7771639227867
test acc: top1 ->  70.77 ; top5 ->  89.766  and loss:  954.7161263823509
forward train acc: top1 ->  69.40625 ; top5 ->  87.53125  and loss:  257.86934292316437
test acc: top1 ->  70.796 ; top5 ->  89.822  and loss:  955.0953463315964
forward train acc: top1 ->  68.5859375 ; top5 ->  87.4140625  and loss:  258.71126836538315
test acc: top1 ->  70.758 ; top5 ->  89.752  and loss:  951.2610118985176
forward train acc: top1 ->  68.71875 ; top5 ->  86.9609375  and loss:  262.0931481719017
test acc: top1 ->  70.926 ; top5 ->  89.75  and loss:  950.0617328882217
forward train acc: top1 ->  69.875 ; top5 ->  87.5390625  and loss:  253.55232322216034
test acc: top1 ->  70.896 ; top5 ->  89.784  and loss:  951.0786536931992
forward train acc: top1 ->  69.640625 ; top5 ->  87.28125  and loss:  259.2638301849365
test acc: top1 ->  70.952 ; top5 ->  89.798  and loss:  949.1131380796432
forward train acc: top1 ->  69.046875 ; top5 ->  87.609375  and loss:  257.01960426568985
test acc: top1 ->  70.9 ; top5 ->  89.828  and loss:  947.228443145752
>>>>>>> reverse layer  27  since performance drop >>>>>>>
==> this epoch:  505 / 512 , inc:  1
---------------- start layer  28  ---------------
adv train loss:  -62.51113510131836 , diff:  62.51113510131836
adv train loss:  -66.1701227426529 , diff:  3.6589876413345337
layer  28  adv train finish, try to retain  506
>>>>>>> reverse layer  28  since no improvement >>>>>>>
---------------- start layer  29  ---------------
adv train loss:  -63.940322279930115 , diff:  63.940322279930115
adv train loss:  -64.56540477275848 , diff:  0.6250824928283691
layer  29  adv train finish, try to retain  489
test acc: top1 ->  70.93 ; top5 ->  89.816  and loss:  949.7009932398796
forward train acc: top1 ->  69.046875 ; top5 ->  86.7109375  and loss:  260.7826398611069
test acc: top1 ->  70.796 ; top5 ->  89.826  and loss:  949.770992577076
forward train acc: top1 ->  69.2890625 ; top5 ->  87.5625  and loss:  257.53623908758163
test acc: top1 ->  70.786 ; top5 ->  89.788  and loss:  949.8871618509293
forward train acc: top1 ->  70.359375 ; top5 ->  87.984375  and loss:  247.9202772974968
test acc: top1 ->  70.93 ; top5 ->  89.798  and loss:  948.5891790390015
forward train acc: top1 ->  70.0859375 ; top5 ->  88.140625  and loss:  249.8780951499939
test acc: top1 ->  70.832 ; top5 ->  89.862  and loss:  948.7619513273239
forward train acc: top1 ->  70.234375 ; top5 ->  88.0234375  and loss:  248.8423364162445
test acc: top1 ->  70.83 ; top5 ->  89.82  and loss:  945.8933516144753
forward train acc: top1 ->  69.421875 ; top5 ->  87.8203125  and loss:  251.90181678533554
test acc: top1 ->  70.974 ; top5 ->  89.84  and loss:  945.9717798829079
forward train acc: top1 ->  69.3359375 ; top5 ->  86.984375  and loss:  260.4425644874573
test acc: top1 ->  70.998 ; top5 ->  89.902  and loss:  941.9440203905106
forward train acc: top1 ->  69.6171875 ; top5 ->  87.796875  and loss:  255.2717946767807
test acc: top1 ->  70.988 ; top5 ->  89.926  and loss:  943.3542098999023
forward train acc: top1 ->  70.015625 ; top5 ->  87.609375  and loss:  251.10606181621552
test acc: top1 ->  71.0 ; top5 ->  89.86  and loss:  942.0360090732574
forward train acc: top1 ->  69.1796875 ; top5 ->  87.5859375  and loss:  254.4017259478569
test acc: top1 ->  71.01 ; top5 ->  89.912  and loss:  942.5073150992393
>>>>>>> reverse layer  29  since performance drop >>>>>>>
==> this epoch:  498 / 512 , inc:  1
---------------- start layer  30  ---------------
adv train loss:  -60.92465937137604 , diff:  60.92465937137604
adv train loss:  -61.49229460954666 , diff:  0.5676352381706238
layer  30  adv train finish, try to retain  482
test acc: top1 ->  70.384 ; top5 ->  89.466  and loss:  1011.7880573272705
forward train acc: top1 ->  69.046875 ; top5 ->  87.328125  and loss:  258.92333483695984
test acc: top1 ->  70.504 ; top5 ->  89.632  and loss:  967.6072353720665
forward train acc: top1 ->  69.1640625 ; top5 ->  87.671875  and loss:  259.24469858407974
test acc: top1 ->  70.62 ; top5 ->  89.536  and loss:  962.7901790738106
forward train acc: top1 ->  69.234375 ; top5 ->  87.7578125  and loss:  256.62320750951767
test acc: top1 ->  70.552 ; top5 ->  89.632  and loss:  965.9835129976273
forward train acc: top1 ->  69.0703125 ; top5 ->  87.4921875  and loss:  258.74216026067734
test acc: top1 ->  70.734 ; top5 ->  89.782  and loss:  960.3335824608803
forward train acc: top1 ->  69.21875 ; top5 ->  86.9140625  and loss:  261.31814658641815
test acc: top1 ->  70.678 ; top5 ->  89.736  and loss:  962.8664581775665
forward train acc: top1 ->  68.921875 ; top5 ->  86.9921875  and loss:  262.0341565608978
test acc: top1 ->  70.71 ; top5 ->  89.72  and loss:  957.2028911709785
forward train acc: top1 ->  69.484375 ; top5 ->  87.5  and loss:  254.42184853553772
test acc: top1 ->  70.738 ; top5 ->  89.754  and loss:  959.7317494750023
forward train acc: top1 ->  70.0859375 ; top5 ->  87.9375  and loss:  252.80426841974258
test acc: top1 ->  70.74 ; top5 ->  89.714  and loss:  959.1485538482666
forward train acc: top1 ->  69.109375 ; top5 ->  87.328125  and loss:  257.3051242828369
test acc: top1 ->  70.776 ; top5 ->  89.706  and loss:  957.324886739254
forward train acc: top1 ->  69.3984375 ; top5 ->  87.2421875  and loss:  260.00256884098053
test acc: top1 ->  70.836 ; top5 ->  89.7  and loss:  956.2902447581291
>>>>>>> reverse layer  30  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  31  ---------------
adv train loss:  -61.17451751232147 , diff:  61.17451751232147
adv train loss:  -61.541413962841034 , diff:  0.36689645051956177
layer  31  adv train finish, try to retain  491
test acc: top1 ->  70.506 ; top5 ->  89.636  and loss:  961.8467575311661
forward train acc: top1 ->  69.2109375 ; top5 ->  87.234375  and loss:  259.74548053741455
test acc: top1 ->  70.658 ; top5 ->  89.674  and loss:  957.850637614727
forward train acc: top1 ->  69.0078125 ; top5 ->  87.046875  and loss:  260.0182032585144
test acc: top1 ->  70.74 ; top5 ->  89.676  and loss:  953.9727808833122
forward train acc: top1 ->  69.2734375 ; top5 ->  87.640625  and loss:  258.3098841905594
test acc: top1 ->  70.67 ; top5 ->  89.68  and loss:  955.4070873856544
forward train acc: top1 ->  69.6328125 ; top5 ->  87.03125  and loss:  258.0568894147873
test acc: top1 ->  70.85 ; top5 ->  89.812  and loss:  950.5997794866562
forward train acc: top1 ->  69.3203125 ; top5 ->  87.453125  and loss:  258.25014239549637
test acc: top1 ->  70.926 ; top5 ->  89.81  and loss:  946.7101128101349
forward train acc: top1 ->  70.03125 ; top5 ->  87.9609375  and loss:  250.87355989217758
test acc: top1 ->  70.998 ; top5 ->  89.864  and loss:  946.6582431197166
forward train acc: top1 ->  69.4296875 ; top5 ->  87.421875  and loss:  256.1669690608978
test acc: top1 ->  71.084 ; top5 ->  89.836  and loss:  944.1243313550949
forward train acc: top1 ->  69.375 ; top5 ->  87.921875  and loss:  253.79544109106064
test acc: top1 ->  70.936 ; top5 ->  89.902  and loss:  944.786646425724
forward train acc: top1 ->  70.171875 ; top5 ->  88.0234375  and loss:  249.2805380821228
test acc: top1 ->  71.048 ; top5 ->  89.886  and loss:  943.65107858181
forward train acc: top1 ->  70.0546875 ; top5 ->  87.7421875  and loss:  249.9805520772934
test acc: top1 ->  71.072 ; top5 ->  89.874  and loss:  944.4166013598442
>>>>>>> reverse layer  31  since performance drop >>>>>>>
==> this epoch:  501 / 512 , inc:  1
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.11390625000000001, 0.04271484375000001, 0.0010011291503906252, 0.028476562500000004, 0.021357421875000004, 0.11390625000000001, 0.0005005645751953126, 0.00018771171569824223, 0.014238281250000002, 0.0010011291503906252, 0.005339355468750001, 0.005339355468750001, 0.005339355468750001, 0.05695312500000001, 0.0002502822875976563, 9.385585784912111e-05, 0.0002502822875976563, 0.0005005645751953126, 0.0005005645751953126, 0.005339355468750001, 0.0005005645751953126, 0.0002502822875976563, 0.0026696777343750006, 0.005339355468750001, 0.0002502822875976563, 0.0002502822875976563, 0.00012514114379882815, 4.692792892456056e-05, 0.00012514114379882815, 0.00012514114379882815, 0.00012514114379882815, 0.00012514114379882815]  wait [2, 4, 2, 0, 3, 2, 2, 4, 0, 3, 2, 2, 2, 2, 2, 4, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 4, 2, 2, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 4
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  20  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -64.7108461856842 , diff:  64.7108461856842
adv train loss:  -64.67097479104996 , diff:  0.039871394634246826
layer  0  adv train finish, try to retain  31
test acc: top1 ->  63.866 ; top5 ->  85.072  and loss:  1200.7761489152908
forward train acc: top1 ->  68.453125 ; top5 ->  87.2890625  and loss:  261.78447461128235
test acc: top1 ->  70.442 ; top5 ->  89.436  and loss:  963.5767632722855
forward train acc: top1 ->  69.5625 ; top5 ->  87.0078125  and loss:  259.13771599531174
test acc: top1 ->  70.534 ; top5 ->  89.55  and loss:  963.610168159008
forward train acc: top1 ->  69.1484375 ; top5 ->  87.109375  and loss:  262.1227762699127
test acc: top1 ->  70.584 ; top5 ->  89.596  and loss:  961.0455039739609
forward train acc: top1 ->  69.6015625 ; top5 ->  87.3984375  and loss:  254.669080555439
test acc: top1 ->  70.586 ; top5 ->  89.618  and loss:  961.3228601813316
forward train acc: top1 ->  68.6640625 ; top5 ->  86.9609375  and loss:  263.0837767124176
test acc: top1 ->  70.708 ; top5 ->  89.68  and loss:  957.5452779531479
forward train acc: top1 ->  69.4921875 ; top5 ->  87.4375  and loss:  254.50405204296112
test acc: top1 ->  70.732 ; top5 ->  89.6  and loss:  955.8179008364677
forward train acc: top1 ->  69.453125 ; top5 ->  87.4921875  and loss:  257.91061967611313
test acc: top1 ->  70.796 ; top5 ->  89.672  and loss:  954.6573057770729
forward train acc: top1 ->  69.0390625 ; top5 ->  87.03125  and loss:  262.4521397948265
test acc: top1 ->  70.76 ; top5 ->  89.616  and loss:  955.750955760479
forward train acc: top1 ->  69.4453125 ; top5 ->  87.6640625  and loss:  256.56131279468536
test acc: top1 ->  70.768 ; top5 ->  89.678  and loss:  954.0698640346527
forward train acc: top1 ->  69.1796875 ; top5 ->  87.4609375  and loss:  259.68963146209717
test acc: top1 ->  70.836 ; top5 ->  89.66  and loss:  954.5211812257767
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  37 / 64 , inc:  1
---------------- start layer  1  ---------------
### skip layer  1 wait:  4  ###
---------------- start layer  2  ---------------
adv train loss:  -63.65881448984146 , diff:  63.65881448984146
adv train loss:  -65.16915130615234 , diff:  1.5103368163108826
************ all values are small in this layer **********
layer  2  adv train finish, try to retain  56
test acc: top1 ->  70.564 ; top5 ->  89.47  and loss:  959.7254531383514
forward train acc: top1 ->  69.21875 ; top5 ->  86.9453125  and loss:  261.2760378718376
test acc: top1 ->  70.78 ; top5 ->  89.76  and loss:  951.2662252783775
forward train acc: top1 ->  69.7421875 ; top5 ->  87.359375  and loss:  257.1510474085808
test acc: top1 ->  70.944 ; top5 ->  89.756  and loss:  948.663909971714
forward train acc: top1 ->  68.828125 ; top5 ->  87.3515625  and loss:  258.75244241952896
test acc: top1 ->  70.964 ; top5 ->  89.77  and loss:  946.7199454903603
forward train acc: top1 ->  68.5859375 ; top5 ->  86.84375  and loss:  266.36973118782043
test acc: top1 ->  70.958 ; top5 ->  89.834  and loss:  948.582923412323
forward train acc: top1 ->  69.296875 ; top5 ->  87.1640625  and loss:  257.89422339200974
test acc: top1 ->  70.97 ; top5 ->  89.85  and loss:  943.3070642948151
forward train acc: top1 ->  69.5234375 ; top5 ->  87.6015625  and loss:  254.96670442819595
test acc: top1 ->  71.02 ; top5 ->  89.784  and loss:  943.7540891170502
forward train acc: top1 ->  69.765625 ; top5 ->  87.3828125  and loss:  254.86710757017136
test acc: top1 ->  71.088 ; top5 ->  89.836  and loss:  944.2581779956818
forward train acc: top1 ->  69.7109375 ; top5 ->  87.8125  and loss:  254.32733064889908
test acc: top1 ->  71.04 ; top5 ->  89.884  and loss:  943.3583310246468
forward train acc: top1 ->  70.5390625 ; top5 ->  88.1484375  and loss:  246.99181133508682
test acc: top1 ->  71.096 ; top5 ->  89.904  and loss:  941.3258418440819
forward train acc: top1 ->  69.78125 ; top5 ->  87.9140625  and loss:  251.63693696260452
test acc: top1 ->  71.054 ; top5 ->  89.884  and loss:  942.0210577249527
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  57 / 64 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -64.88928091526031 , diff:  64.88928091526031
adv train loss:  -64.67105406522751 , diff:  0.2182268500328064
layer  3  adv train finish, try to retain  34
test acc: top1 ->  65.846 ; top5 ->  86.732  and loss:  1121.8195527791977
forward train acc: top1 ->  68.7734375 ; top5 ->  87.125  and loss:  264.33106952905655
test acc: top1 ->  70.326 ; top5 ->  89.476  and loss:  968.2160431742668
forward train acc: top1 ->  68.4609375 ; top5 ->  87.359375  and loss:  262.45871663093567
test acc: top1 ->  70.492 ; top5 ->  89.556  and loss:  961.9941884279251
forward train acc: top1 ->  69.6015625 ; top5 ->  87.7421875  and loss:  255.77508115768433
test acc: top1 ->  70.664 ; top5 ->  89.692  and loss:  963.8145205974579
forward train acc: top1 ->  68.6796875 ; top5 ->  87.484375  and loss:  259.6656633615494
test acc: top1 ->  70.806 ; top5 ->  89.71  and loss:  956.9886553883553
forward train acc: top1 ->  69.5625 ; top5 ->  87.578125  and loss:  254.5697146654129
test acc: top1 ->  70.726 ; top5 ->  89.738  and loss:  959.1421459317207
forward train acc: top1 ->  69.734375 ; top5 ->  87.515625  and loss:  256.0913838148117
test acc: top1 ->  70.824 ; top5 ->  89.722  and loss:  956.2980469465256
forward train acc: top1 ->  68.921875 ; top5 ->  87.4453125  and loss:  259.22684544324875
test acc: top1 ->  70.852 ; top5 ->  89.83  and loss:  953.3609975576401
forward train acc: top1 ->  68.953125 ; top5 ->  87.1796875  and loss:  259.4421554207802
test acc: top1 ->  70.792 ; top5 ->  89.726  and loss:  956.6065982580185
forward train acc: top1 ->  69.3359375 ; top5 ->  87.734375  and loss:  255.51662051677704
test acc: top1 ->  70.858 ; top5 ->  89.8  and loss:  950.9462658762932
forward train acc: top1 ->  69.125 ; top5 ->  87.109375  and loss:  257.2117183804512
test acc: top1 ->  70.842 ; top5 ->  89.826  and loss:  952.7801371216774
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  43 / 64 , inc:  1
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
adv train loss:  -65.50726956129074 , diff:  65.50726956129074
adv train loss:  -68.28955781459808 , diff:  2.7822882533073425
layer  5  adv train finish, try to retain  33
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -63.65959960222244 , diff:  63.65959960222244
adv train loss:  -63.31081873178482 , diff:  0.34878087043762207
************ all values are small in this layer **********
layer  6  adv train finish, try to retain  111
test acc: top1 ->  69.138 ; top5 ->  88.596  and loss:  1015.929942548275
forward train acc: top1 ->  70.21875 ; top5 ->  87.625  and loss:  254.27160388231277
test acc: top1 ->  70.948 ; top5 ->  89.614  and loss:  954.958900988102
forward train acc: top1 ->  69.5859375 ; top5 ->  87.8046875  and loss:  253.18915182352066
test acc: top1 ->  70.96 ; top5 ->  89.716  and loss:  952.5280959606171
forward train acc: top1 ->  68.9375 ; top5 ->  87.1796875  and loss:  261.11480885744095
test acc: top1 ->  70.964 ; top5 ->  89.732  and loss:  951.0949954986572
forward train acc: top1 ->  69.5078125 ; top5 ->  87.7578125  and loss:  251.93342089653015
test acc: top1 ->  71.094 ; top5 ->  89.748  and loss:  947.242800951004
forward train acc: top1 ->  69.203125 ; top5 ->  87.6640625  and loss:  256.64042925834656
test acc: top1 ->  71.072 ; top5 ->  89.794  and loss:  949.3432075381279
forward train acc: top1 ->  69.265625 ; top5 ->  87.2578125  and loss:  262.76149725914
test acc: top1 ->  71.054 ; top5 ->  89.824  and loss:  946.2815598249435
forward train acc: top1 ->  70.25 ; top5 ->  88.1953125  and loss:  247.22628420591354
test acc: top1 ->  71.112 ; top5 ->  89.77  and loss:  947.420767724514
forward train acc: top1 ->  70.1640625 ; top5 ->  88.0390625  and loss:  250.0010575056076
test acc: top1 ->  71.106 ; top5 ->  89.854  and loss:  945.4696654081345
forward train acc: top1 ->  69.328125 ; top5 ->  87.8125  and loss:  255.53851902484894
test acc: top1 ->  70.974 ; top5 ->  89.808  and loss:  947.8717387318611
forward train acc: top1 ->  70.375 ; top5 ->  87.734375  and loss:  254.21304672956467
test acc: top1 ->  71.144 ; top5 ->  89.808  and loss:  943.4088917970657
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  112 / 128 , inc:  1
---------------- start layer  7  ---------------
### skip layer  7 wait:  4  ###
---------------- start layer  8  ---------------
adv train loss:  -63.90318638086319 , diff:  63.90318638086319
adv train loss:  -64.69561737775803 , diff:  0.7924309968948364
layer  8  adv train finish, try to retain  79
test acc: top1 ->  68.79 ; top5 ->  88.63  and loss:  1015.4054086208344
forward train acc: top1 ->  68.90625 ; top5 ->  86.953125  and loss:  262.11761206388474
test acc: top1 ->  70.584 ; top5 ->  89.672  and loss:  957.0038264393806
forward train acc: top1 ->  69.578125 ; top5 ->  87.5  and loss:  254.7897242307663
test acc: top1 ->  70.68 ; top5 ->  89.764  and loss:  957.0284321308136
forward train acc: top1 ->  69.0234375 ; top5 ->  87.109375  and loss:  260.3259752392769
test acc: top1 ->  70.578 ; top5 ->  89.722  and loss:  957.2179474234581
forward train acc: top1 ->  69.4765625 ; top5 ->  88.0078125  and loss:  255.58307242393494
test acc: top1 ->  70.632 ; top5 ->  89.742  and loss:  956.1965699195862
forward train acc: top1 ->  69.625 ; top5 ->  86.9296875  and loss:  260.27936083078384
test acc: top1 ->  70.688 ; top5 ->  89.834  and loss:  951.7326956391335
forward train acc: top1 ->  69.03125 ; top5 ->  88.0234375  and loss:  255.23736137151718
test acc: top1 ->  70.722 ; top5 ->  89.752  and loss:  953.6719095110893
forward train acc: top1 ->  69.328125 ; top5 ->  87.6015625  and loss:  257.3375470638275
test acc: top1 ->  70.808 ; top5 ->  89.772  and loss:  952.1983981728554
forward train acc: top1 ->  70.1171875 ; top5 ->  87.9453125  and loss:  252.51057356595993
test acc: top1 ->  70.798 ; top5 ->  89.818  and loss:  950.1258152127266
forward train acc: top1 ->  69.21875 ; top5 ->  87.609375  and loss:  256.01161539554596
test acc: top1 ->  70.802 ; top5 ->  89.808  and loss:  948.6440788507462
forward train acc: top1 ->  69.4765625 ; top5 ->  87.671875  and loss:  253.73731178045273
test acc: top1 ->  70.834 ; top5 ->  89.862  and loss:  949.9123489260674
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  9  ---------------
### skip layer  9 wait:  3  ###
---------------- start layer  10  ---------------
adv train loss:  -61.9538671374321 , diff:  61.9538671374321
adv train loss:  -64.59415602684021 , diff:  2.6402888894081116
layer  10  adv train finish, try to retain  88
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
adv train loss:  -62.988748013973236 , diff:  62.988748013973236
adv train loss:  -63.226315796375275 , diff:  0.23756778240203857
layer  11  adv train finish, try to retain  82
test acc: top1 ->  68.336 ; top5 ->  88.448  and loss:  1039.2247775793076
forward train acc: top1 ->  69.0625 ; top5 ->  87.1953125  and loss:  260.4181146621704
test acc: top1 ->  70.838 ; top5 ->  89.724  and loss:  956.1099701523781
forward train acc: top1 ->  69.859375 ; top5 ->  87.9296875  and loss:  251.8112734556198
test acc: top1 ->  70.886 ; top5 ->  89.836  and loss:  953.9037142395973
forward train acc: top1 ->  69.609375 ; top5 ->  87.5078125  and loss:  253.89907640218735
test acc: top1 ->  70.768 ; top5 ->  89.728  and loss:  956.57605022192
forward train acc: top1 ->  69.796875 ; top5 ->  87.453125  and loss:  255.7152801156044
test acc: top1 ->  70.802 ; top5 ->  89.778  and loss:  954.9369086623192
forward train acc: top1 ->  69.5625 ; top5 ->  87.2734375  and loss:  255.90441954135895
test acc: top1 ->  70.952 ; top5 ->  89.852  and loss:  949.3174268603325
forward train acc: top1 ->  68.7890625 ; top5 ->  87.4609375  and loss:  257.8436106443405
test acc: top1 ->  70.934 ; top5 ->  89.81  and loss:  949.9713530540466
forward train acc: top1 ->  69.8046875 ; top5 ->  87.9609375  and loss:  250.68705332279205
test acc: top1 ->  70.966 ; top5 ->  89.824  and loss:  949.8222743868828
forward train acc: top1 ->  69.671875 ; top5 ->  87.4140625  and loss:  257.71671837568283
test acc: top1 ->  71.012 ; top5 ->  89.798  and loss:  947.1789820790291
forward train acc: top1 ->  69.0703125 ; top5 ->  87.0234375  and loss:  257.4621813297272
test acc: top1 ->  70.956 ; top5 ->  89.788  and loss:  948.2292163968086
forward train acc: top1 ->  70.15625 ; top5 ->  87.546875  and loss:  254.39540684223175
test acc: top1 ->  71.034 ; top5 ->  89.818  and loss:  948.3701809644699
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -61.40446352958679 , diff:  61.40446352958679
adv train loss:  -61.05186855792999 , diff:  0.3525949716567993
layer  12  adv train finish, try to retain  95
test acc: top1 ->  70.192 ; top5 ->  89.468  and loss:  973.7485543489456
forward train acc: top1 ->  69.5390625 ; top5 ->  87.640625  and loss:  255.39961516857147
test acc: top1 ->  70.716 ; top5 ->  89.73  and loss:  957.3377684950829
forward train acc: top1 ->  69.6875 ; top5 ->  87.578125  and loss:  255.06630450487137
test acc: top1 ->  70.83 ; top5 ->  89.738  and loss:  953.8910889029503
forward train acc: top1 ->  70.2109375 ; top5 ->  88.03125  and loss:  251.59400576353073
test acc: top1 ->  70.698 ; top5 ->  89.658  and loss:  955.3502006530762
forward train acc: top1 ->  69.1640625 ; top5 ->  86.9296875  and loss:  259.56912112236023
test acc: top1 ->  70.786 ; top5 ->  89.812  and loss:  951.0510485172272
forward train acc: top1 ->  70.03125 ; top5 ->  87.796875  and loss:  251.8733590245247
test acc: top1 ->  70.858 ; top5 ->  89.81  and loss:  951.0264366865158
forward train acc: top1 ->  69.734375 ; top5 ->  87.6875  and loss:  254.03214395046234
test acc: top1 ->  70.864 ; top5 ->  89.878  and loss:  947.6355323791504
forward train acc: top1 ->  69.921875 ; top5 ->  87.9453125  and loss:  249.29388213157654
test acc: top1 ->  70.796 ; top5 ->  89.856  and loss:  948.1403386592865
forward train acc: top1 ->  69.3203125 ; top5 ->  87.625  and loss:  258.886339366436
test acc: top1 ->  70.848 ; top5 ->  89.88  and loss:  947.421001136303
forward train acc: top1 ->  69.5390625 ; top5 ->  86.9921875  and loss:  258.1865389943123
test acc: top1 ->  70.908 ; top5 ->  89.856  and loss:  947.2066482901573
forward train acc: top1 ->  69.453125 ; top5 ->  87.7890625  and loss:  251.33919095993042
test acc: top1 ->  70.964 ; top5 ->  89.928  and loss:  945.8808081150055
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  98 / 128 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -63.0983150601387 , diff:  63.0983150601387
adv train loss:  -63.728337705135345 , diff:  0.6300226449966431
layer  13  adv train finish, try to retain  74
>>>>>>> reverse layer  13  since no improvement >>>>>>>
---------------- start layer  14  ---------------
adv train loss:  -66.82882732152939 , diff:  66.82882732152939
adv train loss:  -63.92795866727829 , diff:  2.9008686542510986
layer  14  adv train finish, try to retain  244
test acc: top1 ->  70.478 ; top5 ->  89.59  and loss:  960.4119256734848
forward train acc: top1 ->  70.0078125 ; top5 ->  87.34375  and loss:  254.26486283540726
test acc: top1 ->  70.9 ; top5 ->  89.834  and loss:  950.3272452950478
forward train acc: top1 ->  69.25 ; top5 ->  87.34375  and loss:  260.062238574028
test acc: top1 ->  70.996 ; top5 ->  89.842  and loss:  950.5615036487579
forward train acc: top1 ->  69.2578125 ; top5 ->  86.9453125  and loss:  260.2429589629173
test acc: top1 ->  71.014 ; top5 ->  89.904  and loss:  945.1145721673965
forward train acc: top1 ->  69.75 ; top5 ->  87.5546875  and loss:  257.17921966314316
test acc: top1 ->  70.954 ; top5 ->  89.866  and loss:  946.6498992443085
forward train acc: top1 ->  69.53125 ; top5 ->  87.0390625  and loss:  257.8988599181175
test acc: top1 ->  71.012 ; top5 ->  89.928  and loss:  943.6421414613724
forward train acc: top1 ->  69.6484375 ; top5 ->  87.2421875  and loss:  258.72150480747223
test acc: top1 ->  71.188 ; top5 ->  90.004  and loss:  943.4432737231255
forward train acc: top1 ->  69.0859375 ; top5 ->  87.2734375  and loss:  257.609110057354
test acc: top1 ->  71.122 ; top5 ->  90.016  and loss:  941.8659479618073
forward train acc: top1 ->  70.3671875 ; top5 ->  87.8046875  and loss:  248.77830362319946
test acc: top1 ->  71.194 ; top5 ->  89.988  and loss:  943.5634651184082
forward train acc: top1 ->  69.484375 ; top5 ->  87.265625  and loss:  255.28455871343613
test acc: top1 ->  71.226 ; top5 ->  90.034  and loss:  944.5455795526505
forward train acc: top1 ->  69.65625 ; top5 ->  87.296875  and loss:  256.6189766526222
test acc: top1 ->  71.234 ; top5 ->  89.978  and loss:  942.5291392207146
>>>>>>> reverse layer  14  since performance drop >>>>>>>
==> this epoch:  250 / 256 , inc:  1
---------------- start layer  15  ---------------
### skip layer  15 wait:  4  ###
---------------- start layer  16  ---------------
adv train loss:  -64.67434906959534 , diff:  64.67434906959534
adv train loss:  -63.05815303325653 , diff:  1.6161960363388062
layer  16  adv train finish, try to retain  241
>>>>>>> reverse layer  16  since no improvement >>>>>>>
---------------- start layer  17  ---------------
### skip layer  17 wait:  3  ###
---------------- start layer  18  ---------------
### skip layer  18 wait:  3  ###
---------------- start layer  19  ---------------
### skip layer  19 wait:  3  ###
---------------- start layer  20  ---------------
### skip layer  20 wait:  3  ###
---------------- start layer  21  ---------------
adv train loss:  -61.831216394901276 , diff:  61.831216394901276
adv train loss:  -62.777131378650665 , diff:  0.9459149837493896
layer  21  adv train finish, try to retain  233
test acc: top1 ->  70.956 ; top5 ->  89.808  and loss:  949.9736997485161
forward train acc: top1 ->  69.453125 ; top5 ->  87.6875  and loss:  257.6179352402687
test acc: top1 ->  70.938 ; top5 ->  89.892  and loss:  949.7095795273781
forward train acc: top1 ->  70.4453125 ; top5 ->  87.921875  and loss:  250.70702254772186
test acc: top1 ->  71.0 ; top5 ->  89.888  and loss:  945.4817878007889
forward train acc: top1 ->  70.8515625 ; top5 ->  87.4375  and loss:  249.42586147785187
test acc: top1 ->  70.866 ; top5 ->  89.878  and loss:  948.6658107042313
forward train acc: top1 ->  69.9296875 ; top5 ->  87.9375  and loss:  251.34160488843918
test acc: top1 ->  70.924 ; top5 ->  89.96  and loss:  945.3268394470215
forward train acc: top1 ->  69.734375 ; top5 ->  87.65625  and loss:  256.1229577064514
test acc: top1 ->  70.952 ; top5 ->  89.92  and loss:  944.1058333516121
forward train acc: top1 ->  68.6171875 ; top5 ->  86.9375  and loss:  260.01515424251556
test acc: top1 ->  70.982 ; top5 ->  89.924  and loss:  944.1880659461021
forward train acc: top1 ->  69.28125 ; top5 ->  87.2890625  and loss:  259.7058729529381
test acc: top1 ->  71.08 ; top5 ->  89.982  and loss:  942.375606238842
forward train acc: top1 ->  70.3203125 ; top5 ->  88.0703125  and loss:  250.33126401901245
test acc: top1 ->  71.014 ; top5 ->  90.006  and loss:  941.7812075614929
forward train acc: top1 ->  69.9140625 ; top5 ->  87.578125  and loss:  250.69562309980392
test acc: top1 ->  71.056 ; top5 ->  89.998  and loss:  943.6616179943085
forward train acc: top1 ->  70.671875 ; top5 ->  88.1171875  and loss:  247.72497123479843
test acc: top1 ->  70.972 ; top5 ->  89.97  and loss:  940.0321888327599
>>>>>>> reverse layer  21  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  22  ---------------
adv train loss:  -64.49958980083466 , diff:  64.49958980083466
adv train loss:  -64.05795067548752 , diff:  0.44163912534713745
layer  22  adv train finish, try to retain  206
test acc: top1 ->  70.766 ; top5 ->  89.706  and loss:  952.0757585763931
forward train acc: top1 ->  69.53125 ; top5 ->  87.6328125  and loss:  253.95314437150955
test acc: top1 ->  70.932 ; top5 ->  89.808  and loss:  947.2522698044777
forward train acc: top1 ->  69.34375 ; top5 ->  87.4140625  and loss:  257.1691808104515
test acc: top1 ->  70.71 ; top5 ->  89.712  and loss:  952.2759946584702
forward train acc: top1 ->  69.875 ; top5 ->  87.4453125  and loss:  253.24239486455917
test acc: top1 ->  70.9 ; top5 ->  89.828  and loss:  950.7009965777397
forward train acc: top1 ->  69.6875 ; top5 ->  87.671875  and loss:  253.06434309482574
test acc: top1 ->  70.764 ; top5 ->  89.834  and loss:  949.2811340093613
forward train acc: top1 ->  70.234375 ; top5 ->  87.984375  and loss:  249.58715337514877
test acc: top1 ->  70.984 ; top5 ->  89.83  and loss:  948.031949877739
forward train acc: top1 ->  69.59375 ; top5 ->  87.21875  and loss:  257.9329010248184
test acc: top1 ->  70.992 ; top5 ->  89.83  and loss:  950.0306566953659
forward train acc: top1 ->  69.78125 ; top5 ->  88.1640625  and loss:  248.02281177043915
test acc: top1 ->  71.06 ; top5 ->  89.874  and loss:  947.3284237980843
forward train acc: top1 ->  69.8359375 ; top5 ->  87.78125  and loss:  251.17313879728317
test acc: top1 ->  70.896 ; top5 ->  89.862  and loss:  947.9793639183044
forward train acc: top1 ->  69.4375 ; top5 ->  87.78125  and loss:  253.00423330068588
test acc: top1 ->  70.956 ; top5 ->  89.84  and loss:  950.1720179915428
forward train acc: top1 ->  69.7734375 ; top5 ->  87.59375  and loss:  255.24110227823257
test acc: top1 ->  71.014 ; top5 ->  89.924  and loss:  945.7118021249771
>>>>>>> reverse layer  22  since performance drop >>>>>>>
==> this epoch:  208 / 256 , inc:  1
---------------- start layer  23  ---------------
### skip layer  23 wait:  3  ###
---------------- start layer  24  ---------------
adv train loss:  -61.79243701696396 , diff:  61.79243701696396
adv train loss:  -61.137142300605774 , diff:  0.6552947163581848
layer  24  adv train finish, try to retain  239
test acc: top1 ->  70.928 ; top5 ->  89.858  and loss:  951.4599317312241
forward train acc: top1 ->  69.1796875 ; top5 ->  87.203125  and loss:  257.17834997177124
test acc: top1 ->  70.964 ; top5 ->  89.802  and loss:  947.2870371341705
forward train acc: top1 ->  69.171875 ; top5 ->  87.5390625  and loss:  255.6202399134636
test acc: top1 ->  70.894 ; top5 ->  89.842  and loss:  946.721096932888
forward train acc: top1 ->  70.0703125 ; top5 ->  88.15625  and loss:  247.60879921913147
test acc: top1 ->  70.806 ; top5 ->  89.816  and loss:  948.4589926600456
forward train acc: top1 ->  68.8046875 ; top5 ->  86.96875  and loss:  261.5682631134987
test acc: top1 ->  71.046 ; top5 ->  89.806  and loss:  950.268138229847
forward train acc: top1 ->  70.0703125 ; top5 ->  88.09375  and loss:  247.27522391080856
test acc: top1 ->  71.048 ; top5 ->  89.8  and loss:  947.3039566874504
forward train acc: top1 ->  69.578125 ; top5 ->  87.7421875  and loss:  252.479731798172
test acc: top1 ->  70.954 ; top5 ->  89.806  and loss:  947.8225855827332
forward train acc: top1 ->  70.6171875 ; top5 ->  87.796875  and loss:  250.14008182287216
test acc: top1 ->  70.938 ; top5 ->  89.834  and loss:  947.0730921030045
forward train acc: top1 ->  69.4921875 ; top5 ->  87.890625  and loss:  253.90799766778946
test acc: top1 ->  70.972 ; top5 ->  89.86  and loss:  943.6380016803741
forward train acc: top1 ->  69.78125 ; top5 ->  87.640625  and loss:  251.23130851984024
test acc: top1 ->  71.004 ; top5 ->  89.864  and loss:  941.9505566954613
forward train acc: top1 ->  69.40625 ; top5 ->  87.7421875  and loss:  254.7443664073944
test acc: top1 ->  71.018 ; top5 ->  89.882  and loss:  942.1672765612602
>>>>>>> reverse layer  24  since performance drop >>>>>>>
==> this epoch:  244 / 256 , inc:  1
---------------- start layer  25  ---------------
adv train loss:  -65.5377568602562 , diff:  65.5377568602562
adv train loss:  -62.20993775129318 , diff:  3.3278191089630127
layer  25  adv train finish, try to retain  232
test acc: top1 ->  70.574 ; top5 ->  89.632  and loss:  961.737567126751
forward train acc: top1 ->  69.84375 ; top5 ->  87.5859375  and loss:  252.03026098012924
test acc: top1 ->  70.88 ; top5 ->  89.814  and loss:  950.7676991224289
forward train acc: top1 ->  69.8828125 ; top5 ->  87.734375  and loss:  251.8023036122322
test acc: top1 ->  70.672 ; top5 ->  89.754  and loss:  956.7900368571281
forward train acc: top1 ->  69.3828125 ; top5 ->  87.6015625  and loss:  257.92810344696045
test acc: top1 ->  70.594 ; top5 ->  89.636  and loss:  954.7006097435951
forward train acc: top1 ->  69.8203125 ; top5 ->  87.4609375  and loss:  256.80831545591354
test acc: top1 ->  70.75 ; top5 ->  89.764  and loss:  952.6399940252304
forward train acc: top1 ->  69.453125 ; top5 ->  87.7421875  and loss:  255.0215060710907
test acc: top1 ->  70.908 ; top5 ->  89.822  and loss:  948.5197587013245
forward train acc: top1 ->  69.8671875 ; top5 ->  87.8359375  and loss:  251.88108015060425
test acc: top1 ->  70.922 ; top5 ->  89.85  and loss:  948.0600364208221
forward train acc: top1 ->  70.1015625 ; top5 ->  87.546875  and loss:  250.799385368824
test acc: top1 ->  70.91 ; top5 ->  89.862  and loss:  946.5283918976784
forward train acc: top1 ->  69.75 ; top5 ->  87.71875  and loss:  254.99967354536057
test acc: top1 ->  70.874 ; top5 ->  89.894  and loss:  943.6800109744072
forward train acc: top1 ->  69.921875 ; top5 ->  88.125  and loss:  247.60718941688538
test acc: top1 ->  70.96 ; top5 ->  89.888  and loss:  944.8397067785263
forward train acc: top1 ->  70.1171875 ; top5 ->  87.7109375  and loss:  250.7410296201706
test acc: top1 ->  70.93 ; top5 ->  89.926  and loss:  943.7301942110062
>>>>>>> reverse layer  25  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  26  ---------------
adv train loss:  -63.89907366037369 , diff:  63.89907366037369
adv train loss:  -63.91072851419449 , diff:  0.011654853820800781
layer  26  adv train finish, try to retain  492
test acc: top1 ->  70.506 ; top5 ->  89.706  and loss:  958.5735473632812
forward train acc: top1 ->  69.0859375 ; top5 ->  87.578125  and loss:  257.16876578330994
test acc: top1 ->  70.726 ; top5 ->  89.744  and loss:  951.8949459791183
forward train acc: top1 ->  69.5703125 ; top5 ->  87.7421875  and loss:  250.90016114711761
test acc: top1 ->  70.67 ; top5 ->  89.704  and loss:  952.6463406682014
forward train acc: top1 ->  68.9921875 ; top5 ->  87.4375  and loss:  257.0170982480049
test acc: top1 ->  70.768 ; top5 ->  89.744  and loss:  950.8768006563187
forward train acc: top1 ->  69.9453125 ; top5 ->  87.6171875  and loss:  253.8964456319809
test acc: top1 ->  70.854 ; top5 ->  89.816  and loss:  947.9583882689476
forward train acc: top1 ->  70.2265625 ; top5 ->  88.1015625  and loss:  249.29033440351486
test acc: top1 ->  70.984 ; top5 ->  89.806  and loss:  950.822493493557
forward train acc: top1 ->  69.71875 ; top5 ->  87.6796875  and loss:  254.23217457532883
test acc: top1 ->  70.904 ; top5 ->  89.782  and loss:  948.5178896188736
forward train acc: top1 ->  69.8984375 ; top5 ->  87.6171875  and loss:  256.56874990463257
test acc: top1 ->  71.042 ; top5 ->  89.856  and loss:  944.4151773452759
forward train acc: top1 ->  69.84375 ; top5 ->  87.6875  and loss:  253.80351334810257
test acc: top1 ->  70.996 ; top5 ->  89.922  and loss:  944.818028151989
forward train acc: top1 ->  68.8828125 ; top5 ->  87.171875  and loss:  260.2493476867676
test acc: top1 ->  71.008 ; top5 ->  89.872  and loss:  945.5990601181984
forward train acc: top1 ->  69.6328125 ; top5 ->  87.75  and loss:  252.49935537576675
test acc: top1 ->  71.068 ; top5 ->  89.9  and loss:  945.8383872509003
>>>>>>> reverse layer  26  since performance drop >>>>>>>
==> this epoch:  502 / 512 , inc:  1
---------------- start layer  27  ---------------
### skip layer  27 wait:  4  ###
---------------- start layer  28  ---------------
adv train loss:  -63.45445519685745 , diff:  63.45445519685745
adv train loss:  -59.8890922665596 , diff:  3.5653629302978516
layer  28  adv train finish, try to retain  493
test acc: top1 ->  71.012 ; top5 ->  89.89  and loss:  959.7780761122704
forward train acc: top1 ->  70.0 ; top5 ->  87.3125  and loss:  253.66544473171234
test acc: top1 ->  70.95 ; top5 ->  89.826  and loss:  947.3518454432487
forward train acc: top1 ->  69.84375 ; top5 ->  87.640625  and loss:  254.95648854970932
test acc: top1 ->  70.86 ; top5 ->  89.774  and loss:  948.4891939759254
forward train acc: top1 ->  68.546875 ; top5 ->  87.171875  and loss:  262.61161893606186
test acc: top1 ->  70.968 ; top5 ->  89.878  and loss:  945.5728729367256
forward train acc: top1 ->  69.609375 ; top5 ->  87.5390625  and loss:  258.18290865421295
test acc: top1 ->  70.968 ; top5 ->  89.848  and loss:  946.4127330780029
forward train acc: top1 ->  68.640625 ; top5 ->  87.6953125  and loss:  256.8125312924385
test acc: top1 ->  70.896 ; top5 ->  89.79  and loss:  948.146758556366
forward train acc: top1 ->  69.7734375 ; top5 ->  87.765625  and loss:  251.54537898302078
test acc: top1 ->  70.934 ; top5 ->  89.828  and loss:  944.4178662896156
forward train acc: top1 ->  70.234375 ; top5 ->  87.9453125  and loss:  250.68647080659866
test acc: top1 ->  70.954 ; top5 ->  89.854  and loss:  946.1203261017799
forward train acc: top1 ->  69.734375 ; top5 ->  87.3984375  and loss:  256.0317590236664
test acc: top1 ->  71.016 ; top5 ->  89.924  and loss:  943.6900050640106
forward train acc: top1 ->  70.5 ; top5 ->  87.96875  and loss:  249.17550003528595
test acc: top1 ->  70.952 ; top5 ->  89.85  and loss:  943.4907009005547
forward train acc: top1 ->  69.6171875 ; top5 ->  87.84375  and loss:  255.14766561985016
test acc: top1 ->  71.046 ; top5 ->  89.898  and loss:  942.5310306549072
>>>>>>> reverse layer  28  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  29  ---------------
adv train loss:  -64.03969699144363 , diff:  64.03969699144363
adv train loss:  -60.35259926319122 , diff:  3.687097728252411
layer  29  adv train finish, try to retain  496
test acc: top1 ->  71.078 ; top5 ->  89.914  and loss:  946.6426811814308
forward train acc: top1 ->  70.2578125 ; top5 ->  87.96875  and loss:  248.67391085624695
test acc: top1 ->  70.928 ; top5 ->  89.886  and loss:  948.7773878574371
forward train acc: top1 ->  69.625 ; top5 ->  87.6640625  and loss:  254.80022251605988
test acc: top1 ->  71.046 ; top5 ->  89.874  and loss:  949.7630849480629
forward train acc: top1 ->  69.8203125 ; top5 ->  87.8046875  and loss:  253.140518784523
test acc: top1 ->  70.848 ; top5 ->  89.864  and loss:  948.2434815764427
forward train acc: top1 ->  69.515625 ; top5 ->  87.3046875  and loss:  253.98136872053146
test acc: top1 ->  70.934 ; top5 ->  89.804  and loss:  948.7838692069054
forward train acc: top1 ->  70.15625 ; top5 ->  87.921875  and loss:  248.2038545012474
test acc: top1 ->  71.008 ; top5 ->  89.894  and loss:  945.0547822117805
forward train acc: top1 ->  70.1328125 ; top5 ->  87.6640625  and loss:  252.10655558109283
test acc: top1 ->  71.038 ; top5 ->  89.846  and loss:  944.3110707998276
forward train acc: top1 ->  69.8046875 ; top5 ->  88.25  and loss:  249.6858851313591
test acc: top1 ->  70.982 ; top5 ->  89.832  and loss:  944.6428495645523
forward train acc: top1 ->  69.8046875 ; top5 ->  87.6796875  and loss:  251.65997219085693
test acc: top1 ->  70.952 ; top5 ->  89.896  and loss:  943.4740918278694
forward train acc: top1 ->  69.984375 ; top5 ->  87.90625  and loss:  248.55331242084503
test acc: top1 ->  71.062 ; top5 ->  89.828  and loss:  941.9000060558319
forward train acc: top1 ->  70.1875 ; top5 ->  87.765625  and loss:  250.97380965948105
test acc: top1 ->  71.06 ; top5 ->  89.888  and loss:  941.6788274645805
>>>>>>> reverse layer  29  since performance drop >>>>>>>
==> this epoch:  498 / 512 , inc:  1
---------------- start layer  30  ---------------
adv train loss:  -61.95781981945038 , diff:  61.95781981945038
adv train loss:  -61.453428745269775 , diff:  0.504391074180603
layer  30  adv train finish, try to retain  488
test acc: top1 ->  70.702 ; top5 ->  89.708  and loss:  952.5004391074181
forward train acc: top1 ->  69.2265625 ; top5 ->  86.984375  and loss:  260.05020147562027
test acc: top1 ->  70.74 ; top5 ->  89.85  and loss:  953.683155298233
forward train acc: top1 ->  69.6171875 ; top5 ->  87.59375  and loss:  255.7618606686592
test acc: top1 ->  70.69 ; top5 ->  89.802  and loss:  954.7971352338791
forward train acc: top1 ->  69.6015625 ; top5 ->  87.828125  and loss:  254.28553992509842
test acc: top1 ->  70.792 ; top5 ->  89.778  and loss:  953.7933949828148
forward train acc: top1 ->  69.9609375 ; top5 ->  88.1875  and loss:  247.7995656132698
test acc: top1 ->  70.83 ; top5 ->  89.766  and loss:  954.7561326026917
forward train acc: top1 ->  69.5625 ; top5 ->  87.6015625  and loss:  253.53439211845398
test acc: top1 ->  70.73 ; top5 ->  89.752  and loss:  951.9796274900436
forward train acc: top1 ->  69.03125 ; top5 ->  87.2265625  and loss:  260.0506205558777
test acc: top1 ->  70.834 ; top5 ->  89.808  and loss:  952.6130863428116
forward train acc: top1 ->  69.359375 ; top5 ->  87.8984375  and loss:  256.1822404265404
test acc: top1 ->  70.872 ; top5 ->  89.792  and loss:  948.5448699593544
forward train acc: top1 ->  69.125 ; top5 ->  87.1953125  and loss:  257.6976126432419
test acc: top1 ->  70.894 ; top5 ->  89.822  and loss:  948.1779706478119
forward train acc: top1 ->  69.484375 ; top5 ->  87.2109375  and loss:  257.4449351429939
test acc: top1 ->  71.012 ; top5 ->  89.88  and loss:  945.2272650003433
forward train acc: top1 ->  69.796875 ; top5 ->  87.8515625  and loss:  249.19660383462906
test acc: top1 ->  70.952 ; top5 ->  89.79  and loss:  947.6746746897697
>>>>>>> reverse layer  30  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  31  ---------------
adv train loss:  -63.6437166929245 , diff:  63.6437166929245
adv train loss:  -67.86115658283234 , diff:  4.217439889907837
layer  31  adv train finish, try to retain  493
test acc: top1 ->  70.738 ; top5 ->  89.712  and loss:  958.8526722788811
forward train acc: top1 ->  70.0546875 ; top5 ->  88.046875  and loss:  250.478486597538
test acc: top1 ->  70.754 ; top5 ->  89.648  and loss:  956.2608595490456
forward train acc: top1 ->  69.484375 ; top5 ->  87.1953125  and loss:  258.5831115245819
test acc: top1 ->  70.838 ; top5 ->  89.694  and loss:  955.552196085453
forward train acc: top1 ->  69.3515625 ; top5 ->  87.53125  and loss:  251.40991985797882
test acc: top1 ->  70.804 ; top5 ->  89.684  and loss:  953.841844201088
forward train acc: top1 ->  69.328125 ; top5 ->  87.4296875  and loss:  256.1744205355644
test acc: top1 ->  70.904 ; top5 ->  89.746  and loss:  951.5632857084274
forward train acc: top1 ->  69.078125 ; top5 ->  87.0625  and loss:  261.2660039663315
test acc: top1 ->  70.87 ; top5 ->  89.786  and loss:  948.1720391511917
forward train acc: top1 ->  69.328125 ; top5 ->  87.625  and loss:  254.81176871061325
test acc: top1 ->  70.856 ; top5 ->  89.822  and loss:  950.2438635826111
forward train acc: top1 ->  70.1484375 ; top5 ->  87.6875  and loss:  253.56044167280197
test acc: top1 ->  70.942 ; top5 ->  89.746  and loss:  947.4642394781113
forward train acc: top1 ->  69.5859375 ; top5 ->  87.84375  and loss:  256.3548262119293
test acc: top1 ->  70.992 ; top5 ->  89.798  and loss:  947.8320568799973
forward train acc: top1 ->  69.7421875 ; top5 ->  87.6953125  and loss:  256.00576108694077
test acc: top1 ->  71.056 ; top5 ->  89.808  and loss:  946.7559926509857
forward train acc: top1 ->  70.28125 ; top5 ->  88.2578125  and loss:  247.3193460702896
test acc: top1 ->  71.008 ; top5 ->  89.886  and loss:  945.5888718962669
>>>>>>> reverse layer  31  since performance drop >>>>>>>
==> this epoch:  501 / 512 , inc:  1
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.08542968750000002, 0.04271484375000001, 0.0007508468627929689, 0.021357421875000004, 0.021357421875000004, 0.22781250000000003, 0.00037542343139648445, 0.00018771171569824223, 0.010678710937500002, 0.0010011291503906252, 0.010678710937500002, 0.004004516601562501, 0.004004516601562501, 0.11390625000000001, 0.00018771171569824223, 9.385585784912111e-05, 0.0005005645751953126, 0.0005005645751953126, 0.0005005645751953126, 0.005339355468750001, 0.0005005645751953126, 0.00018771171569824223, 0.0020022583007812504, 0.005339355468750001, 0.00018771171569824223, 0.00018771171569824223, 9.385585784912111e-05, 4.692792892456056e-05, 9.385585784912111e-05, 9.385585784912111e-05, 9.385585784912111e-05, 9.385585784912111e-05]  wait [4, 3, 4, 2, 2, 2, 4, 3, 2, 2, 2, 4, 4, 2, 4, 3, 2, 2, 2, 2, 2, 4, 4, 2, 4, 4, 4, 3, 4, 4, 4, 4]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 4
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  21  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -62.272422194480896 , diff:  62.272422194480896
adv train loss:  -63.37808632850647 , diff:  1.1056641340255737
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  36
test acc: top1 ->  65.858 ; top5 ->  86.61  and loss:  1118.8249191641808
forward train acc: top1 ->  69.65625 ; top5 ->  87.359375  and loss:  254.76283276081085
test acc: top1 ->  70.482 ; top5 ->  89.542  and loss:  966.2602375149727
forward train acc: top1 ->  69.671875 ; top5 ->  87.1015625  and loss:  258.13987147808075
test acc: top1 ->  70.706 ; top5 ->  89.678  and loss:  957.3851241469383
forward train acc: top1 ->  69.6640625 ; top5 ->  87.625  and loss:  252.53967082500458
test acc: top1 ->  70.646 ; top5 ->  89.72  and loss:  958.5978115797043
forward train acc: top1 ->  69.25 ; top5 ->  87.6015625  and loss:  256.045593559742
test acc: top1 ->  70.84 ; top5 ->  89.702  and loss:  955.4012355208397
forward train acc: top1 ->  69.9453125 ; top5 ->  87.2265625  and loss:  253.05035465955734
test acc: top1 ->  70.908 ; top5 ->  89.754  and loss:  953.0058174133301
forward train acc: top1 ->  69.609375 ; top5 ->  87.5390625  and loss:  252.42823046445847
test acc: top1 ->  70.854 ; top5 ->  89.72  and loss:  953.5469987988472
forward train acc: top1 ->  69.9765625 ; top5 ->  87.90625  and loss:  253.70382964611053
test acc: top1 ->  70.884 ; top5 ->  89.706  and loss:  953.5872750878334
forward train acc: top1 ->  69.6953125 ; top5 ->  87.8515625  and loss:  251.22590881586075
test acc: top1 ->  70.906 ; top5 ->  89.756  and loss:  949.446790099144
forward train acc: top1 ->  69.2421875 ; top5 ->  87.671875  and loss:  256.8305768966675
test acc: top1 ->  70.93 ; top5 ->  89.786  and loss:  949.1323400735855
forward train acc: top1 ->  70.0078125 ; top5 ->  87.734375  and loss:  254.06248193979263
test acc: top1 ->  70.896 ; top5 ->  89.768  and loss:  951.0981884598732
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  37 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -66.31721669435501 , diff:  66.31721669435501
adv train loss:  -63.29391837120056 , diff:  3.0232983231544495
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  43
test acc: top1 ->  66.292 ; top5 ->  86.78  and loss:  1113.6428925991058
forward train acc: top1 ->  68.9765625 ; top5 ->  87.3046875  and loss:  257.9231386780739
test acc: top1 ->  70.574 ; top5 ->  89.646  and loss:  962.8569291234016
forward train acc: top1 ->  69.1875 ; top5 ->  87.5078125  and loss:  256.4913834929466
test acc: top1 ->  70.724 ; top5 ->  89.702  and loss:  959.0287227630615
forward train acc: top1 ->  69.78125 ; top5 ->  87.7109375  and loss:  252.92319631576538
test acc: top1 ->  70.72 ; top5 ->  89.69  and loss:  958.231085062027
forward train acc: top1 ->  69.2578125 ; top5 ->  87.515625  and loss:  256.49992513656616
test acc: top1 ->  70.716 ; top5 ->  89.74  and loss:  955.9195686578751
forward train acc: top1 ->  69.59375 ; top5 ->  87.4765625  and loss:  253.6669590473175
test acc: top1 ->  70.872 ; top5 ->  89.738  and loss:  953.0464774370193
forward train acc: top1 ->  70.1328125 ; top5 ->  87.7421875  and loss:  253.52057659626007
test acc: top1 ->  70.862 ; top5 ->  89.758  and loss:  951.0739268064499
forward train acc: top1 ->  70.0859375 ; top5 ->  88.0625  and loss:  245.22977894544601
test acc: top1 ->  70.808 ; top5 ->  89.76  and loss:  950.7035962343216
forward train acc: top1 ->  69.828125 ; top5 ->  87.375  and loss:  257.8959725499153
test acc: top1 ->  70.962 ; top5 ->  89.822  and loss:  948.0179349780083
forward train acc: top1 ->  69.4140625 ; top5 ->  87.671875  and loss:  253.68753498792648
test acc: top1 ->  70.926 ; top5 ->  89.808  and loss:  949.320140182972
forward train acc: top1 ->  69.390625 ; top5 ->  87.65625  and loss:  253.38875329494476
test acc: top1 ->  70.974 ; top5 ->  89.752  and loss:  947.849357187748
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  44 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -65.34274637699127 , diff:  65.34274637699127
adv train loss:  -63.0669909119606 , diff:  2.27575546503067
layer  2  adv train finish, try to retain  58
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
adv train loss:  -63.524512350559235 , diff:  63.524512350559235
adv train loss:  -63.769313871860504 , diff:  0.24480152130126953
************ all values are small in this layer **********
layer  3  adv train finish, try to retain  42
test acc: top1 ->  65.698 ; top5 ->  86.538  and loss:  1132.1754234433174
forward train acc: top1 ->  69.0 ; top5 ->  87.0703125  and loss:  262.9473863840103
test acc: top1 ->  70.494 ; top5 ->  89.5  and loss:  966.8775011897087
forward train acc: top1 ->  69.9296875 ; top5 ->  87.7421875  and loss:  256.14180368185043
test acc: top1 ->  70.54 ; top5 ->  89.59  and loss:  965.5688623189926
forward train acc: top1 ->  68.7734375 ; top5 ->  86.9609375  and loss:  260.58671975135803
test acc: top1 ->  70.544 ; top5 ->  89.582  and loss:  962.5344948768616
forward train acc: top1 ->  69.296875 ; top5 ->  87.5078125  and loss:  256.6322761774063
test acc: top1 ->  70.608 ; top5 ->  89.638  and loss:  956.5170742273331
forward train acc: top1 ->  69.53125 ; top5 ->  87.515625  and loss:  257.0978356599808
test acc: top1 ->  70.616 ; top5 ->  89.674  and loss:  956.0839233994484
forward train acc: top1 ->  70.171875 ; top5 ->  87.875  and loss:  249.67860651016235
test acc: top1 ->  70.712 ; top5 ->  89.69  and loss:  955.7066384553909
forward train acc: top1 ->  69.578125 ; top5 ->  87.5703125  and loss:  251.7046838402748
test acc: top1 ->  70.736 ; top5 ->  89.766  and loss:  957.302449285984
forward train acc: top1 ->  70.1171875 ; top5 ->  87.8671875  and loss:  251.9307319521904
test acc: top1 ->  70.788 ; top5 ->  89.734  and loss:  955.053973376751
forward train acc: top1 ->  69.140625 ; top5 ->  87.4609375  and loss:  258.4475308060646
test acc: top1 ->  70.828 ; top5 ->  89.744  and loss:  954.9965044856071
forward train acc: top1 ->  69.6640625 ; top5 ->  87.6640625  and loss:  253.490964949131
test acc: top1 ->  70.838 ; top5 ->  89.734  and loss:  954.216036260128
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  43 / 64 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -67.40831315517426 , diff:  67.40831315517426
adv train loss:  -62.83265745639801 , diff:  4.575655698776245
************ all values are small in this layer **********
layer  4  adv train finish, try to retain  39
test acc: top1 ->  68.656 ; top5 ->  88.382  and loss:  1028.1954453587532
forward train acc: top1 ->  69.3203125 ; top5 ->  87.15625  and loss:  257.6132164001465
test acc: top1 ->  70.858 ; top5 ->  89.69  and loss:  953.5650290250778
forward train acc: top1 ->  69.3671875 ; top5 ->  87.3046875  and loss:  259.880470097065
test acc: top1 ->  70.866 ; top5 ->  89.704  and loss:  952.1985124349594
forward train acc: top1 ->  69.171875 ; top5 ->  87.359375  and loss:  256.1727126836777
test acc: top1 ->  70.762 ; top5 ->  89.712  and loss:  951.751182615757
forward train acc: top1 ->  69.5625 ; top5 ->  87.609375  and loss:  254.85399293899536
test acc: top1 ->  70.86 ; top5 ->  89.762  and loss:  949.3518462181091
forward train acc: top1 ->  69.78125 ; top5 ->  87.734375  and loss:  252.7616981267929
test acc: top1 ->  70.842 ; top5 ->  89.734  and loss:  951.2781277894974
forward train acc: top1 ->  69.5703125 ; top5 ->  87.28125  and loss:  256.6439617872238
test acc: top1 ->  70.888 ; top5 ->  89.768  and loss:  950.7699782848358
forward train acc: top1 ->  69.546875 ; top5 ->  87.4921875  and loss:  254.7328406572342
test acc: top1 ->  70.996 ; top5 ->  89.836  and loss:  946.9994293451309
forward train acc: top1 ->  69.8828125 ; top5 ->  87.6484375  and loss:  253.50754916667938
test acc: top1 ->  71.046 ; top5 ->  89.828  and loss:  945.1272631287575
forward train acc: top1 ->  70.0859375 ; top5 ->  87.703125  and loss:  252.37914592027664
test acc: top1 ->  70.966 ; top5 ->  89.762  and loss:  947.9939680099487
forward train acc: top1 ->  69.9765625 ; top5 ->  87.4921875  and loss:  253.4178763628006
test acc: top1 ->  71.002 ; top5 ->  89.828  and loss:  944.5430866479874
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  40 / 64 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -65.45264852046967 , diff:  65.45264852046967
adv train loss:  -64.89254081249237 , diff:  0.5601077079772949
layer  5  adv train finish, try to retain  26
test acc: top1 ->  67.188 ; top5 ->  87.466  and loss:  1078.584961593151
forward train acc: top1 ->  68.6171875 ; top5 ->  86.71875  and loss:  265.17782324552536
test acc: top1 ->  70.7 ; top5 ->  89.608  and loss:  956.8817926049232
forward train acc: top1 ->  69.4765625 ; top5 ->  87.3515625  and loss:  258.7611897587776
test acc: top1 ->  70.77 ; top5 ->  89.668  and loss:  952.9839609265327
forward train acc: top1 ->  70.0078125 ; top5 ->  87.7421875  and loss:  252.93726634979248
test acc: top1 ->  70.854 ; top5 ->  89.672  and loss:  951.5902779102325
forward train acc: top1 ->  69.8125 ; top5 ->  87.5  and loss:  254.16792118549347
test acc: top1 ->  70.912 ; top5 ->  89.742  and loss:  948.9558851718903
forward train acc: top1 ->  69.75 ; top5 ->  87.8515625  and loss:  249.27009570598602
test acc: top1 ->  71.018 ; top5 ->  89.766  and loss:  948.9361485242844
forward train acc: top1 ->  69.8125 ; top5 ->  87.890625  and loss:  251.91159278154373
test acc: top1 ->  71.016 ; top5 ->  89.728  and loss:  947.6160192489624
forward train acc: top1 ->  70.234375 ; top5 ->  88.125  and loss:  247.83573228120804
test acc: top1 ->  71.046 ; top5 ->  89.742  and loss:  947.0197153091431
forward train acc: top1 ->  70.3671875 ; top5 ->  88.125  and loss:  249.4870349764824
test acc: top1 ->  70.942 ; top5 ->  89.724  and loss:  948.217328608036
forward train acc: top1 ->  70.328125 ; top5 ->  87.78125  and loss:  251.45303678512573
test acc: top1 ->  71.054 ; top5 ->  89.786  and loss:  946.504690349102
forward train acc: top1 ->  70.2265625 ; top5 ->  87.796875  and loss:  252.5560177564621
test acc: top1 ->  70.928 ; top5 ->  89.758  and loss:  947.0365732312202
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  29 / 64 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -65.89881736040115 , diff:  65.89881736040115
adv train loss:  -62.58910530805588 , diff:  3.309712052345276
layer  6  adv train finish, try to retain  119
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -62.50097358226776 , diff:  62.50097358226776
adv train loss:  -62.255162835121155 , diff:  0.24581074714660645
layer  7  adv train finish, try to retain  119
test acc: top1 ->  70.164 ; top5 ->  89.36  and loss:  979.3500565290451
forward train acc: top1 ->  69.84375 ; top5 ->  87.5703125  and loss:  256.5161148905754
test acc: top1 ->  70.946 ; top5 ->  89.81  and loss:  947.0160660147667
forward train acc: top1 ->  68.921875 ; top5 ->  87.453125  and loss:  258.5992582440376
test acc: top1 ->  70.94 ; top5 ->  89.838  and loss:  948.2020502090454
forward train acc: top1 ->  69.75 ; top5 ->  88.2109375  and loss:  247.96383315324783
test acc: top1 ->  70.932 ; top5 ->  89.798  and loss:  947.7436655163765
forward train acc: top1 ->  70.0859375 ; top5 ->  87.609375  and loss:  254.2310004234314
test acc: top1 ->  70.916 ; top5 ->  89.794  and loss:  946.0155535936356
forward train acc: top1 ->  70.4296875 ; top5 ->  87.84375  and loss:  250.03374910354614
test acc: top1 ->  70.936 ; top5 ->  89.842  and loss:  943.1850038766861
forward train acc: top1 ->  70.0234375 ; top5 ->  87.96875  and loss:  249.4759839773178
test acc: top1 ->  71.024 ; top5 ->  89.908  and loss:  943.3077208995819
forward train acc: top1 ->  69.7109375 ; top5 ->  87.65625  and loss:  253.07220137119293
test acc: top1 ->  71.066 ; top5 ->  89.882  and loss:  942.9159563779831
forward train acc: top1 ->  70.5546875 ; top5 ->  87.90625  and loss:  249.50049537420273
test acc: top1 ->  71.06 ; top5 ->  89.9  and loss:  943.0828158259392
forward train acc: top1 ->  70.296875 ; top5 ->  87.7421875  and loss:  250.78487712144852
test acc: top1 ->  71.138 ; top5 ->  89.936  and loss:  939.6067138314247
forward train acc: top1 ->  69.5859375 ; top5 ->  87.390625  and loss:  257.12377965450287
test acc: top1 ->  71.034 ; top5 ->  89.934  and loss:  940.8008690476418
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  122 / 128 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -63.86029839515686 , diff:  63.86029839515686
adv train loss:  -64.71071130037308 , diff:  0.850412905216217
layer  8  adv train finish, try to retain  78
test acc: top1 ->  69.382 ; top5 ->  89.112  and loss:  998.8858885765076
forward train acc: top1 ->  69.6171875 ; top5 ->  87.5078125  and loss:  253.67329627275467
test acc: top1 ->  70.678 ; top5 ->  89.622  and loss:  959.5565000772476
forward train acc: top1 ->  69.140625 ; top5 ->  87.40625  and loss:  261.0958034992218
test acc: top1 ->  70.81 ; top5 ->  89.764  and loss:  952.3184484243393
forward train acc: top1 ->  69.8046875 ; top5 ->  87.7421875  and loss:  256.2141812443733
test acc: top1 ->  70.71 ; top5 ->  89.682  and loss:  956.0223252177238
forward train acc: top1 ->  69.359375 ; top5 ->  86.859375  and loss:  261.2032832503319
test acc: top1 ->  70.734 ; top5 ->  89.726  and loss:  953.2953874468803
forward train acc: top1 ->  70.2265625 ; top5 ->  87.78125  and loss:  250.97657763957977
test acc: top1 ->  70.79 ; top5 ->  89.76  and loss:  953.6957525610924
forward train acc: top1 ->  69.796875 ; top5 ->  87.28125  and loss:  254.6186910867691
test acc: top1 ->  70.824 ; top5 ->  89.786  and loss:  951.2152035832405
forward train acc: top1 ->  69.3046875 ; top5 ->  87.484375  and loss:  256.6707546710968
test acc: top1 ->  70.886 ; top5 ->  89.746  and loss:  952.6401207447052
forward train acc: top1 ->  69.3515625 ; top5 ->  87.109375  and loss:  256.62368470430374
test acc: top1 ->  70.914 ; top5 ->  89.798  and loss:  949.0737217068672
forward train acc: top1 ->  69.6015625 ; top5 ->  87.2421875  and loss:  257.8544701933861
test acc: top1 ->  70.874 ; top5 ->  89.774  and loss:  947.3295483589172
forward train acc: top1 ->  69.5234375 ; top5 ->  87.1484375  and loss:  259.1769569516182
test acc: top1 ->  70.95 ; top5 ->  89.818  and loss:  947.6891914606094
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -62.59862017631531 , diff:  62.59862017631531
adv train loss:  -63.36875784397125 , diff:  0.7701376676559448
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  114
test acc: top1 ->  70.672 ; top5 ->  89.66  and loss:  957.9761747717857
forward train acc: top1 ->  69.28125 ; top5 ->  87.09375  and loss:  255.6649859547615
test acc: top1 ->  70.85 ; top5 ->  89.856  and loss:  951.8460299372673
forward train acc: top1 ->  70.015625 ; top5 ->  88.1796875  and loss:  246.5675597190857
test acc: top1 ->  71.02 ; top5 ->  89.824  and loss:  947.3912189602852
forward train acc: top1 ->  69.375 ; top5 ->  87.7109375  and loss:  255.38519698381424
test acc: top1 ->  70.92 ; top5 ->  89.842  and loss:  951.010756611824
forward train acc: top1 ->  70.34375 ; top5 ->  87.3125  and loss:  253.5153369307518
test acc: top1 ->  71.014 ; top5 ->  89.84  and loss:  945.4470256567001
forward train acc: top1 ->  70.3046875 ; top5 ->  88.0546875  and loss:  248.3322080373764
test acc: top1 ->  71.008 ; top5 ->  89.864  and loss:  945.667044043541
forward train acc: top1 ->  69.953125 ; top5 ->  87.96875  and loss:  249.16205328702927
test acc: top1 ->  70.976 ; top5 ->  89.832  and loss:  944.1655212044716
forward train acc: top1 ->  70.453125 ; top5 ->  88.3203125  and loss:  243.8463163971901
test acc: top1 ->  71.086 ; top5 ->  89.902  and loss:  942.4809007048607
forward train acc: top1 ->  70.2578125 ; top5 ->  87.5625  and loss:  251.37417924404144
test acc: top1 ->  71.154 ; top5 ->  89.944  and loss:  941.1651456356049
forward train acc: top1 ->  69.953125 ; top5 ->  87.8828125  and loss:  252.96547555923462
test acc: top1 ->  71.026 ; top5 ->  89.878  and loss:  943.2656915783882
forward train acc: top1 ->  70.2421875 ; top5 ->  88.15625  and loss:  246.60042315721512
test acc: top1 ->  71.03 ; top5 ->  89.878  and loss:  945.3922132253647
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  115 / 128 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -61.313664734363556 , diff:  61.313664734363556
adv train loss:  -63.322671830654144 , diff:  2.0090070962905884
layer  10  adv train finish, try to retain  72
test acc: top1 ->  68.312 ; top5 ->  88.492  and loss:  1038.9926298856735
forward train acc: top1 ->  68.7265625 ; top5 ->  87.2109375  and loss:  262.00083112716675
test acc: top1 ->  70.516 ; top5 ->  89.648  and loss:  959.9706142544746
forward train acc: top1 ->  69.4296875 ; top5 ->  87.40625  and loss:  258.5348125100136
test acc: top1 ->  70.574 ; top5 ->  89.604  and loss:  958.6540132761002
forward train acc: top1 ->  69.203125 ; top5 ->  87.3046875  and loss:  261.9194141626358
test acc: top1 ->  70.682 ; top5 ->  89.672  and loss:  958.5137550234795
forward train acc: top1 ->  70.03125 ; top5 ->  87.96875  and loss:  248.73765635490417
test acc: top1 ->  70.55 ; top5 ->  89.648  and loss:  960.7023924589157
forward train acc: top1 ->  69.484375 ; top5 ->  86.9296875  and loss:  261.03172409534454
test acc: top1 ->  70.756 ; top5 ->  89.726  and loss:  957.853395819664
forward train acc: top1 ->  69.4765625 ; top5 ->  87.4609375  and loss:  253.47799092531204
test acc: top1 ->  70.71 ; top5 ->  89.744  and loss:  956.465603530407
forward train acc: top1 ->  69.21875 ; top5 ->  87.34375  and loss:  259.6173444390297
test acc: top1 ->  70.73 ; top5 ->  89.752  and loss:  955.3639358878136
forward train acc: top1 ->  69.375 ; top5 ->  87.28125  and loss:  259.3581842184067
test acc: top1 ->  70.804 ; top5 ->  89.796  and loss:  953.7921075224876
forward train acc: top1 ->  69.5390625 ; top5 ->  87.6484375  and loss:  255.27788001298904
test acc: top1 ->  70.734 ; top5 ->  89.806  and loss:  954.7055857181549
forward train acc: top1 ->  69.8828125 ; top5 ->  87.7890625  and loss:  254.7567282319069
test acc: top1 ->  70.802 ; top5 ->  89.82  and loss:  950.7867378592491
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -62.65044295787811 , diff:  62.65044295787811
adv train loss:  -65.35338932275772 , diff:  2.702946364879608
layer  11  adv train finish, try to retain  93
test acc: top1 ->  68.622 ; top5 ->  88.476  and loss:  1030.4950643777847
forward train acc: top1 ->  70.015625 ; top5 ->  87.6171875  and loss:  254.2699477672577
test acc: top1 ->  70.814 ; top5 ->  89.688  and loss:  955.9376094937325
forward train acc: top1 ->  69.4296875 ; top5 ->  87.84375  and loss:  252.84001195430756
test acc: top1 ->  70.812 ; top5 ->  89.71  and loss:  959.1169444322586
forward train acc: top1 ->  69.734375 ; top5 ->  87.5703125  and loss:  250.18274056911469
test acc: top1 ->  70.87 ; top5 ->  89.79  and loss:  954.436380147934
forward train acc: top1 ->  69.8046875 ; top5 ->  87.421875  and loss:  255.91070437431335
test acc: top1 ->  70.916 ; top5 ->  89.77  and loss:  950.2757472395897
forward train acc: top1 ->  69.4765625 ; top5 ->  87.65625  and loss:  254.80246371030807
test acc: top1 ->  70.824 ; top5 ->  89.768  and loss:  953.4647822380066
forward train acc: top1 ->  69.65625 ; top5 ->  87.5  and loss:  253.87672239542007
test acc: top1 ->  70.864 ; top5 ->  89.768  and loss:  951.555917263031
forward train acc: top1 ->  69.3359375 ; top5 ->  87.4921875  and loss:  255.00293600559235
test acc: top1 ->  70.874 ; top5 ->  89.808  and loss:  950.0661121606827
forward train acc: top1 ->  70.1015625 ; top5 ->  88.0546875  and loss:  248.82717591524124
test acc: top1 ->  70.918 ; top5 ->  89.864  and loss:  947.3506581783295
forward train acc: top1 ->  69.375 ; top5 ->  87.5546875  and loss:  253.82813692092896
test acc: top1 ->  70.992 ; top5 ->  89.822  and loss:  945.097070991993
forward train acc: top1 ->  70.765625 ; top5 ->  88.28125  and loss:  245.50965797901154
test acc: top1 ->  71.108 ; top5 ->  89.836  and loss:  944.44502389431
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -63.610465705394745 , diff:  63.610465705394745
adv train loss:  -65.08920991420746 , diff:  1.4787442088127136
layer  12  adv train finish, try to retain  94
test acc: top1 ->  69.824 ; top5 ->  89.128  and loss:  988.9897131919861
forward train acc: top1 ->  69.4765625 ; top5 ->  87.6640625  and loss:  254.49128544330597
test acc: top1 ->  70.854 ; top5 ->  89.748  and loss:  953.2308730483055
forward train acc: top1 ->  69.2890625 ; top5 ->  86.9921875  and loss:  262.07126516103745
test acc: top1 ->  70.858 ; top5 ->  89.756  and loss:  951.1010145545006
forward train acc: top1 ->  69.734375 ; top5 ->  87.6328125  and loss:  253.92408227920532
test acc: top1 ->  70.894 ; top5 ->  89.76  and loss:  951.5026282668114
forward train acc: top1 ->  69.4140625 ; top5 ->  87.6875  and loss:  255.93783098459244
test acc: top1 ->  70.918 ; top5 ->  89.822  and loss:  950.4264340400696
forward train acc: top1 ->  69.921875 ; top5 ->  88.0078125  and loss:  250.9819552898407
test acc: top1 ->  70.944 ; top5 ->  89.75  and loss:  949.1660809516907
forward train acc: top1 ->  70.09375 ; top5 ->  87.8203125  and loss:  251.47403264045715
test acc: top1 ->  70.9 ; top5 ->  89.776  and loss:  951.0174019932747
forward train acc: top1 ->  69.359375 ; top5 ->  87.4375  and loss:  256.3520467877388
test acc: top1 ->  71.016 ; top5 ->  89.822  and loss:  949.876761496067
forward train acc: top1 ->  69.875 ; top5 ->  87.34375  and loss:  249.64270770549774
test acc: top1 ->  71.06 ; top5 ->  89.88  and loss:  947.6043545603752
forward train acc: top1 ->  70.4375 ; top5 ->  88.1328125  and loss:  246.88826495409012
test acc: top1 ->  71.062 ; top5 ->  89.834  and loss:  944.1133010387421
forward train acc: top1 ->  70.3203125 ; top5 ->  87.609375  and loss:  253.0047743320465
test acc: top1 ->  71.146 ; top5 ->  89.9  and loss:  946.2224225401878
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  98 / 128 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -61.249044716358185 , diff:  61.249044716358185
adv train loss:  -66.70641458034515 , diff:  5.457369863986969
layer  13  adv train finish, try to retain  59
test acc: top1 ->  68.148 ; top5 ->  87.992  and loss:  1050.8877888917923
forward train acc: top1 ->  69.3828125 ; top5 ->  87.71875  and loss:  257.1003724336624
test acc: top1 ->  70.684 ; top5 ->  89.492  and loss:  960.2579039335251
forward train acc: top1 ->  69.2109375 ; top5 ->  87.2734375  and loss:  258.84924840927124
test acc: top1 ->  70.682 ; top5 ->  89.614  and loss:  959.0781899094582
forward train acc: top1 ->  68.7109375 ; top5 ->  87.09375  and loss:  260.2192911505699
test acc: top1 ->  70.748 ; top5 ->  89.598  and loss:  954.0834864377975
forward train acc: top1 ->  69.953125 ; top5 ->  87.6484375  and loss:  253.73329615592957
test acc: top1 ->  70.74 ; top5 ->  89.642  and loss:  955.7097495794296
forward train acc: top1 ->  68.6796875 ; top5 ->  87.25  and loss:  261.66802674531937
test acc: top1 ->  70.702 ; top5 ->  89.674  and loss:  953.2829170823097
forward train acc: top1 ->  69.5234375 ; top5 ->  87.4140625  and loss:  256.2799663543701
test acc: top1 ->  70.776 ; top5 ->  89.67  and loss:  953.0979970097542
forward train acc: top1 ->  69.0 ; top5 ->  87.40625  and loss:  258.29178750514984
test acc: top1 ->  70.746 ; top5 ->  89.69  and loss:  950.1787365078926
forward train acc: top1 ->  69.2109375 ; top5 ->  87.046875  and loss:  258.1029336452484
test acc: top1 ->  70.762 ; top5 ->  89.694  and loss:  948.6584357023239
forward train acc: top1 ->  69.703125 ; top5 ->  87.5  and loss:  253.9239457845688
test acc: top1 ->  70.792 ; top5 ->  89.654  and loss:  952.8585251569748
forward train acc: top1 ->  69.546875 ; top5 ->  87.796875  and loss:  253.88773268461227
test acc: top1 ->  70.862 ; top5 ->  89.686  and loss:  950.5912442207336
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  71 / 128 , inc:  1
---------------- start layer  14  ---------------
adv train loss:  -61.22067439556122 , diff:  61.22067439556122
adv train loss:  -63.71370267868042 , diff:  2.4930282831192017
layer  14  adv train finish, try to retain  242
test acc: top1 ->  70.462 ; top5 ->  89.666  and loss:  957.1362128257751
forward train acc: top1 ->  69.3125 ; top5 ->  88.0  and loss:  252.32655358314514
test acc: top1 ->  70.976 ; top5 ->  89.858  and loss:  943.2276252508163
forward train acc: top1 ->  70.3125 ; top5 ->  87.96875  and loss:  245.8909267783165
test acc: top1 ->  70.99 ; top5 ->  89.798  and loss:  946.118938267231
forward train acc: top1 ->  69.9921875 ; top5 ->  87.828125  and loss:  250.0899088382721
test acc: top1 ->  70.892 ; top5 ->  89.84  and loss:  947.3463398814201
forward train acc: top1 ->  69.9296875 ; top5 ->  87.96875  and loss:  249.685830950737
test acc: top1 ->  71.05 ; top5 ->  89.886  and loss:  943.0137674808502
forward train acc: top1 ->  69.828125 ; top5 ->  87.453125  and loss:  250.84337759017944
test acc: top1 ->  71.098 ; top5 ->  89.95  and loss:  940.8803742527962
forward train acc: top1 ->  69.5234375 ; top5 ->  87.2421875  and loss:  258.2216445803642
test acc: top1 ->  71.034 ; top5 ->  89.904  and loss:  942.7044221758842
forward train acc: top1 ->  70.203125 ; top5 ->  87.859375  and loss:  249.9025206565857
test acc: top1 ->  71.032 ; top5 ->  89.938  and loss:  943.4329968094826
forward train acc: top1 ->  69.890625 ; top5 ->  87.8125  and loss:  249.53989523649216
test acc: top1 ->  71.174 ; top5 ->  89.978  and loss:  942.69466817379
forward train acc: top1 ->  69.5546875 ; top5 ->  87.625  and loss:  253.90814352035522
test acc: top1 ->  71.122 ; top5 ->  89.974  and loss:  940.3508485555649
forward train acc: top1 ->  69.75 ; top5 ->  87.796875  and loss:  250.2226293683052
test acc: top1 ->  71.14 ; top5 ->  90.026  and loss:  941.2332496643066
>>>>>>> reverse layer  14  since performance drop >>>>>>>
==> this epoch:  250 / 256 , inc:  1
---------------- start layer  15  ---------------
adv train loss:  -61.04393512010574 , diff:  61.04393512010574
adv train loss:  -63.60814547538757 , diff:  2.56421035528183
layer  15  adv train finish, try to retain  252
>>>>>>> reverse layer  15  since no improvement >>>>>>>
---------------- start layer  16  ---------------
adv train loss:  -60.543546199798584 , diff:  60.543546199798584
adv train loss:  -62.5630966424942 , diff:  2.0195504426956177
************ all values are small in this layer **********
layer  16  adv train finish, try to retain  238
test acc: top1 ->  70.95 ; top5 ->  89.846  and loss:  946.3190410137177
forward train acc: top1 ->  70.046875 ; top5 ->  87.6875  and loss:  249.8418248295784
test acc: top1 ->  70.868 ; top5 ->  89.828  and loss:  949.0689408183098
forward train acc: top1 ->  69.5078125 ; top5 ->  87.3359375  and loss:  257.1991210579872
test acc: top1 ->  70.658 ; top5 ->  89.86  and loss:  951.380141377449
forward train acc: top1 ->  69.3046875 ; top5 ->  87.546875  and loss:  256.8039199113846
test acc: top1 ->  70.788 ; top5 ->  89.8  and loss:  949.7826669216156
forward train acc: top1 ->  69.40625 ; top5 ->  87.9375  and loss:  249.47824841737747
test acc: top1 ->  70.782 ; top5 ->  89.826  and loss:  948.5960622429848
forward train acc: top1 ->  69.2890625 ; top5 ->  87.3515625  and loss:  253.85598748922348
test acc: top1 ->  70.896 ; top5 ->  89.916  and loss:  947.4653052687645
forward train acc: top1 ->  69.5078125 ; top5 ->  87.7265625  and loss:  252.9658061861992
test acc: top1 ->  70.856 ; top5 ->  89.904  and loss:  947.3532952666283
forward train acc: top1 ->  70.1015625 ; top5 ->  87.71875  and loss:  252.84772384166718
test acc: top1 ->  70.982 ; top5 ->  89.91  and loss:  946.2794454097748
forward train acc: top1 ->  69.8984375 ; top5 ->  87.703125  and loss:  251.82732111215591
test acc: top1 ->  71.004 ; top5 ->  89.936  and loss:  944.6452276110649
forward train acc: top1 ->  69.7109375 ; top5 ->  87.765625  and loss:  252.42840856313705
test acc: top1 ->  70.966 ; top5 ->  89.896  and loss:  944.3845978975296
forward train acc: top1 ->  70.359375 ; top5 ->  87.6015625  and loss:  252.7607135772705
test acc: top1 ->  70.974 ; top5 ->  89.932  and loss:  944.4026017785072
>>>>>>> reverse layer  16  since performance drop >>>>>>>
==> this epoch:  239 / 256 , inc:  1
---------------- start layer  17  ---------------
adv train loss:  -60.86996763944626 , diff:  60.86996763944626
adv train loss:  -58.43906760215759 , diff:  2.4309000372886658
************ all values are small in this layer **********
layer  17  adv train finish, try to retain  229
test acc: top1 ->  70.914 ; top5 ->  89.892  and loss:  950.6767308115959
forward train acc: top1 ->  69.6796875 ; top5 ->  88.0390625  and loss:  250.95763617753983
test acc: top1 ->  70.922 ; top5 ->  89.888  and loss:  946.6862450242043
forward train acc: top1 ->  69.65625 ; top5 ->  87.4140625  and loss:  253.51334232091904
test acc: top1 ->  70.852 ; top5 ->  89.924  and loss:  948.51004332304
forward train acc: top1 ->  70.296875 ; top5 ->  88.1171875  and loss:  250.84043049812317
test acc: top1 ->  70.916 ; top5 ->  89.906  and loss:  945.6928575634956
forward train acc: top1 ->  70.5390625 ; top5 ->  87.796875  and loss:  247.53099209070206
test acc: top1 ->  70.982 ; top5 ->  89.902  and loss:  942.6310268044472
forward train acc: top1 ->  69.6015625 ; top5 ->  87.4609375  and loss:  254.27807307243347
test acc: top1 ->  70.828 ; top5 ->  89.91  and loss:  944.350779235363
forward train acc: top1 ->  70.0234375 ; top5 ->  87.890625  and loss:  249.76877760887146
test acc: top1 ->  70.922 ; top5 ->  89.842  and loss:  948.6033042669296
forward train acc: top1 ->  69.71875 ; top5 ->  87.8359375  and loss:  252.91646373271942
test acc: top1 ->  71.096 ; top5 ->  89.934  and loss:  942.3651306033134
forward train acc: top1 ->  69.875 ; top5 ->  87.453125  and loss:  253.44829964637756
test acc: top1 ->  71.066 ; top5 ->  89.946  and loss:  945.1283659338951
forward train acc: top1 ->  70.3515625 ; top5 ->  88.1171875  and loss:  248.8286481499672
test acc: top1 ->  71.15 ; top5 ->  89.954  and loss:  944.9551349282265
forward train acc: top1 ->  70.8515625 ; top5 ->  88.1875  and loss:  248.8071362376213
test acc: top1 ->  71.024 ; top5 ->  89.98  and loss:  940.631228864193
>>>>>>> reverse layer  17  since performance drop >>>>>>>
==> this epoch:  230 / 256 , inc:  1
---------------- start layer  18  ---------------
adv train loss:  -62.759055614471436 , diff:  62.759055614471436
adv train loss:  -58.924487829208374 , diff:  3.8345677852630615
************ all values are small in this layer **********
layer  18  adv train finish, try to retain  234
test acc: top1 ->  71.058 ; top5 ->  89.912  and loss:  945.881463944912
forward train acc: top1 ->  70.296875 ; top5 ->  87.578125  and loss:  251.3565639257431
test acc: top1 ->  70.848 ; top5 ->  89.878  and loss:  947.980504989624
forward train acc: top1 ->  70.1640625 ; top5 ->  87.7890625  and loss:  251.3732705116272
test acc: top1 ->  71.016 ; top5 ->  89.904  and loss:  946.3330589532852
forward train acc: top1 ->  69.5703125 ; top5 ->  87.703125  and loss:  252.2074150443077
test acc: top1 ->  70.796 ; top5 ->  89.79  and loss:  947.9670404195786
forward train acc: top1 ->  70.0 ; top5 ->  88.046875  and loss:  250.2474570274353
test acc: top1 ->  71.014 ; top5 ->  89.992  and loss:  940.3587079048157
forward train acc: top1 ->  69.59375 ; top5 ->  87.234375  and loss:  255.7284379005432
test acc: top1 ->  70.95 ; top5 ->  89.966  and loss:  943.9512111544609
forward train acc: top1 ->  69.5390625 ; top5 ->  87.6796875  and loss:  252.8612426519394
test acc: top1 ->  70.994 ; top5 ->  89.974  and loss:  942.6120845675468
forward train acc: top1 ->  70.0078125 ; top5 ->  87.8671875  and loss:  249.87950110435486
test acc: top1 ->  71.146 ; top5 ->  90.002  and loss:  941.931255698204
forward train acc: top1 ->  69.984375 ; top5 ->  87.8515625  and loss:  250.29917365312576
test acc: top1 ->  71.134 ; top5 ->  89.964  and loss:  942.9047747254372
forward train acc: top1 ->  70.40625 ; top5 ->  87.8984375  and loss:  249.0843010544777
test acc: top1 ->  71.124 ; top5 ->  89.99  and loss:  941.7972235679626
forward train acc: top1 ->  69.9296875 ; top5 ->  87.7734375  and loss:  250.22043025493622
test acc: top1 ->  71.154 ; top5 ->  90.02  and loss:  941.8241854310036
>>>>>>> reverse layer  18  since performance drop >>>>>>>
==> this epoch:  235 / 256 , inc:  1
---------------- start layer  19  ---------------
adv train loss:  -59.79741168022156 , diff:  59.79741168022156
adv train loss:  -62.98046654462814 , diff:  3.1830548644065857
layer  19  adv train finish, try to retain  174
test acc: top1 ->  70.372 ; top5 ->  89.588  and loss:  963.7989910840988
forward train acc: top1 ->  70.046875 ; top5 ->  87.3203125  and loss:  256.1754105091095
test acc: top1 ->  70.686 ; top5 ->  89.746  and loss:  956.0597013235092
forward train acc: top1 ->  68.8671875 ; top5 ->  87.1015625  and loss:  259.73283392190933
test acc: top1 ->  70.528 ; top5 ->  89.71  and loss:  956.3906553387642
forward train acc: top1 ->  69.28125 ; top5 ->  88.0  and loss:  251.04245483875275
test acc: top1 ->  70.684 ; top5 ->  89.74  and loss:  957.0707275271416
forward train acc: top1 ->  69.765625 ; top5 ->  87.5390625  and loss:  253.7538869380951
test acc: top1 ->  70.708 ; top5 ->  89.854  and loss:  953.4494130015373
forward train acc: top1 ->  69.265625 ; top5 ->  87.2109375  and loss:  260.39637529850006
test acc: top1 ->  70.714 ; top5 ->  89.756  and loss:  955.2743943929672
forward train acc: top1 ->  69.859375 ; top5 ->  88.125  and loss:  248.4952985048294
test acc: top1 ->  70.698 ; top5 ->  89.808  and loss:  953.9980966448784
forward train acc: top1 ->  69.6796875 ; top5 ->  87.6796875  and loss:  254.04871141910553
test acc: top1 ->  70.798 ; top5 ->  89.772  and loss:  951.1054531335831
forward train acc: top1 ->  69.65625 ; top5 ->  87.9296875  and loss:  252.2513244152069
test acc: top1 ->  70.822 ; top5 ->  89.822  and loss:  953.4099205732346
forward train acc: top1 ->  70.1875 ; top5 ->  88.0234375  and loss:  247.94849109649658
test acc: top1 ->  70.908 ; top5 ->  89.838  and loss:  948.9517829418182
forward train acc: top1 ->  69.8359375 ; top5 ->  87.3671875  and loss:  255.901540517807
test acc: top1 ->  70.866 ; top5 ->  89.824  and loss:  949.1270929574966
>>>>>>> reverse layer  19  since performance drop >>>>>>>
==> this epoch:  179 / 256 , inc:  1
---------------- start layer  20  ---------------
adv train loss:  -61.655853033065796 , diff:  61.655853033065796
adv train loss:  -64.87859427928925 , diff:  3.2227412462234497
************ all values are small in this layer **********
layer  20  adv train finish, try to retain  224
test acc: top1 ->  70.992 ; top5 ->  89.846  and loss:  948.5845563411713
forward train acc: top1 ->  69.6640625 ; top5 ->  87.4609375  and loss:  253.43150305747986
test acc: top1 ->  70.848 ; top5 ->  89.802  and loss:  947.798689186573
forward train acc: top1 ->  69.140625 ; top5 ->  87.4609375  and loss:  258.29120123386383
test acc: top1 ->  70.938 ; top5 ->  89.806  and loss:  947.7798371911049
forward train acc: top1 ->  70.1953125 ; top5 ->  87.84375  and loss:  250.5951966047287
test acc: top1 ->  70.938 ; top5 ->  89.842  and loss:  951.4155765771866
forward train acc: top1 ->  69.8671875 ; top5 ->  88.2890625  and loss:  247.67856949567795
test acc: top1 ->  70.996 ; top5 ->  89.868  and loss:  946.4273471236229
forward train acc: top1 ->  70.3359375 ; top5 ->  87.765625  and loss:  251.30728423595428
test acc: top1 ->  71.068 ; top5 ->  89.938  and loss:  946.6316420435905
forward train acc: top1 ->  69.9921875 ; top5 ->  87.4140625  and loss:  256.07127487659454
test acc: top1 ->  71.014 ; top5 ->  89.814  and loss:  947.2783743739128
forward train acc: top1 ->  70.140625 ; top5 ->  87.359375  and loss:  254.20726376771927
test acc: top1 ->  71.012 ; top5 ->  89.808  and loss:  944.8039408922195
forward train acc: top1 ->  70.0546875 ; top5 ->  87.4921875  and loss:  251.16690760850906
test acc: top1 ->  71.026 ; top5 ->  89.846  and loss:  944.8182015419006
forward train acc: top1 ->  70.1640625 ; top5 ->  87.8125  and loss:  250.6079488992691
test acc: top1 ->  71.186 ; top5 ->  89.874  and loss:  945.1880784630775
forward train acc: top1 ->  70.4296875 ; top5 ->  88.1015625  and loss:  245.7513768672943
test acc: top1 ->  71.008 ; top5 ->  89.904  and loss:  943.8467066287994
>>>>>>> reverse layer  20  since performance drop >>>>>>>
==> this epoch:  225 / 256 , inc:  1
---------------- start layer  21  ---------------
adv train loss:  -60.59008926153183 , diff:  60.59008926153183
adv train loss:  -59.89778834581375 , diff:  0.6923009157180786
layer  21  adv train finish, try to retain  246
>>>>>>> reverse layer  21  since no improvement >>>>>>>
---------------- start layer  22  ---------------
adv train loss:  -60.7412423491478 , diff:  60.7412423491478
adv train loss:  -64.84603744745255 , diff:  4.1047950983047485
************ all values are small in this layer **********
layer  22  adv train finish, try to retain  207
test acc: top1 ->  70.954 ; top5 ->  89.75  and loss:  954.3294178247452
forward train acc: top1 ->  69.734375 ; top5 ->  87.609375  and loss:  251.3491280078888
test acc: top1 ->  70.876 ; top5 ->  89.774  and loss:  952.8781193494797
forward train acc: top1 ->  69.8125 ; top5 ->  87.8046875  and loss:  251.36650961637497
test acc: top1 ->  70.874 ; top5 ->  89.824  and loss:  949.5954706668854
forward train acc: top1 ->  70.046875 ; top5 ->  87.7109375  and loss:  251.59453094005585
test acc: top1 ->  70.92 ; top5 ->  89.77  and loss:  948.5367761254311
forward train acc: top1 ->  70.2265625 ; top5 ->  87.84375  and loss:  249.09667593240738
test acc: top1 ->  70.856 ; top5 ->  89.8  and loss:  948.608008146286
forward train acc: top1 ->  70.0390625 ; top5 ->  88.0625  and loss:  247.48347508907318
test acc: top1 ->  70.978 ; top5 ->  89.898  and loss:  951.536551296711
forward train acc: top1 ->  70.4296875 ; top5 ->  87.671875  and loss:  250.376658141613
test acc: top1 ->  71.058 ; top5 ->  89.886  and loss:  950.8898075819016
forward train acc: top1 ->  69.9921875 ; top5 ->  87.8046875  and loss:  252.5757983326912
test acc: top1 ->  71.062 ; top5 ->  89.904  and loss:  947.4328124523163
forward train acc: top1 ->  70.1640625 ; top5 ->  88.0234375  and loss:  249.2876200079918
test acc: top1 ->  71.16 ; top5 ->  89.91  and loss:  945.5240152478218
forward train acc: top1 ->  70.65625 ; top5 ->  87.984375  and loss:  247.48872864246368
test acc: top1 ->  71.094 ; top5 ->  89.91  and loss:  946.7523545622826
forward train acc: top1 ->  69.8046875 ; top5 ->  87.703125  and loss:  252.81011998653412
test acc: top1 ->  71.124 ; top5 ->  89.832  and loss:  948.0510743260384
>>>>>>> reverse layer  22  since performance drop >>>>>>>
==> this epoch:  208 / 256 , inc:  1
---------------- start layer  23  ---------------
adv train loss:  -61.87927436828613 , diff:  61.87927436828613
adv train loss:  -58.865986704826355 , diff:  3.013287663459778
layer  23  adv train finish, try to retain  176
test acc: top1 ->  70.75 ; top5 ->  89.638  and loss:  961.6417627930641
forward train acc: top1 ->  69.640625 ; top5 ->  88.0546875  and loss:  253.38617998361588
test acc: top1 ->  70.922 ; top5 ->  89.738  and loss:  952.610505759716
forward train acc: top1 ->  69.3203125 ; top5 ->  87.4921875  and loss:  257.0998484492302
test acc: top1 ->  70.958 ; top5 ->  89.79  and loss:  953.2849863171577
forward train acc: top1 ->  69.3828125 ; top5 ->  87.6328125  and loss:  256.4332255125046
test acc: top1 ->  70.988 ; top5 ->  89.734  and loss:  953.5910503864288
forward train acc: top1 ->  69.7265625 ; top5 ->  87.59375  and loss:  255.1781329512596
test acc: top1 ->  70.972 ; top5 ->  89.786  and loss:  950.4074736833572
forward train acc: top1 ->  70.3984375 ; top5 ->  87.9765625  and loss:  246.7228507399559
test acc: top1 ->  71.04 ; top5 ->  89.768  and loss:  950.7622948884964
forward train acc: top1 ->  69.7421875 ; top5 ->  87.9453125  and loss:  253.0407282114029
test acc: top1 ->  71.058 ; top5 ->  89.848  and loss:  947.7281903624535
forward train acc: top1 ->  69.921875 ; top5 ->  87.8046875  and loss:  252.31013298034668
test acc: top1 ->  71.188 ; top5 ->  89.914  and loss:  945.8633996248245
forward train acc: top1 ->  69.7421875 ; top5 ->  87.65625  and loss:  253.19056916236877
test acc: top1 ->  71.15 ; top5 ->  89.9  and loss:  945.4687494635582
forward train acc: top1 ->  70.9453125 ; top5 ->  87.8671875  and loss:  248.03231662511826
test acc: top1 ->  71.126 ; top5 ->  89.832  and loss:  945.3998349905014
forward train acc: top1 ->  70.46875 ; top5 ->  88.09375  and loss:  245.64098769426346
test acc: top1 ->  71.188 ; top5 ->  89.85  and loss:  945.0258209705353
>>>>>>> reverse layer  23  since performance drop >>>>>>>
==> this epoch:  187 / 256 , inc:  1
---------------- start layer  24  ---------------
adv train loss:  -62.00468170642853 , diff:  62.00468170642853
adv train loss:  -62.50331711769104 , diff:  0.4986354112625122
layer  24  adv train finish, try to retain  238
test acc: top1 ->  70.914 ; top5 ->  89.83  and loss:  947.7333099842072
forward train acc: top1 ->  69.421875 ; top5 ->  87.6796875  and loss:  256.47781533002853
test acc: top1 ->  70.944 ; top5 ->  89.846  and loss:  943.2262289524078
forward train acc: top1 ->  69.7109375 ; top5 ->  87.9921875  and loss:  251.62955224514008
test acc: top1 ->  70.978 ; top5 ->  89.802  and loss:  945.8489991426468
forward train acc: top1 ->  70.140625 ; top5 ->  87.7890625  and loss:  247.59109270572662
test acc: top1 ->  71.028 ; top5 ->  89.866  and loss:  946.0105083584785
forward train acc: top1 ->  70.515625 ; top5 ->  87.953125  and loss:  247.0326938033104
test acc: top1 ->  70.912 ; top5 ->  89.838  and loss:  948.0358992815018
forward train acc: top1 ->  69.8125 ; top5 ->  87.5625  and loss:  250.39821749925613
test acc: top1 ->  71.072 ; top5 ->  89.848  and loss:  943.8258848786354
forward train acc: top1 ->  69.9375 ; top5 ->  87.6875  and loss:  251.04168701171875
test acc: top1 ->  71.06 ; top5 ->  89.86  and loss:  946.184240937233
forward train acc: top1 ->  70.4140625 ; top5 ->  88.1484375  and loss:  247.68882805109024
test acc: top1 ->  71.084 ; top5 ->  89.91  and loss:  942.8188325762749
forward train acc: top1 ->  69.6328125 ; top5 ->  87.8671875  and loss:  255.20390856266022
test acc: top1 ->  71.1 ; top5 ->  89.88  and loss:  943.3306936621666
forward train acc: top1 ->  70.2578125 ; top5 ->  87.90625  and loss:  248.77519154548645
test acc: top1 ->  71.012 ; top5 ->  89.886  and loss:  944.2823501229286
forward train acc: top1 ->  69.453125 ; top5 ->  87.75  and loss:  253.2787362933159
test acc: top1 ->  71.012 ; top5 ->  89.884  and loss:  944.2674980163574
>>>>>>> reverse layer  24  since performance drop >>>>>>>
==> this epoch:  244 / 256 , inc:  1
---------------- start layer  25  ---------------
adv train loss:  -62.10793536901474 , diff:  62.10793536901474
adv train loss:  -64.33800023794174 , diff:  2.230064868927002
layer  25  adv train finish, try to retain  238
test acc: top1 ->  71.004 ; top5 ->  89.848  and loss:  951.2258488535881
forward train acc: top1 ->  69.609375 ; top5 ->  87.703125  and loss:  256.42142856121063
test acc: top1 ->  70.946 ; top5 ->  89.858  and loss:  946.1198801994324
forward train acc: top1 ->  69.90625 ; top5 ->  87.7890625  and loss:  253.01345139741898
test acc: top1 ->  71.086 ; top5 ->  89.884  and loss:  944.8341847658157
forward train acc: top1 ->  70.03125 ; top5 ->  87.8203125  and loss:  249.35308414697647
test acc: top1 ->  71.148 ; top5 ->  89.902  and loss:  945.1490904092789
forward train acc: top1 ->  69.9375 ; top5 ->  87.7421875  and loss:  250.02802032232285
test acc: top1 ->  71.032 ; top5 ->  89.952  and loss:  944.7847881317139
forward train acc: top1 ->  68.71875 ; top5 ->  87.03125  and loss:  257.9079569578171
test acc: top1 ->  71.178 ; top5 ->  89.888  and loss:  942.4302866458893
forward train acc: top1 ->  69.96875 ; top5 ->  87.6640625  and loss:  251.55438607931137
test acc: top1 ->  71.116 ; top5 ->  89.94  and loss:  941.83546179533
forward train acc: top1 ->  69.84375 ; top5 ->  87.5078125  and loss:  258.28737676143646
test acc: top1 ->  71.142 ; top5 ->  89.926  and loss:  940.6215043067932
forward train acc: top1 ->  70.6328125 ; top5 ->  88.0234375  and loss:  247.41107213497162
test acc: top1 ->  71.23 ; top5 ->  89.934  and loss:  939.1649888753891
forward train acc: top1 ->  70.5546875 ; top5 ->  88.171875  and loss:  244.84565937519073
test acc: top1 ->  71.214 ; top5 ->  89.986  and loss:  936.8018066883087
forward train acc: top1 ->  70.84375 ; top5 ->  88.3125  and loss:  240.15960496664047
test acc: top1 ->  71.216 ; top5 ->  90.024  and loss:  939.3183143138885
>>>>>>> reverse layer  25  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  26  ---------------
adv train loss:  -65.0789430141449 , diff:  65.0789430141449
adv train loss:  -61.69401115179062 , diff:  3.3849318623542786
layer  26  adv train finish, try to retain  490
test acc: top1 ->  70.808 ; top5 ->  89.666  and loss:  955.486940741539
forward train acc: top1 ->  69.34375 ; top5 ->  87.8203125  and loss:  256.8883059024811
test acc: top1 ->  70.838 ; top5 ->  89.692  and loss:  953.5389963388443
forward train acc: top1 ->  69.5703125 ; top5 ->  87.2109375  and loss:  254.02502065896988
test acc: top1 ->  70.788 ; top5 ->  89.74  and loss:  951.6270020008087
forward train acc: top1 ->  70.1484375 ; top5 ->  88.0  and loss:  248.4855923652649
test acc: top1 ->  70.812 ; top5 ->  89.766  and loss:  950.7866061925888
forward train acc: top1 ->  69.5078125 ; top5 ->  87.53125  and loss:  256.59733551740646
test acc: top1 ->  70.816 ; top5 ->  89.712  and loss:  952.7881198525429
forward train acc: top1 ->  70.0859375 ; top5 ->  87.5390625  and loss:  253.91349476575851
test acc: top1 ->  70.992 ; top5 ->  89.804  and loss:  945.9763535857201
forward train acc: top1 ->  69.546875 ; top5 ->  87.8359375  and loss:  252.6642793416977
test acc: top1 ->  70.968 ; top5 ->  89.846  and loss:  946.2679824829102
forward train acc: top1 ->  70.0078125 ; top5 ->  87.3359375  and loss:  254.11761856079102
test acc: top1 ->  71.01 ; top5 ->  89.854  and loss:  946.9392755627632
forward train acc: top1 ->  69.796875 ; top5 ->  87.484375  and loss:  254.16346472501755
test acc: top1 ->  71.074 ; top5 ->  89.872  and loss:  944.9599257707596
forward train acc: top1 ->  70.5078125 ; top5 ->  87.4765625  and loss:  250.81842875480652
test acc: top1 ->  71.028 ; top5 ->  89.864  and loss:  946.1721648573875
forward train acc: top1 ->  69.96875 ; top5 ->  87.6796875  and loss:  252.3231942653656
test acc: top1 ->  71.026 ; top5 ->  89.828  and loss:  944.7602405548096
>>>>>>> reverse layer  26  since performance drop >>>>>>>
==> this epoch:  502 / 512 , inc:  1
---------------- start layer  27  ---------------
adv train loss:  -62.311007142066956 , diff:  62.311007142066956
adv train loss:  -60.182972609996796 , diff:  2.12803453207016
layer  27  adv train finish, try to retain  502
test acc: top1 ->  71.018 ; top5 ->  89.812  and loss:  949.5295034050941
forward train acc: top1 ->  69.953125 ; top5 ->  87.4921875  and loss:  253.13021475076675
test acc: top1 ->  70.92 ; top5 ->  89.786  and loss:  952.4453311562538
forward train acc: top1 ->  69.796875 ; top5 ->  87.546875  and loss:  254.46062976121902
test acc: top1 ->  70.956 ; top5 ->  89.822  and loss:  948.0813157558441
forward train acc: top1 ->  70.0234375 ; top5 ->  88.0  and loss:  247.89527320861816
test acc: top1 ->  71.074 ; top5 ->  89.852  and loss:  947.6645525693893
forward train acc: top1 ->  70.390625 ; top5 ->  87.7109375  and loss:  249.46681821346283
test acc: top1 ->  71.068 ; top5 ->  89.83  and loss:  944.8890093564987
forward train acc: top1 ->  69.8671875 ; top5 ->  87.640625  and loss:  252.8258513212204
test acc: top1 ->  71.072 ; top5 ->  89.86  and loss:  946.6583715081215
forward train acc: top1 ->  70.2109375 ; top5 ->  87.453125  and loss:  251.65977001190186
test acc: top1 ->  71.084 ; top5 ->  89.89  and loss:  945.3960993289948
forward train acc: top1 ->  70.078125 ; top5 ->  87.765625  and loss:  252.71934258937836
test acc: top1 ->  71.1 ; top5 ->  89.916  and loss:  942.5346283912659
forward train acc: top1 ->  69.7265625 ; top5 ->  87.8828125  and loss:  255.0099887251854
test acc: top1 ->  71.2 ; top5 ->  90.016  and loss:  939.9852765798569
forward train acc: top1 ->  70.078125 ; top5 ->  87.8515625  and loss:  249.93246048688889
test acc: top1 ->  71.036 ; top5 ->  89.898  and loss:  941.0219116806984
forward train acc: top1 ->  70.6484375 ; top5 ->  88.125  and loss:  244.4722140431404
test acc: top1 ->  71.186 ; top5 ->  89.958  and loss:  942.1024742126465
>>>>>>> reverse layer  27  since performance drop >>>>>>>
==> this epoch:  505 / 512 , inc:  1
---------------- start layer  28  ---------------
adv train loss:  -61.8886736035347 , diff:  61.8886736035347
adv train loss:  -63.68362075090408 , diff:  1.7949471473693848
layer  28  adv train finish, try to retain  498
test acc: top1 ->  70.984 ; top5 ->  90.0  and loss:  945.4837359189987
forward train acc: top1 ->  69.7109375 ; top5 ->  87.65625  and loss:  254.37127912044525
test acc: top1 ->  71.056 ; top5 ->  89.84  and loss:  945.2280604839325
forward train acc: top1 ->  69.8515625 ; top5 ->  87.6875  and loss:  250.47213071584702
test acc: top1 ->  70.852 ; top5 ->  89.87  and loss:  947.4977762699127
forward train acc: top1 ->  69.390625 ; top5 ->  87.515625  and loss:  256.6783630847931
test acc: top1 ->  71.01 ; top5 ->  89.906  and loss:  945.0341602563858
forward train acc: top1 ->  70.7734375 ; top5 ->  88.125  and loss:  244.74949496984482
test acc: top1 ->  71.062 ; top5 ->  89.918  and loss:  944.2482638955116
forward train acc: top1 ->  69.7734375 ; top5 ->  88.2890625  and loss:  247.1899139881134
test acc: top1 ->  71.128 ; top5 ->  89.994  and loss:  943.0746165513992
forward train acc: top1 ->  69.7421875 ; top5 ->  87.671875  and loss:  252.4972447156906
test acc: top1 ->  71.102 ; top5 ->  89.972  and loss:  944.7840824723244
forward train acc: top1 ->  70.0234375 ; top5 ->  87.9375  and loss:  247.8142510652542
test acc: top1 ->  71.162 ; top5 ->  89.932  and loss:  943.29472053051
forward train acc: top1 ->  69.609375 ; top5 ->  88.0546875  and loss:  249.47381126880646
test acc: top1 ->  71.134 ; top5 ->  89.986  and loss:  941.3482564687729
forward train acc: top1 ->  70.3671875 ; top5 ->  87.6171875  and loss:  250.21553188562393
test acc: top1 ->  71.21 ; top5 ->  89.92  and loss:  941.6916453242302
forward train acc: top1 ->  70.2265625 ; top5 ->  87.71875  and loss:  247.45632368326187
test acc: top1 ->  71.27 ; top5 ->  89.994  and loss:  940.2701397538185
>>>>>>> reverse layer  28  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  29  ---------------
adv train loss:  -62.756093978881836 , diff:  62.756093978881836
adv train loss:  -63.54273474216461 , diff:  0.7866407632827759
layer  29  adv train finish, try to retain  502
>>>>>>> reverse layer  29  since no improvement >>>>>>>
---------------- start layer  30  ---------------
adv train loss:  -62.25235438346863 , diff:  62.25235438346863
adv train loss:  -64.0322578549385 , diff:  1.7799034714698792
layer  30  adv train finish, try to retain  493
test acc: top1 ->  70.868 ; top5 ->  89.768  and loss:  951.5215717554092
forward train acc: top1 ->  69.40625 ; top5 ->  87.6875  and loss:  254.519893348217
test acc: top1 ->  70.906 ; top5 ->  89.788  and loss:  950.7310878634453
forward train acc: top1 ->  69.09375 ; top5 ->  87.3671875  and loss:  257.61669236421585
test acc: top1 ->  70.978 ; top5 ->  89.808  and loss:  949.3478906154633
forward train acc: top1 ->  70.515625 ; top5 ->  88.0234375  and loss:  247.65732032060623
test acc: top1 ->  71.076 ; top5 ->  89.808  and loss:  951.7691023349762
forward train acc: top1 ->  70.359375 ; top5 ->  87.625  and loss:  252.83568024635315
test acc: top1 ->  70.978 ; top5 ->  89.866  and loss:  945.6471340060234
forward train acc: top1 ->  70.609375 ; top5 ->  87.9140625  and loss:  250.9739054441452
test acc: top1 ->  71.004 ; top5 ->  89.876  and loss:  946.0990357995033
forward train acc: top1 ->  70.4140625 ; top5 ->  87.984375  and loss:  249.27391982078552
test acc: top1 ->  71.038 ; top5 ->  89.87  and loss:  945.4454140663147
forward train acc: top1 ->  70.4375 ; top5 ->  87.8828125  and loss:  249.47932249307632
test acc: top1 ->  71.108 ; top5 ->  89.906  and loss:  944.7701472640038
forward train acc: top1 ->  70.0625 ; top5 ->  88.203125  and loss:  247.3139972090721
test acc: top1 ->  71.068 ; top5 ->  89.938  and loss:  941.8993120193481
forward train acc: top1 ->  70.53125 ; top5 ->  88.40625  and loss:  244.405404150486
test acc: top1 ->  71.07 ; top5 ->  89.896  and loss:  942.3677738904953
forward train acc: top1 ->  71.015625 ; top5 ->  88.09375  and loss:  244.0036398768425
test acc: top1 ->  71.06 ; top5 ->  89.856  and loss:  944.4873115420341
>>>>>>> reverse layer  30  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  31  ---------------
adv train loss:  -59.33294892311096 , diff:  59.33294892311096
adv train loss:  -60.54069310426712 , diff:  1.2077441811561584
layer  31  adv train finish, try to retain  500
test acc: top1 ->  70.798 ; top5 ->  89.812  and loss:  955.2388664484024
forward train acc: top1 ->  69.875 ; top5 ->  88.1171875  and loss:  248.51480931043625
test acc: top1 ->  70.968 ; top5 ->  89.868  and loss:  951.6652401685715
forward train acc: top1 ->  69.890625 ; top5 ->  87.9609375  and loss:  248.38794219493866
test acc: top1 ->  70.888 ; top5 ->  89.824  and loss:  947.4550232887268
forward train acc: top1 ->  69.5078125 ; top5 ->  87.6015625  and loss:  258.2654467821121
test acc: top1 ->  70.948 ; top5 ->  89.88  and loss:  945.6465928554535
forward train acc: top1 ->  69.8515625 ; top5 ->  87.8046875  and loss:  252.1279102563858
test acc: top1 ->  71.0 ; top5 ->  89.86  and loss:  948.0810777544975
forward train acc: top1 ->  70.0859375 ; top5 ->  87.796875  and loss:  249.89927291870117
test acc: top1 ->  71.094 ; top5 ->  89.864  and loss:  945.0186166763306
forward train acc: top1 ->  69.875 ; top5 ->  87.734375  and loss:  254.14901101589203
test acc: top1 ->  71.126 ; top5 ->  89.816  and loss:  946.8040882349014
forward train acc: top1 ->  70.046875 ; top5 ->  87.9921875  and loss:  250.600932598114
test acc: top1 ->  71.204 ; top5 ->  89.854  and loss:  942.9183166027069
forward train acc: top1 ->  70.140625 ; top5 ->  88.09375  and loss:  249.191073179245
test acc: top1 ->  71.122 ; top5 ->  89.83  and loss:  941.0824681520462
forward train acc: top1 ->  70.078125 ; top5 ->  87.59375  and loss:  252.9069446325302
test acc: top1 ->  71.07 ; top5 ->  89.888  and loss:  943.8207996487617
forward train acc: top1 ->  69.9375 ; top5 ->  87.7265625  and loss:  252.64252597093582
test acc: top1 ->  71.112 ; top5 ->  89.924  and loss:  939.8557597994804
>>>>>>> reverse layer  31  since performance drop >>>>>>>
==> this epoch:  501 / 512 , inc:  1
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.06407226562500001, 0.03203613281250001, 0.0015016937255859378, 0.016018066406250003, 0.016018066406250003, 0.17085937500000004, 0.0007508468627929689, 0.00014078378677368167, 0.008009033203125002, 0.0007508468627929689, 0.008009033203125002, 0.0030033874511718756, 0.0030033874511718756, 0.08542968750000002, 0.00014078378677368167, 0.00018771171569824223, 0.00037542343139648445, 0.00037542343139648445, 0.00037542343139648445, 0.004004516601562501, 0.00037542343139648445, 0.00037542343139648445, 0.0015016937255859378, 0.004004516601562501, 0.00014078378677368167, 0.00014078378677368167, 7.039189338684083e-05, 3.519594669342042e-05, 7.039189338684083e-05, 0.00018771171569824223, 7.039189338684083e-05, 7.039189338684083e-05]  wait [4, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 4, 4, 2, 4, 1, 2, 2, 2, 2, 2, 2, 4, 2, 4, 4, 4, 3, 4, 2, 4, 4]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 5
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  22  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
### skip layer  1 wait:  3  ###
---------------- start layer  2  ---------------
adv train loss:  -59.93364214897156 , diff:  59.93364214897156
adv train loss:  -63.11787831783295 , diff:  3.184236168861389
************ all values are small in this layer **********
layer  2  adv train finish, try to retain  56
test acc: top1 ->  70.66 ; top5 ->  89.736  and loss:  958.6153295636177
forward train acc: top1 ->  70.0078125 ; top5 ->  87.6015625  and loss:  251.23788690567017
test acc: top1 ->  70.84 ; top5 ->  89.774  and loss:  948.9180256128311
forward train acc: top1 ->  70.0546875 ; top5 ->  88.0625  and loss:  249.49375301599503
test acc: top1 ->  70.922 ; top5 ->  89.84  and loss:  948.5825958848
forward train acc: top1 ->  69.953125 ; top5 ->  87.453125  and loss:  252.486394405365
test acc: top1 ->  70.892 ; top5 ->  89.812  and loss:  950.8375633955002
forward train acc: top1 ->  70.59375 ; top5 ->  88.0234375  and loss:  246.56600785255432
test acc: top1 ->  71.072 ; top5 ->  89.894  and loss:  947.1654678583145
forward train acc: top1 ->  70.21875 ; top5 ->  87.828125  and loss:  250.33586859703064
test acc: top1 ->  71.094 ; top5 ->  89.888  and loss:  946.9253048300743
forward train acc: top1 ->  70.09375 ; top5 ->  88.1796875  and loss:  248.97191870212555
test acc: top1 ->  71.06 ; top5 ->  89.868  and loss:  946.8234878778458
forward train acc: top1 ->  69.515625 ; top5 ->  87.4375  and loss:  255.69981062412262
test acc: top1 ->  71.204 ; top5 ->  89.916  and loss:  943.8828752040863
forward train acc: top1 ->  70.2109375 ; top5 ->  88.0390625  and loss:  251.01558804512024
test acc: top1 ->  71.202 ; top5 ->  89.924  and loss:  944.8493144512177
forward train acc: top1 ->  69.609375 ; top5 ->  87.4375  and loss:  255.17818349599838
test acc: top1 ->  71.2 ; top5 ->  89.94  and loss:  940.2280810475349
forward train acc: top1 ->  70.4296875 ; top5 ->  88.3984375  and loss:  244.24800527095795
test acc: top1 ->  71.146 ; top5 ->  89.982  and loss:  940.3809326291084
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  57 / 64 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -59.06325942277908 , diff:  59.06325942277908
adv train loss:  -62.82673621177673 , diff:  3.76347678899765
layer  3  adv train finish, try to retain  43
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -62.04602909088135 , diff:  62.04602909088135
adv train loss:  -61.45981007814407 , diff:  0.5862190127372742
layer  4  adv train finish, try to retain  31
test acc: top1 ->  68.262 ; top5 ->  88.218  and loss:  1036.6881900429726
forward train acc: top1 ->  69.328125 ; top5 ->  87.6875  and loss:  257.16471737623215
test acc: top1 ->  70.756 ; top5 ->  89.718  and loss:  954.6069549918175
forward train acc: top1 ->  69.703125 ; top5 ->  87.15625  and loss:  256.27607065439224
test acc: top1 ->  70.76 ; top5 ->  89.674  and loss:  956.1660160422325
forward train acc: top1 ->  70.6015625 ; top5 ->  87.8203125  and loss:  248.89667785167694
test acc: top1 ->  70.704 ; top5 ->  89.794  and loss:  953.8252097964287
forward train acc: top1 ->  69.859375 ; top5 ->  87.7265625  and loss:  253.9869859814644
test acc: top1 ->  70.808 ; top5 ->  89.7  and loss:  950.3328404426575
forward train acc: top1 ->  70.0390625 ; top5 ->  87.34375  and loss:  252.6062279343605
test acc: top1 ->  70.798 ; top5 ->  89.756  and loss:  951.9398446679115
forward train acc: top1 ->  70.03125 ; top5 ->  87.8359375  and loss:  251.22854244709015
test acc: top1 ->  71.004 ; top5 ->  89.826  and loss:  951.3157514333725
forward train acc: top1 ->  69.6640625 ; top5 ->  87.609375  and loss:  253.30385947227478
test acc: top1 ->  70.934 ; top5 ->  89.828  and loss:  950.3287245035172
forward train acc: top1 ->  69.9765625 ; top5 ->  88.046875  and loss:  248.29562735557556
test acc: top1 ->  71.014 ; top5 ->  89.832  and loss:  946.8388298749924
forward train acc: top1 ->  70.3515625 ; top5 ->  88.1328125  and loss:  248.4391167163849
test acc: top1 ->  71.04 ; top5 ->  89.888  and loss:  945.994572520256
forward train acc: top1 ->  71.03125 ; top5 ->  88.09375  and loss:  245.439858853817
test acc: top1 ->  71.02 ; top5 ->  89.848  and loss:  949.0424202084541
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  40 / 64 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -63.062461853027344 , diff:  63.062461853027344
adv train loss:  -62.891351759433746 , diff:  0.1711100935935974
layer  5  adv train finish, try to retain  30
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -61.721594393253326 , diff:  61.721594393253326
adv train loss:  -61.22812104225159 , diff:  0.4934733510017395
layer  6  adv train finish, try to retain  109
test acc: top1 ->  66.94 ; top5 ->  87.46  and loss:  1075.681960761547
forward train acc: top1 ->  68.9453125 ; top5 ->  87.4375  and loss:  258.89120799303055
test acc: top1 ->  70.758 ; top5 ->  89.746  and loss:  956.5588135123253
forward train acc: top1 ->  69.9765625 ; top5 ->  87.7890625  and loss:  251.83957695960999
test acc: top1 ->  70.806 ; top5 ->  89.74  and loss:  952.9861038923264
forward train acc: top1 ->  70.03125 ; top5 ->  88.34375  and loss:  246.97519451379776
test acc: top1 ->  70.878 ; top5 ->  89.73  and loss:  953.6812341809273
forward train acc: top1 ->  69.65625 ; top5 ->  87.671875  and loss:  253.32407462596893
test acc: top1 ->  70.896 ; top5 ->  89.788  and loss:  952.9226437807083
forward train acc: top1 ->  69.3671875 ; top5 ->  87.7109375  and loss:  252.96875125169754
test acc: top1 ->  70.998 ; top5 ->  89.842  and loss:  951.5283995270729
forward train acc: top1 ->  69.859375 ; top5 ->  87.6171875  and loss:  251.2213430404663
test acc: top1 ->  70.988 ; top5 ->  89.852  and loss:  950.0450178980827
forward train acc: top1 ->  69.78125 ; top5 ->  87.6484375  and loss:  250.49049884080887
test acc: top1 ->  70.968 ; top5 ->  89.868  and loss:  949.5612528920174
forward train acc: top1 ->  69.640625 ; top5 ->  87.6875  and loss:  252.72825622558594
test acc: top1 ->  71.042 ; top5 ->  89.886  and loss:  945.7877349257469
forward train acc: top1 ->  70.1875 ; top5 ->  87.7265625  and loss:  251.29234212636948
test acc: top1 ->  71.006 ; top5 ->  89.854  and loss:  945.9458722472191
forward train acc: top1 ->  70.6484375 ; top5 ->  88.0703125  and loss:  248.28218108415604
test acc: top1 ->  71.15 ; top5 ->  89.89  and loss:  946.7389666438103
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  112 / 128 , inc:  1
---------------- start layer  7  ---------------
### skip layer  7 wait:  3  ###
---------------- start layer  8  ---------------
adv train loss:  -62.853197515010834 , diff:  62.853197515010834
adv train loss:  -62.88847374916077 , diff:  0.03527623414993286
layer  8  adv train finish, try to retain  73
test acc: top1 ->  67.978 ; top5 ->  88.032  and loss:  1055.0030211806297
forward train acc: top1 ->  69.2578125 ; top5 ->  87.1953125  and loss:  261.351964533329
test acc: top1 ->  70.438 ; top5 ->  89.698  and loss:  962.5189457535744
forward train acc: top1 ->  70.1171875 ; top5 ->  87.3984375  and loss:  255.89521026611328
test acc: top1 ->  70.532 ; top5 ->  89.712  and loss:  958.5140804648399
forward train acc: top1 ->  69.453125 ; top5 ->  87.234375  and loss:  254.0640048980713
test acc: top1 ->  70.602 ; top5 ->  89.788  and loss:  959.4600201249123
forward train acc: top1 ->  69.3359375 ; top5 ->  87.640625  and loss:  254.0153683423996
test acc: top1 ->  70.71 ; top5 ->  89.786  and loss:  955.5351782441139
forward train acc: top1 ->  69.6953125 ; top5 ->  87.8046875  and loss:  252.30025720596313
test acc: top1 ->  70.852 ; top5 ->  89.8  and loss:  953.5256100296974
forward train acc: top1 ->  70.1328125 ; top5 ->  87.7578125  and loss:  251.00203973054886
test acc: top1 ->  70.784 ; top5 ->  89.796  and loss:  954.8382858037949
forward train acc: top1 ->  68.7421875 ; top5 ->  87.375  and loss:  259.8448639512062
test acc: top1 ->  70.828 ; top5 ->  89.852  and loss:  951.7530332803726
forward train acc: top1 ->  69.6171875 ; top5 ->  87.6328125  and loss:  252.9067007303238
test acc: top1 ->  70.846 ; top5 ->  89.83  and loss:  953.7413261532784
forward train acc: top1 ->  70.5625 ; top5 ->  88.203125  and loss:  245.79078829288483
test acc: top1 ->  70.798 ; top5 ->  89.822  and loss:  952.8098430037498
forward train acc: top1 ->  69.2109375 ; top5 ->  87.1328125  and loss:  256.25569242239
test acc: top1 ->  70.814 ; top5 ->  89.858  and loss:  950.7386430501938
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -64.16249197721481 , diff:  64.16249197721481
adv train loss:  -62.55167829990387 , diff:  1.6108136773109436
layer  9  adv train finish, try to retain  114
test acc: top1 ->  70.95 ; top5 ->  89.862  and loss:  945.0467305183411
forward train acc: top1 ->  69.9921875 ; top5 ->  87.546875  and loss:  255.45191770792007
test acc: top1 ->  70.832 ; top5 ->  89.848  and loss:  949.6598228812218
forward train acc: top1 ->  70.0 ; top5 ->  88.3046875  and loss:  249.58962219953537
test acc: top1 ->  70.978 ; top5 ->  89.826  and loss:  945.2148516178131
forward train acc: top1 ->  70.3359375 ; top5 ->  87.71875  and loss:  250.78431576490402
test acc: top1 ->  70.982 ; top5 ->  89.844  and loss:  942.2708818912506
forward train acc: top1 ->  70.140625 ; top5 ->  87.875  and loss:  248.7818334698677
test acc: top1 ->  71.172 ; top5 ->  89.962  and loss:  939.0008719563484
forward train acc: top1 ->  69.9140625 ; top5 ->  87.765625  and loss:  251.25339144468307
test acc: top1 ->  71.174 ; top5 ->  89.96  and loss:  941.146107673645
forward train acc: top1 ->  69.6796875 ; top5 ->  87.7109375  and loss:  258.06402337551117
test acc: top1 ->  71.14 ; top5 ->  89.888  and loss:  942.4156315922737
forward train acc: top1 ->  70.0390625 ; top5 ->  87.3984375  and loss:  252.83003395795822
test acc: top1 ->  71.256 ; top5 ->  89.922  and loss:  938.0999063253403
forward train acc: top1 ->  70.703125 ; top5 ->  88.125  and loss:  244.29037976264954
test acc: top1 ->  71.232 ; top5 ->  89.92  and loss:  941.821068584919
forward train acc: top1 ->  70.171875 ; top5 ->  87.65625  and loss:  251.72446411848068
test acc: top1 ->  71.204 ; top5 ->  89.916  and loss:  940.4134637117386
forward train acc: top1 ->  69.8046875 ; top5 ->  88.109375  and loss:  249.9143573641777
test acc: top1 ->  71.198 ; top5 ->  89.95  and loss:  938.5019327998161
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  115 / 128 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -64.14469689130783 , diff:  64.14469689130783
adv train loss:  -62.295085072517395 , diff:  1.8496118187904358
layer  10  adv train finish, try to retain  89
>>>>>>> reverse layer  10  since no improvement >>>>>>>
---------------- start layer  11  ---------------
### skip layer  11 wait:  4  ###
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
adv train loss:  -61.80037569999695 , diff:  61.80037569999695
adv train loss:  -62.70015126466751 , diff:  0.8997755646705627
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  70
test acc: top1 ->  68.054 ; top5 ->  88.166  and loss:  1049.992167711258
forward train acc: top1 ->  68.8203125 ; top5 ->  87.3359375  and loss:  258.8490527868271
test acc: top1 ->  70.684 ; top5 ->  89.668  and loss:  961.2005352973938
forward train acc: top1 ->  69.71875 ; top5 ->  88.0859375  and loss:  250.24157243967056
test acc: top1 ->  70.834 ; top5 ->  89.658  and loss:  956.8423348069191
forward train acc: top1 ->  69.4296875 ; top5 ->  87.5546875  and loss:  256.891141474247
test acc: top1 ->  70.772 ; top5 ->  89.748  and loss:  957.5855875611305
forward train acc: top1 ->  68.765625 ; top5 ->  87.25  and loss:  260.6413562297821
test acc: top1 ->  70.848 ; top5 ->  89.72  and loss:  954.7351177334785
forward train acc: top1 ->  70.09375 ; top5 ->  87.796875  and loss:  252.85057938098907
test acc: top1 ->  70.914 ; top5 ->  89.782  and loss:  951.1136447787285
forward train acc: top1 ->  68.890625 ; top5 ->  87.515625  and loss:  260.5937730073929
test acc: top1 ->  70.742 ; top5 ->  89.846  and loss:  950.3174685239792
forward train acc: top1 ->  70.53125 ; top5 ->  88.3046875  and loss:  248.37082904577255
test acc: top1 ->  70.906 ; top5 ->  89.824  and loss:  949.1568027734756
forward train acc: top1 ->  69.7265625 ; top5 ->  87.8984375  and loss:  252.79055505990982
test acc: top1 ->  70.93 ; top5 ->  89.862  and loss:  947.9776458740234
forward train acc: top1 ->  69.6796875 ; top5 ->  87.65625  and loss:  251.9875584244728
test acc: top1 ->  70.97 ; top5 ->  89.866  and loss:  949.1362842917442
forward train acc: top1 ->  69.75 ; top5 ->  87.8046875  and loss:  252.07463467121124
test acc: top1 ->  70.978 ; top5 ->  89.804  and loss:  946.4989938139915
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  71 / 128 , inc:  1
---------------- start layer  14  ---------------
### skip layer  14 wait:  4  ###
---------------- start layer  15  ---------------
adv train loss:  -62.453453540802 , diff:  62.453453540802
adv train loss:  -63.87915760278702 , diff:  1.4257040619850159
layer  15  adv train finish, try to retain  244
test acc: top1 ->  70.182 ; top5 ->  89.362  and loss:  968.3842015862465
forward train acc: top1 ->  69.359375 ; top5 ->  87.515625  and loss:  255.6994313597679
test acc: top1 ->  70.878 ; top5 ->  89.758  and loss:  951.2705801725388
forward train acc: top1 ->  70.0546875 ; top5 ->  87.90625  and loss:  250.18301981687546
test acc: top1 ->  70.968 ; top5 ->  89.812  and loss:  949.0657137036324
forward train acc: top1 ->  69.6953125 ; top5 ->  87.890625  and loss:  253.9006089568138
test acc: top1 ->  71.07 ; top5 ->  89.822  and loss:  950.1443264484406
forward train acc: top1 ->  69.5390625 ; top5 ->  87.5859375  and loss:  254.19137942790985
test acc: top1 ->  70.964 ; top5 ->  89.868  and loss:  948.5731739401817
forward train acc: top1 ->  69.7421875 ; top5 ->  87.5625  and loss:  254.74251979589462
test acc: top1 ->  71.074 ; top5 ->  89.834  and loss:  947.7044909596443
forward train acc: top1 ->  69.5703125 ; top5 ->  87.90625  and loss:  253.8994711637497
test acc: top1 ->  71.098 ; top5 ->  89.842  and loss:  948.2992477416992
forward train acc: top1 ->  70.1484375 ; top5 ->  87.625  and loss:  252.1297089457512
test acc: top1 ->  71.188 ; top5 ->  89.88  and loss:  945.6577799916267
forward train acc: top1 ->  69.46875 ; top5 ->  87.40625  and loss:  254.7741061449051
test acc: top1 ->  71.154 ; top5 ->  89.908  and loss:  943.5842605829239
forward train acc: top1 ->  70.4296875 ; top5 ->  88.0625  and loss:  247.31525617837906
test acc: top1 ->  71.294 ; top5 ->  89.952  and loss:  940.9616932272911
forward train acc: top1 ->  69.84375 ; top5 ->  87.8359375  and loss:  248.90144848823547
test acc: top1 ->  71.274 ; top5 ->  89.9  and loss:  943.4476609826088
>>>>>>> reverse layer  15  since performance drop >>>>>>>
==> this epoch:  248 / 256 , inc:  1
---------------- start layer  16  ---------------
adv train loss:  -64.7124919295311 , diff:  64.7124919295311
adv train loss:  -63.141172647476196 , diff:  1.5713192820549011
layer  16  adv train finish, try to retain  234
test acc: top1 ->  71.138 ; top5 ->  89.992  and loss:  946.4007241129875
forward train acc: top1 ->  70.7578125 ; top5 ->  88.1328125  and loss:  244.55877697467804
test acc: top1 ->  71.058 ; top5 ->  89.98  and loss:  948.0490440130234
forward train acc: top1 ->  69.3515625 ; top5 ->  87.890625  and loss:  251.55895978212357
test acc: top1 ->  70.84 ; top5 ->  89.888  and loss:  950.6456909179688
forward train acc: top1 ->  70.2265625 ; top5 ->  87.828125  and loss:  253.49470728635788
test acc: top1 ->  70.95 ; top5 ->  89.882  and loss:  947.4883266687393
forward train acc: top1 ->  68.765625 ; top5 ->  86.9296875  and loss:  260.054023206234
test acc: top1 ->  71.148 ; top5 ->  89.992  and loss:  943.8635945320129
forward train acc: top1 ->  69.796875 ; top5 ->  87.9140625  and loss:  250.43114668130875
test acc: top1 ->  71.062 ; top5 ->  90.034  and loss:  943.261131644249
forward train acc: top1 ->  70.0078125 ; top5 ->  88.109375  and loss:  250.59511637687683
test acc: top1 ->  71.126 ; top5 ->  89.982  and loss:  942.2405822277069
forward train acc: top1 ->  70.421875 ; top5 ->  87.7734375  and loss:  249.48136049509048
test acc: top1 ->  71.154 ; top5 ->  90.05  and loss:  942.8228806257248
forward train acc: top1 ->  70.578125 ; top5 ->  88.4375  and loss:  244.02125537395477
test acc: top1 ->  71.142 ; top5 ->  90.008  and loss:  940.3241406083107
forward train acc: top1 ->  70.2421875 ; top5 ->  88.171875  and loss:  247.06000131368637
test acc: top1 ->  71.18 ; top5 ->  90.014  and loss:  943.3276708126068
forward train acc: top1 ->  69.953125 ; top5 ->  87.7109375  and loss:  249.757425904274
test acc: top1 ->  71.198 ; top5 ->  90.03  and loss:  941.9281994104385
>>>>>>> reverse layer  16  since performance drop >>>>>>>
==> this epoch:  239 / 256 , inc:  1
---------------- start layer  17  ---------------
adv train loss:  -60.27140820026398 , diff:  60.27140820026398
adv train loss:  -63.442542135715485 , diff:  3.1711339354515076
layer  17  adv train finish, try to retain  235
>>>>>>> reverse layer  17  since no improvement >>>>>>>
---------------- start layer  18  ---------------
adv train loss:  -61.89578539133072 , diff:  61.89578539133072
adv train loss:  -64.41797411441803 , diff:  2.522188723087311
layer  18  adv train finish, try to retain  234
test acc: top1 ->  71.032 ; top5 ->  90.042  and loss:  941.8318569660187
forward train acc: top1 ->  69.953125 ; top5 ->  87.625  and loss:  253.6854631304741
test acc: top1 ->  71.078 ; top5 ->  89.946  and loss:  944.4480094909668
forward train acc: top1 ->  69.5546875 ; top5 ->  87.6953125  and loss:  252.31535601615906
test acc: top1 ->  71.188 ; top5 ->  90.008  and loss:  942.9027378559113
forward train acc: top1 ->  70.2890625 ; top5 ->  87.9765625  and loss:  248.55833303928375
test acc: top1 ->  70.94 ; top5 ->  89.958  and loss:  944.2708932757378
forward train acc: top1 ->  70.171875 ; top5 ->  87.640625  and loss:  251.1624169945717
test acc: top1 ->  71.118 ; top5 ->  90.034  and loss:  944.0109777450562
forward train acc: top1 ->  70.3984375 ; top5 ->  87.8203125  and loss:  248.8103238940239
test acc: top1 ->  71.048 ; top5 ->  90.032  and loss:  943.3119032979012
forward train acc: top1 ->  70.3984375 ; top5 ->  88.0703125  and loss:  248.99360144138336
test acc: top1 ->  71.21 ; top5 ->  90.064  and loss:  939.7404760122299
forward train acc: top1 ->  70.5 ; top5 ->  88.1796875  and loss:  246.89781719446182
test acc: top1 ->  71.286 ; top5 ->  90.028  and loss:  938.7667383551598
forward train acc: top1 ->  69.640625 ; top5 ->  87.6875  and loss:  251.98308312892914
test acc: top1 ->  71.186 ; top5 ->  90.024  and loss:  941.5966526865959
forward train acc: top1 ->  70.703125 ; top5 ->  87.765625  and loss:  249.0467602610588
test acc: top1 ->  71.26 ; top5 ->  90.06  and loss:  942.582858979702
forward train acc: top1 ->  70.3125 ; top5 ->  88.2578125  and loss:  245.81528055667877
test acc: top1 ->  71.178 ; top5 ->  90.018  and loss:  938.4018359184265
>>>>>>> reverse layer  18  since performance drop >>>>>>>
==> this epoch:  235 / 256 , inc:  1
---------------- start layer  19  ---------------
adv train loss:  -64.52022701501846 , diff:  64.52022701501846
adv train loss:  -63.36013853549957 , diff:  1.1600884795188904
layer  19  adv train finish, try to retain  179
>>>>>>> reverse layer  19  since no improvement >>>>>>>
---------------- start layer  20  ---------------
adv train loss:  -59.74857580661774 , diff:  59.74857580661774
adv train loss:  -60.09662526845932 , diff:  0.34804946184158325
layer  20  adv train finish, try to retain  232
>>>>>>> reverse layer  20  since no improvement >>>>>>>
---------------- start layer  21  ---------------
adv train loss:  -63.819482266902924 , diff:  63.819482266902924
adv train loss:  -63.3239888548851 , diff:  0.49549341201782227
layer  21  adv train finish, try to retain  234
test acc: top1 ->  71.048 ; top5 ->  89.964  and loss:  945.0286419391632
forward train acc: top1 ->  70.203125 ; top5 ->  87.71875  and loss:  250.54855287075043
test acc: top1 ->  71.086 ; top5 ->  89.95  and loss:  947.124117732048
forward train acc: top1 ->  70.0625 ; top5 ->  87.796875  and loss:  250.59652572870255
test acc: top1 ->  71.1 ; top5 ->  90.062  and loss:  946.0225180983543
forward train acc: top1 ->  70.15625 ; top5 ->  88.1640625  and loss:  248.35061651468277
test acc: top1 ->  71.076 ; top5 ->  90.052  and loss:  946.0037706494331
forward train acc: top1 ->  70.140625 ; top5 ->  87.765625  and loss:  252.89050596952438
test acc: top1 ->  71.138 ; top5 ->  90.026  and loss:  942.9941703081131
forward train acc: top1 ->  70.15625 ; top5 ->  87.6640625  and loss:  248.94087678194046
test acc: top1 ->  71.262 ; top5 ->  90.014  and loss:  939.6597275137901
forward train acc: top1 ->  70.171875 ; top5 ->  87.9921875  and loss:  250.0824047923088
test acc: top1 ->  71.246 ; top5 ->  90.074  and loss:  937.6230709552765
forward train acc: top1 ->  70.046875 ; top5 ->  87.5390625  and loss:  254.63731759786606
test acc: top1 ->  71.246 ; top5 ->  90.106  and loss:  940.2677723765373
forward train acc: top1 ->  69.90625 ; top5 ->  87.9296875  and loss:  251.54458463191986
test acc: top1 ->  71.232 ; top5 ->  90.086  and loss:  939.2935926318169
forward train acc: top1 ->  70.1953125 ; top5 ->  88.015625  and loss:  250.64290636777878
test acc: top1 ->  71.296 ; top5 ->  90.056  and loss:  939.6782922148705
forward train acc: top1 ->  69.625 ; top5 ->  87.796875  and loss:  257.2245341539383
test acc: top1 ->  71.232 ; top5 ->  90.072  and loss:  938.9747586250305
>>>>>>> reverse layer  21  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  22  ---------------
### skip layer  22 wait:  4  ###
---------------- start layer  23  ---------------
adv train loss:  -65.89675217866898 , diff:  65.89675217866898
adv train loss:  -58.70584982633591 , diff:  7.190902352333069
layer  23  adv train finish, try to retain  210
>>>>>>> reverse layer  23  since no improvement >>>>>>>
---------------- start layer  24  ---------------
### skip layer  24 wait:  4  ###
---------------- start layer  25  ---------------
### skip layer  25 wait:  4  ###
---------------- start layer  26  ---------------
### skip layer  26 wait:  4  ###
---------------- start layer  27  ---------------
### skip layer  27 wait:  3  ###
---------------- start layer  28  ---------------
### skip layer  28 wait:  4  ###
---------------- start layer  29  ---------------
adv train loss:  -62.93335002660751 , diff:  62.93335002660751
adv train loss:  -62.601855516433716 , diff:  0.3314945101737976
layer  29  adv train finish, try to retain  488
test acc: top1 ->  71.014 ; top5 ->  89.972  and loss:  961.1474512815475
forward train acc: top1 ->  69.8671875 ; top5 ->  87.3125  and loss:  255.2664526104927
test acc: top1 ->  70.888 ; top5 ->  89.976  and loss:  945.6598982214928
forward train acc: top1 ->  70.265625 ; top5 ->  88.015625  and loss:  249.86434280872345
test acc: top1 ->  71.01 ; top5 ->  89.968  and loss:  944.1562500596046
forward train acc: top1 ->  69.671875 ; top5 ->  87.515625  and loss:  255.61680817604065
test acc: top1 ->  70.894 ; top5 ->  89.844  and loss:  945.9304202198982
forward train acc: top1 ->  70.0625 ; top5 ->  87.4609375  and loss:  255.80365747213364
test acc: top1 ->  71.06 ; top5 ->  89.952  and loss:  942.5005939006805
forward train acc: top1 ->  70.328125 ; top5 ->  88.3125  and loss:  246.37795639038086
test acc: top1 ->  71.096 ; top5 ->  89.996  and loss:  943.5388613939285
forward train acc: top1 ->  70.390625 ; top5 ->  88.140625  and loss:  247.1645542383194
test acc: top1 ->  71.196 ; top5 ->  89.966  and loss:  944.1768738627434
forward train acc: top1 ->  70.484375 ; top5 ->  87.953125  and loss:  246.1933138370514
test acc: top1 ->  71.196 ; top5 ->  90.048  and loss:  940.2325900197029
forward train acc: top1 ->  70.53125 ; top5 ->  88.0859375  and loss:  248.99155229330063
test acc: top1 ->  71.25 ; top5 ->  90.016  and loss:  939.3353807330132
forward train acc: top1 ->  70.265625 ; top5 ->  88.0859375  and loss:  249.78394615650177
test acc: top1 ->  71.224 ; top5 ->  90.07  and loss:  941.1392900943756
forward train acc: top1 ->  69.25 ; top5 ->  87.2265625  and loss:  256.22432762384415
test acc: top1 ->  71.282 ; top5 ->  90.082  and loss:  937.3912587165833
>>>>>>> reverse layer  29  since performance drop >>>>>>>
==> this epoch:  498 / 512 , inc:  1
---------------- start layer  30  ---------------
### skip layer  30 wait:  4  ###
---------------- start layer  31  ---------------
### skip layer  31 wait:  4  ###
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.06407226562500001, 0.03203613281250001, 0.0011262702941894534, 0.03203613281250001, 0.012013549804687502, 0.34171875000000007, 0.0005631351470947267, 0.00014078378677368167, 0.006006774902343751, 0.0005631351470947267, 0.016018066406250003, 0.0030033874511718756, 0.0030033874511718756, 0.06407226562500001, 0.00014078378677368167, 0.00014078378677368167, 0.00028156757354736334, 0.0007508468627929689, 0.00028156757354736334, 0.008009033203125002, 0.0007508468627929689, 0.00028156757354736334, 0.0015016937255859378, 0.008009033203125002, 0.00014078378677368167, 0.00014078378677368167, 7.039189338684083e-05, 3.519594669342042e-05, 7.039189338684083e-05, 0.00014078378677368167, 7.039189338684083e-05, 7.039189338684083e-05]  wait [3, 2, 4, 2, 4, 2, 4, 2, 4, 4, 2, 3, 3, 4, 3, 3, 4, 2, 4, 2, 2, 4, 3, 2, 3, 3, 3, 2, 3, 4, 3, 3]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 5
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  23  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -64.12884652614594 , diff:  64.12884652614594
adv train loss:  -65.01154220104218 , diff:  0.8826956748962402
************ all values are small in this layer **********
layer  0  adv train finish, try to retain  36
test acc: top1 ->  64.804 ; top5 ->  85.638  and loss:  1209.3405722379684
forward train acc: top1 ->  69.578125 ; top5 ->  87.6328125  and loss:  257.0302094221115
test acc: top1 ->  70.552 ; top5 ->  89.558  and loss:  959.1775306463242
forward train acc: top1 ->  69.359375 ; top5 ->  87.1953125  and loss:  259.27254754304886
test acc: top1 ->  70.712 ; top5 ->  89.718  and loss:  955.1535258293152
forward train acc: top1 ->  70.4921875 ; top5 ->  88.0078125  and loss:  250.31207996606827
test acc: top1 ->  70.778 ; top5 ->  89.77  and loss:  953.6429577469826
forward train acc: top1 ->  69.484375 ; top5 ->  87.515625  and loss:  253.08765417337418
test acc: top1 ->  70.88 ; top5 ->  89.844  and loss:  949.1344864368439
forward train acc: top1 ->  70.09375 ; top5 ->  87.640625  and loss:  253.78132981061935
test acc: top1 ->  70.88 ; top5 ->  89.808  and loss:  948.3462478518486
forward train acc: top1 ->  70.46875 ; top5 ->  87.90625  and loss:  248.60617488622665
test acc: top1 ->  70.892 ; top5 ->  89.854  and loss:  949.3230557441711
forward train acc: top1 ->  69.890625 ; top5 ->  87.8125  and loss:  253.55938827991486
test acc: top1 ->  70.826 ; top5 ->  89.796  and loss:  950.1076806783676
forward train acc: top1 ->  69.3828125 ; top5 ->  87.296875  and loss:  255.78653693199158
test acc: top1 ->  70.932 ; top5 ->  89.798  and loss:  944.8300146460533
forward train acc: top1 ->  69.5390625 ; top5 ->  87.8203125  and loss:  251.50814646482468
test acc: top1 ->  70.928 ; top5 ->  89.914  and loss:  945.875218629837
forward train acc: top1 ->  70.359375 ; top5 ->  87.96875  and loss:  247.70095992088318
test acc: top1 ->  71.066 ; top5 ->  89.926  and loss:  943.1236258149147
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  37 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -64.042633831501 , diff:  64.042633831501
adv train loss:  -66.6987037062645 , diff:  2.6560698747634888
layer  1  adv train finish, try to retain  48
>>>>>>> reverse layer  1  since no improvement >>>>>>>
---------------- start layer  2  ---------------
adv train loss:  -64.91666454076767 , diff:  64.91666454076767
adv train loss:  -63.73056346178055 , diff:  1.1861010789871216
layer  2  adv train finish, try to retain  55
test acc: top1 ->  69.596 ; top5 ->  89.128  and loss:  984.2544203996658
forward train acc: top1 ->  70.2890625 ; top5 ->  87.796875  and loss:  249.4190440773964
test acc: top1 ->  70.884 ; top5 ->  89.808  and loss:  951.1912875771523
forward train acc: top1 ->  70.3203125 ; top5 ->  87.6875  and loss:  250.11449497938156
test acc: top1 ->  70.93 ; top5 ->  89.842  and loss:  946.2491700053215
forward train acc: top1 ->  70.546875 ; top5 ->  87.7421875  and loss:  248.47668439149857
test acc: top1 ->  71.01 ; top5 ->  89.92  and loss:  944.8829606175423
forward train acc: top1 ->  70.1328125 ; top5 ->  87.859375  and loss:  247.94217026233673
test acc: top1 ->  70.996 ; top5 ->  89.908  and loss:  943.0332868099213
forward train acc: top1 ->  70.1328125 ; top5 ->  88.0703125  and loss:  250.24989122152328
test acc: top1 ->  71.17 ; top5 ->  89.956  and loss:  946.0292196273804
forward train acc: top1 ->  70.5859375 ; top5 ->  87.75  and loss:  247.99440336227417
test acc: top1 ->  71.224 ; top5 ->  90.0  and loss:  943.6288482546806
forward train acc: top1 ->  69.6953125 ; top5 ->  88.03125  and loss:  251.151036798954
test acc: top1 ->  71.184 ; top5 ->  89.934  and loss:  943.0793964266777
forward train acc: top1 ->  70.3984375 ; top5 ->  87.6796875  and loss:  248.57418078184128
test acc: top1 ->  71.134 ; top5 ->  90.056  and loss:  942.0578366518021
forward train acc: top1 ->  69.2890625 ; top5 ->  87.0546875  and loss:  256.8586212992668
test acc: top1 ->  71.134 ; top5 ->  89.942  and loss:  942.6271926760674
forward train acc: top1 ->  69.6953125 ; top5 ->  87.6875  and loss:  252.17675405740738
test acc: top1 ->  71.182 ; top5 ->  90.014  and loss:  940.5651131272316
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  57 / 64 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -59.96131503582001 , diff:  59.96131503582001
adv train loss:  -60.81859362125397 , diff:  0.85727858543396
layer  3  adv train finish, try to retain  44
>>>>>>> reverse layer  3  since no improvement >>>>>>>
---------------- start layer  4  ---------------
adv train loss:  -63.521776139736176 , diff:  63.521776139736176
adv train loss:  -63.50310689210892 , diff:  0.0186692476272583
layer  4  adv train finish, try to retain  31
test acc: top1 ->  68.376 ; top5 ->  88.398  and loss:  1033.5006783604622
forward train acc: top1 ->  69.8671875 ; top5 ->  88.2734375  and loss:  245.5818383693695
test acc: top1 ->  70.734 ; top5 ->  89.844  and loss:  955.2468101382256
forward train acc: top1 ->  69.4140625 ; top5 ->  87.8046875  and loss:  253.42064595222473
test acc: top1 ->  70.83 ; top5 ->  89.864  and loss:  950.6262696385384
forward train acc: top1 ->  69.9140625 ; top5 ->  87.984375  and loss:  251.02847200632095
test acc: top1 ->  70.824 ; top5 ->  89.81  and loss:  952.2369105219841
forward train acc: top1 ->  69.7578125 ; top5 ->  87.2734375  and loss:  257.91090482473373
test acc: top1 ->  70.964 ; top5 ->  89.904  and loss:  950.6575633883476
forward train acc: top1 ->  69.1171875 ; top5 ->  87.8125  and loss:  254.21691346168518
test acc: top1 ->  70.938 ; top5 ->  89.904  and loss:  949.2472813725471
forward train acc: top1 ->  70.140625 ; top5 ->  87.6328125  and loss:  248.65018713474274
test acc: top1 ->  70.968 ; top5 ->  89.876  and loss:  947.811752140522
forward train acc: top1 ->  69.6015625 ; top5 ->  87.4140625  and loss:  256.57401901483536
test acc: top1 ->  71.054 ; top5 ->  89.91  and loss:  947.2688101530075
forward train acc: top1 ->  70.1796875 ; top5 ->  88.2109375  and loss:  247.0520822405815
test acc: top1 ->  71.086 ; top5 ->  89.98  and loss:  946.8924135565758
forward train acc: top1 ->  69.609375 ; top5 ->  87.5078125  and loss:  255.17407166957855
test acc: top1 ->  71.084 ; top5 ->  89.952  and loss:  944.8492996096611
forward train acc: top1 ->  70.3828125 ; top5 ->  88.234375  and loss:  245.64993822574615
test acc: top1 ->  71.106 ; top5 ->  89.914  and loss:  944.2689859867096
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  40 / 64 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -64.27808833122253 , diff:  64.27808833122253
adv train loss:  -64.31358790397644 , diff:  0.03549957275390625
layer  5  adv train finish, try to retain  6
test acc: top1 ->  63.43 ; top5 ->  84.99  and loss:  1218.8823494911194
forward train acc: top1 ->  69.03125 ; top5 ->  87.2734375  and loss:  258.1666975617409
test acc: top1 ->  70.688 ; top5 ->  89.62  and loss:  963.2820469141006
forward train acc: top1 ->  69.0 ; top5 ->  87.296875  and loss:  257.01034289598465
test acc: top1 ->  70.678 ; top5 ->  89.636  and loss:  962.6615228056908
forward train acc: top1 ->  69.3671875 ; top5 ->  87.390625  and loss:  254.94420444965363
test acc: top1 ->  70.556 ; top5 ->  89.632  and loss:  963.4434254169464
forward train acc: top1 ->  69.65625 ; top5 ->  87.59375  and loss:  254.70583993196487
test acc: top1 ->  70.714 ; top5 ->  89.72  and loss:  956.8410764336586
forward train acc: top1 ->  69.734375 ; top5 ->  87.515625  and loss:  253.10329097509384
test acc: top1 ->  70.776 ; top5 ->  89.798  and loss:  956.0188174843788
forward train acc: top1 ->  69.2578125 ; top5 ->  87.140625  and loss:  260.1024239063263
test acc: top1 ->  70.616 ; top5 ->  89.784  and loss:  954.9090778827667
forward train acc: top1 ->  69.515625 ; top5 ->  87.671875  and loss:  253.53303706645966
test acc: top1 ->  70.75 ; top5 ->  89.84  and loss:  952.8903667926788
forward train acc: top1 ->  69.859375 ; top5 ->  87.84375  and loss:  255.09468984603882
test acc: top1 ->  70.752 ; top5 ->  89.838  and loss:  952.8703776597977
forward train acc: top1 ->  69.9296875 ; top5 ->  87.859375  and loss:  251.9557145833969
test acc: top1 ->  70.9 ; top5 ->  89.906  and loss:  948.962405025959
forward train acc: top1 ->  69.4609375 ; top5 ->  87.9609375  and loss:  252.7132939696312
test acc: top1 ->  70.892 ; top5 ->  89.88  and loss:  950.7686480283737
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  29 / 64 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -65.48770648241043 , diff:  65.48770648241043
adv train loss:  -60.31277686357498 , diff:  5.174929618835449
layer  6  adv train finish, try to retain  110
test acc: top1 ->  68.168 ; top5 ->  88.3  and loss:  1044.1204602122307
forward train acc: top1 ->  69.8515625 ; top5 ->  87.8828125  and loss:  251.20389717817307
test acc: top1 ->  70.896 ; top5 ->  89.944  and loss:  950.0298717617989
forward train acc: top1 ->  69.8515625 ; top5 ->  87.40625  and loss:  252.85910201072693
test acc: top1 ->  70.882 ; top5 ->  89.92  and loss:  947.418409049511
forward train acc: top1 ->  69.8671875 ; top5 ->  87.453125  and loss:  253.98968923091888
test acc: top1 ->  70.904 ; top5 ->  89.94  and loss:  948.3775578141212
forward train acc: top1 ->  69.859375 ; top5 ->  87.75  and loss:  250.09308636188507
test acc: top1 ->  71.046 ; top5 ->  89.922  and loss:  946.3404490351677
forward train acc: top1 ->  70.125 ; top5 ->  88.0625  and loss:  247.21743136644363
test acc: top1 ->  71.016 ; top5 ->  89.988  and loss:  947.8866241574287
forward train acc: top1 ->  70.4453125 ; top5 ->  87.9609375  and loss:  248.56104230880737
test acc: top1 ->  71.142 ; top5 ->  90.014  and loss:  942.8348987102509
forward train acc: top1 ->  70.5390625 ; top5 ->  87.7421875  and loss:  250.66025757789612
test acc: top1 ->  71.162 ; top5 ->  89.954  and loss:  942.3353654742241
forward train acc: top1 ->  70.0234375 ; top5 ->  87.4921875  and loss:  252.87505388259888
test acc: top1 ->  71.136 ; top5 ->  89.998  and loss:  943.1160047650337
forward train acc: top1 ->  69.9375 ; top5 ->  87.4375  and loss:  254.73650354146957
test acc: top1 ->  71.158 ; top5 ->  90.0  and loss:  940.961179971695
forward train acc: top1 ->  69.8671875 ; top5 ->  87.7421875  and loss:  249.76602160930634
test acc: top1 ->  71.202 ; top5 ->  90.022  and loss:  940.6471713781357
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  112 / 128 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -64.53327733278275 , diff:  64.53327733278275
adv train loss:  -61.86080586910248 , diff:  2.6724714636802673
layer  7  adv train finish, try to retain  122
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -60.9953333735466 , diff:  60.9953333735466
adv train loss:  -62.22254294157028 , diff:  1.2272095680236816
layer  8  adv train finish, try to retain  94
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -62.12462478876114 , diff:  62.12462478876114
adv train loss:  -62.87841385602951 , diff:  0.7537890672683716
layer  9  adv train finish, try to retain  109
test acc: top1 ->  70.262 ; top5 ->  89.314  and loss:  972.224215567112
forward train acc: top1 ->  69.640625 ; top5 ->  87.59375  and loss:  253.94310623407364
test acc: top1 ->  70.924 ; top5 ->  89.804  and loss:  954.7538627386093
forward train acc: top1 ->  69.625 ; top5 ->  87.1953125  and loss:  256.2251043319702
test acc: top1 ->  70.934 ; top5 ->  89.91  and loss:  946.5606693029404
forward train acc: top1 ->  69.5234375 ; top5 ->  87.734375  and loss:  256.11170095205307
test acc: top1 ->  70.986 ; top5 ->  89.882  and loss:  946.712230682373
forward train acc: top1 ->  69.515625 ; top5 ->  87.2265625  and loss:  255.03013563156128
test acc: top1 ->  71.01 ; top5 ->  89.944  and loss:  947.6178270578384
forward train acc: top1 ->  69.7734375 ; top5 ->  87.5  and loss:  254.1980961561203
test acc: top1 ->  71.096 ; top5 ->  89.942  and loss:  943.7311043739319
forward train acc: top1 ->  69.9375 ; top5 ->  87.6875  and loss:  252.7983433008194
test acc: top1 ->  71.12 ; top5 ->  89.93  and loss:  943.2437855601311
forward train acc: top1 ->  70.0390625 ; top5 ->  87.6171875  and loss:  251.49227863550186
test acc: top1 ->  71.212 ; top5 ->  89.984  and loss:  941.7021690607071
forward train acc: top1 ->  69.484375 ; top5 ->  87.1640625  and loss:  256.99774956703186
test acc: top1 ->  71.222 ; top5 ->  89.96  and loss:  941.711977481842
forward train acc: top1 ->  70.015625 ; top5 ->  87.5234375  and loss:  252.98672777414322
test acc: top1 ->  71.202 ; top5 ->  90.016  and loss:  940.2147535085678
forward train acc: top1 ->  70.7421875 ; top5 ->  88.1796875  and loss:  244.84711509943008
test acc: top1 ->  71.244 ; top5 ->  89.958  and loss:  941.0049000382423
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  115 / 128 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -60.94940233230591 , diff:  60.94940233230591
adv train loss:  -63.14550018310547 , diff:  2.1960978507995605
layer  10  adv train finish, try to retain  82
test acc: top1 ->  69.676 ; top5 ->  89.17  and loss:  998.2546924352646
forward train acc: top1 ->  69.640625 ; top5 ->  87.3515625  and loss:  255.2036645412445
test acc: top1 ->  70.726 ; top5 ->  89.758  and loss:  950.3306466937065
forward train acc: top1 ->  70.203125 ; top5 ->  88.0625  and loss:  247.6432256102562
test acc: top1 ->  70.842 ; top5 ->  89.874  and loss:  951.0423552393913
forward train acc: top1 ->  69.3359375 ; top5 ->  87.03125  and loss:  258.05334520339966
test acc: top1 ->  70.854 ; top5 ->  89.732  and loss:  954.6147322058678
forward train acc: top1 ->  69.6015625 ; top5 ->  87.7421875  and loss:  251.32681810855865
test acc: top1 ->  70.874 ; top5 ->  89.824  and loss:  950.5492060184479
forward train acc: top1 ->  69.4609375 ; top5 ->  87.3046875  and loss:  258.4269296526909
test acc: top1 ->  71.022 ; top5 ->  89.892  and loss:  946.624061703682
forward train acc: top1 ->  69.828125 ; top5 ->  87.953125  and loss:  251.05569905042648
test acc: top1 ->  70.942 ; top5 ->  89.824  and loss:  950.7949845194817
forward train acc: top1 ->  69.4453125 ; top5 ->  87.375  and loss:  254.43754923343658
test acc: top1 ->  70.974 ; top5 ->  89.906  and loss:  946.4191910624504
forward train acc: top1 ->  70.3203125 ; top5 ->  88.1796875  and loss:  245.10697948932648
test acc: top1 ->  71.01 ; top5 ->  89.938  and loss:  945.8880814909935
forward train acc: top1 ->  70.5546875 ; top5 ->  88.234375  and loss:  247.60796850919724
test acc: top1 ->  71.0 ; top5 ->  89.904  and loss:  948.1662895083427
forward train acc: top1 ->  70.1484375 ; top5 ->  88.171875  and loss:  251.63563615083694
test acc: top1 ->  71.064 ; top5 ->  89.92  and loss:  946.3921266198158
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -62.488960564136505 , diff:  62.488960564136505
adv train loss:  -57.501444876194 , diff:  4.987515687942505
************ all values are small in this layer **********
layer  11  adv train finish, try to retain  99
test acc: top1 ->  69.872 ; top5 ->  89.252  and loss:  986.8263673186302
forward train acc: top1 ->  70.25 ; top5 ->  87.9921875  and loss:  249.51942592859268
test acc: top1 ->  71.004 ; top5 ->  89.884  and loss:  948.1502339243889
forward train acc: top1 ->  70.21875 ; top5 ->  88.34375  and loss:  246.67462772130966
test acc: top1 ->  71.032 ; top5 ->  89.894  and loss:  949.8511568903923
forward train acc: top1 ->  69.65625 ; top5 ->  87.5703125  and loss:  255.63860297203064
test acc: top1 ->  70.88 ; top5 ->  89.844  and loss:  949.3391997218132
forward train acc: top1 ->  69.5546875 ; top5 ->  87.3828125  and loss:  253.8250303864479
test acc: top1 ->  71.026 ; top5 ->  89.852  and loss:  946.4164752960205
forward train acc: top1 ->  70.7734375 ; top5 ->  87.890625  and loss:  249.98484009504318
test acc: top1 ->  71.118 ; top5 ->  89.876  and loss:  945.4659015536308
forward train acc: top1 ->  70.21875 ; top5 ->  87.8359375  and loss:  253.35946416854858
test acc: top1 ->  71.142 ; top5 ->  89.896  and loss:  941.524985730648
forward train acc: top1 ->  70.8359375 ; top5 ->  87.9375  and loss:  246.24264240264893
test acc: top1 ->  71.164 ; top5 ->  89.932  and loss:  939.7918453812599
forward train acc: top1 ->  70.0078125 ; top5 ->  87.9375  and loss:  248.74257093667984
test acc: top1 ->  71.098 ; top5 ->  89.886  and loss:  941.2705530524254
forward train acc: top1 ->  70.265625 ; top5 ->  87.90625  and loss:  249.36067748069763
test acc: top1 ->  71.142 ; top5 ->  89.944  and loss:  940.3015342354774
forward train acc: top1 ->  70.625 ; top5 ->  88.34375  and loss:  243.0942450761795
test acc: top1 ->  71.22 ; top5 ->  89.902  and loss:  940.7888875007629
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -62.18894273042679 , diff:  62.18894273042679
adv train loss:  -62.92882174253464 , diff:  0.7398790121078491
************ all values are small in this layer **********
layer  12  adv train finish, try to retain  97
test acc: top1 ->  70.578 ; top5 ->  89.544  and loss:  968.3243406414986
forward train acc: top1 ->  70.1015625 ; top5 ->  88.0625  and loss:  250.47465074062347
test acc: top1 ->  71.16 ; top5 ->  89.928  and loss:  946.5793700814247
forward train acc: top1 ->  69.8984375 ; top5 ->  88.125  and loss:  251.9015809893608
test acc: top1 ->  70.982 ; top5 ->  89.954  and loss:  946.8990331888199
forward train acc: top1 ->  70.2890625 ; top5 ->  87.8203125  and loss:  253.28070509433746
test acc: top1 ->  71.148 ; top5 ->  89.93  and loss:  947.8303630948067
forward train acc: top1 ->  69.9140625 ; top5 ->  87.875  and loss:  249.08358347415924
test acc: top1 ->  71.038 ; top5 ->  89.926  and loss:  947.0111573338509
forward train acc: top1 ->  70.203125 ; top5 ->  88.09375  and loss:  250.24682664871216
test acc: top1 ->  71.146 ; top5 ->  89.984  and loss:  945.2261001467705
forward train acc: top1 ->  70.2109375 ; top5 ->  87.8984375  and loss:  250.28395730257034
test acc: top1 ->  71.18 ; top5 ->  90.012  and loss:  942.7956587672234
forward train acc: top1 ->  69.890625 ; top5 ->  87.734375  and loss:  252.5279952287674
test acc: top1 ->  71.136 ; top5 ->  89.982  and loss:  943.5813172459602
forward train acc: top1 ->  69.9140625 ; top5 ->  87.5859375  and loss:  253.0005077123642
test acc: top1 ->  71.25 ; top5 ->  90.044  and loss:  941.2016398906708
forward train acc: top1 ->  70.0390625 ; top5 ->  87.9765625  and loss:  249.57972186803818
test acc: top1 ->  71.302 ; top5 ->  90.006  and loss:  943.0732481479645
forward train acc: top1 ->  70.09375 ; top5 ->  87.484375  and loss:  253.3851020336151
test acc: top1 ->  71.264 ; top5 ->  90.026  and loss:  943.267664194107
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  98 / 128 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -63.97524672746658 , diff:  63.97524672746658
adv train loss:  -64.25741285085678 , diff:  0.28216612339019775
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  70
test acc: top1 ->  68.04 ; top5 ->  87.948  and loss:  1051.7402492165565
forward train acc: top1 ->  69.53125 ; top5 ->  87.5859375  and loss:  254.58113861083984
test acc: top1 ->  70.594 ; top5 ->  89.622  and loss:  961.5040872693062
forward train acc: top1 ->  69.5703125 ; top5 ->  87.484375  and loss:  255.38365107774734
test acc: top1 ->  70.71 ; top5 ->  89.704  and loss:  958.033694267273
forward train acc: top1 ->  70.2578125 ; top5 ->  87.59375  and loss:  252.75467139482498
test acc: top1 ->  70.66 ; top5 ->  89.65  and loss:  955.5461295247078
forward train acc: top1 ->  69.8828125 ; top5 ->  87.578125  and loss:  255.35300993919373
test acc: top1 ->  70.634 ; top5 ->  89.656  and loss:  955.2709073424339
forward train acc: top1 ->  69.5390625 ; top5 ->  87.796875  and loss:  250.50076961517334
test acc: top1 ->  70.692 ; top5 ->  89.756  and loss:  955.2324043512344
forward train acc: top1 ->  70.0078125 ; top5 ->  87.6484375  and loss:  252.02085548639297
test acc: top1 ->  70.788 ; top5 ->  89.734  and loss:  952.3832374215126
forward train acc: top1 ->  69.7109375 ; top5 ->  87.1171875  and loss:  256.44612950086594
test acc: top1 ->  70.912 ; top5 ->  89.73  and loss:  951.8583946228027
forward train acc: top1 ->  70.1953125 ; top5 ->  88.2421875  and loss:  247.69064646959305
test acc: top1 ->  70.83 ; top5 ->  89.792  and loss:  951.7918233275414
forward train acc: top1 ->  69.328125 ; top5 ->  87.5234375  and loss:  255.0476879477501
test acc: top1 ->  70.814 ; top5 ->  89.746  and loss:  950.4634891152382
forward train acc: top1 ->  70.109375 ; top5 ->  87.875  and loss:  249.29456770420074
test acc: top1 ->  70.882 ; top5 ->  89.774  and loss:  950.039758682251
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  71 / 128 , inc:  1
---------------- start layer  14  ---------------
adv train loss:  -62.0417543053627 , diff:  62.0417543053627
adv train loss:  -63.51323628425598 , diff:  1.47148197889328
layer  14  adv train finish, try to retain  248
test acc: top1 ->  70.53 ; top5 ->  89.7  and loss:  956.6836544275284
forward train acc: top1 ->  69.6328125 ; top5 ->  87.7265625  and loss:  253.67122161388397
test acc: top1 ->  70.934 ; top5 ->  89.914  and loss:  944.2418730258942
forward train acc: top1 ->  69.7265625 ; top5 ->  87.734375  and loss:  254.63441395759583
test acc: top1 ->  71.048 ; top5 ->  89.924  and loss:  944.4386761784554
forward train acc: top1 ->  70.1875 ; top5 ->  87.734375  and loss:  250.8982993364334
test acc: top1 ->  71.002 ; top5 ->  89.922  and loss:  945.4558665752411
forward train acc: top1 ->  69.296875 ; top5 ->  87.3359375  and loss:  256.15398037433624
test acc: top1 ->  71.034 ; top5 ->  89.96  and loss:  943.154971063137
forward train acc: top1 ->  70.2265625 ; top5 ->  88.046875  and loss:  248.90184569358826
test acc: top1 ->  71.08 ; top5 ->  90.002  and loss:  941.2117202281952
forward train acc: top1 ->  70.484375 ; top5 ->  88.15625  and loss:  245.9721361398697
test acc: top1 ->  71.142 ; top5 ->  90.0  and loss:  940.0888713002205
forward train acc: top1 ->  70.5859375 ; top5 ->  87.9765625  and loss:  246.29488688707352
test acc: top1 ->  71.232 ; top5 ->  90.05  and loss:  939.1517091393471
forward train acc: top1 ->  70.1171875 ; top5 ->  88.171875  and loss:  248.335931122303
test acc: top1 ->  71.204 ; top5 ->  90.054  and loss:  935.3828522562981
forward train acc: top1 ->  70.078125 ; top5 ->  87.796875  and loss:  249.16642808914185
test acc: top1 ->  71.314 ; top5 ->  90.068  and loss:  935.7487885355949
forward train acc: top1 ->  70.5546875 ; top5 ->  87.546875  and loss:  247.0301125049591
test acc: top1 ->  71.332 ; top5 ->  90.116  and loss:  936.8171222805977
>>>>>>> reverse layer  14  since performance drop >>>>>>>
==> this epoch:  250 / 256 , inc:  1
---------------- start layer  15  ---------------
adv train loss:  -60.02044129371643 , diff:  60.02044129371643
adv train loss:  -58.89714843034744 , diff:  1.123292863368988
layer  15  adv train finish, try to retain  244
test acc: top1 ->  70.194 ; top5 ->  89.522  and loss:  974.0771134495735
forward train acc: top1 ->  69.6796875 ; top5 ->  87.8984375  and loss:  251.15412110090256
test acc: top1 ->  70.85 ; top5 ->  89.822  and loss:  950.9652288556099
forward train acc: top1 ->  69.4296875 ; top5 ->  87.75  and loss:  254.5697746872902
test acc: top1 ->  70.996 ; top5 ->  89.784  and loss:  949.2191876173019
forward train acc: top1 ->  70.1328125 ; top5 ->  87.9296875  and loss:  249.47640299797058
test acc: top1 ->  70.938 ; top5 ->  89.8  and loss:  946.1027238965034
forward train acc: top1 ->  69.40625 ; top5 ->  87.71875  and loss:  256.08024048805237
test acc: top1 ->  70.998 ; top5 ->  89.898  and loss:  943.6056132316589
forward train acc: top1 ->  69.84375 ; top5 ->  88.015625  and loss:  253.3131800889969
test acc: top1 ->  71.1 ; top5 ->  89.874  and loss:  942.6188173890114
forward train acc: top1 ->  70.2890625 ; top5 ->  87.953125  and loss:  247.69904732704163
test acc: top1 ->  71.084 ; top5 ->  89.954  and loss:  942.0120059847832
forward train acc: top1 ->  69.9765625 ; top5 ->  87.7265625  and loss:  249.80855005979538
test acc: top1 ->  71.056 ; top5 ->  89.956  and loss:  941.4583075046539
forward train acc: top1 ->  69.703125 ; top5 ->  87.6171875  and loss:  250.61438632011414
test acc: top1 ->  71.134 ; top5 ->  89.978  and loss:  938.4029073119164
forward train acc: top1 ->  70.0 ; top5 ->  88.1953125  and loss:  245.8529144525528
test acc: top1 ->  71.132 ; top5 ->  89.992  and loss:  941.3130123615265
forward train acc: top1 ->  70.375 ; top5 ->  87.6953125  and loss:  252.10487818717957
test acc: top1 ->  71.132 ; top5 ->  89.956  and loss:  939.9300647377968
>>>>>>> reverse layer  15  since performance drop >>>>>>>
==> this epoch:  248 / 256 , inc:  1
---------------- start layer  16  ---------------
adv train loss:  -61.48109954595566 , diff:  61.48109954595566
adv train loss:  -64.9085156917572 , diff:  3.427416145801544
layer  16  adv train finish, try to retain  247
>>>>>>> reverse layer  16  since no improvement >>>>>>>
---------------- start layer  17  ---------------
adv train loss:  -64.6608482003212 , diff:  64.6608482003212
adv train loss:  -62.34553933143616 , diff:  2.3153088688850403
layer  17  adv train finish, try to retain  217
test acc: top1 ->  70.5 ; top5 ->  89.602  and loss:  967.9263945817947
forward train acc: top1 ->  69.1328125 ; top5 ->  88.2265625  and loss:  253.615948677063
test acc: top1 ->  70.78 ; top5 ->  89.87  and loss:  952.4064508676529
forward train acc: top1 ->  69.515625 ; top5 ->  87.8125  and loss:  252.32920694351196
test acc: top1 ->  70.852 ; top5 ->  89.836  and loss:  948.9440353512764
forward train acc: top1 ->  68.8828125 ; top5 ->  86.84375  and loss:  261.4460161328316
test acc: top1 ->  70.782 ; top5 ->  89.846  and loss:  948.1391686201096
forward train acc: top1 ->  70.21875 ; top5 ->  88.28125  and loss:  246.13205909729004
test acc: top1 ->  71.042 ; top5 ->  89.904  and loss:  943.0774250030518
forward train acc: top1 ->  70.078125 ; top5 ->  88.2421875  and loss:  246.9093922972679
test acc: top1 ->  71.048 ; top5 ->  89.952  and loss:  941.9840557575226
forward train acc: top1 ->  70.28125 ; top5 ->  88.015625  and loss:  248.23765295743942
test acc: top1 ->  71.09 ; top5 ->  89.906  and loss:  942.4371017813683
forward train acc: top1 ->  69.3203125 ; top5 ->  87.421875  and loss:  257.7397229075432
test acc: top1 ->  71.174 ; top5 ->  89.98  and loss:  941.4164471030235
forward train acc: top1 ->  69.3515625 ; top5 ->  87.3046875  and loss:  258.5120069384575
test acc: top1 ->  71.154 ; top5 ->  89.98  and loss:  942.1643193364143
forward train acc: top1 ->  69.8359375 ; top5 ->  87.296875  and loss:  252.36244875192642
test acc: top1 ->  71.248 ; top5 ->  90.028  and loss:  940.2702096700668
forward train acc: top1 ->  69.8125 ; top5 ->  87.578125  and loss:  251.5245440006256
test acc: top1 ->  71.138 ; top5 ->  90.026  and loss:  940.565575003624
>>>>>>> reverse layer  17  since performance drop >>>>>>>
==> this epoch:  230 / 256 , inc:  1
---------------- start layer  18  ---------------
adv train loss:  -61.919986724853516 , diff:  61.919986724853516
adv train loss:  -61.863275587558746 , diff:  0.05671113729476929
layer  18  adv train finish, try to retain  233
test acc: top1 ->  71.16 ; top5 ->  90.054  and loss:  935.197976231575
forward train acc: top1 ->  70.40625 ; top5 ->  88.1875  and loss:  246.61705940961838
test acc: top1 ->  71.146 ; top5 ->  89.964  and loss:  943.3282360434532
forward train acc: top1 ->  70.6171875 ; top5 ->  88.1328125  and loss:  247.1187019944191
test acc: top1 ->  71.066 ; top5 ->  89.974  and loss:  941.0952116250992
forward train acc: top1 ->  70.109375 ; top5 ->  87.71875  and loss:  252.20412647724152
test acc: top1 ->  71.136 ; top5 ->  89.956  and loss:  942.6494617462158
forward train acc: top1 ->  69.875 ; top5 ->  87.515625  and loss:  252.9580864906311
test acc: top1 ->  71.236 ; top5 ->  90.022  and loss:  940.80873721838
forward train acc: top1 ->  70.3828125 ; top5 ->  87.8203125  and loss:  249.22993689775467
test acc: top1 ->  71.262 ; top5 ->  90.036  and loss:  938.9746595621109
forward train acc: top1 ->  70.25 ; top5 ->  87.90625  and loss:  250.3609973192215
test acc: top1 ->  71.334 ; top5 ->  90.066  and loss:  935.7181495428085
forward train acc: top1 ->  70.8515625 ; top5 ->  87.9453125  and loss:  245.23392641544342
test acc: top1 ->  71.288 ; top5 ->  90.064  and loss:  936.699528992176
forward train acc: top1 ->  70.4375 ; top5 ->  88.3671875  and loss:  244.38689643144608
test acc: top1 ->  71.396 ; top5 ->  90.074  and loss:  934.2051084637642
forward train acc: top1 ->  69.8828125 ; top5 ->  87.8515625  and loss:  251.24825525283813
test acc: top1 ->  71.468 ; top5 ->  90.114  and loss:  934.5634164810181
forward train acc: top1 ->  70.5546875 ; top5 ->  88.46875  and loss:  242.4763491153717
test acc: top1 ->  71.426 ; top5 ->  90.068  and loss:  934.8874916434288
>>>>>>> reverse layer  18  since performance drop >>>>>>>
==> this epoch:  235 / 256 , inc:  1
---------------- start layer  19  ---------------
adv train loss:  -59.370909571647644 , diff:  59.370909571647644
adv train loss:  -61.54986780881882 , diff:  2.178958237171173
layer  19  adv train finish, try to retain  173
test acc: top1 ->  70.65 ; top5 ->  89.678  and loss:  958.6017481684685
forward train acc: top1 ->  69.9296875 ; top5 ->  87.65625  and loss:  250.14599579572678
test acc: top1 ->  70.956 ; top5 ->  89.816  and loss:  951.4836803078651
forward train acc: top1 ->  69.3203125 ; top5 ->  87.8515625  and loss:  254.16348761320114
test acc: top1 ->  70.81 ; top5 ->  89.86  and loss:  951.2839220166206
forward train acc: top1 ->  69.59375 ; top5 ->  87.3125  and loss:  252.53256464004517
test acc: top1 ->  70.982 ; top5 ->  89.888  and loss:  949.0777342319489
forward train acc: top1 ->  69.1953125 ; top5 ->  87.5859375  and loss:  258.0059144496918
test acc: top1 ->  71.008 ; top5 ->  89.876  and loss:  948.6869487762451
forward train acc: top1 ->  69.4140625 ; top5 ->  87.71875  and loss:  254.6697697043419
test acc: top1 ->  71.044 ; top5 ->  89.878  and loss:  947.7156742811203
forward train acc: top1 ->  69.65625 ; top5 ->  87.421875  and loss:  254.28987526893616
test acc: top1 ->  70.99 ; top5 ->  89.868  and loss:  945.9070001840591
forward train acc: top1 ->  70.0546875 ; top5 ->  87.0859375  and loss:  257.4246373772621
test acc: top1 ->  71.066 ; top5 ->  89.892  and loss:  945.5157699584961
forward train acc: top1 ->  70.5703125 ; top5 ->  88.359375  and loss:  245.94385927915573
test acc: top1 ->  71.004 ; top5 ->  89.868  and loss:  944.5058897137642
forward train acc: top1 ->  70.1171875 ; top5 ->  87.828125  and loss:  249.28862792253494
test acc: top1 ->  71.152 ; top5 ->  89.866  and loss:  942.4376891255379
forward train acc: top1 ->  69.765625 ; top5 ->  87.7265625  and loss:  251.8477172255516
test acc: top1 ->  71.21 ; top5 ->  89.904  and loss:  944.625089764595
>>>>>>> reverse layer  19  since performance drop >>>>>>>
==> this epoch:  179 / 256 , inc:  1
---------------- start layer  20  ---------------
adv train loss:  -62.06756007671356 , diff:  62.06756007671356
adv train loss:  -60.18962752819061 , diff:  1.8779325485229492
layer  20  adv train finish, try to retain  219
test acc: top1 ->  70.816 ; top5 ->  89.762  and loss:  950.5510066151619
forward train acc: top1 ->  70.171875 ; top5 ->  88.0234375  and loss:  248.7985053062439
test acc: top1 ->  70.926 ; top5 ->  89.906  and loss:  950.3473798632622
forward train acc: top1 ->  69.9921875 ; top5 ->  87.7890625  and loss:  253.9976174235344
test acc: top1 ->  71.032 ; top5 ->  89.91  and loss:  948.2258759737015
forward train acc: top1 ->  69.9609375 ; top5 ->  87.578125  and loss:  253.4104585647583
test acc: top1 ->  70.94 ; top5 ->  89.958  and loss:  949.560513317585
forward train acc: top1 ->  69.609375 ; top5 ->  87.5078125  and loss:  256.9811413884163
test acc: top1 ->  71.1 ; top5 ->  89.912  and loss:  944.1519145965576
forward train acc: top1 ->  70.4453125 ; top5 ->  88.0703125  and loss:  245.42013901472092
test acc: top1 ->  71.158 ; top5 ->  89.94  and loss:  946.2711775898933
forward train acc: top1 ->  70.578125 ; top5 ->  88.234375  and loss:  243.39494663476944
test acc: top1 ->  71.104 ; top5 ->  89.936  and loss:  943.6538769602776
forward train acc: top1 ->  69.4921875 ; top5 ->  87.828125  and loss:  250.84842908382416
test acc: top1 ->  71.268 ; top5 ->  89.98  and loss:  941.395622253418
forward train acc: top1 ->  70.234375 ; top5 ->  87.9453125  and loss:  250.2913654446602
test acc: top1 ->  71.242 ; top5 ->  89.962  and loss:  943.4199315905571
forward train acc: top1 ->  70.40625 ; top5 ->  87.515625  and loss:  253.81931096315384
test acc: top1 ->  71.284 ; top5 ->  89.998  and loss:  942.4411439299583
forward train acc: top1 ->  70.703125 ; top5 ->  88.375  and loss:  244.96439111232758
test acc: top1 ->  71.214 ; top5 ->  90.016  and loss:  940.471800506115
>>>>>>> reverse layer  20  since performance drop >>>>>>>
==> this epoch:  225 / 256 , inc:  1
---------------- start layer  21  ---------------
adv train loss:  -59.58878427743912 , diff:  59.58878427743912
adv train loss:  -58.48734813928604 , diff:  1.1014361381530762
layer  21  adv train finish, try to retain  241
test acc: top1 ->  71.204 ; top5 ->  89.96  and loss:  947.5156951546669
forward train acc: top1 ->  70.5234375 ; top5 ->  88.0625  and loss:  248.37572449445724
test acc: top1 ->  71.188 ; top5 ->  89.996  and loss:  944.2853301763535
forward train acc: top1 ->  70.4765625 ; top5 ->  88.0625  and loss:  247.00913017988205
test acc: top1 ->  71.192 ; top5 ->  90.09  and loss:  941.6159914731979
forward train acc: top1 ->  69.8515625 ; top5 ->  87.5234375  and loss:  250.73613768815994
test acc: top1 ->  71.294 ; top5 ->  90.02  and loss:  942.0752285718918
forward train acc: top1 ->  70.125 ; top5 ->  87.7890625  and loss:  247.18180233240128
test acc: top1 ->  71.34 ; top5 ->  90.078  and loss:  939.1073653697968
forward train acc: top1 ->  70.203125 ; top5 ->  87.734375  and loss:  250.5709273815155
test acc: top1 ->  71.276 ; top5 ->  90.034  and loss:  941.6373136639595
forward train acc: top1 ->  70.984375 ; top5 ->  88.375  and loss:  241.79692977666855
test acc: top1 ->  71.38 ; top5 ->  90.04  and loss:  939.0655584931374
forward train acc: top1 ->  70.1640625 ; top5 ->  87.953125  and loss:  251.03521090745926
test acc: top1 ->  71.294 ; top5 ->  90.08  and loss:  938.8277037739754
forward train acc: top1 ->  70.578125 ; top5 ->  88.0234375  and loss:  247.69439202547073
test acc: top1 ->  71.39 ; top5 ->  90.044  and loss:  937.3288375139236
forward train acc: top1 ->  70.5703125 ; top5 ->  88.09375  and loss:  245.64322239160538
test acc: top1 ->  71.292 ; top5 ->  90.104  and loss:  937.9187341928482
forward train acc: top1 ->  70.4609375 ; top5 ->  88.34375  and loss:  245.29693549871445
test acc: top1 ->  71.286 ; top5 ->  90.08  and loss:  940.6757508516312
>>>>>>> reverse layer  21  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  22  ---------------
adv train loss:  -59.677178502082825 , diff:  59.677178502082825
adv train loss:  -64.00575232505798 , diff:  4.328573822975159
************ all values are small in this layer **********
layer  22  adv train finish, try to retain  207
test acc: top1 ->  71.156 ; top5 ->  89.93  and loss:  949.7207853198051
forward train acc: top1 ->  70.390625 ; top5 ->  88.296875  and loss:  249.0959967970848
test acc: top1 ->  71.2 ; top5 ->  89.9  and loss:  946.4098497629166
forward train acc: top1 ->  70.1640625 ; top5 ->  87.6875  and loss:  249.70720911026
test acc: top1 ->  71.09 ; top5 ->  89.898  and loss:  951.5544773936272
forward train acc: top1 ->  70.328125 ; top5 ->  88.2265625  and loss:  248.47788435220718
test acc: top1 ->  71.0 ; top5 ->  89.862  and loss:  948.2270818948746
forward train acc: top1 ->  69.8984375 ; top5 ->  87.6328125  and loss:  252.7149732708931
test acc: top1 ->  71.194 ; top5 ->  89.902  and loss:  944.7913014888763
forward train acc: top1 ->  70.109375 ; top5 ->  88.515625  and loss:  245.0722959637642
test acc: top1 ->  71.34 ; top5 ->  89.97  and loss:  943.5192827582359
forward train acc: top1 ->  70.78125 ; top5 ->  88.5859375  and loss:  244.65824002027512
test acc: top1 ->  71.272 ; top5 ->  89.938  and loss:  944.4381744861603
forward train acc: top1 ->  70.0703125 ; top5 ->  87.46875  and loss:  250.46837931871414
test acc: top1 ->  71.286 ; top5 ->  90.04  and loss:  941.4697468280792
forward train acc: top1 ->  70.9140625 ; top5 ->  88.4609375  and loss:  241.310826420784
test acc: top1 ->  71.252 ; top5 ->  89.972  and loss:  941.8901101350784
forward train acc: top1 ->  70.6171875 ; top5 ->  87.78125  and loss:  247.899778008461
test acc: top1 ->  71.288 ; top5 ->  89.944  and loss:  944.1762988567352
forward train acc: top1 ->  70.6875 ; top5 ->  88.4609375  and loss:  241.26609927415848
test acc: top1 ->  71.286 ; top5 ->  90.016  and loss:  943.4861668348312
>>>>>>> reverse layer  22  since performance drop >>>>>>>
==> this epoch:  208 / 256 , inc:  1
---------------- start layer  23  ---------------
adv train loss:  -60.47545766830444 , diff:  60.47545766830444
adv train loss:  -65.46815353631973 , diff:  4.992695868015289
layer  23  adv train finish, try to retain  175
test acc: top1 ->  70.468 ; top5 ->  89.538  and loss:  971.0658369660378
forward train acc: top1 ->  70.7109375 ; top5 ->  88.5703125  and loss:  242.73103839159012
test acc: top1 ->  70.868 ; top5 ->  89.812  and loss:  952.2113801836967
forward train acc: top1 ->  69.3828125 ; top5 ->  87.203125  and loss:  257.4531689286232
test acc: top1 ->  70.858 ; top5 ->  89.742  and loss:  950.0277189016342
forward train acc: top1 ->  69.703125 ; top5 ->  87.546875  and loss:  252.97012412548065
test acc: top1 ->  70.774 ; top5 ->  89.722  and loss:  952.8834552764893
forward train acc: top1 ->  69.3046875 ; top5 ->  87.203125  and loss:  259.4016890525818
test acc: top1 ->  70.942 ; top5 ->  89.79  and loss:  948.2031526565552
forward train acc: top1 ->  69.7265625 ; top5 ->  87.546875  and loss:  253.49892562627792
test acc: top1 ->  70.966 ; top5 ->  89.856  and loss:  948.7558501958847
forward train acc: top1 ->  70.140625 ; top5 ->  87.9140625  and loss:  248.99278235435486
test acc: top1 ->  71.016 ; top5 ->  89.876  and loss:  947.1613336205482
forward train acc: top1 ->  70.3046875 ; top5 ->  88.1015625  and loss:  246.0517383813858
test acc: top1 ->  71.022 ; top5 ->  89.86  and loss:  946.4951156377792
forward train acc: top1 ->  70.203125 ; top5 ->  87.796875  and loss:  247.43493074178696
test acc: top1 ->  71.054 ; top5 ->  89.942  and loss:  943.1910081505775
forward train acc: top1 ->  70.3984375 ; top5 ->  88.2578125  and loss:  245.04595404863358
test acc: top1 ->  71.076 ; top5 ->  89.868  and loss:  947.3413278460503
forward train acc: top1 ->  69.59375 ; top5 ->  87.6640625  and loss:  252.0706638097763
test acc: top1 ->  71.116 ; top5 ->  89.89  and loss:  944.0981446504593
>>>>>>> reverse layer  23  since performance drop >>>>>>>
==> this epoch:  187 / 256 , inc:  1
---------------- start layer  24  ---------------
adv train loss:  -64.52352112531662 , diff:  64.52352112531662
adv train loss:  -61.633442997932434 , diff:  2.890078127384186
layer  24  adv train finish, try to retain  247
>>>>>>> reverse layer  24  since no improvement >>>>>>>
---------------- start layer  25  ---------------
adv train loss:  -62.81640672683716 , diff:  62.81640672683716
adv train loss:  -58.360963344573975 , diff:  4.455443382263184
layer  25  adv train finish, try to retain  248
>>>>>>> reverse layer  25  since no improvement >>>>>>>
---------------- start layer  26  ---------------
adv train loss:  -61.28400957584381 , diff:  61.28400957584381
adv train loss:  -64.88518691062927 , diff:  3.6011773347854614
layer  26  adv train finish, try to retain  497
test acc: top1 ->  71.118 ; top5 ->  89.912  and loss:  945.9761581420898
forward train acc: top1 ->  70.2421875 ; top5 ->  88.046875  and loss:  247.58011466264725
test acc: top1 ->  71.112 ; top5 ->  89.942  and loss:  940.8926936984062
forward train acc: top1 ->  70.0234375 ; top5 ->  87.9765625  and loss:  250.21705651283264
test acc: top1 ->  71.068 ; top5 ->  89.89  and loss:  945.5661646723747
forward train acc: top1 ->  69.8125 ; top5 ->  87.5546875  and loss:  252.246884226799
test acc: top1 ->  71.09 ; top5 ->  89.852  and loss:  944.6949586868286
forward train acc: top1 ->  70.6015625 ; top5 ->  88.0234375  and loss:  247.49973863363266
test acc: top1 ->  71.158 ; top5 ->  89.864  and loss:  944.4480808377266
forward train acc: top1 ->  70.6328125 ; top5 ->  88.3125  and loss:  246.63789623975754
test acc: top1 ->  71.168 ; top5 ->  89.95  and loss:  940.5594815611839
forward train acc: top1 ->  70.125 ; top5 ->  87.828125  and loss:  250.93133717775345
test acc: top1 ->  71.138 ; top5 ->  89.932  and loss:  940.5318499207497
forward train acc: top1 ->  71.234375 ; top5 ->  88.3671875  and loss:  242.75042724609375
test acc: top1 ->  71.078 ; top5 ->  89.972  and loss:  941.6807757019997
forward train acc: top1 ->  71.1171875 ; top5 ->  88.0  and loss:  242.87915587425232
test acc: top1 ->  71.116 ; top5 ->  89.942  and loss:  940.7944260239601
forward train acc: top1 ->  70.0078125 ; top5 ->  87.8359375  and loss:  250.7873500585556
test acc: top1 ->  71.186 ; top5 ->  90.05  and loss:  938.8250881433487
forward train acc: top1 ->  70.3203125 ; top5 ->  87.84375  and loss:  248.38378620147705
test acc: top1 ->  71.184 ; top5 ->  89.996  and loss:  939.538567006588
>>>>>>> reverse layer  26  since performance drop >>>>>>>
==> this epoch:  502 / 512 , inc:  1
---------------- start layer  27  ---------------
adv train loss:  -63.687571704387665 , diff:  63.687571704387665
adv train loss:  -61.62164741754532 , diff:  2.065924286842346
************ all values are small in this layer **********
layer  27  adv train finish, try to retain  504
test acc: top1 ->  71.154 ; top5 ->  90.066  and loss:  940.3811852931976
forward train acc: top1 ->  70.3125 ; top5 ->  87.9375  and loss:  246.85457253456116
test acc: top1 ->  71.164 ; top5 ->  89.996  and loss:  943.5395446419716
forward train acc: top1 ->  70.6796875 ; top5 ->  88.0078125  and loss:  246.01331514120102
test acc: top1 ->  71.222 ; top5 ->  89.938  and loss:  939.1330953836441
forward train acc: top1 ->  69.7265625 ; top5 ->  87.78125  and loss:  251.1193664073944
test acc: top1 ->  71.15 ; top5 ->  89.986  and loss:  942.6986206769943
forward train acc: top1 ->  70.890625 ; top5 ->  88.28125  and loss:  241.7294859290123
test acc: top1 ->  71.28 ; top5 ->  90.02  and loss:  939.8529953956604
forward train acc: top1 ->  69.6796875 ; top5 ->  87.5  and loss:  255.21216422319412
test acc: top1 ->  71.27 ; top5 ->  89.994  and loss:  939.6897644996643
forward train acc: top1 ->  70.2578125 ; top5 ->  88.125  and loss:  250.6611578464508
test acc: top1 ->  71.406 ; top5 ->  90.068  and loss:  935.5537563562393
forward train acc: top1 ->  70.7734375 ; top5 ->  88.109375  and loss:  245.16330951452255
test acc: top1 ->  71.392 ; top5 ->  90.024  and loss:  937.241447865963
forward train acc: top1 ->  70.40625 ; top5 ->  88.3359375  and loss:  244.01323974132538
test acc: top1 ->  71.378 ; top5 ->  90.07  and loss:  936.226787507534
forward train acc: top1 ->  70.5078125 ; top5 ->  87.9453125  and loss:  249.28430092334747
test acc: top1 ->  71.4 ; top5 ->  90.098  and loss:  935.1824734210968
forward train acc: top1 ->  70.0703125 ; top5 ->  87.8046875  and loss:  252.5557115674019
test acc: top1 ->  71.376 ; top5 ->  90.072  and loss:  938.0213780999184
>>>>>>> reverse layer  27  since performance drop >>>>>>>
==> this epoch:  505 / 512 , inc:  1
---------------- start layer  28  ---------------
adv train loss:  -63.42418086528778 , diff:  63.42418086528778
adv train loss:  -61.02136433124542 , diff:  2.4028165340423584
layer  28  adv train finish, try to retain  501
test acc: top1 ->  71.292 ; top5 ->  89.984  and loss:  942.4476561546326
forward train acc: top1 ->  70.4296875 ; top5 ->  87.8515625  and loss:  248.9833943247795
test acc: top1 ->  71.26 ; top5 ->  89.968  and loss:  942.6736537218094
forward train acc: top1 ->  69.953125 ; top5 ->  87.5859375  and loss:  253.61107909679413
test acc: top1 ->  71.13 ; top5 ->  89.93  and loss:  941.8375836014748
forward train acc: top1 ->  68.859375 ; top5 ->  87.5625  and loss:  256.3716142773628
test acc: top1 ->  71.028 ; top5 ->  89.95  and loss:  940.701990544796
forward train acc: top1 ->  70.625 ; top5 ->  87.9609375  and loss:  246.2594422698021
test acc: top1 ->  71.188 ; top5 ->  90.026  and loss:  940.3251330852509
forward train acc: top1 ->  69.8984375 ; top5 ->  87.796875  and loss:  250.92547422647476
test acc: top1 ->  71.148 ; top5 ->  89.998  and loss:  940.1032826900482
forward train acc: top1 ->  69.8671875 ; top5 ->  87.8828125  and loss:  252.05115389823914
test acc: top1 ->  71.194 ; top5 ->  90.02  and loss:  940.164152443409
forward train acc: top1 ->  69.359375 ; top5 ->  87.4765625  and loss:  256.4009992480278
test acc: top1 ->  71.206 ; top5 ->  90.022  and loss:  939.4103053808212
forward train acc: top1 ->  71.078125 ; top5 ->  88.375  and loss:  242.4857143163681
test acc: top1 ->  71.312 ; top5 ->  90.042  and loss:  937.6870313286781
forward train acc: top1 ->  71.1796875 ; top5 ->  88.296875  and loss:  240.71470248699188
test acc: top1 ->  71.322 ; top5 ->  90.08  and loss:  937.5858350396156
forward train acc: top1 ->  70.859375 ; top5 ->  88.0625  and loss:  247.60148030519485
test acc: top1 ->  71.452 ; top5 ->  90.052  and loss:  938.4405850768089
>>>>>>> reverse layer  28  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  29  ---------------
adv train loss:  -61.567947804927826 , diff:  61.567947804927826
adv train loss:  -62.59538805484772 , diff:  1.0274402499198914
layer  29  adv train finish, try to retain  488
test acc: top1 ->  71.17 ; top5 ->  89.954  and loss:  961.1327977180481
forward train acc: top1 ->  69.5859375 ; top5 ->  87.359375  and loss:  255.8160376548767
test acc: top1 ->  71.196 ; top5 ->  89.942  and loss:  950.4508346319199
forward train acc: top1 ->  70.28125 ; top5 ->  88.1640625  and loss:  247.70037299394608
test acc: top1 ->  71.234 ; top5 ->  89.946  and loss:  950.1808900237083
forward train acc: top1 ->  70.640625 ; top5 ->  88.125  and loss:  249.4523941874504
test acc: top1 ->  71.184 ; top5 ->  89.96  and loss:  951.7455312013626
forward train acc: top1 ->  69.6015625 ; top5 ->  87.5859375  and loss:  254.1018267273903
test acc: top1 ->  71.278 ; top5 ->  89.956  and loss:  948.5273631215096
forward train acc: top1 ->  69.96875 ; top5 ->  87.6484375  and loss:  251.51420491933823
test acc: top1 ->  71.224 ; top5 ->  89.886  and loss:  950.3573424220085
forward train acc: top1 ->  70.09375 ; top5 ->  87.828125  and loss:  251.36215937137604
test acc: top1 ->  71.274 ; top5 ->  89.96  and loss:  945.3737695813179
forward train acc: top1 ->  70.5 ; top5 ->  88.5546875  and loss:  245.1466873884201
test acc: top1 ->  71.192 ; top5 ->  89.9  and loss:  946.7972283959389
forward train acc: top1 ->  70.0625 ; top5 ->  87.421875  and loss:  255.308833360672
test acc: top1 ->  71.292 ; top5 ->  89.986  and loss:  945.9975472688675
forward train acc: top1 ->  70.4375 ; top5 ->  88.4609375  and loss:  246.49440968036652
test acc: top1 ->  71.294 ; top5 ->  90.034  and loss:  946.5540687441826
forward train acc: top1 ->  70.5625 ; top5 ->  87.9921875  and loss:  248.11137545108795
test acc: top1 ->  71.24 ; top5 ->  89.982  and loss:  947.0590102672577
>>>>>>> reverse layer  29  since performance drop >>>>>>>
==> this epoch:  498 / 512 , inc:  1
---------------- start layer  30  ---------------
adv train loss:  -63.95161408185959 , diff:  63.95161408185959
adv train loss:  -60.90009993314743 , diff:  3.051514148712158
layer  30  adv train finish, try to retain  501
test acc: top1 ->  70.852 ; top5 ->  89.92  and loss:  952.3027936816216
forward train acc: top1 ->  70.6875 ; top5 ->  88.6640625  and loss:  243.89162892103195
test acc: top1 ->  70.964 ; top5 ->  89.898  and loss:  949.3346895575523
forward train acc: top1 ->  70.1328125 ; top5 ->  87.6640625  and loss:  252.02699381113052
test acc: top1 ->  71.066 ; top5 ->  89.87  and loss:  948.7825365066528
forward train acc: top1 ->  69.9921875 ; top5 ->  87.7890625  and loss:  251.58403086662292
test acc: top1 ->  71.11 ; top5 ->  89.9  and loss:  945.6071019768715
forward train acc: top1 ->  70.90625 ; top5 ->  88.140625  and loss:  246.40563809871674
test acc: top1 ->  71.088 ; top5 ->  89.858  and loss:  946.5750027298927
forward train acc: top1 ->  69.875 ; top5 ->  87.5234375  and loss:  251.40728437900543
test acc: top1 ->  71.118 ; top5 ->  89.922  and loss:  942.8884155154228
forward train acc: top1 ->  70.875 ; top5 ->  88.1640625  and loss:  243.2425571680069
test acc: top1 ->  71.222 ; top5 ->  89.976  and loss:  944.2206946015358
forward train acc: top1 ->  70.515625 ; top5 ->  88.015625  and loss:  246.66769218444824
test acc: top1 ->  71.238 ; top5 ->  90.062  and loss:  939.2266224622726
forward train acc: top1 ->  69.8359375 ; top5 ->  87.8046875  and loss:  252.86697047948837
test acc: top1 ->  71.244 ; top5 ->  89.996  and loss:  941.1147258281708
forward train acc: top1 ->  70.1640625 ; top5 ->  87.828125  and loss:  250.66117548942566
test acc: top1 ->  71.196 ; top5 ->  90.054  and loss:  939.4436244368553
forward train acc: top1 ->  70.6171875 ; top5 ->  88.296875  and loss:  244.34402906894684
test acc: top1 ->  71.156 ; top5 ->  89.96  and loss:  941.910237967968
>>>>>>> reverse layer  30  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  31  ---------------
adv train loss:  -60.78960806131363 , diff:  60.78960806131363
adv train loss:  -58.94987893104553 , diff:  1.839729130268097
layer  31  adv train finish, try to retain  496
test acc: top1 ->  70.986 ; top5 ->  89.878  and loss:  950.0869925022125
forward train acc: top1 ->  70.5703125 ; top5 ->  88.3046875  and loss:  247.0349496603012
test acc: top1 ->  71.012 ; top5 ->  89.842  and loss:  949.1709723472595
forward train acc: top1 ->  69.9765625 ; top5 ->  87.9765625  and loss:  250.1074941754341
test acc: top1 ->  71.004 ; top5 ->  89.83  and loss:  947.7202535867691
forward train acc: top1 ->  70.2109375 ; top5 ->  87.9609375  and loss:  247.78860813379288
test acc: top1 ->  71.014 ; top5 ->  89.902  and loss:  945.2089148163795
forward train acc: top1 ->  69.8125 ; top5 ->  87.8203125  and loss:  252.21590584516525
test acc: top1 ->  71.056 ; top5 ->  89.916  and loss:  945.0019043087959
forward train acc: top1 ->  69.6875 ; top5 ->  87.828125  and loss:  252.188123524189
test acc: top1 ->  71.178 ; top5 ->  89.98  and loss:  939.605855345726
forward train acc: top1 ->  70.609375 ; top5 ->  88.0078125  and loss:  250.47674250602722
test acc: top1 ->  71.164 ; top5 ->  89.974  and loss:  939.0289068222046
forward train acc: top1 ->  70.0390625 ; top5 ->  87.8515625  and loss:  248.62697380781174
test acc: top1 ->  71.17 ; top5 ->  89.962  and loss:  941.1422545909882
forward train acc: top1 ->  71.0625 ; top5 ->  88.4375  and loss:  240.6010900735855
test acc: top1 ->  71.142 ; top5 ->  89.974  and loss:  939.79353505373
forward train acc: top1 ->  69.96875 ; top5 ->  88.359375  and loss:  245.31172555685043
test acc: top1 ->  71.176 ; top5 ->  89.994  and loss:  937.8611354827881
forward train acc: top1 ->  70.0390625 ; top5 ->  87.6796875  and loss:  253.36398857831955
test acc: top1 ->  71.25 ; top5 ->  90.0  and loss:  939.3982887864113
>>>>>>> reverse layer  31  since performance drop >>>>>>>
==> this epoch:  501 / 512 , inc:  1
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.04805419921875001, 0.06407226562500001, 0.00084470272064209, 0.06407226562500001, 0.009010162353515627, 0.25628906250000005, 0.000422351360321045, 0.00028156757354736334, 0.012013549804687502, 0.000422351360321045, 0.012013549804687502, 0.0022525405883789067, 0.0022525405883789067, 0.04805419921875001, 0.00010558784008026125, 0.00010558784008026125, 0.0005631351470947267, 0.0005631351470947267, 0.0002111756801605225, 0.006006774902343751, 0.0005631351470947267, 0.0002111756801605225, 0.0011262702941894534, 0.006006774902343751, 0.00028156757354736334, 0.00028156757354736334, 5.279392004013062e-05, 2.639696002006531e-05, 5.279392004013062e-05, 0.00010558784008026125, 5.279392004013062e-05, 5.279392004013062e-05]  wait [3, 0, 4, 0, 4, 2, 4, 0, 2, 4, 2, 3, 3, 4, 3, 3, 2, 2, 4, 2, 2, 4, 3, 2, 1, 1, 3, 2, 3, 4, 3, 3]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 6
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  24  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  3  ###
---------------- start layer  1  ---------------
adv train loss:  -62.18504136800766 , diff:  62.18504136800766
adv train loss:  -64.07866442203522 , diff:  1.8936230540275574
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  43
test acc: top1 ->  68.602 ; top5 ->  88.46  and loss:  1020.936797440052
forward train acc: top1 ->  69.34375 ; top5 ->  87.453125  and loss:  255.41145759820938
test acc: top1 ->  70.766 ; top5 ->  89.91  and loss:  954.3593057990074
forward train acc: top1 ->  70.328125 ; top5 ->  88.046875  and loss:  250.1823606491089
test acc: top1 ->  70.838 ; top5 ->  89.754  and loss:  954.5826846957207
forward train acc: top1 ->  69.5859375 ; top5 ->  87.6796875  and loss:  252.95133584737778
test acc: top1 ->  70.91 ; top5 ->  89.916  and loss:  948.9885015487671
forward train acc: top1 ->  69.7109375 ; top5 ->  88.046875  and loss:  250.88537329435349
test acc: top1 ->  70.852 ; top5 ->  89.846  and loss:  950.9608715772629
forward train acc: top1 ->  70.125 ; top5 ->  88.0546875  and loss:  246.02661967277527
test acc: top1 ->  71.008 ; top5 ->  89.926  and loss:  946.280592083931
forward train acc: top1 ->  70.1875 ; top5 ->  87.515625  and loss:  250.4451603293419
test acc: top1 ->  70.97 ; top5 ->  89.88  and loss:  948.5285645723343
forward train acc: top1 ->  69.328125 ; top5 ->  87.7109375  and loss:  255.24794417619705
test acc: top1 ->  71.03 ; top5 ->  89.928  and loss:  945.8221306800842
forward train acc: top1 ->  70.4375 ; top5 ->  88.0078125  and loss:  248.40268582105637
test acc: top1 ->  71.072 ; top5 ->  89.92  and loss:  945.822751045227
forward train acc: top1 ->  70.2890625 ; top5 ->  87.953125  and loss:  248.98242610692978
test acc: top1 ->  71.088 ; top5 ->  89.974  and loss:  942.0124558210373
forward train acc: top1 ->  70.890625 ; top5 ->  88.1875  and loss:  242.7752839922905
test acc: top1 ->  71.166 ; top5 ->  89.992  and loss:  942.0259391665459
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  44 / 64 , inc:  1
---------------- start layer  2  ---------------
### skip layer  2 wait:  4  ###
---------------- start layer  3  ---------------
adv train loss:  -62.22475224733353 , diff:  62.22475224733353
adv train loss:  -67.56820273399353 , diff:  5.343450486660004
************ all values are small in this layer **********
layer  3  adv train finish, try to retain  42
test acc: top1 ->  66.076 ; top5 ->  86.862  and loss:  1120.6524823904037
forward train acc: top1 ->  69.9140625 ; top5 ->  87.703125  and loss:  253.0779104232788
test acc: top1 ->  70.708 ; top5 ->  89.832  and loss:  958.792740881443
forward train acc: top1 ->  70.1328125 ; top5 ->  87.9609375  and loss:  250.48653852939606
test acc: top1 ->  70.788 ; top5 ->  89.832  and loss:  956.7827178239822
forward train acc: top1 ->  69.8515625 ; top5 ->  87.65625  and loss:  252.82060354948044
test acc: top1 ->  70.688 ; top5 ->  89.796  and loss:  955.8343527317047
forward train acc: top1 ->  69.5 ; top5 ->  87.3359375  and loss:  255.24553447961807
test acc: top1 ->  70.78 ; top5 ->  89.894  and loss:  954.3431225419044
forward train acc: top1 ->  69.171875 ; top5 ->  87.625  and loss:  256.2781639099121
test acc: top1 ->  70.874 ; top5 ->  89.888  and loss:  951.6570179462433
forward train acc: top1 ->  70.0234375 ; top5 ->  87.609375  and loss:  253.55091524124146
test acc: top1 ->  70.984 ; top5 ->  89.904  and loss:  950.8990522623062
forward train acc: top1 ->  70.0078125 ; top5 ->  87.5078125  and loss:  253.16147351264954
test acc: top1 ->  71.012 ; top5 ->  89.942  and loss:  948.3089602589607
forward train acc: top1 ->  70.1015625 ; top5 ->  87.7265625  and loss:  250.30676293373108
test acc: top1 ->  71.058 ; top5 ->  89.912  and loss:  950.727630853653
forward train acc: top1 ->  69.8125 ; top5 ->  87.578125  and loss:  251.77561777830124
test acc: top1 ->  71.026 ; top5 ->  89.888  and loss:  947.5036019682884
forward train acc: top1 ->  69.859375 ; top5 ->  88.1015625  and loss:  248.06080967187881
test acc: top1 ->  71.022 ; top5 ->  89.908  and loss:  948.052786886692
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  43 / 64 , inc:  1
---------------- start layer  4  ---------------
### skip layer  4 wait:  4  ###
---------------- start layer  5  ---------------
adv train loss:  -60.88075894117355 , diff:  60.88075894117355
adv train loss:  -64.04128354787827 , diff:  3.160524606704712
layer  5  adv train finish, try to retain  26
test acc: top1 ->  66.598 ; top5 ->  87.074  and loss:  1098.7565445303917
forward train acc: top1 ->  70.1875 ; top5 ->  87.5625  and loss:  249.57119339704514
test acc: top1 ->  70.784 ; top5 ->  89.796  and loss:  955.5572080016136
forward train acc: top1 ->  70.0390625 ; top5 ->  87.3671875  and loss:  254.22569674253464
test acc: top1 ->  70.688 ; top5 ->  89.838  and loss:  950.8655175566673
forward train acc: top1 ->  69.90625 ; top5 ->  87.6015625  and loss:  251.91146701574326
test acc: top1 ->  70.836 ; top5 ->  89.826  and loss:  954.0537588596344
forward train acc: top1 ->  70.765625 ; top5 ->  88.1328125  and loss:  245.53478962183
test acc: top1 ->  70.946 ; top5 ->  89.974  and loss:  947.6815128326416
forward train acc: top1 ->  69.6484375 ; top5 ->  87.34375  and loss:  256.80528777837753
test acc: top1 ->  70.92 ; top5 ->  89.858  and loss:  949.0454763174057
forward train acc: top1 ->  70.09375 ; top5 ->  88.1640625  and loss:  248.15977746248245
test acc: top1 ->  70.952 ; top5 ->  89.84  and loss:  946.6227761507034
forward train acc: top1 ->  70.671875 ; top5 ->  88.2890625  and loss:  246.26482772827148
test acc: top1 ->  71.006 ; top5 ->  89.886  and loss:  947.1100937724113
forward train acc: top1 ->  70.203125 ; top5 ->  87.453125  and loss:  255.41221630573273
test acc: top1 ->  71.068 ; top5 ->  89.95  and loss:  945.9697304368019
forward train acc: top1 ->  70.578125 ; top5 ->  88.15625  and loss:  242.95573782920837
test acc: top1 ->  71.03 ; top5 ->  89.93  and loss:  942.4299673438072
forward train acc: top1 ->  70.359375 ; top5 ->  88.359375  and loss:  247.0078455209732
test acc: top1 ->  71.104 ; top5 ->  89.99  and loss:  942.3489755988121
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  29 / 64 , inc:  1
---------------- start layer  6  ---------------
### skip layer  6 wait:  4  ###
---------------- start layer  7  ---------------
adv train loss:  -62.82309538125992 , diff:  62.82309538125992
adv train loss:  -62.39325851202011 , diff:  0.42983686923980713
layer  7  adv train finish, try to retain  121
test acc: top1 ->  69.502 ; top5 ->  89.056  and loss:  996.484358727932
forward train acc: top1 ->  70.0 ; top5 ->  87.7109375  and loss:  250.50772953033447
test acc: top1 ->  70.95 ; top5 ->  89.872  and loss:  948.9396054148674
forward train acc: top1 ->  69.6484375 ; top5 ->  87.1640625  and loss:  258.5073512792587
test acc: top1 ->  70.994 ; top5 ->  89.892  and loss:  946.4908048510551
forward train acc: top1 ->  70.1015625 ; top5 ->  88.34375  and loss:  247.13822549581528
test acc: top1 ->  70.986 ; top5 ->  89.878  and loss:  947.0141965150833
forward train acc: top1 ->  70.578125 ; top5 ->  88.1875  and loss:  247.58571529388428
test acc: top1 ->  71.098 ; top5 ->  89.956  and loss:  943.2117999196053
forward train acc: top1 ->  69.9921875 ; top5 ->  88.0625  and loss:  247.61738550662994
test acc: top1 ->  71.212 ; top5 ->  89.954  and loss:  944.4849801659584
forward train acc: top1 ->  70.1953125 ; top5 ->  87.9375  and loss:  248.33797186613083
test acc: top1 ->  71.158 ; top5 ->  89.99  and loss:  940.4518088102341
forward train acc: top1 ->  69.8046875 ; top5 ->  87.96875  and loss:  251.9730607867241
test acc: top1 ->  71.236 ; top5 ->  89.986  and loss:  940.4213564991951
forward train acc: top1 ->  69.546875 ; top5 ->  87.890625  and loss:  253.3824811577797
test acc: top1 ->  71.262 ; top5 ->  90.048  and loss:  941.655916929245
forward train acc: top1 ->  70.75 ; top5 ->  88.4140625  and loss:  242.76401263475418
test acc: top1 ->  71.258 ; top5 ->  90.02  and loss:  938.4018578529358
forward train acc: top1 ->  70.5703125 ; top5 ->  88.2578125  and loss:  244.41728419065475
test acc: top1 ->  71.262 ; top5 ->  90.026  and loss:  938.2871594429016
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  122 / 128 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -62.796321392059326 , diff:  62.796321392059326
adv train loss:  -64.37140053510666 , diff:  1.5750791430473328
layer  8  adv train finish, try to retain  83
test acc: top1 ->  69.558 ; top5 ->  88.94  and loss:  1007.2660917043686
forward train acc: top1 ->  69.25 ; top5 ->  87.21875  and loss:  258.46283441782
test acc: top1 ->  70.822 ; top5 ->  89.798  and loss:  957.9545350670815
forward train acc: top1 ->  70.3515625 ; top5 ->  87.9453125  and loss:  249.8572798371315
test acc: top1 ->  70.836 ; top5 ->  89.842  and loss:  957.2762474417686
forward train acc: top1 ->  70.015625 ; top5 ->  87.9765625  and loss:  247.44796270132065
test acc: top1 ->  70.988 ; top5 ->  89.794  and loss:  952.671081840992
forward train acc: top1 ->  70.0859375 ; top5 ->  87.6171875  and loss:  250.86804431676865
test acc: top1 ->  70.996 ; top5 ->  89.85  and loss:  950.893224298954
forward train acc: top1 ->  70.6875 ; top5 ->  87.8125  and loss:  246.96187311410904
test acc: top1 ->  70.97 ; top5 ->  89.888  and loss:  950.4804729223251
forward train acc: top1 ->  70.4765625 ; top5 ->  87.8203125  and loss:  249.4385992884636
test acc: top1 ->  71.144 ; top5 ->  89.898  and loss:  948.0887505412102
forward train acc: top1 ->  69.9140625 ; top5 ->  87.6953125  and loss:  249.96210396289825
test acc: top1 ->  71.042 ; top5 ->  89.894  and loss:  946.9533945918083
forward train acc: top1 ->  69.3984375 ; top5 ->  87.40625  and loss:  255.80250918865204
test acc: top1 ->  71.166 ; top5 ->  89.962  and loss:  946.4480594396591
forward train acc: top1 ->  70.234375 ; top5 ->  87.6015625  and loss:  252.32499939203262
test acc: top1 ->  71.092 ; top5 ->  89.878  and loss:  946.9527961015701
forward train acc: top1 ->  70.1171875 ; top5 ->  88.0  and loss:  249.59376096725464
test acc: top1 ->  71.044 ; top5 ->  89.95  and loss:  943.4620614647865
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  9  ---------------
### skip layer  9 wait:  4  ###
---------------- start layer  10  ---------------
adv train loss:  -60.87552446126938 , diff:  60.87552446126938
adv train loss:  -63.19831746816635 , diff:  2.3227930068969727
layer  10  adv train finish, try to retain  71
test acc: top1 ->  69.13 ; top5 ->  88.864  and loss:  1016.388899564743
forward train acc: top1 ->  69.78125 ; top5 ->  88.0859375  and loss:  249.29184317588806
test acc: top1 ->  70.662 ; top5 ->  89.782  and loss:  957.6489931344986
forward train acc: top1 ->  69.2890625 ; top5 ->  87.1953125  and loss:  257.64663177728653
test acc: top1 ->  70.66 ; top5 ->  89.804  and loss:  955.7570494413376
forward train acc: top1 ->  70.03125 ; top5 ->  87.9453125  and loss:  253.67247939109802
test acc: top1 ->  70.696 ; top5 ->  89.852  and loss:  952.9982935786247
forward train acc: top1 ->  70.1953125 ; top5 ->  87.390625  and loss:  252.81713432073593
test acc: top1 ->  70.926 ; top5 ->  89.896  and loss:  949.3927266001701
forward train acc: top1 ->  69.8515625 ; top5 ->  87.8125  and loss:  250.49661654233932
test acc: top1 ->  70.922 ; top5 ->  89.844  and loss:  950.3296595811844
forward train acc: top1 ->  70.453125 ; top5 ->  87.9296875  and loss:  250.32038962841034
test acc: top1 ->  70.86 ; top5 ->  89.926  and loss:  947.0000284910202
forward train acc: top1 ->  70.078125 ; top5 ->  88.0  and loss:  250.42998230457306
test acc: top1 ->  70.934 ; top5 ->  89.906  and loss:  948.2317641973495
forward train acc: top1 ->  69.71875 ; top5 ->  87.6953125  and loss:  253.64331775903702
test acc: top1 ->  70.934 ; top5 ->  89.918  and loss:  946.6250281333923
forward train acc: top1 ->  69.3515625 ; top5 ->  87.734375  and loss:  257.41318345069885
test acc: top1 ->  71.094 ; top5 ->  89.908  and loss:  943.5511749386787
forward train acc: top1 ->  70.515625 ; top5 ->  88.2578125  and loss:  245.6812915802002
test acc: top1 ->  70.902 ; top5 ->  90.024  and loss:  944.1673399806023
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  11  ---------------
### skip layer  11 wait:  3  ###
---------------- start layer  12  ---------------
### skip layer  12 wait:  3  ###
---------------- start layer  13  ---------------
### skip layer  13 wait:  4  ###
---------------- start layer  14  ---------------
### skip layer  14 wait:  3  ###
---------------- start layer  15  ---------------
### skip layer  15 wait:  3  ###
---------------- start layer  16  ---------------
adv train loss:  -61.03200924396515 , diff:  61.03200924396515
adv train loss:  -59.47535616159439 , diff:  1.556653082370758
layer  16  adv train finish, try to retain  231
test acc: top1 ->  71.016 ; top5 ->  89.776  and loss:  948.469037771225
forward train acc: top1 ->  70.296875 ; top5 ->  87.734375  and loss:  249.1476125717163
test acc: top1 ->  71.088 ; top5 ->  89.898  and loss:  947.8823928833008
forward train acc: top1 ->  70.5234375 ; top5 ->  88.2421875  and loss:  245.78611838817596
test acc: top1 ->  71.012 ; top5 ->  89.904  and loss:  946.0129407644272
forward train acc: top1 ->  70.5390625 ; top5 ->  88.3359375  and loss:  244.9905180335045
test acc: top1 ->  71.18 ; top5 ->  89.954  and loss:  946.5278687477112
forward train acc: top1 ->  70.2734375 ; top5 ->  88.109375  and loss:  244.9859676361084
test acc: top1 ->  71.2 ; top5 ->  90.02  and loss:  940.3347052931786
forward train acc: top1 ->  70.2890625 ; top5 ->  87.9765625  and loss:  249.7155089378357
test acc: top1 ->  71.144 ; top5 ->  89.98  and loss:  943.403780400753
forward train acc: top1 ->  70.2265625 ; top5 ->  88.0625  and loss:  249.78595089912415
test acc: top1 ->  71.08 ; top5 ->  89.97  and loss:  944.6750326156616
forward train acc: top1 ->  70.7265625 ; top5 ->  88.109375  and loss:  247.6308016180992
test acc: top1 ->  71.184 ; top5 ->  89.992  and loss:  939.1386522650719
forward train acc: top1 ->  70.8046875 ; top5 ->  88.3828125  and loss:  245.09466153383255
test acc: top1 ->  71.272 ; top5 ->  90.036  and loss:  936.4890497326851
forward train acc: top1 ->  71.1796875 ; top5 ->  88.71875  and loss:  240.8538328409195
test acc: top1 ->  71.254 ; top5 ->  90.108  and loss:  939.7286137342453
forward train acc: top1 ->  70.390625 ; top5 ->  88.140625  and loss:  247.53931212425232
test acc: top1 ->  71.168 ; top5 ->  90.066  and loss:  937.9882405996323
>>>>>>> reverse layer  16  since performance drop >>>>>>>
==> this epoch:  239 / 256 , inc:  1
---------------- start layer  17  ---------------
adv train loss:  -59.56873494386673 , diff:  59.56873494386673
adv train loss:  -60.68121808767319 , diff:  1.1124831438064575
layer  17  adv train finish, try to retain  218
test acc: top1 ->  70.516 ; top5 ->  89.714  and loss:  961.2463198900223
forward train acc: top1 ->  70.7734375 ; top5 ->  88.0234375  and loss:  246.13452112674713
test acc: top1 ->  70.946 ; top5 ->  89.89  and loss:  949.6753078103065
forward train acc: top1 ->  69.734375 ; top5 ->  87.7734375  and loss:  255.15179961919785
test acc: top1 ->  70.894 ; top5 ->  89.922  and loss:  945.2746824622154
forward train acc: top1 ->  70.328125 ; top5 ->  88.0546875  and loss:  248.09103840589523
test acc: top1 ->  70.88 ; top5 ->  89.848  and loss:  945.1070115566254
forward train acc: top1 ->  70.8046875 ; top5 ->  88.0625  and loss:  246.8734748363495
test acc: top1 ->  71.136 ; top5 ->  89.932  and loss:  938.5628062486649
forward train acc: top1 ->  70.484375 ; top5 ->  88.3125  and loss:  244.54203873872757
test acc: top1 ->  71.254 ; top5 ->  89.928  and loss:  939.9738616943359
forward train acc: top1 ->  70.34375 ; top5 ->  87.9296875  and loss:  245.99589204788208
test acc: top1 ->  71.134 ; top5 ->  89.988  and loss:  941.0207623839378
forward train acc: top1 ->  70.453125 ; top5 ->  87.96875  and loss:  246.57961601018906
test acc: top1 ->  71.222 ; top5 ->  90.056  and loss:  939.0938929915428
forward train acc: top1 ->  69.859375 ; top5 ->  87.625  and loss:  252.97572702169418
test acc: top1 ->  71.206 ; top5 ->  90.03  and loss:  938.4979476332664
forward train acc: top1 ->  70.046875 ; top5 ->  87.859375  and loss:  249.10579162836075
test acc: top1 ->  71.236 ; top5 ->  90.038  and loss:  938.4832178354263
forward train acc: top1 ->  71.09375 ; top5 ->  88.421875  and loss:  241.92913818359375
test acc: top1 ->  71.268 ; top5 ->  90.042  and loss:  937.4927284121513
>>>>>>> reverse layer  17  since performance drop >>>>>>>
==> this epoch:  230 / 256 , inc:  1
---------------- start layer  18  ---------------
### skip layer  18 wait:  4  ###
---------------- start layer  19  ---------------
adv train loss:  -59.84593057632446 , diff:  59.84593057632446
adv train loss:  -65.31685054302216 , diff:  5.470919966697693
layer  19  adv train finish, try to retain  187
>>>>>>> reverse layer  19  since no improvement >>>>>>>
---------------- start layer  20  ---------------
adv train loss:  -60.131006836891174 , diff:  60.131006836891174
adv train loss:  -61.669021129608154 , diff:  1.53801429271698
layer  20  adv train finish, try to retain  225
>>>>>>> reverse layer  20  since no improvement >>>>>>>
---------------- start layer  21  ---------------
### skip layer  21 wait:  4  ###
---------------- start layer  22  ---------------
### skip layer  22 wait:  3  ###
---------------- start layer  23  ---------------
adv train loss:  -59.43243318796158 , diff:  59.43243318796158
adv train loss:  -62.227081060409546 , diff:  2.7946478724479675
layer  23  adv train finish, try to retain  179
test acc: top1 ->  70.69 ; top5 ->  89.744  and loss:  962.1126255989075
forward train acc: top1 ->  70.1953125 ; top5 ->  87.671875  and loss:  253.25124514102936
test acc: top1 ->  70.974 ; top5 ->  89.85  and loss:  947.7366080284119
forward train acc: top1 ->  70.3046875 ; top5 ->  88.1328125  and loss:  247.85128939151764
test acc: top1 ->  71.024 ; top5 ->  89.918  and loss:  946.5877302885056
forward train acc: top1 ->  70.015625 ; top5 ->  87.5234375  and loss:  253.87884455919266
test acc: top1 ->  70.896 ; top5 ->  89.864  and loss:  951.0812810659409
forward train acc: top1 ->  70.3515625 ; top5 ->  87.5859375  and loss:  251.07291793823242
test acc: top1 ->  70.968 ; top5 ->  89.854  and loss:  945.6168165802956
forward train acc: top1 ->  69.7734375 ; top5 ->  87.8671875  and loss:  250.97317612171173
test acc: top1 ->  71.084 ; top5 ->  89.864  and loss:  947.5501950383186
forward train acc: top1 ->  70.875 ; top5 ->  88.015625  and loss:  247.80109632015228
test acc: top1 ->  71.088 ; top5 ->  89.908  and loss:  945.3291178941727
forward train acc: top1 ->  70.578125 ; top5 ->  88.140625  and loss:  247.68116170167923
test acc: top1 ->  71.152 ; top5 ->  89.916  and loss:  945.6410936713219
forward train acc: top1 ->  70.1328125 ; top5 ->  87.9453125  and loss:  248.39182204008102
test acc: top1 ->  71.192 ; top5 ->  89.898  and loss:  944.5160955190659
forward train acc: top1 ->  69.9140625 ; top5 ->  87.9765625  and loss:  250.25713258981705
test acc: top1 ->  71.164 ; top5 ->  89.992  and loss:  940.6117922067642
forward train acc: top1 ->  69.7421875 ; top5 ->  88.109375  and loss:  248.81998318433762
test acc: top1 ->  71.248 ; top5 ->  90.014  and loss:  939.8229724168777
>>>>>>> reverse layer  23  since performance drop >>>>>>>
==> this epoch:  187 / 256 , inc:  1
---------------- start layer  24  ---------------
adv train loss:  -61.89170378446579 , diff:  61.89170378446579
adv train loss:  -61.24255919456482 , diff:  0.6491445899009705
layer  24  adv train finish, try to retain  234
test acc: top1 ->  70.986 ; top5 ->  89.86  and loss:  953.1756619811058
forward train acc: top1 ->  69.7734375 ; top5 ->  87.6953125  and loss:  253.55629116296768
test acc: top1 ->  70.88 ; top5 ->  89.976  and loss:  945.6947246193886
forward train acc: top1 ->  70.109375 ; top5 ->  88.1484375  and loss:  249.58872205018997
test acc: top1 ->  71.004 ; top5 ->  89.936  and loss:  944.582177221775
forward train acc: top1 ->  70.265625 ; top5 ->  87.9375  and loss:  247.8380590081215
test acc: top1 ->  71.092 ; top5 ->  90.01  and loss:  941.9207509160042
forward train acc: top1 ->  70.7109375 ; top5 ->  87.96875  and loss:  245.64201909303665
test acc: top1 ->  71.188 ; top5 ->  90.002  and loss:  943.347526371479
forward train acc: top1 ->  69.8671875 ; top5 ->  87.4765625  and loss:  252.4028912782669
test acc: top1 ->  71.182 ; top5 ->  90.034  and loss:  940.1328068375587
forward train acc: top1 ->  70.5 ; top5 ->  87.9453125  and loss:  247.0238783955574
test acc: top1 ->  71.18 ; top5 ->  90.044  and loss:  941.5503429174423
forward train acc: top1 ->  70.9921875 ; top5 ->  88.375  and loss:  243.8635927438736
test acc: top1 ->  71.188 ; top5 ->  90.022  and loss:  941.5728414654732
forward train acc: top1 ->  70.4375 ; top5 ->  88.3515625  and loss:  247.59909707307816
test acc: top1 ->  71.186 ; top5 ->  90.024  and loss:  941.1239865422249
forward train acc: top1 ->  69.8828125 ; top5 ->  87.875  and loss:  249.0885414481163
test acc: top1 ->  71.194 ; top5 ->  89.984  and loss:  942.6869088411331
forward train acc: top1 ->  70.7890625 ; top5 ->  88.296875  and loss:  246.33801192045212
test acc: top1 ->  71.288 ; top5 ->  90.076  and loss:  939.4564261436462
>>>>>>> reverse layer  24  since performance drop >>>>>>>
==> this epoch:  244 / 256 , inc:  1
---------------- start layer  25  ---------------
adv train loss:  -61.170701920986176 , diff:  61.170701920986176
adv train loss:  -59.86314004659653 , diff:  1.3075618743896484
layer  25  adv train finish, try to retain  244
>>>>>>> reverse layer  25  since no improvement >>>>>>>
---------------- start layer  26  ---------------
### skip layer  26 wait:  3  ###
---------------- start layer  27  ---------------
adv train loss:  -62.09900343418121 , diff:  62.09900343418121
adv train loss:  -63.81143069267273 , diff:  1.7124272584915161
layer  27  adv train finish, try to retain  505
>>>>>>> reverse layer  27  since no improvement >>>>>>>
---------------- start layer  28  ---------------
### skip layer  28 wait:  3  ###
---------------- start layer  29  ---------------
### skip layer  29 wait:  4  ###
---------------- start layer  30  ---------------
### skip layer  30 wait:  3  ###
---------------- start layer  31  ---------------
### skip layer  31 wait:  3  ###
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.04805419921875001, 0.04805419921875001, 0.00084470272064209, 0.04805419921875001, 0.009010162353515627, 0.19221679687500004, 0.000422351360321045, 0.0002111756801605225, 0.009010162353515627, 0.000422351360321045, 0.009010162353515627, 0.0022525405883789067, 0.0022525405883789067, 0.04805419921875001, 0.00010558784008026125, 0.00010558784008026125, 0.000422351360321045, 0.000422351360321045, 0.0002111756801605225, 0.012013549804687502, 0.0011262702941894534, 0.0002111756801605225, 0.0011262702941894534, 0.0045050811767578134, 0.0002111756801605225, 0.0005631351470947267, 5.279392004013062e-05, 5.279392004013062e-05, 5.279392004013062e-05, 0.00010558784008026125, 5.279392004013062e-05, 5.279392004013062e-05]  wait [2, 2, 3, 2, 3, 4, 3, 2, 4, 3, 4, 2, 2, 3, 2, 2, 4, 4, 3, 2, 2, 3, 2, 4, 3, 1, 2, 2, 2, 3, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 6
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  25  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -61.572006046772 , diff:  61.572006046772
adv train loss:  -60.65641701221466 , diff:  0.9155890345573425
layer  0  adv train finish, try to retain  29
test acc: top1 ->  64.154 ; top5 ->  85.202  and loss:  1223.2004307508469
forward train acc: top1 ->  69.03125 ; top5 ->  87.234375  and loss:  259.93027329444885
test acc: top1 ->  70.668 ; top5 ->  89.578  and loss:  959.0970988869667
forward train acc: top1 ->  69.28125 ; top5 ->  87.359375  and loss:  256.41943019628525
test acc: top1 ->  70.904 ; top5 ->  89.68  and loss:  952.9278447628021
forward train acc: top1 ->  70.546875 ; top5 ->  88.1484375  and loss:  248.9680569767952
test acc: top1 ->  70.812 ; top5 ->  89.784  and loss:  957.4269417524338
forward train acc: top1 ->  69.8671875 ; top5 ->  87.796875  and loss:  253.6703903079033
test acc: top1 ->  70.776 ; top5 ->  89.838  and loss:  953.4592002034187
forward train acc: top1 ->  70.9609375 ; top5 ->  88.28125  and loss:  242.93965649604797
test acc: top1 ->  70.976 ; top5 ->  89.818  and loss:  951.9856004118919
forward train acc: top1 ->  70.4140625 ; top5 ->  87.8203125  and loss:  248.94871884584427
test acc: top1 ->  70.81 ; top5 ->  89.824  and loss:  952.7463113069534
forward train acc: top1 ->  70.2734375 ; top5 ->  88.078125  and loss:  248.2872794866562
test acc: top1 ->  70.874 ; top5 ->  89.904  and loss:  948.6276580691338
forward train acc: top1 ->  70.1953125 ; top5 ->  87.765625  and loss:  249.43294537067413
test acc: top1 ->  70.968 ; top5 ->  89.88  and loss:  946.1744592189789
forward train acc: top1 ->  70.28125 ; top5 ->  88.1171875  and loss:  247.65770089626312
test acc: top1 ->  70.996 ; top5 ->  89.912  and loss:  947.2244902253151
forward train acc: top1 ->  70.3515625 ; top5 ->  87.828125  and loss:  250.47125327587128
test acc: top1 ->  70.936 ; top5 ->  89.926  and loss:  948.3562908768654
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  37 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -65.25582194328308 , diff:  65.25582194328308
adv train loss:  -60.852814733982086 , diff:  4.403007209300995
layer  1  adv train finish, try to retain  40
test acc: top1 ->  67.28 ; top5 ->  87.386  and loss:  1086.4885982871056
forward train acc: top1 ->  69.5859375 ; top5 ->  87.34375  and loss:  257.2194580435753
test acc: top1 ->  70.786 ; top5 ->  89.844  and loss:  953.4706610441208
forward train acc: top1 ->  69.9921875 ; top5 ->  87.9140625  and loss:  249.314118206501
test acc: top1 ->  70.932 ; top5 ->  89.856  and loss:  951.8656656742096
forward train acc: top1 ->  69.5390625 ; top5 ->  87.71875  and loss:  254.26735454797745
test acc: top1 ->  70.984 ; top5 ->  89.826  and loss:  954.8326178789139
forward train acc: top1 ->  69.1640625 ; top5 ->  87.375  and loss:  255.6392492055893
test acc: top1 ->  71.074 ; top5 ->  89.924  and loss:  948.3122327923775
forward train acc: top1 ->  70.8984375 ; top5 ->  87.5625  and loss:  247.83922827243805
test acc: top1 ->  71.078 ; top5 ->  89.964  and loss:  944.9949285984039
forward train acc: top1 ->  69.8984375 ; top5 ->  87.7265625  and loss:  248.70549547672272
test acc: top1 ->  71.146 ; top5 ->  89.88  and loss:  946.6118097305298
forward train acc: top1 ->  70.8203125 ; top5 ->  88.375  and loss:  244.11056369543076
test acc: top1 ->  71.05 ; top5 ->  89.92  and loss:  945.393440246582
forward train acc: top1 ->  70.0546875 ; top5 ->  87.6953125  and loss:  250.85423344373703
test acc: top1 ->  71.066 ; top5 ->  89.976  and loss:  945.200569152832
forward train acc: top1 ->  69.828125 ; top5 ->  87.78125  and loss:  251.3261637687683
test acc: top1 ->  71.066 ; top5 ->  89.942  and loss:  943.8080413937569
forward train acc: top1 ->  70.2265625 ; top5 ->  88.1953125  and loss:  247.67485415935516
test acc: top1 ->  71.032 ; top5 ->  89.988  and loss:  942.3513389825821
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  44 / 64 , inc:  1
---------------- start layer  2  ---------------
### skip layer  2 wait:  3  ###
---------------- start layer  3  ---------------
adv train loss:  -63.457064151763916 , diff:  63.457064151763916
adv train loss:  -63.20650815963745 , diff:  0.25055599212646484
layer  3  adv train finish, try to retain  34
test acc: top1 ->  62.666 ; top5 ->  84.808  and loss:  1236.2196412086487
forward train acc: top1 ->  68.8203125 ; top5 ->  87.25  and loss:  260.1315692663193
test acc: top1 ->  70.35 ; top5 ->  89.61  and loss:  967.6401619315147
forward train acc: top1 ->  69.6015625 ; top5 ->  87.40625  and loss:  255.77875238656998
test acc: top1 ->  70.308 ; top5 ->  89.516  and loss:  966.7116938829422
forward train acc: top1 ->  69.5390625 ; top5 ->  87.8125  and loss:  252.48373091220856
test acc: top1 ->  70.446 ; top5 ->  89.59  and loss:  965.979758143425
forward train acc: top1 ->  69.828125 ; top5 ->  87.9296875  and loss:  252.0662146806717
test acc: top1 ->  70.388 ; top5 ->  89.596  and loss:  966.2855771183968
forward train acc: top1 ->  69.59375 ; top5 ->  87.5  and loss:  255.84485000371933
test acc: top1 ->  70.586 ; top5 ->  89.72  and loss:  960.7291242480278
forward train acc: top1 ->  69.1015625 ; top5 ->  87.4140625  and loss:  256.83702486753464
test acc: top1 ->  70.61 ; top5 ->  89.766  and loss:  954.1990245580673
forward train acc: top1 ->  69.046875 ; top5 ->  87.5078125  and loss:  256.0665656924248
test acc: top1 ->  70.656 ; top5 ->  89.772  and loss:  956.6861791014671
forward train acc: top1 ->  70.140625 ; top5 ->  88.21875  and loss:  247.35391569137573
test acc: top1 ->  70.71 ; top5 ->  89.744  and loss:  956.2567279338837
forward train acc: top1 ->  70.4140625 ; top5 ->  87.921875  and loss:  247.91100591421127
test acc: top1 ->  70.666 ; top5 ->  89.818  and loss:  952.9328615665436
forward train acc: top1 ->  70.015625 ; top5 ->  87.703125  and loss:  252.85052800178528
test acc: top1 ->  70.61 ; top5 ->  89.722  and loss:  952.6537424325943
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  43 / 64 , inc:  1
---------------- start layer  4  ---------------
### skip layer  4 wait:  3  ###
---------------- start layer  5  ---------------
### skip layer  5 wait:  4  ###
---------------- start layer  6  ---------------
### skip layer  6 wait:  3  ###
---------------- start layer  7  ---------------
adv train loss:  -61.64332073926926 , diff:  61.64332073926926
adv train loss:  -62.280874371528625 , diff:  0.6375536322593689
layer  7  adv train finish, try to retain  122
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
### skip layer  8 wait:  4  ###
---------------- start layer  9  ---------------
### skip layer  9 wait:  3  ###
---------------- start layer  10  ---------------
### skip layer  10 wait:  4  ###
---------------- start layer  11  ---------------
adv train loss:  -61.18667387962341 , diff:  61.18667387962341
adv train loss:  -61.5654137134552 , diff:  0.3787398338317871
layer  11  adv train finish, try to retain  102
>>>>>>> reverse layer  11  since no improvement >>>>>>>
---------------- start layer  12  ---------------
adv train loss:  -60.69281816482544 , diff:  60.69281816482544
adv train loss:  -59.58727842569351 , diff:  1.1055397391319275
layer  12  adv train finish, try to retain  96
test acc: top1 ->  69.914 ; top5 ->  89.288  and loss:  988.1276898980141
forward train acc: top1 ->  69.5078125 ; top5 ->  87.9140625  and loss:  251.523552775383
test acc: top1 ->  70.926 ; top5 ->  89.784  and loss:  951.2277334928513
forward train acc: top1 ->  70.375 ; top5 ->  87.8515625  and loss:  247.02490508556366
test acc: top1 ->  70.95 ; top5 ->  89.818  and loss:  948.5674636363983
forward train acc: top1 ->  70.1796875 ; top5 ->  87.6796875  and loss:  251.4657239317894
test acc: top1 ->  71.02 ; top5 ->  89.846  and loss:  946.5689015984535
forward train acc: top1 ->  70.0078125 ; top5 ->  87.625  and loss:  250.16898292303085
test acc: top1 ->  70.938 ; top5 ->  89.786  and loss:  948.903480052948
forward train acc: top1 ->  70.453125 ; top5 ->  87.5078125  and loss:  250.64297318458557
test acc: top1 ->  70.924 ; top5 ->  89.844  and loss:  949.3804969191551
forward train acc: top1 ->  70.3828125 ; top5 ->  88.1328125  and loss:  245.44476091861725
test acc: top1 ->  71.008 ; top5 ->  89.906  and loss:  947.9537241458893
forward train acc: top1 ->  70.625 ; top5 ->  87.546875  and loss:  248.87757748365402
test acc: top1 ->  71.122 ; top5 ->  89.95  and loss:  945.2402793169022
forward train acc: top1 ->  70.515625 ; top5 ->  88.3203125  and loss:  245.251866877079
test acc: top1 ->  71.112 ; top5 ->  89.9  and loss:  945.5411307215691
forward train acc: top1 ->  70.2421875 ; top5 ->  88.1875  and loss:  247.59002524614334
test acc: top1 ->  71.048 ; top5 ->  89.87  and loss:  948.014385998249
forward train acc: top1 ->  70.5390625 ; top5 ->  88.1796875  and loss:  244.3371523618698
test acc: top1 ->  71.224 ; top5 ->  89.918  and loss:  941.9795303940773
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  98 / 128 , inc:  1
---------------- start layer  13  ---------------
### skip layer  13 wait:  3  ###
---------------- start layer  14  ---------------
adv train loss:  -64.41312980651855 , diff:  64.41312980651855
adv train loss:  -57.81319838762283 , diff:  6.599931418895721
layer  14  adv train finish, try to retain  243
test acc: top1 ->  70.552 ; top5 ->  89.576  and loss:  959.7465296387672
forward train acc: top1 ->  70.1328125 ; top5 ->  87.734375  and loss:  250.4873769879341
test acc: top1 ->  70.984 ; top5 ->  89.888  and loss:  950.1647252440453
forward train acc: top1 ->  69.9375 ; top5 ->  88.21875  and loss:  247.93983399868011
test acc: top1 ->  70.96 ; top5 ->  89.958  and loss:  946.4813759922981
forward train acc: top1 ->  70.3671875 ; top5 ->  88.109375  and loss:  248.6292558312416
test acc: top1 ->  71.0 ; top5 ->  89.928  and loss:  942.3117184042931
forward train acc: top1 ->  70.3515625 ; top5 ->  88.1015625  and loss:  245.72171133756638
test acc: top1 ->  71.152 ; top5 ->  90.014  and loss:  939.4654892683029
forward train acc: top1 ->  71.015625 ; top5 ->  88.28125  and loss:  245.1153306365013
test acc: top1 ->  71.08 ; top5 ->  90.08  and loss:  936.4718516469002
forward train acc: top1 ->  70.2890625 ; top5 ->  87.859375  and loss:  247.16585117578506
test acc: top1 ->  71.232 ; top5 ->  90.078  and loss:  938.5078399181366
forward train acc: top1 ->  71.046875 ; top5 ->  88.3515625  and loss:  242.9099685549736
test acc: top1 ->  71.088 ; top5 ->  90.104  and loss:  936.6788217425346
forward train acc: top1 ->  70.3515625 ; top5 ->  88.0  and loss:  248.9806632399559
test acc: top1 ->  71.29 ; top5 ->  90.154  and loss:  936.6732024550438
forward train acc: top1 ->  70.75 ; top5 ->  88.046875  and loss:  243.01545786857605
test acc: top1 ->  71.246 ; top5 ->  90.042  and loss:  937.638752579689
forward train acc: top1 ->  70.6015625 ; top5 ->  87.8828125  and loss:  246.73639953136444
test acc: top1 ->  71.24 ; top5 ->  90.048  and loss:  935.5908762216568
>>>>>>> reverse layer  14  since performance drop >>>>>>>
==> this epoch:  250 / 256 , inc:  1
---------------- start layer  15  ---------------
adv train loss:  -62.85032516717911 , diff:  62.85032516717911
adv train loss:  -62.44119322299957 , diff:  0.4091319441795349
layer  15  adv train finish, try to retain  248
>>>>>>> reverse layer  15  since no improvement >>>>>>>
---------------- start layer  16  ---------------
### skip layer  16 wait:  4  ###
---------------- start layer  17  ---------------
### skip layer  17 wait:  4  ###
---------------- start layer  18  ---------------
### skip layer  18 wait:  3  ###
---------------- start layer  19  ---------------
adv train loss:  -61.31983369588852 , diff:  61.31983369588852
adv train loss:  -59.73142218589783 , diff:  1.5884115099906921
layer  19  adv train finish, try to retain  159
test acc: top1 ->  70.514 ; top5 ->  89.71  and loss:  958.4450739622116
forward train acc: top1 ->  69.8671875 ; top5 ->  87.3828125  and loss:  252.87371844053268
test acc: top1 ->  70.654 ; top5 ->  89.744  and loss:  957.7073931097984
forward train acc: top1 ->  69.578125 ; top5 ->  87.5625  and loss:  253.7873918414116
test acc: top1 ->  70.812 ; top5 ->  89.85  and loss:  952.8182248473167
forward train acc: top1 ->  70.328125 ; top5 ->  88.015625  and loss:  250.48068207502365
test acc: top1 ->  70.784 ; top5 ->  89.856  and loss:  952.755040705204
forward train acc: top1 ->  69.5625 ; top5 ->  87.5703125  and loss:  255.08966201543808
test acc: top1 ->  70.836 ; top5 ->  89.87  and loss:  950.0511135458946
forward train acc: top1 ->  69.828125 ; top5 ->  87.7734375  and loss:  251.55140733718872
test acc: top1 ->  70.784 ; top5 ->  89.876  and loss:  952.249770462513
forward train acc: top1 ->  69.3828125 ; top5 ->  87.5  and loss:  255.89541482925415
test acc: top1 ->  70.834 ; top5 ->  89.94  and loss:  949.874805867672
forward train acc: top1 ->  70.109375 ; top5 ->  87.8046875  and loss:  246.7276752591133
test acc: top1 ->  70.858 ; top5 ->  89.952  and loss:  949.4453259110451
forward train acc: top1 ->  70.4765625 ; top5 ->  88.34375  and loss:  245.36362379789352
test acc: top1 ->  70.884 ; top5 ->  89.912  and loss:  948.0300452709198
forward train acc: top1 ->  69.3046875 ; top5 ->  87.90625  and loss:  251.89947098493576
test acc: top1 ->  70.886 ; top5 ->  90.006  and loss:  949.5362984538078
forward train acc: top1 ->  70.8203125 ; top5 ->  88.046875  and loss:  245.60860151052475
test acc: top1 ->  70.974 ; top5 ->  90.036  and loss:  944.081981420517
>>>>>>> reverse layer  19  since performance drop >>>>>>>
==> this epoch:  179 / 256 , inc:  1
---------------- start layer  20  ---------------
adv train loss:  -62.41092598438263 , diff:  62.41092598438263
adv train loss:  -62.759718239307404 , diff:  0.34879225492477417
layer  20  adv train finish, try to retain  217
test acc: top1 ->  70.956 ; top5 ->  89.838  and loss:  955.59319460392
forward train acc: top1 ->  69.2109375 ; top5 ->  87.6015625  and loss:  256.63075441122055
test acc: top1 ->  70.996 ; top5 ->  89.904  and loss:  945.3655841946602
forward train acc: top1 ->  70.53125 ; top5 ->  88.7578125  and loss:  243.17247784137726
test acc: top1 ->  70.95 ; top5 ->  89.904  and loss:  943.7508015036583
forward train acc: top1 ->  70.4921875 ; top5 ->  88.15625  and loss:  248.05806732177734
test acc: top1 ->  71.098 ; top5 ->  89.932  and loss:  944.4645390510559
forward train acc: top1 ->  69.7421875 ; top5 ->  88.0  and loss:  251.01359617710114
test acc: top1 ->  71.094 ; top5 ->  89.848  and loss:  943.8407607078552
forward train acc: top1 ->  69.9453125 ; top5 ->  87.796875  and loss:  251.6059266924858
test acc: top1 ->  71.154 ; top5 ->  89.962  and loss:  943.2464802861214
forward train acc: top1 ->  70.96875 ; top5 ->  88.828125  and loss:  239.01546400785446
test acc: top1 ->  71.116 ; top5 ->  89.97  and loss:  943.988231420517
forward train acc: top1 ->  70.3515625 ; top5 ->  88.0859375  and loss:  249.64286243915558
test acc: top1 ->  71.21 ; top5 ->  90.036  and loss:  943.3861773610115
forward train acc: top1 ->  70.2578125 ; top5 ->  87.953125  and loss:  248.0004625916481
test acc: top1 ->  71.232 ; top5 ->  90.06  and loss:  939.7767597436905
forward train acc: top1 ->  69.7734375 ; top5 ->  87.875  and loss:  248.04738813638687
test acc: top1 ->  71.252 ; top5 ->  90.046  and loss:  938.5288208127022
forward train acc: top1 ->  70.7578125 ; top5 ->  88.3125  and loss:  243.81607776880264
test acc: top1 ->  71.158 ; top5 ->  89.984  and loss:  937.7830880284309
>>>>>>> reverse layer  20  since performance drop >>>>>>>
==> this epoch:  225 / 256 , inc:  1
---------------- start layer  21  ---------------
### skip layer  21 wait:  3  ###
---------------- start layer  22  ---------------
adv train loss:  -61.598762929439545 , diff:  61.598762929439545
adv train loss:  -61.30491232872009 , diff:  0.2938506007194519
layer  22  adv train finish, try to retain  216
>>>>>>> reverse layer  22  since no improvement >>>>>>>
---------------- start layer  23  ---------------
### skip layer  23 wait:  4  ###
---------------- start layer  24  ---------------
### skip layer  24 wait:  3  ###
---------------- start layer  25  ---------------
adv train loss:  -65.37166184186935 , diff:  65.37166184186935
adv train loss:  -61.779123067855835 , diff:  3.5925387740135193
layer  25  adv train finish, try to retain  231
test acc: top1 ->  70.66 ; top5 ->  89.83  and loss:  953.7840505242348
forward train acc: top1 ->  69.7265625 ; top5 ->  87.2265625  and loss:  258.6493162512779
test acc: top1 ->  71.006 ; top5 ->  89.902  and loss:  947.967439353466
forward train acc: top1 ->  70.546875 ; top5 ->  88.2421875  and loss:  248.22638148069382
test acc: top1 ->  71.08 ; top5 ->  89.92  and loss:  944.5396837592125
forward train acc: top1 ->  70.1640625 ; top5 ->  88.015625  and loss:  252.9653079509735
test acc: top1 ->  71.092 ; top5 ->  89.88  and loss:  946.169002354145
forward train acc: top1 ->  69.9609375 ; top5 ->  88.1328125  and loss:  248.67007690668106
test acc: top1 ->  71.192 ; top5 ->  89.9  and loss:  942.9090579748154
forward train acc: top1 ->  70.2265625 ; top5 ->  87.75  and loss:  247.81526070833206
test acc: top1 ->  71.272 ; top5 ->  90.024  and loss:  941.989170730114
forward train acc: top1 ->  70.546875 ; top5 ->  88.2265625  and loss:  244.01487851142883
test acc: top1 ->  71.216 ; top5 ->  89.952  and loss:  942.2830790281296
forward train acc: top1 ->  70.453125 ; top5 ->  88.09375  and loss:  248.22046262025833
test acc: top1 ->  71.268 ; top5 ->  90.026  and loss:  939.8493012189865
forward train acc: top1 ->  70.1328125 ; top5 ->  88.0546875  and loss:  247.66980892419815
test acc: top1 ->  71.374 ; top5 ->  90.07  and loss:  935.6606073379517
forward train acc: top1 ->  70.21875 ; top5 ->  88.6328125  and loss:  244.60754084587097
test acc: top1 ->  71.23 ; top5 ->  90.004  and loss:  940.1510188579559
forward train acc: top1 ->  70.2734375 ; top5 ->  87.5703125  and loss:  247.73626273870468
test acc: top1 ->  71.254 ; top5 ->  89.972  and loss:  938.2087688446045
>>>>>>> reverse layer  25  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  26  ---------------
adv train loss:  -64.31617867946625 , diff:  64.31617867946625
adv train loss:  -62.939461171627045 , diff:  1.3767175078392029
layer  26  adv train finish, try to retain  504
>>>>>>> reverse layer  26  since no improvement >>>>>>>
---------------- start layer  27  ---------------
adv train loss:  -61.24762308597565 , diff:  61.24762308597565
adv train loss:  -61.1193670630455 , diff:  0.12825602293014526
layer  27  adv train finish, try to retain  504
test acc: top1 ->  71.164 ; top5 ->  89.888  and loss:  943.518560230732
forward train acc: top1 ->  70.953125 ; top5 ->  88.046875  and loss:  244.07641685009003
test acc: top1 ->  71.136 ; top5 ->  89.954  and loss:  944.220571577549
forward train acc: top1 ->  70.4140625 ; top5 ->  87.9921875  and loss:  248.71750903129578
test acc: top1 ->  71.026 ; top5 ->  89.906  and loss:  945.2333993315697
forward train acc: top1 ->  70.6328125 ; top5 ->  88.3203125  and loss:  244.91033148765564
test acc: top1 ->  71.14 ; top5 ->  89.944  and loss:  943.9934837222099
forward train acc: top1 ->  70.09375 ; top5 ->  88.3359375  and loss:  247.85243648290634
test acc: top1 ->  71.214 ; top5 ->  89.946  and loss:  939.5097458362579
forward train acc: top1 ->  70.890625 ; top5 ->  88.4140625  and loss:  243.01918584108353
test acc: top1 ->  71.218 ; top5 ->  89.98  and loss:  938.5106346011162
forward train acc: top1 ->  69.875 ; top5 ->  87.8515625  and loss:  248.3493782877922
test acc: top1 ->  71.19 ; top5 ->  90.048  and loss:  941.0086894631386
forward train acc: top1 ->  70.84375 ; top5 ->  88.3046875  and loss:  243.48109132051468
test acc: top1 ->  71.36 ; top5 ->  90.052  and loss:  937.3338550329208
forward train acc: top1 ->  70.2109375 ; top5 ->  87.9140625  and loss:  246.3482547402382
test acc: top1 ->  71.272 ; top5 ->  90.036  and loss:  939.1027961373329
forward train acc: top1 ->  70.578125 ; top5 ->  88.1015625  and loss:  247.9581463932991
test acc: top1 ->  71.298 ; top5 ->  90.024  and loss:  940.2501895427704
forward train acc: top1 ->  70.4296875 ; top5 ->  88.0625  and loss:  245.6332186460495
test acc: top1 ->  71.348 ; top5 ->  90.044  and loss:  936.7320898771286
>>>>>>> reverse layer  27  since performance drop >>>>>>>
==> this epoch:  505 / 512 , inc:  1
---------------- start layer  28  ---------------
adv train loss:  -60.464344561100006 , diff:  60.464344561100006
adv train loss:  -61.13003200292587 , diff:  0.6656874418258667
layer  28  adv train finish, try to retain  501
test acc: top1 ->  71.358 ; top5 ->  90.056  and loss:  936.1475375890732
forward train acc: top1 ->  70.1796875 ; top5 ->  88.5078125  and loss:  244.22607672214508
test acc: top1 ->  71.236 ; top5 ->  89.986  and loss:  946.486930847168
forward train acc: top1 ->  69.921875 ; top5 ->  87.8671875  and loss:  250.31605225801468
test acc: top1 ->  71.124 ; top5 ->  89.974  and loss:  943.2692305445671
forward train acc: top1 ->  70.328125 ; top5 ->  87.8125  and loss:  249.01635015010834
test acc: top1 ->  71.088 ; top5 ->  89.966  and loss:  945.6422609686852
forward train acc: top1 ->  70.359375 ; top5 ->  87.90625  and loss:  250.92080128192902
test acc: top1 ->  71.206 ; top5 ->  90.048  and loss:  941.468635737896
forward train acc: top1 ->  70.296875 ; top5 ->  87.8046875  and loss:  247.97491359710693
test acc: top1 ->  71.232 ; top5 ->  90.028  and loss:  941.9457504153252
forward train acc: top1 ->  70.984375 ; top5 ->  88.0859375  and loss:  243.7004216313362
test acc: top1 ->  71.128 ; top5 ->  90.086  and loss:  939.2977235913277
forward train acc: top1 ->  70.2421875 ; top5 ->  88.140625  and loss:  248.22704756259918
test acc: top1 ->  71.164 ; top5 ->  90.038  and loss:  937.9671922922134
forward train acc: top1 ->  70.4765625 ; top5 ->  87.859375  and loss:  248.60926759243011
test acc: top1 ->  71.284 ; top5 ->  90.1  and loss:  937.380158662796
forward train acc: top1 ->  70.4375 ; top5 ->  88.125  and loss:  245.59449064731598
test acc: top1 ->  71.226 ; top5 ->  90.052  and loss:  937.9647672176361
forward train acc: top1 ->  70.1015625 ; top5 ->  87.7890625  and loss:  254.31498378515244
test acc: top1 ->  71.2 ; top5 ->  90.084  and loss:  937.6878347992897
>>>>>>> reverse layer  28  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  29  ---------------
### skip layer  29 wait:  3  ###
---------------- start layer  30  ---------------
adv train loss:  -59.518269538879395 , diff:  59.518269538879395
adv train loss:  -58.43749874830246 , diff:  1.0807707905769348
layer  30  adv train finish, try to retain  503
>>>>>>> reverse layer  30  since no improvement >>>>>>>
---------------- start layer  31  ---------------
adv train loss:  -59.41121435165405 , diff:  59.41121435165405
adv train loss:  -60.97534167766571 , diff:  1.5641273260116577
layer  31  adv train finish, try to retain  503
>>>>>>> reverse layer  31  since no improvement >>>>>>>
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.03604064941406251, 0.03604064941406251, 0.00084470272064209, 0.03604064941406251, 0.009010162353515627, 0.19221679687500004, 0.000422351360321045, 0.000422351360321045, 0.009010162353515627, 0.000422351360321045, 0.009010162353515627, 0.0045050811767578134, 0.00168940544128418, 0.04805419921875001, 7.919088006019593e-05, 0.0002111756801605225, 0.000422351360321045, 0.000422351360321045, 0.0002111756801605225, 0.009010162353515627, 0.00084470272064209, 0.0002111756801605225, 0.0022525405883789067, 0.0045050811767578134, 0.0002111756801605225, 0.000422351360321045, 0.00010558784008026125, 3.959544003009797e-05, 3.959544003009797e-05, 0.00010558784008026125, 0.00010558784008026125, 0.00010558784008026125]  wait [4, 4, 2, 4, 2, 3, 2, 2, 3, 2, 3, 2, 4, 2, 4, 2, 3, 3, 2, 4, 4, 2, 2, 3, 2, 3, 2, 4, 4, 2, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 6
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  26  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -60.92174917459488 , diff:  60.92174917459488
adv train loss:  -62.59799671173096 , diff:  1.6762475371360779
layer  0  adv train finish, try to retain  39
>>>>>>> reverse layer  0  since no improvement >>>>>>>
---------------- start layer  1  ---------------
adv train loss:  -60.21059864759445 , diff:  60.21059864759445
adv train loss:  -60.052284359931946 , diff:  0.1583142876625061
layer  1  adv train finish, try to retain  38
test acc: top1 ->  60.644 ; top5 ->  82.41  and loss:  1346.7689244747162
forward train acc: top1 ->  68.6484375 ; top5 ->  87.0625  and loss:  262.7240107059479
test acc: top1 ->  70.63 ; top5 ->  89.604  and loss:  962.2185481786728
forward train acc: top1 ->  69.484375 ; top5 ->  87.5625  and loss:  257.00292229652405
test acc: top1 ->  70.71 ; top5 ->  89.7  and loss:  962.3717402219772
forward train acc: top1 ->  69.9296875 ; top5 ->  87.46875  and loss:  254.3250049352646
test acc: top1 ->  70.712 ; top5 ->  89.774  and loss:  955.7050946950912
forward train acc: top1 ->  69.9609375 ; top5 ->  87.96875  and loss:  248.47945600748062
test acc: top1 ->  70.82 ; top5 ->  89.76  and loss:  953.4706771969795
forward train acc: top1 ->  70.6796875 ; top5 ->  88.1015625  and loss:  248.1945434808731
test acc: top1 ->  70.812 ; top5 ->  89.808  and loss:  952.0188735127449
forward train acc: top1 ->  70.046875 ; top5 ->  88.03125  and loss:  248.1369590163231
test acc: top1 ->  70.862 ; top5 ->  89.818  and loss:  952.1282284259796
forward train acc: top1 ->  70.28125 ; top5 ->  88.1171875  and loss:  248.12726110219955
test acc: top1 ->  70.908 ; top5 ->  89.848  and loss:  950.8595262765884
forward train acc: top1 ->  69.1953125 ; top5 ->  87.6015625  and loss:  257.2278191447258
test acc: top1 ->  71.008 ; top5 ->  89.972  and loss:  946.763393163681
forward train acc: top1 ->  69.8359375 ; top5 ->  88.015625  and loss:  251.26853185892105
test acc: top1 ->  70.996 ; top5 ->  89.868  and loss:  949.8684690594673
forward train acc: top1 ->  70.125 ; top5 ->  87.734375  and loss:  251.2375367283821
test acc: top1 ->  71.014 ; top5 ->  89.892  and loss:  946.6928564310074
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  44 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -64.47583854198456 , diff:  64.47583854198456
adv train loss:  -62.41032803058624 , diff:  2.0655105113983154
layer  2  adv train finish, try to retain  56
test acc: top1 ->  70.28 ; top5 ->  89.494  and loss:  972.3153573274612
forward train acc: top1 ->  69.4765625 ; top5 ->  87.8359375  and loss:  254.12176340818405
test acc: top1 ->  71.096 ; top5 ->  89.904  and loss:  948.3626825809479
forward train acc: top1 ->  70.765625 ; top5 ->  87.9375  and loss:  245.3922334909439
test acc: top1 ->  71.12 ; top5 ->  89.98  and loss:  945.1591409444809
forward train acc: top1 ->  70.7265625 ; top5 ->  87.9921875  and loss:  244.51875269412994
test acc: top1 ->  71.142 ; top5 ->  89.898  and loss:  946.8617796897888
forward train acc: top1 ->  70.4375 ; top5 ->  87.8984375  and loss:  247.2933550477028
test acc: top1 ->  71.126 ; top5 ->  89.978  and loss:  944.6894271969795
forward train acc: top1 ->  70.5234375 ; top5 ->  87.9296875  and loss:  246.22940868139267
test acc: top1 ->  71.182 ; top5 ->  90.014  and loss:  942.5326734185219
forward train acc: top1 ->  70.0390625 ; top5 ->  87.7109375  and loss:  250.60176968574524
test acc: top1 ->  71.322 ; top5 ->  90.02  and loss:  939.0745636820793
forward train acc: top1 ->  69.9765625 ; top5 ->  87.859375  and loss:  250.15096378326416
test acc: top1 ->  71.356 ; top5 ->  89.924  and loss:  937.5023156404495
forward train acc: top1 ->  70.9375 ; top5 ->  88.1796875  and loss:  240.74362707138062
test acc: top1 ->  71.438 ; top5 ->  90.094  and loss:  936.3955312371254
forward train acc: top1 ->  71.0 ; top5 ->  88.328125  and loss:  240.56452530622482
test acc: top1 ->  71.362 ; top5 ->  90.03  and loss:  935.774031996727
forward train acc: top1 ->  70.71875 ; top5 ->  88.0625  and loss:  244.83266937732697
test acc: top1 ->  71.39 ; top5 ->  90.054  and loss:  936.2251069545746
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  57 / 64 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -57.906955778598785 , diff:  57.906955778598785
adv train loss:  -60.45026582479477 , diff:  2.543310046195984
layer  3  adv train finish, try to retain  36
test acc: top1 ->  65.438 ; top5 ->  86.518  and loss:  1137.3942223787308
forward train acc: top1 ->  70.0234375 ; top5 ->  88.0078125  and loss:  248.82134479284286
test acc: top1 ->  70.776 ; top5 ->  89.784  and loss:  957.5613968372345
forward train acc: top1 ->  70.046875 ; top5 ->  87.5390625  and loss:  253.20132958889008
test acc: top1 ->  70.812 ; top5 ->  89.83  and loss:  956.2361969947815
forward train acc: top1 ->  70.609375 ; top5 ->  87.796875  and loss:  250.62119925022125
test acc: top1 ->  70.786 ; top5 ->  89.792  and loss:  955.3144118785858
forward train acc: top1 ->  70.09375 ; top5 ->  87.6015625  and loss:  252.90031725168228
test acc: top1 ->  70.898 ; top5 ->  89.79  and loss:  953.1862228512764
forward train acc: top1 ->  70.421875 ; top5 ->  88.078125  and loss:  249.11289143562317
test acc: top1 ->  70.81 ; top5 ->  89.848  and loss:  950.7335817813873
forward train acc: top1 ->  69.7890625 ; top5 ->  87.625  and loss:  252.58800739049911
test acc: top1 ->  71.03 ; top5 ->  89.872  and loss:  952.26715695858
forward train acc: top1 ->  69.9375 ; top5 ->  87.8359375  and loss:  250.38224494457245
test acc: top1 ->  70.998 ; top5 ->  89.896  and loss:  946.1569173932076
forward train acc: top1 ->  69.953125 ; top5 ->  87.8515625  and loss:  251.4682013988495
test acc: top1 ->  70.952 ; top5 ->  89.874  and loss:  949.09782153368
forward train acc: top1 ->  70.40625 ; top5 ->  87.9765625  and loss:  249.48065811395645
test acc: top1 ->  70.976 ; top5 ->  89.902  and loss:  948.586207807064
forward train acc: top1 ->  69.984375 ; top5 ->  88.453125  and loss:  248.14417910575867
test acc: top1 ->  71.064 ; top5 ->  89.916  and loss:  946.9940428733826
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  43 / 64 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -60.70434355735779 , diff:  60.70434355735779
adv train loss:  -58.046501874923706 , diff:  2.657841682434082
layer  4  adv train finish, try to retain  38
test acc: top1 ->  70.29 ; top5 ->  89.508  and loss:  972.090778529644
forward train acc: top1 ->  69.953125 ; top5 ->  87.90625  and loss:  249.01391381025314
test acc: top1 ->  71.19 ; top5 ->  89.87  and loss:  947.2356116771698
forward train acc: top1 ->  70.4921875 ; top5 ->  87.5546875  and loss:  249.8720692396164
test acc: top1 ->  71.188 ; top5 ->  89.862  and loss:  947.7558007836342
forward train acc: top1 ->  70.0234375 ; top5 ->  88.0078125  and loss:  248.9546377658844
test acc: top1 ->  71.144 ; top5 ->  89.812  and loss:  948.1411246061325
forward train acc: top1 ->  71.03125 ; top5 ->  88.3671875  and loss:  241.310427069664
test acc: top1 ->  71.248 ; top5 ->  89.844  and loss:  942.162774503231
forward train acc: top1 ->  70.0 ; top5 ->  87.828125  and loss:  251.39562582969666
test acc: top1 ->  71.176 ; top5 ->  89.934  and loss:  943.8405911326408
forward train acc: top1 ->  71.4765625 ; top5 ->  88.3125  and loss:  242.87401098012924
test acc: top1 ->  71.128 ; top5 ->  89.942  and loss:  945.5012956261635
forward train acc: top1 ->  69.703125 ; top5 ->  87.375  and loss:  251.75444561243057
test acc: top1 ->  71.22 ; top5 ->  89.926  and loss:  942.1806472539902
forward train acc: top1 ->  70.0703125 ; top5 ->  88.015625  and loss:  246.4110300540924
test acc: top1 ->  71.248 ; top5 ->  89.944  and loss:  941.3975610733032
forward train acc: top1 ->  70.390625 ; top5 ->  88.265625  and loss:  245.73616695404053
test acc: top1 ->  71.27 ; top5 ->  89.962  and loss:  943.64521753788
forward train acc: top1 ->  71.0078125 ; top5 ->  88.2578125  and loss:  241.7236996293068
test acc: top1 ->  71.284 ; top5 ->  89.948  and loss:  941.4562299251556
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  40 / 64 , inc:  1
---------------- start layer  5  ---------------
adv train loss:  -63.57741141319275 , diff:  63.57741141319275
adv train loss:  -63.4686576128006 , diff:  0.10875380039215088
layer  5  adv train finish, try to retain  28
test acc: top1 ->  68.196 ; top5 ->  88.21  and loss:  1045.9266828298569
forward train acc: top1 ->  70.625 ; top5 ->  88.0859375  and loss:  244.77590560913086
test acc: top1 ->  71.096 ; top5 ->  89.968  and loss:  950.0560185313225
forward train acc: top1 ->  70.9453125 ; top5 ->  87.9921875  and loss:  247.24211311340332
test acc: top1 ->  71.148 ; top5 ->  89.924  and loss:  948.0608446598053
forward train acc: top1 ->  71.0546875 ; top5 ->  88.140625  and loss:  248.1055057644844
test acc: top1 ->  71.018 ; top5 ->  89.99  and loss:  946.9052919149399
forward train acc: top1 ->  70.671875 ; top5 ->  88.5703125  and loss:  243.08777284622192
test acc: top1 ->  70.994 ; top5 ->  89.98  and loss:  945.8752350211143
forward train acc: top1 ->  70.8828125 ; top5 ->  88.2109375  and loss:  243.4227876663208
test acc: top1 ->  71.024 ; top5 ->  90.022  and loss:  943.3742005228996
forward train acc: top1 ->  70.1171875 ; top5 ->  87.7109375  and loss:  249.4864091873169
test acc: top1 ->  71.152 ; top5 ->  89.974  and loss:  943.5918309688568
forward train acc: top1 ->  71.3203125 ; top5 ->  88.9921875  and loss:  233.49715042114258
test acc: top1 ->  71.192 ; top5 ->  90.062  and loss:  938.8984466195107
forward train acc: top1 ->  70.40625 ; top5 ->  87.984375  and loss:  247.06934654712677
test acc: top1 ->  71.142 ; top5 ->  89.992  and loss:  943.0013698339462
forward train acc: top1 ->  70.03125 ; top5 ->  88.203125  and loss:  247.44941318035126
test acc: top1 ->  71.144 ; top5 ->  90.016  and loss:  941.9209307432175
forward train acc: top1 ->  71.1015625 ; top5 ->  88.4375  and loss:  241.66966849565506
test acc: top1 ->  71.266 ; top5 ->  90.066  and loss:  938.4887988567352
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  29 / 64 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -63.56868499517441 , diff:  63.56868499517441
adv train loss:  -59.20804798603058 , diff:  4.360637009143829
layer  6  adv train finish, try to retain  115
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -64.14082962274551 , diff:  64.14082962274551
adv train loss:  -58.55419600009918 , diff:  5.586633622646332
layer  7  adv train finish, try to retain  117
test acc: top1 ->  69.906 ; top5 ->  89.198  and loss:  978.5144966244698
forward train acc: top1 ->  70.1015625 ; top5 ->  88.3515625  and loss:  247.3160735964775
test acc: top1 ->  71.058 ; top5 ->  89.946  and loss:  944.8317514061928
forward train acc: top1 ->  70.15625 ; top5 ->  87.6875  and loss:  250.1393345594406
test acc: top1 ->  71.044 ; top5 ->  89.904  and loss:  946.4746749401093
forward train acc: top1 ->  70.46875 ; top5 ->  87.8828125  and loss:  247.72864371538162
test acc: top1 ->  71.058 ; top5 ->  89.834  and loss:  948.3671008944511
forward train acc: top1 ->  69.875 ; top5 ->  87.53125  and loss:  253.1022411584854
test acc: top1 ->  71.088 ; top5 ->  89.904  and loss:  946.2435763478279
forward train acc: top1 ->  70.1640625 ; top5 ->  87.875  and loss:  250.85362046957016
test acc: top1 ->  71.036 ; top5 ->  89.922  and loss:  942.8688663840294
forward train acc: top1 ->  69.875 ; top5 ->  88.234375  and loss:  249.93311095237732
test acc: top1 ->  71.074 ; top5 ->  89.92  and loss:  943.0223208665848
forward train acc: top1 ->  70.859375 ; top5 ->  88.46875  and loss:  243.32142561674118
test acc: top1 ->  71.186 ; top5 ->  89.918  and loss:  944.6253042817116
forward train acc: top1 ->  70.2578125 ; top5 ->  88.0625  and loss:  249.51508688926697
test acc: top1 ->  71.162 ; top5 ->  89.956  and loss:  940.5111191272736
forward train acc: top1 ->  70.609375 ; top5 ->  87.9609375  and loss:  248.11078917980194
test acc: top1 ->  71.166 ; top5 ->  89.976  and loss:  939.7430926561356
forward train acc: top1 ->  70.453125 ; top5 ->  87.8671875  and loss:  248.94222748279572
test acc: top1 ->  71.202 ; top5 ->  89.99  and loss:  939.5643678307533
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  122 / 128 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -61.80373978614807 , diff:  61.80373978614807
adv train loss:  -63.71051561832428 , diff:  1.9067758321762085
layer  8  adv train finish, try to retain  81
test acc: top1 ->  69.428 ; top5 ->  88.946  and loss:  1003.4168648123741
forward train acc: top1 ->  69.09375 ; top5 ->  86.7578125  and loss:  261.48951095342636
test acc: top1 ->  70.558 ; top5 ->  89.718  and loss:  960.0054560899734
forward train acc: top1 ->  69.84375 ; top5 ->  88.0703125  and loss:  249.69461625814438
test acc: top1 ->  70.7 ; top5 ->  89.798  and loss:  957.2361499071121
forward train acc: top1 ->  69.5859375 ; top5 ->  87.4921875  and loss:  251.50150841474533
test acc: top1 ->  70.772 ; top5 ->  89.778  and loss:  956.7678434848785
forward train acc: top1 ->  69.8203125 ; top5 ->  87.4296875  and loss:  253.34213650226593
test acc: top1 ->  70.858 ; top5 ->  89.782  and loss:  955.1223220229149
forward train acc: top1 ->  69.9453125 ; top5 ->  87.6640625  and loss:  253.35772097110748
test acc: top1 ->  70.892 ; top5 ->  89.874  and loss:  949.6599835157394
forward train acc: top1 ->  70.3984375 ; top5 ->  87.921875  and loss:  248.80656504631042
test acc: top1 ->  70.986 ; top5 ->  89.864  and loss:  948.0792107582092
forward train acc: top1 ->  69.828125 ; top5 ->  88.0859375  and loss:  249.80513966083527
test acc: top1 ->  70.98 ; top5 ->  89.88  and loss:  948.9765688180923
forward train acc: top1 ->  70.65625 ; top5 ->  88.0  and loss:  246.74954563379288
test acc: top1 ->  71.038 ; top5 ->  89.89  and loss:  945.7718895673752
forward train acc: top1 ->  71.453125 ; top5 ->  88.65625  and loss:  239.78166913986206
test acc: top1 ->  70.916 ; top5 ->  89.928  and loss:  948.5746874809265
forward train acc: top1 ->  70.46875 ; top5 ->  87.6328125  and loss:  249.43627673387527
test acc: top1 ->  71.11 ; top5 ->  89.922  and loss:  947.523663520813
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -60.460172176361084 , diff:  60.460172176361084
adv train loss:  -58.06643623113632 , diff:  2.393735945224762
layer  9  adv train finish, try to retain  112
test acc: top1 ->  71.044 ; top5 ->  89.952  and loss:  947.2320954203606
forward train acc: top1 ->  70.2734375 ; top5 ->  88.0  and loss:  247.74941557645798
test acc: top1 ->  71.04 ; top5 ->  90.022  and loss:  944.7673172354698
forward train acc: top1 ->  70.46875 ; top5 ->  88.265625  and loss:  244.87072485685349
test acc: top1 ->  71.154 ; top5 ->  89.986  and loss:  942.9392445087433
forward train acc: top1 ->  70.625 ; top5 ->  88.2890625  and loss:  246.53962188959122
test acc: top1 ->  71.004 ; top5 ->  89.98  and loss:  944.7532218694687
forward train acc: top1 ->  70.2265625 ; top5 ->  88.1328125  and loss:  245.0240518450737
test acc: top1 ->  71.268 ; top5 ->  90.126  and loss:  941.1895393133163
forward train acc: top1 ->  71.1484375 ; top5 ->  88.5234375  and loss:  240.15605968236923
test acc: top1 ->  71.248 ; top5 ->  90.02  and loss:  940.8388319015503
forward train acc: top1 ->  70.3515625 ; top5 ->  88.25  and loss:  244.74994260072708
test acc: top1 ->  71.278 ; top5 ->  90.044  and loss:  939.7268435955048
forward train acc: top1 ->  70.6953125 ; top5 ->  88.2578125  and loss:  243.3651361465454
test acc: top1 ->  71.234 ; top5 ->  90.102  and loss:  938.8333992362022
forward train acc: top1 ->  70.84375 ; top5 ->  88.1953125  and loss:  242.85250425338745
test acc: top1 ->  71.252 ; top5 ->  90.142  and loss:  937.6563934087753
forward train acc: top1 ->  70.0625 ; top5 ->  88.1875  and loss:  248.68312638998032
test acc: top1 ->  71.222 ; top5 ->  90.11  and loss:  938.4587078094482
forward train acc: top1 ->  71.9140625 ; top5 ->  88.7578125  and loss:  237.45666766166687
test acc: top1 ->  71.29 ; top5 ->  90.128  and loss:  935.6643317937851
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  115 / 128 , inc:  1
---------------- start layer  10  ---------------
adv train loss:  -58.14980125427246 , diff:  58.14980125427246
adv train loss:  -60.807935416698456 , diff:  2.658134162425995
layer  10  adv train finish, try to retain  76
test acc: top1 ->  70.41 ; top5 ->  89.578  and loss:  970.0798853635788
forward train acc: top1 ->  70.1953125 ; top5 ->  88.4296875  and loss:  246.342303276062
test acc: top1 ->  70.792 ; top5 ->  89.758  and loss:  953.7888807058334
forward train acc: top1 ->  70.1640625 ; top5 ->  87.5859375  and loss:  251.56831449270248
test acc: top1 ->  71.002 ; top5 ->  89.804  and loss:  950.2708860039711
forward train acc: top1 ->  70.1015625 ; top5 ->  88.0234375  and loss:  249.11939412355423
test acc: top1 ->  70.97 ; top5 ->  89.87  and loss:  947.9263441562653
forward train acc: top1 ->  70.1875 ; top5 ->  88.1875  and loss:  247.87133693695068
test acc: top1 ->  70.914 ; top5 ->  89.794  and loss:  946.934125483036
forward train acc: top1 ->  69.5625 ; top5 ->  87.84375  and loss:  251.7926384806633
test acc: top1 ->  71.008 ; top5 ->  89.852  and loss:  946.7150955200195
forward train acc: top1 ->  70.484375 ; top5 ->  88.109375  and loss:  247.29011368751526
test acc: top1 ->  71.112 ; top5 ->  89.858  and loss:  945.8501515388489
forward train acc: top1 ->  70.1484375 ; top5 ->  88.21875  and loss:  249.2572684288025
test acc: top1 ->  71.118 ; top5 ->  89.872  and loss:  942.6068727374077
forward train acc: top1 ->  70.5859375 ; top5 ->  88.1171875  and loss:  247.7875755429268
test acc: top1 ->  71.152 ; top5 ->  89.906  and loss:  943.5051236748695
forward train acc: top1 ->  70.109375 ; top5 ->  87.703125  and loss:  249.1698722243309
test acc: top1 ->  71.208 ; top5 ->  89.93  and loss:  945.0617271661758
forward train acc: top1 ->  70.1953125 ; top5 ->  88.53125  and loss:  246.90124768018723
test acc: top1 ->  71.128 ; top5 ->  89.904  and loss:  944.0435242056847
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -60.83498817682266 , diff:  60.83498817682266
adv train loss:  -61.30460315942764 , diff:  0.46961498260498047
layer  11  adv train finish, try to retain  91
test acc: top1 ->  69.702 ; top5 ->  89.142  and loss:  989.1923280358315
forward train acc: top1 ->  70.90625 ; top5 ->  88.1953125  and loss:  244.63415509462357
test acc: top1 ->  71.118 ; top5 ->  89.892  and loss:  945.7413012385368
forward train acc: top1 ->  70.1015625 ; top5 ->  87.9453125  and loss:  249.72498089075089
test acc: top1 ->  71.096 ; top5 ->  89.948  and loss:  943.5876560807228
forward train acc: top1 ->  70.71875 ; top5 ->  87.9765625  and loss:  245.15897393226624
test acc: top1 ->  71.084 ; top5 ->  89.9  and loss:  946.422607421875
forward train acc: top1 ->  69.9921875 ; top5 ->  88.15625  and loss:  248.55099046230316
test acc: top1 ->  71.166 ; top5 ->  90.022  and loss:  943.423431456089
forward train acc: top1 ->  70.25 ; top5 ->  87.6484375  and loss:  249.22386902570724
test acc: top1 ->  71.174 ; top5 ->  90.02  and loss:  943.7612374424934
forward train acc: top1 ->  70.515625 ; top5 ->  88.296875  and loss:  243.8992476463318
test acc: top1 ->  71.016 ; top5 ->  89.986  and loss:  943.8429148197174
forward train acc: top1 ->  71.2890625 ; top5 ->  88.71875  and loss:  236.68676656484604
test acc: top1 ->  71.064 ; top5 ->  90.004  and loss:  939.3187693357468
forward train acc: top1 ->  70.3046875 ; top5 ->  88.0703125  and loss:  246.83758610486984
test acc: top1 ->  71.13 ; top5 ->  89.974  and loss:  940.0073180794716
forward train acc: top1 ->  70.3125 ; top5 ->  87.7734375  and loss:  246.75432497262955
test acc: top1 ->  71.244 ; top5 ->  89.97  and loss:  940.1873052120209
forward train acc: top1 ->  70.640625 ; top5 ->  87.96875  and loss:  248.1217891573906
test acc: top1 ->  71.216 ; top5 ->  90.002  and loss:  939.5875957012177
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -59.62472075223923 , diff:  59.62472075223923
adv train loss:  -61.946083068847656 , diff:  2.321362316608429
layer  12  adv train finish, try to retain  106
>>>>>>> reverse layer  12  since no improvement >>>>>>>
---------------- start layer  13  ---------------
adv train loss:  -62.30093330144882 , diff:  62.30093330144882
adv train loss:  -60.74213898181915 , diff:  1.5587943196296692
layer  13  adv train finish, try to retain  67
test acc: top1 ->  66.936 ; top5 ->  87.19  and loss:  1097.8233053684235
forward train acc: top1 ->  69.1484375 ; top5 ->  87.6171875  and loss:  258.09238988161087
test acc: top1 ->  70.774 ; top5 ->  89.738  and loss:  960.1494483947754
forward train acc: top1 ->  70.171875 ; top5 ->  87.6796875  and loss:  250.31741350889206
test acc: top1 ->  70.702 ; top5 ->  89.722  and loss:  960.7867725491524
forward train acc: top1 ->  69.7578125 ; top5 ->  88.2109375  and loss:  252.88187849521637
test acc: top1 ->  70.834 ; top5 ->  89.766  and loss:  952.8042262196541
forward train acc: top1 ->  69.8359375 ; top5 ->  87.7734375  and loss:  250.91902947425842
test acc: top1 ->  70.71 ; top5 ->  89.67  and loss:  957.2106390595436
forward train acc: top1 ->  69.2578125 ; top5 ->  87.40625  and loss:  252.68854385614395
test acc: top1 ->  70.908 ; top5 ->  89.786  and loss:  950.6155965328217
forward train acc: top1 ->  70.7578125 ; top5 ->  87.7578125  and loss:  247.6868972182274
test acc: top1 ->  71.04 ; top5 ->  89.796  and loss:  950.1008917093277
forward train acc: top1 ->  70.0546875 ; top5 ->  87.6328125  and loss:  250.50142830610275
test acc: top1 ->  70.898 ; top5 ->  89.788  and loss:  954.0755807757378
forward train acc: top1 ->  70.5546875 ; top5 ->  88.2734375  and loss:  247.93962627649307
test acc: top1 ->  70.972 ; top5 ->  89.818  and loss:  950.6344640254974
forward train acc: top1 ->  70.2265625 ; top5 ->  87.9140625  and loss:  248.9132158756256
test acc: top1 ->  71.026 ; top5 ->  89.814  and loss:  947.0298548340797
forward train acc: top1 ->  70.140625 ; top5 ->  87.8203125  and loss:  249.65474033355713
test acc: top1 ->  70.972 ; top5 ->  89.812  and loss:  948.352926492691
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  71 / 128 , inc:  1
---------------- start layer  14  ---------------
adv train loss:  -64.14546918869019 , diff:  64.14546918869019
adv train loss:  -61.012848258018494 , diff:  3.132620930671692
layer  14  adv train finish, try to retain  250
>>>>>>> reverse layer  14  since no improvement >>>>>>>
---------------- start layer  15  ---------------
adv train loss:  -58.22619700431824 , diff:  58.22619700431824
adv train loss:  -60.10300159454346 , diff:  1.8768045902252197
layer  15  adv train finish, try to retain  238
test acc: top1 ->  70.084 ; top5 ->  89.51  and loss:  976.6917306184769
forward train acc: top1 ->  69.5546875 ; top5 ->  88.078125  and loss:  253.96246296167374
test acc: top1 ->  70.774 ; top5 ->  89.788  and loss:  954.317418217659
forward train acc: top1 ->  70.0625 ; top5 ->  87.703125  and loss:  250.29695814847946
test acc: top1 ->  70.898 ; top5 ->  89.932  and loss:  948.9842435717583
forward train acc: top1 ->  70.234375 ; top5 ->  88.0234375  and loss:  247.56936639547348
test acc: top1 ->  70.864 ; top5 ->  89.944  and loss:  949.9942021369934
forward train acc: top1 ->  70.9140625 ; top5 ->  88.328125  and loss:  242.4172711968422
test acc: top1 ->  71.004 ; top5 ->  89.928  and loss:  950.7339041829109
forward train acc: top1 ->  71.0078125 ; top5 ->  87.9140625  and loss:  244.5916627049446
test acc: top1 ->  70.984 ; top5 ->  89.954  and loss:  945.7072969079018
forward train acc: top1 ->  69.8359375 ; top5 ->  88.1328125  and loss:  248.2699203491211
test acc: top1 ->  70.938 ; top5 ->  89.872  and loss:  949.2670988440514
forward train acc: top1 ->  70.0703125 ; top5 ->  87.7578125  and loss:  247.61402678489685
test acc: top1 ->  71.03 ; top5 ->  90.004  and loss:  943.5386793613434
forward train acc: top1 ->  70.3203125 ; top5 ->  88.078125  and loss:  245.8251147866249
test acc: top1 ->  71.036 ; top5 ->  90.02  and loss:  944.0351612567902
forward train acc: top1 ->  69.75 ; top5 ->  87.9921875  and loss:  247.42594587802887
test acc: top1 ->  70.934 ; top5 ->  90.042  and loss:  942.6616715788841
forward train acc: top1 ->  70.3203125 ; top5 ->  87.9765625  and loss:  248.9543713927269
test acc: top1 ->  70.932 ; top5 ->  89.98  and loss:  943.7854648828506
>>>>>>> reverse layer  15  since performance drop >>>>>>>
==> this epoch:  248 / 256 , inc:  1
---------------- start layer  16  ---------------
adv train loss:  -61.73174345493317 , diff:  61.73174345493317
adv train loss:  -59.687117755413055 , diff:  2.044625699520111
layer  16  adv train finish, try to retain  238
test acc: top1 ->  71.072 ; top5 ->  89.852  and loss:  948.5146607160568
forward train acc: top1 ->  70.609375 ; top5 ->  87.9296875  and loss:  249.62124532461166
test acc: top1 ->  71.152 ; top5 ->  89.874  and loss:  948.0039098262787
forward train acc: top1 ->  70.515625 ; top5 ->  88.03125  and loss:  248.84260016679764
test acc: top1 ->  71.04 ; top5 ->  89.868  and loss:  946.6134822964668
forward train acc: top1 ->  71.2421875 ; top5 ->  88.9296875  and loss:  234.67578327655792
test acc: top1 ->  71.164 ; top5 ->  89.966  and loss:  943.4056652784348
forward train acc: top1 ->  70.3515625 ; top5 ->  88.6171875  and loss:  243.9044406414032
test acc: top1 ->  71.244 ; top5 ->  89.988  and loss:  943.6251075863838
forward train acc: top1 ->  71.0 ; top5 ->  88.2578125  and loss:  243.24096983671188
test acc: top1 ->  71.298 ; top5 ->  89.998  and loss:  941.6956125497818
forward train acc: top1 ->  70.515625 ; top5 ->  88.3125  and loss:  243.77235573530197
test acc: top1 ->  71.48 ; top5 ->  90.068  and loss:  937.4091268777847
forward train acc: top1 ->  71.078125 ; top5 ->  88.2109375  and loss:  244.27283000946045
test acc: top1 ->  71.284 ; top5 ->  90.04  and loss:  940.3409070968628
forward train acc: top1 ->  70.5390625 ; top5 ->  88.046875  and loss:  245.03750962018967
test acc: top1 ->  71.332 ; top5 ->  90.09  and loss:  939.2399119734764
forward train acc: top1 ->  70.5625 ; top5 ->  87.90625  and loss:  246.42873561382294
test acc: top1 ->  71.298 ; top5 ->  90.028  and loss:  939.1995387077332
forward train acc: top1 ->  71.5234375 ; top5 ->  88.578125  and loss:  239.95786255598068
test acc: top1 ->  71.312 ; top5 ->  90.056  and loss:  937.127701997757
>>>>>>> reverse layer  16  since performance drop >>>>>>>
==> this epoch:  239 / 256 , inc:  1
---------------- start layer  17  ---------------
adv train loss:  -61.490366756916046 , diff:  61.490366756916046
adv train loss:  -59.71114790439606 , diff:  1.779218852519989
layer  17  adv train finish, try to retain  231
>>>>>>> reverse layer  17  since no improvement >>>>>>>
---------------- start layer  18  ---------------
adv train loss:  -60.20220363140106 , diff:  60.20220363140106
adv train loss:  -61.76912271976471 , diff:  1.5669190883636475
layer  18  adv train finish, try to retain  241
>>>>>>> reverse layer  18  since no improvement >>>>>>>
---------------- start layer  19  ---------------
adv train loss:  -56.375657975673676 , diff:  56.375657975673676
adv train loss:  -62.67389523983002 , diff:  6.2982372641563416
layer  19  adv train finish, try to retain  155
test acc: top1 ->  70.762 ; top5 ->  89.774  and loss:  954.9250818490982
forward train acc: top1 ->  70.3046875 ; top5 ->  87.8984375  and loss:  247.44499552249908
test acc: top1 ->  70.878 ; top5 ->  89.802  and loss:  954.8094127178192
forward train acc: top1 ->  69.3515625 ; top5 ->  87.375  and loss:  256.2848806977272
test acc: top1 ->  70.922 ; top5 ->  89.826  and loss:  955.7147518992424
forward train acc: top1 ->  70.8671875 ; top5 ->  87.890625  and loss:  246.41430991888046
test acc: top1 ->  70.82 ; top5 ->  89.888  and loss:  955.5588381886482
forward train acc: top1 ->  70.015625 ; top5 ->  88.3046875  and loss:  247.74976497888565
test acc: top1 ->  70.818 ; top5 ->  89.834  and loss:  953.4386254549026
forward train acc: top1 ->  70.78125 ; top5 ->  88.140625  and loss:  242.3165364265442
test acc: top1 ->  71.024 ; top5 ->  89.914  and loss:  952.5535033941269
forward train acc: top1 ->  70.4765625 ; top5 ->  88.171875  and loss:  245.29875260591507
test acc: top1 ->  70.946 ; top5 ->  89.92  and loss:  948.1640650629997
forward train acc: top1 ->  69.9140625 ; top5 ->  87.875  and loss:  251.7809683084488
test acc: top1 ->  71.098 ; top5 ->  89.91  and loss:  945.2418624162674
forward train acc: top1 ->  69.7109375 ; top5 ->  87.765625  and loss:  251.59825271368027
test acc: top1 ->  71.028 ; top5 ->  89.926  and loss:  946.6695674061775
forward train acc: top1 ->  69.75 ; top5 ->  87.7734375  and loss:  250.8371440768242
test acc: top1 ->  71.032 ; top5 ->  89.898  and loss:  948.1515920162201
forward train acc: top1 ->  70.3828125 ; top5 ->  88.3203125  and loss:  246.54828816652298
test acc: top1 ->  71.094 ; top5 ->  89.94  and loss:  946.6634201407433
>>>>>>> reverse layer  19  since performance drop >>>>>>>
==> this epoch:  179 / 256 , inc:  1
---------------- start layer  20  ---------------
adv train loss:  -62.78079640865326 , diff:  62.78079640865326
adv train loss:  -61.55480247735977 , diff:  1.2259939312934875
layer  20  adv train finish, try to retain  217
test acc: top1 ->  70.976 ; top5 ->  89.818  and loss:  952.0831415057182
forward train acc: top1 ->  70.21875 ; top5 ->  87.9453125  and loss:  248.197367310524
test acc: top1 ->  71.034 ; top5 ->  89.834  and loss:  946.2709582448006
forward train acc: top1 ->  70.0234375 ; top5 ->  88.1796875  and loss:  246.45681011676788
test acc: top1 ->  70.916 ; top5 ->  89.95  and loss:  946.7945410609245
forward train acc: top1 ->  70.1875 ; top5 ->  87.8125  and loss:  248.8605963587761
test acc: top1 ->  70.98 ; top5 ->  89.896  and loss:  945.7628930211067
forward train acc: top1 ->  70.7890625 ; top5 ->  87.7578125  and loss:  246.68472385406494
test acc: top1 ->  71.096 ; top5 ->  89.974  and loss:  946.51016664505
forward train acc: top1 ->  69.4765625 ; top5 ->  87.46875  and loss:  254.55937707424164
test acc: top1 ->  71.098 ; top5 ->  89.968  and loss:  944.0776265263557
forward train acc: top1 ->  70.296875 ; top5 ->  88.203125  and loss:  245.14312517642975
test acc: top1 ->  71.114 ; top5 ->  90.01  and loss:  943.475533246994
forward train acc: top1 ->  70.859375 ; top5 ->  88.375  and loss:  243.44920831918716
test acc: top1 ->  71.12 ; top5 ->  90.048  and loss:  940.2749155759811
forward train acc: top1 ->  70.1171875 ; top5 ->  88.0390625  and loss:  246.61736238002777
test acc: top1 ->  71.244 ; top5 ->  90.092  and loss:  940.2672625780106
forward train acc: top1 ->  70.8359375 ; top5 ->  88.2421875  and loss:  244.3294000029564
test acc: top1 ->  71.252 ; top5 ->  90.084  and loss:  938.6663487553596
forward train acc: top1 ->  70.6640625 ; top5 ->  88.328125  and loss:  245.45930886268616
test acc: top1 ->  71.188 ; top5 ->  90.058  and loss:  939.5970913767815
>>>>>>> reverse layer  20  since performance drop >>>>>>>
==> this epoch:  225 / 256 , inc:  1
---------------- start layer  21  ---------------
adv train loss:  -60.50965732336044 , diff:  60.50965732336044
adv train loss:  -64.04912859201431 , diff:  3.5394712686538696
layer  21  adv train finish, try to retain  243
>>>>>>> reverse layer  21  since no improvement >>>>>>>
---------------- start layer  22  ---------------
adv train loss:  -59.69520419836044 , diff:  59.69520419836044
adv train loss:  -61.855631589889526 , diff:  2.1604273915290833
layer  22  adv train finish, try to retain  200
test acc: top1 ->  70.946 ; top5 ->  89.852  and loss:  954.298412501812
forward train acc: top1 ->  70.4765625 ; top5 ->  88.46875  and loss:  245.04806125164032
test acc: top1 ->  71.016 ; top5 ->  89.928  and loss:  948.2207008004189
forward train acc: top1 ->  70.21875 ; top5 ->  87.875  and loss:  248.9747964143753
test acc: top1 ->  70.998 ; top5 ->  89.92  and loss:  949.3589727282524
forward train acc: top1 ->  70.09375 ; top5 ->  87.8671875  and loss:  249.07392233610153
test acc: top1 ->  71.134 ; top5 ->  89.93  and loss:  947.3914705514908
forward train acc: top1 ->  70.2890625 ; top5 ->  87.6875  and loss:  250.82177019119263
test acc: top1 ->  71.21 ; top5 ->  90.0  and loss:  944.4664243459702
forward train acc: top1 ->  70.1015625 ; top5 ->  87.9296875  and loss:  249.73943275213242
test acc: top1 ->  71.178 ; top5 ->  89.958  and loss:  942.1123406291008
forward train acc: top1 ->  71.0078125 ; top5 ->  88.2578125  and loss:  240.7473800778389
test acc: top1 ->  71.216 ; top5 ->  89.92  and loss:  943.6856501698494
forward train acc: top1 ->  70.2421875 ; top5 ->  88.109375  and loss:  247.61701267957687
test acc: top1 ->  71.168 ; top5 ->  89.972  and loss:  944.798178255558
forward train acc: top1 ->  70.140625 ; top5 ->  87.7421875  and loss:  248.506980240345
test acc: top1 ->  71.326 ; top5 ->  90.0  and loss:  941.7985016107559
forward train acc: top1 ->  71.3203125 ; top5 ->  88.4765625  and loss:  240.02071303129196
test acc: top1 ->  71.286 ; top5 ->  90.016  and loss:  939.7384807467461
forward train acc: top1 ->  70.140625 ; top5 ->  87.890625  and loss:  250.2535484433174
test acc: top1 ->  71.382 ; top5 ->  89.986  and loss:  940.951124727726
>>>>>>> reverse layer  22  since performance drop >>>>>>>
==> this epoch:  208 / 256 , inc:  1
---------------- start layer  23  ---------------
adv train loss:  -60.02642202377319 , diff:  60.02642202377319
adv train loss:  -60.07881289720535 , diff:  0.052390873432159424
layer  23  adv train finish, try to retain  174
test acc: top1 ->  70.884 ; top5 ->  89.758  and loss:  965.5801661014557
forward train acc: top1 ->  70.734375 ; top5 ->  87.8828125  and loss:  250.1447755098343
test acc: top1 ->  71.062 ; top5 ->  89.868  and loss:  950.3560107350349
forward train acc: top1 ->  70.234375 ; top5 ->  88.1015625  and loss:  247.0154756307602
test acc: top1 ->  71.074 ; top5 ->  89.924  and loss:  946.2160169482231
forward train acc: top1 ->  70.65625 ; top5 ->  88.0  and loss:  248.0456589460373
test acc: top1 ->  71.074 ; top5 ->  89.88  and loss:  947.5330641269684
forward train acc: top1 ->  71.2578125 ; top5 ->  88.15625  and loss:  242.6755982041359
test acc: top1 ->  70.992 ; top5 ->  89.938  and loss:  947.1233692765236
forward train acc: top1 ->  70.15625 ; top5 ->  87.40625  and loss:  251.42521739006042
test acc: top1 ->  71.118 ; top5 ->  89.898  and loss:  942.473494887352
forward train acc: top1 ->  70.3046875 ; top5 ->  88.09375  and loss:  245.61458402872086
test acc: top1 ->  71.066 ; top5 ->  90.012  and loss:  942.3564251065254
forward train acc: top1 ->  70.1484375 ; top5 ->  88.40625  and loss:  244.208331823349
test acc: top1 ->  71.254 ; top5 ->  89.976  and loss:  939.8107025623322
forward train acc: top1 ->  70.09375 ; top5 ->  87.953125  and loss:  245.90956276655197
test acc: top1 ->  71.22 ; top5 ->  89.988  and loss:  941.8411050438881
forward train acc: top1 ->  71.140625 ; top5 ->  88.453125  and loss:  244.67826342582703
test acc: top1 ->  71.264 ; top5 ->  90.03  and loss:  942.7245539426804
forward train acc: top1 ->  71.2578125 ; top5 ->  88.4765625  and loss:  241.34556984901428
test acc: top1 ->  71.358 ; top5 ->  89.956  and loss:  942.0877062082291
>>>>>>> reverse layer  23  since performance drop >>>>>>>
==> this epoch:  187 / 256 , inc:  1
---------------- start layer  24  ---------------
adv train loss:  -62.11100614070892 , diff:  62.11100614070892
adv train loss:  -59.43294221162796 , diff:  2.678063929080963
layer  24  adv train finish, try to retain  240
test acc: top1 ->  71.174 ; top5 ->  89.972  and loss:  940.5600246191025
forward train acc: top1 ->  70.0078125 ; top5 ->  87.6953125  and loss:  252.32039380073547
test acc: top1 ->  71.198 ; top5 ->  89.988  and loss:  940.4388882517815
forward train acc: top1 ->  70.1171875 ; top5 ->  88.09375  and loss:  248.42499947547913
test acc: top1 ->  71.094 ; top5 ->  89.954  and loss:  941.1967101693153
forward train acc: top1 ->  70.671875 ; top5 ->  88.2890625  and loss:  244.45846962928772
test acc: top1 ->  71.106 ; top5 ->  89.968  and loss:  942.3992155194283
forward train acc: top1 ->  70.421875 ; top5 ->  87.65625  and loss:  246.66556018590927
test acc: top1 ->  71.118 ; top5 ->  90.068  and loss:  939.6874440908432
forward train acc: top1 ->  70.9453125 ; top5 ->  88.5078125  and loss:  242.6461005806923
test acc: top1 ->  71.162 ; top5 ->  90.068  and loss:  938.0614433288574
forward train acc: top1 ->  70.46875 ; top5 ->  88.140625  and loss:  247.51918196678162
test acc: top1 ->  71.294 ; top5 ->  90.094  and loss:  936.3925271630287
forward train acc: top1 ->  70.421875 ; top5 ->  87.9140625  and loss:  248.22627586126328
test acc: top1 ->  71.248 ; top5 ->  90.12  and loss:  938.826401591301
forward train acc: top1 ->  70.515625 ; top5 ->  88.1171875  and loss:  244.35108441114426
test acc: top1 ->  71.236 ; top5 ->  90.098  and loss:  937.8070957064629
forward train acc: top1 ->  70.8359375 ; top5 ->  88.6328125  and loss:  237.5631247162819
test acc: top1 ->  71.29 ; top5 ->  90.122  and loss:  938.9151787161827
forward train acc: top1 ->  70.7421875 ; top5 ->  88.3203125  and loss:  243.49640160799026
test acc: top1 ->  71.274 ; top5 ->  90.13  and loss:  940.0547149777412
>>>>>>> reverse layer  24  since performance drop >>>>>>>
==> this epoch:  244 / 256 , inc:  1
---------------- start layer  25  ---------------
adv train loss:  -60.61696815490723 , diff:  60.61696815490723
adv train loss:  -58.96596306562424 , diff:  1.6510050892829895
layer  25  adv train finish, try to retain  238
test acc: top1 ->  71.032 ; top5 ->  89.952  and loss:  947.8738589286804
forward train acc: top1 ->  70.4296875 ; top5 ->  87.84375  and loss:  248.3069058060646
test acc: top1 ->  71.196 ; top5 ->  90.068  and loss:  941.8788227438927
forward train acc: top1 ->  69.8671875 ; top5 ->  87.484375  and loss:  248.69091093540192
test acc: top1 ->  71.126 ; top5 ->  90.078  and loss:  941.9224005937576
forward train acc: top1 ->  70.53125 ; top5 ->  88.6640625  and loss:  241.36329919099808
test acc: top1 ->  71.166 ; top5 ->  90.05  and loss:  943.9322347640991
forward train acc: top1 ->  70.109375 ; top5 ->  87.9765625  and loss:  248.61410057544708
test acc: top1 ->  71.376 ; top5 ->  90.09  and loss:  938.4201976656914
forward train acc: top1 ->  71.1875 ; top5 ->  88.2109375  and loss:  241.11908090114594
test acc: top1 ->  71.404 ; top5 ->  90.058  and loss:  937.9356206059456
forward train acc: top1 ->  71.0625 ; top5 ->  88.3125  and loss:  243.85792130231857
test acc: top1 ->  71.352 ; top5 ->  90.078  and loss:  939.3503056168556
forward train acc: top1 ->  70.828125 ; top5 ->  88.3984375  and loss:  245.15515291690826
test acc: top1 ->  71.308 ; top5 ->  90.13  and loss:  938.1403961777687
forward train acc: top1 ->  70.546875 ; top5 ->  88.2578125  and loss:  246.4347156882286
test acc: top1 ->  71.49 ; top5 ->  90.11  and loss:  934.4360082745552
forward train acc: top1 ->  70.5625 ; top5 ->  87.9296875  and loss:  245.8747615814209
test acc: top1 ->  71.48 ; top5 ->  90.168  and loss:  932.5865817666054
forward train acc: top1 ->  70.171875 ; top5 ->  88.2109375  and loss:  249.3926050066948
test acc: top1 ->  71.39 ; top5 ->  90.128  and loss:  933.6554862856865
>>>>>>> reverse layer  25  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  26  ---------------
adv train loss:  -58.11989068984985 , diff:  58.11989068984985
adv train loss:  -59.350767850875854 , diff:  1.230877161026001
layer  26  adv train finish, try to retain  502
>>>>>>> reverse layer  26  since no improvement >>>>>>>
---------------- start layer  27  ---------------
adv train loss:  -61.089156687259674 , diff:  61.089156687259674
adv train loss:  -57.85051816701889 , diff:  3.2386385202407837
************ all values are small in this layer **********
layer  27  adv train finish, try to retain  504
test acc: top1 ->  71.478 ; top5 ->  90.164  and loss:  934.8891603946686
forward train acc: top1 ->  70.4140625 ; top5 ->  88.078125  and loss:  247.03198260068893
test acc: top1 ->  71.25 ; top5 ->  90.02  and loss:  940.3641277551651
forward train acc: top1 ->  70.515625 ; top5 ->  88.0  and loss:  245.33788871765137
test acc: top1 ->  71.258 ; top5 ->  89.99  and loss:  942.0253583788872
forward train acc: top1 ->  71.234375 ; top5 ->  88.359375  and loss:  243.63483440876007
test acc: top1 ->  71.184 ; top5 ->  90.024  and loss:  939.5488542318344
forward train acc: top1 ->  70.1953125 ; top5 ->  87.90625  and loss:  250.63874125480652
test acc: top1 ->  71.188 ; top5 ->  90.02  and loss:  938.1833162307739
forward train acc: top1 ->  70.734375 ; top5 ->  88.234375  and loss:  244.84825956821442
test acc: top1 ->  71.294 ; top5 ->  90.044  and loss:  935.2854874134064
forward train acc: top1 ->  70.7890625 ; top5 ->  88.390625  and loss:  243.13608592748642
test acc: top1 ->  71.264 ; top5 ->  90.042  and loss:  937.9108628034592
forward train acc: top1 ->  70.7421875 ; top5 ->  88.5546875  and loss:  241.70910316705704
test acc: top1 ->  71.294 ; top5 ->  90.042  and loss:  939.762449502945
forward train acc: top1 ->  71.0546875 ; top5 ->  88.2265625  and loss:  242.8849128484726
test acc: top1 ->  71.37 ; top5 ->  90.056  and loss:  934.6472361087799
forward train acc: top1 ->  70.890625 ; top5 ->  88.3125  and loss:  241.55076831579208
test acc: top1 ->  71.356 ; top5 ->  90.144  and loss:  935.6978740096092
forward train acc: top1 ->  71.2890625 ; top5 ->  87.9296875  and loss:  244.8133419752121
test acc: top1 ->  71.398 ; top5 ->  90.094  and loss:  934.8041479587555
>>>>>>> reverse layer  27  since performance drop >>>>>>>
==> this epoch:  505 / 512 , inc:  1
---------------- start layer  28  ---------------
adv train loss:  -60.90021097660065 , diff:  60.90021097660065
adv train loss:  -57.43030649423599 , diff:  3.4699044823646545
************ all values are small in this layer **********
layer  28  adv train finish, try to retain  502
test acc: top1 ->  71.332 ; top5 ->  90.018  and loss:  942.314782679081
forward train acc: top1 ->  69.890625 ; top5 ->  87.4765625  and loss:  252.85569369792938
test acc: top1 ->  71.272 ; top5 ->  89.974  and loss:  939.9135017991066
forward train acc: top1 ->  70.8984375 ; top5 ->  88.5234375  and loss:  242.35326886177063
test acc: top1 ->  71.262 ; top5 ->  90.03  and loss:  938.7009093761444
forward train acc: top1 ->  70.5625 ; top5 ->  88.4453125  and loss:  245.03778022527695
test acc: top1 ->  71.158 ; top5 ->  90.078  and loss:  940.8747957348824
forward train acc: top1 ->  70.484375 ; top5 ->  87.8515625  and loss:  248.13981068134308
test acc: top1 ->  71.25 ; top5 ->  90.05  and loss:  938.9848324656487
forward train acc: top1 ->  70.6171875 ; top5 ->  87.7734375  and loss:  246.68658047914505
test acc: top1 ->  71.226 ; top5 ->  90.06  and loss:  938.4031149744987
forward train acc: top1 ->  70.84375 ; top5 ->  88.640625  and loss:  244.70539408922195
test acc: top1 ->  71.358 ; top5 ->  90.068  and loss:  939.4761156439781
forward train acc: top1 ->  70.765625 ; top5 ->  88.0390625  and loss:  244.66165620088577
test acc: top1 ->  71.364 ; top5 ->  90.066  and loss:  935.0264740586281
forward train acc: top1 ->  70.5234375 ; top5 ->  88.3515625  and loss:  245.20488572120667
test acc: top1 ->  71.25 ; top5 ->  90.074  and loss:  935.575644493103
forward train acc: top1 ->  70.734375 ; top5 ->  88.0703125  and loss:  246.50749909877777
test acc: top1 ->  71.362 ; top5 ->  90.104  and loss:  936.5747616291046
forward train acc: top1 ->  70.9765625 ; top5 ->  88.46875  and loss:  241.6480101943016
test acc: top1 ->  71.406 ; top5 ->  90.038  and loss:  935.9322746396065
>>>>>>> reverse layer  28  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  29  ---------------
adv train loss:  -62.71938306093216 , diff:  62.71938306093216
adv train loss:  -61.395765602588654 , diff:  1.3236174583435059
layer  29  adv train finish, try to retain  492
test acc: top1 ->  71.252 ; top5 ->  90.076  and loss:  957.6259592175484
forward train acc: top1 ->  69.7890625 ; top5 ->  87.828125  and loss:  250.97855281829834
test acc: top1 ->  71.212 ; top5 ->  89.932  and loss:  946.8719054460526
forward train acc: top1 ->  70.3671875 ; top5 ->  88.046875  and loss:  250.75556939840317
test acc: top1 ->  71.238 ; top5 ->  89.998  and loss:  944.2042745351791
forward train acc: top1 ->  70.984375 ; top5 ->  88.5078125  and loss:  243.684161901474
test acc: top1 ->  71.214 ; top5 ->  90.008  and loss:  943.157437980175
forward train acc: top1 ->  71.25 ; top5 ->  88.3671875  and loss:  242.2907658815384
test acc: top1 ->  71.222 ; top5 ->  90.032  and loss:  940.559353351593
forward train acc: top1 ->  70.890625 ; top5 ->  88.0078125  and loss:  243.66732466220856
test acc: top1 ->  71.296 ; top5 ->  90.096  and loss:  938.3774083852768
forward train acc: top1 ->  70.5078125 ; top5 ->  88.1484375  and loss:  246.13346630334854
test acc: top1 ->  71.34 ; top5 ->  90.106  and loss:  934.1665219068527
forward train acc: top1 ->  70.7109375 ; top5 ->  88.3515625  and loss:  246.90008020401
test acc: top1 ->  71.436 ; top5 ->  90.098  and loss:  935.767045378685
forward train acc: top1 ->  70.2109375 ; top5 ->  87.640625  and loss:  250.67160195112228
test acc: top1 ->  71.292 ; top5 ->  90.09  and loss:  936.1258243918419
forward train acc: top1 ->  70.875 ; top5 ->  88.3671875  and loss:  243.74343407154083
test acc: top1 ->  71.384 ; top5 ->  90.108  and loss:  936.8328950405121
forward train acc: top1 ->  70.953125 ; top5 ->  88.4375  and loss:  241.87670451402664
test acc: top1 ->  71.428 ; top5 ->  90.112  and loss:  934.721584379673
>>>>>>> reverse layer  29  since performance drop >>>>>>>
==> this epoch:  498 / 512 , inc:  1
---------------- start layer  30  ---------------
adv train loss:  -57.87891328334808 , diff:  57.87891328334808
adv train loss:  -60.942211389541626 , diff:  3.0632981061935425
layer  30  adv train finish, try to retain  493
test acc: top1 ->  71.038 ; top5 ->  89.834  and loss:  947.0553960204124
forward train acc: top1 ->  70.78125 ; top5 ->  88.2734375  and loss:  245.116117477417
test acc: top1 ->  71.272 ; top5 ->  89.91  and loss:  944.3700529336929
forward train acc: top1 ->  69.890625 ; top5 ->  87.4765625  and loss:  253.20156401395798
test acc: top1 ->  71.162 ; top5 ->  89.946  and loss:  942.1885563135147
forward train acc: top1 ->  70.1796875 ; top5 ->  88.09375  and loss:  248.77809023857117
test acc: top1 ->  71.238 ; top5 ->  89.978  and loss:  944.0122427940369
forward train acc: top1 ->  70.0 ; top5 ->  87.9453125  and loss:  250.01171392202377
test acc: top1 ->  71.272 ; top5 ->  89.966  and loss:  940.7264249324799
forward train acc: top1 ->  69.921875 ; top5 ->  88.46875  and loss:  246.51870959997177
test acc: top1 ->  71.34 ; top5 ->  90.024  and loss:  941.2767333984375
forward train acc: top1 ->  70.9453125 ; top5 ->  88.265625  and loss:  244.16359394788742
test acc: top1 ->  71.308 ; top5 ->  90.098  and loss:  939.0764976143837
forward train acc: top1 ->  70.6015625 ; top5 ->  87.9453125  and loss:  245.41346698999405
test acc: top1 ->  71.39 ; top5 ->  90.078  and loss:  936.2924258708954
forward train acc: top1 ->  70.5546875 ; top5 ->  88.3125  and loss:  246.97706305980682
test acc: top1 ->  71.404 ; top5 ->  90.096  and loss:  936.690624833107
forward train acc: top1 ->  70.3828125 ; top5 ->  88.109375  and loss:  248.7969908118248
test acc: top1 ->  71.396 ; top5 ->  90.0  and loss:  937.2675747871399
forward train acc: top1 ->  70.5390625 ; top5 ->  87.703125  and loss:  249.77279156446457
test acc: top1 ->  71.396 ; top5 ->  90.106  and loss:  939.6526499390602
>>>>>>> reverse layer  30  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  31  ---------------
adv train loss:  -59.27215164899826 , diff:  59.27215164899826
adv train loss:  -63.20199757814407 , diff:  3.929845929145813
layer  31  adv train finish, try to retain  498
test acc: top1 ->  71.018 ; top5 ->  89.898  and loss:  947.0457543134689
forward train acc: top1 ->  70.1171875 ; top5 ->  88.2734375  and loss:  247.27657705545425
test acc: top1 ->  71.076 ; top5 ->  89.954  and loss:  946.5349418520927
forward train acc: top1 ->  69.9296875 ; top5 ->  88.1640625  and loss:  250.35290694236755
test acc: top1 ->  71.17 ; top5 ->  89.996  and loss:  943.3723220825195
forward train acc: top1 ->  70.453125 ; top5 ->  88.2578125  and loss:  244.7263988852501
test acc: top1 ->  71.112 ; top5 ->  89.912  and loss:  946.8894630074501
forward train acc: top1 ->  69.9921875 ; top5 ->  87.6796875  and loss:  251.72622162103653
test acc: top1 ->  71.274 ; top5 ->  90.03  and loss:  940.7379223108292
forward train acc: top1 ->  69.03125 ; top5 ->  87.5078125  and loss:  257.9415158033371
test acc: top1 ->  71.382 ; top5 ->  90.05  and loss:  937.2565366029739
forward train acc: top1 ->  70.4921875 ; top5 ->  88.3359375  and loss:  248.54986363649368
test acc: top1 ->  71.336 ; top5 ->  90.046  and loss:  938.9591121673584
forward train acc: top1 ->  70.015625 ; top5 ->  87.6484375  and loss:  253.54134422540665
test acc: top1 ->  71.444 ; top5 ->  90.074  and loss:  937.1420298814774
forward train acc: top1 ->  70.5703125 ; top5 ->  88.046875  and loss:  246.55578887462616
test acc: top1 ->  71.446 ; top5 ->  90.016  and loss:  937.6436275243759
forward train acc: top1 ->  71.1015625 ; top5 ->  88.09375  and loss:  243.44807517528534
test acc: top1 ->  71.41 ; top5 ->  89.974  and loss:  936.1424425244331
forward train acc: top1 ->  71.421875 ; top5 ->  88.1796875  and loss:  240.89698779582977
test acc: top1 ->  71.482 ; top5 ->  90.146  and loss:  933.4400214552879
>>>>>>> reverse layer  31  since performance drop >>>>>>>
==> this epoch:  501 / 512 , inc:  1
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.07208129882812501, 0.02703048706054688, 0.0006335270404815675, 0.02703048706054688, 0.00675762176513672, 0.14416259765625003, 0.00084470272064209, 0.00031676352024078374, 0.00675762176513672, 0.00031676352024078374, 0.00675762176513672, 0.00337881088256836, 0.00337881088256836, 0.03604064941406251, 0.00015838176012039187, 0.00015838176012039187, 0.00031676352024078374, 0.00084470272064209, 0.000422351360321045, 0.00675762176513672, 0.0006335270404815675, 0.000422351360321045, 0.00168940544128418, 0.00337881088256836, 0.00015838176012039187, 0.00031676352024078374, 0.0002111756801605225, 2.9696580022573477e-05, 2.9696580022573477e-05, 7.919088006019593e-05, 7.919088006019593e-05, 7.919088006019593e-05]  wait [2, 4, 2, 4, 2, 3, 0, 2, 3, 2, 3, 2, 2, 2, 2, 2, 3, 1, 0, 4, 4, 0, 2, 3, 2, 3, 0, 4, 4, 2, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 7
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  27  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -63.28269428014755 , diff:  63.28269428014755
adv train loss:  -61.682950258255005 , diff:  1.5997440218925476
layer  0  adv train finish, try to retain  31
test acc: top1 ->  59.334 ; top5 ->  81.618  and loss:  1380.6313356161118
forward train acc: top1 ->  68.8828125 ; top5 ->  87.1640625  and loss:  259.1399757862091
test acc: top1 ->  70.752 ; top5 ->  89.646  and loss:  956.5727505683899
forward train acc: top1 ->  69.9609375 ; top5 ->  87.8828125  and loss:  252.54846507310867
test acc: top1 ->  70.738 ; top5 ->  89.768  and loss:  953.4356557130814
forward train acc: top1 ->  70.0859375 ; top5 ->  88.3828125  and loss:  246.1150386929512
test acc: top1 ->  70.852 ; top5 ->  89.73  and loss:  953.862330853939
forward train acc: top1 ->  70.2890625 ; top5 ->  88.03125  and loss:  247.32546561956406
test acc: top1 ->  70.834 ; top5 ->  89.73  and loss:  952.4814392328262
forward train acc: top1 ->  70.125 ; top5 ->  87.7890625  and loss:  250.50382035970688
test acc: top1 ->  70.908 ; top5 ->  89.824  and loss:  949.2557926177979
forward train acc: top1 ->  70.6015625 ; top5 ->  88.5  and loss:  241.0906770825386
test acc: top1 ->  70.99 ; top5 ->  89.858  and loss:  949.8801971077919
forward train acc: top1 ->  70.453125 ; top5 ->  87.8125  and loss:  250.185780107975
test acc: top1 ->  70.896 ; top5 ->  89.878  and loss:  946.0354179739952
forward train acc: top1 ->  69.8984375 ; top5 ->  87.4140625  and loss:  250.52601355314255
test acc: top1 ->  71.012 ; top5 ->  89.886  and loss:  947.3934318423271
forward train acc: top1 ->  70.3671875 ; top5 ->  87.9765625  and loss:  248.4257891178131
test acc: top1 ->  71.012 ; top5 ->  89.902  and loss:  944.1034449934959
forward train acc: top1 ->  70.234375 ; top5 ->  88.09375  and loss:  245.5521537065506
test acc: top1 ->  71.014 ; top5 ->  89.882  and loss:  948.7542625665665
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  37 / 64 , inc:  1
---------------- start layer  1  ---------------
### skip layer  1 wait:  4  ###
---------------- start layer  2  ---------------
adv train loss:  -63.793856739997864 , diff:  63.793856739997864
adv train loss:  -60.6407567858696 , diff:  3.1530999541282654
layer  2  adv train finish, try to retain  59
>>>>>>> reverse layer  2  since no improvement >>>>>>>
---------------- start layer  3  ---------------
### skip layer  3 wait:  4  ###
---------------- start layer  4  ---------------
adv train loss:  -62.460445046424866 , diff:  62.460445046424866
adv train loss:  -60.09529745578766 , diff:  2.365147590637207
************ all values are small in this layer **********
layer  4  adv train finish, try to retain  39
test acc: top1 ->  69.2 ; top5 ->  89.12  and loss:  1003.4965950250626
forward train acc: top1 ->  69.78125 ; top5 ->  87.8828125  and loss:  250.98320597410202
test acc: top1 ->  71.086 ; top5 ->  89.912  and loss:  947.5024043917656
forward train acc: top1 ->  70.4921875 ; top5 ->  88.2890625  and loss:  245.08384066820145
test acc: top1 ->  70.95 ; top5 ->  89.95  and loss:  949.2263483405113
forward train acc: top1 ->  69.984375 ; top5 ->  88.3046875  and loss:  248.73454904556274
test acc: top1 ->  71.16 ; top5 ->  90.038  and loss:  944.9501168727875
forward train acc: top1 ->  70.8359375 ; top5 ->  87.7734375  and loss:  246.30912047624588
test acc: top1 ->  71.2 ; top5 ->  90.028  and loss:  939.8472657799721
forward train acc: top1 ->  70.421875 ; top5 ->  87.9296875  and loss:  248.5790957212448
test acc: top1 ->  71.146 ; top5 ->  90.092  and loss:  937.9470447301865
forward train acc: top1 ->  71.0390625 ; top5 ->  88.4453125  and loss:  242.98271411657333
test acc: top1 ->  71.252 ; top5 ->  90.122  and loss:  938.4001260399818
forward train acc: top1 ->  70.171875 ; top5 ->  87.515625  and loss:  249.1086773276329
test acc: top1 ->  71.292 ; top5 ->  90.092  and loss:  939.9095329046249
forward train acc: top1 ->  70.6171875 ; top5 ->  88.0859375  and loss:  245.0534524321556
test acc: top1 ->  71.36 ; top5 ->  90.16  and loss:  934.4673554897308
forward train acc: top1 ->  70.8515625 ; top5 ->  88.3359375  and loss:  243.95416444540024
test acc: top1 ->  71.28 ; top5 ->  90.172  and loss:  937.2693479061127
forward train acc: top1 ->  70.65625 ; top5 ->  88.171875  and loss:  244.08103227615356
test acc: top1 ->  71.284 ; top5 ->  90.188  and loss:  935.6050590276718
>>>>>>> reverse layer  4  since performance drop >>>>>>>
==> this epoch:  40 / 64 , inc:  1
---------------- start layer  5  ---------------
### skip layer  5 wait:  3  ###
---------------- start layer  6  ---------------
adv train loss:  -61.09398287534714 , diff:  61.09398287534714
adv train loss:  -61.61822271347046 , diff:  0.5242398381233215
layer  6  adv train finish, try to retain  108
test acc: top1 ->  68.232 ; top5 ->  88.336  and loss:  1034.0909782648087
forward train acc: top1 ->  69.8515625 ; top5 ->  88.1796875  and loss:  249.60216289758682
test acc: top1 ->  70.952 ; top5 ->  89.928  and loss:  943.7986142635345
forward train acc: top1 ->  71.0703125 ; top5 ->  87.9765625  and loss:  243.0273940563202
test acc: top1 ->  71.046 ; top5 ->  89.886  and loss:  946.5696930289268
forward train acc: top1 ->  70.7421875 ; top5 ->  88.046875  and loss:  246.06372964382172
test acc: top1 ->  70.958 ; top5 ->  89.948  and loss:  948.0514952540398
forward train acc: top1 ->  70.171875 ; top5 ->  87.890625  and loss:  246.83554351329803
test acc: top1 ->  71.106 ; top5 ->  89.958  and loss:  942.6999962329865
forward train acc: top1 ->  70.2109375 ; top5 ->  87.890625  and loss:  248.1460217833519
test acc: top1 ->  71.184 ; top5 ->  89.95  and loss:  945.0720897316933
forward train acc: top1 ->  70.3125 ; top5 ->  88.03125  and loss:  248.23760026693344
test acc: top1 ->  71.262 ; top5 ->  89.966  and loss:  939.7026700973511
forward train acc: top1 ->  71.1015625 ; top5 ->  88.640625  and loss:  238.74304729700089
test acc: top1 ->  71.28 ; top5 ->  89.99  and loss:  939.0897100567818
forward train acc: top1 ->  70.203125 ; top5 ->  87.9453125  and loss:  247.80113154649734
test acc: top1 ->  71.178 ; top5 ->  90.008  and loss:  939.2170383930206
forward train acc: top1 ->  70.875 ; top5 ->  88.1640625  and loss:  247.6169615983963
test acc: top1 ->  71.292 ; top5 ->  90.088  and loss:  936.8548448681831
forward train acc: top1 ->  71.046875 ; top5 ->  88.625  and loss:  239.8298161625862
test acc: top1 ->  71.308 ; top5 ->  90.048  and loss:  938.9171166419983
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  112 / 128 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -63.56524461507797 , diff:  63.56524461507797
adv train loss:  -62.957616209983826 , diff:  0.6076284050941467
************ all values are small in this layer **********
layer  7  adv train finish, try to retain  121
test acc: top1 ->  70.832 ; top5 ->  89.736  and loss:  961.4795961976051
forward train acc: top1 ->  70.640625 ; top5 ->  88.265625  and loss:  245.67889922857285
test acc: top1 ->  71.198 ; top5 ->  90.02  and loss:  940.0836262106895
forward train acc: top1 ->  70.8828125 ; top5 ->  88.0703125  and loss:  245.62042796611786
test acc: top1 ->  71.026 ; top5 ->  89.92  and loss:  946.3787207007408
forward train acc: top1 ->  71.046875 ; top5 ->  88.7421875  and loss:  243.3589147925377
test acc: top1 ->  71.078 ; top5 ->  89.972  and loss:  947.3675665855408
forward train acc: top1 ->  70.546875 ; top5 ->  87.8984375  and loss:  246.54769903421402
test acc: top1 ->  71.162 ; top5 ->  90.07  and loss:  942.217331290245
forward train acc: top1 ->  70.3984375 ; top5 ->  87.921875  and loss:  248.97778886556625
test acc: top1 ->  71.234 ; top5 ->  90.072  and loss:  936.2411097884178
forward train acc: top1 ->  70.75 ; top5 ->  88.265625  and loss:  244.8109508752823
test acc: top1 ->  71.272 ; top5 ->  90.012  and loss:  940.3211959600449
forward train acc: top1 ->  71.0 ; top5 ->  87.90625  and loss:  244.31762301921844
test acc: top1 ->  71.276 ; top5 ->  90.088  and loss:  938.7487159967422
forward train acc: top1 ->  70.828125 ; top5 ->  88.15625  and loss:  242.51219886541367
test acc: top1 ->  71.31 ; top5 ->  90.102  and loss:  935.7983496189117
forward train acc: top1 ->  70.640625 ; top5 ->  88.2265625  and loss:  244.94124108552933
test acc: top1 ->  71.302 ; top5 ->  90.062  and loss:  938.1233488321304
forward train acc: top1 ->  71.171875 ; top5 ->  87.953125  and loss:  244.36238127946854
test acc: top1 ->  71.378 ; top5 ->  90.112  and loss:  933.7889322042465
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  122 / 128 , inc:  1
---------------- start layer  8  ---------------
### skip layer  8 wait:  3  ###
---------------- start layer  9  ---------------
adv train loss:  -58.06796133518219 , diff:  58.06796133518219
adv train loss:  -63.233902990818024 , diff:  5.165941655635834
************ all values are small in this layer **********
layer  9  adv train finish, try to retain  114
test acc: top1 ->  70.482 ; top5 ->  89.518  and loss:  971.5370012521744
forward train acc: top1 ->  70.1328125 ; top5 ->  88.1328125  and loss:  247.36991167068481
test acc: top1 ->  71.174 ; top5 ->  89.972  and loss:  945.3130099773407
forward train acc: top1 ->  70.625 ; top5 ->  88.0859375  and loss:  242.41766625642776
test acc: top1 ->  71.262 ; top5 ->  90.104  and loss:  942.9826045632362
forward train acc: top1 ->  70.546875 ; top5 ->  88.2578125  and loss:  243.013197183609
test acc: top1 ->  71.242 ; top5 ->  89.96  and loss:  946.5173808932304
forward train acc: top1 ->  70.59375 ; top5 ->  88.546875  and loss:  243.71370500326157
test acc: top1 ->  71.366 ; top5 ->  90.072  and loss:  940.6630930900574
forward train acc: top1 ->  69.9296875 ; top5 ->  88.140625  and loss:  245.55655717849731
test acc: top1 ->  71.228 ; top5 ->  90.032  and loss:  944.2505974769592
forward train acc: top1 ->  70.6015625 ; top5 ->  87.8828125  and loss:  248.14981299638748
test acc: top1 ->  71.236 ; top5 ->  90.02  and loss:  939.6797004938126
forward train acc: top1 ->  70.515625 ; top5 ->  87.9296875  and loss:  247.60074973106384
test acc: top1 ->  71.204 ; top5 ->  90.07  and loss:  937.8311868906021
forward train acc: top1 ->  70.828125 ; top5 ->  88.2421875  and loss:  242.68368524312973
test acc: top1 ->  71.422 ; top5 ->  90.148  and loss:  937.4719506502151
forward train acc: top1 ->  70.3984375 ; top5 ->  87.8828125  and loss:  248.51277548074722
test acc: top1 ->  71.394 ; top5 ->  90.116  and loss:  938.7148458361626
forward train acc: top1 ->  70.671875 ; top5 ->  88.1875  and loss:  244.89538115262985
test acc: top1 ->  71.274 ; top5 ->  90.05  and loss:  938.281056702137
>>>>>>> reverse layer  9  since performance drop >>>>>>>
==> this epoch:  115 / 128 , inc:  1
---------------- start layer  10  ---------------
### skip layer  10 wait:  3  ###
---------------- start layer  11  ---------------
adv train loss:  -60.96656882762909 , diff:  60.96656882762909
adv train loss:  -63.21336555480957 , diff:  2.246796727180481
layer  11  adv train finish, try to retain  94
test acc: top1 ->  69.192 ; top5 ->  88.852  and loss:  1010.2093854546547
forward train acc: top1 ->  70.34375 ; top5 ->  88.1640625  and loss:  249.57522517442703
test acc: top1 ->  71.026 ; top5 ->  89.884  and loss:  949.993410885334
forward train acc: top1 ->  69.8984375 ; top5 ->  87.3515625  and loss:  256.2888180613518
test acc: top1 ->  70.946 ; top5 ->  89.916  and loss:  949.5195958018303
forward train acc: top1 ->  70.5 ; top5 ->  87.9609375  and loss:  247.54476296901703
test acc: top1 ->  71.048 ; top5 ->  90.004  and loss:  946.5359524488449
forward train acc: top1 ->  70.1796875 ; top5 ->  88.234375  and loss:  246.7751287817955
test acc: top1 ->  71.086 ; top5 ->  89.978  and loss:  946.0248634815216
forward train acc: top1 ->  69.7734375 ; top5 ->  87.7890625  and loss:  250.31531590223312
test acc: top1 ->  71.062 ; top5 ->  89.92  and loss:  943.0235833525658
forward train acc: top1 ->  70.0703125 ; top5 ->  87.8125  and loss:  251.97491246461868
test acc: top1 ->  71.172 ; top5 ->  90.084  and loss:  939.7271754145622
forward train acc: top1 ->  69.78125 ; top5 ->  88.015625  and loss:  251.0816822052002
test acc: top1 ->  71.23 ; top5 ->  90.022  and loss:  940.6458008885384
forward train acc: top1 ->  70.546875 ; top5 ->  88.3046875  and loss:  245.46773046255112
test acc: top1 ->  71.196 ; top5 ->  90.038  and loss:  940.8067475557327
forward train acc: top1 ->  71.1484375 ; top5 ->  88.5  and loss:  239.41271901130676
test acc: top1 ->  71.218 ; top5 ->  90.016  and loss:  937.6893990039825
forward train acc: top1 ->  70.8671875 ; top5 ->  87.984375  and loss:  245.16973263025284
test acc: top1 ->  71.288 ; top5 ->  90.098  and loss:  940.3276546597481
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -58.81255668401718 , diff:  58.81255668401718
adv train loss:  -59.24537855386734 , diff:  0.4328218698501587
layer  12  adv train finish, try to retain  95
test acc: top1 ->  70.26 ; top5 ->  89.506  and loss:  968.3983073234558
forward train acc: top1 ->  71.1796875 ; top5 ->  88.2890625  and loss:  242.8392231464386
test acc: top1 ->  71.064 ; top5 ->  89.846  and loss:  945.5789346694946
forward train acc: top1 ->  70.5703125 ; top5 ->  88.0390625  and loss:  246.79918587207794
test acc: top1 ->  71.108 ; top5 ->  89.798  and loss:  945.3798149824142
forward train acc: top1 ->  70.7578125 ; top5 ->  88.078125  and loss:  248.84603106975555
test acc: top1 ->  71.0 ; top5 ->  89.802  and loss:  950.0850404500961
forward train acc: top1 ->  70.6796875 ; top5 ->  88.34375  and loss:  246.4844126701355
test acc: top1 ->  71.108 ; top5 ->  89.906  and loss:  943.2035872340202
forward train acc: top1 ->  69.9296875 ; top5 ->  87.75  and loss:  250.3569124341011
test acc: top1 ->  71.196 ; top5 ->  89.872  and loss:  944.2379238605499
forward train acc: top1 ->  71.1171875 ; top5 ->  88.875  and loss:  238.58338403701782
test acc: top1 ->  71.228 ; top5 ->  89.894  and loss:  943.8105482459068
forward train acc: top1 ->  70.53125 ; top5 ->  88.1796875  and loss:  246.93286073207855
test acc: top1 ->  71.166 ; top5 ->  89.894  and loss:  942.6880760192871
forward train acc: top1 ->  70.46875 ; top5 ->  88.2890625  and loss:  242.94300264120102
test acc: top1 ->  71.202 ; top5 ->  89.928  and loss:  942.6635650396347
forward train acc: top1 ->  70.703125 ; top5 ->  88.1640625  and loss:  244.64046889543533
test acc: top1 ->  71.204 ; top5 ->  89.984  and loss:  941.2098407149315
forward train acc: top1 ->  70.1875 ; top5 ->  87.9140625  and loss:  245.86214935779572
test acc: top1 ->  71.218 ; top5 ->  89.976  and loss:  940.5371077656746
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  98 / 128 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -60.70761555433273 , diff:  60.70761555433273
adv train loss:  -61.63731038570404 , diff:  0.9296948313713074
layer  13  adv train finish, try to retain  64
test acc: top1 ->  66.922 ; top5 ->  87.252  and loss:  1101.7751165628433
forward train acc: top1 ->  69.6875 ; top5 ->  87.265625  and loss:  253.51217019557953
test acc: top1 ->  70.928 ; top5 ->  89.768  and loss:  953.6785924434662
forward train acc: top1 ->  69.46875 ; top5 ->  87.578125  and loss:  253.3590745329857
test acc: top1 ->  70.862 ; top5 ->  89.846  and loss:  954.3527503609657
forward train acc: top1 ->  70.4609375 ; top5 ->  87.7734375  and loss:  248.21457970142365
test acc: top1 ->  70.788 ; top5 ->  89.854  and loss:  952.3588908314705
forward train acc: top1 ->  69.9453125 ; top5 ->  87.5234375  and loss:  249.35239505767822
test acc: top1 ->  70.944 ; top5 ->  89.852  and loss:  950.9448583722115
forward train acc: top1 ->  69.7265625 ; top5 ->  87.6953125  and loss:  253.49176859855652
test acc: top1 ->  70.968 ; top5 ->  89.908  and loss:  951.4390328526497
forward train acc: top1 ->  70.09375 ; top5 ->  87.9921875  and loss:  249.96167188882828
test acc: top1 ->  71.032 ; top5 ->  89.964  and loss:  948.3667675256729
forward train acc: top1 ->  70.5078125 ; top5 ->  87.890625  and loss:  250.32652920484543
test acc: top1 ->  70.98 ; top5 ->  89.946  and loss:  948.8653616905212
forward train acc: top1 ->  70.1796875 ; top5 ->  87.890625  and loss:  250.20670276880264
test acc: top1 ->  71.064 ; top5 ->  90.018  and loss:  944.9684401750565
forward train acc: top1 ->  70.5703125 ; top5 ->  87.640625  and loss:  249.54165643453598
test acc: top1 ->  71.002 ; top5 ->  90.034  and loss:  945.6769062280655
forward train acc: top1 ->  70.65625 ; top5 ->  87.7421875  and loss:  247.89529126882553
test acc: top1 ->  71.146 ; top5 ->  89.996  and loss:  945.5443077683449
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  71 / 128 , inc:  1
---------------- start layer  14  ---------------
adv train loss:  -61.15969842672348 , diff:  61.15969842672348
adv train loss:  -60.085234463214874 , diff:  1.074463963508606
layer  14  adv train finish, try to retain  246
test acc: top1 ->  70.47 ; top5 ->  89.518  and loss:  965.6895020604134
forward train acc: top1 ->  71.3203125 ; top5 ->  88.171875  and loss:  242.9504354596138
test acc: top1 ->  71.21 ; top5 ->  89.962  and loss:  945.7416486144066
forward train acc: top1 ->  70.515625 ; top5 ->  88.2890625  and loss:  246.89718306064606
test acc: top1 ->  71.174 ; top5 ->  90.0  and loss:  942.1647285223007
forward train acc: top1 ->  70.359375 ; top5 ->  88.2734375  and loss:  246.0194748044014
test acc: top1 ->  71.25 ; top5 ->  90.052  and loss:  945.0921858549118
forward train acc: top1 ->  70.4921875 ; top5 ->  88.0546875  and loss:  245.78673434257507
test acc: top1 ->  71.256 ; top5 ->  90.004  and loss:  940.6378895044327
forward train acc: top1 ->  70.6484375 ; top5 ->  88.359375  and loss:  242.6622969508171
test acc: top1 ->  71.338 ; top5 ->  90.072  and loss:  940.3501096367836
forward train acc: top1 ->  70.4375 ; top5 ->  88.03125  and loss:  248.05858951807022
test acc: top1 ->  71.338 ; top5 ->  90.018  and loss:  940.2150762677193
forward train acc: top1 ->  70.96875 ; top5 ->  88.203125  and loss:  244.83927017450333
test acc: top1 ->  71.326 ; top5 ->  90.076  and loss:  937.9926370978355
forward train acc: top1 ->  70.6953125 ; top5 ->  88.2578125  and loss:  244.49937909841537
test acc: top1 ->  71.416 ; top5 ->  90.082  and loss:  939.7648555636406
forward train acc: top1 ->  70.671875 ; top5 ->  88.3359375  and loss:  242.84380614757538
test acc: top1 ->  71.384 ; top5 ->  90.17  and loss:  934.3624796271324
forward train acc: top1 ->  70.2890625 ; top5 ->  88.3046875  and loss:  244.44187313318253
test acc: top1 ->  71.426 ; top5 ->  90.114  and loss:  938.9818522930145
>>>>>>> reverse layer  14  since performance drop >>>>>>>
==> this epoch:  250 / 256 , inc:  1
---------------- start layer  15  ---------------
adv train loss:  -60.48579454421997 , diff:  60.48579454421997
adv train loss:  -59.060419380664825 , diff:  1.4253751635551453
layer  15  adv train finish, try to retain  248
>>>>>>> reverse layer  15  since no improvement >>>>>>>
---------------- start layer  16  ---------------
### skip layer  16 wait:  3  ###
---------------- start layer  17  ---------------
adv train loss:  -61.353688299655914 , diff:  61.353688299655914
adv train loss:  -59.91351532936096 , diff:  1.4401729702949524
layer  17  adv train finish, try to retain  226
test acc: top1 ->  70.81 ; top5 ->  89.872  and loss:  953.0499606728554
forward train acc: top1 ->  69.984375 ; top5 ->  87.671875  and loss:  253.03361278772354
test acc: top1 ->  71.1 ; top5 ->  90.0  and loss:  945.3498966693878
forward train acc: top1 ->  70.5 ; top5 ->  88.03125  and loss:  243.3923186659813
test acc: top1 ->  71.156 ; top5 ->  90.002  and loss:  945.8987644314766
forward train acc: top1 ->  70.5234375 ; top5 ->  87.9453125  and loss:  246.89019733667374
test acc: top1 ->  71.068 ; top5 ->  89.976  and loss:  945.2799125313759
forward train acc: top1 ->  70.8359375 ; top5 ->  88.2109375  and loss:  246.1957527399063
test acc: top1 ->  71.208 ; top5 ->  90.01  and loss:  938.7463480234146
forward train acc: top1 ->  71.375 ; top5 ->  88.546875  and loss:  240.86897200345993
test acc: top1 ->  71.192 ; top5 ->  90.092  and loss:  939.9660401344299
forward train acc: top1 ->  70.703125 ; top5 ->  87.859375  and loss:  249.40466445684433
test acc: top1 ->  71.22 ; top5 ->  90.122  and loss:  940.4309546351433
forward train acc: top1 ->  70.1796875 ; top5 ->  87.59375  and loss:  253.5447689294815
test acc: top1 ->  71.29 ; top5 ->  90.198  and loss:  936.6847459077835
forward train acc: top1 ->  70.6015625 ; top5 ->  88.109375  and loss:  246.41502404212952
test acc: top1 ->  71.316 ; top5 ->  90.2  and loss:  936.7226923704147
forward train acc: top1 ->  70.7890625 ; top5 ->  88.21875  and loss:  244.9215945005417
test acc: top1 ->  71.32 ; top5 ->  90.154  and loss:  935.8614005446434
forward train acc: top1 ->  70.640625 ; top5 ->  88.2109375  and loss:  245.90302228927612
test acc: top1 ->  71.358 ; top5 ->  90.186  and loss:  936.020489692688
>>>>>>> reverse layer  17  since performance drop >>>>>>>
==> this epoch:  230 / 256 , inc:  1
---------------- start layer  18  ---------------
adv train loss:  -58.732625007629395 , diff:  58.732625007629395
adv train loss:  -62.67476361989975 , diff:  3.9421386122703552
layer  18  adv train finish, try to retain  222
test acc: top1 ->  71.126 ; top5 ->  89.966  and loss:  944.7770015597343
forward train acc: top1 ->  70.8046875 ; top5 ->  88.28125  and loss:  243.01011675596237
test acc: top1 ->  71.226 ; top5 ->  90.016  and loss:  939.2230166196823
forward train acc: top1 ->  70.9921875 ; top5 ->  88.4140625  and loss:  242.60502409934998
test acc: top1 ->  71.258 ; top5 ->  90.074  and loss:  942.1302306056023
forward train acc: top1 ->  70.5546875 ; top5 ->  88.15625  and loss:  247.54373043775558
test acc: top1 ->  71.212 ; top5 ->  90.044  and loss:  944.3085898756981
forward train acc: top1 ->  70.0 ; top5 ->  88.0859375  and loss:  249.1169469356537
test acc: top1 ->  71.212 ; top5 ->  90.086  and loss:  942.834831237793
forward train acc: top1 ->  70.6484375 ; top5 ->  87.9375  and loss:  247.17611688375473
test acc: top1 ->  71.28 ; top5 ->  90.072  and loss:  938.2043526768684
forward train acc: top1 ->  70.8125 ; top5 ->  88.5546875  and loss:  241.16982871294022
test acc: top1 ->  71.232 ; top5 ->  90.096  and loss:  937.6817440390587
forward train acc: top1 ->  70.8203125 ; top5 ->  87.9453125  and loss:  247.35624760389328
test acc: top1 ->  71.264 ; top5 ->  90.134  and loss:  937.4323551654816
forward train acc: top1 ->  71.421875 ; top5 ->  88.640625  and loss:  239.47394329309464
test acc: top1 ->  71.308 ; top5 ->  90.08  and loss:  938.9834187626839
forward train acc: top1 ->  70.765625 ; top5 ->  87.984375  and loss:  246.7733432650566
test acc: top1 ->  71.366 ; top5 ->  90.122  and loss:  935.8521073460579
forward train acc: top1 ->  70.21875 ; top5 ->  88.15625  and loss:  245.59980410337448
test acc: top1 ->  71.378 ; top5 ->  90.17  and loss:  934.6437678337097
>>>>>>> reverse layer  18  since performance drop >>>>>>>
==> this epoch:  235 / 256 , inc:  1
---------------- start layer  19  ---------------
### skip layer  19 wait:  4  ###
---------------- start layer  20  ---------------
### skip layer  20 wait:  4  ###
---------------- start layer  21  ---------------
adv train loss:  -60.423678398132324 , diff:  60.423678398132324
adv train loss:  -59.91781044006348 , diff:  0.5058679580688477
layer  21  adv train finish, try to retain  227
test acc: top1 ->  71.312 ; top5 ->  90.014  and loss:  948.4933719038963
forward train acc: top1 ->  70.3515625 ; top5 ->  88.2578125  and loss:  243.96407675743103
test acc: top1 ->  71.238 ; top5 ->  90.042  and loss:  943.7221776843071
forward train acc: top1 ->  71.203125 ; top5 ->  88.0546875  and loss:  244.0454679131508
test acc: top1 ->  71.202 ; top5 ->  90.064  and loss:  941.9221578240395
forward train acc: top1 ->  71.03125 ; top5 ->  88.2421875  and loss:  243.21939712762833
test acc: top1 ->  71.194 ; top5 ->  90.044  and loss:  943.0964391827583
forward train acc: top1 ->  70.078125 ; top5 ->  87.9453125  and loss:  250.16779625415802
test acc: top1 ->  71.33 ; top5 ->  90.054  and loss:  939.8367078900337
forward train acc: top1 ->  70.71875 ; top5 ->  88.671875  and loss:  238.82150381803513
test acc: top1 ->  71.368 ; top5 ->  90.11  and loss:  938.0530548095703
forward train acc: top1 ->  71.3125 ; top5 ->  88.3828125  and loss:  242.1726335287094
test acc: top1 ->  71.362 ; top5 ->  90.042  and loss:  938.5070357322693
forward train acc: top1 ->  70.6875 ; top5 ->  88.46875  and loss:  241.4510149359703
test acc: top1 ->  71.416 ; top5 ->  90.124  and loss:  935.9377284049988
forward train acc: top1 ->  71.265625 ; top5 ->  88.578125  and loss:  239.62914192676544
test acc: top1 ->  71.434 ; top5 ->  90.114  and loss:  935.6752533912659
forward train acc: top1 ->  70.375 ; top5 ->  88.109375  and loss:  245.91205888986588
test acc: top1 ->  71.388 ; top5 ->  90.098  and loss:  937.2944731712341
forward train acc: top1 ->  71.125 ; top5 ->  88.234375  and loss:  244.48048853874207
test acc: top1 ->  71.35 ; top5 ->  90.126  and loss:  936.0001377463341
>>>>>>> reverse layer  21  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  22  ---------------
adv train loss:  -57.85745769739151 , diff:  57.85745769739151
adv train loss:  -61.431948602199554 , diff:  3.5744909048080444
layer  22  adv train finish, try to retain  210
>>>>>>> reverse layer  22  since no improvement >>>>>>>
---------------- start layer  23  ---------------
### skip layer  23 wait:  3  ###
---------------- start layer  24  ---------------
adv train loss:  -60.405613243579865 , diff:  60.405613243579865
adv train loss:  -60.93784707784653 , diff:  0.5322338342666626
layer  24  adv train finish, try to retain  251
>>>>>>> reverse layer  24  since no improvement >>>>>>>
---------------- start layer  25  ---------------
### skip layer  25 wait:  3  ###
---------------- start layer  26  ---------------
adv train loss:  -58.32479250431061 , diff:  58.32479250431061
adv train loss:  -57.97088027000427 , diff:  0.35391223430633545
layer  26  adv train finish, try to retain  482
test acc: top1 ->  70.52 ; top5 ->  89.612  and loss:  958.8906081914902
forward train acc: top1 ->  70.2734375 ; top5 ->  88.0703125  and loss:  246.93164867162704
test acc: top1 ->  70.92 ; top5 ->  89.944  and loss:  948.582362473011
forward train acc: top1 ->  71.296875 ; top5 ->  88.453125  and loss:  240.4660006761551
test acc: top1 ->  70.95 ; top5 ->  89.89  and loss:  947.2832742333412
forward train acc: top1 ->  70.6953125 ; top5 ->  88.1015625  and loss:  244.7930001616478
test acc: top1 ->  70.986 ; top5 ->  89.892  and loss:  949.6459084749222
forward train acc: top1 ->  70.34375 ; top5 ->  87.9453125  and loss:  249.3508775830269
test acc: top1 ->  71.096 ; top5 ->  89.92  and loss:  947.5489540100098
forward train acc: top1 ->  70.625 ; top5 ->  88.4765625  and loss:  243.74194687604904
test acc: top1 ->  71.154 ; top5 ->  89.924  and loss:  943.8142060637474
forward train acc: top1 ->  70.6875 ; top5 ->  87.890625  and loss:  245.5036257505417
test acc: top1 ->  71.246 ; top5 ->  89.996  and loss:  943.0796006321907
forward train acc: top1 ->  70.7265625 ; top5 ->  88.2265625  and loss:  244.71786445379257
test acc: top1 ->  71.168 ; top5 ->  89.98  and loss:  943.5197469592094
forward train acc: top1 ->  70.765625 ; top5 ->  88.4375  and loss:  243.48780900239944
test acc: top1 ->  71.302 ; top5 ->  89.974  and loss:  939.8100521564484
forward train acc: top1 ->  70.5 ; top5 ->  87.9453125  and loss:  245.06619983911514
test acc: top1 ->  71.302 ; top5 ->  89.922  and loss:  939.702466905117
forward train acc: top1 ->  70.6015625 ; top5 ->  88.0546875  and loss:  247.75442039966583
test acc: top1 ->  71.228 ; top5 ->  90.032  and loss:  941.5201458334923
>>>>>>> reverse layer  26  since performance drop >>>>>>>
==> this epoch:  502 / 512 , inc:  1
---------------- start layer  27  ---------------
### skip layer  27 wait:  4  ###
---------------- start layer  28  ---------------
### skip layer  28 wait:  4  ###
---------------- start layer  29  ---------------
adv train loss:  -57.9345480799675 , diff:  57.9345480799675
adv train loss:  -62.151067316532135 , diff:  4.216519236564636
layer  29  adv train finish, try to retain  501
>>>>>>> reverse layer  29  since no improvement >>>>>>>
---------------- start layer  30  ---------------
adv train loss:  -62.733330726623535 , diff:  62.733330726623535
adv train loss:  -61.821025133132935 , diff:  0.9123055934906006
layer  30  adv train finish, try to retain  505
>>>>>>> reverse layer  30  since no improvement >>>>>>>
---------------- start layer  31  ---------------
adv train loss:  -57.95967274904251 , diff:  57.95967274904251
adv train loss:  -63.38933193683624 , diff:  5.429659187793732
layer  31  adv train finish, try to retain  506
>>>>>>> reverse layer  31  since no improvement >>>>>>>
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.05406097412109376, 0.02703048706054688, 0.001267054080963135, 0.02703048706054688, 0.00506821632385254, 0.14416259765625003, 0.0006335270404815675, 0.00023757264018058782, 0.00675762176513672, 0.00023757264018058782, 0.00675762176513672, 0.00253410816192627, 0.00253410816192627, 0.02703048706054688, 0.00011878632009029391, 0.00031676352024078374, 0.00031676352024078374, 0.0006335270404815675, 0.00031676352024078374, 0.00675762176513672, 0.0006335270404815675, 0.00031676352024078374, 0.00337881088256836, 0.00337881088256836, 0.00031676352024078374, 0.00031676352024078374, 0.00015838176012039187, 2.9696580022573477e-05, 2.9696580022573477e-05, 0.00015838176012039187, 0.00015838176012039187, 0.00015838176012039187]  wait [4, 3, 2, 3, 4, 2, 2, 4, 2, 4, 2, 4, 4, 4, 4, 2, 2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 7
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  28  $$$$$$$$$$$$
---------------- start layer  0  ---------------
adv train loss:  -60.0033597946167 , diff:  60.0033597946167
adv train loss:  -60.235146045684814 , diff:  0.23178625106811523
layer  0  adv train finish, try to retain  32
test acc: top1 ->  64.362 ; top5 ->  85.634  and loss:  1169.8010540008545
forward train acc: top1 ->  69.65625 ; top5 ->  87.9296875  and loss:  248.38967740535736
test acc: top1 ->  70.86 ; top5 ->  89.796  and loss:  952.7709448337555
forward train acc: top1 ->  70.21875 ; top5 ->  87.765625  and loss:  248.87239253520966
test acc: top1 ->  70.936 ; top5 ->  89.854  and loss:  952.0921460986137
forward train acc: top1 ->  70.8671875 ; top5 ->  88.3671875  and loss:  243.2399302124977
test acc: top1 ->  70.952 ; top5 ->  89.786  and loss:  952.721941113472
forward train acc: top1 ->  69.8359375 ; top5 ->  87.9375  and loss:  249.04640185832977
test acc: top1 ->  71.016 ; top5 ->  89.842  and loss:  948.4455313682556
forward train acc: top1 ->  69.9296875 ; top5 ->  87.9140625  and loss:  250.51045680046082
test acc: top1 ->  71.044 ; top5 ->  89.894  and loss:  948.8216839432716
forward train acc: top1 ->  70.0 ; top5 ->  88.1484375  and loss:  250.2241204380989
test acc: top1 ->  71.172 ; top5 ->  89.918  and loss:  944.8973892331123
forward train acc: top1 ->  70.3359375 ; top5 ->  87.8828125  and loss:  247.26392596960068
test acc: top1 ->  71.162 ; top5 ->  89.968  and loss:  944.0986658334732
forward train acc: top1 ->  70.8359375 ; top5 ->  88.0625  and loss:  244.51095736026764
test acc: top1 ->  71.172 ; top5 ->  89.998  and loss:  941.0248789787292
forward train acc: top1 ->  71.0078125 ; top5 ->  88.4375  and loss:  244.4298774600029
test acc: top1 ->  71.122 ; top5 ->  89.916  and loss:  943.31994754076
forward train acc: top1 ->  70.3515625 ; top5 ->  87.8515625  and loss:  248.98744785785675
test acc: top1 ->  71.244 ; top5 ->  89.976  and loss:  943.1579028367996
>>>>>>> reverse layer  0  since performance drop >>>>>>>
==> this epoch:  37 / 64 , inc:  1
---------------- start layer  1  ---------------
adv train loss:  -60.8298077583313 , diff:  60.8298077583313
adv train loss:  -60.97311043739319 , diff:  0.14330267906188965
************ all values are small in this layer **********
layer  1  adv train finish, try to retain  43
test acc: top1 ->  65.324 ; top5 ->  86.06  and loss:  1153.360084593296
forward train acc: top1 ->  69.46875 ; top5 ->  87.3515625  and loss:  257.21018075942993
test acc: top1 ->  70.66 ; top5 ->  89.818  and loss:  956.458721101284
forward train acc: top1 ->  69.6484375 ; top5 ->  87.7421875  and loss:  253.05644804239273
test acc: top1 ->  70.906 ; top5 ->  89.88  and loss:  949.8837206959724
forward train acc: top1 ->  70.7578125 ; top5 ->  87.9375  and loss:  244.738647043705
test acc: top1 ->  70.916 ; top5 ->  89.864  and loss:  948.5860206484795
forward train acc: top1 ->  69.8125 ; top5 ->  87.6953125  and loss:  249.01342058181763
test acc: top1 ->  71.098 ; top5 ->  89.906  and loss:  946.7233626842499
forward train acc: top1 ->  70.671875 ; top5 ->  88.3125  and loss:  245.05873399972916
test acc: top1 ->  71.072 ; top5 ->  89.936  and loss:  945.4901685714722
forward train acc: top1 ->  70.015625 ; top5 ->  88.6796875  and loss:  245.628368973732
test acc: top1 ->  71.014 ; top5 ->  89.962  and loss:  947.6486759781837
forward train acc: top1 ->  71.2578125 ; top5 ->  88.59375  and loss:  238.11473774909973
test acc: top1 ->  71.042 ; top5 ->  89.922  and loss:  944.4419454336166
forward train acc: top1 ->  70.703125 ; top5 ->  87.8125  and loss:  247.07910853624344
test acc: top1 ->  71.07 ; top5 ->  89.946  and loss:  946.265875518322
forward train acc: top1 ->  70.6953125 ; top5 ->  88.2421875  and loss:  244.89611500501633
test acc: top1 ->  71.024 ; top5 ->  89.952  and loss:  945.2401957511902
forward train acc: top1 ->  70.8125 ; top5 ->  88.484375  and loss:  243.26321923732758
test acc: top1 ->  71.158 ; top5 ->  89.972  and loss:  943.9147847890854
>>>>>>> reverse layer  1  since performance drop >>>>>>>
==> this epoch:  44 / 64 , inc:  1
---------------- start layer  2  ---------------
adv train loss:  -62.20325583219528 , diff:  62.20325583219528
adv train loss:  -62.15946924686432 , diff:  0.043786585330963135
layer  2  adv train finish, try to retain  51
test acc: top1 ->  68.362 ; top5 ->  88.134  and loss:  1038.7538923025131
forward train acc: top1 ->  69.9453125 ; top5 ->  87.71875  and loss:  252.94281148910522
test acc: top1 ->  71.012 ; top5 ->  89.95  and loss:  950.0850512385368
forward train acc: top1 ->  69.9296875 ; top5 ->  87.4140625  and loss:  252.708331823349
test acc: top1 ->  71.272 ; top5 ->  89.888  and loss:  944.5191169977188
forward train acc: top1 ->  70.5625 ; top5 ->  87.828125  and loss:  246.21190851926804
test acc: top1 ->  71.136 ; top5 ->  89.904  and loss:  947.2201744914055
forward train acc: top1 ->  70.2734375 ; top5 ->  88.359375  and loss:  245.73796045780182
test acc: top1 ->  71.162 ; top5 ->  89.914  and loss:  945.1949002742767
forward train acc: top1 ->  71.2890625 ; top5 ->  88.34375  and loss:  242.45107918977737
test acc: top1 ->  71.242 ; top5 ->  89.99  and loss:  941.0596963763237
forward train acc: top1 ->  70.625 ; top5 ->  88.421875  and loss:  242.8738203048706
test acc: top1 ->  71.306 ; top5 ->  90.004  and loss:  941.2079802751541
forward train acc: top1 ->  70.578125 ; top5 ->  88.1953125  and loss:  245.2764528989792
test acc: top1 ->  71.198 ; top5 ->  90.026  and loss:  939.4835936427116
forward train acc: top1 ->  70.484375 ; top5 ->  88.484375  and loss:  242.60716527700424
test acc: top1 ->  71.218 ; top5 ->  89.97  and loss:  941.6183218359947
forward train acc: top1 ->  71.078125 ; top5 ->  88.453125  and loss:  240.76507604122162
test acc: top1 ->  71.236 ; top5 ->  90.048  and loss:  939.920000731945
forward train acc: top1 ->  69.6875 ; top5 ->  87.921875  and loss:  248.25488156080246
test acc: top1 ->  71.408 ; top5 ->  90.058  and loss:  936.7623022198677
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  57 / 64 , inc:  1
---------------- start layer  3  ---------------
adv train loss:  -59.74808716773987 , diff:  59.74808716773987
adv train loss:  -61.575073182582855 , diff:  1.826986014842987
************ all values are small in this layer **********
layer  3  adv train finish, try to retain  42
test acc: top1 ->  67.878 ; top5 ->  88.022  and loss:  1058.8179904222488
forward train acc: top1 ->  70.2734375 ; top5 ->  88.1796875  and loss:  247.7453931570053
test acc: top1 ->  70.732 ; top5 ->  89.768  and loss:  958.3782424330711
forward train acc: top1 ->  70.15625 ; top5 ->  87.5  and loss:  251.24327367544174
test acc: top1 ->  70.866 ; top5 ->  89.834  and loss:  950.8834784030914
forward train acc: top1 ->  70.0703125 ; top5 ->  88.046875  and loss:  248.00641334056854
test acc: top1 ->  70.918 ; top5 ->  89.874  and loss:  951.4238011240959
forward train acc: top1 ->  70.046875 ; top5 ->  87.59375  and loss:  249.82424688339233
test acc: top1 ->  71.078 ; top5 ->  89.926  and loss:  945.3124461174011
forward train acc: top1 ->  69.984375 ; top5 ->  88.1484375  and loss:  250.2811010479927
test acc: top1 ->  71.1 ; top5 ->  89.864  and loss:  944.5445224642754
forward train acc: top1 ->  70.46875 ; top5 ->  88.296875  and loss:  243.23513054847717
test acc: top1 ->  71.13 ; top5 ->  89.902  and loss:  948.9574443697929
forward train acc: top1 ->  70.4140625 ; top5 ->  87.8203125  and loss:  249.99866741895676
test acc: top1 ->  71.152 ; top5 ->  89.95  and loss:  945.7666923999786
forward train acc: top1 ->  70.953125 ; top5 ->  88.3359375  and loss:  244.11442416906357
test acc: top1 ->  71.164 ; top5 ->  89.924  and loss:  945.7814484834671
forward train acc: top1 ->  70.0625 ; top5 ->  88.296875  and loss:  247.40419334173203
test acc: top1 ->  71.18 ; top5 ->  89.914  and loss:  945.4750694036484
forward train acc: top1 ->  70.28125 ; top5 ->  87.7890625  and loss:  251.14902806282043
test acc: top1 ->  71.252 ; top5 ->  89.978  and loss:  944.4657920002937
>>>>>>> reverse layer  3  since performance drop >>>>>>>
==> this epoch:  43 / 64 , inc:  1
---------------- start layer  4  ---------------
adv train loss:  -57.91205823421478 , diff:  57.91205823421478
adv train loss:  -61.273140370845795 , diff:  3.361082136631012
layer  4  adv train finish, try to retain  50
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -60.61820638179779 , diff:  60.61820638179779
adv train loss:  -63.261554479599 , diff:  2.6433480978012085
layer  5  adv train finish, try to retain  29
>>>>>>> reverse layer  5  since no improvement >>>>>>>
---------------- start layer  6  ---------------
adv train loss:  -59.56990963220596 , diff:  59.56990963220596
adv train loss:  -62.23520082235336 , diff:  2.6652911901474
layer  6  adv train finish, try to retain  107
test acc: top1 ->  67.972 ; top5 ->  88.01  and loss:  1067.065645635128
forward train acc: top1 ->  70.703125 ; top5 ->  88.234375  and loss:  245.82686495780945
test acc: top1 ->  71.05 ; top5 ->  89.956  and loss:  946.7074691653252
forward train acc: top1 ->  71.0 ; top5 ->  87.890625  and loss:  244.03178161382675
test acc: top1 ->  71.026 ; top5 ->  89.884  and loss:  947.4182170629501
forward train acc: top1 ->  70.296875 ; top5 ->  88.09375  and loss:  247.2761424779892
test acc: top1 ->  71.008 ; top5 ->  89.886  and loss:  948.6735100746155
forward train acc: top1 ->  71.4453125 ; top5 ->  88.3984375  and loss:  242.01027655601501
test acc: top1 ->  71.18 ; top5 ->  89.976  and loss:  943.3156614899635
forward train acc: top1 ->  70.25 ; top5 ->  88.171875  and loss:  245.34795981645584
test acc: top1 ->  71.204 ; top5 ->  89.906  and loss:  942.6481584906578
forward train acc: top1 ->  70.578125 ; top5 ->  88.1328125  and loss:  245.73625814914703
test acc: top1 ->  71.29 ; top5 ->  90.01  and loss:  939.7075305581093
forward train acc: top1 ->  71.4140625 ; top5 ->  88.796875  and loss:  236.57493352890015
test acc: top1 ->  71.176 ; top5 ->  90.066  and loss:  939.4190810322762
forward train acc: top1 ->  70.6484375 ; top5 ->  87.875  and loss:  244.67689090967178
test acc: top1 ->  71.308 ; top5 ->  90.056  and loss:  937.4925304055214
forward train acc: top1 ->  71.390625 ; top5 ->  88.6171875  and loss:  238.76313173770905
test acc: top1 ->  71.27 ; top5 ->  90.002  and loss:  939.1469404101372
forward train acc: top1 ->  71.5703125 ; top5 ->  88.6875  and loss:  240.5215418934822
test acc: top1 ->  71.288 ; top5 ->  90.046  and loss:  937.2374327778816
>>>>>>> reverse layer  6  since performance drop >>>>>>>
==> this epoch:  112 / 128 , inc:  1
---------------- start layer  7  ---------------
adv train loss:  -58.309500336647034 , diff:  58.309500336647034
adv train loss:  -60.072311103343964 , diff:  1.76281076669693
layer  7  adv train finish, try to retain  123
>>>>>>> reverse layer  7  since no improvement >>>>>>>
---------------- start layer  8  ---------------
adv train loss:  -58.888375639915466 , diff:  58.888375639915466
adv train loss:  -60.82572364807129 , diff:  1.9373480081558228
************ all values are small in this layer **********
layer  8  adv train finish, try to retain  86
test acc: top1 ->  69.276 ; top5 ->  88.86  and loss:  1012.5751253962517
forward train acc: top1 ->  69.90625 ; top5 ->  87.6953125  and loss:  253.3003444671631
test acc: top1 ->  70.982 ; top5 ->  89.796  and loss:  952.9977370500565
forward train acc: top1 ->  70.359375 ; top5 ->  87.9375  and loss:  252.4033356308937
test acc: top1 ->  70.948 ; top5 ->  89.812  and loss:  952.8562209606171
forward train acc: top1 ->  70.546875 ; top5 ->  87.78125  and loss:  250.03540301322937
test acc: top1 ->  70.98 ; top5 ->  89.882  and loss:  949.5950053334236
forward train acc: top1 ->  70.375 ; top5 ->  88.1171875  and loss:  248.0526820421219
test acc: top1 ->  71.046 ; top5 ->  89.884  and loss:  946.2794242501259
forward train acc: top1 ->  69.9765625 ; top5 ->  87.8984375  and loss:  248.48101496696472
test acc: top1 ->  71.132 ; top5 ->  89.922  and loss:  944.3796190619469
forward train acc: top1 ->  71.1484375 ; top5 ->  88.265625  and loss:  243.69938212633133
test acc: top1 ->  71.162 ; top5 ->  89.914  and loss:  941.6836330294609
forward train acc: top1 ->  70.4296875 ; top5 ->  88.15625  and loss:  246.37075263261795
test acc: top1 ->  71.004 ; top5 ->  89.908  and loss:  945.2412492632866
forward train acc: top1 ->  70.515625 ; top5 ->  88.328125  and loss:  244.53144043684006
test acc: top1 ->  71.214 ; top5 ->  89.906  and loss:  943.0462656617165
forward train acc: top1 ->  70.265625 ; top5 ->  88.1875  and loss:  246.6609724164009
test acc: top1 ->  71.182 ; top5 ->  89.906  and loss:  943.2699752449989
forward train acc: top1 ->  70.921875 ; top5 ->  88.09375  and loss:  246.36956983804703
test acc: top1 ->  71.202 ; top5 ->  90.0  and loss:  941.181776881218
>>>>>>> reverse layer  8  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  9  ---------------
adv train loss:  -59.450083911418915 , diff:  59.450083911418915
adv train loss:  -63.02301466464996 , diff:  3.5729307532310486
layer  9  adv train finish, try to retain  119
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -63.671547651290894 , diff:  63.671547651290894
adv train loss:  -61.54787105321884 , diff:  2.123676598072052
************ all values are small in this layer **********
layer  10  adv train finish, try to retain  86
test acc: top1 ->  70.036 ; top5 ->  89.218  and loss:  987.9509207606316
forward train acc: top1 ->  70.3984375 ; top5 ->  87.9140625  and loss:  246.3243265748024
test acc: top1 ->  71.114 ; top5 ->  89.936  and loss:  946.0309516787529
forward train acc: top1 ->  70.6171875 ; top5 ->  88.2578125  and loss:  244.68654358386993
test acc: top1 ->  71.058 ; top5 ->  89.92  and loss:  946.8620334863663
forward train acc: top1 ->  69.5078125 ; top5 ->  88.140625  and loss:  250.34289926290512
test acc: top1 ->  71.222 ; top5 ->  89.89  and loss:  947.0289990305901
forward train acc: top1 ->  71.296875 ; top5 ->  88.5390625  and loss:  240.25326895713806
test acc: top1 ->  71.188 ; top5 ->  89.926  and loss:  944.6810587048531
forward train acc: top1 ->  70.9921875 ; top5 ->  88.640625  and loss:  242.24798148870468
test acc: top1 ->  71.226 ; top5 ->  89.886  and loss:  942.6922746896744
forward train acc: top1 ->  70.8671875 ; top5 ->  88.359375  and loss:  240.45973616838455
test acc: top1 ->  71.318 ; top5 ->  89.946  and loss:  941.4357376098633
forward train acc: top1 ->  70.4921875 ; top5 ->  88.046875  and loss:  247.6837084889412
test acc: top1 ->  71.28 ; top5 ->  89.966  and loss:  938.306519985199
forward train acc: top1 ->  71.421875 ; top5 ->  88.6875  and loss:  240.87818104028702
test acc: top1 ->  71.282 ; top5 ->  89.908  and loss:  941.8849875926971
forward train acc: top1 ->  70.9375 ; top5 ->  88.3046875  and loss:  243.23869371414185
test acc: top1 ->  71.25 ; top5 ->  90.006  and loss:  941.0545940995216
forward train acc: top1 ->  70.578125 ; top5 ->  88.078125  and loss:  242.89050257205963
test acc: top1 ->  71.296 ; top5 ->  89.976  and loss:  941.8001884222031
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  11  ---------------
adv train loss:  -59.33222359418869 , diff:  59.33222359418869
adv train loss:  -60.6358699798584 , diff:  1.3036463856697083
layer  11  adv train finish, try to retain  99
test acc: top1 ->  70.634 ; top5 ->  89.648  and loss:  963.2198640108109
forward train acc: top1 ->  70.453125 ; top5 ->  88.15625  and loss:  244.86788773536682
test acc: top1 ->  71.326 ; top5 ->  90.036  and loss:  941.2280240654945
forward train acc: top1 ->  70.65625 ; top5 ->  87.8984375  and loss:  245.70494842529297
test acc: top1 ->  71.22 ; top5 ->  89.916  and loss:  944.4205669760704
forward train acc: top1 ->  70.515625 ; top5 ->  88.484375  and loss:  243.51624941825867
test acc: top1 ->  71.296 ; top5 ->  90.038  and loss:  940.6141899228096
forward train acc: top1 ->  69.96875 ; top5 ->  88.015625  and loss:  247.59943270683289
test acc: top1 ->  71.26 ; top5 ->  90.016  and loss:  937.155689895153
forward train acc: top1 ->  70.1875 ; top5 ->  88.1015625  and loss:  251.05004453659058
test acc: top1 ->  71.312 ; top5 ->  90.052  and loss:  937.5309754610062
forward train acc: top1 ->  71.125 ; top5 ->  88.46875  and loss:  241.05318123102188
test acc: top1 ->  71.328 ; top5 ->  90.09  and loss:  934.7070407867432
forward train acc: top1 ->  71.0546875 ; top5 ->  88.3984375  and loss:  241.09857481718063
test acc: top1 ->  71.36 ; top5 ->  90.13  and loss:  933.1116392016411
forward train acc: top1 ->  71.046875 ; top5 ->  88.109375  and loss:  244.49289894104004
test acc: top1 ->  71.33 ; top5 ->  90.128  and loss:  934.0710260868073
forward train acc: top1 ->  70.453125 ; top5 ->  88.5078125  and loss:  246.62278974056244
test acc: top1 ->  71.49 ; top5 ->  90.136  and loss:  934.0334345698357
forward train acc: top1 ->  71.140625 ; top5 ->  87.9609375  and loss:  243.71590214967728
test acc: top1 ->  71.34 ; top5 ->  90.118  and loss:  933.5869129300117
>>>>>>> reverse layer  11  since performance drop >>>>>>>
==> this epoch:  100 / 128 , inc:  1
---------------- start layer  12  ---------------
adv train loss:  -59.123361587524414 , diff:  59.123361587524414
adv train loss:  -59.78805238008499 , diff:  0.6646907925605774
layer  12  adv train finish, try to retain  92
test acc: top1 ->  70.6 ; top5 ->  89.444  and loss:  970.1134858131409
forward train acc: top1 ->  69.625 ; top5 ->  87.6484375  and loss:  253.28904843330383
test acc: top1 ->  70.972 ; top5 ->  89.926  and loss:  944.1389154791832
forward train acc: top1 ->  70.328125 ; top5 ->  88.1484375  and loss:  245.168879032135
test acc: top1 ->  71.094 ; top5 ->  89.89  and loss:  947.1454236507416
forward train acc: top1 ->  69.78125 ; top5 ->  87.484375  and loss:  251.91411900520325
test acc: top1 ->  71.052 ; top5 ->  90.01  and loss:  943.1209689974785
forward train acc: top1 ->  70.890625 ; top5 ->  88.5859375  and loss:  242.8018484711647
test acc: top1 ->  71.178 ; top5 ->  90.018  and loss:  941.3832225203514
forward train acc: top1 ->  70.203125 ; top5 ->  87.7109375  and loss:  246.07268619537354
test acc: top1 ->  71.11 ; top5 ->  89.994  and loss:  943.1798936128616
forward train acc: top1 ->  70.8125 ; top5 ->  88.828125  and loss:  240.21908444166183
test acc: top1 ->  71.166 ; top5 ->  90.084  and loss:  940.5518191456795
forward train acc: top1 ->  70.375 ; top5 ->  88.3515625  and loss:  245.61120027303696
test acc: top1 ->  71.188 ; top5 ->  90.004  and loss:  941.8398605585098
forward train acc: top1 ->  69.53125 ; top5 ->  87.546875  and loss:  255.82573240995407
test acc: top1 ->  71.218 ; top5 ->  90.058  and loss:  939.9994195103645
forward train acc: top1 ->  70.7265625 ; top5 ->  88.671875  and loss:  240.97072911262512
test acc: top1 ->  71.272 ; top5 ->  90.1  and loss:  938.6314860582352
forward train acc: top1 ->  70.5234375 ; top5 ->  88.515625  and loss:  243.62853854894638
test acc: top1 ->  71.192 ; top5 ->  90.096  and loss:  938.3186045885086
>>>>>>> reverse layer  12  since performance drop >>>>>>>
==> this epoch:  98 / 128 , inc:  1
---------------- start layer  13  ---------------
adv train loss:  -64.01791626214981 , diff:  64.01791626214981
adv train loss:  -61.03783309459686 , diff:  2.980083167552948
************ all values are small in this layer **********
layer  13  adv train finish, try to retain  70
test acc: top1 ->  66.808 ; top5 ->  87.392  and loss:  1106.5867173671722
forward train acc: top1 ->  69.8984375 ; top5 ->  87.640625  and loss:  252.09148025512695
test acc: top1 ->  70.748 ; top5 ->  89.804  and loss:  957.1882772445679
forward train acc: top1 ->  70.1015625 ; top5 ->  87.8125  and loss:  252.30694258213043
test acc: top1 ->  70.81 ; top5 ->  89.802  and loss:  954.7901547551155
forward train acc: top1 ->  70.484375 ; top5 ->  87.9140625  and loss:  248.81108099222183
test acc: top1 ->  70.748 ; top5 ->  89.74  and loss:  955.5016725063324
forward train acc: top1 ->  70.046875 ; top5 ->  87.90625  and loss:  246.74267786741257
test acc: top1 ->  70.986 ; top5 ->  89.84  and loss:  951.4815181493759
forward train acc: top1 ->  70.1015625 ; top5 ->  88.1484375  and loss:  247.89358347654343
test acc: top1 ->  70.958 ; top5 ->  89.9  and loss:  951.0944856405258
forward train acc: top1 ->  70.765625 ; top5 ->  88.0234375  and loss:  246.3712392449379
test acc: top1 ->  71.068 ; top5 ->  89.924  and loss:  947.715614438057
forward train acc: top1 ->  70.5703125 ; top5 ->  88.0703125  and loss:  245.59730821847916
test acc: top1 ->  71.046 ; top5 ->  89.882  and loss:  946.5941957235336
forward train acc: top1 ->  70.359375 ; top5 ->  88.1875  and loss:  246.50410449504852
test acc: top1 ->  71.046 ; top5 ->  89.926  and loss:  949.8523507118225
forward train acc: top1 ->  70.6328125 ; top5 ->  88.2421875  and loss:  245.4686155319214
test acc: top1 ->  71.058 ; top5 ->  89.948  and loss:  947.3063263893127
forward train acc: top1 ->  71.1953125 ; top5 ->  88.6328125  and loss:  239.29840797185898
test acc: top1 ->  70.974 ; top5 ->  89.922  and loss:  947.2499222159386
>>>>>>> reverse layer  13  since performance drop >>>>>>>
==> this epoch:  71 / 128 , inc:  1
---------------- start layer  14  ---------------
adv train loss:  -60.11857599020004 , diff:  60.11857599020004
adv train loss:  -59.722864747047424 , diff:  0.3957112431526184
layer  14  adv train finish, try to retain  249
test acc: top1 ->  70.862 ; top5 ->  89.878  and loss:  950.9248580336571
forward train acc: top1 ->  71.03125 ; top5 ->  88.109375  and loss:  245.52013611793518
test acc: top1 ->  71.166 ; top5 ->  90.078  and loss:  940.4398250579834
forward train acc: top1 ->  70.796875 ; top5 ->  88.53125  and loss:  242.96002793312073
test acc: top1 ->  70.984 ; top5 ->  90.06  and loss:  943.1813584566116
forward train acc: top1 ->  70.9140625 ; top5 ->  88.3203125  and loss:  242.3136437535286
test acc: top1 ->  71.068 ; top5 ->  89.99  and loss:  941.6738187670708
forward train acc: top1 ->  70.390625 ; top5 ->  87.953125  and loss:  245.69533222913742
test acc: top1 ->  71.208 ; top5 ->  90.074  and loss:  940.1420211791992
forward train acc: top1 ->  70.234375 ; top5 ->  88.2265625  and loss:  245.66277474164963
test acc: top1 ->  71.15 ; top5 ->  90.032  and loss:  940.3914837837219
forward train acc: top1 ->  71.2890625 ; top5 ->  89.15625  and loss:  234.18152564764023
test acc: top1 ->  71.218 ; top5 ->  90.036  and loss:  939.9816598296165
forward train acc: top1 ->  71.0859375 ; top5 ->  89.0703125  and loss:  237.6832094192505
test acc: top1 ->  71.33 ; top5 ->  90.098  and loss:  935.9020899534225
forward train acc: top1 ->  70.5546875 ; top5 ->  88.078125  and loss:  243.708287358284
test acc: top1 ->  71.318 ; top5 ->  90.116  and loss:  936.8194454312325
forward train acc: top1 ->  71.1328125 ; top5 ->  88.34375  and loss:  243.04149341583252
test acc: top1 ->  71.338 ; top5 ->  90.086  and loss:  937.5974612832069
forward train acc: top1 ->  70.5234375 ; top5 ->  88.234375  and loss:  243.65381056070328
test acc: top1 ->  71.338 ; top5 ->  90.164  and loss:  934.2487096190453
>>>>>>> reverse layer  14  since performance drop >>>>>>>
==> this epoch:  250 / 256 , inc:  1
---------------- start layer  15  ---------------
adv train loss:  -62.041720151901245 , diff:  62.041720151901245
adv train loss:  -59.322119653224945 , diff:  2.7196004986763
************ all values are small in this layer **********
layer  15  adv train finish, try to retain  247
test acc: top1 ->  71.012 ; top5 ->  89.944  and loss:  950.3262870311737
forward train acc: top1 ->  70.7734375 ; top5 ->  88.0859375  and loss:  244.96871429681778
test acc: top1 ->  71.09 ; top5 ->  89.97  and loss:  943.1624248027802
forward train acc: top1 ->  70.875 ; top5 ->  88.3984375  and loss:  245.82862466573715
test acc: top1 ->  71.162 ; top5 ->  90.012  and loss:  939.4697898626328
forward train acc: top1 ->  70.8125 ; top5 ->  88.7265625  and loss:  241.14456516504288
test acc: top1 ->  71.186 ; top5 ->  89.94  and loss:  944.0694353580475
forward train acc: top1 ->  70.1875 ; top5 ->  87.46875  and loss:  251.86758494377136
test acc: top1 ->  71.266 ; top5 ->  90.078  and loss:  940.1813035011292
forward train acc: top1 ->  71.4140625 ; top5 ->  88.9375  and loss:  238.11561399698257
test acc: top1 ->  71.202 ; top5 ->  90.018  and loss:  941.4332439303398
forward train acc: top1 ->  70.8671875 ; top5 ->  88.1171875  and loss:  242.65918123722076
test acc: top1 ->  71.166 ; top5 ->  90.058  and loss:  940.9293168187141
forward train acc: top1 ->  70.671875 ; top5 ->  87.9140625  and loss:  246.3201498389244
test acc: top1 ->  71.212 ; top5 ->  90.07  and loss:  942.4737655520439
forward train acc: top1 ->  70.7890625 ; top5 ->  88.546875  and loss:  243.08213967084885
test acc: top1 ->  71.318 ; top5 ->  90.024  and loss:  936.9417049884796
forward train acc: top1 ->  70.828125 ; top5 ->  88.2890625  and loss:  242.0391587615013
test acc: top1 ->  71.254 ; top5 ->  90.054  and loss:  936.2397089600563
forward train acc: top1 ->  70.5390625 ; top5 ->  88.0625  and loss:  242.38144892454147
test acc: top1 ->  71.318 ; top5 ->  90.06  and loss:  937.9297127723694
>>>>>>> reverse layer  15  since performance drop >>>>>>>
==> this epoch:  248 / 256 , inc:  1
---------------- start layer  16  ---------------
adv train loss:  -61.18007153272629 , diff:  61.18007153272629
adv train loss:  -58.44584912061691 , diff:  2.734222412109375
************ all values are small in this layer **********
layer  16  adv train finish, try to retain  238
test acc: top1 ->  71.096 ; top5 ->  90.024  and loss:  942.3447068929672
forward train acc: top1 ->  70.75 ; top5 ->  88.2265625  and loss:  243.15984892845154
test acc: top1 ->  71.152 ; top5 ->  90.028  and loss:  944.3025124669075
forward train acc: top1 ->  70.7421875 ; top5 ->  88.15625  and loss:  244.5686028599739
test acc: top1 ->  71.128 ; top5 ->  89.964  and loss:  940.7399643063545
forward train acc: top1 ->  70.140625 ; top5 ->  88.0  and loss:  245.3354538679123
test acc: top1 ->  71.282 ; top5 ->  90.014  and loss:  941.2954067587852
forward train acc: top1 ->  70.5625 ; top5 ->  87.8828125  and loss:  246.7327160835266
test acc: top1 ->  71.328 ; top5 ->  90.122  and loss:  938.297464132309
forward train acc: top1 ->  70.9453125 ; top5 ->  88.3984375  and loss:  243.2966850399971
test acc: top1 ->  71.322 ; top5 ->  90.098  and loss:  937.1768337488174
forward train acc: top1 ->  71.25 ; top5 ->  88.3515625  and loss:  241.5954648256302
test acc: top1 ->  71.356 ; top5 ->  90.12  and loss:  937.5838779211044
forward train acc: top1 ->  71.4296875 ; top5 ->  88.6484375  and loss:  239.97110575437546
test acc: top1 ->  71.422 ; top5 ->  90.096  and loss:  935.0846915841103
forward train acc: top1 ->  71.4609375 ; top5 ->  88.5546875  and loss:  238.89056473970413
test acc: top1 ->  71.456 ; top5 ->  90.136  and loss:  934.554220855236
forward train acc: top1 ->  70.5703125 ; top5 ->  88.578125  and loss:  243.6091292500496
test acc: top1 ->  71.444 ; top5 ->  90.04  and loss:  934.7579671144485
forward train acc: top1 ->  70.96875 ; top5 ->  88.25  and loss:  242.5206001996994
test acc: top1 ->  71.382 ; top5 ->  90.05  and loss:  936.0548763871193
>>>>>>> reverse layer  16  since performance drop >>>>>>>
==> this epoch:  239 / 256 , inc:  1
---------------- start layer  17  ---------------
adv train loss:  -59.162763595581055 , diff:  59.162763595581055
adv train loss:  -56.780321061611176 , diff:  2.382442533969879
layer  17  adv train finish, try to retain  224
test acc: top1 ->  70.932 ; top5 ->  89.842  and loss:  951.8629842400551
forward train acc: top1 ->  70.65625 ; top5 ->  88.3515625  and loss:  244.9595690369606
test acc: top1 ->  71.024 ; top5 ->  89.934  and loss:  945.7267104387283
forward train acc: top1 ->  70.5390625 ; top5 ->  87.890625  and loss:  247.09153020381927
test acc: top1 ->  71.204 ; top5 ->  90.032  and loss:  944.3569893240929
forward train acc: top1 ->  70.40625 ; top5 ->  88.15625  and loss:  247.06146776676178
test acc: top1 ->  71.272 ; top5 ->  90.042  and loss:  940.5356314182281
forward train acc: top1 ->  70.8671875 ; top5 ->  88.0390625  and loss:  245.6587432026863
test acc: top1 ->  71.12 ; top5 ->  89.99  and loss:  943.8783013224602
forward train acc: top1 ->  70.7421875 ; top5 ->  87.8359375  and loss:  249.45761489868164
test acc: top1 ->  71.19 ; top5 ->  89.996  and loss:  942.4161342382431
forward train acc: top1 ->  70.3828125 ; top5 ->  88.3046875  and loss:  243.74663084745407
test acc: top1 ->  71.318 ; top5 ->  90.096  and loss:  936.5197172164917
forward train acc: top1 ->  70.921875 ; top5 ->  88.1796875  and loss:  243.9270172715187
test acc: top1 ->  71.23 ; top5 ->  90.068  and loss:  937.3620437383652
forward train acc: top1 ->  70.7890625 ; top5 ->  88.3828125  and loss:  243.50554591417313
test acc: top1 ->  71.308 ; top5 ->  90.142  and loss:  937.9885984063148
forward train acc: top1 ->  70.953125 ; top5 ->  88.9609375  and loss:  239.602363884449
test acc: top1 ->  71.318 ; top5 ->  90.126  and loss:  937.0123875141144
forward train acc: top1 ->  70.921875 ; top5 ->  88.0859375  and loss:  244.8800328373909
test acc: top1 ->  71.36 ; top5 ->  90.142  and loss:  936.3593806028366
>>>>>>> reverse layer  17  since performance drop >>>>>>>
==> this epoch:  230 / 256 , inc:  1
---------------- start layer  18  ---------------
adv train loss:  -60.7061505317688 , diff:  60.7061505317688
adv train loss:  -60.71251207590103 , diff:  0.006361544132232666
************ all values are small in this layer **********
layer  18  adv train finish, try to retain  234
test acc: top1 ->  71.2 ; top5 ->  89.964  and loss:  938.1933934092522
forward train acc: top1 ->  71.2734375 ; top5 ->  88.34375  and loss:  242.55334895849228
test acc: top1 ->  71.184 ; top5 ->  90.006  and loss:  939.644547522068
forward train acc: top1 ->  70.5 ; top5 ->  87.7890625  and loss:  248.29426234960556
test acc: top1 ->  71.21 ; top5 ->  90.01  and loss:  941.0960220098495
forward train acc: top1 ->  70.8515625 ; top5 ->  88.5546875  and loss:  240.2911021709442
test acc: top1 ->  71.152 ; top5 ->  90.046  and loss:  939.2640092372894
forward train acc: top1 ->  70.6796875 ; top5 ->  87.6328125  and loss:  248.57333827018738
test acc: top1 ->  71.334 ; top5 ->  90.08  and loss:  941.049349963665
forward train acc: top1 ->  71.0390625 ; top5 ->  88.390625  and loss:  240.7399686574936
test acc: top1 ->  71.312 ; top5 ->  90.068  and loss:  938.4005055427551
forward train acc: top1 ->  71.3046875 ; top5 ->  88.6484375  and loss:  239.40696096420288
test acc: top1 ->  71.33 ; top5 ->  90.106  and loss:  936.5650355219841
forward train acc: top1 ->  71.171875 ; top5 ->  88.6015625  and loss:  238.1254858970642
test acc: top1 ->  71.304 ; top5 ->  90.104  and loss:  935.4227020740509
forward train acc: top1 ->  70.9609375 ; top5 ->  88.5390625  and loss:  241.4992486834526
test acc: top1 ->  71.43 ; top5 ->  90.116  and loss:  936.4855009913445
forward train acc: top1 ->  71.25 ; top5 ->  88.2578125  and loss:  242.0309744477272
test acc: top1 ->  71.386 ; top5 ->  90.138  and loss:  933.0967144370079
forward train acc: top1 ->  70.9609375 ; top5 ->  88.5703125  and loss:  243.15201258659363
test acc: top1 ->  71.504 ; top5 ->  90.178  and loss:  934.0216978788376
>>>>>>> reverse layer  18  since performance drop >>>>>>>
==> this epoch:  235 / 256 , inc:  1
---------------- start layer  19  ---------------
adv train loss:  -61.329210460186005 , diff:  61.329210460186005
adv train loss:  -60.51887774467468 , diff:  0.810332715511322
************ all values are small in this layer **********
layer  19  adv train finish, try to retain  178
test acc: top1 ->  70.796 ; top5 ->  89.79  and loss:  954.3811833262444
forward train acc: top1 ->  70.2265625 ; top5 ->  88.0859375  and loss:  246.8264076113701
test acc: top1 ->  71.014 ; top5 ->  89.988  and loss:  949.140402674675
forward train acc: top1 ->  70.46875 ; top5 ->  88.234375  and loss:  243.79540646076202
test acc: top1 ->  71.114 ; top5 ->  89.922  and loss:  943.8059871196747
forward train acc: top1 ->  71.15625 ; top5 ->  88.5703125  and loss:  240.6739856004715
test acc: top1 ->  71.192 ; top5 ->  89.884  and loss:  949.2873333096504
forward train acc: top1 ->  70.1875 ; top5 ->  87.828125  and loss:  247.92828673124313
test acc: top1 ->  71.094 ; top5 ->  89.992  and loss:  945.3347210288048
forward train acc: top1 ->  70.828125 ; top5 ->  88.3671875  and loss:  243.36356914043427
test acc: top1 ->  71.268 ; top5 ->  89.984  and loss:  940.4019206166267
forward train acc: top1 ->  70.4140625 ; top5 ->  87.78125  and loss:  249.14941835403442
test acc: top1 ->  71.078 ; top5 ->  89.918  and loss:  941.0513653159142
forward train acc: top1 ->  70.1796875 ; top5 ->  88.171875  and loss:  246.82876586914062
test acc: top1 ->  71.122 ; top5 ->  89.954  and loss:  941.8206700086594
forward train acc: top1 ->  69.9140625 ; top5 ->  87.9453125  and loss:  248.89603227376938
test acc: top1 ->  71.174 ; top5 ->  89.996  and loss:  941.3427363038063
forward train acc: top1 ->  70.109375 ; top5 ->  87.8125  and loss:  248.28837072849274
test acc: top1 ->  71.23 ; top5 ->  90.026  and loss:  938.3152269721031
forward train acc: top1 ->  70.5 ; top5 ->  87.984375  and loss:  246.7326807975769
test acc: top1 ->  71.196 ; top5 ->  89.992  and loss:  941.8645993471146
>>>>>>> reverse layer  19  since performance drop >>>>>>>
==> this epoch:  179 / 256 , inc:  1
---------------- start layer  20  ---------------
adv train loss:  -60.80994749069214 , diff:  60.80994749069214
adv train loss:  -60.41268342733383 , diff:  0.3972640633583069
layer  20  adv train finish, try to retain  221
test acc: top1 ->  71.224 ; top5 ->  89.97  and loss:  940.6964480280876
forward train acc: top1 ->  70.7109375 ; top5 ->  88.1875  and loss:  242.9807312488556
test acc: top1 ->  71.222 ; top5 ->  90.0  and loss:  941.1194458007812
forward train acc: top1 ->  70.265625 ; top5 ->  87.7421875  and loss:  247.0836060643196
test acc: top1 ->  71.104 ; top5 ->  89.99  and loss:  943.9070062637329
forward train acc: top1 ->  70.6953125 ; top5 ->  88.1953125  and loss:  244.00637739896774
test acc: top1 ->  71.022 ; top5 ->  90.008  and loss:  943.1063373088837
forward train acc: top1 ->  70.671875 ; top5 ->  87.9921875  and loss:  244.31626665592194
test acc: top1 ->  71.184 ; top5 ->  90.03  and loss:  944.113695204258
forward train acc: top1 ->  70.8671875 ; top5 ->  88.2578125  and loss:  242.8760393857956
test acc: top1 ->  71.176 ; top5 ->  90.046  and loss:  941.891538798809
forward train acc: top1 ->  70.9296875 ; top5 ->  88.3359375  and loss:  243.33474135398865
test acc: top1 ->  71.268 ; top5 ->  90.122  and loss:  937.6355087161064
forward train acc: top1 ->  70.8984375 ; top5 ->  88.3984375  and loss:  243.85297346115112
test acc: top1 ->  71.246 ; top5 ->  90.132  and loss:  936.4921557307243
forward train acc: top1 ->  70.9140625 ; top5 ->  88.4375  and loss:  242.36871004104614
test acc: top1 ->  71.26 ; top5 ->  90.066  and loss:  939.6475450992584
forward train acc: top1 ->  71.203125 ; top5 ->  88.3359375  and loss:  240.8558269739151
test acc: top1 ->  71.366 ; top5 ->  90.118  and loss:  938.5788959264755
forward train acc: top1 ->  71.0625 ; top5 ->  88.578125  and loss:  240.36135011911392
test acc: top1 ->  71.384 ; top5 ->  90.128  and loss:  933.796733379364
>>>>>>> reverse layer  20  since performance drop >>>>>>>
==> this epoch:  225 / 256 , inc:  1
---------------- start layer  21  ---------------
adv train loss:  -58.319301545619965 , diff:  58.319301545619965
adv train loss:  -58.145968556404114 , diff:  0.17333298921585083
************ all values are small in this layer **********
layer  21  adv train finish, try to retain  241
test acc: top1 ->  71.32 ; top5 ->  90.002  and loss:  939.4207780957222
forward train acc: top1 ->  70.9296875 ; top5 ->  88.0234375  and loss:  246.11495804786682
test acc: top1 ->  71.226 ; top5 ->  90.034  and loss:  941.0719683170319
forward train acc: top1 ->  71.875 ; top5 ->  88.546875  and loss:  238.00191694498062
test acc: top1 ->  71.156 ; top5 ->  90.104  and loss:  943.3846065402031
forward train acc: top1 ->  71.109375 ; top5 ->  87.9765625  and loss:  243.53086018562317
test acc: top1 ->  71.086 ; top5 ->  89.982  and loss:  943.047850549221
forward train acc: top1 ->  70.515625 ; top5 ->  88.3359375  and loss:  244.48100459575653
test acc: top1 ->  71.186 ; top5 ->  90.084  and loss:  939.9118918180466
forward train acc: top1 ->  71.46875 ; top5 ->  88.3359375  and loss:  240.8875417113304
test acc: top1 ->  71.286 ; top5 ->  90.128  and loss:  936.5888385772705
forward train acc: top1 ->  71.5 ; top5 ->  88.7734375  and loss:  235.17654103040695
test acc: top1 ->  71.346 ; top5 ->  90.116  and loss:  935.1233779191971
forward train acc: top1 ->  70.7578125 ; top5 ->  88.1484375  and loss:  243.97246170043945
test acc: top1 ->  71.292 ; top5 ->  90.108  and loss:  937.9864947795868
forward train acc: top1 ->  71.078125 ; top5 ->  88.3046875  and loss:  242.0519706606865
test acc: top1 ->  71.41 ; top5 ->  90.084  and loss:  936.2842205762863
forward train acc: top1 ->  70.734375 ; top5 ->  88.3203125  and loss:  243.0179427266121
test acc: top1 ->  71.386 ; top5 ->  90.094  and loss:  935.0825080871582
forward train acc: top1 ->  70.3515625 ; top5 ->  88.1953125  and loss:  247.41131579875946
test acc: top1 ->  71.426 ; top5 ->  90.114  and loss:  933.4587360024452
>>>>>>> reverse layer  21  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  22  ---------------
adv train loss:  -59.789800345897675 , diff:  59.789800345897675
adv train loss:  -59.55674695968628 , diff:  0.23305338621139526
layer  22  adv train finish, try to retain  186
test acc: top1 ->  71.004 ; top5 ->  89.824  and loss:  957.6133313775063
forward train acc: top1 ->  69.75 ; top5 ->  87.5546875  and loss:  250.6875422000885
test acc: top1 ->  70.976 ; top5 ->  89.868  and loss:  949.1167593002319
forward train acc: top1 ->  70.34375 ; top5 ->  88.09375  and loss:  250.58982235193253
test acc: top1 ->  70.982 ; top5 ->  89.896  and loss:  947.694563806057
forward train acc: top1 ->  69.8515625 ; top5 ->  87.9609375  and loss:  250.14824259281158
test acc: top1 ->  70.972 ; top5 ->  89.816  and loss:  950.3857130408287
forward train acc: top1 ->  70.28125 ; top5 ->  87.796875  and loss:  247.69706654548645
test acc: top1 ->  71.094 ; top5 ->  89.922  and loss:  949.0762079358101
forward train acc: top1 ->  70.9296875 ; top5 ->  88.390625  and loss:  240.61173403263092
test acc: top1 ->  71.106 ; top5 ->  90.032  and loss:  947.1496337652206
forward train acc: top1 ->  70.7890625 ; top5 ->  88.3515625  and loss:  242.6495566368103
test acc: top1 ->  71.046 ; top5 ->  89.95  and loss:  943.6560196876526
forward train acc: top1 ->  70.125 ; top5 ->  87.9296875  and loss:  249.73336851596832
test acc: top1 ->  71.158 ; top5 ->  89.92  and loss:  944.1249044537544
forward train acc: top1 ->  71.5 ; top5 ->  89.015625  and loss:  233.27977979183197
test acc: top1 ->  71.15 ; top5 ->  89.978  and loss:  946.1403671503067
forward train acc: top1 ->  70.6953125 ; top5 ->  88.6484375  and loss:  240.19667613506317
test acc: top1 ->  71.24 ; top5 ->  90.094  and loss:  942.640965461731
forward train acc: top1 ->  70.96875 ; top5 ->  87.78125  and loss:  246.59769129753113
test acc: top1 ->  71.236 ; top5 ->  90.066  and loss:  941.7087231874466
>>>>>>> reverse layer  22  since performance drop >>>>>>>
==> this epoch:  208 / 256 , inc:  1
---------------- start layer  23  ---------------
adv train loss:  -58.75706958770752 , diff:  58.75706958770752
adv train loss:  -60.9932923913002 , diff:  2.236222803592682
layer  23  adv train finish, try to retain  197
>>>>>>> reverse layer  23  since no improvement >>>>>>>
---------------- start layer  24  ---------------
adv train loss:  -61.54136663675308 , diff:  61.54136663675308
adv train loss:  -58.85324317216873 , diff:  2.6881234645843506
************ all values are small in this layer **********
layer  24  adv train finish, try to retain  243
test acc: top1 ->  71.264 ; top5 ->  90.154  and loss:  939.828738451004
forward train acc: top1 ->  71.5078125 ; top5 ->  88.75  and loss:  236.5617344379425
test acc: top1 ->  71.19 ; top5 ->  90.138  and loss:  941.7317745089531
forward train acc: top1 ->  70.6953125 ; top5 ->  88.078125  and loss:  245.0479575395584
test acc: top1 ->  71.284 ; top5 ->  90.05  and loss:  942.7359888553619
forward train acc: top1 ->  71.6484375 ; top5 ->  88.109375  and loss:  243.37560731172562
test acc: top1 ->  71.274 ; top5 ->  90.15  and loss:  938.8502777814865
forward train acc: top1 ->  70.8671875 ; top5 ->  88.09375  and loss:  243.7086679339409
test acc: top1 ->  71.366 ; top5 ->  90.162  and loss:  940.2064723372459
forward train acc: top1 ->  71.1484375 ; top5 ->  88.453125  and loss:  238.48158770799637
test acc: top1 ->  71.336 ; top5 ->  90.224  and loss:  936.3049083948135
forward train acc: top1 ->  71.5390625 ; top5 ->  89.1796875  and loss:  235.74105274677277
test acc: top1 ->  71.35 ; top5 ->  90.202  and loss:  936.9889693260193
forward train acc: top1 ->  71.28125 ; top5 ->  88.3984375  and loss:  241.38714635372162
test acc: top1 ->  71.424 ; top5 ->  90.174  and loss:  935.507722735405
forward train acc: top1 ->  70.84375 ; top5 ->  88.515625  and loss:  243.98707884550095
test acc: top1 ->  71.502 ; top5 ->  90.214  and loss:  937.2067351341248
forward train acc: top1 ->  71.1484375 ; top5 ->  88.3046875  and loss:  242.882273375988
test acc: top1 ->  71.454 ; top5 ->  90.11  and loss:  936.2148171067238
forward train acc: top1 ->  70.8671875 ; top5 ->  88.8984375  and loss:  239.28747779130936
test acc: top1 ->  71.456 ; top5 ->  90.18  and loss:  934.044677734375
>>>>>>> reverse layer  24  since performance drop >>>>>>>
==> this epoch:  244 / 256 , inc:  1
---------------- start layer  25  ---------------
adv train loss:  -61.71829068660736 , diff:  61.71829068660736
adv train loss:  -60.37278234958649 , diff:  1.345508337020874
************ all values are small in this layer **********
layer  25  adv train finish, try to retain  241
test acc: top1 ->  71.044 ; top5 ->  89.902  and loss:  953.0161696076393
forward train acc: top1 ->  71.3203125 ; top5 ->  88.5078125  and loss:  242.68843746185303
test acc: top1 ->  71.23 ; top5 ->  89.96  and loss:  940.71095007658
forward train acc: top1 ->  70.90625 ; top5 ->  88.3125  and loss:  241.06395119428635
test acc: top1 ->  71.28 ; top5 ->  89.964  and loss:  942.6164474487305
forward train acc: top1 ->  70.9453125 ; top5 ->  88.3828125  and loss:  240.6404172182083
test acc: top1 ->  71.144 ; top5 ->  89.978  and loss:  941.0739673376083
forward train acc: top1 ->  71.2734375 ; top5 ->  88.21875  and loss:  242.33453822135925
test acc: top1 ->  71.188 ; top5 ->  90.036  and loss:  941.2371480464935
forward train acc: top1 ->  71.4296875 ; top5 ->  88.4609375  and loss:  240.0934692621231
test acc: top1 ->  71.346 ; top5 ->  90.102  and loss:  937.4021245837212
forward train acc: top1 ->  70.7734375 ; top5 ->  88.4921875  and loss:  243.2353948354721
test acc: top1 ->  71.326 ; top5 ->  90.092  and loss:  937.6324272751808
forward train acc: top1 ->  70.9296875 ; top5 ->  88.09375  and loss:  246.76687771081924
test acc: top1 ->  71.472 ; top5 ->  90.064  and loss:  934.3725784420967
forward train acc: top1 ->  71.1796875 ; top5 ->  87.6953125  and loss:  243.20767760276794
test acc: top1 ->  71.388 ; top5 ->  90.05  and loss:  935.916023850441
forward train acc: top1 ->  70.75 ; top5 ->  88.4453125  and loss:  242.10961002111435
test acc: top1 ->  71.428 ; top5 ->  90.144  and loss:  933.7009630799294
forward train acc: top1 ->  70.59375 ; top5 ->  88.3125  and loss:  243.30060678720474
test acc: top1 ->  71.412 ; top5 ->  90.114  and loss:  933.4283335804939
>>>>>>> reverse layer  25  since performance drop >>>>>>>
==> this epoch:  242 / 256 , inc:  1
---------------- start layer  26  ---------------
adv train loss:  -60.467566192150116 , diff:  60.467566192150116
adv train loss:  -60.44709873199463 , diff:  0.02046746015548706
layer  26  adv train finish, try to retain  492
test acc: top1 ->  71.088 ; top5 ->  89.868  and loss:  946.5376719236374
forward train acc: top1 ->  70.25 ; top5 ->  87.484375  and loss:  249.57856810092926
test acc: top1 ->  71.03 ; top5 ->  90.01  and loss:  947.6957908272743
forward train acc: top1 ->  70.875 ; top5 ->  88.078125  and loss:  241.13269525766373
test acc: top1 ->  71.062 ; top5 ->  90.032  and loss:  942.7573425769806
forward train acc: top1 ->  70.0625 ; top5 ->  88.0078125  and loss:  244.87997990846634
test acc: top1 ->  71.09 ; top5 ->  90.014  and loss:  942.8279289007187
forward train acc: top1 ->  70.1328125 ; top5 ->  87.578125  and loss:  251.9438931941986
test acc: top1 ->  71.194 ; top5 ->  90.058  and loss:  941.5158663988113
forward train acc: top1 ->  70.75 ; top5 ->  87.8671875  and loss:  245.77078759670258
test acc: top1 ->  71.114 ; top5 ->  89.976  and loss:  942.559828042984
forward train acc: top1 ->  70.9609375 ; top5 ->  87.9921875  and loss:  242.6029573082924
test acc: top1 ->  71.324 ; top5 ->  90.012  and loss:  939.8269990682602
forward train acc: top1 ->  70.515625 ; top5 ->  88.28125  and loss:  241.84561544656754
test acc: top1 ->  71.276 ; top5 ->  90.052  and loss:  940.7489194869995
forward train acc: top1 ->  71.265625 ; top5 ->  87.75  and loss:  244.27560502290726
test acc: top1 ->  71.238 ; top5 ->  90.064  and loss:  941.1256048083305
forward train acc: top1 ->  71.4296875 ; top5 ->  88.2265625  and loss:  239.6528360247612
test acc: top1 ->  71.36 ; top5 ->  90.09  and loss:  936.9239956140518
forward train acc: top1 ->  70.8828125 ; top5 ->  88.359375  and loss:  245.61580085754395
test acc: top1 ->  71.34 ; top5 ->  90.062  and loss:  936.0530652999878
>>>>>>> reverse layer  26  since performance drop >>>>>>>
==> this epoch:  502 / 512 , inc:  1
---------------- start layer  27  ---------------
adv train loss:  -57.75780612230301 , diff:  57.75780612230301
adv train loss:  -63.507548093795776 , diff:  5.749741971492767
************ all values are small in this layer **********
layer  27  adv train finish, try to retain  504
test acc: top1 ->  71.308 ; top5 ->  90.076  and loss:  936.4267567396164
forward train acc: top1 ->  71.0859375 ; top5 ->  88.7890625  and loss:  240.00279796123505
test acc: top1 ->  71.27 ; top5 ->  90.062  and loss:  940.2254330515862
forward train acc: top1 ->  70.9140625 ; top5 ->  88.5703125  and loss:  241.00374853610992
test acc: top1 ->  71.286 ; top5 ->  90.108  and loss:  939.3048908114433
forward train acc: top1 ->  71.3046875 ; top5 ->  88.3046875  and loss:  239.72224962711334
test acc: top1 ->  71.186 ; top5 ->  90.096  and loss:  939.2667151689529
forward train acc: top1 ->  70.7265625 ; top5 ->  88.4921875  and loss:  240.01223427057266
test acc: top1 ->  71.38 ; top5 ->  90.112  and loss:  935.0209299325943
forward train acc: top1 ->  71.171875 ; top5 ->  88.171875  and loss:  242.78311479091644
test acc: top1 ->  71.368 ; top5 ->  90.12  and loss:  940.1973506808281
forward train acc: top1 ->  71.2109375 ; top5 ->  88.3515625  and loss:  242.48722517490387
test acc: top1 ->  71.406 ; top5 ->  90.178  and loss:  933.580216050148
forward train acc: top1 ->  71.6015625 ; top5 ->  88.4921875  and loss:  238.33483982086182
test acc: top1 ->  71.412 ; top5 ->  90.148  and loss:  934.6808122992516
forward train acc: top1 ->  71.59375 ; top5 ->  88.5  and loss:  239.11862337589264
test acc: top1 ->  71.396 ; top5 ->  90.196  and loss:  933.601110458374
forward train acc: top1 ->  70.4140625 ; top5 ->  88.0390625  and loss:  244.88516873121262
test acc: top1 ->  71.45 ; top5 ->  90.1  and loss:  933.951468527317
forward train acc: top1 ->  70.6484375 ; top5 ->  88.5546875  and loss:  243.84554541110992
test acc: top1 ->  71.434 ; top5 ->  90.2  and loss:  934.5062571167946
>>>>>>> reverse layer  27  since performance drop >>>>>>>
==> this epoch:  505 / 512 , inc:  1
---------------- start layer  28  ---------------
adv train loss:  -61.41786187887192 , diff:  61.41786187887192
adv train loss:  -61.81742870807648 , diff:  0.3995668292045593
************ all values are small in this layer **********
layer  28  adv train finish, try to retain  502
test acc: top1 ->  71.312 ; top5 ->  90.124  and loss:  933.1572621464729
forward train acc: top1 ->  71.1796875 ; top5 ->  88.296875  and loss:  241.53213649988174
test acc: top1 ->  71.308 ; top5 ->  90.15  and loss:  937.8868547677994
forward train acc: top1 ->  71.1484375 ; top5 ->  88.203125  and loss:  240.94967132806778
test acc: top1 ->  71.302 ; top5 ->  90.104  and loss:  941.1091460585594
forward train acc: top1 ->  70.8671875 ; top5 ->  88.2734375  and loss:  246.93058288097382
test acc: top1 ->  71.126 ; top5 ->  90.102  and loss:  941.6907429099083
forward train acc: top1 ->  70.5546875 ; top5 ->  88.359375  and loss:  242.3383793234825
test acc: top1 ->  71.124 ; top5 ->  90.158  and loss:  937.4089117050171
forward train acc: top1 ->  71.3046875 ; top5 ->  88.578125  and loss:  238.214848279953
test acc: top1 ->  71.318 ; top5 ->  90.172  and loss:  934.8524987101555
forward train acc: top1 ->  72.0234375 ; top5 ->  88.609375  and loss:  235.53387373685837
test acc: top1 ->  71.366 ; top5 ->  90.162  and loss:  935.6910753250122
forward train acc: top1 ->  71.0078125 ; top5 ->  88.34375  and loss:  240.16403567790985
test acc: top1 ->  71.29 ; top5 ->  90.18  and loss:  935.5182577967644
forward train acc: top1 ->  71.125 ; top5 ->  88.359375  and loss:  240.5521474480629
test acc: top1 ->  71.304 ; top5 ->  90.188  and loss:  935.7653737068176
forward train acc: top1 ->  70.875 ; top5 ->  88.0859375  and loss:  243.68851047754288
test acc: top1 ->  71.33 ; top5 ->  90.208  and loss:  935.7953396439552
forward train acc: top1 ->  71.046875 ; top5 ->  88.5625  and loss:  240.22720366716385
test acc: top1 ->  71.402 ; top5 ->  90.184  and loss:  935.8589438199997
>>>>>>> reverse layer  28  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  29  ---------------
adv train loss:  -58.41247218847275 , diff:  58.41247218847275
adv train loss:  -59.02744108438492 , diff:  0.6149688959121704
layer  29  adv train finish, try to retain  487
test acc: top1 ->  71.236 ; top5 ->  90.112  and loss:  938.5430266261101
forward train acc: top1 ->  71.328125 ; top5 ->  88.828125  and loss:  237.38433122634888
test acc: top1 ->  71.178 ; top5 ->  90.07  and loss:  941.362141430378
forward train acc: top1 ->  70.9921875 ; top5 ->  88.3125  and loss:  244.35079711675644
test acc: top1 ->  71.21 ; top5 ->  90.076  and loss:  938.6774094700813
forward train acc: top1 ->  70.53125 ; top5 ->  88.171875  and loss:  243.9245138168335
test acc: top1 ->  71.14 ; top5 ->  90.048  and loss:  941.7510871291161
forward train acc: top1 ->  71.28125 ; top5 ->  88.6796875  and loss:  238.733972966671
test acc: top1 ->  71.048 ; top5 ->  90.06  and loss:  941.4432757496834
forward train acc: top1 ->  70.640625 ; top5 ->  87.78125  and loss:  249.36433923244476
test acc: top1 ->  71.228 ; top5 ->  90.092  and loss:  938.0052542090416
forward train acc: top1 ->  70.5546875 ; top5 ->  87.8828125  and loss:  246.67926967144012
test acc: top1 ->  71.278 ; top5 ->  90.092  and loss:  937.0944740176201
forward train acc: top1 ->  70.8828125 ; top5 ->  88.5  and loss:  242.49561953544617
test acc: top1 ->  71.354 ; top5 ->  90.146  and loss:  935.2730314135551
forward train acc: top1 ->  70.609375 ; top5 ->  88.484375  and loss:  241.82827931642532
test acc: top1 ->  71.408 ; top5 ->  90.104  and loss:  933.1481192111969
forward train acc: top1 ->  71.2109375 ; top5 ->  88.078125  and loss:  242.52020227909088
test acc: top1 ->  71.34 ; top5 ->  90.192  and loss:  932.4898327589035
forward train acc: top1 ->  71.328125 ; top5 ->  88.8125  and loss:  237.25100243091583
test acc: top1 ->  71.32 ; top5 ->  90.162  and loss:  930.1732270121574
>>>>>>> reverse layer  29  since performance drop >>>>>>>
==> this epoch:  498 / 512 , inc:  1
---------------- start layer  30  ---------------
adv train loss:  -61.63069933652878 , diff:  61.63069933652878
adv train loss:  -60.76712262630463 , diff:  0.8635767102241516
layer  30  adv train finish, try to retain  485
test acc: top1 ->  71.074 ; top5 ->  89.97  and loss:  949.8421568870544
forward train acc: top1 ->  71.3984375 ; top5 ->  88.3515625  and loss:  240.82651126384735
test acc: top1 ->  70.986 ; top5 ->  89.982  and loss:  948.4695165157318
forward train acc: top1 ->  70.1015625 ; top5 ->  87.9765625  and loss:  249.6909658908844
test acc: top1 ->  71.056 ; top5 ->  89.938  and loss:  948.0618329644203
forward train acc: top1 ->  70.09375 ; top5 ->  87.7734375  and loss:  249.64258700609207
test acc: top1 ->  71.002 ; top5 ->  89.976  and loss:  949.9866164922714
forward train acc: top1 ->  70.46875 ; top5 ->  87.578125  and loss:  250.2609292268753
test acc: top1 ->  71.092 ; top5 ->  89.972  and loss:  946.324550151825
forward train acc: top1 ->  70.984375 ; top5 ->  88.609375  and loss:  243.1563915014267
test acc: top1 ->  71.14 ; top5 ->  90.05  and loss:  941.3114873766899
forward train acc: top1 ->  70.484375 ; top5 ->  88.15625  and loss:  246.0013144016266
test acc: top1 ->  71.174 ; top5 ->  90.056  and loss:  941.5311625003815
forward train acc: top1 ->  70.4140625 ; top5 ->  88.0625  and loss:  248.4089040160179
test acc: top1 ->  71.182 ; top5 ->  90.11  and loss:  943.3128806352615
forward train acc: top1 ->  71.3828125 ; top5 ->  88.6171875  and loss:  239.4694834947586
test acc: top1 ->  71.376 ; top5 ->  90.14  and loss:  939.3951203227043
forward train acc: top1 ->  70.296875 ; top5 ->  88.125  and loss:  247.26183050870895
test acc: top1 ->  71.186 ; top5 ->  90.074  and loss:  941.080961227417
forward train acc: top1 ->  70.8671875 ; top5 ->  88.453125  and loss:  240.11363005638123
test acc: top1 ->  71.228 ; top5 ->  90.018  and loss:  941.4456714391708
>>>>>>> reverse layer  30  since performance drop >>>>>>>
==> this epoch:  503 / 512 , inc:  1
---------------- start layer  31  ---------------
adv train loss:  -60.41565042734146 , diff:  60.41565042734146
adv train loss:  -58.048830687999725 , diff:  2.366819739341736
layer  31  adv train finish, try to retain  495
test acc: top1 ->  70.782 ; top5 ->  89.834  and loss:  963.6172744631767
forward train acc: top1 ->  71.203125 ; top5 ->  88.6640625  and loss:  241.52441412210464
test acc: top1 ->  70.974 ; top5 ->  89.882  and loss:  952.6672512888908
forward train acc: top1 ->  70.234375 ; top5 ->  87.9921875  and loss:  248.03789222240448
test acc: top1 ->  71.024 ; top5 ->  89.96  and loss:  948.459590792656
forward train acc: top1 ->  71.2890625 ; top5 ->  88.265625  and loss:  240.50587701797485
test acc: top1 ->  71.092 ; top5 ->  89.928  and loss:  946.6937681436539
forward train acc: top1 ->  70.25 ; top5 ->  87.8046875  and loss:  250.33806335926056
test acc: top1 ->  71.188 ; top5 ->  89.932  and loss:  944.2383739352226
forward train acc: top1 ->  70.6640625 ; top5 ->  88.375  and loss:  244.02501183748245
test acc: top1 ->  71.274 ; top5 ->  89.99  and loss:  942.2700958848
forward train acc: top1 ->  70.6953125 ; top5 ->  88.2421875  and loss:  245.39406019449234
test acc: top1 ->  71.318 ; top5 ->  89.996  and loss:  940.5685372948647
forward train acc: top1 ->  71.4140625 ; top5 ->  88.8046875  and loss:  236.5063190460205
test acc: top1 ->  71.288 ; top5 ->  90.082  and loss:  940.0561115145683
forward train acc: top1 ->  70.0859375 ; top5 ->  88.0703125  and loss:  249.15054762363434
test acc: top1 ->  71.3 ; top5 ->  90.036  and loss:  939.2977664470673
forward train acc: top1 ->  70.90625 ; top5 ->  88.3828125  and loss:  243.60457175970078
test acc: top1 ->  71.368 ; top5 ->  90.068  and loss:  937.0050233602524
forward train acc: top1 ->  71.2890625 ; top5 ->  88.6796875  and loss:  238.4326332807541
test acc: top1 ->  71.402 ; top5 ->  90.04  and loss:  938.7571784853935
>>>>>>> reverse layer  31  since performance drop >>>>>>>
==> this epoch:  501 / 512 , inc:  1
layer  0  :  0.578125  ==>  37 / 64 , inc:  1
layer  1  :  0.6875  ==>  44 / 64 , inc:  1
layer  2  :  0.890625  ==>  57 / 64 , inc:  1
layer  3  :  0.671875  ==>  43 / 64 , inc:  1
layer  4  :  0.625  ==>  40 / 64 , inc:  1
layer  5  :  0.453125  ==>  29 / 64 , inc:  1
layer  6  :  0.875  ==>  112 / 128 , inc:  1
layer  7  :  0.953125  ==>  122 / 128 , inc:  1
layer  8  :  0.6796875  ==>  87 / 128 , inc:  1
layer  9  :  0.8984375  ==>  115 / 128 , inc:  1
layer  10  :  0.6796875  ==>  87 / 128 , inc:  1
layer  11  :  0.78125  ==>  100 / 128 , inc:  1
layer  12  :  0.765625  ==>  98 / 128 , inc:  1
layer  13  :  0.5546875  ==>  71 / 128 , inc:  1
layer  14  :  0.9765625  ==>  250 / 256 , inc:  1
layer  15  :  0.96875  ==>  248 / 256 , inc:  1
layer  16  :  0.93359375  ==>  239 / 256 , inc:  1
layer  17  :  0.8984375  ==>  230 / 256 , inc:  1
layer  18  :  0.91796875  ==>  235 / 256 , inc:  1
layer  19  :  0.69921875  ==>  179 / 256 , inc:  1
layer  20  :  0.87890625  ==>  225 / 256 , inc:  1
layer  21  :  0.9453125  ==>  242 / 256 , inc:  1
layer  22  :  0.8125  ==>  208 / 256 , inc:  1
layer  23  :  0.73046875  ==>  187 / 256 , inc:  1
layer  24  :  0.953125  ==>  244 / 256 , inc:  1
layer  25  :  0.9453125  ==>  242 / 256 , inc:  1
layer  26  :  0.98046875  ==>  502 / 512 , inc:  1
layer  27  :  0.986328125  ==>  505 / 512 , inc:  1
layer  28  :  0.982421875  ==>  503 / 512 , inc:  1
layer  29  :  0.97265625  ==>  498 / 512 , inc:  1
layer  30  :  0.982421875  ==>  503 / 512 , inc:  1
layer  31  :  0.978515625  ==>  501 / 512 , inc:  1
eps [0.04054573059082032, 0.02027286529541016, 0.0009502905607223513, 0.02027286529541016, 0.01013643264770508, 0.28832519531250006, 0.00047514528036117563, 0.00047514528036117563, 0.00506821632385254, 0.00047514528036117563, 0.00506821632385254, 0.0019005811214447025, 0.0019005811214447025, 0.02027286529541016, 8.908974006772043e-05, 0.00023757264018058782, 0.00023757264018058782, 0.00047514528036117563, 0.00023757264018058782, 0.00506821632385254, 0.00047514528036117563, 0.00023757264018058782, 0.00253410816192627, 0.00675762176513672, 0.00023757264018058782, 0.00023757264018058782, 0.00011878632009029391, 2.2272435016930108e-05, 2.2272435016930108e-05, 0.00011878632009029391, 0.00011878632009029391, 0.00011878632009029391]  wait [4, 3, 2, 3, 2, 0, 2, 2, 2, 2, 2, 4, 4, 4, 4, 2, 2, 3, 2, 3, 3, 2, 2, 0, 2, 2, 2, 3, 3, 2, 2, 2]  inc [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  tol: 8
!!!!!!!!!SAVING!!!!!!!!!
$$$$$$$$$$$$$ epoch  29  $$$$$$$$$$$$
---------------- start layer  0  ---------------
### skip layer  0 wait:  4  ###
---------------- start layer  1  ---------------
### skip layer  1 wait:  3  ###
---------------- start layer  2  ---------------
adv train loss:  -63.76629203557968 , diff:  63.76629203557968
adv train loss:  -59.6955646276474 , diff:  4.0707274079322815
layer  2  adv train finish, try to retain  49
test acc: top1 ->  70.074 ; top5 ->  89.526  and loss:  977.5087659358978
forward train acc: top1 ->  70.0703125 ; top5 ->  87.84375  and loss:  249.19023823738098
test acc: top1 ->  71.058 ; top5 ->  89.882  and loss:  947.2185060381889
forward train acc: top1 ->  70.875 ; top5 ->  87.96875  and loss:  244.74231362342834
test acc: top1 ->  71.11 ; top5 ->  89.956  and loss:  944.3206818699837
forward train acc: top1 ->  70.7890625 ; top5 ->  88.1015625  and loss:  246.0267989039421
test acc: top1 ->  71.064 ; top5 ->  90.02  and loss:  945.6021550893784
forward train acc: top1 ->  70.390625 ; top5 ->  87.90625  and loss:  247.10840809345245
test acc: top1 ->  71.108 ; top5 ->  89.994  and loss:  942.3024486899376
forward train acc: top1 ->  71.28125 ; top5 ->  88.484375  and loss:  242.50879538059235
test acc: top1 ->  71.168 ; top5 ->  90.036  and loss:  943.0152168273926
forward train acc: top1 ->  71.2265625 ; top5 ->  88.4140625  and loss:  243.67092317342758
test acc: top1 ->  71.264 ; top5 ->  90.062  and loss:  940.8628472685814
forward train acc: top1 ->  71.0546875 ; top5 ->  88.328125  and loss:  242.34376418590546
test acc: top1 ->  71.192 ; top5 ->  90.144  and loss:  939.3717173933983
forward train acc: top1 ->  70.5390625 ; top5 ->  87.9921875  and loss:  245.25062704086304
test acc: top1 ->  71.326 ; top5 ->  90.096  and loss:  939.287489771843
forward train acc: top1 ->  70.5078125 ; top5 ->  87.984375  and loss:  247.26816695928574
test acc: top1 ->  71.296 ; top5 ->  90.062  and loss:  939.6950178146362
forward train acc: top1 ->  71.390625 ; top5 ->  89.046875  and loss:  235.6780155301094
test acc: top1 ->  71.4 ; top5 ->  90.124  and loss:  936.5556152462959
>>>>>>> reverse layer  2  since performance drop >>>>>>>
==> this epoch:  57 / 64 , inc:  1
---------------- start layer  3  ---------------
### skip layer  3 wait:  3  ###
---------------- start layer  4  ---------------
adv train loss:  -61.31627118587494 , diff:  61.31627118587494
adv train loss:  -62.349283039569855 , diff:  1.0330118536949158
layer  4  adv train finish, try to retain  42
>>>>>>> reverse layer  4  since no improvement >>>>>>>
---------------- start layer  5  ---------------
adv train loss:  -58.543072283267975 , diff:  58.543072283267975
adv train loss:  -62.560916781425476 , diff:  4.017844498157501
layer  5  adv train finish, try to retain  12
test acc: top1 ->  64.806 ; top5 ->  86.056  and loss:  1165.4443464875221
forward train acc: top1 ->  70.09375 ; top5 ->  88.0625  and loss:  247.61531352996826
test acc: top1 ->  70.87 ; top5 ->  89.812  and loss:  954.7272288799286
forward train acc: top1 ->  69.9609375 ; top5 ->  87.875  and loss:  250.80621129274368
test acc: top1 ->  70.92 ; top5 ->  89.818  and loss:  953.8404162526131
forward train acc: top1 ->  70.484375 ; top5 ->  88.40625  and loss:  243.4106643795967
test acc: top1 ->  71.04 ; top5 ->  89.942  and loss:  956.201540350914
forward train acc: top1 ->  70.625 ; top5 ->  88.5546875  and loss:  242.77838903665543
test acc: top1 ->  70.994 ; top5 ->  89.9  and loss:  951.4553461074829
forward train acc: top1 ->  70.6796875 ; top5 ->  88.3984375  and loss:  246.25184798240662
test acc: top1 ->  71.004 ; top5 ->  89.978  and loss:  948.2360715270042
forward train acc: top1 ->  70.53125 ; top5 ->  88.015625  and loss:  246.9044587612152
test acc: top1 ->  71.13 ; top5 ->  90.002  and loss:  943.9031041264534
forward train acc: top1 ->  70.8515625 ; top5 ->  88.4921875  and loss:  242.36747735738754
test acc: top1 ->  71.114 ; top5 ->  89.968  and loss:  948.5617535114288
forward train acc: top1 ->  70.9140625 ; top5 ->  88.0703125  and loss:  246.7048241496086
test acc: top1 ->  71.156 ; top5 ->  89.992  and loss:  946.3845963478088
forward train acc: top1 ->  71.2734375 ; top5 ->  88.40625  and loss:  239.13765692710876
test acc: top1 ->  71.164 ; top5 ->  89.998  and loss:  947.3671382665634
forward train acc: top1 ->  71.2265625 ; top5 ->  87.9453125  and loss:  246.85919570922852
test acc: top1 ->  71.242 ; top5 ->  90.082  and loss:  944.6188690066338
>>>>>>> reverse layer  5  since performance drop >>>>>>>
==> this epoch:  29 / 64 , inc:  1
---------------- start layer  6  ---------------
adv train loss:  -60.81195878982544 , diff:  60.81195878982544
adv train loss:  -64.37562334537506 , diff:  3.5636645555496216
layer  6  adv train finish, try to retain  117
>>>>>>> reverse layer  6  since no improvement >>>>>>>
---------------- start layer  7  ---------------
adv train loss:  -57.596773982048035 , diff:  57.596773982048035
adv train loss:  -61.24193620681763 , diff:  3.6451622247695923
layer  7  adv train finish, try to retain  111
test acc: top1 ->  69.39 ; top5 ->  88.974  and loss:  999.6420474648476
forward train acc: top1 ->  70.328125 ; top5 ->  88.5859375  and loss:  247.35511684417725
test acc: top1 ->  70.938 ; top5 ->  89.878  and loss:  951.0729026794434
forward train acc: top1 ->  70.28125 ; top5 ->  87.9609375  and loss:  247.31472820043564
test acc: top1 ->  71.034 ; top5 ->  89.978  and loss:  948.5962604284286
forward train acc: top1 ->  70.1015625 ; top5 ->  88.234375  and loss:  246.62218523025513
test acc: top1 ->  71.108 ; top5 ->  90.044  and loss:  947.1591052412987
forward train acc: top1 ->  70.5078125 ; top5 ->  88.25  and loss:  244.3845835328102
test acc: top1 ->  71.136 ; top5 ->  90.054  and loss:  943.1606811285019
forward train acc: top1 ->  70.3671875 ; top5 ->  88.2890625  and loss:  244.1501390337944
test acc: top1 ->  71.088 ; top5 ->  90.028  and loss:  944.8066778182983
forward train acc: top1 ->  71.09375 ; top5 ->  88.234375  and loss:  244.17696052789688
test acc: top1 ->  71.124 ; top5 ->  90.016  and loss:  941.8515443205833
forward train acc: top1 ->  70.7421875 ; top5 ->  88.578125  and loss:  241.93798553943634
test acc: top1 ->  71.232 ; top5 ->  90.08  and loss:  940.3535507917404
forward train acc: top1 ->  71.09375 ; top5 ->  88.4609375  and loss:  241.7568999528885
test acc: top1 ->  71.3 ; top5 ->  90.046  and loss:  943.4116186499596
forward train acc: top1 ->  70.75 ; top5 ->  88.1328125  and loss:  243.7223865389824
test acc: top1 ->  71.264 ; top5 ->  90.066  and loss:  941.6878758072853
forward train acc: top1 ->  70.8046875 ; top5 ->  88.1875  and loss:  241.2636547088623
test acc: top1 ->  71.234 ; top5 ->  89.968  and loss:  941.4197479486465
>>>>>>> reverse layer  7  since performance drop >>>>>>>
==> this epoch:  122 / 128 , inc:  1
---------------- start layer  8  ---------------
adv train loss:  -60.13053047657013 , diff:  60.13053047657013
adv train loss:  -60.80313241481781 , diff:  0.6726019382476807
layer  8  adv train finish, try to retain  101
>>>>>>> reverse layer  8  since no improvement >>>>>>>
---------------- start layer  9  ---------------
adv train loss:  -62.73491054773331 , diff:  62.73491054773331
adv train loss:  -60.578027188777924 , diff:  2.1568833589553833
layer  9  adv train finish, try to retain  122
>>>>>>> reverse layer  9  since no improvement >>>>>>>
---------------- start layer  10  ---------------
adv train loss:  -61.85234224796295 , diff:  61.85234224796295
adv train loss:  -59.21283608675003 , diff:  2.639506161212921
layer  10  adv train finish, try to retain  82
test acc: top1 ->  69.96 ; top5 ->  89.23  and loss:  992.3408084511757
forward train acc: top1 ->  70.71875 ; top5 ->  88.140625  and loss:  244.40370053052902
test acc: top1 ->  70.988 ; top5 ->  89.938  and loss:  949.2430990338326
forward train acc: top1 ->  70.8203125 ; top5 ->  88.0703125  and loss:  246.6617681980133
test acc: top1 ->  70.92 ; top5 ->  90.022  and loss:  948.0256344676018
forward train acc: top1 ->  70.28125 ; top5 ->  87.96875  and loss:  249.8074397444725
test acc: top1 ->  70.954 ; top5 ->  90.0  and loss:  947.9707219004631
forward train acc: top1 ->  70.6171875 ; top5 ->  88.53125  and loss:  242.42084681987762
test acc: top1 ->  71.072 ; top5 ->  89.934  and loss:  947.314266204834
forward train acc: top1 ->  69.890625 ; top5 ->  88.203125  and loss:  245.2004017829895
test acc: top1 ->  71.164 ; top5 ->  90.01  and loss:  943.7706146836281
forward train acc: top1 ->  70.65625 ; top5 ->  87.8515625  and loss:  246.05994874238968
test acc: top1 ->  71.126 ; top5 ->  90.046  and loss:  943.1726011037827
forward train acc: top1 ->  70.6875 ; top5 ->  88.0859375  and loss:  246.5113958120346
test acc: top1 ->  71.15 ; top5 ->  90.062  and loss:  941.8572247624397
forward train acc: top1 ->  69.6484375 ; top5 ->  87.3359375  and loss:  253.52309399843216
test acc: top1 ->  71.106 ; top5 ->  90.13  and loss:  942.8015222549438
forward train acc: top1 ->  70.5546875 ; top5 ->  88.21875  and loss:  244.32761907577515
test acc: top1 ->  71.2 ; top5 ->  90.12  and loss:  941.4771333932877
forward train acc: top1 ->  70.2109375 ; top5 ->  88.2578125  and loss:  247.14529883861542
test acc: top1 ->  71.184 ; top5 ->  90.086  and loss:  943.8326100111008
>>>>>>> reverse layer  10  since performance drop >>>>>>>
==> this epoch:  87 / 128 , inc:  1
---------------- start layer  11  ---------------
### skip layer  11 wait:  4  ###
---------------- start layer  12  ---------------
### skip layer  12 wait:  4  ###
---------------- start layer  13  ---------------
### skip layer  13 wait:  4  ###
---------------- start layer  14  ---------------
### skip layer  14 wait:  4  ###
---------------- start layer  15  ---------------
adv train loss:  -59.064645648002625 , diff:  59.064645648002625
adv train loss:  -61.98163914680481 , diff:  2.916993498802185
layer  15  adv train finish, try to retain  241
test acc: top1 ->  70.354 ; top5 ->  89.606  and loss:  969.7651090621948
forward train acc: top1 ->  70.671875 ; top5 ->  88.3125  and loss:  245.2057398557663
test acc: top1 ->  70.942 ; top5 ->  89.86  and loss:  951.8828511238098
forward train acc: top1 ->  70.2109375 ; top5 ->  87.65625  and loss:  248.81912326812744
test acc: top1 ->  71.104 ; top5 ->  89.936  and loss:  950.2056651115417
forward train acc: top1 ->  70.7109375 ; top5 ->  88.1953125  and loss:  248.07255721092224
test acc: top1 ->  70.954 ; top5 ->  89.898  and loss:  947.8657395243645
forward train acc: top1 ->  70.6171875 ; top5 ->  88.2578125  and loss:  244.64461588859558
test acc: top1 ->  71.028 ; top5 ->  89.928  and loss:  947.8851869702339
forward train acc: top1 ->  70.9375 ; top5 ->  88.4453125  and loss:  241.42381936311722
test acc: top1 ->  71.108 ; top5 ->  89.978  and loss:  943.8128777146339
forward train acc: top1 ->  71.375 ; top5 ->  88.3515625  and loss:  243.9431156516075
test acc: top1 ->  71.094 ; top5 ->  90.022  and loss:  940.7646409869194
forward train acc: top1 ->  70.4921875 ; top5 ->  88.4296875  and loss:  243.11525827646255
test acc: top1 ->  71.198 ; top5 ->  90.036  and loss:  941.4656277894974
forward train acc: top1 ->  70.796875 ; top5 ->  88.3046875  and loss:  245.0341054201126
test acc: top1 ->  71.202 ; top5 ->  90.082  and loss:  942.316920042038
forward train acc: top1 ->  70.5078125 ; top5 ->  87.8984375  and loss:  248.18584644794464
test acc: top1 ->  71.106 ; top5 ->  90.054  and loss:  944.3521058559418
forward train acc: top1 ->  70.921875 ; top5 ->  88.1640625  and loss:  242.2073221206665
test acc: top1 ->  71.122 ; top5 ->  90.032  and loss:  942.3977243304253
>>>>>>> reverse layer  15  since performance drop >>>>>>>
==> this epoch:  248 / 256 , inc:  1
---------------- start layer  16  ---------------
adv train loss:  -59.35816991329193 , diff:  59.35816991329193
adv train loss:  -58.054424822330475 , diff:  1.3037450909614563
layer  16  adv train finish, try to retain  240
>>>>>>> reverse layer  16  since no improvement >>>>>>>
---------------- start layer  17  ---------------
### skip layer  17 wait:  3  ###
---------------- start layer  18  ---------------
adv train loss:  -59.611130356788635 , diff:  59.611130356788635
adv train loss:  -62.80458337068558 , diff:  3.193453013896942
layer  18  adv train finish, try to retain  245
>>>>>>> reverse layer  18  since no improvement >>>>>>>
---------------- start layer  19  ---------------
### skip layer  19 wait:  3  ###
---------------- start layer  20  ---------------
### skip layer  20 wait:  3  ###
---------------- start layer  21  ---------------
adv train loss:  -61.4854821562767 , diff:  61.4854821562767
adv train loss:  -59.936839044094086 , diff:  1.5486431121826172
layer  21  adv train finish, try to retain  244
>>>>>>> reverse layer  21  since no improvement >>>>>>>
---------------- start layer  22  ---------------
adv train loss:  -61.098359167575836 , diff:  61.098359167575836
adv train loss:  -58.87723118066788 , diff:  2.221127986907959
layer  22  adv train finish, try to retain  197
test acc: top1 ->  70.774 ; top5 ->  89.83  and loss:  964.6314069032669
forward train acc: top1 ->  70.4375 ; top5 ->  88.109375  and loss:  244.66791397333145
test acc: top1 ->  70.934 ; top5 ->  89.896  and loss:  954.2506311535835
forward train acc: top1 ->  70.265625 ; top5 ->  87.9375  and loss:  249.9379028081894
test acc: top1 ->  70.974 ; top5 ->  89.998  and loss:  946.2275359630585
